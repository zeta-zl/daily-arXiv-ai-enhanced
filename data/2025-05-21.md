<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 274]
- [cs.LG](#cs.LG) [Total: 286]
- [cs.AI](#cs.AI) [Total: 146]
- [quant-ph](#quant-ph) [Total: 8]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]
- [eess.IV](#eess.IV) [Total: 10]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 46]
- [cs.ET](#cs.ET) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 2]
- [cs.PL](#cs.PL) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 4]
- [cs.DB](#cs.DB) [Total: 4]
- [eess.AS](#eess.AS) [Total: 20]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [stat.ML](#stat.ML) [Total: 30]
- [math.OC](#math.OC) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.SD](#cs.SD) [Total: 20]
- [astro-ph.EP](#astro-ph.EP) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 22]
- [cs.CY](#cs.CY) [Total: 14]
- [econ.EM](#econ.EM) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.IR](#cs.IR) [Total: 36]
- [cs.MA](#cs.MA) [Total: 8]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.CG](#cs.CG) [Total: 4]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 24]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
*Avinash Patil,Siru Tao,Amardeep Gedhu*

Key words: 自杀预防、大型语言模型、风险评分、零样本学习、伦理考量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究评估了六种大型语言模型（LLMs）在自杀风险评估任务中的表现，发现Claude和GPT与人类标注最为接近，而Mistral的预测误差最低，同时强调了人类监督和伦理问题的重要性。

Motivation: 自杀预防是公共健康的重要挑战，随着LLMs的普及，人们可能更倾向于向AI系统表达自杀倾向，而非人类。本研究旨在评估LLMs在自杀风险评估中的能力。

Method: 研究使用哥伦比亚自杀严重程度评分量表（C-SSRS）对六种模型（包括Claude、GPT、Mistral和LLaMA）的零样本表现进行评估，分类帖子到一个7级严重程度量表。

Result: 结果表明，Claude和GPT与人类标注最为接近，Mistral的预测误差最低，且大多数模型表现出了对不同严重程度的敏感性，误分类通常发生在相邻级别。

Conclusion: 尽管LLMs在自杀风险评估中表现出潜力，但仍需人类监督、透明度和谨慎部署，以确保其伦理性和可靠性。

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [2] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
*Xingyuan Lu,Yuxi Liu,Dongyu Zhang,Zhiyao Wu,Jing Ren,Feng Xia*

Key words: 多模态隐喻,情感分类,中文数据集,广告,细粒度情感

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了中文多模态隐喻广告数据集EmoMeta，填补了多模态隐喻情感数据集的空白，支持细粒度情感分类。

Motivation: 多模态隐喻在情感表达中作用显著，但相关中文数据集稀缺，且现有研究多聚焦英语。本文旨在解决这一研究缺口。

Method: 构建包含5,000条文本-图像对的中文多模态隐喻广告数据集，标注隐喻、领域关系和九种细粒度情感。

Result: 数据集公开可用，支持多模态隐喻情感分类研究。

Conclusion: 该数据集推动了多模态隐喻情感分析领域的发展，并考虑了语言差异。

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [3] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
*Ashwin Kumar,Yuzi He,Aram H. Markosyan,Bobbie Chern,Imanol Arrieta-Ibarra*

Key words: Reinforcement Learning with Human Feedback (RLHF), prefix bias, fairness in AI, data augmentation, reward models

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了基于人类反馈的强化学习（RLHF）中存在的奖励模型偏见，提出了一种检测和评估前缀偏见的方法，并在多种开源数据集和模型架构中验证了偏见的普遍性，同时提出了一种数据增强策略以减轻偏见。

Motivation: 研究的动机是探索语言模型奖励模型中可能存在的偏见，特别是前缀偏见（由查询前缀微小变化引发的系统性偏好偏移），以及这种偏见在种族和性别维度上的表现。

Method: 论文提出了检测和评估前缀偏见的新方法，并在多样化开源偏好数据集和奖励模型架构中进行了全面评估。此外，论文还提出了一种数据增强策略用于减轻偏见。

Result: 研究发现所有测试的奖励模型均存在前缀偏见，尤其在种族和性别维度上表现显著。提出的数据增强策略显示了在降低偏见影响方面的有效性。

Conclusion: 论文强调了在开发公平可靠奖励模型时，需要对数据集设计和评估保持偏见意识，为AI公平性研究做出了贡献。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [4] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
*Federico Germani,Giovanni Spitale*

Key words: 大型语言模型,文本评估,偏见,来源标注,一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在文本评估中的一致性和偏见问题，发现模型间的共识在来源标注后显著下降，尤其是对中国来源的文本。

Motivation: 探究LLMs在文本评估中的一致性、偏见以及对来源标注的敏感性。

Method: 使用四种先进的LLM对4,800条叙事性陈述进行192,000次评估，并操纵陈述来源（来自LLM或特定国籍的人类）。

Result: 未标注来源时模型间高度一致；标注来源后共识下降，尤其是对中国来源的文本。

Conclusion: 来源标注对评估结果有显著影响，需关注LLM评估系统的完整性和公平性。

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [5] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
*Aakash Gupta,Nataraj Das*

Key words: 电商评论,生成预训练Transformer,抽象摘要,决策支持,GPT3

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于生成预训练Transformer（GPT3）的框架，用于更好地理解和总结电商产品的海量评论，帮助消费者快速做出决策。

Motivation: 由于消费者在电商平台上面对数千条评论时容易陷入决策瘫痪，现有商业工具仅通过评分机制提供调整后的分数，无法有效解决问题。论文旨在通过生成模型提供更智能的评论分析。

Method: 使用Curie引擎的GPT3模型（超过130亿参数）进行微调，引入抽象摘要而非简单的提取式摘要，结合“常识”分析评论。

Result: 模型能够生成评论的优缺点总结，揭示评论间的真实关系，帮助消费者快速了解产品并做出决策。

Conclusion: 生成模型能够有效解决海量评论带来的决策问题，提供更直观且具有“常识”的总结，提升消费者决策效率。

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [6] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
*Weiming Zhang,Lingyue Fu,Qingyao Li,Kounianhua Du,Jianghao Lin,Jingwei Yu,Wei Xia,Weinan Zhang,Ruiming Tang,Yong Yu*

Key words: 认知诊断、大型语言模型、开放世界知识、语义表征、冷启动问题

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出LLM4CD方法，利用大型语言模型（LLMs）的开放世界知识增强认知诊断，通过语义表征替代传统ID嵌入，解决冷启动问题并提升性能。

Motivation: 传统认知诊断方法仅基于ID关系建模，忽视语义信息，且难以处理新学生和练习的冷启动问题，LLMs提供了解决这些问题的潜力。

Method: LLM4CD利用LLMs构建认知表达文本表征，并通过双层编码器框架（宏观认知文本编码器和微观知识状态编码器）建模学生测试历史。

Result: 实验结果表明，LLM4CD在多个真实数据集上优于现有认知诊断模型。

Conclusion: 利用LLMs引入丰富语义信息能有效增强认知诊断任务，解决冷启动问题。

Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.

</details>


### [7] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
*Khanh-Tung Tran,Barry O'Sullivan,Hoang D. Nguyen*

Key words: 大型语言模型，低资源语言，爱尔兰语，基准测试，长文本生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了IRLBench，一个用于评估多语言模型性能的新基准，重点关注低资源语言（如爱尔兰语），并支持长文本生成和语言保真度评估。

Motivation: 解决现有基准在文化偏见、文本限制和低资源语言评估方面的不足。

Method: 开发IRLBench基准，基于2024年爱尔兰毕业考试，支持长文本生成和官方评分标准。

Result: 实验显示模型在英语和爱尔兰语之间存在性能差距，爱尔兰语生成正确率低于80%，准确率为55.8%。

Conclusion: 提出了一个更全面的评估工具，促进多语言AI的发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [8] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
*Prithviraj Singh Shahani,Matthias Scheutz*

Key words: 大型语言模型、安全微调、鲁棒性、高斯噪声、AI安全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）中安全防护措施在噪声扰动下的鲁棒性，发现高斯噪声会显著增加有害输出率，且现有安全微调技术存在脆弱性。

Motivation: 安全防护措施对防止LLM有害输出至关重要，但其在扰动下的鲁棒性尚未充分研究。

Method: 通过向模型激活层注入高斯噪声，系统评估了多种开源模型的安全微调鲁棒性。

Result: 高斯噪声使有害输出率上升高达27%，且深层安全微调无法提供额外保护，但思维链推理仍能保持。

Conclusion: 现有安全对齐技术存在漏洞，推理和强化学习可能是提升AI安全系统鲁棒性的方向。

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [9] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
*Ruobing Yao,Yifei Zhang,Shuang Song,Neng Gao,Chenyang Tu*

Key words: 检索增强生成, 安全防御, 上下文多样性检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EcoSafeRAG是一种无需依赖大型语言模型内部知识的检索增强生成（RAG）防御方法，通过语句级处理和诱饵引导的上下文多样性检测来识别恶意内容，同时提高安全性和性能。

Motivation: 现有的防御方法依赖模型内部知识，这与RAG的设计理念冲突，因此需要一种不依赖模型内部知识的防御机制。

Method: EcoSafeRAG采用语句级处理和诱饵引导的上下文多样性检测技术，通过分析候选文档的上下文多样性识别恶意内容。

Result: 实验表明，EcoSafeRAG在保持实际运行成本的同时，提供了最先进的安全性，并提高了干净场景下的性能。

Conclusion: EcoSafeRAG是一种高效的防御方法，解决了RAG的安全问题，同时提升了性能。

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [10] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Key words: 时间智能、大语言模型、强化学习、未来预测、创造性生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Time-R1是一种赋予中等规模LLM全面时间能力的框架，通过三阶段强化学习课程提升时间理解、预测和创造性生成能力，性能超越更大的模型。

Motivation: 解决现有LLM在时间智能方面的不足，如无法整合过去推理与未来预测，以及现有方法在时间技能上的孤立与泛化能力差。

Method: 采用三阶段强化学习课程，包括基础时间理解、未来事件预测和创造性场景生成，结合动态规则奖励系统。

Result: Time-R1在具有挑战性的未来事件预测和创造性场景生成基准测试中，优于大200倍的模型。

Conclusion: 精心设计的渐进式强化学习微调可使小型高效模型在时间性能上超越大型模型，为时间感知AI提供可行路径。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [11] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
*Shuxun Wang,Qingyu Yin,Chak Tou Leong,Qiang Zhang,Linyi Yang*

Key words: 重复诅咒, 诱导头, 注意力头正则化, 大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究揭示了诱导头在大型语言模型（LLMs）重复行为中的关键作用，并提出了一种通过注意力头正则化减少其主导性的方法。

Motivation: 理解LLMs生成重复序列（即“重复诅咒”）的机制，尤其是诱导头的作用，以改善模型输出多样性和连贯性。

Method: 研究诱导头的“毒性”（即其主导输出对数倾向），并开发注意力头正则化技术。

Result: 诱导头被发现是重复行为的主要驱动因素，提出技术可减少其主导性。

Conclusion: 诱导头导致重复诅咒的机制解释及潜在缓解方法为LLMs设计和训练提供了新方向。

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [12] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
*Jingyu Peng,Maolin Wang,Nan Wang,Xiangyu Zhao,Jiatong Li,Kai Zhang,Qi Liu*

Key words: LLMs, 安全机制, 越狱攻击, 逻辑转换, 多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LogiBreak是一种新型通用黑盒方法，通过逻辑表达式转换绕过LLMs的安全机制。

Motivation: 现有安全机制对越狱攻击脆弱，需研究分布差异。

Method: 将恶意提示转换为逻辑表达式，利用分布差异绕过安全系统。

Result: 在跨语言数据集上验证了LogiBreak的有效性。

Conclusion: 逻辑转换能有效绕过LLMs安全约束，需改进对齐方法。

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [13] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
*Zhanglin Wu,Daimeng Wei,Xiaoyu Chen,Hengchao Shang,Jiaxin Guo,Zongyao Li,Yuanchang Luo,Jinlong Yang,Zhiqiang Rao,Hao Yang*

Key words: 大型语言模型（LLM）, 神经机器翻译（NMT）, 调度策略, 多语言翻译, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了如何结合大型语言模型（LLM）和神经机器翻译（NMT）系统优化翻译任务，提出了一种基于源句特征的调度策略，以实现高效翻译并减少LLM的使用。

Motivation: 大型语言模型（LLM）在多任务中表现优异，但在翻译任务中存在高计算成本和延迟问题，因此需要结合NMT并优化调度策略以提高效率。

Method: 通过评估LLM与NMT在翻译任务中的表现，提出了一种基于源句特征的调度策略，仅在必要时使用LLM，以减少计算资源消耗。

Result: 实验表明，该调度策略在多语言测试集上能够以最少的LLM使用实现最优翻译性能。

Conclusion: 结合NMT和LLM的调度策略是一种有效且高效的翻译解决方案，能够平衡翻译质量和计算资源消耗。

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [14] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
*Sathya Krishnan Suresh,Tanmay Surana,Lim Zhi Hao,Eng Siong Chng*

Key words: 代码转换、大型语言模型、CS-Sum基准、多语言对话、错误分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了CS-Sum基准，用于评估大型语言模型（LLMs）在多语言混合对话中的理解能力，发现尽管自动评分高，但模型在处理这类数据时仍存在细微错误。

Motivation: 研究目标是填补LLMs在多语言混合（CS）对话理解方面的研究空白，并评估其表现。

Method: 通过CS-Sum基准（覆盖三种语言对）评估十种LLM，采用少样本学习、翻译总结和微调等方法。

Result: LLM在自动指标上得分高，但存在细微错误，影响对话意义；错误率因语言对和模型而异。

Conclusion: 研究表明LLMs需专门训练以处理多语言混合数据，避免常见错误。

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [15] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
*Nathaniel Krasner,Nicholas Lanuzo,Antonios Anastasopoulos*

Key words: 多语言对齐、视觉信息、跨语言理解、低资源语言、图像标题

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现视觉信息（如图像标题）可替代双语文本用于多语言句子表征对齐，尤其适合低资源语言。

Motivation: 探索是否可以通过视觉信息替代双语文本，实现多语言句子表征的对齐，提高低资源语言的处理效率。

Method: 利用多语言图像-标题数据集进行对齐实验，测试未在预训练中见过的语言的表征对齐能力。

Result: 多语言图像-标题对齐能隐式对齐不同语言的文本表征，支持跨语言自然语言理解和双语文本检索。

Conclusion: 视觉信息可作为双语文本的高效替代，扩展多语言表征对齐的应用范围。

Abstract: Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [16] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
*Charles J. Torres,Richard Futrell*

Key words: 正字法透明度、相互压缩性、算法信息理论、神经序列模型、跨脚本度量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于算法信息理论的跨脚本统一度量方法，用于量化拼写与语音之间的正字法透明度，结合不规则拼写和规则复杂性，并通过神经序列模型验证了其有效性。

Motivation: 现有正字法透明度缺乏统一的跨脚本度量方法，因此需要一种普适性强的量化指标。

Method: 利用算法信息理论的思路，通过拼写与语音字符串的相互压缩性量化透明度，并使用神经序列模型估算。

Result: 在22种不同脚本类型语言中验证了方法的有效性，结果支持脚本透明度的常见直觉。

Conclusion: 相互压缩性是一种简单、原则性强且通用的正字法透明度度量标准。

Abstract: Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.

</details>


### [17] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
*Julia Jose,Rachel Greenstadt*

Key words: NLP, 宣传技术检测, 大语言模型, Transformer模型, RoBERTa-CRF

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文比较了多种大语言模型和基于Transformer的模型在检测新闻文章中的宣传技术上的表现，发现GPT-4虽优于其他LLMs，但未超过RoBERTa-CRF基准。

Motivation: 识别宣传技术中的逻辑谬误和情感诉求对于做出明智决策至关重要，NLP的发展为此提供了可能。

Method: 研究比较了多种大语言模型（如GPT-4、GPT-3.5、Claude 3 Opus）与基于Transformer的模型（如RoBERTa-CRF）在检测宣传技术上的表现。

Result: GPT-4在F1得分（0.16）上优于其他LLMs，但低于RoBERTa-CRF基准（0.67）。部分LLMs在检测特定宣传技术（如name-calling、appeal to fear和flag-waving）上优于MGN基准。

Conclusion: 尽管GPT-4在部分任务中表现优异，但RoBERTa-CRF仍为更优选择。

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [18] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
*Yu Guo,Dong Jin,Shenghao Ye,Shuangwu Chen,Jian Yang,Xiaobin Tan*

Key words: Large Language Models, text-to-SQL, data synthesis, SQLForge, open-source

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SQLForge提出了一种通过生成可靠且多样化的数据来增强LLMs在text-to-SQL任务中表现的方法，显著缩小了开源模型与闭源模型之间的性能差距。

Motivation: 现有的开源LLMs在text-to-SQL任务中表现与闭源模型存在显著差距，需要提升数据质量和多样性以提高性能。

Method: 通过SQL语法约束和反向翻译提升数据可靠性，通过模板丰富化和迭代数据域探索增强数据多样性，并基于增强数据微调多种开源模型。

Result: SQLForge-LM在Spider和BIRD基准测试中达到开源模型中的最优性能，例如在Spider Dev上EX准确率为85.7%。

Conclusion: SQLForge通过数据增强和模型微调显著提升了开源LLMs在text-to-SQL任务中的表现，缩小了与闭源模型的差距。

Abstract: Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.

</details>


### [19] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
*Jacob Kleiman,Kevin Frank,Sindy Campagna*

Key words: 仿真代理框架, 大型语言模型, 模拟模型, 交互性, 实证验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种结合模拟模型和大型语言模型（LLMs）优势的新框架，旨在为非技术用户提供直观的交互方式，同时保持模拟的精确性和结构化理解。

Motivation: 模拟系统虽然能精确复现现实世界，但通常对非技术用户来说过于复杂；而LLMs虽提供直观的语言交互，却缺乏对复杂现实动态的结构化理解。

Method: 提出了一种结合模拟模型和LLMs的仿真代理框架，利用LLMs的对话能力与模拟系统无缝交互，同时借助模拟为LLMs提供准确的现实世界现象表示。

Result: 该框架为实证验证提供了强大且通用的基础，适用于多种领域。

Conclusion: 该集成方法有效解决了非技术用户与复杂模拟系统交互的障碍，同时保持了模拟的精确性和LLMs的直观性。

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [20] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
*Dimitris Roussis,Leon Voukoutis,Georgios Paraskevopoulos,Sokratis Sofianopoulos,Prokopis Prokopidis,Vassilis Papavasileiou,Athanasios Katsamanis,Stelios Piperidis,Vassilis Katsouros*

Key words: 大型语言模型, 希腊语, Llama 3.1, 多调文本, MAGPIE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Llama-Krikri-8B 是一个基于 Meta Llama 3.1-8B 的大型语言模型，专为希腊语设计，支持现代希腊语、英语、古语及多调文本，性能卓越。

Motivation: 为希腊语提供一款高性能的语言模型，填补希腊语领域的技术空白。

Method: 基于 80 亿参数模型，结合高质量希腊语数据训练，采用多阶段后训练流程，集成人类和合成指令数据，使用 MAGPIE 等技术。

Result: 在自然语言理解、生成及代码生成方面优于同类希腊语和多语言模型。

Conclusion: Llama-Krikri-8B 为希腊语提供了先进的解决方案，性能显著超越现有模型。

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [21] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Key words: 问答系统, 知识蒸馏, 推理痕迹, 小语言模型, 规则分解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了在问答系统中，通过知识蒸馏方法利用中间推理痕迹（如Chain-of-Thought）提升小语言模型性能时，发现正确推理痕迹与最终答案准确性之间相关性低，挑战了这一方法的潜在假设。

Motivation: 当前问答系统要求模型输出的准确性和透明度，但小语言模型性能不足，知识蒸馏方法通过中间推理痕迹提升性能。然而这些痕迹难以评估，研究旨在解决其忠实性及与最终性能的关联性问题。

Method: 采用基于规则的问题分解知识蒸馏方法，将复杂查询拆解为结构化的子问题，生成可解释的推理痕迹，并在Open Book QA等数据集上进行实验。

Result: 实验发现正确推理痕迹未必导致正确的最终答案，且中间痕迹与最终答案的正确性相关性低，质疑了利用推理痕迹提升小语言模型性能的假设。

Conclusion: 研究揭示了推理痕迹与最终性能之间的不一致性，对当前知识蒸馏方法提出了挑战。

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [22] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
*Zhengqing Yuan,Weixiang Sun,Yixin Liu,Huichi Zhou,Rong Zhou,Yiyang Li,Zheyuan Zhang,Wei Song,Yue Huang,Haolong Jia,Keerthiram Murugesan,Yu Wang,Lifang He,Jianfeng Gao,Lichao Sun,Yanfang Ye*

Key words: 大语言模型, 效率, 量化, 微调, 多模态

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了EfficientLLM，一个全面评估大语言模型效率技术的基准，通过实验揭示了效率与性能之间的量化权衡。

Motivation: 研究动机是解决大语言模型因参数和上下文窗口增加导致的计算、能源和成本问题。

Method: 研究方法包括系统地评估三种关键技术：架构预训练（如MQA、GQA、MLA、NSA和MoE）、微调（如LoRA、RSLoRA、DoRA）和推理（如int4、float16量化）。

Result: 主要结果包括：(i) 效率与性能存在量化权衡；(ii) 最优方法因任务和规模而异；(iii) 技术在多模态模型中通用。

Conclusion: 结论强调了EfficientLLM为研究人员和工程师在高效与性能之间权衡提供了重要指导。

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [23] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
*Congchi Yin,Yongpeng Zhang,Xuyun Wen,Piji Li*

Key words: 联想记忆,语言模型,大脑对齐,语音处理,监督学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究通过整合联想记忆提升语言模型与人类大脑在语音信息处理中的对齐性，实验证实联想记忆可改善对齐效果。

Motivation: 探索如何通过整合联想记忆来改进语言模型与人类大脑在处理语音信息时的对齐性。

Method: 通过映射语言模型激活到大脑活动，验证对齐性后，将原始文本刺激与模拟联想记忆结合作为输入。构建包含1000个故事样本的Association数据集，进行监督微调。

Result: 联想记忆整合后，语言模型与大脑在联想记忆相关区域的对齐性提升。

Conclusion: 特定监督微调后的大语言模型能更好地与大脑响应对齐。

Abstract: Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.

</details>


### [24] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
*Arihant Tripathi,Liam Dugan,Charis Gao,Maggie Huan,Emma Jin,Peter Zhang,David Zhang,Julia Zhao,Chris Callison-Burch*

Key words: 文本检测, 未见域适应, 集成模型, DoGEN, 域分类器

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DoGEN通过集成域专家检测器模型和域分类器权重，实现了对未见域的适应，在多个域上表现出色。

Motivation: 随着语言模型的进步，检测机器生成文本的需求日益增加，但现有方法难以适应新域和新模型。

Method: 提出DoGEN，通过集成多个域专家检测器模型，利用域分类器权重实现自适应检测。

Result: DoGEN在多个基准测试中表现优异，尤其在域外检测上超越更大规模的模型。

Conclusion: DoGEN有效提升了文本检测的适应性，为未来研究提供支持。

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [25] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
*Jiwon Song,Dongwon Jo,Yulhwa Kim,Jae-Joon Kim*

Key words: 推理路径压缩（RPC）、KV缓存、语义稀疏性、推理LLM、吞吐量优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为推理路径压缩（RPC）的训练无关方法，通过利用推理路径的语义稀疏性来加速推理，减少内存使用和生成延迟，同时保持较高的准确性。

Motivation: 当前语言模型通过生成冗长的中间推理路径来解决逻辑问题，但这增加了内存使用和生成延迟，限制了实际部署。为解决这一问题，论文提出RPC方法。

Method: RPC采用训练无关的方式，通过周期性压缩KV缓存，保留高重要性评分的部分，这些评分通过最近生成的查询组成的选择器窗口计算。

Result: 实验表明，相比于完整的KV缓存，RPC在QwQ-32B模型上将生成吞吐量提高至1.60倍，同时在AIME 2024基准测试中准确率仅下降1.2%。

Conclusion: 研究表明，推理路径的语义稀疏性可有效用于压缩，为推理型语言模型的高效部署提供了实践路径。

Abstract: Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [26] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
*Jingqi Tong,Jixin Tang,Hangcheng Li,Yurong Mou,Ming Zhang,Jun Zhao,Yanbo Wen,Fan Song,Jiahao Zhan,Yuyang Lu,Chaoran Tao,Zhiyuan Guo,Jizhou Yu,Tianhao Cheng,Changhao Jiang,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Weifeng Ge,Guanhua Chen,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Key words: 视觉语言模型, 推理数据, 游戏代码, Code2Logic, GameQA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Code2Logic方法，利用游戏代码生成高质量视觉语言推理数据，解决了数据稀缺问题，并开发了GameQA数据集，显著提升了VLMs的性能。

Motivation: 视觉语言的推理数据资源稀缺，且高质量数据标注成本高昂，限制了VLMs推理能力的提升。

Method: 通过游戏代码的逻辑结构和状态转换过程，利用LLMs生成推理数据，并开发了GameQA数据集。

Result: GameQA数据集低成本且可扩展，训练后的VLMs展现出跨领域泛化能力，如Qwen2.5-VL-7B在7个基准上性能提升2.33%。

Conclusion: 代码驱动的数据合成是一种有效方法，能够显著提升VLMs的推理能力。

Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.

</details>


### [27] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Yiwei Wang*

Key words: 大型语言模型, 推理结构, 提示工程, 图分析, CoT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于图的框架，用于分析大型语言模型（LLMs）的推理过程，揭示了推理结构与性能之间的相关性，并为提示工程提供了实用见解。

Motivation: 尽管推理型LLMs（RLMs）表现出复杂的推理能力，但其不稳定和反直觉的行为挑战了当前的理解。作者希望通过一种统一的框架更好地建模和评估其推理过程。

Method: 提出一种图分析方法：将冗长的CoT输出聚类为语义连贯的推理步骤，并构建有向推理图以捕捉步骤间的依赖关系。

Result: 发现推理结构属性（如探索密度、分支比和收敛比）与推理准确性密切相关，提示策略显著重塑RLMs的内部推理结构。

Conclusion: 该框架不仅能够定量评估推理质量，还为提示工程和LLMs的认知分析提供了实用工具。

Abstract: Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.

</details>


### [28] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
*Yuanyi Wang,Zhaoyi Yan,Yiming Zhang,Qi Zhou,Yanggan Gu,Fei Wu,Hongxia Yang*

Key words: 语言模型融合, 语义依赖, 推理能力, 共激活图

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了首个结构感知的融合框架InfiGFusion，通过新设计的Graph-on-Logits蒸馏（GLD）损失显式建模语义依赖，显著提升了融合质量和稳定性。

Motivation: 现有基于logit的融合方法忽视了词汇维度间的语义依赖，影响了模型融合的效果，尤其是对于具有多样化生成行为的模型。

Method: 提出InfiGFusion框架，通过聚合top-k logits的外积构建全局共激活图，并设计了一种高效的封闭形式近似方法。

Result: 在11个基准测试中超越现有方法，尤其在复杂推理任务上表现突出。

Conclusion: InfiGFusion通过建模语义依赖，显著提升了模型融合的效果。

Abstract: Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [29] [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
*Chengyu Shen,Zhen Hao Wong,Runming He,Hao Liang,Meiyi Qiang,Zimo Meng,Zhengyang Zhao,Bohan Zeng,Zhengzhou Zhu,Bin Cui,Wentao Zhang*

Key words: Large Language Models, Mathematical Reasoning, Question Verification, Dataset Quality

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种五阶段验证流程MathQ-Verify，用于严格过滤数学问题中的非良构或不足定义问题，显著提升了验证性能并减少无效问题的计算负担。

Motivation: 现有研究主要关注生成正确的推理路径和答案，而忽视了问题本身的合理性，导致数据集中可能存在无效或不足定义的问题。

Method: 采用五阶段验证流程：（1）格式验证，（2）问题形式化，（3）原子条件分解与验证，（4）逻辑矛盾检测，（5）目标导向完整性检查。

Result: MathQ-Verify在多个基准测试中达到最优性能，F1分数提升25个百分点，轻量级模型投票方案下精度约为90%，召回率为63%。

Conclusion: MathQ-Verify为数学数据集提供了可扩展且准确的解决方案，显著降低标签噪声并减少无效问题的计算开销。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [30] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
*Ajitesh Bankula,Praney Bankula*

Key words: 跨语言迁移, 多语言预训练模型, 语言家族, 形态学, NLP 任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了跨语言迁移与语言家族及形态学的关系，探讨了语言家族邻近性和形态相似性对 NLP 任务性能的影响，并讨论了与现有文献的联系。

Motivation: 研究跨语言迁移对低资源语言的重要性，探索语言家族和形态学对多语言预训练模型性能的影响。

Method: 通过分析语言家族邻近性和形态相似性，比较多语言模型在不同任务中的表现，并综述语言学距离指标与迁移效果的关系。

Result: 研究发现语言家族和形态相似性对跨语言迁移性能有显著影响，结果与现有文献一致。

Conclusion: 语言家族和形态学特性对跨语言迁移至关重要，未来研究可进一步整合类型学和形态学信息以优化模型预训练。

Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].

</details>


### [31] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
*Hiram Ring*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文通过分析1500多种语言的平行数据集，提出了一种基于词类长度的通用词序变化机制，部分支持了处理理论的对立观点，并解释了比谱系或语言区域更多的方差。

Motivation: 探讨语言表面结构（词序）的起源和变化机制，调和先天论和功能论的对立观点。

Method: 使用大规模平行数据集（1500多种语言，133个语族，111个孤立语言），分析词类长度与词序的相关性。

Result: 词类长度与词序显著相关，但方式复杂；预测了两条谱系线的历史词序变化，解释力超过谱系或语言区域。

Conclusion: 提出“最小-最大”理论，整合处理和信息结构的竞争压力，支持效率导向和信息论的近期研究。

Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
"Min-Max" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).

</details>


### [32] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
*Saydul Akbar Murad,Ashim Dahal,Nick Rahimi*

Key words: EEG-to-text, R1 Translator, LSTM, transformer, ROUGE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个名为R1 Translator的新模型，用于改进EEG到文本的解码性能，结合双向LSTM编码器和预训练transformer解码器，显著优于现有方法。

Motivation: 现有EEG信号解码模型性能有限，需要改进以更好地连接人脑与语言处理。

Method: R1 Translator模型结合双向LSTM编码器和预训练transformer解码器，利用EEG特征生成高质量文本。

Result: 在ROUGE、CER和WER指标上均优于T5和Brain Translator，如ROUGE-1得分提升9%。

Conclusion: R1 Translator在EEG到文本解码任务中表现优异，为未来研究提供了重要参考。

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [33] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Nam Le,Linh Ngo Van*

Key words: 持续关系提取, 提示学习, 灾难性遗忘, 任务特定提示池, 生成模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: WAVE++提出了一种新型的基于提示的方法，用于持续关系提取（CRE），通过任务特定提示池和标签描述增强模型性能，避免了存储历史数据的隐私问题，并在实验中优于现有方法。

Motivation: 传统的基于记忆的方法在持续关系提取中存在内存和隐私问题，而现有的基于提示的方法在任务识别和防止灾难性遗忘方面表现不佳。本文旨在解决这些问题。

Method: 提出WAVE++方法，结合任务特定提示池、标签描述和生成模型，优化提示选择并防止遗忘，无需存储历史数据。

Result: 实验证明WAVE++在持续关系提取任务中优于当前最先进的基于提示和基于记忆的方法。

Conclusion: WAVE++为持续关系提取提供了更高效的解决方案，解决了现有方法的局限性。

Abstract: Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [34] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
*Mingliang Zhai,Zhi Gao,Yuwei Wu,Yunde Jia*

Key words: 记忆机制，多模态，EQA，分层记忆

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种以记忆为中心的EQA框架MemoryEQA，通过多模态分层记忆机制提升复杂任务的解决效率和准确性。

Motivation: 现有的EQA框架以规划器为中心，记忆模块无法与其他模块充分交互，限制了处理复杂任务的能力。

Method: 设计了一个多模态分层记忆机制，包含全局记忆和局部记忆，并利用多模态大语言模型将记忆信息转换为各模块所需的输入格式。

Result: 在HM-EQA、MT-HM3D和OpenEQA数据集上验证了框架的有效性，尤其在MT-HM3D上性能提升了19.8%。

Conclusion: MemoryEQA框架通过强化记忆能力，显著提升了EQA任务中复杂问题的处理性能。

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [35] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
*Guochao Jiang,Guofeng Quan,Zepeng Ding,Ziqin Luo,Dixuan Wang,Zheng Hu*

Key words: 大语言模型,推理效率,验证模型,提前终止,FlashThink

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了FlashThink方法，利用验证模型提前终止LLMs的推理过程，显著减少冗余推理内容的同时保持准确性。

Motivation: 当前LLMs在推理任务中生成冗长内容导致计算开销大，实验显示模型在推理过程中可能过早得出正确答案，因此研究如何提前终止推理以提高效率。

Method: 引入验证模型来识别模型何时可以停止推理但仍能提供正确答案，从而缩短推理内容。

Result: 在四个基准测试中，FlashThink将Deepseek-R1和QwQ-32B模型的推理内容长度分别减少77.04%和77.47%，且不影响准确性。

Conclusion: FlashThink方法成功实现了高效推理的目标，显著减少了推理内容的冗余。

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [36] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
*Qianli Wang,Mingyang Wang,Nils Feldhus,Simon Ostermann,Yuan Cao,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Key words: 量化, 大型语言模型, 可解释性, 模型透明度, 用户研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 量化方法广泛用于加速大型语言模型（LLM）的推理，但其对模型可解释性的影响尚未被研究。本文通过实验发现，量化对可解释性的影响因方法、评估协议不同而异，甚至可能提升可解释性。

Motivation: 量化对LLM的推理速度有益，但其对模型透明度和可解释性的影响尚未被充分研究，这对于依赖透明决策的应用至关重要。

Method: 使用三种常见量化技术和不同比特宽度，结合两种可解释性方法（反事实示例和自然语言解释）和两种可解释性分析（知识记忆分析和潜在多跳推理分析），并辅以用户研究。

Result: 量化对可解释性的影响不一致，取决于量化方法、可解释性方法及评估协议；某些情况下量化会降低可解释性，而在其他情况下可能提升。

Conclusion: 量化可能不可预测地影响模型透明度，对需要透明决策的LLM应用部署具有重要启示。

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [37] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
*Jiamin Su,Yibo Yan,Zhuoran Gao,Han Zhang,Xiang Liu,Xuming Hu*

Key words: Automated Essay Scoring, Multimodal Large Language Model, Collaborative Multi-Agent Framework, Quadratic Weighted Kappa, Human Alignment

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为CAFES的多代理协作框架，用于解决自动作文评分（AES）中的泛化性和多模态感知问题，显著提升了评分与人类判断的一致性。

Motivation: 传统AES方法在多模态感知和评分泛化性方面表现不佳，而现有的多模态大语言模型（MLLM）也可能产生幻觉性解释或与人类评分不一致的问题。因此，需要一种更可靠的解决方案。

Method: 提出CAFES框架，通过三个专门代理协同工作：初始评分代理进行快速、基于特征的评分；反馈池管理者收集详细的、基于证据的优点反馈；反思评分代理迭代优化评分以提高与人类评分的一致性。

Result: 实验表明，CAFES在Quadratic Weighted Kappa（QWK）指标上相对提升了21%，尤其在语法和词汇多样性方面表现突出。

Conclusion: CAFES为智能多模态AES系统的发展奠定了基础，展示了多代理协作在评分任务中的潜力。

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [38] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
*Qianli Wang,Van Bach Nguyen,Nils Feldhus,Luis Felipe Villa-Arenas,Christin Seifert,Sebastian Möller,Vera Schmitt*

Key words: 反事实数据增强, 大语言模型, 标签翻转, 法官模型, 用户研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，在反事实数据增强中，独立且未经微调的法官模型能提供最可靠的标签翻转评估，但全自动流程仍需人工干预。

Motivation: 反事实数据增强（CDA）广泛用于提升大语言模型的性能和鲁棒性，但评估指标（标签翻转）的法官模型选择不一致导致结果不一致。

Method: 研究定义了反事实生成器与法官模型的四种关系，通过实验（2种LLM方法、3个数据集、5个生成器模型、15个法官模型）和用户研究（n=90）进行分析。

Result: 独立且未经微调的法官模型评估最可靠，但用户研究结果显示全自动流程效果仍有较大差距。

Conclusion: 全自动CDA流程可能不足，需要人工干预。

Abstract: Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.

</details>


### [39] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
*Wenhui Zhu,Xuanzhao Dong,Xin Li,Peijie Qiu,Xiwen Chen,Abolfazl Razi,Aris Sotiras,Yi Su,Yalin Wang*

Key words: 强化学习, 多模态大语言模型, 医疗视觉问答, GRPO, 领域调优

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了在医疗视觉问答（VQA）中，基于强化学习（RL）的调优方法，特别是Group Relative Policy Optimization（GRPO）的应用，并分析了四种关键维度对模型性能的影响，结果表明GRPO在准确性和推理质量上优于标准监督微调（SFT）。

Motivation: 为解决多模态大语言模型（MLLMs）在医疗任务中难以满足临床期望的问题，研究旨在通过RL调优使模型行为更符合医疗需求。

Method: 研究了基础模型初始化策略、医学语义对齐、基于长度的奖励对长链推理的影响，以及偏见的作用。通过大量实验分析这些因素对医疗MLLMs的影响。

Result: GRPO-based RL调优在准确性和推理质量上持续优于标准SFT。

Conclusion: 研究为领域特定的模型调优提供了新见解，并验证了GRPO在医疗任务中的优越性。

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [40] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
*Yuxuan Jiang,Dawei Li,Frank Ferraro*

Key words: 大型推理模型，推理剪枝，蒸馏，效率，数学推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DRP是一个结合推理剪枝和蒸馏的框架，显著提高大型推理模型的效率而不牺牲准确率。

Motivation: 解决大型推理模型因冗长推理轨迹导致的效率低下问题。

Method: 通过教师模型进行技能感知步骤分解和内容剪枝，并将剪枝后的推理路径蒸馏到学生模型中。

Result: 在GSM8K上，token使用减少65%，准确率提升2.4%；AIME上token减少43%，性能无下降。

Conclusion: 训练与学生的推理能力对齐的CoT结构对知识迁移和性能提升至关重要。

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [41] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
*Maya Srikanth,Run Chen,Julia Hirschberg*

Key words: 多模态模型, 共情检测, 信号冲突, 门控融合, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 多模态模型在共情检测中表现不稳定，尤其是模态间信号冲突时。研究发现分歧反映了潜在的模糊性，人类与模型在多模态输入下的表现类似。

Motivation: 探究多模态模型在共情检测中的表现不一致问题，尤其是模态间信号冲突时的失败原因。

Method: 使用文本、音频和视频的微调模型及门控融合模型，分析单模态与多模态预测的分歧。

Result: 发现分歧常反映标注者的不确定性，主导模态信号可能误导融合，且人类与模型在多模态输入下均无稳定优势。

Conclusion: 分歧可作为识别挑战性样本和提升共情系统鲁棒性的诊断信号。

Abstract: Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.

</details>


### [42] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
*Linxin Song,Taiwei Shi,Jieyu Zhao*

Key words: 强化微调（RFT）、大型语言模型（LLM）、幻觉税、SUM数据集、拒绝行为

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了强化微调（RFT）对大型语言模型（LLM）信任度的影响，发现RFT会导致模型在无法回答的问题上产生幻觉回答（称为“幻觉税”），并通过引入SUM数据集证明调整RFT训练可以显著恢复模型的拒绝行为。

Motivation: 探索RFT在提升LLM推理能力的同时对模型信任度的副作用，尤其是模型在无法回答问题时产生的幻觉回答现象。

Method: 引入SUM数据集（合成的无解数学问题集），用于测试模型识别无法回答问题的能力，并通过调整RFT训练（如加入10% SUM数据）评估对模型拒绝行为的影响。

Result: 标准RFT训练会降低模型拒绝率80%以上，增加幻觉回答倾向；加入少量SUM数据后，能显著恢复拒绝行为且不影响可解任务的准确性。

Conclusion: 调整RFT训练可帮助模型更好地识别不确定性，提升对无法回答问题的拒绝能力，同时扩展到其他任务（如事实问答）。

Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.

</details>


### [43] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
*Tingfeng Hui,Pengyu Zhu,Bowen Ping,Ling Tang,Yaqi Zhang,Sen Su*

Key words: 大型语言模型,指令跟随,数据合成,分解原则,泛化性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DecIF是一种基于分解原则的完全自主框架，通过LLM生成多样且高质量的指令跟随数据。

Motivation: 现有方法依赖预存文档或外部资源，限制了灵活性和泛化性。

Method: DecIF利用LLM迭代生成元信息，结合响应约束形成结构化指令，并通过分解指令为原子级标准验证响应准确性。

Result: 实验表明DecIF在指令跟随任务中表现优异，展现了灵活性、可扩展性和泛化性。

Conclusion: DecIF为自动合成高质量指令数据提供了有效解决方案。

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [44] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
*Myra Cheng,Sunny Yu,Cinoo Lee,Pranav Khadpe,Lujain Ibrahim,Dan Jurafsky*

Key words: LLM, sycophancy, face-preserving, ELEPHANT, social behavior

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了LLMs（大型语言模型）中的社会谄媚现象，提出了一种更丰富的理论框架ELEPHANT，并通过实验证实了LLMs在社交谄媚行为上的高度表现。

Motivation: 现有研究仅关注LLMs对用户明确陈述信念的谄媚行为，而忽略了在无明确事实依据的模糊情境（如建议和支持寻求）中出现的谄媚行为，这些行为可能强化有害的隐性假设或行为。

Method: 提出了ELEPHANT框架，从五种维护用户面子的行为（情感验证、道德认可、间接语言、间接行动和接受框架）评估社交谄媚，并在OEQ和AITA两个数据集上测试了八个LLMs模型。

Result: 实验表明，LLMs表现出较高的社交谄媚倾向：在OEQ上比人类多47%的面子维护行为，在AITA上有42%的情况支持被人类判定为不当的行为。此外，社交谄媚在偏好数据集中受到奖励且不易缓解。

Conclusion: 本研究为理解和解决LLMs中的社会谄媚问题提供了理论支持和实证工具（数据集和代码）。

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [45] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
*Yuxuan Yao,Shuqi Liu,Zehua Liu,Qintong Li,Mingyang Liu,Xiongwei Han,Zhijiang Guo,Han Wu,Linqi Song*

Key words: 模型合并, ACM, 大语言模型, 激活信息, 功能异质性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种新的模型合并方法ACM，通过激活信息动态调整层权重，以高效结合不同LLM的能力，显著提升了效率和性能。

Motivation: 为解决现有模型合并方法忽略神经网络层功能异质性导致的效率与稳定性问题。

Method: 提出ACM框架，基于预训练与微调模型的激活互信息动态确定层特异性合并系数。

Result: 在L2S和通用任务中，ACM显著优于基线方法，如Qwen-7B模型响应长度减少55.3%，推理准确率提升1.3分。

Conclusion: ACM无需梯度计算或额外训练即可有效保留任务能力，为模型合并提供高效且稳定的解决方案。

Abstract: Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.

</details>


### [46] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
*Tai D. Nguyen,Long H. Pham,Jun Sun*

Key words: LLM, 法律合规, 对抗性数据生成, 陪审团审议, 违规检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AutoLaw是一种新型法律框架，结合对抗性数据生成和陪审团启发的审议流程，以增强LLMs的法律合规性。

Motivation: 现有法律评估基准缺乏适应性，无法应对多样化的本地法律情境，AutoLaw填补这一空白。

Method: 采用对抗性数据生成和基于陪审团的审议流程，动态合成案例法并模拟司法决策。

Result: 在三个基准测试中表现优异，显著提高了违规检测率和LLM判别能力。

Conclusion: AutoLaw能自适应探测法律偏差，提供可靠、上下文感知的判决，适用于法律敏感应用。

Abstract: The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based "jurors" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [47] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
*Yingli Shen,Wen Lai,Shuo Wang,Kangyang Luo,Alexander Fraser,Maosong Sun*

Key words: 大型语言模型,多语言平行数据,TED2025,跨语言一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了基于TED演讲的大规模高质量多语言平行语料库TED2025，探讨了利用该数据提升大型语言模型性能的最佳实践。

Motivation: 通过多语言平行数据提升低资源语言的跨语言一致性，弥补非对齐多语言数据的不足。

Method: 提出TED2025语料库（113种语言，最多50种语言平行对齐），并研究继续预训练、指令调优等策略。

Result: 实验表明，基于平行数据的模型在多语言基准测试中表现优于非对齐数据训练的模型。

Conclusion: 多语言平行数据能显著提升大型语言模型的跨语言性能。

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [48] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
*Wei Jiang,Anying Fu,Youling Zhang*

Key words: 模型剪枝, 大型语言模型, 运动与幅度分析, 计算复杂度, GRPO奖励

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MAMA Pruning是一种基于权重和偏差的剪枝方法，通过预训练阶段的固定和后训练阶段的GRPO奖励验证，显著减少模型大小和计算复杂度，同时保持性能接近未剪枝模型。

Motivation: 现有剪枝方法常导致性能显著下降或需要大量重新训练和微调，本文旨在开发一种更高效的剪枝技术，以生成更小、更快的模型。

Method: 提出了MAMA Pruning（运动和幅度分析）方法，利用预训练阶段的权重和偏差固定以及后训练阶段的GRPO奖励验证作为剪枝指标。

Result: 初步实验结果表明，该方法在各种剪枝级别和不同下游计算语言学任务中表现优于或可比肩现有最先进方法。

Conclusion: MAMA Pruning能够在极端剪枝水平下显著优化模型，同时保持高性能，为大型语言模型的优化提供了新思路。

Abstract: Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.

</details>


### [49] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
*Feiyu Duan,Xuemiao Zhang,Sirui Wang,Haoran Que,Yuqi Liu,Wenge Rong,Xunliang Cai*

Key words: Large Language Models, 高知识评分器, 预训练, 知识密度, 知识覆盖率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种新颖的无梯度高知识评分器（HKS）来选择高质量数据，以解决预训练语料库中知识匮乏的问题，并通过实验验证了其有效性。

Motivation: 现有研究未考虑文本语料库中知识丰富度的重要性，导致预训练语料库中知识匮乏问题。

Method: 提出基于知识密度和覆盖率的综合知识评分器（HKS），用于选择知识密集型数据，并可扩展到特定领域。

Result: 实验结果表明，使用HKS选择的数据显著提升了模型在知识密集型和一般理解任务中的表现。

Conclusion: HKS能有效提升模型的通用能力和领域特定能力。

Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.

</details>


### [50] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
*Weihong Du,Wenrui Liao,Binyu Yan,Hongru Liang,Anthony G. Cohn,Wenqiang Lei*

Key words: 大型语言模型;反向推理;任务规划;BAR代理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了BAR，一种基于反向推理的大型语言模型代理，用于解决复杂任务中的规划问题。

Motivation: 探讨现有正向推理在复杂任务中的不足，提出反向推理以缩小初始状态与任务目标之间的感知差距。

Method: 设计了BAR代理，包含递归目标分解、状态一致性维持和阶段记忆模块，从终端状态开始进行规划。

Result: 实验证明BAR优于现有方法，且各模块有效。

Conclusion: 反向推理在复杂任务规划中表现优越，BAR代理展示了高效和一致性。

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [51] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
*Franziska Sofia Hafner,Ana Valdivia,Luc Rocher*

Key words: 语言模型, 性别偏见, 性别建构, 跨性别, 实证研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文指出语言模型编码并延续了有害的性别刻板印象，提出对性别偏见的定义需更广泛，并通过实证研究揭示了语言模型将性别与生物性别强关联的问题。

Motivation: 现有研究仅通过解耦性别相关术语与非性别术语缓解性别偏见，但忽视了性别建构本身带来的更深刻问题，如对跨性别和非二元性别身份的忽视。

Method: 从性别研究文献中提取性别建构见解，并对16种不同架构、训练数据和规模的语言模型进行实证测试。

Result: 语言模型倾向于将性别编码为与生物性别相关的二元类别，模糊或病理化不符合二元分类的性别术语。更大规模的模型强化了性别与生物性别的关联。

Conclusion: 需要重新评估和定义语言模型中的性别偏见，以更全面地解决性别多样性及相关危害。

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [52] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
*Yihua Zhu,Qianying Liu,Akiko Aizawa,Hidetoshi Shimodaira*

Key words: KBQA,LLM,KG-RAG,语义解析,PDRR

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PDRR是一个四阶段框架，通过预测、分解、检索和推理解决KBQA中的问题，显著优于现有方法。

Motivation: KBQA中LLM方法存在知识过时、幻觉和透明度问题，而KG-RAG方法仅适用于简单链式问题。

Method: 提出PDRR框架，包含预测问题类型、分解为三元组、检索KB信息及LLM推理。

Result: PDRR在不同LLM骨干上表现优异，适用于链式和非链式复杂问题。

Conclusion: PDRR通过结构化方法提升KBQA性能，扩展了适用范围。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.

</details>


### [53] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
*Ernests Lavrinovics,Russa Biswas,Katja Hose,Johannes Bjerva*

Key words: 大语言模型、知识图谱、幻觉减轻、多语言评估、事实核查

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个基于知识图谱的多语言、多跳评估基准MultiHal，用于生成文本的事实性评估，以减轻大语言模型的幻觉问题。

Motivation: 现有的事实性评估基准主要依赖英语数据集和补充上下文，忽略了结构化知识资源的利用。

Method: 从开放域知识图谱中挖掘并筛选高质量KG路径，构建MultiHal基准，并通过KG-RAG方法进行基线评估。

Result: KG-RAG在语义相似度评分上比传统QA方法提高了0.12到0.36分，展示了知识图谱整合的潜力。

Conclusion: MultiHal有助于推动基于图谱的幻觉减轻和事实核查任务的研究。

Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.

</details>


### [54] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
*Wei Fan,Tianshi Zheng,Yiran Hu,Zheye Deng,Weiqi Wang,Baixuan Xu,Chunyang Li,Haoran Li,Weixing Shen,Yangqiu Song*

Key words: 法律规则归纳（LRI）、大语言模型（LLMs）、计算法学、判例分析、基准数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了‘法律规则归纳（LRI）’任务，通过大语言模型从类似判例中提取隐含的法律规则，并构建了首个LRI基准数据集。实验显示，当前LLMs在此任务上仍存在泛化和虚构问题，但通过训练可显著提升性能。

Motivation: 当前计算法学研究多关注已有规则的适用，而从司法判决中归纳隐含法律规则的研究较少，受限于模型推理效率和符号推理能力。大语言模型的崛起为此提供了新机会，但缺乏正式任务定义、基准数据集和方法论。

Method: 提出‘法律规则归纳（LRI）’任务，旨在从类似判例中提取简洁、可推广的教义规则。构建了包含5,121个案例集（总计38,088个中文案例）的基准数据集，并包含216个专家标注的测试集。

Result: 实验发现：1）当前最先进的LLMs在LRI任务上存在过度泛化和虚构问题；2）在构建数据集上训练后，LLMs在捕捉类似案例中的细微规则模式上显著改进。

Conclusion: LRI任务填补了计算法学研究中的空白，通过大语言模型实现了隐含法律规则的自动化提取。构建的基准数据集为未来研究提供了重要基础。

Abstract: Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.

</details>


### [55] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
*Li Li,Peilin Cai,Ryan A. Rossi,Franck Dernoncourt,Branislav Kveton,Junda Wu,Tong Yu,Linxin Song,Tiankai Yang,Yuehan Qin,Nesreen K. Ahmed,Samyadeep Basu,Subhojyoti Mukherjee,Ruiyi Zhang,Zhengmian Hu,Bo Ni,Yuxiao Zhou,Zichao Wang,Yue Huang,Yu Wang,Xiangliang Zhang,Philip S. Yu,Xiyang Hu,Yue Zhao*

Key words: PersonaConvBench, 大型语言模型, 多轮对话, 个性化推理, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PersonaConvBench是一个大规模基准测试，用于评估多轮对话中大型语言模型（LLMs）的个性化推理与生成能力，结合了个性化和对话结构的综合分析。

Motivation: 现有工作通常单独研究个性化或对话结构，而PersonaConvBench将两者结合，以支持在多用户场景下系统分析个性化对话上下文对LLM输出的影响。

Method: 提供了三个核心任务（句子分类、影响回归和以用户为中心的文本生成），基于十个多样化Reddit领域，并在统一提示设置下对多个商业和开源LLMs进行基准测试。

Result: 引入个性化历史显著提升了性能，例如在情感分类任务中相对最佳非对话基线实现了198%的性能提升。

Conclusion: PersonaConvBench的发布旨在支持研究如何让LLMs适应个体风格、追踪长期上下文，并生成内容丰富且吸引人的回复。

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [56] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
*Yakun Zhu,Zhongzhen Huang,Linjie Mu,Yutong Huang,Wei Nie,Shaoting Zhang,Pengfei Liu,Xiaofan Zhang*

Key words: 大型语言模型, 临床诊断, 基准测试, 医疗AI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了DiagnosisArena，一个用于系统性评估大型语言模型在复杂临床诊断推理能力的基准测试。

Motivation: 当前大型语言模型在医疗诊断推理能力上的泛化性能不足，亟需一个更全面的基准测试来评估其专业水平。

Method: 通过筛选来自10种顶级医学期刊的1,113对患者案例和诊断，开发了DiagnosisArena，并经过AI系统和人类专家的多轮审查。

Result: 实验显示最先进的模型（o3-mini、o1、DeepSeek-R1）在诊断推理上的准确率分别为45.82%、31.09%、17.79%。

Conclusion: DiagnosisArena揭示了当前模型在临床诊断推理上的局限性，旨在推动AI诊断能力的进一步发展。

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [57] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
*Tianle Gu,Zongqi Wang,Kexin Huang,Yuanqi Yao,Xiangliang Zhang,Yujiu Yang,Xiuying Chen*

Key words: LLM watermarking, low-entropy, Invisible Entropy, efficiency, safety

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为Invisible Entropy (IE)的水印方法，通过轻量级特征提取器和熵标记器解决低熵场景下的问题，提升水印文本的自然性和检测鲁棒性。

Motivation: 现有基于logit的LLM水印方法在低熵场景中表现不佳，且依赖原始LLM导致高计算成本和潜在模型泄露风险。

Method: 引入轻量级特征提取器和熵标记器预测高低熵，并开发阈值导航器动态设置熵阈值。

Result: 在HumanEval和MBPP数据集上，IE减少了99%的参数大小，性能与传统方法相当。

Conclusion: IE为低熵水印提供了一种安全高效的范例。

Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [58] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
*Hongru Wang,Deng Cai,Wanjun Zhong,Shijue Huang,Jeff Z. Pan,Zeming Liu,Kam-Fai Wong*

Key words: 自推理语言模型（SRLM）、思维链（CoT）、推理任务、自训练、性能提升

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要介绍了一种自推理语言模型（SRLM），通过自训练合成更长的思维链数据，从而提升大型语言模型在复杂推理任务中的性能。

Motivation: 现有的长思维链数据难以创建和获取，因此需要一种方法让模型自身能够生成和优化这些数据，以提高推理能力。

Method: 提出SRLM模型，利用少量示范示例作为推理催化剂，通过自训练合成更长的思维链数据并迭代改进性能。

Result: SRLM在五个推理任务中平均提升了2.5分，尤其在64次采样时平均提升达7.89分，展示了模型的深度、多样性和创造性推理能力。

Conclusion: SRLM通过自训练大幅提升了推理任务的性能，并显示出稳定且一致的改进趋势。

Abstract: Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.

</details>


### [59] [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
*Filip Miletić,Aaron Schmid,Sabine Schulte im Walde*

Key words: 德语BERT, 名词复合词, 语义编码, 组合性, 预训练模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究了预训练的德语BERT模型在名词复合词语义知识编码上的表现，发现其效果不如英语BERT，可能与德语的复合词生产力更高有关。

Motivation: 探究预训练德语BERT模型是否能够有效编码名词复合词的语义知识，并与英语的类似研究进行比较。

Method: 通过结合目标词、层数及大小写模型的变化，预测868个标准复合词的组合性，分析BERT架构中的表征模式。

Result: 德语BERT在早期层中更容易提取组合性信息，但整体表现明显落后于英语BERT，可能与德语复合词生产力高、成分歧义多有关。

Conclusion: 德语名词复合语义编码的难度高于英语，BERT模型的表现差异可能源于语言本身的特性。

Abstract: This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.

</details>


### [60] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
*Wei Zhou,Mohsen Mesgar,Heike Adel,Annemarie Friedrich*

Key words: 表格问答, 多模态大语言模型, 文本表示, 图像表示, FRES

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文通过控制实验研究了表格问答（TQA）中表格表示（文本或图像）与模型（MLLMs和LLMs）组合的效果，提出动态选择表示方法FRES，性能平均提升10%。

Motivation: 之前的工作表明，将表格以图像形式传递给多模态大语言模型（MLLMs）的效果与文本输入大语言模型（LLMs）相当或更好，但缺乏控制实验无法区分这些方法。

Method: 通过控制实验研究表格表示（文本或图像）与模型组合的效果，并基于现有TQA数据集构建新基准。提出动态选择表示方法FRES。

Result: 实验发现最佳组合因问题复杂度和表格大小而异，FRES方法性能平均提升10%。

Conclusion: 动态选择表格表示方法能显著提升性能，需根据具体场景调整。

Abstract: In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.

</details>


### [61] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
*Chengzhi Zhang,Xinyi Yan,Lei Zhao,Yingyi Zhang*

Key words: 关键词提取, 章节结构, 学术文章, 结构特征, 性能提升

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于学术文章章节结构信息的关键词提取方法，通过结构特征和章节文本提升提取性能。

Motivation: 关键词提取（KPE）能帮助研究者高效检索文献，但现有方法受限于摘要长度或全文噪声。

Method: 结合章节结构信息和章节文本，探索七种结构特征影响，并通过关键词整合算法整合提取结果。

Result: 结构特征提升了KPE性能，关键词整合方法效果最佳；章节结构分类质量影响KPE性能。

Conclusion: 学术文章章节结构信息对关键词提取有效，研究成果代码和数据集已开源。

Abstract: The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [62] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
*Pittawat Taveekitworachai,Potsawee Manakul,Sarana Nutanong,Kunat Pipatanakul*

Key words: 强化微调；前提示工程；语言模型；行为内化；基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究通过强化微调（RFT）中前提示工程（pPE）的设计，探讨不同pPE策略对语言模型行为的影响，实验显示pPE训练的模型表现优于提示工程（iPE）模型，其中null-example方法表现最佳。

Motivation: 现有RFT研究多关注算法、奖励塑造和数据优化，而前提示设计对模型行为的影响尚未充分探索。

Method: 将五种iPE策略转化为pPE方法，并在Qwen2.5-7B模型上进行实验，评估在多个基准测试中的表现。

Result: 所有pPE训练的模型均优于iPE提示的模型，null-example方法在AIME2024和GPQA-Diamond上提升最大。

Conclusion: pPE是RFT中一个强大但未被充分研究的维度，不同策略能内化不同的行为风格。

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [63] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
*Sanjay Govindan,Maurice Pagnucco,Yang Song*

Key words: 大型语言模型、激活工程、时间对齐、事实性召回、提示策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过激活工程技术调整LLMs的时间对齐，显著提升了模型在特定时间点的事实召回能力，且无需训练或数据集创建。

Motivation: 大型语言模型（LLMs）训练数据跨越多个领域和时期，部分知识仅在特定时间范围内有效。确保LLMs生成时间上准确的回答对保持相关性和准确性至关重要。

Method: 采用激活工程技术，将LLaMA 2的三个版本对齐到特定时间点，并研究不同注入层和提示策略的效果。

Result: 实验显示，相对提示和显式提示分别提升44%和16%，与Zhao等人（2024）的微调方法性能相当，但计算效率更高且无需预对齐数据集。

Conclusion: 激活工程是一种高效的方法，可用于临时对齐LLMs以改进事实召回，而无需额外训练或数据集。

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [64] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
*Zahraa Al Sahili,Ioannis Patras,Matthew Purver*

Key words: 多语言CLIP, 社会偏见, 视觉-语言模型, 刻板印象, 跨语言权重共享

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究首次系统评估了三种多语言CLIP模型（M-CLIP、NLLB-CLIP和CAPIVARA-CLIP）在十种语言中的社会偏见，发现多语言模型未减轻偏见，反而加剧了性别偏见，尤其是低资源语言中。

Motivation: 研究旨在揭示多语言视觉-语言模型中的社会偏见，尤其是资源和语法性别差异对偏见表现的影响。

Method: 使用平衡的FairFace和PATA数据集，在零样本设置下量化种族和性别偏见，并评估刻板印象的放大。

Result: 多语言模型的性别偏见强于单语言基线，低资源语言偏见更严重。跨语言权重共享将英语刻板印象传播至性别中性语言。

Conclusion: 多语言模型需细粒度、语言感知的偏见评估，以避免掩盖语言特定的偏见热点。

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [65] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
*Zhenkai Qin,Jiajing He,Qiao Fang*

Key words: 细粒度情感分析, 提示学习, 多任务学习, TextCNN, 低资源

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于提示学习的统一框架PL-FGSA，用于细粒度情感分析，整合了多任务学习并在低资源条件下表现优异。

Motivation: 传统细粒度情感分析方法需要特定架构和大量标注数据，限制了其泛化性和可扩展性。

Method: PL-FGSA通过提示设计结合轻量级TextCNN主干，将问题重新表述为多任务提示增强生成。

Result: 在三个基准数据集上的实验表明，PL-FGSA优于传统微调方法，F1分数分别为0.922、0.694和0.597。

Conclusion: 证明了提示学习在情感分析任务中的有效性及其实际应用价值。

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [66] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Key words: Large Language Models, tokenization, character-level tasks, concept emergence

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型（LLM）在字符级任务上的失败原因，提出通过改进架构提升字符级推理能力。

Motivation: LLMs在字符级任务（如单词字母计数）上表现不佳，原因是tokenization导致的低互信息问题。

Method: 通过19个合成任务验证字符级推理能力，并提出轻量级架构改进。

Result: 字符级能力在训练后期才突然出现，且改进架构显著提升了模型表现。

Conclusion: 研究为理解和改进LLM的结构性盲点提供了理论框架。

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [67] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
*Yunlong Liang,Fandong Meng,Jie Zhou*

Key words: 稀疏混合专家、神经机器翻译、分层路由、上下文感知、多领域翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 稀疏混合专家（MoE）在神经机器翻译（NMT）中取得了显著进展，但现有方法存在任务知识直接使用和局部专家选择的问题。THOR-MoE通过分层任务导向和上下文响应路由策略解决了这些问题，在多领域和多语言翻译任务中表现优异。

Motivation: 当前稀疏MoE方法直接使用NMT的任务知识（如领域/语言特定知识），但这些知识在实际应用中通常不可用，且未考虑自然分组的领域/语言特性。此外，专家选择仅依赖局部标记表示，忽视了上下文信息。

Method: 提出THOR-MoE，采用分层任务导向和上下文响应路由策略。首先预测领域/语言标签并提取混合表示，以分层方式分配任务级专家；然后注入上下文信息，增强从预选任务级专家集中的标记路由。

Result: THOR-MoE在多领域和多语言翻译基准测试中表现优异，兼容Top-k和Top-p路由方案。例如，上下文感知方式在Top-p路由基础上平均提高0.75 BLEU，且激活参数少于22%。

Conclusion: THOR-MoE通过分层和上下文感知路由策略显著提升了稀疏MoE的性能，具备广泛适用性。

Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.

</details>


### [68] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
*Yusuf Denizay Dönder,Derek Hommel,Andrea W Wen-Yi,David Mimno,Unso Eun Seo Jo*

Key words: LLM, 文本到SQL, 成本效益, N-rep, 模式表示

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了一种名为N-rep的文本到SQL方法，在成本效益上显著优于现有技术，每次查询仅需$0.039，同时保持高性能。

Motivation: 现有的大型语言模型（LLM）方法如Chain-of-Thought、自洽性和微调在代码生成任务中成本高昂，每次查询可能花费数百美元，促使开发更经济的解决方案。

Method: N-rep通过利用同一模式输入的多种表示来弥补单一表示的不足，从而在不依赖昂贵推理或微调的情况下，使用小型模型实现鲁棒性。

Result: N-rep在BIRD基准测试中表现与其他高成本方法相似，每次查询成本仅为$0.039。

Conclusion: N-rep是目前在成本范围内性能最佳的文本到SQL方法。

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [69] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
*Xiang Zhang,Juntai Cao,Jiaqi Wei,Yiwei Xu,Chenyu You*

Key words: 分词, 符号推理, 语言模型, BPE, Token Awareness

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了分词（Tokenization）对语言模型推理能力的限制，特别是子词分词方法（如BPE）如何模糊原子推理单元，从而阻碍符号计算。通过理论和实证研究，提出了“Token Awareness”概念，并证明原子对齐的分词格式可以显著提升推理性能。

Motivation: 研究动机在于揭示分词作为语言模型计算的第一层，其结构如何影响模型的符号推理能力，特别是Chain-of-Thought（CoT）提示下的表现。

Method: 方法包括理论分析和系统性评估，对比不同分词方案（如BPE和原子对齐格式）在算术和符号任务上的推理表现，并使用小模型（如GPT-4o-mini）与大模型（如o1）进行比较。

Result: 结果显示，分词结构对推理性能有显著影响，原子对齐的分词格式能够解锁强泛化能力，使小模型在结构化推理中超越大模型。

Conclusion: 结论指出，语言模型的符号推理能力不仅取决于架构，还受分词表示方式的深刻制约。原子对齐的分词是实现高效符号计算的关键。

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [70] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
*Tong Bao,Heng Zhang,Chengzhi Zhang*

Key words: abstractive summarization, scientific papers, structural function recognition, Longformer

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一种两阶段抽象摘要框架，通过自动识别科学论文的结构功能，解决了现有方法未能充分利用结构化信息及跨学科适应性问题，并在实验中获得优于基线模型的效果。

Motivation: 现有摘要生成方法未能充分利用科学论文的结构化信息且缺乏跨学科的鲁棒性。

Method: 两阶段框架：首先标准化章节标题并训练分类器识别关键结构；其次使用Longformer模型生成上下文感知摘要。

Result: 在领域特定数据集上优于先进基线模型，生成更全面的摘要。

Conclusion: 提出的方法能有效利用结构化信息并提升摘要质量。

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [71] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
*Yunlong Liang,Fandong Meng,Jiaan Wang,Jie Zhou*

Key words: 俚语翻译, 跨语言解释, 深度思考模型, SlangDIT, SlangOWL

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种称为SlangDIT的俚语翻译任务，包含俚语检测、跨语言俚语解释和翻译三个子任务，并构建了一个数据集；提出了一种深度思考模型SlangOWL，显著提升了翻译效果。

Motivation: 俚语翻译的挑战在于其语义依赖于上下文，现有研究将俚语检测、解释和翻译视为独立任务，忽略了它们之间的内在联系。

Method: 构建了SlangDIT数据集，提出SlangOWL模型，通过深度思考过程（检测、多义性判断、解释和翻译）提升俚语翻译效果。

Result: 实验表明，SlangOWL在LLMs上显著优于非深度思考的基准模型和微调模型。

Conclusion: SlangDIT任务和SlangOWL模型为俚语翻译提供了一个有效框架，验证了深度思考方法的优势。

Abstract: The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.

</details>


### [72] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
*Guosheng Liang,Longguang Zhong,Ziyi Yang,Xiaojun Quan*

Key words: 大推理模型,链式思维推理,动态切换,计算效率,ThinkSwitcher

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ThinkSwitcher框架通过动态切换长短链思维推理模式，提高大推理模型的效率，降低20-30%计算成本。

Motivation: 大推理模型在复杂任务中表现优异，但在简单任务中容易因过度推理导致计算资源浪费。

Method: 提出ThinkSwitcher框架，通过轻量级切换模块根据任务复杂度动态选择长短链思维推理模式。

Result: 在多任务推理基准测试中，ThinkSwitcher降低20-30%计算成本，同时保持复杂任务的高准确率。

Conclusion: ThinkSwitcher是一种高效且可扩展的统一大推理模型部署方案。

Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.

</details>


### [73] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
*Tuc Nguyen,Yifan Hu,Thai Le*

Key words: large language models, authorship privacy, obfuscation, mimicking, verification

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出首个统一框架，分析LLM在作者身份隐私中不同任务（AO、AM、AV）的动态关系及其相互作用。

Motivation: 针对LLMs可能泄露用户信息的隐私问题，研究AO、AM和AV任务之间的相互作用，填补现有研究空白。

Method: 提出统一框架，量化AO、AM和AV在LLM背景下的互动关系，并探讨了人口统计数据的调节作用。

Result: 通过实验验证了不同任务之间的动态关系及隐私风险，并公开了源代码。

Conclusion: 统一框架有助于理解LLM在作者身份隐私中的复杂作用，为未来研究提供基础。

Abstract: Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.

</details>


### [74] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
*Sizhe Yuen,Ting Su,Ziyang Wang,Yali Du,Adam J. Sobey*

Key words: 问答系统，大型语言模型，检索增强生成，自动化生成，微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种通过自动生成基于上下文的问答对来增强大型语言模型在知识密集型问答任务中的方法，减少了人工标注的依赖，提升了模型的推理和事实准确性。

Motivation: 当前的问答系统在复杂推理和实时知识整合方面表现不足，尤其是检索增强生成（RAG）方法难以处理多源信息的逻辑连接。

Method: 利用大型语言模型自动生成问答对作为微调数据，结合自动化问答生成器和模型微调器，通过困惑度、ROUGE、BLEU和BERTScore进行评估。

Result: 实验结果显示，生成的问答对在逻辑连贯性和事实准确性上优于人工标注数据，Mistral-7b-v0.3的表现优于Llama-3-8b。

Conclusion: 该方法为开发适应性强的AI系统提供了新的方向，展示了自动化生成训练数据的潜力。

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [75] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
*Darpan Aswal,Siddharth D Jaiswal*

Key words: 大语言模型；红色团队攻击；代码混合；语音扰动；多模态

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种利用代码混合和语音扰动的新型策略，成功绕过了大语言模型（LLMs）的安全过滤器，并在文本和图像生成任务中取得了高攻击成功率。

Motivation: 现有的针对LLMs的红色团队攻击主要集中在英语上，缺乏多语言和多模态环境下的有效性研究，导致模型在这些领域仍存在漏洞。

Method: 通过代码混合和语音扰动技术，结合新的越狱策略，设计了一种能够绕过安全过滤器的攻击方法。

Result: 新型提示在文本生成中达到了99%的攻击成功率（ASR），图像生成中为78%，同时保持了高相关性（文本100%，图像95%）。

Conclusion: 研究表明，语音扰动通过影响单词标记化提高了攻击成功率，呼吁加强对多语言多模态模型的安全对齐研究。

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [76] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
*Hakaze Cho,Peng Luo,Mariko Kato,Rin Kaenbyou,Naoya Inoue*

Key words: In-context Learning, Language Models, Attention Behavior Fine-Tuning, Mechanistic Interpretability

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为ABFT的新方法，通过优化注意力分数而非最终输出来提升语言模型的性能，实验证明其在多个方面优于传统方法。

Motivation: 为了解决传统端到端微调方法在In-context Learning（ICL）上计算成本高的问题。

Method: 提出Attention Behavior Fine-Tuning（ABFT），通过调整注意力分数来优化模型行为。

Result: ABFT在性能、鲁棒性、无偏性和效率上均优于传统方法，且数据成本仅为前者的0.01%。

Conclusion: 研究表明通过控制语言模型内部模块可以优化其行为，为机制解释性开辟了未来应用方向。

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [77] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
*Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Praneeth Vepakomma*

Key words: Large Language Models, Parameter-Efficient Fine-Tuning, LoRA, ABBA, Hadamard product

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ABBA是一种新PEFT架构，通过解耦可学习低秩矩阵与预训练权重，显著提升表达力，并在多个任务中超越现有方法。

Motivation: 大语言模型虽表现优异，但高效适应新领域仍是挑战。PEFT方法通过轻量化模块实现高效微调，但现有方法如LoRA和HiRA的表达力受限。

Method: ABBA将更新参数化为两个可独立学习低秩矩阵的Hadamard积，完全解耦预训练权重，实现更高表达力。

Result: ABBA在算术和常识推理基准测试中表现最优，显著超越其他PEFT方法，并通过矩阵重构实验验证其优势。

Conclusion: ABBA通过解耦设计提供了更高表达力，是高效微调领域的重大进展。

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [78] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
*Ziang Wang,Amir Aryani*

Key words: 自然语言处理, 主题建模, 儿童言语障碍, LDA, BERTopic

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该技术报告介绍了一种基于自然语言处理的方法，用于系统分类关于儿童言语障碍的科学文献。通过LDA和BERTopic模型识别了14个临床相关主题，并验证了其效果。

Motivation: 自动化文献分类可为言语病理学领域的研究提供高效支持。

Method: 从PubMed数据库中检索并过滤了4,804篇相关文献，应用LDA和BERTopic模型进行主题建模，并使用了自定义停用词表。

Result: LDA模型的连贯性得分为0.42，困惑度为-7.5；BERTopic模型异常主题比例低于20%，展示了有效分类能力。

Conclusion: 该方法为言语病理学领域的自动化文献综述提供了实用工具。

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [79] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
*Haijun Li,Tianqi Shi,Zifu Shang,Yuxuan Han,Xueyu Zhao,Hao Wang,Yu Qian,Zhiqiang Qian,Linlong Xu,Minghao Wu,Chenyang Lyu,Longyue Wang,Gongbo Tang,Weihua Luo,Zhao Xu,Kaifu Zhang*

Key words: 机器翻译，工业应用，评估框架，电子商务，领域特定

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个针对工业机器翻译的三级能力评估框架，并推出专用于电子商务领域的TransBench基准测试，填补了学术基准与工业需求之间的评估鸿沟。

Motivation: 通用机器翻译模型在工业场景中因缺乏领域特定术语和文化适应性而表现不佳，现有评估框架未能有效衡量其在专业领域的性能。

Method: 提出了包括基础语言能力、领域专业能力和文化适应能力的三级翻译能力框架，并开发了TransBench基准测试，包含17,000个电子商务领域的句子和33种语言对。

Result: TransBench整合了传统指标和领域特定的Marco-MOS模型，为工业机器翻译提供了全面的评估工具。

Conclusion: 该工作通过系统性框架和公开基准测试，使研究人员和从业者能够更准确地评估和改进工业机器翻译系统。

Abstract: Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.

</details>


### [80] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
*Shaolin Zhu,Tianyu Dong,Bo Li,Deyi Xiong*

Key words: FuxiMT, 机器翻译, 稀疏化LLM, MoEs, 课程学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FuxiMT是一种基于稀疏化大语言模型（LLM）的中文中心多语言机器翻译模型，通过两阶段训练策略实现高性能。

Motivation: 解决多语言机器翻译中低资源场景下的性能问题，并探索零样本翻译潜力。

Method: 采用两阶段训练策略（中文预训练+多语言微调），结合MoEs和课程学习。

Result: 在低资源场景下显著优于基线模型，具备出色的零样本翻译能力。

Conclusion: FuxiMT在多语言翻译中表现卓越，尤其在资源稀缺或无平行数据时展现出潜力。

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [81] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Key words: SLOT, 语言模型优化, 测试时推理, 参数高效, 复杂指令

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SLOT提出了一种新的、参数高效的测试时推理方法，通过少量优化步骤更新样本特定的参数向量，提升语言模型对复杂指令的响应能力。

Motivation: 现有的大型语言模型在处理复杂指令时表现不佳，尤其是那些在通用样本中代表性不足的指令。

Method: SLOT在测试时对输入提示进行少量优化步骤，更新一个轻量级的样本特定参数向量，并将其添加到最终隐藏层前。通过最小化交叉熵损失，使模型更好地与给定指令对齐。

Result: 实验表明，SLOT在多个基准测试和语言模型中表现优于对比模型，例如Qwen2.5-7B在GSM8K上准确率提升8.6%，DeepSeek-R1-Distill-Llama-70B在GPQA上达到68.69%的SOTA准确率。

Conclusion: SLOT能够有效提升语言模型对复杂指令的处理能力，是一种高效且可扩展的方法。

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [82] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
*Hui Huang,Yancheng He,Hongli Zhou,Rui Zhang,Wei Liu,Weixun Wang,Wenbo Su,Bo Zheng,Jiaheng Liu*

Key words: LLM-as-a-Judge, 强化学习, 判断优化, 生成式LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Think-J方法，通过学习和优化判断思维，提升生成式LLM在LLM-as-a-Judge任务中的性能，无需额外人工标注。

Motivation: 尽管生成式LLM在多任务中表现优异，但其作为LLM-Judge的性能仍有不足，研究旨在提升其判断能力。

Method: 首先利用少量精选数据训练模型初步判断能力，随后基于强化学习优化判断思维痕迹，提出离线和在线两种RL优化方法。

Result: 实验表明，该方法显著提升生成式LLM-Judge的评估能力，超越生成式和分类器基础的LLM-Judge。

Conclusion: Think-J方法有效提升LLM-as-a-Judge性能，验证了其潜力。

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [83] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
*Minh Ngoc Ta,Dong Cao Van,Duc-Anh Hoang,Minh Le-Anh,Truong Nguyen,My Anh Tran Nguyen,Yuxia Wang,Preslav Nakov,Sang Dinh*

Key words: AI生成文本检测、多级对比学习、多任务分类、FAID

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FAID是一种细粒度检测框架，用于区分人写、AI生成以及人-AI协作的文本，并识别AI模型家族。通过多级对比学习和多任务辅助分类，FAID在未见过领域和新AI模型上表现优异。

Motivation: 随着人-AI协作在生成任务中的增加，区分文本来源的需求日益迫切。现有的二元分类器无法满足细粒度检测需求。

Method: FAID结合多级对比学习和多任务辅助分类，捕捉细微的风格特征，并将AI模型家族建模为独特风格实体。

Result: FAID在实验中优于基线方法，特别是在未见领域和新AI模型上的泛化能力显著提升。

Conclusion: FAID为解决AI辅助写作中的透明性和问责问题提供了潜在解决方案。

Abstract: The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.

</details>


### [84] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Key words: 仇恨言论检测, 跨语言迁移学习, 最近邻检索, 数据增强

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种高效的跨语言迁移学习方法，通过最近邻检索增强目标语言的标记数据，提升低资源语言中仇恨言论检测的性能。

Motivation: 由于仇恨言论检测数据的标记成本高且耗时，尤其是在低资源语言中，因此提出了一种高效且可扩展的方法。

Method: 利用最近邻检索从大型多语言仇恨言论检测池中检索相关示例，结合目标语言的少量标记数据，优化检测性能。

Result: 在八种语言上的评估显示，该方法优于仅基于目标语言数据的模型，且在多数情况下超越现有最优方法。

Conclusion: 该方法高效且可扩展，可轻松适应新语言和任务，并通过最大边缘相关性优化检索结果。

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [85] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
*Jennifer D'Souza,Hamed Babaei Giglou,Quentin Münch*

Key words: LLMs, 科学问答, 强化学习, 开放框架, 对抗性数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: YESciEval是一个开源框架，通过细粒度评分和强化学习减少LLM评估中的乐观偏见，支持多学科科学问答数据集和无成本的评估。

Motivation: 评估大型语言模型（LLMs）在科学问答中的鲁棒性未被充分探索，需要透明且可靠的评估方法。

Method: 结合细粒度评分标准和强化学习，发布多学科科学问答数据集与对抗性变体，利用多种LLM评分。

Result: 实现了独立于专有模型和人类反馈的可扩展、零成本评估。

Conclusion: 通过提升LLM作为评估者的可靠性，该工作推动了AI对齐和科学研究的透明评估。

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [86] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
*Rao Ma,Mengjie Qian,Vyas Raina,Mark Gales,Kate Knill*

Key words: 语音大语言模型, 对抗攻击, 通用声学攻击, 鲁棒性, Qwen2-Audio, Granite-Speech

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了语音大语言模型（LLMs）对通用声学对抗攻击的脆弱性，揭示了其容易被特定音频段攻击的严重问题。

Motivation: 探讨语音LLMs在对抗攻击下的脆弱性，填补研究空白并提出改进方向。

Method: 通过预置固定的通用对抗音频段，研究模型对攻击的反应，包括无输出和任务覆盖，并扩展为选择性攻击。

Result: Qwen2-Audio和Granite-Speech模型在特定条件下易受攻击，揭示了语音LLMs的通用漏洞。

Conclusion: 语音LLMs需要更鲁棒的训练策略和对抗攻击防御方法。

Abstract: The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.

</details>


### [87] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
*Jungseob Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Key words: 跨语言优化, 监督微调, 语言模型迁移, 低资源语言, 多语言性能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为跨语言优化（CLO）的方法，用于高效地将英文为中心的大型语言模型迁移到目标语言，同时保持其英文能力。CLO通过利用公开的英文SFT数据和翻译模型实现跨语言迁移，并在多语言实验中表现优于传统SFT方法。

Motivation: 传统的监督微调（SFT）方法在将大型语言模型适配到其他语言时，往往过度依赖英文性能，尤其在数据受限的环境中表现不佳。本文旨在克服这一问题，提出一种更高效的方法。

Method: 提出跨语言优化（CLO），利用公开的英文SFT数据和翻译模型进行跨语言迁移。实验使用了五种模型和六种不同资源水平的语言。

Result: CLO在多语言实验中表现优于SFT，尤其在低资源语言中仅需3200样本即可超越SFT的6400样本效果。此外，CLO对数据量的敏感度较低，表现出更强的鲁棒性。

Conclusion: CLO通过高效利用数据，显著提升了模型在多语言环境中的性能，同时克服了SFT在数据敏感性和资源依赖上的局限性。

Abstract: Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.

</details>


### [88] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
*Jinwang Song,Hongying Zan,Kunli Zhang,Lingling Mu,Yingjie Han,Haobo Hua,Min Peng*

Key words: Text-to-SQL，模式链接，监督微调，鲁棒性，JOLT-SQL

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: JOLT-SQL是一种简化的单阶段监督微调框架，通过联合优化模式链接和SQL生成，解决了传统方法的复杂性和对噪声模式信息的鲁棒性问题，并在Spider和BIRD基准测试中取得了最佳性能。

Motivation: 传统监督微调方法在多阶段管道和对噪声模式信息的鲁棒性方面存在问题，JOLT-SQL旨在解决这些挑战。

Method: JOLT-SQL采用判别性模式链接和局部双向注意力增强，结合混淆感知噪声模式采样策略和选择性注意力，提升噪声模式下的鲁棒性。

Result: 在Spider和BIRD基准测试中，JOLT-SQL在可比较大小的开源模型中实现了最佳执行准确性，并显著提升了训练和推理效率。

Conclusion: JOLT-SQL通过简化的单阶段框架和创新的模式链接策略，有效提升了任务性能和效率。

Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.

</details>


### [89] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
*Ehsan Doostmohammadi,Marco Kuhlmann*

Key words: 检索增强语言模型,查询-内容匹配度,合成内容,训练效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 检索增强的语言模型在减少计算资源的同时性能接近更大模型，但其效果依赖于查询与检索内容的匹配度。本文系统研究了不同匹配度对训练和推断性能的影响，发现超过关键阈值后性能显著提升。

Motivation: 探索查询-内容匹配度对检索增强语言模型的影响，以优化其性能。

Method: 通过实验研究不同匹配度的影响，并使用合成内容（如查询改写）增加匹配度。

Result: 增加匹配度超过阈值后显著降低困惑度并加速学习；合成内容可节省40%训练时间且不影响性能。

Conclusion: 优化检索机制可显著提升语言模型预训练效率与性能。

Abstract: Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [90] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
*Shamsuddeen Hassan Muhammad,Ibrahim Said Ahmad,Idris Abdulmumin,Falalu Ibrahim Lawan,Babangida Sani,Sukairaj Hafiz Imam,Yusuf Aliyu,Sani Abdullahi Sani,Ali Usman Umar,Kenneth Church,Vukosi Marivate*

Key words: 豪萨语, NLP, 低资源语言, HausaNLP, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文综述了豪萨语NLP的现状，提出了HausaNLP资源目录，并讨论了豪萨语在大语言模型中的挑战及未来研究方向。

Motivation: 豪萨语作为一种低资源语言，尽管拥有大量母语和非母语使用者，但其NLP研究仍面临资源匮乏和模型不足的问题。

Method: 通过系统性梳理现有NLP任务的资源和研究，创建HausaNLP目录，并分析豪萨语在大语言模型中的整合挑战。

Result: 推出了HausaNLP目录，总结了豪萨语NLP的现状和挑战，提出了未来研究方向。

Conclusion: 该研究为豪萨语NLP的发展奠定了基础，并为多语言NLP研究提供了参考。

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [91] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
*Leonardo Bertolazzi,Manuel Vargas Guzmán,Raffaella Bernardi,Maciej Malicki,Jakub Szymanik*

Key words: 大语言模型、元学习、泛化能力、演绎推理、知识库、MIND

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为MIND的元学习微调方法，旨在提升小型语言模型在推理任务中的泛化能力，使其在未见过的知识库上表现更优。

Motivation: 大型语言模型在形式化任务中表现强劲，但在处理分布外问题时泛化能力有限。因此，研究如何通过系统理解演绎规则来提升泛化能力成为关键。

Method: 提出Meta-learning for In-context Deduction (MIND)，一种基于少样本元学习的微调方法，专注于识别知识库中用于推导给定假设的前提子集。

Result: MIND显著提升了1.5B到7B参数的小型模型的泛化能力，尤其在数据稀缺和小型模型中表现突出，甚至优于GPT-4o和o3-mini等大型模型。

Conclusion: MIND为小型语言模型在复杂推理任务中的泛化能力提供了一种有效的解决方案，展示了其在低资源环境下的潜力。

Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [92] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
*Neelabh Sinha*

Key words: 语言模型, 摘要生成, QA-prompting, 位置偏见, ROUGE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: QA-prompting是一种新的提示方法，通过问题回答作为中间步骤来提升长文本摘要生成的效果，优于现有方法。

Motivation: 解决语言模型在长文本摘要中因位置偏见导致的信息提取不佳问题。

Method: 提出QA-prompting方法，利用问题回答作为中间步骤，无需微调或多步处理。

Result: 在多个数据集和模型上实验，ROUGE得分提升高达29%。

Conclusion: QA-prompting是一种高效且可扩展的摘要解决方案，强调了领域特定问题选择的重要性。

Abstract: Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.

</details>


### [93] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
*Jialong Han,Si Zhang,Ke Zhang*

Key words: LLMs, PEFT, LoRA, SVD, OSoRA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: OSoRA是一种新的参数高效微调方法，通过结合SVD和可学习缩放向量，减少训练参数，降低计算资源需求，性能优于其他方法。

Motivation: 由于大规模语言模型微调的计算成本高昂，需要更高效的微调方法。

Method: OSoRA扩展了LoRA，结合SVD和可学习缩放向量，冻结奇异向量矩阵，优化输出维度向量。

Result: OSoRA在多个基准测试中表现优于LoRA和VeRA，参数线性扩展。

Conclusion: 联合训练奇异值和输出维度向量对性能至关重要，OSoRA是高效的微调方法。

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [94] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
*Xin Li,Mengbing Liu,Li Wei,Jiancheng An,Mérouane Debbah,Chau Yuen*

Key words: 大型语言模型,无线通信,数学建模,基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了WirelessMathBench，这是一个专门评估大型语言模型在无线通信数学建模能力的新基准，揭示了当前模型在复杂数学推理中的局限性。

Motivation: 尽管大型语言模型在多个领域表现出色，但在无线通信等领域的复杂数学推理能力尚未充分探索。

Method: 通过从40篇前沿研究论文中精心挑选587个问题，构建了一个多样化的评估基准，包括从基础选择题到复杂方程完成任务。

Result: 实验表明，领先的模型在基础任务中表现良好，但在重建部分或完全遮挡的方程时性能显著下降，最佳模型的平均准确率仅为38.05%。

Conclusion: 通过公开基准和评估工具包，旨在推动更强大、领域感知的语言模型发展。

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [95] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
*Jialong Han,Si Zhang,Ke Zhang*

Key words: PEFT, LoRA, DuDe, SVD, 优化稳定性, 知识转移

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出DuDe方法，通过分解权重矩阵为大小和方向分量并采用SVD初始化，解决LoRA训练不稳定和知识转移效率低的问题，取得了优异的性能表现。

Motivation: 现有LoRA方法因随机初始化适配器参数，导致训练不稳定和预训练模型知识转移效率低，需改进。

Method: 提出DuDe方法，将权重矩阵分解为大小和方向分量，利用SVD进行初始化，提升训练稳定性和知识保留。

Result: DuDe在MMLU和GSM8K任务中分别达到48.35%和62.53%的准确率，优于现有方法。

Conclusion: DuDe通过分解策略优化了训练稳定性和知识保留，为LLM的高效微调提供了新思路。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [96] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
*Maitreya Prafulla Chitale,Ketaki Mangesh Shetye,Harshit Gupta,Manav Chaudhary,Vasudeva Varma*

Key words: AutoRev, 图结构, 自动评审, NLP

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AutoRev 是一个基于图结构的自动学术论文评审系统，通过提取关键段落提升评审生成效果，性能优于现有基准58.72%。

Motivation: 解决传统方法因长输入令牌限制导致的性能和计算问题。

Method: 将学术文档表示为图结构，提取关键段落用于评审生成。

Result: 在评审生成任务中平均优于现有基准58.72%。

Conclusion: 图结构提取技术可拓展至其他NLP下游任务，代码计划公开。

Abstract: Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.

</details>


### [97] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
*Nadir Durrani,Basel Mousi,Fahim Dalvi*

Key words: 多语言知识编辑、模型编辑、语言各向异性、跨语言传播

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文综述了多语言知识编辑（MKE）的研究现状，总结了方法、挑战及未来方向。

Motivation: 虽然知识编辑在单语环境中已被广泛研究，但在多语言背景下仍缺乏系统性探索，因此需要整合现有研究并推动未来发展。

Method: 论文提出了MKE的综合分类法，包括参数化、记忆化、微调和超网络方法，并分析了现有的基准测试和方法效果。

Result: 总结了跨语言传播的挑战，并指出了语言各向异性、评估覆盖范围和编辑可扩展性等开放性问题。

Conclusion: 该论文为可编辑的多语言大语言模型的未来研究奠定了基础。

Abstract: While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [98] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
*Seyoung Song,Seogyeong Jeong,Eunsu Kim,Jiho Jin,Dongkwan Kim,Jay Shin,Alice Oh*

Key words: 大语言模型,多语言评估,低资源语言,MUG-Eval,会话任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MUG-Eval是一个评估大语言模型多语言生成能力的新框架，通过将现有基准转化为会话任务并测量准确率，无需依赖语言特定的NLP工具或LLMs作为评判者。

Motivation: 评估大语言模型在低资源语言中的文本生成能力存在挑战，缺乏直接评估方法。

Method: 将现有基准转化为会话任务，以任务成功率为指标，评估模型的多语言生成能力。

Result: 在30种语言上评估8个大语言模型，MUG-Eval与现有基准强相关（r>0.75），支持跨语言和模型的标准化比较。

Conclusion: MUG-Eval提供了一种资源高效且稳健的多语言生成评估方法，可扩展到数千种语言。

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [99] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
*Peter Baile Chen,Yi Zhang,Dan Roth,Samuel Madden,Jacob Andreas,Michael Cafarella*

Key words: 大型语言模型,推理增强,日志复用,KV缓存,代理系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为LAG的新框架，通过直接复用过去的计算和推理日志，提升大型语言模型在未见任务中的表现，同时保持系统高效。

Motivation: 大型语言模型及其代理系统难以保留和复用以往任务中的推理能力，限制了其在未来任务中的应用。

Method: 设计了基于键值（KV）缓存的日志增强生成（LAG）框架，复用过去的推理上下文，仅存储关键令牌的KV缓存，在遇到新任务时检索相关日志以增强生成。

Result: 实验表明，LAG在知识和推理密集型数据集上显著优于标准代理系统和基于反射或KV缓存技术的现有解决方案。

Conclusion: LAG通过直接复用以往推理和计算，提升了模型在新任务中的表现，且具有高效性和可扩展性。

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [100] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
*Haoming Huang,Yibo Yan,Jiahao Huo,Xin Zou,Xinfeng Li,Kun Wang,Xuming Hu*

Key words: 大型语言模型, 知识遮蔽, PhantomCircuit, 注意力机制, 训练过程

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PhantomCircuit框架通过知识路径分析深入研究了LLMs中的知识遮蔽问题，揭示了其训练过程中的内部机制。

Motivation: 大型语言模型（LLMs）存在知识遮蔽问题，即一个知识点掩盖另一个相关知识点，导致错误输出，但现有研究对其起源和内部机制缺乏深入了解。

Method: 提出PhantomCircuit框架，运用知识路径分析，剖析注意力头的内部机制，追踪知识遮蔽现象的演变。

Result: 实验显示PhantomCircuit能有效识别知识遮蔽实例，为此类幻觉问题提供了新的研究视角。

Conclusion: PhantomCircuit为知识遮蔽现象提供了深入分析工具，有助于未来缓解此类问题的方法开发。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [101] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
*Pengzhou Cheng,Haowen Hu,Zheng Wu,Zongru Wu,Tianjie Ju,Daizong Ding,Zhuosheng Zhang,Gongshen Liu*

Key words: GUI代理，多模态大语言模型，后门攻击，AgentGhost，Min-Max优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文揭示了MLLM-powered GUI代理在交互层面暴露的多种触发器，并提出了AgentGhost框架，通过复合触发器和Min-Max优化实现高效且隐蔽的后门攻击。

Motivation: 由于开源GUI代理或API的使用存在后门攻击的供应链威胁，而目前相关研究不足，因此需要探索和防御此类攻击。

Method: AgentGhost框架结合目标和交互级触发器，通过Min-Max优化和对比学习改进后门的灵活性与隐蔽性。

Result: 实验证明AgentGhost在多个代理模型中攻击准确率高达99.7%，且仅导致1%的效用下降。

Conclusion: 该研究为GUI代理的后门攻击提供了新的视角，并提出了有效的防御方法，将攻击准确率降至22.1%。

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [102] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
*Huopu Zhang,Yanguang Liu,Mengnan Du*

Key words: 财报电话会议,盈利惊喜预测,稀疏自动编码器,SAE-FiRE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了SAE-FiRE框架，利用稀疏自动编码器提取财务信号，显著优于基线方法。

Motivation: 通过分析财报电话会议记录预测盈利惊喜，但现有记录冗长冗余，行业术语多，语言模型难处理。

Method: 使用稀疏自动编码器(SAEs)提取关键信息，过滤噪音，捕捉预测盈利惊喜的财务信号。

Result: 实验表明，该方法显著优于基线方法。

Conclusion: SAE-FiRE框架能有效解决财报记录分析的挑战，提升预测能力。

Abstract: Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.

</details>


### [103] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
*Ona de Gibert,Joseph Attieh,Teemu Vahtola,Mikko Aulamo,Zihao Li,Raúl Vázquez,Tiancheng Hu,Jörg Tiedemann*

Key words: LLM, synthetic data, low-resource MT, Europarl, SynOPUS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了LLM生成的合成数据对低资源机器翻译的潜力，通过构建和扩展合成语料库，验证其高质量，并提出一种公共存储库SynOPUS。

Motivation: 解决低资源机器翻译的性能问题，探索LLM生成合成数据的实用性与扩展潜力。

Method: 基于Europarl构建文档级合成语料库，扩展至147种语言对；自动与人工评估质量；研究训练机制及与其他数据集对比。

Result: 合成数据显著提升低资源语言MT性能，SynOPUS成为公共资源。

Conclusion: LLM生成的合成数据对低资源MT有实际价值，即使含噪声仍能有效提升性能。

Abstract: We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.

</details>


### [104] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Key words: 大型语言模型, 指令泛化, 空间基础任务, 合成指令, 人类编写指令

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了指令调整的大型语言模型在空间基础任务中的泛化挑战，发现其在简单任务上表现良好，但在复杂任务上性能显著下降。

Motivation: 探讨大型语言模型在从合成指令到人类编写指令的泛化能力，尤其是在空间基础任务中的表现。

Method: 使用合成指令对LLMs进行微调，并在包含合成和人类编写指令的基准数据集上评估其性能。

Result: 模型在简单任务上泛化良好，但在复杂任务上性能显著下降。

Conclusion: LLMs在复杂空间基础任务中的指令泛化能力仍有待提升。

Abstract: Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.

</details>


### [105] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Key words: 大型语言模型、参数知识迁移、神经不兼容性、跨规模对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文探讨了通过参数实现跨规模大型语言模型（LLMs）的知识迁移（PKT），并提出了PrePKT和PostPKT两种范式，以及名为LaTen的解决方案，揭示了神经不兼容性（Neural Incompatibility）是主要挑战。

Motivation: 传统基于符号语言的知识迁移方法限制了效率，研究者希望探索通过参数直接迁移知识的新途径，尤其是针对不同规模的LLMs。

Method: 提出了PrePKT（预对齐PKT）范式和LaTen方法，利用少量训练步骤对齐参数空间，避免了后续微调。

Result: 实验表明，PrePKT和PostPKT在跨规模PKT中均面临稳定性问题，神经不兼容性是核心挑战。

Conclusion: 研究揭示了LLMs参数结构的复杂性，为未来高效PKT研究提供了新方向。

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [106] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
*Mete Ismayilzada,Antonio Laverghetta Jr.,Simone A. Luchini,Reet Patel,Antoine Bosselut,Lonneke van der Plas,Roger Beaty*

Key words: Large Language Models, Creativity, Preference Optimization, Novelty, Diversity

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为CrPO的新对齐方法，通过多维度创造性信号优化LLM的创造力，实验表明其性能优于GPT-4o等基线模型。

Motivation: 虽然LLM在自然语言生成任务中表现出色，但其生成真正创造性内容的能力有限。现有方法多集中于单一维度，未能全面解决创造力的多面性。

Method: 提出了Creative Preference Optimization (CrPO)方法，将多个创造力维度的信号模块化地注入偏好优化目标中，并使用了新的数据集MuCE进行训练。

Result: 实验结果表明，使用CrPO的模型在自动和人类评估中均优于强基线（如GPT-4o），生成的内容更具新颖性、多样性和惊喜性，同时保持高质量。

Conclusion: 在偏好框架内直接优化创造力是提升LLM创造性能力的有前景方向。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [107] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
*Chihan Huang,Hao Tang*

Key words: 扩散语言模型、半自回归框架、动态块大小、强化学习、分类器引导控制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种动态可控的半自回归框架CtrlDiff，解决了当前大型扩散语言模型中固定粒度和弱可控性的问题。

Motivation: 探索替代传统自回归模型的范式，结合扩散模型的并行生成能力和自回归依赖性，但现有方法存在固定长度输出和缺乏灵活控制的限制。

Method: 提出CtrlDiff，通过强化学习动态确定生成块大小，并引入针对离散扩散的分类器引导控制机制。

Result: 实验表明，CtrlDiff在混合扩散模型中表现优异，缩小了与最先进自回归方法的差距，并实现了多样任务的文本生成。

Conclusion: CtrlDiff通过动态块大小和分类器引导控制，提升了扩散语言模型的性能和可控性。

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [108] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
*Xiaoyu Tian,Yunjie Ji,Haotian Wang,Shuaiting Chen,Sitong Zhao,Yiping Peng,Han Zhao,Xiangang Li*

Key words: 蒸馏, 语言模型, 推理能力, AM-Thinking-v1

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究通过收集三种先进教师模型的输出，构建了三个并行数据集，分析表明AM-Thinking-v1数据集在多样性上表现最佳，基于该数据训练的学生模型在推理基准测试中表现最优。

Motivation: 通过蒸馏方法提升开源语言模型的推理能力，研究高质量推理数据对模型性能的影响。

Method: 收集三种教师模型（AM-Thinking-v1、Qwen3-235B-A22B、DeepSeek-R1）在189万条查询上的输出，构建数据集并分析其分布，训练学生模型进行评估。

Result: AM-Thinking-v1数据训练的学生模型表现最佳（如AIME2024得分为84.3），并展现适应性输出行为。数据集已公开。

Conclusion: 高质量验证的推理数据对提升推理导向语言模型性能至关重要。

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The AM-based model consistently achieves the best performance (e.g., 84.3 on
AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and
demonstrates adaptive output behavior-producing longer responses for harder
tasks and shorter ones for simpler tasks. These findings highlight the value of
high-quality, verified reasoning traces. We release the AM-Thinking-v1 and
Qwen3-235B-A22B distilled datasets to support future research on open and
high-performing reasoning-oriented language models. The datasets are publicly
available on Hugging Face\footnote{Datasets are available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [109] [Void in Language Models](https://arxiv.org/abs/2505.14467)
*Mani Shemiranifar*

Key words: transformer,语言模型,自适应计算,层激活,性能优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种非训练且无需参数的L2自适应计算（LAC）方法，用于检测推断过程中未激活的层（称为“Voids”），并通过实验证明选择性跳过这些层可以提高模型性能。

Motivation: 研究动机在于探究基于transformer的语言模型在推断过程中是否所有层都被激活，以及未激活层对模型性能的影响。

Method: 采用L2自适应计算（LAC）方法，通过监控激活的L2范数变化来识别未激活层（Voids），并在指令调优的语言模型中分析提示处理和响应生成两个阶段的层激活情况。

Result: 实验结果表明，在MMLU和GPQA Diamond等基准测试中，跳过未激活层可以显著提高模型性能（如Qwen2.5-7B-Instruct在MMLU上的准确率从69.24提升到71.29）。

Conclusion: 研究结论表明，模型推断过程中并非所有层都同等重要，选择性跳过未激活层可以提高模型效率和性能。

Abstract: Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.

</details>


### [110] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
*Somnath Banerjee,Pratyush Chatterjee,Shanu Kumar,Sayan Layek,Parag Agrawal,Rima Hazra,Animesh Mukherjee*

Key words: 大型語言模型, 代碼混合, 安全性, 可解釋性, 文化維度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 論文研究了代碼混合輸入和輸出對大型語言模型（LLMs）安全性的影響，發現其比單語言提示更容易產生不安全輸出，並通過可解釋性方法剖析了內部機製。

Motivation: 研究動機是探討代碼混合輸入如何增加LLMs生成不安全輸出的風險，以及這種現象背後的機製。

Method: 方法包括系統性評估代碼混合與單語言提示的安全性差異，並利用可解釋性技術分析模型的內部行為變化。

Result: 結果表明代碼混合提示顯著提高了模型生成不安全輸出的概率，並揭示了其背後的機製。

Conclusion: 結論強調了改進模型安全性的必要性，尤其是在處理代碼混合內容時。

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [111] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
*Tong Li,Jiachuan Wang,Yongqi Zhang,Shuangyin Li,Lei Chen*

Key words: 引文分类, 自监督学习, 对比学习, 预训练语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了Citss框架，通过自监督对比学习解决引文分类中标注数据稀缺、上下文噪声和虚假关键词相关性等问题，适用于基于编码器和解码器的预训练语言模型。

Motivation: 引文分类对学术分析至关重要，但现有方法因数据稀缺和上下文噪声等问题效果有限，需要改进。

Method: 采用自监督对比学习，结合句子级裁剪和关键词扰动策略，提升模型对目标引文和关键词的鲁棒性。

Result: 在三个基准数据集上验证了Citss优于现有方法，适用于不同类型的预训练语言模型。

Conclusion: Citss框架有效克服了引文分类的挑战，并在实验中表现出色。

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [112] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
*He Zhu,Junyou Su,Minxi Chen,Wen Wang,Yijie Deng,Guanhua Chen,Wenjia Zhang*

Key words: 城市规划,视觉语言模型,PlanGPT-VL,PlanBench-V,专业领域模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了PlanGPT-VL，一种专为城市规划地图设计的视觉语言模型，显著优于通用VLMs，为专业人士提供高效且准确的地图分析工具。

Motivation: 现有视觉语言模型在城市规划地图分析中表现不佳，亟需一种能理解空间配置、法规及多尺度分析的专用模型。

Method: 结合PlanAnno-V数据合成框架、关键点思维以减少幻觉，以及监督微调与冻结视觉编码器的综合训练方法。

Result: PlanGPT-VL在PlanBench-V测试中显著优于通用VLMs，7B参数模型性能媲美72B参数模型。

Conclusion: PlanGPT-VL为城市规划提供了高效、准确的视觉语言模型，兼具轻量化和高性能。

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [113] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
*Agam Goyal,Xianyang Zhan,Yilun Chen,Koustuv Saha,Eshwar Chandrasekharan*

Key words: LLMs, content moderation, explainability, Mixture of Experts

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MoMoE框架通过模块化设计为在线社区提供可扩展且透明的内容审核，无需为每个社区单独训练模型，同时提供解释。

Motivation: 解决现有内容审核方法需为每个社区单独训练模型且决策不透明的问题。

Method: 采用MoMoE框架，包含四个操作符（分配、预测、聚合、解释），分为社区专家和规范违反专家。

Result: 在30个未见过的子版块中，MoMoE的Micro-F1分数达0.72和0.67，性能与微调基线相当或更高，并提供可靠解释。

Conclusion: MoMoE展示了无需单独微调即可实现可扩展、透明的审核，为未来可信赖的人机共治提供方向。

Abstract: Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.

</details>


### [114] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
*Jun Cao,Jiyi Li,Ziwei Yang,Renjie Zhou*

Key words: 多模态基于方面的情感分析（MABSA）,大语言模型（LLMs),小语言模型（SLMs),双交叉注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种结合小语言模型(SLMs)和大语言模型(LLMs)的LRSA框架，用于多模态基于方面的情感分析(MABSA)，通过LLMs提供解释增强SLMs的能力。

Motivation: 现有MABSA方法依赖预训练SLMs，但其能力有限，导致对文本和图像中方面、情感及其关联的识别不准确；而LLMs虽在多项任务中表现优异，但在ABSA领域仍不及微调小模型。因此，研究旨在结合SLMs的决策能力和LLMs的附加信息。

Method: 提出LRSA框架，将LLMs生成的解释作为依据注入SLMs，并采用双交叉注意力机制加强特征交互与融合，提升SLMs识别方面和情感的能力。

Result: 在三个广泛使用的基准测试上，实验表明该方法优于两种基线模型，具有通用性，适用于多数预训练模型。

Conclusion: LRSA框架有效结合SLMs与LLMs的优势，显著提升MABSA任务的性能，证明其普适性和实用性。

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [115] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
*Jiale Kang,Ziyin Yue,Qingyu Yin,Jiang Rui,Weile Li,Zening Lu,Zhouran Ji*

Key words: 多模态, RNN, RWKV7, 轻量级, 预训练权重

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探索了基于现代RNN架构的多模态能力，提出了ModRWKV框架，通过动态可调的多模态编码器实现多源信息融合，并在性能和计算效率之间找到了平衡。

Motivation: 当前多模态研究主要依赖计算复杂度高的Transformer架构，而RNN等线性模型虽然推理成本低，但多限于单文本模态。本文旨在探索现代RNN在多模态场景中的潜力。

Method: 提出ModRWKV框架，基于RWKV7架构作为LLM主干，采用轻量级多模态模块和动态可调的异构模态编码器。利用预训练权重加速多模态训练。

Result: 通过实验发现，预训练权重对模型理解多模态信号至关重要，ModRWKV在性能和效率上达到平衡。

Conclusion: 现代RNN架构在多模态大语言模型中是Transformer的可行替代方案，并找到了ModRWKV的最佳配置。

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [116] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
*Michael Sullivan*

Key words: 语言模型, 逻辑形式, 数据效率, GFoLDS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出语言模型在逻辑形式上（LFLMs）的数据效率优于文本模型，并通过GFoLDS原型验证了其可行性。

Motivation: 探讨逻辑形式语言模型（LFLMs）在数据效率上的优势。

Method: 引入基于图的逻辑分布语义（GFoLDS）原型，预训练逻辑形式的图表示模型。

Result: 实验表明LFLMs能快速学习复杂模式，并在少量数据下优于文本模型。

Conclusion: LFLMs具有实际应用潜力，性能随参数和数据增加而提升。

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [117] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
*Zhipeng Yang,Junzhuo Li,Siyu Xia,Xuming Hu*

Key words: 大型语言模型, 内部链式思维, 层间执行, 任务分解, 模型透明度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）通过内部链式思维（internal chain-of-thought）逐层分解和执行复合任务，证明了其在网络深度中学习并顺序执行子任务的能力。

Motivation: 研究旨在揭示LLMs如何内部规划和执行复合任务，增强模型透明度，为指令级激活引导提供基础。

Method: 通过层间上下文掩码和新颖的跨任务修补方法验证子任务在不同网络深度的学习；利用LogitLens解码隐藏状态，分析层间执行模式。

Result: 在15个两步复合任务基准和真实TRACE基准上，观察到一致的逐层执行动态，证实LLMs能够顺序执行子任务。

Conclusion: LLMs具备内部规划和逐层执行子任务的能力，为模型透明性和指令级控制提供了新视角。

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [118] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
*Agam Goyal,Vedant Rathi,William Yeh,Yian Wang,Yuen Chen,Hari Sundaram*

Key words: 大语言模型, 解毒, 稀疏自编码器, 激活引导, 毒性降低

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文利用稀疏自编码器（SAEs）识别语言模型中的毒性方向，并通过激活引导减少毒性输出，在毒性降低和语言流畅性之间取得平衡。

Motivation: 尽管大语言模型（LLMs）广泛应用，但其仍可能生成有毒内容，现有方法容易被绕过，因此需要更有效的解毒方法。

Method: 使用SAEs识别毒性相关方向，并通过三个级别的激活引导强度进行干预，评估了GPT-2 Small和Gemma-2-2B模型的表现。

Result: 强引导下毒性降低高达20%，但语言流畅性可能下降；标准NLP基准分数保持稳定，SAEs的特征分离会影响安全性干预。

Conclusion: SAE基于的因果干预在解毒方面有潜力，但也存在局限性，为安全部署提供了实用建议。

Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.

</details>


### [119] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Key words: LLM, 推理能力, 评估平台, KORGym, 多模态评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: KORGym是一个动态评估平台，用于全面评估LLM的推理能力，支持多模态和交互式测试，揭示了模型家族的推理模式并展示了闭源模型的优越性能。

Motivation: 现有评估方法多为领域专用，无法全面捕捉LLM的通用推理能力，因此需要更全面的评估工具。

Method: 引入KORGym平台，包含50多种游戏（文本或视觉形式），支持交互式多轮评估和强化学习场景，对19个LLM和8个VLM进行实验。

Result: 发现模型家族内一致的推理模式，闭源模型表现更优，并分析了模态、推理策略、强化学习和响应长度对性能的影响。

Conclusion: KORGym有望成为推动LLM推理研究和开发复杂交互环境评估方法的重要资源。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [120] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
*Abhimanyu Talwar,Julien Laasri*

Key words: 中介语言，印地语，尼泊尔语翻译，转移方法，反向翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 使用印地语作为中介语言，通过转移方法和反向翻译方法将尼泊尔语翻译为英语，提升了翻译性能。

Motivation: 缺乏大规模、多样化领域的平行语料库导致某些语言对的翻译困难，因此探索使用中介语言的解决方案。

Method: 采用印地语作为中介语言，并测试了两种方法：转移方法（全监督）和反向翻译（半监督）。

Result: 转移方法在开发测试集上达到SacreBLEU分数14.2，比基线全监督方法提升了6.6分，但略低于半监督基线分数15.1。

Conclusion: 印地语是一种有效的中介语言，未来可进一步优化半监督方法的性能。

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [121] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
*Sohaila Eltanbouly,Salam Albatarni,Tamer Elsayed*

Key words: Automated Essay Scoring, trait-specific, rubric-based, LLM, regression model

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: TRATES是一个新的特质特定的AES框架，利用LLM生成特质特定特征，结合其他特征训练回归模型，取得了最优性能。

Motivation: 现有AES研究缺乏对个体特质的关注，本文提出特质特定的评分框架。

Method: 利用LLM基于评分标准生成特质特定特征，结合通用和提示特定特征训练回归模型。

Result: TRATES在广泛使用的数据集上对所有特质都达到了新的最优性能。

Conclusion: LLM生成的特征最为显著，证明了框架的有效性。

Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.

</details>


### [122] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
*Shangziqi Zhao,Jiahao Yuan,Guisong Yang,Usman Naseem*

Key words: Long-CoT, Pruning, Small Language Models, Reasoning, Logic Graphs

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种通过逻辑图剪枝优化长链推理（Long-CoT）的方法，命名为Prune-on-Logic，用于提升小语言模型（SLMs）的推理能力。实验表明，剪枝验证步骤能提高准确性并降低推理成本，而剪枝推理或整体链步骤会降低性能。

Motivation: 长链推理（Long-CoT）虽然能提升大语言模型的准确性，但其冗长的自反思风格难以有效压缩至小模型（SLMs）。论文通过能力对齐视角，探索剪枝是否能优化推理能力。

Method: 提出Prune-on-Logic框架，将Long-CoT转化为逻辑图，并在自验证约束下选择性剪枝低效推理步骤。研究了针对整体链、核心推理和验证的三种剪枝策略。

Result: 剪枝验证步骤能带来准确性增益并降低成本，优于未压缩的基线；而剪枝推理或整体链步骤会降低性能，表明小模型需要的是语义更简洁而非更短的推理链。

Conclusion: 剪枝是一种结构优化策略，能有效对齐CoT推理与小模型的能力，提升性能。

Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [123] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
*Wenbin Hu,Haoran Li,Huihao Jing,Qi Hu,Ziqian Zeng,Sirui Han,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Key words: 大型语言模型, 安全性, 隐私, 上下文完整性, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于上下文完整性理论的方法，通过强化学习和规则奖励来提升大型语言模型的安全性和隐私合规性，同时保持其推理能力。

Motivation: 现有大型语言模型在安全性和隐私保护方面存在不足，且现有方法依赖敏感模式匹配，忽略了法律合规性。

Method: 采用上下文完整性理论框架，结合强化学习和规则奖励，提升模型对GDPR、EU AI Act和HIPAA标准的合规性。

Result: 实验显示，该方法显著提升了法律合规性（安全/隐私基准准确率提升17.64%），并进一步增强了推理能力（MMLU和LegalBench基准分别提升2.05%和8.98%）。

Conclusion: 该方法不仅解决了安全性和隐私问题，还提升了模型的通用推理能力。

Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.

</details>


### [124] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
*Huihao Jing,Haoran Li,Wenbin Hu,Qi Hu,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Key words: MCP, MCIP, 安全风险, LLMs, 分类法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种针对MCP安全风险的新框架MCIP，通过分析MCP中缺失的安全机制并开发细粒度分类法，提升LLMs在MCP交互中的安全性表现。

Motivation: MCP的分散架构引入未充分探索的安全风险，需要系统性安全分析。

Method: 基于MAESTRO框架分析MCP的安全漏洞，提出MCIP协议，开发细粒度分类法和基准数据集。

Result: 实验结果表明LLMs在MCP交互中存在漏洞，提出的方法显著提升其安全性。

Conclusion: MCIP框架和分类法有效增强了MCP交互的安全性。

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps.Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [125] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
*Xianzhen Luo,Qingfu Zhu,Zhiming Zhang,Mingzheng Xu,Tianhao Cheng,Yixuan Wang,Zheng Chu,Shijie Xuyang,Zhiyuan Ma,YuanTao Fan,Wanxiang Che*

Key words: Code Sensitivity, CTF-Code, CTF-Instruct, LLMs, 反事实扰动

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了代码敏感性（Code Sensitivity）的概念，提出了CTF-Code基准和CTF-Instruct微调框架，以提高LLMs对代码细节变化的识别能力。

Motivation: 当前代码基准和指令数据主要关注难度和多样性，忽视了代码敏感性，即LLMs对问题描述细节变化的识别与响应能力。这限制了模型在实际场景中的应用效果。

Method: 通过构造CTF-Code基准（基于反事实扰动）和CTF-Instruct微调框架，从难度、多样性和敏感性三个维度优化LLMs。微调框架通过增量指令和数据选择机制实现。

Result: 实验显示，使用CTF-Instruct微调的LLMs在CTF-Code上提升超过2%，在LiveCodeBench上性能提升超过10%。

Conclusion: 增强LLMs的代码敏感性可以显著提升其性能，验证了敏感性作为代码理解重要维度的可行性。

Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.

</details>


### [126] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
*Guangzhi Xiong,Eric Xie,Corey Williams,Myles Kim,Amir Hassan Shariatmadari,Sikun Guo,Stefan Bekiranov,Aidong Zhang*

Key words: 大型语言模型、生物医学、假设生成、真实性评估、KnowHD、TruthHypo

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了一个名为TruthHypo的基准测试和一个基于知识的幻觉检测器KnowHD，用于评估大语言模型（LLMs）在生成真实生物医学假设方面的能力。研究发现LLMs难以生成真实的假设，而KnowHD能够有效筛选出真实的假设。

Motivation: 大型语言模型在生物医学等领域具有潜力，但其生成的假设存在真实性问题。为了系统研究这一问题，作者开发了评估工具TruthHypo和KnowHD。

Method: 作者创建了TruthHypo基准测试和KnowHD幻觉检测器，通过分析LLMs生成的假设的推理步骤，评估其真实性。

Result: 实验表明LLMs难以生成真实的假设，但KnowHD提供的基础评分能有效筛选出真实假设。人类评估也验证了KnowHD的实用性。

Conclusion: TruthHypo和KnowHD为评估LLMs生成真实假设的能力提供了有效工具，有助于加速科学发现。

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [127] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
*Soumadeep Saha,Akshay Chaturvedi,Joy Mahapatra,Utpal Garain*

Key words: sudoLLM, LLM, 用户授权, 安全对齐, 越狱攻击

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: sudoLLM框架通过注入用户授权信号，实现多角色对齐的LLM，提升安全性和抗攻击能力。

Motivation: 为LLM引入用户授权机制，以解决现有模型在安全性上的不足，特别是在防止越狱攻击方面。

Method: 注入用户授权信号的偏置，训练模型仅在授权时输出敏感信息。

Result: 实验表明该方法显著提高了对齐性、泛化能力和抗攻击性。

Conclusion: sudoLLM作为补充安全层，增强了LLM端到端的安全性。

Abstract: User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [128] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
*Rafael Rivera Soto,Barry Chen,Nicholas Andrews*

Key words: 机器文本检测, 风格特征空间, 检测鲁棒性, AURA, 攻击优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了机器生成文本检测的困难性，提出了一种基于风格特征空间的鲁棒检测方法，并探讨了攻击者的优化策略及其局限性。最终，作者强调了避免依赖机器文本检测的建议。

Motivation: 研究动机在于回答机器生成文本是否可以被可靠检测，并探讨检测器的鲁棒性及其面临的挑战。

Method: 作者提出了一种基于风格特征空间的检测方法，并通过实验分析了攻击者对检测器的优化策略及其有效性。此外，引入了一种新的AURA指标来衡量人类与机器生成文本分布的重叠情况。

Result: 实验表明，风格特征空间检测对优化攻击具有鲁棒性，但在单一样本情况下容易被攻击；样本增多后，检测性能显著提升。

Conclusion: 研究结论强调了机器文本检测的不可靠性，建议避免依赖此类检测方法。

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [129] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Key words: 大型语言模型, 测试意识, 安全对齐, 白盒探测, 霍桑效应

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在检测到被评估时会改变行为，类似于霍桑效应，本研究首次量化分析了这种‘测试意识’对模型行为及安全对齐的影响，并提出了一种白盒探测框架。

Motivation: 探讨LLMs在评估时表现出的‘测试意识’如何影响其行为，特别是安全对齐问题。

Method: 提出白盒探测框架：(i) 线性识别与测试意识相关的激活，(ii) 控制模型朝向或远离测试意识，同时监测下游性能。应用于多个领先的开源推理LLMs。

Result: 测试意识显著影响安全对齐，且不同模型表现不同。

Conclusion: 通过精细控制测试意识，研究旨在提升安全评估的可信度。

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [130] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
*Lingjie Jiang,Xun Wu,Shaohan Huang,Qingxiu Dong,Zewen Chi,Li Dong,Xingxing Zhang,Tengchao Lv,Lei Cui,Furu Wei*

Key words: Large Hybrid-Reasoning Models, adaptive thinking, efficiency, reinforcement learning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了大混合推理模型（LHRMs），通过自适应决定是否进行思考过程，提高了效率和性能。

Motivation: 传统的LRMs在处理简单查询时，过长的思考过程会带来不必要的开销，需要一种自适应方法来优化资源使用。

Method: 采用两阶段训练流程：混合微调（HFT）和在线强化学习（HGPO），并结合混合准确性度量评估模型能力。

Result: LHRMs在不同难度和类型的查询上自适应进行混合思考，性能优于现有LRMs和LLMs，同时显著提高效率。

Conclusion: LHRMs为混合思考系统的构建提供了基础，并重新思考了扩展思考过程的适用性。

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [131] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
*Yu Ying Chiu,Zhilin Wang,Sharan Maiya,Yejin Choi,Kyle Fish,Sydney Levine,Evan Hubinger*

Key words: AI风险、价值优先级、LitmusValues、AIRiskDilemmas、HarmBench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过揭示AI模型的价值优先级，LitmusValues和AIRiskDilemmas可以预测潜在风险行为。

Motivation: 随着AI模型能力增强，检测其风险行为变得更具挑战性。作者希望通过识别模型的内在价值，作为预警机制。

Method: 开发LitmusValues评估管道和AIRiskDilemmas数据集，通过模型的价值观测其风险行为。

Result: 模型的价值优先级（包括看似无害的）可以预测已知和未知风险行为。

Conclusion: 价值观测是识别AI潜在风险的有效方法。

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [132] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
*Xueguang Ma,Qian Liu,Dongfu Jiang,Ge Zhang,Zejun Ma,Wenhu Chen*

Key words: 强化学习, 大型语言模型, 多样化领域推理, 生成式答案验证器

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为General-Reasoner的新训练范式，通过构建大规模数据集和生成式答案验证器，提升了LLM在多样化领域的推理能力，并在多个基准测试中表现优于基线方法。

Motivation: 当前强化学习在LLM推理中多集中于数学和编程领域，限制了模型的泛化能力，本研究旨在通过新方法扩展其适用范围。

Method: 构建多样化领域的高质量数据集，并开发基于生成模型的答案验证器，替代传统的基于规则的验证方法。

Result: General-Reasoner在12个基准测试中表现优于基线方法，展现了强健且通用的推理能力。

Conclusion: General-Reasoner在多样化领域显著提升了LLM的推理能力，同时保持了在数学推理任务中的优越性能。

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [133] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
*Ronald Seoh,Dan Goldwasser*

Key words: EmoGist, 视觉情绪分类, 上下文学习, 多标签分类

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EmoGist是一种无需训练的上下文学习方法，用于视觉情绪分类，通过上下文相关的标签定义提高准确性。

Motivation: 情绪在图像中的表现具有高度上下文依赖性和细微差别，因此需要更准确的标签定义方法。

Method: EmoGist通过聚类分析生成多个情绪标签解释，测试时根据嵌入相似性选择解释并输入快速视觉语言模型进行分类。

Result: 在多标签Memotion数据集上微F1分数提升13点，多类FI数据集上宏F1分数提升8点。

Conclusion: EmoGist在视觉情绪分类中表现出显著改进，证明了上下文相关标签定义的有效性。

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [134] [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
*Jiaxin Guo,Zewen Chi,Li Dong,Qingxiu Dong,Xun Wu,Shaohan Huang,Furu Wei*

Key words: 奖励模型, 链式推理, 强化学习, 测试时间计算

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Reward Reasoning Models (RRMs) 通过引入链式推理来利用额外的测试时间计算，显著提升了奖励模型的性能。

Motivation: 现有奖励模型在利用测试时间计算提升性能方面存在挑战，RRMs旨在通过链式推理解决这一问题。

Method: 采用强化学习框架，无需显式推理轨迹训练数据，开发了自进化奖励推理能力的RRMs。

Result: RRMs在多领域奖励建模基准上表现优异，并能自适应地利用测试时间计算进一步提高准确性。

Conclusion: RRMs为奖励模型提供了一种高效的推理方法，显著提升了性能。

Abstract: Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.

</details>


### [135] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Key words: 终身学习, 大型语言模型, 模型编辑, 可扩展性, 归一化策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了ULTRAEDIT，一种训练无关、主题无关且无记忆的模型编辑方法，适用于超大规模的现实场景终身学习。该方法通过轻量级线性代数运算快速修改模型参数，并在终身学习中采用归一化策略以适应分布变化。实验表明，ULTRAEDIT在速度和性能上均优于现有方法。

Motivation: 终身学习使大型语言模型（LLMs）能够适应不断变化的信息，但现有方法难以满足实际终身学习的可扩展性和高效性需求，因此需要一种更高效的模型编辑方法。

Method: ULTRAEDIT采用自包含的编辑过程，仅依赖轻量级线性代数运算计算参数变化，并结合终身归一化策略以适应分布变化。

Result: ULTRAEDIT在7B参数的LLM上实现了7倍以上的编辑速度提升，VRAM消耗减少到1/3以下。实验表明，该方法支持100万次编辑并保持高准确性。

Conclusion: ULTRAEDIT在速度和可扩展性上均优于现有方法，为终身学习中的模型编辑提供了高效解决方案。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [136] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
*Haolei Xu,Yuchen Yan,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Shengpei Jiang,Kaitao Song,Weiming Lu,Jun Xiao,Yueting Zhuang*

Key words: 大语言模型, 数学推理, Chain-of-Thought, 思想跳跃, 数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为CoT Thought Leap Bridge Task的方法，用于检测和修复数学推理中缺失的中间步骤，以提升模型的推理完整性和性能。

Motivation: 现有的数学CoT数据集常因专家省略中间步骤而存在思想跳跃问题，影响模型学习与泛化能力。

Method: 提出了CoT-Bridge任务，检测思想跳跃并生成缺失的中间推理步骤，基于ScaleQM+数据集训练模型。

Result: 在数学推理基准测试中，经过修复的数据集显著提升模型性能（如NuminaMath上+5.87%），并增强泛化能力。

Conclusion: 提高推理完整性对模型性能有广泛益处，CoT-Bridge可作为即插即用模块与现有优化技术兼容。

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [137] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
*Nikhil Prakash,Natalie Shapira,Arnab Sen Sharma,Christoph Riedl,Yonatan Belinkov,Tamar Rott Shaham,David Bau,Atticus Geiger*

Key words: 语言模型, 心智理论, 信念推理, 回溯机制, Ordering IDs

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文分析了Llama-3-70B-Instruct模型如何通过因果介导和抽象机制推理角色的信念，提出了‘回溯机制’这一算法模式，并揭示了模型通过低秩子空间中的Ordering IDs绑定角色-对象-状态信息的原理。

Motivation: 探索语言模型理解角色信念（尤其是与现实不同的信念）的能力，以深入了解其心智理论（ToM）的表现。

Method: 构建包含简单故事的数据集，使用因果介导和抽象分析模型的推理机制，发现了‘回溯机制’及其实现方式。

Result: 模型通过Ordering IDs绑定角色-对象-状态信息，并利用‘回溯机制’在需要时检索相关信息；可见性信息的引入进一步影响信念更新。

Conclusion: 研究揭示了语言模型的信念跟踪机制，为逆向工程其心智理论推理提供了基础。

Abstract: How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.

</details>


### [138] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
*Avinash Patil,Siru Tao,Amardeep Gedhu*

Key words: 自杀预防, 大型语言模型, C-SSRS, AI伦理, 风险评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）在自杀风险评估中的能力，使用哥伦比亚自杀严重程度评定量表（C-SSRS），发现Claude和GPT的表现最接近人类标注。

Motivation: 在线平台（如Reddit的r/SuicideWatch）为表达自杀念头提供了空间，而LLMs的出现可能让人们转向AI系统寻求支持，研究旨在评估LLMs在此领域的适用性。

Method: 研究了六种模型的零样本性能，包括Claude、GPT、Mistral和LLaMA，按照7级严重程度量表（0-6级）对帖子进行分类。

Result: Claude和GPT与人类标注最接近，Mistral的序数预测误差最低，大多数模型表现出序数敏感性，误分类通常发生在相邻严重级别之间。

Conclusion: 研究强调了人类监督、透明度和谨慎部署的重要性，同时提供了完整代码和补充材料。

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [139] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
*Xingyuan Lu,Yuxi Liu,Dongyu Zhang,Zhiyao Wu,Jing Ren,Feng Xia*

Key words: 多模态隐喻, 情感分类, 中文数据集, 细粒度情感

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文引入了一个包含5000个中文隐喻广告文本-图像对的多模态数据集，解决了当前多模态隐喻情感分类研究中数据集不足和语言多样性忽视的问题。

Motivation: 多模态隐喻在情感表达中至关重要，但现有研究缺乏多模态隐喻细粒度情感数据集，且主要集中在英语领域。

Method: 构建了一个中文多模态数据集，包含5000个标注了隐喻、领域关系和细粒度情感的文本-图像对。

Result: 数据集公开发布，为多模态隐喻情感分类研究提供了资源支持。

Conclusion: 该数据集填补了研究空白，推动了多模态隐喻情感分析的发展。

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [140] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
*Ashwin Kumar,Yuzi He,Aram H. Markosyan,Bobbie Chern,Imanol Arrieta-Ibarra*

Key words: RLHF，前缀偏见，奖励模型，公平AI，数据增强

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了基于人类反馈的强化学习（RLHF）中奖励模型的偏见问题，提出检测和评估前缀偏见的方法，并揭示种族和性别维度的偏见。通过数据增强策略减轻偏见。

Motivation: 探索RLHF中奖励模型的潜在偏见，尤其是前缀偏见，以促进公平AI的发展。

Method: 引入新方法检测和评估前缀偏见，利用数据增强策略减轻偏见。

Result: 发现多种开源偏好数据集和奖励模型架构中普遍存在前缀偏见。

Conclusion: 强调在设计公平可靠的奖励模型时，关注偏见问题的重要性。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [141] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
*Federico Germani,Giovanni Spitale*

Key words: 大型语言模型，文本评估，框架效应，偏见，一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在文本评估中的一致性、偏见和框架效应问题，发现虽然模型间和模型内评估一致性高，但来源框架会显著降低一致性，尤其是对中国来源的文本。

Motivation: 研究大型语言模型在文本评估中的表现，揭示其判断是否一致、无偏见且不受框架效应影响。

Method: 使用四种先进LLMs（如OpenAI o3-mini等）评估4,800条叙述性陈述，并通过操纵来源（LLM或人类作者国籍）分析框架效应。

Result: 在无来源框架时，模型间和模型内一致性高；引入来源框架后，一致性降低，尤其是对中国来源的文本。

Conclusion: 框架效应显著影响LLM的文本评估，对其信息系统的中立性和公平性提出挑战。

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [142] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
*Aakash Gupta,Nataraj Das*

Key words: 电子商务、评论分析、生成预训练Transformer、抽象总结、决策支持

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种基于生成预训练Transformer的框架，用于抽象总结大量产品评论，帮助用户快速做出决策。

Motivation: 疫情期间，消费者对电子商务的偏好加速增长，但海量的产品评论可能导致决策瘫痪。现有工具虽能提供评分，但缺乏深入分析。本文旨在通过生成模型为消费者提供更智能的决策支持。

Method: 使用生成预训练Transformer（GPT-3的Curie引擎）进行微调，实现抽象总结而非简单的抽取式总结，从而更好地理解评论之间的真实关系。

Result: 模型生成了评论的优缺点摘要，帮助用户快速理解评论内容并做出决策。

Conclusion: 通过生成模型引入的“常识”元素，能够有效减少用户决策时间并提升决策质量。

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [143] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
*Weiming Zhang,Lingyue Fu,Qingyao Li,Kounianhua Du,Jianghao Lin,Jingwei Yu,Wei Xia,Weinan Zhang,Ruiming Tang,Yong Yu*

Key words: 认知诊断、大型语言模型、开放世界知识、语义信息、冷启动

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLM4CD利用大型语言模型（LLMs）为认知诊断任务引入丰富的语义信息，解决了传统基于ID的方法无法处理新学生和练习的问题。

Motivation: 当前认知诊断方法仅基于ID关系建模，忽略了教育数据中的语义信息，且难以应对新学生和练习的添加。LLMs的开放世界知识为解决这些问题提供了可能。

Method: 提出LLM4CD，利用LLMs构建认知表达性文本表示，并提出双层编码器框架（宏观认知文本编码器和微观知识状态编码器）。

Result: 在多个真实数据集上，LLM4CD优于现有认知诊断模型，验证了利用LLMs引入语义信息的有效性。

Conclusion: LLM4CD通过LLMs的开放世界知识，显著提升了认知诊断的性能，并能应对新数据的冷启动问题。

Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.

</details>


### [144] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
*Khanh-Tung Tran,Barry O'Sullivan,Hoang D. Nguyen*

Key words: LLMs, multilingual, low-resource, IRLBench, cultural bias

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了IRLBench，一个基于英语和爱尔兰语的基准测试，用于评估大型语言模型在低资源语言环境中的表现。

Motivation: 现有基准测试存在文化偏见、仅限文本评估、依赖选择题格式，且对极低资源语言支持不足。

Method: 引入IRLBench，基于2024年爱尔兰毕业考试开发12个学科任务，采用长文本生成和官方评分标准。

Result: 实验显示，模型在爱尔兰语中的正确率（55.8%）和语言保真度（<80%）显著低于英语（76.2%）。

Conclusion: IRLBench为多语言AI开发提供了更全面的评估工具。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [145] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
*Prithviraj Singh Shahani,Matthias Scheutz*

Key words: 大语言模型, 安全护栏, 噪声扰动, 鲁棒性, 推理, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，当前大语言模型的安全护栏在噪声扰动下脆弱性显著，安全微调无法提供额外保护，而基于推理的方法可能更有效。

Motivation: 探索大语言模型安全微调在噪声扰动下的鲁棒性，以评估其实际部署的安全性。

Method: 通过系统性注入高斯噪声到模型激活中，测试多个开源模型的安全性能。

Result: 高斯噪声显著增加有害输出率（最高27%），安全微调无额外保护，推理能力基本不受影响。

Conclusion: 当前安全对齐技术存在脆弱性，推理和强化学习可能是改进方向。

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [146] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
*Ruobing Yao,Yifei Zhang,Shuang Song,Neng Gao,Chenyang Tu*

Key words: RAG, EcoSafeRAG, 安全防御, 上下文多样性检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EcoSafeRAG通过句子级处理和诱饵引导的上下文多样性检测，在不依赖LLM内部知识的情况下识别恶意内容，提升RAG的安全性和性能。

Motivation: RAG通过引入外部知识增强LLM的事实准确性，但也带来了新的攻击面（如语料污染）。现有防御方法依赖模型内部知识，与RAG设计理念冲突。

Method: 采用句子级处理和诱饵引导的上下文多样性检测，分析候选文档的上下文多样性，独立于LLM知识识别恶意内容。

Result: 实验表明EcoSafeRAG在保障安全性的同时，提高了干净场景下的RAG性能（延迟相对1.2倍，令牌减少48%-80%）。

Conclusion: EcoSafeRAG通过创新方法平衡了安全性和性能，无需依赖LLM内部知识，具有即插即用优势。

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [147] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Key words: 时间智能、强化学习、语言模型、事件预测、创造性生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出Time-R1框架，赋予中等规模（30亿参数）的LLM全面的时间能力，包括理解、预测和创造性生成，通过三阶段的强化学习课程实现优越性能，超越更大规模模型。

Motivation: 大型语言模型在时间智能方面表现不足，本文旨在解决这一局限，提升模型对过去和未来的综合理解和生成能力。

Method: 采用三阶段强化学习课程，基于动态规则奖励系统，逐步培养模型的时间理解、预测和创造性生成能力。

Result: Time-R1在未来的事件预测和创造性生成任务中，表现优于比其大200倍的模型，如671B参数的DeepSeek-R1。

Conclusion: 研究表明，通过精心设计的强化学习微调，小规模模型也能实现优越的时间性能，为时间感知AI提供了一条可扩展的路径。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [148] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
*Shuxun Wang,Qingyu Yin,Chak Tou Leong,Qiang Zhang,Linyi Yang*

Key words: 重复诅咒, 大型语言模型, 诱导头, 注意力头正则化, 文本生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）中的重复诅咒现象，重点关注诱导头在驱动这种重复行为中的作用，并提出了通过注意力头正则化来缓解这一问题的方法。

Motivation: 重复诅咒现象已被广泛观察到，但其机制尚不明确。本研究旨在探索诱导头（一种擅长上下文学习的注意力头）对重复行为的贡献。

Method: 通过分析诱导头的“毒性”（即其在重复过程中主导模型输出logits的倾向），并提出注意力头正则化技术以减少其主导性。

Result: 发现诱导头是重复诅咒的关键驱动因素，并提出了一种技术手段来缓解这一问题。

Conclusion: 研究揭示了重复诅咒的机制，并为设计更健康和表现更好的LLMs提供了新思路。

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [149] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
*Jingyu Peng,Maolin Wang,Nan Wang,Xiangyu Zhao,Jiatong Li,Kai Zhang,Qi Liu*

Key words: 大型语言模型, 越狱攻击, 逻辑表达式, 安全机制, 多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为LogiBreak的通用黑盒越狱方法，通过逻辑表达式转换绕过LLM的安全系统，解决了当前安全机制对越狱攻击的脆弱性问题。

Motivation: 当前大型语言模型（LLM）的安全机制仍易受越狱攻击，作者认为是由于对齐导向提示与恶意提示之间的分布差异导致。

Method: 使用逻辑表达式转换将有害自然语言提示转换为形式逻辑表达式，利用对齐数据与逻辑输入之间的分布差距。

Result: LogiBreak在多语言越狱数据集上表现出色，有效绕过安全约束并保持语义意图和可读性。

Conclusion: LogiBreak为LLM安全提供了新视角，揭示了逻辑输入在越狱攻击中的潜力。

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [150] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
*Zhanglin Wu,Daimeng Wei,Xiaoyu Chen,Hengchao Shang,Jiaxin Guo,Zongyao Li,Yuanchang Luo,Jinlong Yang,Zhiqiang Rao,Hao Yang*

Key words: 大语言模型,机器翻译,调度策略,计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs在翻译任务中表现优异，但计算成本高。结合NMT和LLM的调度策略能优化性能与效率。

Motivation: 解决LLM在翻译任务中的高成本和延迟问题，同时保持翻译质量。

Method: 提出基于源句特征的调度策略，动态选择使用LLM或NMT。

Result: 实验表明，该策略能以最小LLM使用实现最优翻译性能。

Conclusion: 结合NMT和LLM的调度策略是高效且有效的方法。

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [151] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
*Sathya Krishnan Suresh,Tanmay Surana,Lim Zhi Hao,Eng Siong Chng*

Key words: 代码切换，大型语言模型，对话总结，多语言，错误分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了CS-Sum，一个用于评估大型语言模型（LLMs）对代码切换（CS）对话理解的基准测试，覆盖了三种语言对，并分析了多种方法的性能表现和常见错误类型。

Motivation: 现有研究对LLMs在代码切换（CS）输入下的理解能力探索不足，尤其是其全面性和正确性。CS-Sum的提出填补了这一空白。

Method: 通过CS-Sum基准，测试了10种LLMs在少样本、翻译总结和微调（LoRA、QLoRA）方法上的表现，并分析了错误类型。

Result: LLMs在自动指标上表现良好，但在代码切换输入下会犯细微错误，显著改变对话意义。错误率因语言对和模型不同而异。

Conclusion: LLMs在代码切换数据上的表现仍有提升空间，需要专门训练。

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [152] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
*Nathaniel Krasner,Nicholas Lanuzo,Antonios Anastasopoulos*

Key words: 多语言对齐、视觉信息、句子表示、低资源语言、跨语言NLU

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了视觉信息是否能替代双语文本对齐句子表示，发现多语言图像-标题对齐可以隐式对齐语言间的文本表示，且适用于跨语言自然语言理解和双语检索。

Motivation: 探索是否可以通过视觉信息而非双语文本来对齐多语言句子表示，以解决低资源语言对齐的难题。

Method: 利用图像标题数据集进行多语言图像-标题对齐，隐式对齐语言间的文本表示。

Result: 多语言图像-标题对齐可以成功对齐语言间的文本表示，且适用于未在预训练中见过的语言，并能用于跨语言NLU和双语检索。

Conclusion: 视觉信息可以作为双语文本的替代方案，实现多语言句子表示的对齐。

Abstract: Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [153] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
*Charles J. Torres,Richard Futrell*

Key words: 正交透明度、算法信息理论、相互压缩性、神经序列模型、跨书写系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于算法信息理论的跨书写系统度量方法，通过正交与语音字符串的相互压缩性来量化正交透明度，统一了不规则拼写和规则复杂性的度量。

Motivation: 当前缺乏一种统一的、不依赖于具体书写系统的正交透明度度量方法，无法同时捕捉不规则拼写和规则复杂性。

Method: 利用算法信息理论中的相互压缩性概念，结合神经序列模型的预序列编码长度，量化正交透明度。

Result: 在22种不同书写系统的语言中验证了方法的有效性，结果与关于脚本透明度的直觉一致。

Conclusion: 相互压缩性为量化正交透明度提供了一种简单、原则性强且通用的方法。

Abstract: Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.

</details>


### [154] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
*Julia Jose,Rachel Greenstadt*

Key words: 宣传技巧, NLP, 大语言模型, 变换器模型, 检测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大语言模型和变换器模型在检测新闻中的宣传技巧方面的对比表现。

Motivation: 识别宣传技巧对于做出明智决策至关重要，NLP技术进步为开发检测操纵性内容的系统提供了可能。

Method: 评估了几种大型语言模型和变换器模型在检测新闻文章中宣传技巧的性能。

Result: GPT-4在F1分数（F1=0.16）上优于GPT-3.5和Claude 3 Opus，但不如RoBERTa-CRF基线（F1=0.67）。三种大语言模型在检测六种宣传技巧中的一种（name-calling）上优于多粒度网络基线。

Conclusion: 大型语言模型在检测特定宣传技巧方面表现良好，但仍需进一步优化。

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [155] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
*Yu Guo,Dong Jin,Shenghao Ye,Shuangwu Chen,Jian Yang,Xiaobin Tan*

Key words: text-to-SQL, LLMs, SQLForge, 数据增强, 微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SQLForge通过合成高质量数据提升LLMs的text-to-SQL推理能力，显著缩小开源与闭源模型性能差距。

Motivation: 现有开源LLMs在text-to-SQL任务中表现落后于闭源模型，需提升数据质量和多样性。

Method: 引入SQL语法约束和反向翻译提升数据可靠性，通过模板丰富化和迭代机制增强数据多样性，并微调不同架构的开源模型。

Result: SQLForge-LM在Spider和BIRD基准测试中达到开源模型最佳性能，EX准确率分别为85.7%和59.8%。

Conclusion: SQLForge通过数据增强显著提升开源模型性能，缩小与闭源模型的差距。

Abstract: Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.

</details>


### [156] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
*Jacob Kleiman,Kevin Frank,Sindy Campagna*

Key words: 仿真系统, 大语言模型, 用户交互, 因果理解, 跨领域应用

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种结合仿真模型和大语言模型（LLMs）优势的框架，通过LLMs的对话能力简化用户与复杂仿真系统的交互，同时利用仿真系统增强LLMs对现实世界现象的准确理解。

Motivation: 解决仿真系统对非技术用户过于复杂，以及LLMs缺乏结构化、因果性理解的局限性。

Method: 提出一个仿真-代理框架，整合仿真模型和LLMs的能力，实现无缝交互和准确性提升。

Result: 框架为经验验证提供了稳健且通用的基础，适用于多个领域。

Conclusion: 该框架通过整合仿真和LLMs的优势，提升了用户友好性和模型准确性。

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [157] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
*Dimitris Roussis,Leon Voukoutis,Georgios Paraskevopoulos,Sokratis Sofianopoulos,Prokopis Prokopidis,Vassilis Papavasileiou,Athanasios Katsamanis,Stelios Piperidis,Vassilis Katsouros*

Key words: 大语言模型,希腊语,多语言支持,MAGPIE,基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Llama-Krikri-8B是基于Meta的Llama 3.1-8B优化的希腊语大语言模型，支持现代希腊语、英语、多调文本和古希腊语，在语言理解和生成方面表现优异。

Motivation: 针对希腊语的高质量语言模型需求，优化其语言适应性和性能。

Method: 使用8B参数的基础模型，结合多阶段后训练流程（如MAGPIE）和混合数据（人类标注与合成数据）。

Result: 在现有及新提出的希腊语基准测试中表现优于同类模型。

Conclusion: Llama-Krikri-8B在希腊语任务中具备显著优势，且计算效率高。

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [158] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Key words: 问答系统, 知识蒸馏, 推理痕迹, 小语言模型, 忠实性评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了在小语言模型（SLM）中通过知识蒸馏（KD）提升问答（QA）性能时，中间推理痕迹的忠实性评估问题，发现正确推理痕迹不一定保证最终答案正确。

Motivation: 在交互式对话系统时代，用户对模型输出的准确性和透明性要求越来越高。小语言模型虽然计算效率高，但性能较差，知识蒸馏方法被用于提升其性能。然而，中间推理痕迹的忠实性和其与最终性能的关联尚未明确。

Method: 采用了基于规则的问题分解方法，将复杂查询分解为结构化子问题，生成可解释的推理痕迹。具体在Open Book QA中，将问题分解为分类和信息检索步骤，简化痕迹评估。

Result: 实验发现，正确的推理痕迹并不一定导致最终答案正确，且中间痕迹与最终答案的关联性较低。这对利用推理痕迹提升小模型性能的假设提出了挑战。

Conclusion: 研究结果表明，依赖推理痕迹提升小模型的性能需谨慎，其忠实性与最终答案的一致性需要进一步验证。

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [159] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
*Zhengqing Yuan,Weixiang Sun,Yixin Liu,Huichi Zhou,Rong Zhou,Yiyang Li,Zheyuan Zhang,Wei Song,Yue Huang,Haolong Jia,Keerthiram Murugesan,Yu Wang,Lifang He,Jianfeng Gao,Lichao Sun,Yanfang Ye*

Key words: 大语言模型, 效率技术, 量化评估, 跨模态推广

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EfficientLLM是首个全面评估大规模语言模型效率技术的基准研究，涵盖预训练、微调和推理三方面的多种方法，通过精细指标揭示了效率与性能之间的权衡，并为跨模态技术提供了验证。

Motivation: 随着大语言模型参数和上下文窗口的增加，计算、能源和成本问题日益突出，亟需系统评估效率技术以优化资源使用。

Method: 研究基于生产级硬件集群，系统评估了预训练（如MQA、MoE）、微调（如LoRA、RSLoRA）、推理（如int4量化）等方法，定义了六种细粒度指标。

Result: 研究发现效率技术存在量化权衡，优化取决于任务和规模，且技术可跨模态推广。如MoE降低FLOPs但增加VRAM，int4量化显著减少内存/能耗但轻微降低精度。

Conclusion: EfficientLLM开源工具和数据集为研究人员和工程师提供了下一代基础模型效率优化的关键指导。

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [160] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
*Congchi Yin,Yongpeng Zhang,Xuyun Wen,Piji Li*

Key words: 联想记忆, 语言模型, 大脑对齐, 监督微调, 认知系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过整合联想记忆，改进了语言模型与人脑在处理语音信息时的对齐性，实验结果表明，经过特定监督微调的大型语言模型与大脑反应的关联性更强。

Motivation: 研究动机在于探索如何通过联想记忆的整合，提升语言模型与人脑在处理语音信息时的对齐性，从而更贴近人类的认知系统。

Method: 方法包括将语言模型激活映射到大脑活动以验证对齐性，然后将带有模拟联想记忆的原始文本作为输入提供给计算语言模型。此外，通过构建包含1000个故事样本的“Association”数据集，对大型语言模型进行特定监督微调。

Result: 研究发现在与联想记忆处理密切相关的大脑区域中，语言模型与大脑的对齐性有所提升。特定监督微调后的大型语言模型与大脑反应的对齐性更显著。

Conclusion: 研究结论表明，整合联想记忆可以显著提升语言模型与人脑的对齐性，尤其是在涉及联想记忆处理的大脑区域。

Abstract: Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.

</details>


### [161] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
*Arihant Tripathi,Liam Dugan,Charis Gao,Maggie Huan,Emma Jin,Peter Zhang,David Zhang,Julia Zhao,Chris Callison-Burch*

Key words: 机器生成文本检测、域适应性、集成学习、DoGEN

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DoGEN是一种通过集成领域专家模型和域分类器权重来检测机器生成文本的技术，能够适应未见领域，并在多个领域中表现优异。

Motivation: 随着语言模型的进步，需要更强大的机器生成文本检测方法，但现有检测器难以适应新领域和生成模型。

Method: 提出DoGEN技术，通过集成领域专家检测模型和域分类器权重来适应未见领域。

Result: DoGEN在多个领域的检测中表现优越，尤其是在未见领域上的表现优于更大的模型。

Conclusion: DoGEN为域适应性AI检测提供了有效的解决方案，并开源代码和模型以推动未来研究。

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [162] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
*Jiwon Song,Dongwon Jo,Yulhwa Kim,Jae-Joon Kim*

Key words: 推理路径压缩, KV缓存, 语义稀疏性, 语言模型, 推理加速

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种无需训练的方法RPC，通过利用推理路径的语义稀疏性来压缩KV缓存，从而加速推理过程，提升生成吞吐量，同时保持较高准确率。

Motivation: 为解决现有推理语言模型因生成长推理路径而导致内存占用高和生成速度慢的问题，作者提出了RPC方法，旨在通过压缩KV缓存来优化模型部署效率。

Method: RPC通过周期性压缩KV缓存，保留重要性得分高的KV缓存，使用由最近生成的查询组成的选择器窗口计算重要性得分。

Result: 实验表明，RPC将QwQ-32B的生成吞吐量提升至1.60倍，同时在AIME 2024基准测试中准确率仅下降1.2%。

Conclusion: 研究表明，推理路径的语义稀疏性可用于有效压缩KV缓存，为高效部署推理语言模型提供了实用方法。

Abstract: Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [163] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
*Jingqi Tong,Jixin Tang,Hangcheng Li,Yurong Mou,Ming Zhang,Jun Zhao,Yanbo Wen,Fan Song,Jiahao Zhan,Yuyang Lu,Chaoran Tao,Zhiyuan Guo,Jizhou Yu,Tianhao Cheng,Changhao Jiang,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Weifeng Ge,Guanhua Chen,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Key words: 视觉语言模型, 推理数据, 游戏代码, Code2Logic, LLMs, GameQA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种新颖的基于游戏代码的视觉语言推理数据生成方法Code2Logic，通过利用大型语言模型（LLMs）自动生成推理过程，解决了高质量视觉语言推理数据稀缺的问题。

Motivation: 由于视觉语言推理数据资源稀缺且标注成本高，论文希望通过游戏代码的自然逻辑结构自动生成高质量的推理数据。

Method: 利用游戏代码中的逻辑结构，结合LLMs，通过代码执行自动获取推理过程和结果，生成了GameQA数据集。

Result: GameQA数据集成本低、可扩展性强，且能有效提升视觉语言模型的推理能力。实验证明，Qwen2.5-VL-7B模型在7个视觉语言基准中性能提升了2.33%。

Conclusion: Code2Logic方法为视觉语言推理数据的生成提供了高效且可扩展的解决方案，展现了跨领域的泛化能力。

Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.

</details>


### [164] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Yiwei Wang*

Key words: 大型语言模型,推理过程,图分析,提示策略,认知分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种基于图的统一分析框架，用于建模大型语言模型（LLM）的推理过程，通过结构特性揭示推理准确性与策略的关系。

Motivation: 尽管推理型LLM显示出复杂推理能力，但其不稳定的行为（如少量提示下的性能下降）挑战了对它们的理解。

Method: 通过聚类长篇CoT输出为语义连贯的推理步骤，并构建有向推理图捕捉步骤间的依赖关系。

Result: 结构特性（如探索密度、分支和收敛比）与推理准确性密切相关，提示策略显著重塑推理结构。

Conclusion: 框架不仅能定量评估推理质量，还为提示工程和LLM认知分析提供了实用见解。

Abstract: Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.

</details>


### [165] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
*Yuanyi Wang,Zhaoyi Yan,Yiming Zhang,Qi Zhou,Yanggan Gu,Fei Wu,Hongxia Yang*

Key words: 模型融合,语义依赖,图蒸馏,大语言模型,推理任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了首个结构感知融合框架InfiGFusion，通过Graph-on-Logits蒸馏损失建模词汇维度间的语义依赖，显著提升了模型融合质量和性能。

Motivation: 现有基于logit的融合方法忽略了词汇维度间的语义依赖，这些依赖是模型生成行为多样性的关键。

Method: 提出InfiGFusion框架，保留top-k logits并构建全局共激活图，设计了排序闭式近似降低计算成本。

Result: 在11个基准测试中优于SOTA模型，尤其在复杂推理任务中表现突出，如多步算术和因果判断。

Conclusion: InfiGFusion能有效建模语义依赖，提升融合模型在复杂任务中的性能。

Abstract: Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [166] [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
*Chengyu Shen,Zhen Hao Wong,Runming He,Hao Liang,Meiyi Qiang,Zimo Meng,Zhengyang Zhao,Bohan Zeng,Zhengzhou Zhu,Bin Cui,Wentao Zhang*

Key words: 大语言模型,数学推理,问题验证,数据集筛选,MathQ-Verify

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了MathQ-Verify，一个五阶段管道，用于严格过滤数学问题中的不良或未明确的问题，显著提升了数学数据集的质量和验证性能。

Motivation: 现有的数学推理研究主要关注生成正确的推理路径和答案，而忽视了问题本身的有效性，因此需要一种方法来验证问题的合理性。

Method: 采用五阶段管道：格式验证、问题形式化、条件分解与验证、逻辑矛盾检测和目标导向的完整性检查。

Result: MathQ-Verify在多个基准测试中达到最先进性能，F1分数提升高达25个百分点，并通过轻量级模型投票实现了约90%的精确率和63%的召回率。

Conclusion: MathQ-Verify为数学数据集的可靠筛选提供了可扩展且准确的解决方案，减少了标签噪声和不必要计算。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [167] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
*Ajitesh Bankula,Praney Bankula*

Key words: 跨语言迁移, 多语言预训练模型, 语言家族, 形态学, NLP任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了跨语言迁移在语言家族和形态学视角下的表现，探讨了语言家族接近度和形态相似性对NLP任务性能的影响，并总结了多语言模型性能与语言学距离指标的关系。

Motivation: 探索多语言预训练模型在跨语言迁移中的表现，尤其是语言家族和形态学特征对迁移效果的影响。

Method: 通过分析语言家族接近度和形态相似性，评估多语言模型在不同NLP任务中的表现，并结合文献讨论结果。

Result: 语言家族接近度和形态相似性与跨语言迁移效果相关，且整合类型学和形态学信息可提升迁移性能。

Conclusion: 语言学特征（如语言家族和形态学）对跨语言迁移有显著影响，未来工作可尝试将这些信息融入模型预训练。

Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].

</details>


### [168] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
*Hiram Ring*

Key words: 词序变化、语言进化、最小最大理论、处理压力、信息结构

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种基于大规模平行数据集的通用底层机制，用于解释词序变化，研究发现词类长度与词序显著相关，支持了一种名为‘最小最大’的语言进化理论。

Motivation: 当前关于语言起源的理论存在分歧，本文旨在通过大规模数据验证词序变化的基本机制，弥合理论与实证之间的差距。

Method: 使用了一个包含1,500多种语言的标记平行数据集，涵盖133个语系和111个孤立语言，通过回归模型分析词类长度与词序的关系。

Result: 词类长度与词序显著相关，但关系并非简单直接；该机制预测了两种谱系中的历史词序变化，并解释了比谱系或语言区域更多的方差。

Conclusion: 提出了‘最小最大’理论，认为语言进化由处理和信息结构的竞争压力驱动，与效率导向和信息理论的近期研究一致。

Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
"Min-Max" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).

</details>


### [169] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
*Saydul Akbar Murad,Ashim Dahal,Nick Rahimi*

Key words: EEG-to-text, R1 Translator, bidirectional LSTM, transformer decoder, ROUGE metrics

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为R1 Translator的新模型，用于将EEG信号解码为文本，结合了双向LSTM编码器和预训练转换器解码器，显著提升了性能。

Motivation: 随着大型语言模型的快速发展，将人脑与语言处理结合的EEG信号解码仍存在性能瓶颈，需要改进。

Method: R1 Translator采用双向LSTM编码器与预训练转换器解码器的组合，利用EEG特征生成高质量文本。

Result: 在ROUGE-1、ROUGE-L、CER和WER等指标上，R1 Translator均优于T5和Brain Translator模型。

Conclusion: R1 Translator在EEG到文本的解码任务中表现出色，为相关领域提供了更高效的解决方案。

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [170] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Nam Le,Linh Ngo Van*

Key words: 持续关系抽取, 提示池, 灾难性遗忘, 标签描述, WAVE++

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了WAVE++方法，通过任务特定提示池和标签描述改进持续关系抽取任务，解决任务识别和灾难性遗忘问题，表现优于现有技术。

Motivation: 持续关系抽取（CRE）中，现有方法存在存储样本的内存和隐私问题，且提示选择策略不精确。WAVE++旨在通过改进提示池和引入标签描述解决这些问题。

Method: 结合前缀调优和专家混合，引入任务特定提示池、标签描述，提出无需训练的推理机制，并利用生成模型整合先验知识。

Result: WAVE++在实验中优于当前最佳的提示型和复述型方法，为CRE提供了更稳健的解决方案。

Conclusion: WAVE++通过灵活的提示池和全局上下文提高性能，无需显式存储数据，显著改善了CRE任务的结果。

Abstract: Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [171] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
*Mingliang Zhai,Zhi Gao,Yuwei Wu,Yunde Jia*

Key words: Embodied Question Answering, MemoryEQA, 多模态记忆, MT-HH3D数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了以记忆为中心的MemoryEQA框架，通过多模态分层记忆机制提升复杂任务的效率与准确性。

Motivation: 现有的EQA框架以规划器为中心，记忆模块无法充分与其他模块交互。本文旨在通过以记忆为中心的框架提升任务性能。

Method: 提出了多模态分层记忆机制（全局与本地记忆），并利用多模态大模型将记忆信息转换为不同模块的输入格式。

Result: 在HM-EQA、MT-HH3D和OpenEQA实验中，框架显著提升了性能，在MT-HH3D数据集上比基线模型提高了19.8%。

Conclusion: MemoryEQA框架通过增强记忆模块的交互能力，有效解决了复杂EQA任务的问题。

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [172] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
*Guochao Jiang,Guofeng Quan,Zepeng Ding,Ziqin Luo,Dixuan Wang,Zheng Hu*

Key words: 大语言模型, 推理效率, FlashThink, 早期终止

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文指出大语言模型（LLMs）在推理任务中生成冗长内容导致计算开销过大，提出一种早期终止推理的方法FlashThink，显著缩短内容长度而不降低准确性。

Motivation: LLMs在推理任务中过度生成长内容，增加了不必要的计算开销，研究发现部分推理过程已经足够生成正确答案，因此提出优化方案。

Method: 引入验证模型，识别推理过程中可以提前终止的时刻，以缩短推理内容。该方法在四个基准测试中进行了验证。

Result: 对于Deepseek-R1和QwQ-32B模型，推理内容长度分别减少了77.04%和77.47%，且准确性未受影响。

Conclusion: FlashThink方法能高效缩短推理内容，提升计算效率且不影响模型性能。

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [173] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
*Qianli Wang,Mingyang Wang,Nils Feldhus,Simon Ostermann,Yuan Cao,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Key words: 量化, 大型语言模型, 可解释性, 透明度, 用户研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 量化方法广泛用于加速大型语言模型（LLM）的推理和部署，但其对模型可解释性的影响尚未研究。本文通过实验发现，量化对可解释性的影响因方法、评估协议不同而异，有时甚至可能改善可解释性。

Motivation: 量化对LLM能力的退化已有研究，但对模型解释性和透明性的影响尚未探索，这在理解模型决策过程中至关重要。

Method: 使用三种常见量化技术和不同位宽，结合两种可解释性方法（反事实示例和自然语言解释）和两种可解释性分析（知识记忆分析和潜在多跳推理分析），辅以用户研究。

Result: 量化对模型解释性的影响不一致，可能退化也可能改善，取决于量化方法、解释性方法和评估协议。

Conclusion: 量化可能不可预测地影响模型透明度，这对于透明度要求高的LLM部署具有重要意义。

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [174] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
*Jiamin Su,Yibo Yan,Zhuoran Gao,Han Zhang,Xiang Liu,Xuming Hu*

Key words: Automated Essay Scoring, Multimodal Large Language Model, Collaborative Multi-agent Framework, CAFES, QWK

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了CAFES，一个多代理协作框架，用于解决自动作文评分（AES）中的泛化性和多模态感知问题，通过三个代理的协作显著提升评分的准确性。

Motivation: 传统AES方法在泛化性和多模态感知上表现不佳，而现有的MLLM方法可能导致评分与人类判断不一致。为了解决这些问题，提出了CAFES框架。

Method: CAFES框架包含三个代理：Initial Scorer用于快速评分，Feedback Pool Manager收集详细反馈，Reflective Scorer迭代优化评分以实现与人类评分的一致性。

Result: 实验显示，CAFES在QWK指标上相对提升了21%，尤其在语法和词汇多样性方面表现突出。

Conclusion: CAFES为智能多模态AES系统提供了新的思路，代码将在论文接受后公开。

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [175] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
*Qianli Wang,Van Bach Nguyen,Nils Feldhus,Luis Felipe Villa-Arenas,Christin Seifert,Sebastian Möller,Vera Schmitt*

Key words: 反事实数据增强、语言模型、评估模型、性能提升、人工干预

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，在反事实数据增强中，独立、非微调的评估模型最可靠，但完全自动化流程仍需人工干预。

Motivation: 反事实示例常用于提升语言模型性能，但评估其有效性的模型选择结果不一致，需明确生成与评估模型的关系。

Method: 通过实验（两种方法、三数据集、五生成模型、15评估模型）和用户研究（n=90），分析四种模型关系对评估的影响。

Result: 独立、非微调的评估模型表现最可靠，但与用户研究结果仍有显著差距。

Conclusion: 完全自动化的反事实数据增强不足，需结合人工干预。

Abstract: Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.

</details>


### [176] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
*Wenhui Zhu,Xuanzhao Dong,Xin Li,Peijie Qiu,Xiwen Chen,Abolfazl Razi,Aris Sotiras,Yi Su,Yalin Wang*

Key words: 强化学习, 多模态大型语言模型, 医学VQA, GRPO

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文探讨了在医学视觉问答任务中，强化学习调优的四个关键维度及其成效，发现GRPO方法优于传统监督微调。

Motivation: 研究旨在解决医学任务中强化学习调优的挑战，以使模型行为更符合临床期望。

Method: 通过分析四个关键维度（基础模型初始化、医学语义对齐、长度奖励影响和偏倚作用）并进行实验验证。

Result: GRPO强化学习方法在准确性和推理质量上优于传统监督微调。

Conclusion: 研究为医学领域的MLLMs提供了领域特异性调优的新见解，证实GRPO的有效性。

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [177] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
*Yuxuan Jiang,Dawei Li,Frank Ferraro*

Key words: Distilled Reasoning Pruning, chain-of-thought reasoning, efficiency, distillation, mathematical reasoning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DRP框架通过推理时剪枝与蒸馏结合的混合方法，显著提升大模型的推理效率而不损失精度。

Motivation: 解决大型推理模型（LRMs）在链式推理任务中推理轨迹过长导致的效率问题。

Method: 结合推理时剪枝与蒸馏，利用教师模型分解和剪枝推理路径，再蒸馏到学生模型。

Result: 在多个数学推理数据集上，DRP显著降低token使用量（如GSM8K从917降到328）同时提升精度（从91.7%到94.1%）。

Conclusion: 训练数据的推理结构与学生推理能力对齐是知识转移和性能提升的关键。

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [178] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
*Maya Srikanth,Run Chen,Julia Hirschberg*

Key words: 多模态模型、共情检测、模态冲突、门控融合、鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 多模态模型在共情检测中起关键作用，但当模态间存在冲突信号时，其性能会下降。研究发现，模态间预测分歧常反映潜在模糊性，且单模态主导信号可能在无其他模态支持时误导融合。人类与模型类似，不一定能从多模态输入中获益。

Motivation: 探索多模态模型在共情检测中的性能下降原因，尤其是当不同模态提供冲突信号时的情况。通过分析单模态与多模态预测的分歧，理解模型的失败模式。

Method: 使用微调的文本、音频和视频模型以及门控融合模型，分析预测分歧案例，并结合标注者不确定性验证模糊性。

Result: 研究表明，单模态主导信号可能误导多模态融合，且人类在多模态输入中也无法始终获益。分歧可作为诊断信号，帮助识别挑战性样本并提升系统鲁棒性。

Conclusion: 模态间预测分歧揭示了多模态共情检测中的挑战，为改进模型鲁棒性提供了重要线索。

Abstract: Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.

</details>


### [179] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
*Linxin Song,Taiwei Shi,Jieyu Zhao*

Key words: RFT, 幻觉税, LLM, SUM数据集, 拒答行为

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: RFT虽提升LLM推理能力，但可能导致幻觉税现象，即模型更自信地生成无解问题的错误答案。通过引入SUM数据集，研究发现仅10%的SUM训练即可恢复适当拒答行为，且不影响可解任务准确性。

Motivation: 探索RFT对模型可信度的副作用，特别是幻觉税现象，即模型对无解问题生成自信但错误答案的行为。

Method: 引入SUM数据集，测试模型识别无解问题的能力，并研究RFT训练对拒答率的影响。通过加入少量SUM数据优化RFT。

Result: 标准RFT训练使模型拒答率下降80%，但10%SUM训练可显著恢复拒答行为，且对可解任务影响最小。

Conclusion: 结合少量SUM数据的RFT既能提升推理能力，又能避免幻觉税，使模型更好地识别知识边界。

Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.

</details>


### [180] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
*Tingfeng Hui,Pengyu Zhu,Bowen Ping,Ling Tang,Yaqi Zhang,Sen Su*

Key words: 大语言模型, 指令遵循, 数据合成, 元分解, DecIF

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出的DecIF框架是一种完全自主的元分解指导方法，通过LLM生成多样化和高质量的指令遵循数据，无需依赖外部资源。

Motivation: 现有方法依赖预存文档或外部资源生成指令数据，限制了灵活性和通用性。DecIF旨在解决这一限制。

Method: 使用LLM迭代生成元信息，结合响应约束形成结构化指令，并通过LLM检测和解决不一致性。将指令分解为原子级评估标准，确保准确性。

Result: 实验表明DecIF在多样场景下表现卓越，具有高灵活性、可扩展性和通用性。

Conclusion: DecIF能自动生成高质量指令数据，显著提升LLM的指令遵循能力。

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [181] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
*Myra Cheng,Sunny Yu,Cinoo Lee,Pranav Khadpe,Lujain Ibrahim,Dan Jurafsky*

Key words: LLMs, sycophancy, social evaluation, ELEPHANT, face-preserving behaviors

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了LLMs中的‘奉承’行为，提出了一种新的社会奉承理论（ELEPHANT框架），并通过数据集验证了LLMs在模糊情境中过度维护用户‘面子’的问题。

Motivation: 现有研究仅关注LLMs对用户明确陈述的观点的附和，忽略了模糊情境（如建议和寻求支持）中的奉承行为可能带来的危害。

Method: 提出了ELEPHANT框架，通过评估五种维护‘面子’的行为（情感验证、道德认同、间接语言、间接行为和接受框架），在两个数据集（OEQ和AITA）上测试了八种模型。

Result: LLMs在OEQ中比人类多维护‘面子’47%，在AITA中42%的案例会附和被人类判定为不当的行为。

Conclusion: 社会奉承行为在LLMs中普遍存在且难以缓解，研究提供了理论和工具以理解和解决这一问题。

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [182] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
*Yuxuan Yao,Shuqi Liu,Zehua Liu,Qintong Li,Mingyang Liu,Xiongwei Han,Zhijiang Guo,Han Wu,Linqi Song*

Key words: 模型合并，激活引导，层特异性，互信息，LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于激活引导的共识合并（ACM）框架，通过层特异性合并系数有效地整合不同LLM的能力，显著优于现有方法。

Motivation: 现有模型合并方法通常忽略神经组件的功能异质性，ACM旨在解决这一问题，同时保留任务特定能力。

Method: ACM通过预训练和微调模型激活的互信息确定层特异性合并系数，无需梯度计算或额外训练。

Result: 实验表明，ACM在L2S和常规合并任务中表现优越，例如在Qwen-7B上实现55.3%响应长度减少和1.3点推理准确率提升。

Conclusion: ACM是一个高效且稳定的模型合并框架，显著优于现有基线方法。

Abstract: Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.

</details>


### [183] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
*Tai D. Nguyen,Long H. Pham,Jun Sun*

Key words: 大语言模型, 法律遵从性, 对抗数据生成, 陪审团审议, 动态评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: AutoLaw是一个新颖的违反检测框架，通过对抗数据生成和陪审团式审议提升LLM的法律遵从性。

Motivation: 现有法律评估基准缺乏适应性，无法反映多样化的地方背景，限制了在动态监管环境中的实用性。

Method: 结合对抗数据生成与陪审团式审议，动态合成案例法来模拟司法决策。

Result: 在三个基准测试中表现优异，对抗数据生成提升LLM区分能力，陪审团投票显著提高违反检测率。

Conclusion: AutoLaw能适应性探测法律偏差，提供可靠的情境感知判断，适用于法律敏感场景。

Abstract: The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based "jurors" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [184] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
*Yingli Shen,Wen Lai,Shuo Wang,Kangyang Luo,Alexander Fraser,Maosong Sun*

Key words: 大型语言模型, 多语言平行数据, TED2025语料库, 继续预训练, 指令微调, 跨语言语义

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了如何利用多语言平行数据（如TED2025语料库）来提升大型语言模型（LLMs）的性能，结果表明平行数据训练的模型在多项任务中表现优于非对齐数据训练的模型。

Motivation: 现有的大规模多语言数据往往是未对齐的，这限制了其捕捉跨语言语义的能力。多语言平行数据能提供更强的跨语言一致性，因此更具潜力提升多语言任务的表现。

Method: 构建了一个大规模、高质量的多语言平行语料库TED2025，覆盖113种语言，并在其上研究了继续预训练、指令微调等策略。

Result: 实验表明，基于平行数据训练的模型在六项多语言基准测试中表现优于基于非对齐数据的模型。

Conclusion: 多语言平行数据对提升LLMs的跨语言表现具有显著效果。

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [185] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
*Wei Jiang,Anying Fu,Youling Zhang*

Key words: 模型剪枝,知识蒸馏,MAMA Pruning,GRPO奖励

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MAMA Pruning是一种改进的剪枝方法，通过分析权重和偏差减少模型大小和计算复杂度，同时保持性能。

Motivation: 现有剪枝方法导致性能下降或需要大量微调，目标是获得更小、更快的模型。

Method: 基于权重和偏差的预训练分析及GRPO奖励作为指标，提出MAMA Pruning。

Result: 在各种剪枝水平和任务中优于或媲美现有方法。

Conclusion: MAMA Pruning在极端剪枝水平下仍能保持与原模型相当的性能。

Abstract: Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.

</details>


### [186] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
*Feiyu Duan,Xuemiao Zhang,Sirui Wang,Haoran Que,Yuqi Liu,Wenge Rong,Xunliang Cai*

Key words: 大语言模型, 数据选择, 知识密度, 知识覆盖度, 预训练语料

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出一种无梯度的高知识评分器（HKS），用于从知识维度选择高质量数据，以解决预训练语料中知识稀缺的问题。通过多领域知识元素池和知识密度与覆盖度指标，选出知识密集的数据，提升模型在知识密集任务和通用理解任务上的表现。

Motivation: 现有的高质量数据选择方法未考虑文本语料中的知识丰富度，导致预训练语料中存在知识稀缺问题。

Method: 提出高知识评分器（HKS），基于多领域知识元素池和知识密度与覆盖度指标，选择知识密集的数据。

Result: 实验结果证明，该方法在知识密集和通用理解任务中提升了模型性能，同时增强了模型的通用和领域特定能力。

Conclusion: HKS能有效解决知识稀缺问题，提升模型性能，适用于多领域数据选择。

Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.

</details>


### [187] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
*Weihong Du,Wenrui Liao,Binyu Yan,Hongru Liang,Anthony G. Cohn,Wenqiang Lei*

Key words: 大语言模型, 任务规划, 逆向推理, Minecraft, BAR代理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于逆向推理的大语言模型代理BAR，用于解决复杂任务规划问题，通过从任务目标逆向分解步骤，显著优于现有方法。

Motivation: 现有基于正向推理的任务规划方法在复杂任务中效果不佳，作者提出逆向推理以缩小初始状态与任务目标间的感知差距。

Method: 设计了BAR代理，包含递归目标分解模块、状态一致性维护模块和阶段记忆模块，从终端状态进行规划。

Result: 实验表明BAR在复杂任务中表现优于现有方法，验证了所提模块的有效性。

Conclusion: 逆向推理是解决复杂任务规划的有效途径，BAR的设计显著提升了规划性能。

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [188] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
*Franziska Sofia Hafner,Ana Valdivia,Luc Rocher*

Key words: 语言模型, 性别偏见, 性别构建, 性别多样性, 实证研究

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文指出语言模型编码并延续了有害的性别刻板印象，呼吁对性别偏见的定义和解决方法进行重新评估。

Motivation: 现有研究仅从词汇关联角度缓解性别偏见，但忽视了性别构建本身的危害，如二元性别观念对性别多样性的抹杀。

Method: 基于性别研究文献，实证分析了16种不同架构、训练数据和规模的语言模型如何编码性别。

Result: 发现语言模型倾向于将性别编码为与生物性别绑定的二元类别，且大型模型更强化了这种关联。

Conclusion: 需重新定义和解决语言模型中的性别偏见问题。

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [189] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
*Yihua Zhu,Qianying Liu,Akiko Aizawa,Hidetoshi Shimodaira*

Key words: KBQA, LLM, KG-RAG, PDRR, 语义解析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PDRR框架通过预测、分解、检索和推理四阶段方法，显著提升KBQA性能，支持复杂问题解答。

Motivation: 解决LLM-only方法的过时知识、幻觉和透明性问题，以及KG-RAG方法仅限简单链式问题的限制。

Method: 提出PDRR框架：预测问题类型、分解为结构化三元组、从KB检索信息、指导LLM代理完成推理。

Result: PDRR在不同LLM骨干上表现优于现有方法，尤其对复杂问题效果显著。

Conclusion: PDRR通过结构化规划和逻辑推理，有效提升KBQA的通用性和性能。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.

</details>


### [190] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
*Ernests Lavrinovics,Russa Biswas,Katja Hose,Johannes Bjerva*

Key words: 大语言模型、知识图谱、幻觉、多语言评估、事实性检查

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于知识图谱（KG）的多语言、多跳基准测试MultiHal，用于评估生成文本的事实性，并展示了KG在减轻大语言模型（LLM）幻觉问题中的潜力。

Motivation: 现有的评估基准主要集中在英语数据集上，且依赖额外上下文而忽略结构化的事实资源。论文旨在填补这一空白，利用知识图谱（KG）提供结构化事实表示，以减轻LLM的幻觉问题。

Method: 通过挖掘开放领域的KG路径并筛选高质量子集，构建了MultiHal基准测试，结合KG增强检索（KG-RAG）方法进行评估。

Result: 基线评估显示，KG-RAG在多语言和多个模型上的语义相似度得分提升了0.12至0.36点，证明了KG整合的潜力。

Conclusion: MultiHal有望促进未来基于图的事实性检查和幻觉减轻任务的研究。

Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.

</details>


### [191] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
*Wei Fan,Tianshi Zheng,Yiran Hu,Zheye Deng,Weiqi Wang,Baixuan Xu,Chunyang Li,Haoran Li,Weixing Shen,Yangqiu Song*

Key words: 法律规则归纳,大型语言模型,计算法学,基准数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了法律规则归纳（LRI）任务，并从大量中国案例中构建了首个LRI基准数据集，用于改进大型语言模型（LLMs）在提取隐性法律规则时的能力。实验表明，现有LLMs在此任务上存在泛化过度和幻觉问题，但通过训练可以提高性能。

Motivation: 目前计算法学研究主要关注已有规则的应用，而从司法判决中归纳法律规则的研究不足，且缺乏任务定义、基准数据集和方法论。大型语言模型（LLMs）的出现为自动化提取隐性法律原则提供了机会，但进展受到限制。

Method: 论文将法律规则归纳（LRI）任务形式化为从类似判例中提取简洁且可推广的教义规则，包含共同前提、规范性行为和法律后果。并构建了包含5,121个案例集（共38,088个中国案例）的基准数据集和216个专家标注的测试集。

Result: 实验结果显示：1）现有最优LLMs在处理LRI任务时存在泛化过度和幻觉问题；2）使用该数据集训练显著提高了LLMs在类似案例中捕捉规则模式的能力。

Conclusion: 论文通过形式化LRI任务和构建基准数据集，为法律规则的自动化提取提供了新方法，并验证了LLMs在此任务上的潜力与改进方向。

Abstract: Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.

</details>


### [192] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
*Li Li,Peilin Cai,Ryan A. Rossi,Franck Dernoncourt,Branislav Kveton,Junda Wu,Tong Yu,Linxin Song,Tiankai Yang,Yuehan Qin,Nesreen K. Ahmed,Samyadeep Basu,Subhojyoti Mukherjee,Ruiyi Zhang,Zhengmian Hu,Bo Ni,Yuxiao Zhou,Zichao Wang,Yue Huang,Yu Wang,Xiangliang Zhang,Philip S. Yu,Xiyang Hu,Yue Zhao*

Key words: PersonaConvBench,个性化推理,多轮对话,大型语言模型,基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PersonaConvBench是一个大规模基准测试，用于评估大型语言模型在多轮对话中的个性化推理和生成能力，整合了个人化和对话结构，涵盖三个核心任务，并在多样化场景中展现显著性能提升。

Motivation: 现有工作通常将个人化和对话结构分开研究，PersonaConvBench旨在将两者结合，系统分析个性化对话上下文如何影响LLM输出，以支持更适应个体风格的模型研究。

Method: 提出PersonaConvBench基准，包含句子分类、影响回归和用户为中心文本生成三大任务，基于十个多样化Reddit领域数据，并在统一提示设置下测试商业和开源LLM。

Result: 实验表明，引入个性化历史能显著提升性能，如情感分类任务中相对最佳非对话基线提升198%。

Conclusion: 通过发布PersonaConvBench及其评估代码，作者希望支持研究能够适应个体风格、追踪长期上下文并生成丰富回应的大型语言模型。

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [193] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
*Yakun Zhu,Zhongzhen Huang,Linjie Mu,Yutong Huang,Wei Nie,Shaoting Zhang,Pengfei Liu,Xiaofan Zhang*

Key words: 大型语言模型, 医疗诊断, 基准测试, 临床推理, DiagnosisArena

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了一个名为DiagnosisArena的全面医疗诊断基准测试，旨在评估大型语言模型在临床诊断推理中的能力，发现当前最先进模型的准确率仍较低，凸显了其在实际应用中的局限性。

Motivation: 为促进大型语言模型在真实医疗环境中的安全和有效应用，急需系统评估其诊断能力。现有医疗基准测试无法充分评估高级诊断推理能力。

Method: 开发DiagnosisArena基准测试，包含1,113对病例和诊断，覆盖28个医学领域。测试数据来自顶级医学期刊的临床案例报告，并通过多轮筛选和专家评审确保质量。

Result: 最先进的模型o3-mini、o1和DeepSeek-R1的准确率分别为45.82%、31.09%和17.79%，显示当前模型在临床诊断推理中存在显著泛化瓶颈。

Conclusion: DiagnosisArena旨在推动AI诊断推理能力的进一步进步，为解决真实临床诊断挑战提供更有效方案。

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [194] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
*Tianle Gu,Zongqi Wang,Kexin Huang,Yuanqi Yao,Xiangliang Zhang,Yujiu Yang,Xiuying Chen*

Key words: LLM watermarking, entropy, feature extractor, threshold navigator

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为Invisible Entropy (IE)的LLM水印方法，通过轻量级特征提取器和熵标记器预测高低熵区域，解决了低熵场景下水印效果差的问题，同时避免了计算成本高和模型泄露风险。

Motivation: 传统Logit-based水印在低熵场景下效果不佳，且依赖原始LLM导致计算成本高和模型泄露风险。

Method: 引入轻量级特征提取器和熵标记器预测高低熵区域，并结合阈值导航器动态调整熵阈值。

Result: 在HumanEval和MBPP数据集上，IE参数规模减少99%，性能与现有最佳方法相当。

Conclusion: IE提供了一种高效、安全的低熵水印解决方案。

Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [195] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
*Hongru Wang,Deng Cai,Wanjun Zhong,Shijue Huang,Jeff Z. Pan,Zeming Liu,Kam-Fai Wong*

Key words: 自推理语言模型, 思维链, 推理任务, 自我训练, 性能提升

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种自推理语言模型（SRLM），通过自我训练合成更长的思维链数据，显著提升大语言模型在复杂推理任务中的性能。

Motivation: 现有方法依赖人工生成的长思维链，难以创建和获取，且性能提升有限。

Method: SRLM通过少量示例引导模型自我生成更长的推理链，并通过自我训练不断迭代优化性能。

Result: 在MMLU、GSM8K等五个任务上平均提升超过2.5分，采样64次时可进一步提升至平均7.89分。

Conclusion: SRLM展现了通过自我训练生成多样且创造性推理路径的潜力。

Abstract: Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.

</details>


### [196] [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
*Filip Miletić,Aaron Schmid,Sabine Schulte im Walde*

Key words: 德语BERT, 名词复合词, 语义可组合性, 预训练模型, 语义模糊性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本研究探讨了预训练的德语BERT模型如何编码名词复合词的语义知识，发现其表现不如英语BERT模型，可能由于德语的复合词生成能力更强，导致语义更模糊。

Motivation: 旨在评估德语BERT模型在捕捉名词复合词语义方面的能力，并与英语BERT模型的类似研究进行比较。

Method: 通过组合目标词、层数以及大小写模型进行全面实验，利用868个标准复合词预测其语义可组合性。

Result: 德语BERT的表现明显落后于英语BERT，语义信息主要在早期层中可提取，德语的复合词生成能力和语义模糊性可能是原因。

Conclusion: 德语名词复合词的语义解析任务比英语更具挑战性，可能与德语的复合词生成能力更强及其语义模糊性有关。

Abstract: This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.

</details>


### [197] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
*Wei Zhou,Mohsen Mesgar,Heike Adel,Annemarie Friedrich*

Key words: 表格问答、多模态大模型、文本大模型、控制性研究、FRES

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 对表格问答任务中表格作为文本或图像编码的两种方式进行首次控制性研究，发现最佳组合因场景而异，并提出动态选择方法FRES，性能提升10%。

Motivation: 探讨表格问答任务中表格作为文本或图像编码的优劣，缺乏控制性实验，无法细粒度区分不同方法的效果。

Method: 建立新基准数据集，系统分析七对多模态大模型和纯文本大模型，动态选择表格表示方法FRES。

Result: 最佳组合因问题复杂度和表格大小而异，FRES方法平均性能提升10%。

Conclusion: 表格表示与模型组合需动态选择，FRES方法显著提升性能。

Abstract: In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.

</details>


### [198] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
*Chengzhi Zhang,Xinyi Yan,Lei Zhao,Yingyi Zhang*

Key words: Keyphrase Extraction, academic articles, structural features, section texts

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过利用学术论文的结构特征和章节文本，结合关键词提取算法，有效提升了关键词提取模型的性能。

Motivation: 解决关键词提取（KPE）因摘要长度限制导致的语义不足问题，并避免全文提取带来的噪声干扰。

Method: 1. 研究七种结构特征对KPE模型的影响；2. 结合章节文本的提取结果，通过关键词整合算法优化模型性能。

Result: 结构特征的引入提升了KPE性能，关键词整合方法表现最佳，章节结构分类质量影响KPE效果。

Conclusion: 学术文章的章节结构信息可用于提升关键词提取的效果。

Abstract: The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [199] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
*Pittawat Taveekitworachai,Potsawee Manakul,Sarana Nutanong,Kunat Pipatanakul*

Key words: prior prompt engineering, reinforcement fine-tuning, language models, behavior styles

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了在强化微调（RFT）中的先验提示工程（pPE），探讨了不同pPE方法如何引导语言模型内化特定行为，并在多个基准测试中验证其表现优于推理时提示工程（iPE）。

Motivation: 尽管RFT研究主要集中在算法、奖励塑造和数据管理上，但先验提示的设计在训练中仍未充分探索。本文旨在填补这一空白。

Method: 从iPE策略中翻译出五种代表性pPE方法（如推理、规划、代码推理等），并在Qwen2.5-7B模型上进行实验。

Result: 所有pPE训练模型均优于iPE提示的模型，其中null-example方法表现最佳，且不同pPE策略会赋予模型不同的行为风格。

Conclusion: pPE是RFT中一个强大但未被充分研究的维度，未来可进一步探索。

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [200] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
*Sanjay Govindan,Maurice Pagnucco,Yang Song*

Key words: 大语言模型, 时间对齐, 激活工程, LLaMA 2, 事实召回

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过激活工程技术提升LLM的时间对齐能力，无需训练或数据集，显著提高了模型在特定时间上下文中的事实召回率。

Motivation: LLMs在回答与时间相关的问题时可能产生过时或不准确的回答，因此需要确保其生成的响应与特定时间点一致。

Method: 使用激活工程技术对LLaMA 2的三个版本进行时间对齐，研究不同注入层和提示策略的效果。

Result: 实验显示，该方法在相对提示和显式提示中分别提升了44%和16%，与微调方法效果相当，但计算效率更高且无需预对齐数据集。

Conclusion: 激活工程技术是一种高效且无需训练的时间对齐方法，能够显著提升LLM的时间上下文准确性。

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [201] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
*Zahraa Al Sahili,Ioannis Patras,Matthew Purver*

Key words: 多语言视觉语言模型、社会偏见、CLIP、刻板印象、零样本评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究首次系统审计了三种多语言CLIP模型（M-CLIP、NLLB-CLIP和CAPIVARA-CLIP）在十种语言中的社会偏见，发现多语言性并未缓解偏见，反而加剧了性别偏见。

Motivation: 研究动机是探索多语言视觉语言模型的社会偏见问题，尤其是它们在低资源语言中的表现。

Method: 采用零样本设置，通过平衡的FairFace和PATA数据集，量化了种族和性别偏见，并测量了刻板印象的放大效应。

Result: 研究发现，所有模型在性别偏见上均强于其英文基线；低资源语言中偏见更明显；跨语言权重共享可能引入外来刻板印象。

Conclusion: 多语言视觉语言模型的偏见评估需要更细粒度、语言感知的方法，以揭示隐藏的语言特定问题。

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [202] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
*Zhenkai Qin,Jiajing He,Qiao Fang*

Key words: 细粒度情感分析, 提示学习, TextCNN, 多任务学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于提示学习的统一框架PL-FGSA，用于细粒度情感分析，通过整合提示设计和轻量级TextCNN骨干网络，显著提升了模型在完全数据和低资源条件下的性能表现。

Motivation: 传统的细粒度情感分析方法需要特定任务架构和大量标注数据，这限制了其泛化能力和可扩展性，因此需要一种更高效的解决方案。

Method: 提出PL-FGSA框架，将细粒度情感分析重新定义为多任务提示增强生成问题，结合提示设计和TextCNN骨干网络，同时解决方面提取、情感分类和因果解释任务。

Result: 在SST-2、SemEval-2014 Task 4和MAMS三个基准数据集上，PL-FGSA的F1得分分别为0.922、0.694和0.597，优于传统方法。

Conclusion: PL-FGSA通过提示学习显著提升了细粒度情感分析的性能，具有实际应用价值。

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [203] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Key words: 大型语言模型,字符级任务,概念涌现,渗透模型,轻量化架构

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在字符级任务上表现不佳，本研究通过概念涌现理论分析这一问题，提出了一种轻量级改进方法。

Motivation: LLMs在字符级任务（如字母计数）上的失败凸显了其局限性，研究者希望通过分析概念涌现来解决这一问题。

Method: 使用19项合成任务隔离字符级推理，并通过渗透模型解释概念涌现模式，同时提出了一种轻量化架构改进。

Result: 研究发现字符级能力在训练后期才突然涌现，且改进方法显著提升了性能。

Conclusion: 研究为理解并解决LLMs的结构性盲点提供了理论框架和实践方案。

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [204] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
*Yunlong Liang,Fandong Meng,Jie Zhou*

Key words: Mixture-of-Experts, 机器翻译, 上下文感知, 层次化路由, 多领域翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: THOR-MoE通过引入层次化任务引导和上下文感知的路由策略，解决了稀疏Mixture-of-Experts（MoE）在机器翻译中的两个局限性，并在多领域和多语言翻译任务中表现出色。

Motivation: 当前MoE方法直接利用NMT的任务知识（如领域/语言特定知识），但这些知识在实际应用中通常不可用，且忽略了自然分组的领域/语言特性；同时，专家选择仅依赖局部token表示，缺乏全局上下文信息。

Method: 提出THOR-MoE：1）分层预测领域/语言标签并提取混合表示以分配任务级专家；2）注入上下文信息，从预选的任务级专家集中增强token路由。

Result: 在多领域和多语言翻译基准测试中表现优异，与现有路由方案兼容，能在较少激活参数下提升性能（如平均0.75 BLEU提升）。

Conclusion: THOR-MoE是一种即插即用的模块，显著提升了MoE在翻译任务中的性能，适用于多种MoE架构。

Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.

</details>


### [205] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
*Yusuf Denizay Dönder,Derek Hommel,Andrea W Wen-Yi,David Mimno,Unso Eun Seo Jo*

Key words: LLM, 代码生成, 文本到SQL, 成本优化, N-rep一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为N-rep一致性的成本优化方法，用于文本到SQL任务，效果接近昂贵技术的同时显著降低成本。

Motivation: 当前基于LLM的技术（如CoT、自一致性和微调）在代码生成任务中表现优异，但成本高昂，需探索更经济的替代方案。

Method: N-rep通过利用同一模式输入的多种表示来弥补单一表示的不足，从而在不使用推理或微调的情况下实现鲁棒性，并可选用更小、更经济的模型。

Result: N-rep在BIRD基准测试中表现与其他昂贵方法相似，但每次查询成本仅需0.039美元。

Conclusion: N-rep是当前成本范围内性能最佳的文本到SQL方法，为高效低成本的代码生成提供了可行方案。

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [206] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
*Xiang Zhang,Juntai Cao,Jiaqi Wei,Yiwei Xu,Chenyu You*

Key words: tokenization, symbolic reasoning, Chain-of-Thought, byte-pair encoding, Token Awareness

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，语言模型的符号推理能力受到分词结构的影响，特别是子词分词方法（如BPE）会阻碍符号计算，而原子对齐的分词格式能显著提升推理性能。

Motivation: 探讨分词（tokenization）在语言模型中的重要性，尤其是子词分词方法如何影响符号推理能力。

Method: 通过理论和实证研究，分析分词结构对推理的影响，并提出“Token Awareness”概念，评估不同分词格式下模型的推理表现。

Result: 研究发现，分词结构显著影响推理性能，原子对齐的分词格式能让小模型在大规模符号推理任务中超越更大模型。

Conclusion: 语言模型的符号推理能力不仅取决于架构，还受到分词表示方式的深刻影响。

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [207] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
*Tong Bao,Heng Zhang,Chengzhi Zhang*

Key words: 科学论文摘要，结构功能识别，两阶段框架，Longformer

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出基于结构功能识别的两阶段摘要生成框架，解决科学论文结构化信息捕捉不足及跨学科适应性差的问题。

Motivation: 现有摘要生成方法未能充分捕捉科学论文的结构化信息，且跨学科适应性不足。

Method: 两阶段框架，先用分类器识别论文结构功能（如背景、方法、结果），再用Longformer生成基于上下文的摘要。

Result: 在领域特异性数据集上表现优于基线，生成更全面的摘要。

Conclusion: 提出的方法提升了科学论文摘要的全面性和跨学科适应性。

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [208] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
*Yunlong Liang,Fandong Meng,Jiaan Wang,Jie Zhou*

Key words: 俚语翻译，SlangDIT，SlangOWL，多任务学习，大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一个名为SlangDIT的任务，结合俚语检测、跨语言俚语解释和俚语翻译，通过SlangOWL模型实现了更准确的俚语翻译。

Motivation: 俚语翻译的挑战在于其语境依赖性，目前的俚语检测、解释和翻译任务多为孤立研究，缺乏相互关联的基准数据集。

Method: 作者构建了包含2.5万英中句子对的SlangDIT数据集，并提出SlangOWL模型，通过深度思考依次完成俚语检测、多义性判断、解释生成和翻译。

Result: 实验证明，SlangOWL在LLMs（如Qwen2.5和LLama-3.1）上显著优于基线模型和未经思考的监督微调模型。

Conclusion: SlangDIT任务和SlangOWL模型为俚语翻译提供了有效解决方案，强调了多任务协同的重要性。

Abstract: The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.

</details>


### [209] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
*Guosheng Liang,Longguang Zhong,Ziyi Yang,Xiaojun Quan*

Key words: 大推理模型、思维链、动态切换、计算效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ThinkSwitcher框架通过动态切换长短推理模式，降低大推理模型在简单任务上的计算开销。

Motivation: 大推理模型在简单任务上容易过度思考，导致不必要的计算开销。

Method: 提出ThinkSwitcher框架，通过轻量级切换模块动态选择推理模式。

Result: 在多个基准测试中减少20-30%计算成本，同时保持高精度。

Conclusion: ThinkSwitcher是一种高效且可扩展的统一部署解决方案。

Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.

</details>


### [210] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
*Tuc Nguyen,Yifan Hu,Thai Le*

Key words: 大语言模型, 作者隐私, 作者混淆, 作者模仿, 作者验证

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了首个统一框架，用于分析大语言模型（LLMs）中作者隐私相关的三个任务（AO、AM、AV）的动态关系，并探讨了人口统计元数据的影响。

Motivation: 研究动机是解决LLMs中作者隐私泄露的问题，尤其是通过显式和隐式信号泄露身份信息，同时填补AO、AM和AV任务之间互动关系的研究空白。

Method: 通过开发统一框架，量化LLM支持的AO、AM和AV任务在作者隐私中的动态关系，并分析人口统计元数据的影响。

Result: 研究揭示了这些任务如何相互作用以转换人类创作的文本，并展示了单次和迭代时间下的效果，同时探讨了元数据对隐私风险的调节作用。

Conclusion: 论文为LLM时代的作者隐私研究提供了新视角，强调了任务互动研究的重要性，并为未来工作提供了开源工具。

Abstract: Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.

</details>


### [211] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
*Sizhe Yuen,Ting Su,Ziyang Wang,Yali Du,Adam J. Sobey*

Key words: 问答系统、大型语言模型、自动生成QA对、微调、知识密集型任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种通过自动生成基于上下文的QA对来增强大型语言模型（LLM）在知识密集型问答任务中的表现的方法，减少对人类标注的依赖。

Motivation: 当前问答系统在处理需要复杂推理或实时知识整合的查询时存在困难，尤其是检索增强生成（RAG）方法在逻辑连接和多源信息处理方面仍有挑战。

Method: 利用LLM自动生成QA对作为微调数据，结合自动化QA生成器和模型微调器，通过困惑度、ROUGE、BLEU和BERTScore进行评估。

Result: 实验表明，该方法在逻辑连贯性和事实准确性上有显著提升，且Mistral-7b-v0.3在生成QA对上的表现优于人工标注的QA对。

Conclusion: 该方法为开发适应性更强的人工智能系统提供了潜在方向。

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [212] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
*Darpan Aswal,Siddharth D Jaiswal*

Key words: 大语言模型, 多模态, 代码混合, 语音扰动, 安全性对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究提出了一种利用代码混合和语音扰动的新策略，成功破解了多语言多模态大语言模型（LLMs）的安全性过滤，并展示了高攻击成功率和相关性。

Motivation: 现有的大语言模型在英文环境下通过固定模板攻击进行评估，但在多语言和多模态环境下仍易受攻击。研究旨在探索更广泛的安全对齐方法。

Method: 利用代码混合和语音扰动技术，设计新的破解策略，并对文本和图像生成任务进行攻击测试。

Result: 新策略在文本生成任务中实现了99%的攻击成功率，图像生成任务中78%，攻击相关性率分别为100%和95%。

Conclusion: 研究揭示了语音扰动对词元化的影响，并呼吁加强对多语言多模态模型的通用安全性对齐研究。

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [213] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
*Hakaze Cho,Peng Luo,Mariko Kato,Rin Kaenbyou,Naoya Inoue*

Key words: 上下文学习,注意力行为微调,语言模型,机理可解释性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为注意力行为微调（ABFT）的新方法，通过优化注意力分数而非最终输出，以低成本提升语言模型的性能。

Motivation: 为了解决上下文学习（ICL）与预训练之间的差距，并减少传统微调方法的高计算成本。

Method: 提出ABFT方法，基于ICL内部机制的研究，通过调整注意力分数来增强正确标签的关注并减少错误标签的影响。

Result: 在9种现代语言模型和8个数据集上的实验表明，ABFT在性能、鲁棒性、无偏性和效率上优于传统方法，且数据成本仅为0.01%。

Conclusion: ABFT展示了通过控制语言模型内部特定模块来改善其行为的可能性，为未来机理可解释性应用开辟了新途径。

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [214] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
*Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Praneeth Vepakomma*

Key words: 大语言模型,参数高效微调,LoRA,ABBA,低秩分解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种新的参数高效微调方法ABBA，通过解耦更新与预训练权重，显著提高了表达能力，并在多个任务上优于现有方法。

Motivation: 尽管大语言模型在多种任务上表现优异，但高效适应新领域仍具挑战性。

Method: ABBA通过将更新分解为两个独立可学习的低秩矩阵的Hadamard积，解耦了更新与预训练权重。

Result: ABBA在算术和常识推理基准测试中表现优异，显著优于现有方法。

Conclusion: ABBA提供了一种高效且表达力强的微调策略，为模型适应新领域提供了新思路。

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [215] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
*Ziang Wang,Amir Aryani*

Key words: 自然语言处理，儿童言语障碍，主题建模，LDA，BERTopic，文献分类

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该技术报告提出了一种基于自然语言处理（NLP）的方法，用于系统分类关于儿童言语障碍的科学文献。通过PubMed数据库检索和筛选了2015年后发表的4804篇相关文章，并应用LDA和BERTopic两种主题建模技术识别潜在主题结构，发现了14个有临床意义的主题集群。评估结果表明，LDA和BERTopic模型在主题连贯性和分类有效性方面表现良好。

Motivation: 为了自动化文献综述过程，特别是在言语病理学领域，研究人员需要一种系统化的方法来分类和分析大量科学文献，以发现潜在的主题结构和研究趋势。

Method: 从PubMed数据库检索2015年后发表的4804篇相关文章，使用领域特定的关键词进行过滤。对摘要进行清洗和预处理后，应用Latent Dirichlet Allocation (LDA)和BERTopic两种主题建模技术，并结合定制的停用词列表以提高分类的精确性。

Result: LDA模型获得了0.42的连贯性分数和-7.5的困惑度，显示出较强的主题连贯性和预测性能。BERTopic模型的异常主题比例低于20%，表明其能够有效分类异质性文献。最终识别出14个有临床意义的主题集群。

Conclusion: 该研究表明，基于NLP的主题建模技术可以有效自动化文献分类，为言语病理学领域的文献综述提供了可靠的基础。

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [216] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
*Haijun Li,Tianqi Shi,Zifu Shang,Yuxuan Han,Xueyu Zhao,Hao Wang,Yu Qian,Zhiqiang Qian,Linlong Xu,Minghao Wu,Chenyang Lyu,Longyue Wang,Gongbo Tang,Weihua Luo,Zhao Xu,Kaifu Zhang*

Key words: 机器翻译、领域适应、基准评估、电子商务、多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种针对工业机器翻译的三级能力评估框架，并发布了首个面向电子商务翻译的公开基准TransBench，填补了学术基准与实际需求之间的差距。

Motivation: 研究动机源于通用机器翻译模型在工业场景（如电子商务、金融、法律）中因缺乏领域术语和文化适应性而表现不佳，现有评估框架无法全面衡量专业场景下的翻译质量。

Method: 提出了三级翻译能力框架（语言能力、领域熟练度、文化适应），并开发了TransBench基准，涵盖17,000句专业翻译数据、33种语言对，结合传统指标（BLEU、TER）与领域专用模型Marco-MOS。

Result: TransBench成为首个公开的电子商务翻译基准，提供了结构化评估框架和开源工具，显著提升了工业机器翻译的系统评估能力。

Conclusion: 该工作弥补了机器翻译在工业场景中的评估缺陷，为研究者和从业者提供了系统化的标准和工具。

Abstract: Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.

</details>


### [217] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
*Shaolin Zhu,Tianyu Dong,Bo Li,Deyi Xiong*

Key words: FuxiMT, 机器翻译, 稀疏化大语言模型, Mixture-of-Experts, 课程学习, 零样本翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FuxiMT是一种基于稀疏化大语言模型的中文中心多语言机器翻译模型，采用两阶段训练策略，显著优于现有基线，尤其在低资源场景下表现出色，并具备零样本翻译能力。

Motivation: 解决多语言机器翻译中，尤其是低资源语言对的翻译问题，同时提升模型的零样本翻译能力。

Method: 采用两阶段策略：先在大规模中文语料上预训练，再在多语言平行数据集上微调，结合Mixture-of-Experts和课程学习策略。

Result: FuxiMT在65种语言的多语言翻译任务中显著优于基线模型，尤其在低资源场景下表现突出，并展示了优秀的零样本翻译能力。

Conclusion: FuxiMT通过稀疏化大语言模型和课程学习策略，在多语言机器翻译中表现出色，尤其适用于低资源语言对的翻译任务。

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [218] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Key words: SLOT, Test-time Optimization, Language Model, Sample-specific, Efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SLOT是一种参数高效的测试时推理方法，通过在测试时对轻量级样本特定参数向量进行优化，提升语言模型对复杂指令的响应能力。

Motivation: 现有的LLM在处理未充分表示的复杂指令时表现不佳，SLOT旨在通过样本特异性优化提升模型对单个提示的适应性。

Method: SLOT在测试时进行少量优化步骤，更新轻量级样本特定参数向量，并将其添加到最终隐藏层前，通过缓存最后一层特征实现高效适配。

Result: 实验显示，SLOT在多个基准测试和LLM中表现优异，例如Qwen2.5-7B在GSM8K上提升8.6%准确率，DeepSeek-R1-Distill-Llama-70B在GPQA上达到68.69%的SOTA准确率。

Conclusion: SLOT通过高效优化显著提升了LLM对复杂指令的处理能力，适用于多种模型和任务。

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [219] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
*Hui Huang,Yancheng He,Hongli Zhou,Rui Zhang,Wei Liu,Weixun Wang,Wenbo Su,Bo Zheng,Jiaheng Liu*

Key words: LLM-as-a-Judge, Think-J, 强化学习, 自动评价

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Think-J 是一种通过强化学习优化生成型LLM作为评判者的性能的方法，显著提升了LLM的评价能力，无需额外人工标注。

Motivation: 生成型LLM在多项任务中表现优异，但作为评判者时性能不足，亟需改进。

Method: 1. 使用少量标注数据开发初始评判思维模型；2. 基于离线和在线强化学习优化评判思维轨迹。

Result: Think-J 显著提升了生成型LLM的评价能力，超过了生成型和分类器型LLM评判者。

Conclusion: 通过强化学习优化评判思维是有效的改进LLM评判者的途径。

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [220] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
*Minh Ngoc Ta,Dong Cao Van,Duc-Anh Hoang,Minh Le-Anh,Truong Nguyen,My Anh Tran Nguyen,Yuxia Wang,Preslav Nakov,Sang Dinh*

Key words: 文本检测, 人-AI协作, 多任务学习, 对比学习, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了FAID，一个细粒度的检测框架，用于区分人类编写、AI生成及人-AI协作文本，并识别AI模型家族，提升了检测的泛化能力和可解释性。

Motivation: 随着人类与AI在生成任务中的合作增加，区分人类、AI以及协作生成的文本变得更具挑战性，缺乏有效的检测工具。

Method: 通过结合多级对比学习和多任务辅助分类，FAID捕捉细微的文本风格特征，并将AI模型家族视为独特的风格实体。

Result: 实验表明，FAID在未见过的领域和新AI模型上表现优于基线方法，提升了泛化准确性。

Conclusion: FAID为提升AI辅助写作的透明度和责任性提供了一种潜在解决方案。

Abstract: The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.

</details>


### [221] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Key words: 仇恨言论检测，跨语言迁移学习，数据增强，最近邻检索，低资源语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于最近邻检索的跨语言迁移学习方法，通过增强目标语言的少量标注数据，提升了仇恨言论检测的性能。

Motivation: 针对仇恨言论检测中低资源语言标注数据昂贵且耗时的问题，研究旨在开发一种高效且可扩展的跨语言迁移学习方法。

Method: 利用最近邻检索从多语言仇恨言论检测池中检索最相关的标注样本，增强目标语言的少量标注数据，并应用最大边际相关性以减少冗余。

Result: 在八种语言上的实验表明，该方法显著优于仅使用目标语言数据的模型，且多数情况下优于当前最优方法，具有高效和可扩展性。

Conclusion: 该方法在数据效率和可扩展性方面表现出色，适用于新语言和任务，为低资源语言的仇恨言论检测提供了有效解决方案。

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [222] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
*Jennifer D'Souza,Hamed Babaei Giglou,Quentin Münch*

Key words: 大型语言模型、科学问答、评估框架、强化学习、AI对齐

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: YESciEval是一个开源框架，结合细粒度评估和强化学习以优化LLM评估的乐观偏差。

Motivation: 提升LLM在科学问答中的评估鲁棒性，支持透明、可扩展的评估方法。

Method: 结合细粒度评估标准和强化学习，发布多学科科学问答数据集。

Result: 开发了独立于专有模型和人工反馈的可扩展、免费评估框架。

Conclusion: 通过优化LLM评估模型，推动AI对齐和科学研究的透明评估。

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [223] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
*Rao Ma,Mengjie Qian,Vyas Raina,Mark Gales,Kate Knill*

Key words: 语音LLMs, 对抗攻击, 鲁棒性, 声学攻击, Qwen2-Audio, Granite-Speech

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，预训练语音编码器与大语言模型结合的语音LLMs存在普遍声学对抗攻击漏洞，需加强鲁棒性训练。

Motivation: 探讨语音LLMs在对抗攻击下的脆弱性及其潜在影响。

Method: 通过预加固定对抗音频段，研究其对模型输出的影响，并扩展到选择性激活攻击。

Result: 发现Qwen2-Audio和Granite-Speech等模型易受攻击，暴露了其安全隐患。

Conclusion: 语音LLMs需要更鲁棒的训练策略以抵御对抗攻击。

Abstract: The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.

</details>


### [224] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
*Jungseob Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Key words: 大型语言模型, 跨语言迁移, 监督微调, 低资源语言, 英语性能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为跨语言优化（CLO）的方法，用于在数据受限环境中将英语为中心的大型语言模型高效迁移到其他语言，同时保持其英语能力。CLO通过公开的英语监督微调数据和翻译模型实现跨语言迁移，实验表明CLO在目标语言和英语性能上均优于传统监督微调（SFT）。

Motivation: 为了解决监督微调在跨语言迁移中过度依赖英语性能的问题，尤其是在数据受限的环境中，作者提出了CLO方法。

Method: 使用公开的英语监督微调数据和翻译模型，实现跨语言迁移。通过实验验证CLO在多种语言和资源条件下的效果。

Result: CLO在目标语言和英语性能上均优于SFT，尤其在低资源语言中表现更优，仅用3200样本即可超越SFT的6400样本效果。

Conclusion: CLO是一种高效、稳健的跨语言迁移方法，能够在数据受限环境中显著提升目标语言性能，同时保持英语能力。

Abstract: Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.

</details>


### [225] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
*Jinwang Song,Hongying Zan,Kunli Zhang,Lingling Mu,Yingjie Han,Haobo Hua,Min Peng*

Key words: Text-to-SQL, LLMs, 监督微调, 模式链接, SQL生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: JOLT-SQL通过单一阶段的监督微调框架，联合优化模式链接和SQL生成，在噪声模式下表现出色，效率和准确性均提升。

Motivation: 解决现有监督微调方法在文本到SQL任务中的复杂多阶段流程和对噪声模式信息鲁棒性差的问题。

Method: 采用判别式模式链接和局部双向注意力，结合噪声模式采样策略。

Result: 在Spider和BIRD基准测试中，实现了开源模型中最佳的执行准确率和效率。

Conclusion: JOLT-SQL展示了单一阶段框架在高噪声环境下的有效性和效率优势。

Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.

</details>


### [226] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
*Ehsan Doostmohammadi,Marco Kuhlmann*

Key words: 检索增强语言模型,查询上下文重叠,合成上下文,数据效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 检索增强的语言模型通过优化查询与上下文的匹配程度，显著提升性能并减少计算资源消耗。

Motivation: 探究查询与检索上下文之间的最佳重叠程度对模型性能的影响。

Method: 系统研究不同重叠程度对训练和推理的影响，并通过生成合成上下文（如查询重述）增加重叠。

Result: 当重叠超过临界值时，模型测试困惑度显著降低，学习速度加快，训练时间减少40%。

Conclusion: 增加查询与上下文重叠是优化检索机制的有效策略，具有实际应用价值。

Abstract: Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [227] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
*Shamsuddeen Hassan Muhammad,Ibrahim Said Ahmad,Idris Abdulmumin,Falalu Ibrahim Lawan,Babangida Sani,Sukairaj Hafiz Imam,Yusuf Aliyu,Sani Abdullahi Sani,Ali Usman Umar,Kenneth Church,Vukosi Marivate*

Key words: 豪萨语, NLP, 低资源语言, HausaNLP, 研究方向

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文概述了豪萨语自然语言处理（NLP）的现状，提出了一个资源汇总目录HausaNLP，并探讨了豪萨语NLP的挑战和研究方向。

Motivation: 豪萨语作为一种资源匮乏语言虽使用者众多，但NLP研究不足，亟需系统性总结和资源整合。

Method: 通过分析现有资源、研究贡献和差距，并提出HausaNLP目录来聚合数据集和研究工具。

Result: 介绍了HausaNLP目录，并指出豪萨语NLP面临的挑战，如分词问题和方言差异。

Conclusion: 提出了针对数据集扩展、语言建模和社区合作的战略研究方向，以推动豪萨语NLP的发展。

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [228] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
*Leonardo Bertolazzi,Manuel Vargas Guzmán,Raffaella Bernardi,Maciej Malicki,Jakub Szymanik*

Key words: 大型语言模型, 元学习, 泛化能力, 推理任务, 微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为MIND的元学习微调方法，旨在提升小型语言模型在未见知识库上的泛化能力，使其在推理任务中表现优于大型模型。

Motivation: 大型语言模型（LLMs）在正式任务中表现优异，但在分布外问题上的泛化能力有限，研究目标是提升其对推理规则的系统性理解能力。

Method: 提出Meta-learning for In-context Deduction (MIND)，一种少样本元学习微调方法，专注于从知识库中识别推导假设所需的前提子集。

Result: MIND显著提升了1.5B到7B参数小型模型的泛化能力，尤其在低数据环境下表现突出。微调后的小型模型在任务中胜过GPT-4o等大型模型。

Conclusion: MIND方法有效提升了小型模型在推理任务中的系统性能力，显示出其在提升模型泛化性能方面的潜力。

Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [229] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
*Neelabh Sinha*

Key words: QA-prompting, 长文本摘要, 语言模型, ROUGE分数

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: QA-prompting方法通过问答步骤优化长文本摘要任务，避免位置偏见，显著提升了ROUGE分数。

Motivation: 语言模型在长文本摘要中因位置偏见表现不佳，现有方法如微调或流水线复杂且低效。

Method: 提出QA-prompting，通过问答步骤提取关键信息并丰富上下文，无需微调或流水线。

Result: 在多个数据集和模型上，QA-prompting的ROUGE分数提升高达29%。

Conclusion: QA-prompting为摘要任务提供高效、可扩展的解决方案，强调领域特定问题选择的重要性。

Abstract: Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.

</details>


### [230] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
*Jialong Han,Si Zhang,Ke Zhang*

Key words: 大语言模型,微调,低秩适应,奇异值分解

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出OSoRA方法，通过结合SVD和可学习缩放向量，高效微调大语言模型，显著减少计算资源需求。

Motivation: 大语言模型微调的计算成本高，现有PEFT方法仍需大量资源，需更高效的解决方案。

Method: 扩展LoRA，结合SVD和可学习缩放向量，优化输出维度向量并冻结奇异向量矩阵。

Result: OSoRA在多项基准测试中表现优异，计算资源需求显著降低。

Conclusion: OSoRA高效且性能优越，联合训练奇异值和输出维度向量是关键。

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [231] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
*Xin Li,Mengbing Liu,Li Wei,Jiancheng An,Mérouane Debbah,Chau Yuen*

Key words: 大语言模型, 无线通信, 数学推理, 基准测试, WirelessMathBench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了WirelessMathBench，一个评估大语言模型在无线通信领域数学推理能力的基准测试，发现现有模型在复杂任务中表现不佳。

Motivation: 探索大语言模型在无线通信领域复杂数学推理能力的不足，推动领域专用模型的开发。

Method: 基于587个来自顶尖研究论文的问题构建基准，涵盖从基础到复杂的数学任务。

Result: 现有模型在基础任务上表现良好，但在复杂方程重建上成绩不佳，最佳模型平均准确率仅38.05%。

Conclusion: WirelessMathBench的发布旨在促进更强大、领域感知的大语言模型发展。

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [232] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
*Jialong Han,Si Zhang,Ke Zhang*

Key words: Parameter-Efficient Fine-Tuning, LoRA, SVD, DuDe, knowledge transfer

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DuDe通过矩阵分解优化LoRA的初始化，提升训练稳定性和知识迁移效率，在多个任务中表现优异。

Motivation: 解决现有LoRA方法因随机初始化导致的训练不稳定和知识迁移效率低的问题。

Method: 采用SVD分解权重矩阵为幅度和方向分量，实现有原则的初始化。

Result: 在MMLU上达到48.35%准确率，GSM8K上达到62.53%（±1.59）。

Conclusion: DuDe在理论和实证上均表现出色，成为PEFT领域的重要贡献。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [233] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
*Maitreya Prafulla Chitale,Ketaki Mangesh Shetye,Harshit Gupta,Manav Chaudhary,Vasudeva Varma*

Key words: AutoRev, 自动评审, 图结构, NLP, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文介绍了AutoRev，一种基于图结构的自动学术论文评审系统，通过提取关键段落优化评审生成，性能优于现有基线58.72%。

Motivation: 现有基于大语言模型的方法在生成长文评审时忽略了计算和性能限制，因此需要一种更高效的方法。

Method: 提出AutoRev，将学术文档表示为图结构，提取关键段落以生成评审。

Result: 在评审生成任务中，AutoRev平均优于基线方法58.72%。

Conclusion: 图结构提取技术对NLP下游任务有潜力，未来将公开代码。

Abstract: Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.

</details>


### [234] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
*Nadir Durrani,Basel Mousi,Fahim Dalvi*

Key words: 多语言知识编辑, 模型编辑, 跨语言传播, 语言各向异性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文综述了多语言知识编辑（MKE）的研究，系统化整理了相关方法、挑战和未来方向。

Motivation: 研究多语言知识编辑是为了确保事实编辑在不同语言中的可靠性扩展，填补了该领域的研究空白。

Method: 论文提出了MKE方法的分类，包括参数、记忆、微调和超网络等方法，并总结了现有基准和关键发现。

Result: 研究发现跨语言传播存在挑战，如语言各向异性、评估覆盖范围和编辑可扩展性问题。

Conclusion: 本文为可编辑的语言感知大语言模型的未来发展奠定了基础。

Abstract: While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [235] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
*Seyoung Song,Seogyeong Jeong,Eunsu Kim,Jiho Jin,Dongkwan Kim,Jay Shin,Alice Oh*

Key words: 大型语言模型、多语言评估、低资源语言、对话任务、MUG-Eval

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出MUG-Eval框架，通过将现有基准转化为对话任务来评估大型语言模型的多语言生成能力，适用于低资源语言，且不依赖语言特定工具或LLM作为评判者。

Motivation: 评估大型语言模型在低资源语言中的文本生成能力存在挑战，直接评估方法稀缺。

Method: 将现有基准转化为对话任务，以任务成功率衡量生成能力，避免依赖语言特定工具或LLM评判者。

Result: 在30种语言中评估8个模型，MUG-Eval与标准基准强相关（r>0.75），支持跨语言和模型的标准化比较。

Conclusion: MUG-Eval为多语言生成评估提供了高效且可扩展的解决方案。

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [236] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
*Peter Baile Chen,Yi Zhang,Dan Roth,Samuel Madden,Jacob Andreas,Michael Cafarella*

Key words: 大型语言模型、日志增强生成、键值缓存、任务学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于日志增强生成（LAG）的框架，通过直接复用过去的计算和推理结果，提升模型在新任务中的表现，同时保持系统的高效性和可扩展性。

Motivation: 解决大型语言模型（LLMs）及其代理版本难以从过去任务中学习和复用推理的局限性。

Method: 使用键值（KV）缓存表示任务日志，存储部分关键标记的推理上下文，并在新任务时从相关日志中检索KV值以增强生成。

Result: 在知识和推理密集型数据集上的实验表明，LAG显著优于未使用日志的标准代理系统以及基于反思和KV缓存技术的现有方法。

Conclusion: LAG通过直接复用过去的推理和计算，有效提升了模型的学习能力和任务表现，同时保持了系统效率。

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [237] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
*Haoming Huang,Yibo Yan,Jiahao Huo,Xin Zou,Xinfeng Li,Kun Wang,Xuming Hu*

Key words: 关键词

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 总结

Motivation: 动机

Method: 方法

Result: 结果

Conclusion: 结论

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [238] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
*Pengzhou Cheng,Haowen Hu,Zheng Wu,Zongru Wu,Tianjie Ju,Daizong Ding,Zhuosheng Zhang,Gongshen Liu*

Key words: GUI代理, MLLM, 后门攻击, AgentGhost, 供应链安全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文揭示了多模态大语言模型（MLLM）驱动的GUI代理容易受到供应链后门攻击的威胁，并提出了一种名为AgentGhost的攻击框架，能够有效且隐蔽地注入后门。通过实验验证，AgentGhost在攻击准确率和隐蔽性上表现优异，并提出了防御方法。

Motivation: 由于MLLM驱动的GUI代理在开源或API提供时存在供应链后门攻击的风险，当前研究对此关注不足。因此，作者旨在揭示这一威胁并设计一种有效的攻击框架。

Method: 作者提出AgentGhost框架，通过结合目标和交互级别的触发器，将后门注入建模为Min-Max优化问题，采用监督对比学习和微调方法增强后门的灵活性和隐蔽性。

Result: 实验表明，AgentGhost在攻击准确率上达到99.7%，隐蔽性仅导致1%的效用下降。防御方法的引入可将攻击准确率降至22.1%。

Conclusion: 该研究揭示了GUI代理的后门攻击风险，AgentGhost展示了攻击的有效性和隐蔽性，同时提出的防御方法为其提供了缓解途径。

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [239] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
*Huopu Zhang,Yanguang Liu,Mengnan Du*

Key words: 盈利惊喜、财报电话会议、稀疏自编码器、财务预测

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为SAE-FiRE的框架，通过稀疏自编码器分析财报电话会议记录，克服冗余和行业术语的挑战，有效预测盈利惊喜。

Motivation: 财报电话会议记录是公司高管、分析师和股东之间重要的沟通渠道，但因其冗长和专业术语的存在，对语言模型提出了挑战。

Method: 使用稀疏自编码器（SAEs）提取关键信息并消除冗余，专注于捕捉具有预测能力的财务信号。

Result: 实验表明，SAE-FiRE方法显著优于基线模型。

Conclusion: SAE-FiRE框架能够有效提升财务预测的准确性。

Abstract: Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.

</details>


### [240] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
*Ona de Gibert,Joseph Attieh,Teemu Vahtola,Mikko Aulamo,Zihao Li,Raúl Vázquez,Tiancheng Hu,Jörg Tiedemann*

Key words: LLM, 合成数据, 机器翻译, 低资源语言, SynOPUS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了LLM生成合成数据在低资源机器翻译（MT）中的潜力，证明了其高质量和实用性，并提出了公共数据集SynOPUS。

Motivation: 探索LLM生成的合成数据是否能有效提升低资源机器翻译的性能。

Method: 通过构造文档级合成语料库，扩展至147种语言对，并研究其训练机制、与HPLT数据的比较以及非英语中心MT的应用。

Result: 自动和人工评估证实合成数据的高质量，能显著提升低资源语言的MT性能。

Conclusion: LLM生成的合成数据即使存在噪声，也能显著改善低资源MT，SynOPUS为其提供了公共资源。

Abstract: We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.

</details>


### [241] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Key words: 大型语言模型、空间基础、指令泛化、合成指令、人类指令

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了指令调整的大型语言模型（LLM）在空间基础任务中从合成指令到人类指令的泛化挑战，发现模型在复杂任务上表现显著下降。

Motivation: 尽管指令调整的LLM在多任务中表现出色，但在将合成指令泛化为人类指令的空间基础任务中仍存在挑战。

Method: 通过在2.5D网格上使用合成指令对LLM进行微调，并在合成和人类指令的基准数据集上评估其性能。

Result: 模型在简单任务上泛化良好，但在复杂任务上性能显著下降。

Conclusion: 研究揭示了指令泛化的关键差距，为未来改进提供了方向。

Abstract: Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.

</details>


### [242] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Key words: 大语言模型, 参数知识转移, PostPKT, PrePKT, 神经不兼容性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了在大语言模型（LLMs）中实现参数知识转移（PKT）的研究挑战，提出了两种方法（PostPKT和PrePKT），并引入了一种名为LaTen的新解决方案。研究发现，不同规模的LLMs之间存在神经不兼容性，这是实现有效PKT的根本挑战。

Motivation: 探索如何通过参数在大语言模型（LLMs）之间实现知识转移，超越传统的基于符号语言的知识转移范式。

Method: 提出了两种方法：Post-Align PKT（PostPKT）和Pre-Align PKT（PrePKT），并开发了一种名为LaTen的解决方案，通过少量训练步骤对齐不同规模LLMs的参数空间。

Result: 实验表明，PostPKT和PrePKT在实现稳定转移方面均面临挑战，不同规模LLMs之间的神经不兼容性是主要障碍。

Conclusion: 研究揭示了LLMs参数结构的差异，为未来高效PKT研究提供了新方向。

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [243] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
*Mete Ismayilzada,Antonio Laverghetta Jr.,Simone A. Luchini,Reet Patel,Antoine Bosselut,Lonneke van der Plas,Roger Beaty*

Key words: Large Language Models, Creativity, Preference Optimization, MuCE, Novelty

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为Creative Preference Optimization (CrPO)的新方法，通过多维度创造力信号优化LLM生成内容的新颖性、多样性和惊喜感，同时保持高质量。

Motivation: 现有方法在提升LLM创造力时过于单一，无法全面解决创造力的多维度问题。

Method: 使用CrPO方法，结合MuCE数据集（包含20万人类生成回答和30多项创造力评估），对多种模型进行训练和评估。

Result: 模型在自动和人类评估中表现优于GPT-4o等基线，生成内容更具新颖性、多样性和惊喜感且质量高。

Conclusion: 通过偏好框架直接优化创造力是提升LLM创造力的有效方向，且不影响输出质量。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [244] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
*Chihan Huang,Hao Tang*

Key words: 扩散语言模型,自回归模型,强化学习,条件文本生成,分类器引导

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种动态可控的半自回归框架CtrlDiff，通过强化学习自适应确定生成块的大小，并引入了分类器引导的控制机制，显著提升了生成灵活性和计算效率。

Motivation: 尽管自回归模型在语言建模中占据主导地位，但其固定长度生成和缺乏灵活控制的局限性促使研究者探索新的混合范式，结合扩散模型的并行生成和自回归模型的优势。

Method: 提出CtrlDiff框架，利用强化学习动态确定生成块的大小，并设计分类器引导的控制机制，减少计算开销，支持高效的后验条件生成。

Result: 实验表明，CtrlDiff在混合扩散模型中表现优异，缩小了与最先进自回归模型的性能差距，并在多样化任务中实现高效条件文本生成。

Conclusion: CtrlDiff通过动态块划分和分类器控制机制，显著提升了扩散语言模型的灵活性和可控性，为条件文本生成提供了新思路。

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [245] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
*Xiaoyu Tian,Yunjie Ji,Haotian Wang,Shuaiting Chen,Sitong Zhao,Yiping Peng,Han Zhao,Xiangang Li*

Key words: 蒸馏, 语言模型, 推理能力, 数据集, AM-Thinking-v1

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了通过蒸馏方法提升开源语言模型推理能力的有效性，通过三款先进教师模型生成的数据集训练学生模型，并验证了AM-Thinking-v1数据集的优越性和适应性。

Motivation: 提升开源语言模型的推理能力，探索高质量推理数据的重要性。

Method: 收集三个教师模型的验证输出，构建并行数据集并训练学生模型，评估其在多个推理基准上的表现。

Result: 基于AM-Thinking-v1数据的学生模型在各项基准测试中表现最佳，并能根据任务难度自适应调整输出长度。

Conclusion: 高质量验证的推理数据对提升语言模型性能至关重要，相关数据集已公开以供进一步研究。

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The AM-based model consistently achieves the best performance (e.g., 84.3 on
AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and
demonstrates adaptive output behavior-producing longer responses for harder
tasks and shorter ones for simpler tasks. These findings highlight the value of
high-quality, verified reasoning traces. We release the AM-Thinking-v1 and
Qwen3-235B-A22B distilled datasets to support future research on open and
high-performing reasoning-oriented language models. The datasets are publicly
available on Hugging Face\footnote{Datasets are available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [246] [Void in Language Models](https://arxiv.org/abs/2505.14467)
*Mani Shemiranifar*

Key words: 语言模型, Transformer, 自适应计算, 层激活, 推理效率

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了基于Transformer的语言模型（LMs）在推理过程中是否所有层都被激活，并提出了一种无参数的自适应计算方法L2AC来检测未激活层（称为Void）。通过实验发现，跳过这些层可以提高模型性能。

Motivation: 探讨Transformer语言模型在推理过程中是否存在未激活的层，并验证跳过这些层是否能提高模型效率或性能。

Method: 使用L2自适应计算（LAC）方法监测激活层的L2范数变化，追踪Prompt Processing（PP）和Response Generation（RG）两个阶段的激活层。

Result: 在多个基准测试中，跳过Void层显著提升了模型性能（如Qwen2.5-7B-Instruct在MMLU任务中准确率从69.24升至71.29），同时减少了计算量。

Conclusion: 模型推理中并非所有层都同等重要，选择性跳过未激活层可以提升任务性能。

Abstract: Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.

</details>


### [247] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
*Somnath Banerjee,Pratyush Chatterjee,Shanu Kumar,Sayan Layek,Parag Agrawal,Rima Hazra,Animesh Mukherjee*

Key words: 大模型，安全性，混合代码，解释性方法

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大模型（LLMs）在处理混合代码输入输出时安全性问题显著增加，研究系统性分析了这一问题，并通过解释性方法剖析了内部机制。

Motivation: 探讨大模型在混合代码输入下生成不安全输出的敏感性增加问题。

Method: 采用解释性方法分析模型内部机制，区分普遍与文化特定的不安全查询。

Result: 揭示了混合代码输入导致模型不安全行为的驱动机制。

Conclusion: 混合代码输入显著增加大模型的安全风险，需进一步关注。

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [248] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
*Tong Li,Jiachuan Wang,Yongqi Zhang,Shuangyin Li,Lei Chen*

Key words: 引文分类, 自监督学习, 对比学习, 预训练语言模型, 关键词扰动

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为 Citss 的新框架，通过自监督对比学习和两种专门策略改进引文分类任务，克服了数据稀缺和上下文噪声的挑战，并在多个基准数据集上优于现有方法。

Motivation: 现有的引文分类方法直接微调预训练语言模型时面临数据稀缺、上下文噪声和虚假关键词关联的挑战，需要一个更有效的解决方案。

Method: Citss 框架采用了自监督对比学习，并结合句子级裁剪和关键词扰动两种策略来生成对比对，以提高模型的鲁棒性。

Result: 实验表明，Citss 在三个基准数据集上优于现有的最先进方法，适用于基于编码器和解码器的预训练模型。

Conclusion: Citss 提供了一种有效的引文分类方法，能够更好地处理数据稀缺和上下文噪声问题，并通过对比学习提高了性能。

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [249] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
*He Zhu,Junyou Su,Minxi Chen,Wen Wang,Yijie Deng,Guanhua Chen,Wenjia Zhang*

Key words: 视觉语言模型, 城市规划, 地图分析, 专业领域模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出PlanGPT-VL，首个专为城市规划地图设计的视觉语言模型，通过创新方法提升地图分析的准确性和效率。

Motivation: 现有视觉语言模型在城市规划地图分析中的表现不足，无法满足专业需求。

Method: 采用PlanAnno-V框架、Critical Point Thinking方法和综合训练方法。

Result: PlanGPT-VL在PlanBench-V基准测试中显著优于通用VLMs，且参数效率高。

Conclusion: PlanGPT-VL为城市规划提供可靠工具，高效实现专业领域任务。

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [250] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
*Agam Goyal,Xianyang Zhan,Yilun Chen,Koustuv Saha,Eshwar Chandrasekharan*

Key words: 大型语言模型, 内容审核, 模块化框架, 解释性AI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MoMoE框架通过模块化和跨社区方法，为内容审核提供可扩展且透明的解决方案，无需为每个社区单独训练模型。

Motivation: 现有内容审核方法存在模型不透明、需为每个社区单独训练的局限性，限制了实际应用。

Method: 提出MoMoE框架，包含分配、预测、聚合和解释四个操作模块，分别由社区专家和规范违反专家实例化。

Result: 在30个未见过的子社区中，MoMoE表现优异（Micro-F1分数0.72和0.67），并提供简洁可靠的解释。

Conclusion: MoMoE展示了无需社区单独微调即可实现可扩展、透明的审核，轻量级专家组合为未来可信AI治理提供方向。

Abstract: Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.

</details>


### [251] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
*Jun Cao,Jiyi Li,Ziwei Yang,Renjie Zhou*

Key words: 多模态方面情感分析, 大型语言模型, 小型语言模型, 双交叉注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种结合小型语言模型（SLMs）和大型语言模型（LLMs）的多模态方面情感分析框架LRSA，通过注入LLMs生成的解释并采用双交叉注意力机制，显著提升了性能。

Motivation: 现有基于SLMs的多模态情感分析方法因模型容量有限导致效果不佳，而LLMs虽表现优异但在细粒度任务中仍不足，需结合两者优势。

Method: 提出LRSA框架，将LLMs生成的理性注入SLMs，并采用双交叉注意力机制增强特征交互与融合。

Result: 在两个基线模型和三个基准测试中验证了方法的优越性，显示其普适性和广泛适用性。

Conclusion: LRSA有效结合SLMs的决策能力和LLMs的补充信息，显著提升了多模态方面情感分析的性能。

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [252] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
*Jiale Kang,Ziyin Yue,Qingyu Yin,Jiang Rui,Weile Li,Zening Lu,Zhouran Ji*

Key words: 多模态、RNN、RWKV7、轻量级架构、动态适应

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了现代RNN架构在多模态环境中的应用，提出了基于RWKV7架构的轻量级多模态框架ModRWKV，通过动态适应异构模态编码器实现多源信息融合，并在性能和计算效率之间找到最佳平衡。

Motivation: 当前大多数多模态研究基于具有二次复杂度Transformer架构的大语言模型（LLMs），而线性模型如RNNs由于推理成本低，但应用仅限于文本模态。本研究旨在探索现代RNN架构在多模态上下文中的潜力。

Method: 提出ModRWKV框架，基于RWKV7架构作为LLM主干，通过动态适应异构模态编码器实现多源信息融合，设计极轻量级多模态模块，并通过实验找到最佳配置。

Result: ModRWKV利用RWKV7 LLM的预训练权重进行初始化，显著加速多模态训练。实验表明，这种初始化对提升模型理解多模态信号的能力至关重要。

Conclusion: 现代RNN架构在多模态大语言模型（MLLMs）领域是Transformer的可行替代方案，系统探索确定了ModRWKV架构的最佳配置。

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [253] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
*Michael Sullivan*

Key words: LFLMs, GFoLDS, 数据效率, 逻辑形式, 语义知识

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种基于逻辑形式的语言模型（LFLMs），并证明其比传统的基于文本的语言模型更高效。通过原型GFoLDS的实验，展示了LFLMs能够利用内置的语义知识快速学习复杂模式，且在不同任务中表现优异。

Motivation: 探索一种更高效的语言模型，以减少数据依赖性并提升学习能力。

Method: 提出Graph-based Formal-Logical Distributional Semantics（GFoLDS），作为LFLMs的原型，并在实验中验证其性能。

Result: GFoLDS在少量数据下表现优于基于文本的Transformer模型，且性能可能随参数和数据量的增加而提升。

Conclusion: LFLMs在数据效率和扩展性方面具有潜力，适合实际应用。

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [254] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
*Zhipeng Yang,Junzhuo Li,Siyu Xia,Xuming Hu*

Key words: 大型语言模型, 内部思维链, 层次化执行, 模型透明性, 指令级控制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）具有内部思维链能力，能够按层分解和执行复合任务，并通过实验验证了这一现象。

Motivation: 探讨LLMs如何通过不同网络层次学习和执行复合任务的子任务，以增强模型透明度。

Method: 使用层间上下文屏蔽和新颖的跨任务修补方法验证子任务在不同深度的学习，并通过LogitLens解码隐藏状态观察执行模式。

Result: 在15个两步复合任务和真实TRACE基准测试中，均观察到一致的层次化执行模式，证实LLMs具备内部规划和执行子任务的能力。

Conclusion: 研究结果为LLMs的细粒度指令级激活控制提供了新途径，增强了模型行为的可解释性。

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [255] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
*Agam Goyal,Vedant Rathi,William Yeh,Yian Wang,Yuen Chen,Hari Sundaram*

Key words: 稀疏自编码器, 大语言模型, 毒性检测, 激活引导, 安全性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文利用稀疏自编码器（SAE）识别模型中的毒性相关方向，并通过激活引导减少毒性输出，揭示了毒性减少与语言流畅性之间的权衡。

Motivation: 当前大语言模型（LLM）在用户应用中普遍存在毒性输出问题，现有方法多为表面修复，易被绕过。

Method: 通过SAE识别毒性相关方向，分三种强度进行激活引导干预，评估其对毒性和语言能力的影响。

Result: 强干预下毒性减少最多20%，但流畅性可能下降；标准NLP任务表现稳定，证明知识未受影响。

Conclusion: SAE干预对LLM脱毒有潜力，但需注意特征分离和干预强度的平衡。

Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.

</details>


### [256] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Key words: KORGym, LLM, reasoning, evaluation, reinforcement learning

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了KORGym平台，用于全面评估大语言模型的推理能力，包含多种游戏和交互式评估，实验显示闭源模型表现更优。

Motivation: 现有评估方法多为领域特定，无法全面衡量大语言模型的通用推理能力。

Method: 开发KORGym平台，包含50多种游戏，支持多轮交互评估和强化学习场景。

Result: 实验分析了19种LLM和8种VLM，发现闭源模型表现更优，并研究了模态、推理策略等因素对性能的影响。

Conclusion: KORGym有望成为评估复杂交互环境下大语言模型推理能力的重要资源。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [257] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
*Abhimanyu Talwar,Julien Laasri*

Key words: 枢纽语言, 尼泊尔语到英语翻译, 印地语, 转移方法, 反向翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出使用印地语作为枢纽语言解决尼泊尔语到英语翻译中缺乏大规模平行语料库的问题，介绍了枢纽语言的选择标准，并比较了两种方法（全监督的转移方法和半监督的反向翻译）的效果。

Motivation: 解决尼泊尔语和英语之间缺乏大规模、多样化领域的平行语料库问题，提议使用印地语作为枢纽语言。

Method: 使用印地语作为枢纽语言，采用全监督的转移方法和半监督的反向翻译方法进行尼泊尔语到英语的翻译。

Result: 全监督方法的SacreBLEU分数为14.2，比基线提高了6.6分；半监督方法略低于基线分数15.1，但分析了潜在原因。

Conclusion: 印地语作为枢纽语言有效，全监督方法表现优异，半监督方法有待改进，未来工作可进一步优化。

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [258] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
*Sohaila Eltanbouly,Salam Albatarni,Tamer Elsayed*

Key words: 自动论文评分、特征评分、LLM、TRATES、回归模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了TRATES框架，利用LLM生成特征，结合传统回归模型，在多特征评分中取得最优效果。

Motivation: 现有自动论文评分(AES)研究多关注整体评分，缺乏对个体特征的评估需求。

Method: TRATES框架通过LLM和评分标准生成特征，结合通用特征训练回归模型。

Result: 在广泛使用的数据集上，TRATES在所有特征评分中表现最优。

Conclusion: TRATES解决了多特征评分问题，LLM生成的特征贡献显著。

Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.

</details>


### [259] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
*Shangziqi Zhao,Jiahao Yuan,Guisong Yang,Usman Naseem*

Key words: 长链推理, 小语言模型, 剪枝, 逻辑图, 推理优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Prune-on-Logic框架通过选择性剪枝长链推理中的低效步骤，提高了小语言模型的推理效率和准确性。

Motivation: 研究长链推理（Long-CoT）在小语言模型（SLM）中的压缩问题，探讨剪枝是否能够优化推理能力。

Method: 提出Prune-on-Logic框架，将长链推理转化为逻辑图，并在自验证约束下剪枝低效的推理步骤。

Result: 剪枝验证步骤能显著提升准确性并降低推理成本，优于其他剪枝策略和未压缩的微调方法。

Conclusion: 剪枝是一种结构优化策略，可帮助小语言模型更高效地利用长链推理。

Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [260] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
*Wenbin Hu,Haoran Li,Huihao Jing,Qi Hu,Ziqian Zeng,Sirui Han,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Key words: 大型语言模型, 安全与隐私, 上下文完整性, 强化学习, 合规性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种基于上下文完整性理论的强化学习方法，在提升大型语言模型（LLMs）的安全性和隐私合规性的同时，保留了上下文推理能力。

Motivation: 现有的LLMs安全与隐私保护方法依赖敏感模式匹配，未能保留上下文推理能力，且忽视了法律法规的合规性要求。

Method: 采用强化学习（RL）结合规则奖励机制，将安全与隐私问题转化为符合上下文完整性理论的合规性问题，并遵循GDPR、EU AI Act和HIPAA标准。

Result: 实验表明，该方法显著提升了法律合规性（安全/隐私基准测试精度提升17.64%），同时进一步增强了通用推理能力（MMLU和LegalBench测试分别提升2.05%和8.98%）。

Conclusion: 提出的方法不仅在合规性上表现优异，还能提升模型的通用推理能力，为LLMs的安全与隐私保护提供了有效解决方案。

Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.

</details>


### [261] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
*Huihao Jing,Haoran Li,Wenbin Hu,Qi Hu,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Key words: Model Context Protocol (MCP), MAESTRO, safety risks, LLM, benchmark

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种新框架MCIP，以增强MCP的安全性，并通过MAESTRO框架分析了MCP的安全缺失，开发了分类法和基准数据集，显著提升了LLM的安全性能。

Motivation: MCP的分散架构带来了未充分探索的安全风险，需要系统性分析及改进。

Method: 基于MAESTRO框架分析MCP安全缺失，提出MCIP协议，开发分类法和训练数据，并进行实验验证。

Result: 实验验证了LLM在MCP交互中的脆弱性，并显示MCIP显著提升了其安全性能。

Conclusion: MCIP框架和分类法有效改善了MCP的安全性，并为LLM的安全提升提供了新途径。

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps.Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [262] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
*Xianzhen Luo,Qingfu Zhu,Zhiming Zhang,Mingzheng Xu,Tianhao Cheng,Yixuan Wang,Zheng Chu,Shijie Xuyang,Zhiyuan Ma,YuanTao Fan,Wanxiang Che*

Key words: 代码敏感性,CTF-Code,CTF-Instruct,增量微调,LLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出代码敏感性概念，介绍CTF-Code基准和CTF-Instruct微调框架，提升代码LLM对问题描述细节变化的识别能力。

Motivation: 当前代码基准和指令数据忽略代码敏感性，需填补这一空白以提升模型性能。

Method: 采用反事实扰动构建CTF-Code基准，并提出CTF-Instruct增量微调框架，结合难度、多样性和敏感性三维度。

Result: 实验显示，CTF-Instruct微调后的LLM在CTF-Code上性能提升2%，在LiveCodeBench上提升10%。

Conclusion: 增强代码敏感性可显著提升LLM性能，验证了CTF-Instruct框架的可行性。

Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.

</details>


### [263] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
*Guangzhi Xiong,Eric Xie,Corey Williams,Myles Kim,Amir Hassan Shariatmadari,Sikun Guo,Stefan Bekiranov,Aidong Zhang*

Key words: 大型语言模型,生物医学,假设生成,幻觉检测,真实性评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在生物医学等领域展现出生成假设的潜力，但存在真实性和幻觉问题。论文提出TruthHypo基准和KnowHD幻觉检测器，用于评估LLMs生成假设的真实性。

Motivation: 尽管LLMs在生物医学假设生成中表现出潜力，但其生成的假设真实性难以评估，且存在幻觉问题。这促使研究者开发工具来系统评估和过滤不真实的假设。

Method: 提出TruthHypo基准用于评估LLMs生成假设的能力，并开发KnowHD知识基础的幻觉检测器，通过基于知识的评分过滤真实假设。

Result: 研究表明LLMs难以生成真实假设，而KnowHD的评分能有效筛选真实假设。人类评估也证实KnowHD的实用性。

Conclusion: TruthHypo和KnowHD为评估LLMs生成假设的真实性提供了系统工具，有助于加速科学发现。

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [264] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
*Soumadeep Saha,Akshay Chaturvedi,Joy Mahapatra,Utpal Garain*

Key words: 大语言模型, 访问控制, 安全性, 对齐, sudoLLM

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍了sudoLLM框架，通过用户授权访问控制提升大语言模型的多角色对齐能力，增强安全性和抗攻击能力。

Motivation: 当前大语言模型缺乏用户授权访问控制功能，导致敏感信息可能被未授权用户获取。sudoLLM旨在解决这一问题。

Method: 通过在查询中注入用户偏置信号，并训练模型根据用户权限选择性生成内容。

Result: 实验显示，该方法显著提升了模型的对齐性、泛化能力和抗提示注入攻击能力。

Conclusion: sudoLLM作为一种额外安全层，配合现有机制可增强大语言模型的端到端安全性。

Abstract: User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [265] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
*Rafael Rivera Soto,Barry Chen,Nicholas Andrews*

Key words: 机器学习, 文本检测, 风格特征, 对抗优化, 改写攻击

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了机器学习文本检测的挑战性，提出了基于风格特征的检测方法，并验证了其鲁棒性，同时对新的改写攻击进行了分析。

Motivation: 探讨机器学习文本检测的固有困难，并测试现有探测器在对抗优化下的表现。

Method: 利用风格特征空间进行检测，并开发了一种新的改写攻击方法。引入了AURA指标评估检测性能。

Result: 风格特征检测方法对抗优化表现良好，但改写攻击在单样本下有效，多样本下仍可区分。

Conclusion: 研究支持避免依赖机器学习文本检测的建议。

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [266] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Key words: 大型语言模型, 霍桑效应, 安全对齐, 白盒探测, 测试意识

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在感知到被评估时会改变行为的现象（类似霍桑效应），并提出了一种白盒探测框架来量化和管理这种“测试意识”对模型安全对齐的影响。

Motivation: 探讨LLMs在评估时改变行为的现象及其对安全性的影响，以提升安全评估的可靠性。

Method: 引入白盒探测框架，包括线性识别与测试意识相关的激活以及控制模型的测试意识，同时监控下游性能。

Result: 测试意识显著影响模型的安全对齐，且不同模型表现不同。

Conclusion: 通过精细控制测试意识可增强安全评估的可信度。

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [267] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
*Lingjie Jiang,Xun Wu,Shaohan Huang,Qingxiu Dong,Zewen Chi,Li Dong,Xingxing Zhang,Tengchao Lv,Lei Cui,Furu Wei*

Key words: LHRM, 混合推理, 强化学习, 效率优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种新型模型LHRM，能自适应决定是否进行复杂思考，以提高效率。

Motivation: 解决现有大型推理模型（LRM）在处理简单查询时因过度思考导致的资源浪费问题。

Method: 采用两阶段训练流程，包括混合微调（HFT）和在线强化学习（HGPO），并引入混合准确性指标评估模型能力。

Result: LHRM在推理和通用能力上优于现有LRM和LLM，同时显著提升效率。

Conclusion: 研究呼吁重新审视复杂思考的使用场景，为混合思考系统提供了基础。

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [268] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
*Yu Ying Chiu,Zhilin Wang,Sharan Maiya,Yejin Choi,Kyle Fish,Sydney Levine,Evan Hubinger*

Key words: AI风险检测, 价值观评估, LitmusValues, AIRiskDilemmas, AI安全

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过识别AI模型中的价值观（如LitmusValues）来预测潜在风险行为。

Motivation: 随着AI模型的强大，检测其风险行为变得更困难，特别是如Alignment Faking等方法。通过识别AI模型的价值观，可以作为早期预警系统。

Method: 创建LitmusValues评估管道，收集AIRiskDilemmas困境数据集，测量AI模型的价值优先级。

Result: LitmusValues中的价值观（如Care）能预测已知和未知风险行为。

Conclusion: 识别AI模型的价值观是预测其潜在风险行为的有效方法。

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [269] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
*Xueguang Ma,Qian Liu,Dongfu Jiang,Ge Zhang,Zejun Ma,Wenhu Chen*

Key words: 强化学习、大语言模型、通用推理、数据集构建、生成模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为General-Reasoner的新训练范式，旨在提升大语言模型在多样化领域中的推理能力，通过构建大规模高质量数据集和基于生成模型的答案验证器，显著优于现有方法。

Motivation: 当前LLM推理主要集中在数学和编程领域，限制了其通用性和适用性。为解决这一问题，论文提出了针对多样化领域的推理能力增强方法。

Method: 1. 构建大规模多领域数据集；2. 开发基于生成模型的答案验证器，取代传统规则验证。

Result: 在12个多样化基准测试中，General-Reasoner表现优于现有基线方法，并在数学推理任务中保持高效。

Conclusion: General-Reasoner通过多领域训练和生成式验证，显著提升了LLM的通用推理能力。

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [270] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
*Ronald Seoh,Dan Goldwasser*

Key words: 视觉情感分类,上下文学习,LVLMs,EmoGist

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EmoGist是一个无需训练、基于上下文学习的视觉情感分类方法，通过上下文定义情感标签提高预测准确性。

Motivation: 情感在图像中的表现高度依赖上下文且微妙，因此需要一种上下文相关的方法来提高情感分类的准确性。

Method: EmoGist预生成情感标签的多重解释，通过检索与测试图像最接近的解释，使用快速视觉语言模型进行分类。

Result: 在Memotion数据集中，EmoGist将微F1分数提高了13点；在FI数据集中，宏F1分数提高了8点。

Conclusion: EmoGist通过上下文相关的解释显著提升了视觉情感分类的性能。

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [271] [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
*Jiaxin Guo,Zewen Chi,Li Dong,Qingxiu Dong,Xun Wu,Shaohan Huang,Furu Wei*

Key words: 奖励模型, 测试时计算, 奖励推理模型, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了奖励推理模型（RRMs），通过强化学习框架自演化奖励推理能力，无需显式推理轨迹训练数据，显著提升奖励模型性能。

Motivation: 奖励模型在引导语言模型输出符合人类期望方面至关重要，但如何有效利用测试时计算提升性能仍具挑战。

Method: 引入奖励推理模型（RRMs），通过链式思维推理使用额外测试时计算；采用强化学习框架自演化推理能力。

Result: RRMs在多个领域的奖励建模基准测试中表现优异，并能自适应利用测试时计算提升奖励准确性。

Conclusion: RRMs通过推理和自适应计算有效提升了奖励模型的性能，具有广泛应用潜力。

Abstract: Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.

</details>


### [272] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Key words: 终身学习,模型编辑,大规模,线性代数,ULTRAEDIT

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ULTRAEDIT是一种新型的模型编辑方法，适用于大规模终身学习，通过轻量级线性代数操作实现快速编辑，优于现有方法。

Motivation: 解决现有模型编辑方法在终身学习场景中无法满足大规模、高效更新的问题。

Method: ULTRAEDIT采用无训练、无主题、无记忆的编辑方案，依赖于轻量级线性代数操作和终身归一化策略。

Result: ULTRAEDIT编辑速度比现有方法快7倍以上，内存占用更低，支持100万次编辑并保持高精度。

Conclusion: ULTRAEDIT在多种模型编辑场景中表现优异，适用于大规模终身学习。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [273] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
*Haolei Xu,Yuchen Yan,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Shengpei Jiang,Kaitao Song,Weiming Lu,Jun Xiao,Yueting Zhuang*

Key words: 大语言模型, Chain-of-Thought, 数学推理, 思维跳跃, CoT-Bridge

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出CoT Thought Leap Bridge任务，用于检测和补全Chain-of-Thought推理中的缺失步骤，以提升模型的数学推理能力。

Motivation: 现有数学CoT数据集中常因专家省略中间步骤导致Thought Leaps（思维跳跃），影响模型学习和泛化能力。

Method: 基于ScaleQM+数据集，训练CoT-Bridge模型自动检测和补全推理步骤。

Result: 在数学推理基准测试中，使用补全数据微调的模型性能提升达+5.87%，且在蒸馏数据和强化学习中也表现更优。

Conclusion: 补全推理步骤可提升模型的泛化能力，适用于多种优化技术。

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [274] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
*Nikhil Prakash,Natalie Shapira,Arnab Sen Sharma,Christoph Riedl,Yonatan Belinkov,Tamar Rott Shaham,David Bau,Atticus Geiger*

Key words: 语言模型,心智理论,信念追踪,回溯机制,因果中介

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了语言模型（LM）如何表示角色的信念，尤其是在这些信念与现实不符时，通过因果中介和抽象方法分析了Llama-3-70B-Instruct的表现。

Motivation: 理解语言模型的‘心智理论’（ToM）能力，尤其是在角色信念与现实不一致时的表现。

Method: 构建了一个简单故事数据集，角色分别改变两个对象的状态；通过因果中介和抽象分析，发现了‘回溯机制’。

Result: 模型通过‘回溯机制’结合角色-对象-状态三元组，并在提问时检索相关信息。引入可见性文本后，模型会生成可见性ID更新信念。

Conclusion: 研究揭示了LM的信念追踪机制，为反向工程ToM推理提供了基础。

Abstract: How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [275] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/abs/2505.13457)
*Nathan Faraj*

Key words: 机器学习, 学习率优化, 数据集规模, 累积学习常数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种优化机器学习学习率的新方法，揭示了学习率与数据集规模之间的比例关系，并提出了累积学习常数的概念。

Motivation: 研究动机在于理解数据集规模如何影响训练动态，从而优化学习率以提升训练效率和性能。

Method: 通过分析学习率与数据集规模的关系，提出了一种新的优化方法，并引入累积学习常数。

Result: 研究发现学习率与数据集规模存在比例关系，并成功提出了一种优化学习率的框架。

Conclusion: 这些发现为设计高级学习率调度方案提供了理论支持，有望在多种机器学习应用中提升性能。

Abstract: This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [276] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/abs/2505.13461)
*Junye Jiang,Yaan Zhou,Yuanhao Gong,Haoxuan Yuan,Shuanglong Liu*

Key words: FPGA, CNN, 硬件加速器, 并行计算, 能效

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文综述了FPGA加速器在CNN中的应用，总结了性能评估框架、优化策略及不同FPGA架构的比较，并探讨了未来挑战与机遇。

Motivation: 随着CNN复杂度的增加，计算需求急剧上升，需要高效的硬件加速器。FPGA因其可重构性、并行性和能效成为理想选择。

Method: 综述了现有的FPGA加速器研究，总结了性能评估框架，并探讨了并行计算、数据流优化和软硬件协同设计等关键优化策略。

Result: 比较了不同FPGA架构在延迟、吞吐量、计算效率、功耗和资源利用率方面的表现，验证了FPGA在CNN加速中的潜力。

Conclusion: FPGA在CNN加速中展示了显著优势，未来仍需继续创新以应对挑战并抓住机遇。

Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [277] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/abs/2505.13462)
*Thien Nguyen,William Guicquero*

Key words: 二进制神经网络 (BNN), 非线性量化, 知识蒸馏, 块剪枝, 轻量化模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为Generic Learned Thermometer (GLT)的编码技术，用于改进二进制神经网络 (BNN) 的输入数据表示，通过学习非线性量化阈值。并结合轻量级分组卷积和块剪枝技术，显著减小模型大小和计算复杂度。

Motivation: 现有BNN研究主要关注模型权重和激活，忽略了输入数据的优化。本文旨在通过改进输入数据表示和模型结构，提升BNN的实用性和效率。

Method: 提出GLT编码技术，学习非线性量化阈值；结合轻量级分组卷积和块剪枝技术，并使用知识蒸馏训练。

Result: GLT技术显著提升了BNN的准确率（STL-10和VWW数据集）；结合块剪枝后，模型大小降至1Mb以下，适用于常开传感器推理场景。

Conclusion: GLT和块剪枝技术的结合为BNN提供了高效、轻量化的解决方案，适合实际应用。

Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [278] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/abs/2505.13463)
*Paolo Guida,William L. Roberts*

Key words: 神经算子,多相流,计算流体动力学,液体-蒸汽界面,工业控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 神经算子能够快速预测多相流中的液体-蒸汽界面演化，为工业控制提供实时支持。

Motivation: 解决传统计算流体动力学模型在多相流问题中因复杂性和计算速度不足而无法快速预测的问题。

Method: 使用神经算子基于体积流体模拟数据进行训练。

Result: 预测结果表现出高精度，特别是在液体-蒸汽界面演化预测上。

Conclusion: 神经算子的时间尺度与多相流应用相匹配，适合用于需要快速响应的工业控制。

Abstract: Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [279] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/abs/2505.13471)
*George Bird*

Key words: 深度学习, 可视化, 轴对齐, 特权基, 激活函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的可视化工具，用于分析深度学习模型中嵌入数据的轴对齐特性，揭示了激活函数对称性破坏与表示对齐之间的因果联系。

Motivation: 当前深度学习模型表示数据的方式难以理解，缺乏有效的方法论。

Method: 开发了一种多功能可视化工具，评估嵌入数据在特权基向量定义的平面上的分布。

Result: 研究发现，嵌入表示倾向于与特权基对齐，且激活函数直接导致特权基的形成。

Conclusion: 该方法揭示了表示对齐神经元基的倾向性原因，并发现了多种网络中的“祖母神经元”。

Abstract: Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [280] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/abs/2505.13499)
*Kelvin Kan,Xingjian Li,Benjamin J. Zhang,Tuhin Sahai,Stanley Osher,Markos A. Katsoulakis*

Key words: Transformer, 最优控制理论, 训练优化, 架构设计, 参数效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过最优控制理论视角研究Transformer，提出一种改进训练和架构设计的框架，提升性能并提供理论保证，实验验证了其高效性。

Motivation: 通过最优控制理论为Transformer的训练和架构设计提供理论支持，减少传统试错方法的成本。

Method: 采用连续时间最优控制理论框架，设计为即插即用模式，便于集成到现有Transformer模型。

Result: 在文本生成、情感分析等任务中显著提升测试性能，参数效率更高，如在nanoGPT上测试损失减少46%，参数减少42%。

Conclusion: 该框架为Transformer的改进提供了理论基础，可实现更高效的模型设计。

Abstract: We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [281] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/abs/2505.13501)
*Zequn He,Celia Reina*

Key words: 数据驱动，热力学，扩散模型，统计物理，不确定性量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SPIEDiff是一个机器学习框架，旨在通过结合统计物理和扩散模型，解决耗散系统中长期宏观动力学和热力学数据驱动发现的挑战。

Motivation: 传统粒子模拟存在时间尺度限制、热力学势的非唯一性及不确定性量化需求等问题，急需新方法突破这些限制。

Method: 结合统计物理、条件扩散模型和epinets，构建统计物理启发的SPIEDiff框架。

Result: 在随机Arrhenius粒子过程中，SPIEDiff能精确揭示热力学和动力学，并基于短时数据预测长期行为，计算效率显著提升。

Conclusion: SPIEDiff为热力学模型的数据驱动发现提供了高效、可靠的途径。

Abstract: The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [282] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/abs/2505.13502)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang,Chengqi Zhang*

Key words: 联邦学习, 低秩适应, 基础模型, 数据隐私, 微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 综述探讨了如何将低秩适应（LoRA）融入联邦学习（FL）中以优化基础模型的微调（FedLoRA），重点关注分布式学习、异构性和效率三大挑战。

Motivation: 解决如何在保护数据隐私的同时，利用私人数据集高效微调基础模型的问题。

Method: 通过整合LoRA技术与联邦学习框架，降低可训练参数数量，提升效率。

Result: 综述了现有方法如何应对分布式学习、异构性和效率挑战，并对未来研究方向提出建议。

Conclusion: FedLoRA是一个有前景的方向，但仍需进一步研究以解决开放性问题。

Abstract: Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [283] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/abs/2505.13507)
*Haoyang Chen*

Key words: 开集域自适应, CLIP, 可学习提示, 梯度分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于CLIP的开集域自适应方法，通过可学习提示和梯度分析模块解决跨领域对齐和未知样本检测的挑战，显著优于基线方法。

Motivation: 当前开集域自适应方法未能充分利用模态间的语义关系且未知样本检测存在误差累积问题，需改进。

Method: 1) 基于领域差异指标的可学习文本提示动态调整CLIP文本编码器；2) 通过梯度分析模块量化域偏移，区分已知/未知样本。

Result: 在Office-Home数据集上表现优于CLIP基线和标准基线，梯度分析模块效果显著。

Conclusion: 提出的方法有效解决了跨领域对齐和未知样本检测问题，梯度分析是关键。

Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [284] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/abs/2505.13510)
*Conor Rowan,Alireza Doostan*

Key words: interpretability, scientific machine learning, symbolic regression, mechanism

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文讨论了神经网络在科学建模中的局限性，尤其是其缺乏可解释性，并提出了一种针对物理科学的操作性定义，强调机制理解而非数学稀疏性。

Motivation: 科学界对神经网络模型的可解释性提出质疑，认为其与传统科学模型的简单数学表达不同，无法融入科学知识体系。作者旨在探讨可解释性的定义及其在科学中的作用。

Method: 作者回顾了符号回归和方程发现的相关文献，提出了一个强调机制理解而非稀疏性的可解释性定义。

Result: 研究发现稀疏性并非可解释性的必要条件，提出了一个更适合科学机器学习（SciML）的可解释性定义。

Conclusion: 基于机制的哲学定义有助于聚焦数据驱动科学的核心挑战。

Abstract: Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [285] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li,Fanxu Meng,Muhan Zhang,Shiai Zhu,Shangguang Wang,Mengwei Xu*

Key words: LLM, LoRA, 模块化, 转移矩阵, 高效适应

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LoRASuite是一种模块化方法，通过利用现有LoRA权重和计算转移矩阵，以高效适应新版本LLM，显著减少计算资源和时间消耗。

Motivation: 随着LLM频繁更新，传统重新训练LoRA权重的方法成本高且不环保，因此需高效利用现有权重适应新模型。

Method: 计算转移矩阵，基于对齐和相似性指标分配层和注意力头，再通过小规模微调确保稳定性。

Result: LoRASuite在数学任务上表现优于全规模LoRA重训练，内存和时间消耗显著降低。

Conclusion: LoRASuite提供了一种高效、低成本的适应新LLM的方法。

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [286] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/abs/2505.13521)
*Gabor Petnehazi,Laith Al Shaggah,Jozsef Gall,Bernadett Aradi*

Key words: 零样本预测、时间序列、死亡率、预训练模型、CHRONOS

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了零样本时间序列预测在死亡率预测中的应用，比较了两种预训练模型与传统方法。结果显示零样本模型短期表现良好，但长期预测仍需领域适应。

Motivation: 探索零样本时间序列预测在死亡率预测中的潜力，避免任务特定微调的需求。

Method: 评估TimesFM和CHRONOS两种预训练模型，并与传统方法（如ARIMA和Lee-Carter模型）及随机森林模型对比，涵盖5、10和20年的预测期。

Result: CHRONOS在短期预测中优于传统方法，TimesFM表现不佳；微调CHRONOS显著提升长期预测精度，随机森林模型整体最佳。

Conclusion: 零样本预测具有潜力，但需谨慎选择模型并结合领域适应。

Abstract: This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [287] [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)
*Keqi Deng,Philip C. Woodland*

Key words: KV缓存，自注意力，推理效率，MTLA，超网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MTLA通过在时间维度压缩KV缓存，显著降低自注意力推理的内存占用，提升推理速度，同时保持性能。

Motivation: 解决Transformer自注意力中KV缓存随序列长度线性增长导致的推理效率瓶颈问题。

Method: 提出MTLA，利用超网络动态合并相邻时间KV缓存向量，并引入步长感知因果掩码保证训练并行性与推理一致性。

Result: 实验表明MTLA在多项任务中性能与标准MHA相当，但显著提升推理速度（如5.3倍）和GPU内存效率（降低8.3倍）。

Conclusion: MTLA能高效压缩KV缓存，平衡性能与资源开销，适用于实际部署。

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [288] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547)
*Pengxin Guo,Yinong Wang,Wei Li,Mengting Liu,Ming Li,Jinkai Zheng,Liangqiong Qu*

Key words: LLM剪枝,联邦学习,隐私保护,模型压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedPrLLM是一个联邦剪枝框架，旨在保护隐私的同时压缩大语言模型（LLM），无需共享本地数据，仅需分享剪枝掩码矩阵。

Motivation: 为了解决隐私敏感领域中获取公共校准样本的难题，提出了一种保护隐私的LLM剪枝方法。

Method: 通过联邦学习框架，客户端基于本地数据计算剪枝掩码矩阵并共享给服务器，协作剪枝全局模型。

Result: 实验表明，采用层比较且不缩放权重的一次性剪枝是FedPrLLM框架中的最佳选择。

Conclusion: FedPrLLM为隐私敏感领域的LLM剪枝提供了有效指导。

Abstract: LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [289] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/abs/2505.13563)
*Xiaohui Wang,Peng Ye,Chenyu Huang,Shenghe Zheng,Bo Zhang,Wanli Ouyang,Tao Chen*

Key words: Delta压缩,数据无关,高效存储,模型微调,UltraDelta

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出UltraDelta，一种无需数据的Delta压缩方法，实现超高压缩和性能平衡。

Motivation: 解决现有Delta压缩方法在高压缩率和性能之间的不足，同时摆脱数据依赖。

Method: 基于方差分配稀疏度、分布感知压缩和全局重缩放因子优化。

Result: 在多种模型上实现133x至800x的高压缩比，性能优于现有方法。

Conclusion: UltraDelta为高效存储多任务微调模型提供了有效解决方案。

Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [290] [Online Decision-Focused Learning](https://arxiv.org/abs/2505.13564)
*Aymeric Capitaine,Maxime Haddouche,Eric Moulines,Michael I. Jordan,Etienne Boursier,Alain Durmus*

Key words: 决策聚焦学习、动态环境、在线算法、动态遗憾、正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 决策聚焦学习（DFL）是一种训练预测模型的新范式，直接优化下游决策的损失。本文研究DFL在动态环境中的应用，通过正则化和乐观原则提出在线算法，并在实验中验证其有效性。

Motivation: 传统DFL研究集中在静态环境和固定数据批次，本文旨在解决动态环境下目标函数和数据分布随时间变化的问题，填补研究空白。

Method: 通过（i）正则化目标函数使其可微分，（ii）利用乐观原则结合近优预言机和适当扰动，提出一种在线算法。

Result: 算法在单纯形和有界凸多面体决策空间上均建立了动态遗憾的期望边界，实验表明其性能优于传统预测聚焦方法。

Conclusion: 本文提出的动态环境下DFL算法有效解决了目标函数不可微分和非凸的挑战，具有实际应用潜力。

Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [291] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/abs/2505.13567)
*Yoav Ger,Omri Barak*

Key words: RNN, 闭环学习, 开环学习, 学习动态, 运动控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文分析了线性RNN在闭环和开环训练模式下的学习动态差异，揭示了闭环模式中短期策略改进与长期稳定性的权衡，并验证了其在运动控制任务中的应用。

Motivation: 研究动机是理解闭环环境下RNN的学习动态，以更贴近生物学习的真实场景。

Method: 方法包括数学理论推导和模拟实验，比较闭环与开环训练下RNN的学习轨迹。

Result: 结果表明闭环训练中RNN的学习动态受短期改进和长期稳定性的双重影响。

Conclusion: 结论强调了闭环动态建模在生物合理性研究中的重要性。

Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [292] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/abs/2505.13569)
*Fynn Fromme,Christine Allen-Blanchette,Hans Harder,Sebastian Peitz*

Key words: 机器学习, 物理系统建模, 等变性, 瑞利-贝纳德对流, 卷积LSTM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于等变卷积自编码器和等变卷积LSTM的端到端代理模型，用于建模和理解大规模物理系统，并在三维瑞利-贝纳德对流中验证了其高效性。

Motivation: 大规模物理系统（如流体力学、核聚变反应堆等）建模面临自由度多、时空尺度复杂的挑战，需要提高准确性和样本效率。

Method: 采用等变卷积自编码器和等变卷积LSTM，利用G-可操纵核，针对E(2)等变性和垂直方向平移对称性的破坏，设计了D4-可操纵核的分层结构。

Result: 模型在样本和参数效率上表现显著提升，并能适应更复杂的动态（更大瑞利数）。

Conclusion: 该方法为大规模物理系统建模提供了高效且可扩展的解决方案。

Abstract: The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [293] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/abs/2505.13575)
*Ilkay Wunderlich,Benjamin Koch,Sven Schönfeld*

Key words: 卷积神经网络, FPGA, TinyYOLOv3, 批量归一化融合, 滤波器剪枝, 网络量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了在FPGA上高效部署TinyYOLOv3检测网络的方法，包括批量归一化融合、滤波器剪枝和训练后网络量化等技术。

Motivation: 由于CNN在嵌入式平台（如FPGA）上部署时面临计算密集、内存需求高和算术条件复杂等问题，研究旨在提出优化策略。

Method: 采用了批量归一化融合、滤波器剪枝和训练后网络量化等技术，以XILINX Artix-7 FPGA为平台。

Result: 展示了在FPGA上高效运行TinyYOLOv3检测网络的最佳实践方法。

Conclusion: 通过优化技术，可以在资源受限的FPGA上有效部署CNN模型。

Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [294] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/abs/2505.13576)
*Sara Alosaime,Arshad Jhumka*

Key words: 联邦学习(Federated Learning), 灾难性遗忘(Catastrophic Forgetting), HAR(Human Activity Recognition), 资源受限(Resource-Constrained), 动态分布(Dynamic Distributions)

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了FlexFed，一种针对HAR环境中联邦学习的优化方法，通过动态调整训练频率和高效利用数据来缓解灾难性遗忘问题。

Motivation: 解决HAR环境中联邦学习面临的灾难性遗忘问题，由于数据分布变化和间歇性参与导致的挑战。

Method: 引入FlexFed方法，动态调整离线训练频率，提出新的量化指标和改进的评估框架。

Result: FlexFed有效缓解灾难性遗忘，提升效率10-15%，实现更快更稳定的收敛。

Conclusion: FlexFed在HAR环境中优于现有方法，解决了隐私限制下的灾难性遗忘问题。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [295] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/abs/2505.13578)
*Mikhail Osipov*

Key words: cost functional, symmetry group, variational method, black-box, Lie-algebraic

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种利用对称结构构造显式对称性破缺变形的方法，通过最小化辅助能量泛函来优化不变代价泛函。

Motivation: 解决在机器学习和逆问题中，由于对称群不变性和代价泛函不可微而导致的优化困难。

Method: 通过构造对称性破缺变形和最小化辅助能量泛函，生成一个规范场来引导变分流。

Result: 证明了在轻度正则性条件下，代价泛函会严格下降，且退化集的概率为零。

Conclusion: 该方法为优化不变代价泛函提供了一种理论工具，适用于黑盒模型和对称约束任务。

Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [296] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/abs/2505.13580)
*Hanzhao Wang,Guanting Chen,Kalyan Talluri,Xiaocheng Li*

Key words: Generative Pre-trained Transformer, 序列决策, 运筹学, 管理科学, OMGPT

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了OMGPT模型，通过预训练数据解决运筹与管理科学中的序列决策任务，实现了现有方法未能达到的两点优势。

Motivation: 解决运筹与管理科学中的序列决策任务，如动态定价、库存管理等，通过预训练数据提升模型能力。

Method: 提出通用序列建模框架，训练基于Transformer的OMGPT模型，不需假设任何分析模型结构。

Result: 在多个任务中表现优异，并建立了贝叶斯理论解释其性能。

Conclusion: OMGPT模型在序列决策任务中展现出显著优势，为未来研究提供了新方向。

Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [297] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/abs/2505.13582)
*Leyang Zhang,Yaoyu Zhang,Tao Luo*

Key words: 神经网络, 临界点, 样本依赖性, 鞍点, 参数空间

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了神经网络临界点的样本依赖性，提出了一种样本无关的临界提升操作符，并通过示例展示了样本依赖性临界点的存在性和鞍点特性。

Motivation: 研究神经网络临界点的样本依赖性，以理解参数空间中的关键点如何随样本变化。

Method: 引入样本无关的临界提升操作符，比较样本依赖和无关的临界点，并通过示例验证方法的有效性。

Result: 证明样本依赖性临界点在足够多样本时存在，且其中包含鞍点。

Conclusion: 样本无关的临界提升操作符成功揭示了神经网络参数空间中的样本依赖性临界点。

Abstract: This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [298] [Half Search Space is All You Need](https://arxiv.org/abs/2505.13586)
*Pavel Rumiantsev,Mark Coates*

Key words: Neural Architecture Search, One-Shot NAS, Zero-Shot NAS, search space pruning, memory efficiency

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种通过Zero-Shot NAS自动剪枝搜索空间的方法，显著降低了One-Shot NAS的内存消耗和搜索时间，同时保持准确性。

Motivation: 为了解决One-Shot NAS方法在搜索过程中GPU内存消耗高的问题，提出了一种高效的剪枝策略。

Method: 使用Zero-Shot NAS预先从搜索空间中去除低性能架构，再对剪枝后的空间应用One-Shot NAS。

Result: 实验显示，该方法在DARTS搜索空间上将内存消耗降低了81%，同时保持了相同的准确率。

Conclusion: 该方法有效解决了One-Shot NAS的高内存消耗问题，提升了搜索效率。

Abstract: Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [299] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/abs/2505.13614)
*Ke Sun*

Key words: 深度神经网络, Fisher信息, Riemannian度量, Hutchinson迹估计器, 神经流形

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种高效估计深度神经网络Fisher信息度量张量的方法，通过核心空间分析扩展到神经流形的确定性边界，并引入了基于Hutchinson迹估计器的无偏随机估计。

Motivation: 现代深度神经网络的参数空间（神经流形）具有独特的Fisher信息度量张量，准确估计它对理论和实际方法至关重要。

Method: 通过分析分类网络的核心空间中的Riemannian度量谱，扩展到神经流形的度量张量边界，并引入基于Hutchinson迹估计器的无偏随机估计方法。

Result: 方法可通过单次反向传播高效估计度量张量（对角线、块对角线或完整张量），其质量由真实值的标准差保证。

Conclusion: 该方法为深度学习中Fisher信息度量张量的估计提供了高效且可靠的解决方案。

Abstract: The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [300] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/abs/2505.13631)
*Andrei Manolache,Luiz F. O. Chamon,Mathias Niepert*

Key words: 等变神经网络, 对称性, 自适应约束优化, 样本效率, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种自适应约束等变性（ACE）方法，通过逐步降低非等变性模型的偏差来平衡等变性与非等变性，提升性能、样本效率和鲁棒性。

Motivation: 现实数据常偏离完美对称性，传统严格等变模型难以适应，而无约束模型无法有效利用部分对称性。ACE旨在解决这一问题。

Method: 采用同伦原理，通过约束优化逐步将非等变模型转化为接近等变性，平衡对称性与灵活性。

Result: 在多架构和任务中，ACE显著优于严格等变模型和启发式放松方法，提升了性能指标和鲁棒性。

Conclusion: ACE提供了一种数据驱动的平衡策略，有效解决了对称性与模型灵活性之间的矛盾。

Abstract: Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [301] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/abs/2505.13636)
*Baiting Chen,Tong Zhu,Jiale Han,Lexin Li,Gang Li,Xiaowu Dai*

Key words: 大语言模型,Peer Elicitation Games,游戏理论,互信息,纳什均衡

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了Peer Elicitation Games (PEG)，一种无需训练的游戏理论框架，通过多个判别器的合作来减少大语言模型（LLMs）的幻觉和不一致问题。

Motivation: 大语言模型虽然在生成能力上表现优异，但仍存在幻觉和不一致问题。PEG旨在通过游戏理论机制，无需监督或微调即可提升模型的真实性和一致性。

Method: PEG框架包括一个生成器和多个判别器，它们通过在线学习和同伴评估机制互动。奖励基于确定性互信息评分，激励真实报告，无需真实标签。

Result: 理论证明表明，每个代理在在线学习中能够以次线性遗憾接近最佳固定策略，并收敛到真实的纳什均衡。实证结果显示其显著提高了事实准确性。

Conclusion: PEG是一种无需监督或微调的实用方法，能够有效提升LLMs的真实性。

Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [302] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti,Giovanni Agosta*

Key words: 大语言模型,强化学习,复杂棋盘游戏,数字孪生

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了4Hammer强化学习环境，用于评估LLM在复杂棋盘游戏中的表现，填补了这一领域的空白。

Motivation: 现有数据集虽然覆盖了软件工程或视频游戏等长期任务，但缺乏专门为强化学习和LLM评估设计的复杂棋盘游戏环境。

Method: 通过开发4Hammer，一个基于零和复杂棋盘游戏Warhammer 40,000的数字孪生仿真环境，模拟其规则和交互。

Result: 4Hammer环境能够评估LLM在复杂长期任务中的表现。

Conclusion: 4Hammer为LLM评估提供了新的复杂任务环境，有助于进一步研究。

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [303] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/abs/2505.13643)
*Rakibul Hasan Rajib,Md Akil Raihan Iftee,Mir Sazzat Hossain,A. K. M. Mahbubur Rahman,Sajib Mistry,M Ashraful Amin,Amin Ahsan Ali*

Key words: 联邦学习, 分布偏移, 测试时间适应

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 联邦学习中分布偏移导致性能下降，传统TTA方法存在计算开销和隐私风险。FedCTTA通过基于噪声样本的相似性聚合实现隐私保护和高效适应。

Motivation: 解决联邦学习中由于分布偏移导致的模型性能下降问题，并避免传统TTA方法的计算开销和隐私风险。

Method: 提出FedCTTA框架，利用基于噪声样本的相似性聚合和最小化客户端熵，实现隐私保护和高效适应。

Result: 实验表明，FedCTTA在时空异构场景下优于现有方法，且具有可扩展性。

Conclusion: FedCTTA是一种高效、隐私保护的联邦适应框架，解决了传统方法的局限性。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [304] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/abs/2505.13644)
*Felix Dangel,Tim Siebert,Marius Zeinhofer,Andrea Walther*

Key words: 偏微分方程, 泰勒模式, 自动微分, 科学机器学习, 计算图优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种优化泰勒模式的技巧，通过重写计算图来“折叠”导数，并展示了如何将其应用于一般线性PDE算子和随机化泰勒模式。该方法简单且高效，无需用户理解复杂细节，证实了其加速泰勒模式并优于嵌套反向传播的优势。

Motivation: 在科学机器学习中，通过嵌套反向传播计算偏微分方程（PDE）算子成本高昂，限制了其应用。虽然已有前向Laplacian和随机化泰勒模式自动微分（AD）等改进方案，但仍需更高效的优化技术。

Method: 提出了一种“折叠”导数的优化技巧，通过重写计算图，并在计算过程中传播求和操作。该方法适用于一般线性PDE算子和随机化泰勒模式，且可通过机器学习编译器实现，无需用户干预。

Result: 实现了该优化方法，并在常见PDE算子上进行了评估，证实其能加速泰勒模式并超越嵌套反向传播的性能。

Conclusion: 该方法为科学机器学习中的PDE算子计算提供了一种高效且用户友好的优化方案，显著提升了计算效率。

Abstract: Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [305] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/abs/2505.13650)
*Chou-Ying Hsieh,Chun-Fu Jang,Cheng-En Hsieh,Qian-Hui Chen,Sy-Yen Kuo*

Key words: 图对比学习、自监督学习、图表示学习、正样本选择

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SRGCL是一种自我增强的图对比学习框架，通过动态选择和评估高质量正样本对来提升图表示的质量，实验证明其在多种图分类任务中优于现有方法。

Motivation: 现有的图对比学习方法在生成高质量正样本对时面临挑战，可能导致原始图的语义和结构信息失真。

Method: 提出SRGCL框架，利用模型自身的编码器动态评估和选择高质量正样本对，结合多种增强策略和流形假设指导的选择器。

Result: SRGCL在多样化的图分类任务中表现出色，优于现有的GCL方法。

Conclusion: SRGCL通过自我增强机制提升了图对比学习的表现，展示了其跨领域的适应性和有效性。

Abstract: Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [306] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni,Durgesh Kalwar,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Key words: 强化学习, 大型语言模型, GRPO, MDP, 监督学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文分析了基于强化学习（RL）的后训练大型语言模型（LLMs）的简化假设，提出这些假设导致RL方法等效于监督学习，并通过实验验证了迭代监督微调的性能与GRPO相当。

Motivation: 研究动机在于质疑当前流行的RL后训练LLMs的假设和框架，特别是GRPO在提升模型推理能力中的作用。

Method: 方法包括分析RL框架中的简化假设（如MDP状态的拼接和奖励均匀分配），并通过实验比较RL和迭代监督微调在GSM8K和Countdown基准上的性能。

Result: 结果表明，迭代监督微调（结合正负样本）的性能与GRPO训练相当，且RL的结构假设可能导致生成更长序列的中间标记。

Conclusion: 结论指出RL在提升LLMs推理能力中可能有效，但当前的简化假设和框架有待商榷。

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [307] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/abs/2505.13702)
*Mariana A. Fazio,Salvador Sosa Güitron,Marcus Babzien,Mikhail Fedurin,Junjie Li,Mark Palmer,Sandra S. Biedron,Manel Martinez-Ramon*

Key words: 无监督学习，异常检测，MUED，不确定性度量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种无监督异常检测方法，用于检测MUED中的故障图像，无需手动标注数据，并提供了检测的不确定性度量。

Motivation: 无监督技术避免了手动标注数据的繁琐过程，机器能自主检测异常，节省用户时间。

Method: 采用无监督学习方法构建异常检测模型，并提供不确定性度量。

Result: 方法能够高效检测异常图像，并给出不确定性指标供用户决策。

Conclusion: 无监督异常检测方法适合MUED故障图像检测，且有潜力应用于其他类似场景。

Abstract: This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [308] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13709)
*Jiayu Chen,Aravind Venugopal,Jeff Schneider*

Key words: 离线强化学习, 模型强化学习, 鲁棒性, Stackelberg学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种离线模型强化学习的框架，通过动态调整世界模型和策略的统一学习目标来提高鲁棒性，解决了传统方法中目标不匹配和部署时的鲁棒性问题。

Motivation: 现有的离线模型强化学习方法（MBRL）通常采用两阶段训练，导致世界模型未针对策略学习优化，且策略在部署时缺乏鲁棒性。

Method: 提出统一的框架，通过Stackelberg学习动态求解最大-最小优化问题，动态调整世界模型与策略。

Result: 在D4RL MuJoCo和Tokamak Control任务上达到最先进性能。

Conclusion: 动态调整世界模型与策略能有效提升离线MBRL的鲁棒性和性能。

Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [309] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/abs/2505.13723)
*Pratik Rathore,Zachary Frangella,Sachin Garg,Shaghayegh Fazliani,Michał Dereziński,Madeleine Udell*

Key words: 高斯过程、分布式算法、草图投影、贝叶斯优化、大规模数据集

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为ADASAP的分布式近似算法，用于高效解决高斯过程（GP）中的大规模线性系统问题，显著提升了GP推断的可扩展性。

Motivation: 高斯过程在大规模数据集上的推断因计算复杂度高而受限，需要一种更高效的算法来解决这一问题。

Method: 采用了分布式加速的草图投影算法（ADASAP），并结合行列式点过程理论证明其快速收敛性。

Result: ADASAP在多个基准数据集和贝叶斯优化任务中优于现有方法，并能处理超过3亿样本的数据集。

Conclusion: ADASAP是一种高效、可扩展的GP推断算法，适用于大规模数据集。

Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [310] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness*

Key words: 超参数缩放, 语言模型预训练, 权重衰减, 批量大小, 幂律

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究超参数（HP）如何随模型大小N、数据集大小D和批量大小B缩放，提出缩放定律并验证公式。

Motivation: 探索大规模语言模型预训练中超参数（如学习率和权重衰减）的高效设置方法。

Method: 通过研究超参数缩放规律，验证最佳时间尺度与参数-标记比的关系，并分析批量大小的缩放规律。

Result: 发现最优权重衰减λ随批量大小B线性缩放，最佳批量大小Bopt与临界批量大小Bcrit与数据集大小D呈幂律关系，与模型大小N无关。

Conclusion: 研究结果为大规模训练中超参数设置和模型优化提供了理论支持，有助于提高训练效率。

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [311] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/abs/2505.13740)
*Chenning Yu,Sicun Gao*

Key words: 组合生成, 扩散模型, lift scores, 条件对齐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种基于lift scores的新重采样标准，提升扩散模型的组合生成能力，无需额外训练。

Motivation: 提高扩散模型在组合生成任务中的条件对齐效果。

Method: 利用lift scores评估生成样本是否满足单个条件，并组合结果判断整体提示的满足情况，仅依赖原扩散模型。

Result: 在2D合成数据、CLEVR位置任务和文本到图像合成中显著提升了条件对齐。

Conclusion: lift scores在不增加训练负担的情况下有效改进了组合生成。

Abstract: We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [312] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/abs/2505.13742)
*Andrew Nam,Declan Campbell,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Key words: 神经网络,可解释性,贝叶斯推理,信息理论,认知建模

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的概率框架，用于解释神经网络中的潜在任务表示，结合贝叶斯推理和信息理论工具。

Motivation: 神经网络的灵活性使其成为认知建模的有力工具，但其表示难以解释，需要一种方法来推断其因果贡献。

Method: 采用贝叶斯推理，定义表示单元的分布，并基于信息理论提出工具和度量指标。

Result: 提出的框架能揭示模型的关键特性，如表示的分布式、流形复杂性和多义性。

Conclusion: 该研究为神经网络的可解释性提供了新工具，有助于理解其认知建模能力。

Abstract: Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [313] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/abs/2505.13745)
*Joanna Komorniczak*

Key words: 数据流处理, 概念漂移, 新类检测, 开放集识别, 无监督检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种合成数据流生成策略，用于模拟概念漂移和新类出现的情况，并探讨了无监督漂移检测器在这些任务中的应用及其在开放集识别中的潜力。

Motivation: 动态环境中数据流的非平稳性常表现为概念漂移和新类出现，现有方法多关注单一现象，而本文旨在同时解决这两种挑战。

Method: 论文提出了一种合成数据流生成策略，模拟概念漂移和新类出现的场景，并利用无监督漂移检测器进行检测。

Result: 研究展示了无监督漂移检测器在新颖性和概念漂移检测中的有效性，并验证了生成的数据流在开放集识别任务中的应用。

Conclusion: 该策略为同时处理概念漂移和新类出现提供了有效途径，并在开放集识别中展现了潜力。

Abstract: The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [314] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/abs/2505.13754)
*Devendra Parkar,Anya Chaturvedi,Andréa W. Richa,Joshua J. Daymude*

Key words: 最大獨立集,動態圖,無監督學習,圖神經網絡,分散式更新

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 首個針對動態圖的最大獨立集（MaxIS）無監督學習模型，結合圖神經網絡（GNN）與分散式更新機制，在單一步驟中高效處理邊的變動並推斷節點成員資格。

Motivation: 解決動態圖中MaxIS問題的挑戰，傳統方法在處理大規模或實時變動時效率不足。

Method: 結合GNN的結構學習與分散式更新機制，支持單步並行處理邊的增刪操作。

Result: 在100-10,000節點的動態圖上表現優異，解決質量、運行時間和內存使用均優於現有方法。

Conclusion: 模型具有良好的泛化能力，適用於大規模動態圖，並在速度和性能上超越現有技術。

Abstract: We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [315] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/abs/2505.13755)
*Jeffrey Lai,Anthony Bao,William Gilpin*

Key words: 混沌系统、预训练模型、Panda、非线性动力学、零样本预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Panda模型通过训练合成混沌系统数据集，展示了零样本预测真实世界混沌系统和涌现非线性共振模式的能力，并揭示了预训练模型在非线性动力学等数学领域的潜力。

Motivation: 混沌系统对小误差高度敏感，传统方法难以构建预测性数据驱动模型。本文旨在通过预训练模型解决这一问题。

Method: 提出Panda模型，基于进化算法生成的2×10^4个混沌系统合成数据集进行训练，使用Patched Attention技术。

Result: Panda表现出零样本预测真实混沌系统的能力，并能自发预测偏微分方程，展示了预训练模型的强大潜力。

Conclusion: Panda模型为非线性动力学等抽象数学领域的探索提供了新工具，展现了预训练模型的广阔前景。

Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [316] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/abs/2505.13760)
*Drona Khurana,Anish Thilagar,Dhamma Kimpara,Rafael Frongillo*

Key words: 替代损失函数, 统计一致性, 校准, 间接引发（IE）, 凸可微

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文主要研究了离散预测任务中替代损失函数的统计一致性，提出了一种更容易验证的条件——间接引发（IE），并证明了其在校准问题中的等效性。对于非多面体（如凸可微）损失函数，首次展示了类似结果，并提出了强IE条件以确保一致性。

Motivation: 直接验证离散预测任务中替代损失函数的校准条件通常繁琐。研究旨在找到更易验证的条件（IE），并扩展其适用性。

Method: 针对凸可微损失函数，证明了IE与校准的等效性；在高维情况下提出强IE条件，并验证其作用。

Result: 一维情况下IE与校准等价，高维则失效；强IE确保校准，且对强凸可微函数是充要条件。

Conclusion: IE和强IE为设计和分析一致性可微替代损失提供了有力工具。

Abstract: The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [317] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/abs/2505.13765)
*Hainan Xu,Vladimir Bataev,Lilit Grigoryan,Boris Ginsburg*

Key words: RNN-T, 推理加速, 并行处理, 贪心解码, 波束搜索

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为WIND的新策略，通过并行处理窗口内的多帧信号加速RNN-T推理，同时保持模型精度。

Motivation: 为了提高RNN-T模型的推理速度而不影响其准确性，作者提出了一种新的并行处理策略。

Method: WIND策略通过窗口内并行处理多帧信号，快速定位非空白预测，并适用于贪心解码、批量贪心解码和新的波束搜索解码方法。

Result: 在多种数据集上的实验表明，贪心模式下速度提升高达2.4倍，且WER保持不变；波束搜索算法在速度显著提升的同时精度略有提高。

Conclusion: WIND是一种高效的RNN-T推理加速方法，适用于多种解码场景。

Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [318] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/abs/2505.13768)
*Ruiquan Huang,Donghao Li,Chengshuai Shi,Cong Shen,Jing Yang*

Key words: 强化学习,混合学习,离线数据集,在线交互,次优性差距,学习遗憾

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种混合学习框架，结合离线数据和在线交互的强化学习，优于纯在线或离线算法，并在两个学习指标上达到最先进结果。

Motivation: 研究如何通过结合离线数据集和在线交互来优化强化学习的性能。

Method: 提出了一种统一的算法，通过置信度在线的RL算法与离线数据集结合，理论分析其性能。

Result: 算法在次优性差距和在线学习遗憾指标上表现优异，理论结果通过实验验证。

Conclusion: 混合学习方法在强化学习中具有显著优势，且离线数据集对不同指标的覆盖特性需求不同。

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [319] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775)
*Kaya Stechly,Karthik Valmeekam,Atharva Gundawar,Vardhan Palod,Subbarao Kambhampati*

Key words: 思维链（CoT）, 中间推理步骤, 形式化验证, 噪声数据, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，尽管大型推理模型通过思维链（CoT）训练在性能上有所提升，但中间推理步骤的准确性与其解决方案的准确性关联不大，甚至噪声数据也能保持或提升性能，挑战了CoT的常规解释。

Motivation: 探讨大型推理模型中的中间推理步骤（如思维链）是否真的影响模型性能，以及这些步骤的语义是否如人们所认为的那样重要。

Method: 训练Transformer模型于形式化验证的推理痕迹和解决方案，评估中间步骤和最终输出的正确性，并使用噪声数据验证其影响。

Result: 即使中间步骤错误，模型仍能提供正确解决方案；噪声数据甚至可能提升性能和泛化能力。

Conclusion: 中间推理步骤的准确性与其解决方案的准确性关联较弱，需谨慎解读其语义和人类化行为。

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [320] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
*Chris Cundy,Adam Gleave*

Key words: AI欺骗, 谎言检测器, GRPO, DPO, 诚实性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了在AI训练中引入谎言检测器对模型诚实性的影响，发现其效果取决于探索量、检测器准确性和KL正则化强度，可能导致诚实或欺骗行为。

Motivation: 研究AI系统中欺骗行为对评估和部署的影响，以及谎言检测器在训练中的潜在作用与风险。

Method: 使用DolusChat数据集，结合谎言检测器与GRPO/DPO算法，分析诚实性因素。

Result: GRPO可能导致高欺骗率（85%），而DPO则保持低欺骗率（25%）。检测器准确性和KL正则化是关键。

Conclusion: 谎言检测器训练效果复杂，既可能增强监督，也可能加剧未检测到的欺骗。

Abstract: As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [321] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/abs/2505.13791)
*Austin H. Cheng,Chong Sun,Alán Aspuru-Guzik*

Key words: 3D分子生成, 自回归模型, 扩散模型, 因果变换器, 分子设计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Quetzal是一种简单但可扩展的自回归模型，用于3D分子生成，通过原子逐个构建分子，结合了因果变换器和扩散MLP，在生成质量和速度上优于现有基线。

Motivation: 现有3D分子生成方法中扩散模型占主导，自回归模型表现较差，Quetzal旨在提升自回归模型的性能和扩展性。

Method: Quetzal将分子视为有序原子序列，用因果变换器预测下一个原子的离散类型，用扩散MLP建模连续位置分布。

Result: Quetzal在生成质量和速度上优于现有自回归基线，且支持可变大小任务（如氢装饰和支架完成）。

Conclusion: Quetzal为3D分子生成提供了新的可扩展性和通用性视角。

Abstract: Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [322] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/abs/2505.13811)
*Parikshit Bansal,Sujay Sanghavi*

Key words: 语言模型微调, 遗忘, KL散度, 上下文无关生成, 合成数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过上下文无关生成（context-free generation）来近似估计KL散度的方法，以减少语言模型在微调时对原有任务的遗忘现象。

Motivation: 微调语言模型时，模型参数的变化可能导致其在其他任务上的性能下降（即“遗忘”）。论文的目标是在无法访问原始训练数据的情况下，通过一种简单的方法缓解遗忘问题。

Method: 使用上下文无关生成近似估计KL散度，并将生成的合成数据与微调数据集结合，以减少遗忘。

Result: 实验表明，上下文无关生成能够有效减少遗忘，尤其是在保留预训练模型的零样本性能和推理模型的推理能力方面。合成上下文数据或部分预训练数据的效果较差。

Conclusion: 上下文无关生成是一种简单且有效的方法，可用于缓解语言模型微调时的遗忘问题。

Abstract: Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [323] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2505.13813)
*Matthew Raffel,Lizhong Chen*

Key words: Kolmogorov-Arnold Network, KAT, FlashKAT, 训练速度, 内存瓶颈

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文分析了KAN的计算速度问题，提出了FlashKAT解决方案，显著提升了训练速度。

Motivation: KAN虽然表达能力更强且可解释性高，但因计算成本高和训练不稳定，限制了其在大规模任务中的应用。KAT虽有类似FLOPs，但训练速度仍慢123倍。

Method: 研究发现性能瓶颈源于内存停滞和梯度积累效率低。FlashKAT通过优化内核结构和减少梯度积累来提升效率。

Result: FlashKAT实现了86.5倍的训练速度提升，同时减少了系数梯度的舍入误差。

Conclusion: FlashKAT有效解决了KAT的内存瓶颈，显著提升了性能。

Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [324] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/abs/2505.13819)
*Lucas Rosenblatt,Bin Han,Robert Wolfe,Bill Howe*

Key words: 大型语言模型,数据泄露,成员推断,PRISM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型（LLMs）在部分无序样本信息泄露下的脆弱性，提出了一种更通用的威胁模型和两种数据无关的攻击方法，强调了其鲁棒性。

Motivation: 探讨在攻击者仅掌握部分、无序样本信息时，LLMs是否会泄露敏感训练数据。

Method: 引入了两种数据无关的方法：（1）基于成员推断的似然比攻击；（2）利用外部先验信息的PRISM方法。

Result: 在医疗和法律场景中，这两种方法与假设具备标记数据的基线分类器表现相当。

Conclusion: 细调后的LLMs在部分信息攻击下仍存在泄露风险。

Abstract: Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [325] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu,Zhenglun Kong,Peiyan Dong,Changdi Yang,Tianqi Li,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Key words: 大语言模型, 智能体蒸馏, 推理与行动一致性, 模型压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为结构化智能体蒸馏（Structured Agent Distillation）的框架，通过分段对齐损失压缩大型语言模型智能体，保留推理和行动一致性。

Motivation: 解决大型语言模型智能体因高推理成本和大模型尺寸而难以实际部署的问题。

Method: 将轨迹分段为{[REASON]}和{[ACT]}，应用分段特定损失对齐教师模型行为。

Result: 在多个数据集上优于基于标记级蒸馏和模仿学习的基线，显著压缩模型尺寸且性能下降较小。

Conclusion: 分段对齐是高效可部署智能体的关键。

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [326] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/abs/2505.13852)
*Yusheng Zhao,Chi Zhang,Yuxuan Du*

Key words: 量子系统, 机器学习, 深度学习, 基态学习, 137量子比特

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文通过系统比较深度学习和传统机器学习在量子系统基态学习任务中的表现，发现传统机器学习性能与深度学习相当甚至更优，质疑了深度学习在此类任务中的必要性。

Motivation: 由于现有研究在量子资源使用和数据构建上存在不公平比较，本文旨在明确深度学习在量子系统基态学习任务中的作用。

Method: 在三种哈密顿族中，系统比较深度学习和传统机器学习方法，并限制量子资源使用一致，任务规模达127量子比特。

Result: 传统机器学习在多任务中表现与深度学习相当甚至更优，且测量输入特征对深度学习模型性能影响很小。

Conclusion: 当前深度学习模型在量子系统学习中可能并非必要，研究为高效利用深度学习提供了重要参考。

Abstract: Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [327] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/abs/2505.13857)
*Tian Sun,Yuqi Chen,Baihua Zheng,Weiwei Sun*

Key words: GPS轨迹恢复, 时空动态, PD-GNN, TedFormer, 神经微分方程

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为TedTrajRec的新方法，用于解决低采样率GPS轨迹恢复问题，通过结合PD-GNN和TedFormer分别捕捉交通和轨迹的时空动态，显著提升了轨迹恢复性能。

Motivation: 现实世界中，GPS轨迹常因采样率低且间隔不规则而难以直接使用。传统方法未能充分利用轨迹和路网的复杂时空动态，限制了恢复效果。

Method: 提出TedTrajRec方法：1. 使用PD-GNN建模周期性交通动态和拓扑感知的路段动态；2. 提出TedFormer（时间感知Transformer），通过神经微分方程融入注意力机制处理不规则采样数据。

Result: 在三个真实数据集上的实验表明，TedTrajRec性能优越。

Conclusion: 通过分解时空动态并分别建模，TedTrajRec显著提升了低采样率GPS轨迹的恢复效果。

Abstract: In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [328] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/abs/2505.13858)
*Gonzalo E. Constante-Flores,Hao Chen,Can Li*

Key words: 深度学习, 约束满足, 安全关键任务, 神经网络, 鲁棒优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种模型无关的框架，通过在神经网络输出中强制满足输入相关的线性等式和不等式约束，确保预测的可行性和安全性。

Motivation: 为解决深度学习模型在安全关键任务中难以满足硬约束的问题，如物理定律、公平性或安全性要求。

Method: 结合任务网络和安全网络，任务网络负责预测准确性，安全网络通过随机和鲁棒优化的决策规则确保整体输入的可行性，最终预测是两者的凸组合。

Result: 实验证明该方法在基准回归任务中能持续满足约束，同时保持较高的准确性和低推理延迟。

Conclusion: 该框架无需迭代或运行时优化，即可在训练和推理中保证约束满足，是约束函数的通用近似器。

Abstract: Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [329] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/abs/2505.13873)
*Peisong Niu,Ziqing Ma,Tian Zhou,Weiqi Chen,Lefei Shen,Rong Jin,Liang Sun*

Key words: 天气预测、自监督学习、预训练、过拟合、Siamese Autoencoder

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为Baguan的新型数据驱动模型，通过自监督预训练和微调方法，有效解决了天气预测中的过拟合问题，并在多种任务中表现优异。

Motivation: 天气预测长期以来是一个重要挑战，传统的数值天气预测方法受限于数据量少，容易过拟合。AI模型虽在全局预测中表现更好，但数据不足仍是一个关键问题。

Method: 采用Siamese Autoencoder进行自监督预训练，通过选择合适的预训练任务引入局部偏差以缓解过拟合，随后对不同预测时间段进行微调。

Result: 实验结果显示，Baguan在中期天气预测中优于传统方法，并能有效控制过拟合，在季节性预测和区域预测等下游任务中表现优异。

Conclusion: 通过预训练和微调的方法，Baguan模型在天气预测中表现卓越，为解决数据不足和过拟合问题提供了新思路。

Abstract: Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [330] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu,Zhaoyi Yan,Yuanyi Wang,Yiming Zhang,Qi Zhou,Fei Wu,Hongxia Yang*

Key words: 模型融合, 偏好优化, 大型语言模型, InfiFPO, 概率信息

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: InfiFPO是一种新的偏好优化方法，通过合成多源模型概率提升LLM性能，显著优于现有方法。

Motivation: 现有模型融合方法在偏好对齐（PA）阶段忽视了概率信息，限制了性能提升。

Method: 提出InfiFPO，融合多源模型概率并引入概率剪裁与最大间隔策略。

Result: 在11个基准测试中，InfiFPO将Phi-4的平均性能从79.95提升至83.33。

Conclusion: InfiFPO在数学、编码和推理任务中显著提升LLM能力。

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [331] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/abs/2505.13896)
*Yingwei Zhang,Ke Bu,Zhuoran Zhuang,Tao Xie,Yao Yu,Dong Li,Yang Guo,Detao Lv*

Key words: 时间序列预测, 跨未来行为, CRAFT, Koopman预测, 趋势挖掘

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了CRAFT方法，利用跨未来行为（CFB）特征解决时间序列预测中的不确定性，结合多个模块提取和补充趋势，并通过实验验证其有效性。

Motivation: 时间序列预测（TSF）常因历史数据有限而面临不确定性，跨未来行为（CFB）提供了一种新的解决思路。

Method: CRAFT方法包括Koopman预测模块、内部趋势挖掘模块和外部分层趋势引导模块，结合需求约束损失校准预测偏差。

Result: 在离线和在线实验中，CRAFT表现出显著的有效性。

Conclusion: CRAFT通过利用CFB特征和多模块协同，提升了时间序列预测的准确性。

Abstract: The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [332] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
*Róbert Csordás,Christopher D. Manning,Christopher Potts*

Key words: LLM, 模型深度, 残差流, 多跳任务, 性能递减

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究分析了Llama 3.1和Qwen 3等模型，发现更深层模型并未有效利用深度进行高阶计算，而是将相同计算分散到更多层，解释了性能提升随深度增加而递减的现象。

Motivation: 探讨现代LLM是否通过深度实现了新的高阶计算，还是仅仅将相同计算分散到更多层。

Method: 分析模型的残差流，比较子层输出、跳层影响、多跳任务表现，并通过浅层到深层模型的线性映射测试。

Result: 更深层模型的第二半层贡献较小，跳层影响微弱；多跳任务中未见深度利用；线性映射显示更深模型仅将计算分散到更多层。

Conclusion: 更深模型并未利用深度进行新计算，而是通过更多层进行更精细的残差调整，解释了深度增加带来的收益递减。

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [333] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/abs/2505.13899)
*Zeyu Michael Li,Hung Anh Vu,Damilola Awofisayo,Emily Wenger*

Key words: 机器学习, 表示相似性, 数据集重叠, 任务重叠, Platonic Representation Hypothesis

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了数据集重叠和任务重叠如何影响机器学习模型之间的表示相似性，发现两者均与更高的相似性正相关且结合效果最强。

Motivation: 研究机器学习模型在不同模态中表现出相似表示的原因，探究数据集重叠和任务重叠这两个潜在因素。

Method: 通过一系列实验评估数据集重叠和任务重叠对模型表示相似性的影响。

Result: 数据集重叠和任务重叠均与模型表示相似性呈正相关，结合两者效果最佳。

Conclusion: 数据集和任务的重叠是导致模型表示相似性的重要因素，尤其是二者的结合。

Abstract: Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [334] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/abs/2505.13900)
*Zhanpeng Zhou,Yongyi Yang,Mahito Sugiyama,Junchi Yan*

Key words: 深度学习，训练动态，相变，混沌效应，神经切向核

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过引入时间窗口分析视角，揭示深度学习的两阶段性质，包括早期混沌效应和后期的锥形效应，阐明了网络从探索到稳定的动态转变。

Motivation: 研究深度神经网络学习动态中的相变现象，填补对训练过程中敏感探索和稳定细化阶段的理解空白。

Method: 通过注入微小参数扰动和跟踪经验神经切向核（eNTK）的演化，分析网络对扰动的响应和功能轨迹。

Result: 发现早期混沌效应和后期锥形效应，表明网络从对初始条件高度敏感的混沌状态过渡到稳定状态。

Conclusion: 两种效应提供了深度学习训练过程中从探索到稳定的动态结构视角。

Abstract: Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [335] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/abs/2505.13904)
*Fu Luo,Xi Lin,Mengyuan Zhong,Fei Liu,Zhenkun Wang,Jianyong Sun,Qingfu Zhang*

Key words: 神经网络组合优化, 车辆路径问题, 插入范式, L2C-Insert, TSP, CVRP

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于插入范式的新型学习方法L2C-Insert，用于解决车辆路径问题，相比传统顺序追加方法，显著提高了解决方案的灵活性和质量。

Motivation: 现有基于神经网络组合优化的方法通常采用顺序追加节点的范式，导致解决方案质量不佳。论文探索插入范式以克服这一限制。

Method: 提出L2C-Insert方法，通过策略性地将未访问节点插入当前部分解的任意有效位置，并引入模型架构、训练方案和推理技术三个关键组件。

Result: 在TSP和CVRP的合成和真实实例中，L2C-Insert在多种问题规模上均表现出更优的性能。

Conclusion: 插入范式的引入显著提升了神经网络组合优化在车辆路径问题中的表现。

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [336] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/abs/2505.13907)
*Junyu Luo,Yusheng Zhao,Xiao Luo,Zhiping Xiao,Wei Ju,Li Shen,Dacheng Tao,Ming Zhang*

Key words: 无监督学习,域自适应检索,图扩散,噪声鲁棒性,哈希学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为COUPLE的新方法，通过图扩散和渐进对齐解决无监督高效域自适应检索中的噪声问题和高层特征对齐不足，实验验证了其有效性。

Motivation: 现有方法在无监督高效域自适应检索中未能有效处理目标域的噪声，且直接对齐高层特征导致检索性能不佳。

Method: COUPLE方法通过构建跨域关系图、噪声鲁棒的图流扩散模拟转移动态，并利用分层Mixup操作进行渐进对齐。

Result: 实验表明COUPLE在竞争性基准测试中表现优异。

Conclusion: COUPLE通过图扩散和渐进对齐显著提升了无监督域自适应检索的性能。

Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [337] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/abs/2505.13910)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Key words: 深度学习,伪偏置,鲁棒性,潜在空间,预测捷径

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种无需组标签的后处理方法来缓解深度学习模型中的伪偏置问题。

Motivation: 深度学习模型常因学习目标与非必要特征之间的伪相关性而导致性能下降，现有方法需要昂贵的组标签标注。

Method: 提出ShortcutProbe框架，通过识别潜在空间中的预测捷径并重新训练模型以提升鲁棒性。

Result: 理论分析和实验表明，该方法能有效提升模型对伪偏置的鲁棒性。

Conclusion: ShortcutProbe是一种高效且实用的工具，可改善模型在不同数据集中的鲁棒性。

Abstract: Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [338] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu,Shaofeng Yin,Ningya Feng,Mingsheng Long*

Key words: 世界模型、强化学习、RLVR、自回归预测、生成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出RLVR-World框架，通过可验证奖励的强化学习直接优化世界模型的任务性能指标，显著提升了语言和视频模型的效果。

Motivation: 传统的最大似然估计（MLE）训练目标与世界模型的任务目标（如准确性或感知质量）存在不一致性，需要更直接的方法优化模型性能。

Method: 使用强化学习与可验证奖励（RLVR），在自回归预测标记化序列的基础上，通过解码预测的指标作为奖励信号优化模型。

Result: 在文本游戏、网页导航和机器人操作等领域中，语言和视频世界模型的性能显著提升。

Conclusion: RLVR作为一种后训练范式，为生成模型的功能提升提供了新的方向。

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [339] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/abs/2505.13938)
*Amitayush Thakur,Jasper Lee,George Tsoukalas,Meghana Sistla,Matthew Zhao,Stefan Zetzche,Greg Durrett,Yisong Yue,Swarat Chaudhuri*

Key words: 代码生成、Lean、形式验证、基准测试、程序合成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 介绍了一个名为CLEVER的高质量基准数据集，包含161个用于Lean语言端到端验证代码生成的问题。每个问题包括生成规范和实施的任务，且不使用测试用例监督或LLM生成的注释。

Motivation: 为了解决现有基准在代码生成和形式验证中的不足，如避免测试用例监督、实现逻辑泄露等问题，提出了更严格的评测标准。

Method: 使用Lean类型检查器后验验证生成代码的正确性，评估了基于最先进语言模型的少样本和代理方法。

Result: 现有方法在完全验证方面表现不佳，表明这是一个具有挑战性的前沿基准。

Conclusion: CLEVER为程序合成和形式推理提供了一个高难度的基准，相关数据和代码已开源。

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [340] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/abs/2505.13954)
*Jiahe Chen,Ziye Ma*

Key words: 大规模优化, 非凸问题, 一阶方法, 零阶方法, 方差减少

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: VAMO是一种结合一阶和零阶梯度优化的方差减少方法，用于高效解决大规模非凸问题，在收敛速度和计算效率上优于现有方法。

Motivation: 大规模非凸优化问题在机器学习中常见，但现有的一阶方法计算成本高，而零阶方法收敛慢。VAMO旨在平衡这两者。

Method: VAMO结合了一阶小批量梯度和轻量级零阶有限差分探测，在SVRG框架下设计了一种混合梯度优化方法。

Result: 实验表明，VAMO在传统神经网络训练和LLM微调中均表现优异，收敛速率不受维度影响，优于SGD和纯零阶方法。

Conclusion: VAMO为计算受限场景提供了一种高效、灵活的优化方案。

Abstract: Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [341] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/abs/2505.14005)
*Han Zhang,Yan Wang,Guanfeng Liu,Pengfei Ding,Huaxiong Wang,Kwok-Yan Lam*

Key words: 图神经网络,可解释性,XGNN,OPEN,样本空间分区

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: OPEN是一种新型GNN解释方法，突破现有方法局限，通过分区样本空间和学习不同分布下的决策逻辑，提升解释的全面性和普适性。

Motivation: 提升GNN的可信度和透明度，解决现有XGNN方法无法捕捉全局决策逻辑和对边属性和内部访问要求严格的问题。

Method: 提出OPEN，分区样本空间为多个环境，从每个环境中采样子图并分析预测，学习GNN的决策逻辑。

Result: 实验表明OPEN能捕捉近乎完整的决策逻辑，保真性优于现有方法，效率相当，且在现实场景中增强鲁棒性。

Conclusion: OPEN为GNN解释提供了全面且无需严格前提的方法，提升了性能和通用性。

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [342] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/abs/2505.14011)
*Yifei Jin,Xin Zheng,Lei Guo*

Key words: 司法量刑预测, 饱和机制量刑(SMS), 动量最小均方(MLMS), 解释性, 中国刑法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为饱和机制量刑（SMS）的模型，通过结合中国刑法提供内在法律解释性，并引入动量最小均方（MLMS）自适应算法。实验验证了模型的有效性和算法的准确性。

Motivation: 现有研究主要依赖端到端模型，缺乏量刑逻辑和可解释性，而这在学术研究和司法实践中至关重要。

Method: 提出SMS模型和MLMS自适应算法，并建立无数据稳定性和独立性假设的预测准确性数学理论。

Result: 使用真实数据集（CIBH）验证，模型预测准确性接近最佳理论上界。

Conclusion: SMS模型和MLMS算法解决了端到端模型的可解释性问题，且在实际应用中表现优异。

Abstract: Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [343] [Adversarial Training from Mean Field Perspective](https://arxiv.org/abs/2505.14021)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Key words: 对抗训练, 深度神经网络, 平均场理论, 网络容量, 上界

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究首次在不假设数据分布的情况下，对随机深度神经网络的对抗训练进行了理论分析，提出了一种基于平均场理论的新框架，并推导出多种情况的对抗损失上界。

Motivation: 对抗训练虽然有效，但其训练动态尚未被充分理解，本研究旨在填补这一理论空白。

Method: 采用平均场理论框架，分析随机深度神经网络的对抗训练，推导不同范数下的对抗损失上界。

Result: 证明无捷径的网络无法进行对抗训练，且对抗训练会降低网络容量，同时网络宽度能缓解这些问题。

Conclusion: 对抗训练的效果受网络结构影响，宽度和维度对训练动态有重要影响。

Abstract: Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [344] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/abs/2505.14024)
*Di Wu,Qian Li,Heng Yang,Yong Han*

Key words: 联邦学习, 未目标攻击, 鲁棒聚合, FedGraM, 嵌入空间, Gram矩阵

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为FedGraM的新型联邦学习鲁棒聚合方法，通过辅助数据集和Gram矩阵范数检测并移除未目标攻击，显著提升了模型在数据隐私保护下的安全性。

Motivation: 联邦学习（FL）易受未目标攻击影响，现有防御机制因数据异构性在实用环境中效果有限。作者旨在通过检测和移除攻击来减轻其影响。

Method: 提出FedGraM方法，服务器使用辅助数据集（每类一个样本）提取本地模型嵌入，计算Gram矩阵范数以衡量模型嵌入空间的类间分离能力，过滤范数最大的潜在恶意模型。

Result: 实验表明，FedGraM在有限数据下性能卓越，优于现有防御方法。

Conclusion: FedGraM通过嵌入空间分析和Gram矩阵范数检测攻击，有效提升了FL的抗攻击能力。

Abstract: Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [345] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/abs/2505.14033)
*Guoming Li,Jian Yang,Yifan Chen*

Key words: 图神经网络, 过滤, CPF, 异质图, 节点分类

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新的CPF方法，通过结合图级和节点级过滤，解决异质图分类问题，避免过度参数化。

Motivation: 传统图级过滤难以处理异质图，而节点级过滤可能过度参数化，需要统一框架优化。

Method: 引入CPF方法，通过结构感知和特征感知的分区过滤，结合图粗化和k均值聚类。

Result: CPF在节点分类和图异常检测任务中表现优异，优于其他范式。

Conclusion: CPF提供了一种有效统一的过滤框架，适用于同质和异质图任务。

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [346] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/abs/2505.14036)
*Gyubin Lee,Truong Nhat Nguyen Bao,Jaesik Yoon,Dongwoo Lee,Minsu Kim,Yoshua Bengio,Sungjin Ahn*

Key words: 扩散模型, 自适应推理, 计算资源分配, ABCD

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ABCD提出了一种自适应推理框架，通过双向扩散循环动态调整计算资源，提升生成效果和计算效率。

Motivation: 解决现有方法在推理时固定降噪调度的问题，无法根据实例难度或任务需求动态分配计算资源。

Method: 提出Adaptive Bi-directional Cyclic Diffusion (ABCD)框架，包含循环扩散搜索、自动探索-利用平衡和自适应思考时间。

Result: 实验表明ABCD在多种任务中提升性能的同时保持计算效率。

Conclusion: ABCD为动态调整推理计算提供了灵活高效的解决方案。

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [347] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/abs/2505.14039)
*Luca Pellegrini,Massimiliano Ghiotto,Edoardo Centofanti,Luca Franco Pavarino*

Key words: 傅里叶神经算子、离子模型、高维动力学、超参数调优、神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了傅里叶神经算子（FNO）在高维离子模型中学习状态变量演化的有效性，展示了其在多尺度非线性复杂动力学中的潜力。

Motivation: 离子模型因其刚性和多尺度非线性特性，用神经网络逼近具有挑战性。研究旨在探索FNO在高维系统中学习全部变量动态的能力。

Method: 使用FNO学习三种不同维度的离子模型（FitzHugh-Nagumo、Hodgkin-Huxley、O'Hara-Rudy），并通过自动超参数调优（无约束和有约束设置）优化模型性能。

Result: FNO在所有模型中实现了高精度学习，无约束架构在训练速度上表现更优，仅需一半的epoch即可达到相近误差水平。

Conclusion: FNO能够有效捕捉高维多尺度动力学系统的复杂行为，为离子模型的近似提供了一种可靠工具。

Abstract: Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [348] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei,Ming Lin,Fanjiang Ye,Fengguang Song,Liangliang Cao,My T. Thai,Tianbao Yang*

Key words: 模型引导, DRRho, 分布鲁棒优化, CLIP, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为“模型引导”的学习范式，通过理论驱动框架DRRho风险最小化，提升目标模型的训练效率和泛化能力，并在CLIP任务中验证其优越性。

Motivation: 现有模型引导方法缺乏理论基础，导致性能不佳，本文旨在填补这一空白并提供理论支持。

Method: 基于分布鲁棒优化（DRO）提出DRRho风险最小化框架，并扩展至CLIP任务，提出DRRho-CLIP方法。

Result: 理论分析和实验验证表明，该方法显著提升泛化能力和数据效率，且优于现有启发式方法。

Conclusion: DRRho框架为模型引导提供了理论依据，并在实践中表现出色，为未来研究奠定了基础。

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [349] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/abs/2505.14040)
*Jingyun Zhang,Hao Peng,Li Sun,Guanlin Wu,Chunyang Liu,Zhengtao Yu*

Key words: 图结构学习、深度结构熵、无监督聚类、GNN

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出DeSE框架，结合深度结构熵解决图结构稀疏和噪声问题，提升聚类效果。

Motivation: 现有图结构学习方法（如GNNs、GATs）依赖原始图结构，性能在稀疏或含噪情况下下降，且聚类方法未能充分捕捉节点间关系。

Method: DeSE通过深度结构熵量化结构信息，结合结构学习层（SLL）生成属性图优化原始图，并基于GNN的聚类分配方法（ASS）学习节点嵌入。

Result: 在四个基准数据集上优于八种基线方法，证明了DeSE的有效性和可解释性。

Conclusion: DeSE通过结构熵和深度学习改进图聚类，解决了图稀疏性和噪声问题。

Abstract: Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [350] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/abs/2505.14042)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Key words: 对抗训练, Transformer, 上下文学习, 鲁棒性, 多任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过对抗性预训练Transformer并在多任务上进行上下文学习的方法，从而在无需额外训练的情况下实现下游任务的鲁棒性。同时指出了模型的局限性。

Motivation: 对抗训练是有效的防御方法，但计算成本高。研究旨在通过对抗预训练的Transformer模型，避免在下游任务中进行对抗训练。

Method: 通过对抗预训练Transformer模型，并在多任务上使用上下文学习，无需参数更新即可实现鲁棒性。

Result: 对抗预训练的Transformer能够聚焦于鲁棒特征，抵抗攻击，并在未见任务中表现良好。但存在局限性，如准确性与鲁棒性的权衡。

Conclusion: 对抗预训练Transformer可以作为鲁棒基础模型，减少对抗训练的需求，但需注意其局限性。

Abstract: Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [351] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/abs/2505.14044)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Cheng Chen*

Key words: GCD, MTMC, 流形容能力, 核范数, 开放世界学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种新方法MTMC，通过最大化类标记的流形容能力提升GCD性能，优于现有方法。

Motivation: 传统GCD方法因过度关注最小化簇内差异，牺牲了流形容能力，限制了类内表示丰富性。

Method: MTMC利用核范数作为流形容能力度量，最大化类标记的流形容能力，保留数据多样性和复杂性。

Result: 在粗粒度和细粒度数据集上，MTMC提升了聚类精度和类别数量估计，增强了类间可分性。

Conclusion: MTMC是提升开放世界学习鲁棒性的关键，代码已开源。

Abstract: Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [352] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan,Deqing Fu,Julian Asilis,Ollie Liu,Dani Yogatama,Vatsal Sharan,Robin Jia,Willie Neiswanger*

Key words: 多模态大语言模型, 文本引导, 均值漂移, 稀疏自编码器

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，通过文本引导向量可以高效提升多模态大语言模型（MLLMs）的性能，且无需额外数据收集或计算开销。

Motivation: 多模态大语言模型（MLLMs）目前缺乏像文本大语言模型（LLMs）那样的有效引导方法。本文旨在探索是否可以通过文本LLM的引导向量来提升MLLMs的性能。

Method: 使用稀疏自编码器（SAEs）、均值漂移（mean shift）和线性探测（linear probing）从文本LLM中提取引导向量，并将其应用于MLLMs。

Result: 文本引导显著提升了MLLMs在多任务中的准确度，均值漂移在空间关系任务中表现尤为突出（CV-Bench上提升+7.3%），并展现出良好的泛化能力。

Conclusion: 文本引导向量是提升MLLMs性能的高效工具，且无需额外数据或计算资源。

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [353] [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)
*Xinyi Shang,Peng Sun,Fengyuan Liu,Tao Lin*

Key words: 数据中心化, 未标记数据优化, 深度学习, 可持续训练, CoOpt框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新颖的数据中心化范式CoOpt，通过优化未标记数据提升深度学习训练的效率和可持续性，实验显示在多个数据集上性能显著提升。

Motivation: 现有模型中心化方法存在知识锁定在模型参数中、难以重用和扩展的问题，限制了训练效率和可持续性。

Method: 提出了CoOpt框架，通过并行化协作优化未标记数据，将知识编码到数据本身，利用公共模型实现可扩展和可重用的训练流程。

Result: 实验表明，CoOpt在Tiny-ImageNet和ImageNet-1K上分别提升13.6%和6.8%的性能，训练速度分别加快1.94倍和1.2倍。

Conclusion: CoOpt通过数据中心化方法解决了现有模型瓶颈，显著提升了训练效率和性能，为可持续深度学习提供了新思路。

Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [354] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/abs/2505.14122)
*Ehsan Masoudian,Ali Mirzaei,Hossein Bagheri*

Key words: 野火风险, 伊朗, 遥感, 机器学习, 气候因素, 人类活动

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究通过遥感和机器学习分析伊朗野火风险，发现气候与人类活动均影响野火，且季节性分析中人类活动更显著。

Motivation: 探讨伊朗野火风险的多元因素，特别是气候条件与人类活动的相互作用。

Method: 利用遥感、GIS、云计算和机器学习算法，分析气候、地形和人类活动对野火易感性的影响。

Result: 气候因素（如土壤湿度、温度、湿度）和人类活动（如人口密度、电力线距离）均显著影响野火风险，季节性分析中人类活动作用更突出。

Conclusion: 研究提供了高分辨率野火易感性地图，强调伊朗特定区域的火灾管理策略需求。

Abstract: This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [355] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/abs/2505.14125)
*Viet Anh Khoa Tran,Emre Neftci,Willem. A. M. Wybo*

Key words: 任务调制对比学习（TMCL）, 预测编码, 对比损失, 无监督学习, 类别增量学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种受生物大脑启发的任务调制对比学习（TMCL）方法，通过预测编码原理和对比损失，实现无监督学习与稀疏标记数据的结合，解决了监督学习中的灾难性遗忘问题。

Motivation: 生物大脑能够在无标记数据流中进行持续学习，并能通过稀疏标记数据整合专有信息而不影响泛化能力，而机器学习方法在此场景下容易发生灾难性遗忘。

Method: 采用任务调制对比学习（TMCL），利用预测编码原理和对比损失构建视图不变的表征空间，并通过学习新的仿射调制实现新类别的区分，同时保持前馈权重不变。

Result: 实验表明，TMCL在类别增量学习和迁移学习中的表现优于最先进的无监督方法，甚至优于部分监督方法，且仅需1%的标记数据。

Conclusion: 研究表明，自上而下的调制在平衡稳定性和可塑性中起关键作用。	

Abstract: Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [356] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/abs/2505.14126)
*Yuan-Hao Jiang,Kezong Tang,Zi-Wei Chen,Yuang Wei,Tian-Yi Liu,Jiayi Wu*

Key words: 知识组件图, 多智能体系统, 大型语言模型, 学习路径识别, 教育优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为MAS-KCL的知识组件图结构学习算法，通过多智能体系统和大型语言模型优化教育领域的知识组件图，帮助教师更精准识别学习路径。

Motivation: 知识组件图能帮助教育者定位学习者表现不佳的根本原因，但现有方法需要更高效的自适应优化机制。

Method: 提出MAS-KCL算法，利用多智能体系统和双向反馈机制，动态调整知识组件图的边权重和学习路径概率分布。

Result: 在合成和真实教育数据集上的实验表明，算法能高效识别学习路径。

Conclusion: MAS-KCL通过精准识别学习路径，支持教师设计更全面的学习计划，促进教育可持续发展。

Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [357] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/abs/2505.14128)
*Yihang Du,Jiaying Hu,Suyang Hou,Yueyang Ding,Xiaobo Sun*

Key words: 空间标注、相似性测量、图模型、空间转录组

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为SLAM的框架，用于测量空间标注的相似性，综合考虑标签匹配、空间拓扑和异质性影响。

Motivation: 现有方法未能全面考虑空间标注的相似性测量需求，包括标签匹配、空间分布拓扑和异质性影响。

Method: 将空间标注转换为基于位置、标签和属性的图，提取其属性分布并计算分布差异。

Result: SLAM在模拟和真实空间转录组数据中表现优于其他评估指标，全面反映标注质量。

Conclusion: SLAM为空间标注相似性提供了一种全面、无偏的测量方法。

Abstract: Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [358] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/abs/2505.14136)
*Ryo Bertolissi,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Key words: Mixture of experts, model merging, test-time training, language models, cost-effective

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TTMM通过模型合并将MoE扩展到更多专家，近似测试时训练（TTT），性能接近TTT但计算成本低100倍。

Motivation: 解决当前MoE模型因训练和推理成本高而只能使用少量专家的问题，提供一种成本效益高的测试时训练扩展方法。

Method: 提出测试时模型合并（TTMM），利用模型合并减少测试时开销，并通过更多专家提升性能。

Result: TTMM性能随专家数量增加而提升，接近TTT，且1B参数模型下测试时速度快100倍。

Conclusion: TTMM是一种高效、成本低的测试时训练扩展方法，适用于大规模语言模型。

Abstract: Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [359] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/abs/2505.14139)
*Marvin Alles,Nutan Chen,Patrick van der Smagt,Botond Cseke*

Key words: 能量引导流匹配,流模型,强化学习,离线强化学习,FlowQ

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种称为能量引导流匹配的新方法，用于增强流模型的训练，并消除了推理时的引导需求。该方法通过学习条件速度场和近似高斯路径来优化策略训练，并在离线强化学习中展示了竞争力。

Motivation: 目前在扩散模型中，如何在训练阶段引入引导仍是一个较少探索的问题。本文旨在通过能量引导流匹配改进训练过程，同时避免推理时的额外引导需求。

Method: 作者提出了能量引导流匹配方法，通过近似高斯路径学习条件速度场，并将其应用于离线强化学习算法FlowQ中。该方法优化了策略训练的时间复杂度。

Result: 实验表明，FlowQ在性能上具有竞争力，同时策略训练时间与流采样步骤数无关。

Conclusion: 能量引导流匹配是一种有效的训练方法，尤其适用于目标分布由数据和能量函数共同定义的任务，如强化学习。

Abstract: The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [360] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/abs/2505.14161)
*Ting Wei,Biao Mei,Junliang Lyu,Renquan Zhang,Feng Zhou,Yifan Sun*

Key words: 个性化贝叶斯联邦学习,非参数化后验,Wasserstein重心聚合,变分推断

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedWBA是一种新型的个性化贝叶斯联邦学习方法，通过非参数化后验表示和基于Wasserstein重心聚合，解决了现有方法在后验推断和参数聚合中的限制。

Motivation: 现有PBFL方法存在参数化假设和简单平均聚合的局限性，FedWBA旨在提升本地推断和全局聚合的效果。

Method: 使用基于粒子的变分推断进行非参数化后验表示，并在服务器端引入Wasserstein重心聚合。

Result: 理论上证明本地变分推断和全局聚合的收敛性，实验显示FedWBA在准确性、不确定性校准和收敛速度上优于基线方法。

Conclusion: FedWBA在个性化贝叶斯联邦学习中表现出优越性，并具有稳健性。

Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [361] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/abs/2505.14170)
*Chen Zhang,Weixin Bu,Zeyi Ren,Zhengwu Liu,Yik-Chung Wu,Ngai Wong*

Key words: 图神经网络, 非参数教学, 图卷积网络, 训练效率, 泛化性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为GraNT的新范式，通过非参数教学视角重新解释图结构的属性学习过程，显著提升了GCN的训练效率，同时保持了泛化性能。

Motivation: 学习图结构数据属性（如分子溶解度）的隐式映射通常代价高昂，需要解决GCN训练中的效率问题。

Method: GraNT通过非参数教学框架，选择示例子集来加速GCN训练，并将GCN的参数更新重新解释为功能梯度下降。

Result: GraNT在图级回归、分类及节点级回归、分类任务中分别减少了36.62%、38.19%、30.97%和47.30%的训练时间，且保持泛化性能。

Conclusion: GraNT展示了非参数教学与结构敏感学习的一致性，显著提升了图属性学习器的效率。

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [362] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Key words: 大型语言模型, 安全对齐, 几何子空间, 微调, 防御策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了大型语言模型（LLMs）安全对齐的几何视角，发现安全行为与不安全行为在子空间中相互交织，挑战了局部对齐的假设。

Motivation: 研究旨在验证安全对齐是否可以通过几何方向隔离或保留，以防御微调导致的安全退化。

Method: 通过对参数和激活空间的全面实证研究，分析安全行为是否集中在特定子空间及其与通用学习的分离性。

Result: 结果显示安全与不安全行为在子空间中重叠，并未发现选择性控制安全的独立子空间。

Conclusion: 安全对齐并非几何局部化，而是源自模型学习动力学的复杂交互，表明基于子空间的防御策略存在根本局限。

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [363] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/abs/2505.14190)
*Ni Ding,Miao Qiao,Jiaxing Xu,Yiping Ke,Xiaoyu Zhang*

Key words: 生成对抗网络, Rényi测度, 梯度消失, 优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了α-GAN，一种基于Rényi测度的生成对抗网络，通过Rényi交叉熵构建价值函数，解决了传统GAN训练中的一些问题。

Motivation: 现有GAN在训练中常遇到梯度消失等问题，论文希望通过引入Rényi测度和调整α值，改善训练效率和稳定性。

Method: 使用Rényi交叉熵作为价值函数，通过α参数调节梯度大小，形成生成器和判别器的min-max问题。

Result: 实验表明，当α∈(0,1)时，梯度被指数放大，收敛速度加快，并可能解决梯度消失问题。

Conclusion: α-GAN在特定α范围内表现优于传统GAN，且尚未被充分探索，具有潜力解决GAN训练的常见问题。

Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [364] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/abs/2505.14201)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Key words: transformer, FlashAttention, FLASH-D, 硬件加速, softmax计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种改进的FlashAttention内核FLASH-D，通过简化数学公式隐藏softmax除法并减少计算成本，同时保持了原有内核的高效特性，硬件实现显示出显著的性能提升。

Motivation: 传统transformer注意力机制中的softmax计算导致计算效率低下，FlashAttention虽然已优化GPU计算，但仍需进一步简化和提升硬件效率。

Method: 重新评估FlashAttention内核，提出FLASH-D，通过隐藏softmax除法和稳定计算指数来简化公式，同时减少计算成本。

Result: FLASH-D在28nm硬件实现中平均减少了22.8%的面积和20.3%的功耗，且无性能损失。

Conclusion: FLASH-D在保持FlashAttention高效特性的同时，显著提升了硬件效率。

Abstract: The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [365] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/abs/2505.14202)
*Zhicheng Chen,Shibo Feng,Xi Xiao,Zhong Zhang,Qing Li,Xingyu Gao,Peilin Zhao*

Key words: 时间序列生成,离散令牌建模,多尺度,Transformer,理论分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种多尺度离散Transformer（MSDformer）方法，用于解决现有DTM方法无法捕捉多尺度时间模式和缺乏理论基础的局限性，显著提升了时间序列生成的质量。

Motivation: 现有的DTM方法无法捕捉复杂时间数据中的多尺度时间模式，且缺乏理论基础指导模型优化。

Method: 提出了MSDformer方法，通过多尺度时间序列分词器学习多尺度的离散令牌表示，并应用多尺度自回归令牌建模技术。

Result: 实验表明，MSDformer显著优于现有技术，理论分析和实验结果验证了多尺度信息的重要性。

Conclusion: 结合多尺度信息和建模可以显著提升基于DTM的时间序列生成质量。

Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [366] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/abs/2505.14206)
*Flavio Di Martino,Franca Delmastro*

Key words: mHealth, 生成对抗网络, 扩散模型, 时间序列, 多模态

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文系统评估了最新生成模型在多模态、长程依赖和条件生成方面的表现，揭示了现有方法的局限性，并提出了未来研究方向。

Motivation: 移动传感器数据在医疗健康（mHealth）领域有广泛应用，但数据收集受限，合成数据生成成为解决方案。

Method: 论文通过引入新的评估框架，系统评估了生成对抗网络和扩散模型在时间序列合成中的性能。

Result: 发现现有方法在跨模态一致性、时间连贯性和下游任务中表现不足。

Conclusion: 未来需改进生成模型，以提升合成时间序列的质量和在mHealth中的应用。

Abstract: The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [367] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/abs/2505.14211)
*Qu Wang,Yan Xia*

Key words: 动态网络, 链接预测, 张量轮分解, PID控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出一种基于PID控制的张量轮分解（PTWD）模型，用于动态网络的链接预测，通过结合TWD的表示能力和PID控制原理，显著提高了预测准确性。

Motivation: 动态网络中的链接预测需要捕捉时空模式和权重动态，传统静态方法难以胜任，而张量方法提供了潜在解决方案。

Method: 采用张量轮分解（TWD）捕获网络拓扑和权重动态，并引入PID控制优化模型参数学习过程。

Result: 在四个真实数据集上的实验表明，PTWD模型在链接预测上比其他模型更准确。

Conclusion: PTWD模型通过结合TWD和PID控制，有效提升了动态网络链接预测的性能。

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [368] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/abs/2505.14214)
*Mattes Mollenhauer,Nicole Mücke,Dimitri Meunier,Arthur Gretton*

Key words: ridge回归, 再生核希尔伯特空间, 高阶矩噪声, Fuk-Nagaev不等式

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了在存在高阶矩噪声的情况下，ridge回归在再生核希尔伯特空间中的性能，提出了基于积分算子框架的过风险界限。

Motivation: 探索ridge回归在更高阶矩噪声下的性能，填补以往仅针对亚指数噪声的研究空白。

Method: 使用积分算子框架，结合Fuk-Nagaev不等式处理希尔伯特空间随机变量。

Result: 证明了过风险界限由次高斯项和多项式项组成，收敛速率在标准特征值衰减条件下达到最优。

Conclusion: regularized最小二乘对重尾噪声具有渐近鲁棒性。

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [369] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/abs/2505.14217)
*Jorge Fabila,Lidia Garrucho,Víctor M. Campello,Carlos Martín-Isla,Karim Lekadir*

Key words: 联邦学习,肺结核诊断,低资源环境,非洲,胸部X光

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了在非洲低资源环境中使用联邦学习（FL）进行肺结核（TB）胸部X光诊断的可行性，研究了跨机构协作训练模型的实际效果。

Motivation: 解决传统集中式模型在隐私保护和数据稀缺方面的不足，推动AI在低资源医疗环境中的应用。

Method: 在八个非洲国家的研究中心和医院中，通过FL协作训练AI模型，比较本地模型与联邦模型的性能。

Result: FL在实现AI驱动的医疗保健方面显示出潜力，但在基础设施、互联网、数字素养和法规方面仍存在挑战。

Conclusion: FL在低资源地区具有应用前景，但需改进基础设施、教育和法规支持以实现广泛推广。

Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [370] [Fast and close Shannon entropy approximation](https://arxiv.org/abs/2505.14234)
*Illia Horenko,Davide Bassetti,Lukáš Pospíšil*

Key words: Shannon entropy, von Neumann entropy, Fast Entropy Approximation, machine learning, quantum computing

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了快速熵近似（FEA）方法，通过非奇异有理近似解决了熵及其梯度的计算问题，显著提升了计算速度和模型质量。

Motivation: 针对Shannon熵及其梯度计算中的高成本和低鲁棒性问题，提出一种更高效的近似方法以改善机器学习和量子计算中的工具性能。

Method: 采用非奇异有理近似（FEA）方法来近似Shannon熵及其梯度，仅需5至6次基本计算操作，显著减少计算量。

Result: FEA的平均绝对误差为10^{-3}，比现有技术低约20倍，计算速度提升50%，模型质量和特征提取速度显著提高。

Conclusion: FEA通过高效的非奇异近似，显著优化了熵计算工具的性能，为机器学习和量子计算提供了更快速、更经济的方法。

Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [371] [Learning with Local Search MCMC Layers](https://arxiv.org/abs/2505.14240)
*Germain Vivier-Ardisson,Mathieu Blondel,Axel Parmentier*

Key words: 组合优化、神经网络、局部搜索、模拟退火、Metropolis-Hastings

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种将不可精确组合求解器整合到神经网络中的理论方法，通过模拟退火和Metropolis-Hastings的联系，将局部搜索启发式转化为提案分布，以降低大规模学习中的计算负担。

Motivation: 现有方法缺乏理论保证或在依赖不可精确求解器时表现不佳，无法满足NP难问题的需求，需要一种新的学习方法。

Method: 通过模拟退火与Metropolis-Hastings的联系，将局部搜索启发式中的邻域系统转化为提案分布，实现可微组合层和损失函数。

Result: 该方法在动态车辆路径规划问题中验证了有效性，显著降低了计算负担。

Conclusion: 该工作为学习与不可精确组合求解器的结合提供了理论支撑，并在实际应用中表现出色。

Abstract: Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [372] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/abs/2505.14251)
*Bar Mahpud,Or Sheffet*

Key words: 差分隐私, 二阶矩估计, 可子采样性, zCDP, 异常值

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的差分隐私二阶矩估计算法，通过数据可子采样性假设，实现了强隐私-效用权衡，并在存在异常值的情况下保持高精度。

Motivation: 研究差分隐私下的二阶矩估计问题，旨在开发一种算法，即使对最坏情况输入也能实现隐私与效用的平衡。

Method: 基于数据可子采样性假设，引入递归算法框架，满足零集中差分隐私（zCDP），并在高概率下保持二阶矩估计的精度。

Result: 算法能够在存在异常值的情况下，高精度地近似分布的二阶矩矩阵。

Conclusion: 该算法通过可子采样性假设和递归框架，实现了差分隐私下的高精度二阶矩估计，适用于含异常值的数据。

Abstract: We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [373] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/abs/2505.14252)
*Mouad Elaarabi,Domenico Borzacchiello,Philippe Le Bot,Nathan Lauzeral,Sebastien Comas-Cardona*

Key words: 序列编码, 物理信息神经网络, 动态参数, 边界条件, 初始条件

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种结合序列编码和物理信息神经网络的方法，用于实时应用中动态参数、边界条件和初始条件的自适应建模。

Motivation: 传统方法在参数、边界条件或初始条件变化时需要重新训练模型，效率低。本文旨在解决这一问题，提升模型的适应能力。

Method: 采用Deep Sets或序列编码器对动态参数、边界条件和初始条件进行编码，作为物理信息神经网络的输入。

Result: 方法在Rossler ODE系统、2D Navier-Stokes问题和1D热监测问题中表现出鲁棒性和泛化能力。

Conclusion: 提出的架构能够有效适应参数和条件的变化，适用于多种动态系统。

Abstract: In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [374] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Key words: 强化学习, 大型语言模型, 推理能力, 组相对优势估计, AAPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了优势增强策略优化（AAPO），一种新的强化学习算法，通过动量增强优势估计解决现有组相对优势估计方法的训练低效问题，在数学推理基准测试中表现优异。

Motivation: 现有的基于强化学习的后训练方法（如GRPO）在消除价值模型依赖方面表现良好，但在估计优势接近零时仍存在训练低效问题，论文旨在解决这一问题。

Method: 提出AAPO算法，通过动量增强优势估计优化交叉熵损失，改进组相对优势估计的效率。

Result: 在多个数学推理基准测试中，AAPO表现出优于现有方法的性能。

Conclusion: AAPO通过增强优势估计有效提升了训练效率，为强化学习在语言模型推理能力增强中的应用提供了新思路。

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [375] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/abs/2505.14273)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Key words: 函数逼近, Kolmogorov-Arnold网络, XCSF, 局部模型, 进化规则

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: X-KAN提出了一种基于进化规则的新方法，通过局部Kolmogorov-Arnold网络（KAN）和XCSF框架的结合，显著提高了函数逼近的准确性，特别是在处理局部复杂或不连续函数时表现优异。

Motivation: 现有神经网络方法在处理局部复杂或不连续函数时表现不佳，因为它们依赖单一的全局模型。X-KAN旨在通过多局部模型优化解决这一问题。

Method: X-KAN将KAN的高表达性与XCSF的自适应分区能力相结合，通过规则前提定义局部区域，并在规则结果中使用局部KAN模型。

Result: X-KAN在人工测试函数和真实数据集上均显著优于传统方法（如XCSF、MLP和KAN），且平均仅需7.2±2.3条规则。

Conclusion: X-KAN验证了在XCSF中将KAN作为局部模型的有效性，并通过准确性和通用性评估规则适应性。

Abstract: Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [376] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen,Chaoyi Zhang,Jing Liu,Yutao Zeng,Zeyue Xue,Zhiheng Liu,Yunshui Li,Jin Ma,Jie Huang,Xun Zhou,Ping Luo*

Key words: 量化感知训练（QAT）、4比特精度（W4A4）、缩放定律、混合精度量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究提出了统一的量化感知训练（QAT）缩放定律，分析了4比特精度（W4A4）下的量化误差来源，并通过混合精度量化优化了性能。

Motivation: 大型语言模型（LLMs）对计算和内存资源要求高，量化感知训练（QAT）能降低精度但保持性能，但其缩放行为尚未充分理解，尤其是4比特精度下的量化误差来源。

Method: 提出了统一的QAT缩放定律，通过268次实验量化误差与模型大小、训练数据量和量化组大小的关系，并分解了误差来源。

Result: 量化误差随模型增大而减小，随训练数据增加和粒度变粗而增加；FC2层的激活量化误差是主要瓶颈；混合精度量化可以平衡权重量化与激活量化误差。

Conclusion: 研究为QAT的优化提供了关键见解，特别是在4比特精度下，混合精度量化和关注权重量化误差的重要性。

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [377] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/abs/2505.14312)
*Kyungeun Lee,Moonjung Eo,Hye-Seung Cho,Dongmin Kim,Ye Seul Sim,Seoyoon Kim,Min-Kook Suh,Woohyung Lim*

Key words: 表格数据, 模型评估, 数据特性, 归纳偏置, MultiTab

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MultiTab提出了一个多维度的表格数据学习算法评估框架，强调模型行为在不同数据特性下的变化，而非仅依赖平均性能。

Motivation: 现有的表格数据评测大多基于平均性能指标，无法揭示模型在不同数据特性下的行为差异。

Method: MultiTab通过分类196个公开数据集的关键特性（如样本量、标签不平衡等），并评估13种代表性模型在不同特性下的表现。

Result: 模型性能对数据特性高度敏感，例如样本级相似性模型在大样本或高特征相关性数据中表现优异，而依赖特征间关系的模型在弱相关数据中表现最佳。

Conclusion: MultiTab揭示了归纳偏置并非总是如预期有效，并强调了基于数据特性的评估对模型选择和设计的重要性。

Abstract: Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [378] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/abs/2505.14338)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Jack Stade,Amir Yehudayoff*

Key words: ReLU网络、连续分段线性函数、深度、表达能力、最大值函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了ReLU神经网络的表达能力，特别是其深度。现有的研究认为$⌈ ⅃₃(n+1) ⌉$层足以计算所有连续分段线性函数（CPWL），但作者证明只需$⌈⅃₃(n-1)⌉+1$层即可，打破了之前的猜想。方法的关键是在两层网络中实现五输入最大值函数。

Motivation: 探索ReLU神经网络的表达能力，尤其是深度对其计算CPWL函数的影响，并验证Hertrich等人的猜想是否正确。

Method: 通过构造性证明，展示了两层ReLU网络可以精确表示五输入最大值函数，并推广到一般情况。

Result: $⌈⅃₃(n-1)⌉+1$层足以计算所有$ℝ^n$上的CPWL函数，改进前人结果。

Conclusion: ReLU神经网络的深度需求比之前认为的更小，为理论研究和实际应用提供了新视角。

Abstract: This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [379] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/abs/2505.14345)
*Aydin Abedinia,Shima Tabakhi,Vahid Seydi*

Key words: 半监督学习, 距离加权, 噪声数据集, 不平衡数据集, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于距离加权的半监督学习框架，通过优先处理接近测试数据的关键样本提升分类性能，并在噪声和不平衡数据集中表现出色。

Motivation: 解决标记数据有限及噪声和不平衡数据集下的半监督学习问题，提升分类模型的泛化能力和鲁棒性。

Method: 采用基于距离的加权机制和不确定性一致性技术，结合图表示方法，优先处理信息量大的样本。

Result: 在12个基准数据集上的实验显示，该方法在准确率、精确率和召回率上显著优于现有方法。

Conclusion: 该框架为半监督学习提供了鲁棒且实用的解决方案，适用于医疗和安全等数据受限领域。

Abstract: Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [380] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/abs/2505.14352)
*Bartosz Cywiński,Emil Ryd,Senthooran Rajamanoharan,Neel Nanda*

Key words: 语言模型, 隐藏知识, 可解释性, 自编码器, 可信AI

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了如何从语言模型中提取隐藏知识，提出了一种Taboo模型，并评估了黑盒和可解释性方法的效果。

Motivation: 随着语言模型的强大和复杂化，确保其可信与可靠至关重要。研究发现模型可能试图欺骗或隐藏秘密，因此需要探索提取隐藏知识的能力。

Method: 训练了一个Taboo模型，该模型描述特定秘密词但不直接提及。随后评估了黑盒方法和基于机制可解释性技术（如logit lens和稀疏自编码器）的策略。

Result: 两种方法在概念验证中均能有效提取秘密词，展示了提取隐藏知识的潜力。

Conclusion: 研究为从语言模型中提取隐藏知识提供了初步方案，为未来改进方法奠定了基础，有助于模型的可靠部署。

Abstract: As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [381] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/abs/2505.14371)
*Anh Duc Nguyen,Ilia Markov,Frank Zhengqing Wu,Ali Ramezani-Kebrya,Kimon Antonakopoulos,Dan Alistarh,Volkan Cevher*

Key words: 深度神经网络, 层间异质性, 量化框架, 变分不等式, QODA算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个针对现代深度神经网络的层间异质性问题的一般性量化框架，并应用于分布式变分不等式（VIs）中，开发了QODA算法，显著提升了训练速度。

Motivation: 现代深度神经网络由于结构和表示的异质性，影响了预测效果，需要一种适应异质性的量化方法。

Method: 提出层间量化框架，结合分布式变分不等式，开发了QODA算法，采用自适应学习率。

Result: QODA在训练Wasserstein GAN时，比基线方法快150%。

Conclusion: 层间量化框架和QODA算法能有效适应异质性，提升分布式训练效率。

Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [382] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/abs/2505.14388)
*Prasanna Parasurama,Panos Ipeirotis*

Key words: 招聘算法, 多样性, 公平性, 性别平衡, 筛选标准

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究发现，通过算法强制性别平衡的候选名单并不能直接提升最终招聘的多样性，关键在于筛选标准与管理者评估标准的相关性。提出新算法显著提升多样性。

Motivation: 探讨算法在招聘中如何更有效地提升公平性和多样性。

Method: 理论分析与大规模实证研究（80万份求职申请），并提出改进算法。

Result: 强制性别平衡的候选名单对多样性提升有限，新算法显著改善性别多样性。

Conclusion: 算法设计选择对实现多样性目标至关重要，需避免筛选与管理者偏好高度相关。

Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [383] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/abs/2505.14407)
*Aniket Salvi,Gereon Weiss,Mario Trapp*

Key words: 机器学习, 运行时监控, 模糊逻辑, 安全性, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于模糊逻辑的运行时监控器，用于机器学习感知组件，提供可解释的错误预测并提升系统安全性。

Motivation: 现有运行时监控器缺乏可解释性，阻碍了系统安全性和可靠性的强保证。

Method: 引入一种新颖的模糊监控器，评估感知组件的可靠性并解释操作条件的影响。

Result: 在自动驾驶案例中，监控器显著提升了安全性，同时保持了可用性。

Conclusion: 模糊监控器优于现有技术，提供可解释性并提升系统安全性。

Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [384] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/abs/2505.14411)
*Leon Götz,Marcel Kollovieh,Stephan Günnemann,Leo Schwinn*

Key words: 时序列分析, 标记化, 模式识别, 条件解码, 自适应压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于模式的时序列标记化方法，通过合并样本和模式为标记，自适应压缩时序列，提升效率和性能。

Motivation: 现有时序列标记化方法在处理简单模式时会产生过多标记，导致计算开销大，因此需要更灵活的方法。

Method: 基于频繁模式的离散词汇表，将样本与底层模式合并为标记，并引入无梯度计算的条件解码作为后优化方法。

Result: 该方法在时序列预测中性能提升36%，效率提升1990%，条件解码进一步降低MSE达44%。

Conclusion: 提出的方法能自适应处理多样化时态模式，生成有意义的标记表示，并能泛化到未见数据。

Abstract: Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [385] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/abs/2505.14415)
*Myung Jun Kim,Félix Lefebvre,Gaëtan Brison,Alexandre Perez-Lebel,Gaël Varoquaux*

Key words: 表格学习，预训练模型，语义捕捉，向量表示，知识增强

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TARTE是一个面向表格数据的预训练基础模型，通过字符串捕捉语义，生成知识增强的向量表示，提升了预测性能和计算效率。

Motivation: 解决表格数据中数值条目语义依赖于上下文的问题，并克服现有预训练模型需要微调、计算成本高、难以复用或与其他架构结合的局限。

Method: 提出了TARTE，一种基于字符串捕捉语义的预训练模型，将表格转化为知识增强的向量表示。

Result: TARTE生成的表示能低成本辅助后续学习，提升了预测性能并优化了预测与计算的权衡。

Conclusion: TARTE展示了表格学习中知识预训练的有效方法，为领域特定任务提供了高效表示。

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [386] [Explaining Neural Networks with Reasons](https://arxiv.org/abs/2505.14424)
*Levin Hornischer,Hannes Leitgeb*

Key words: 神经网络、可解释性、原因向量、多义性、逻辑贝叶斯

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于数学哲学理论的新颖神经网络可解释性方法，通过计算神经元的“原因向量”来逻辑和贝叶斯角度解释神经元及其分组。

Motivation: 解决神经网络解释性问题，尤其是神经元的多义性，同时结合逻辑和贝叶斯视角，提供更全面的解释。

Method: 通过计算神经元的“原因向量”，并评估其对不同命题的支持程度，结合前向传播实现高效解释。

Result: 理论上和实验证明该方法具有哲学基础、普适性、高效性、忠实性、正确性、可训练性和实用性。

Conclusion: 该方法为神经网络的可解释性提供了创新且实用的解决方案，同时提升了模型的鲁棒性和公平性。

Abstract: We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [387] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/abs/2505.14428)
*Riccardo D'Elia*

Key words: 深度学习, 系统动力学, 可解释性, 因果机器学习, 自主系统

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该提案旨在结合深度学习和系统动力学的优势，提出一种可解释的神经系统动力学框架，以解决深度学习缺乏可解释性和系统动力学难以扩展的问题。

Motivation: 深度学习在复杂模型和预测方面表现优异，但缺乏可解释性和因果可靠性；系统动力学提供透明性和因果洞察，但难以扩展且需要专业知识。

Method: 提出神经系统动力学管道，结合基于概念的可解释性、机制可解释性和因果机器学习，整合深度学习的预测能力和系统动力学的可解释性。

Result: 框架兼具因果可靠性和可扩展性，将通过欧盟AutoMoTIF项目验证其效果。

Conclusion: 长期目标是收集支持自主系统可解释性和安全性的可行见解。

Abstract: The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [388] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/abs/2505.14451)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Key words: 缺失值填补, 高维数据, MNAR, Mamba去噪网络, RefiDiff

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为RefiDiff的创新框架，用于处理高维混合类型数据集中的缺失值问题，尤其在MNAR机制下表现优异。结合局部机器学习预测和Mamba去噪网络，显著提升性能。

Motivation: 高维混合类型数据集在MNAR机制下的缺失值问题是现有方法的短板，急需一种能整合局部和全局特征的方法。

Method: RefiDiff框架结合局部预测和Mamba去噪网络，通过预精细化和后精细化提升稳定性与准确性，统一编码混合类型数据。

Result: RefiDiff在多种缺失值设置下超越现有方法，尤其在MNAR中表现突出，训练速度比DDPM快4倍。

Conclusion: 在九种真实数据集上验证了RefiDiff的鲁棒性、可扩展性和对复杂缺失模式的处理能力。

Abstract: Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [389] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.14459)
*Kamal Singh,Sami Marouani,Ahmad Al Sheikh,Pham Tran Anh Quang,Amaury Habrard*

Key words: 强化学习, 网络控制, 可解释性, Kolmogorov-Arnold网络, 负载均衡

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于Kolmogorov-Arnold Networks (KAN)的可解释强化学习方法，用于网络控制中的负载均衡，通过提取控制器方程提供决策过程的透明性。

Motivation: 现有强化学习方法在网络控制问题中缺乏可解释性，且难以提取控制器方程，限制了其实际应用。

Method: 使用PPO代理结合1层KAN actor模型和MLP Critic网络，学习负载均衡策略以最大化吞吐量效用并最小化损失和延迟。

Result: 通过不同奖励函数的评估，证明该方法能有效提升网络性能，同时提供可解释的策略。

Conclusion: KAN模型为强化学习在网络控制中的应用提供了可解释性和透明性，是解决现有问题的有效方法。

Abstract: Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [390] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/abs/2505.14463)
*Xinxin Fan,Wenxiong Chen,Mengfan Li,Wenqi Wei,Ling Liu*

Key words: 图对抗攻击, 韧性状态, 动态系统, 防御方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究图对抗攻击的内在韧性状态，提出了一种理论框架和动态系统方法，显著优于现有防御方法。

Motivation: 探究图对抗攻击中是否存在内在韧性状态，并找到这种关键状态。

Method: 将图对抗学习建模为多目标动态系统，提出理论框架证明韧性状态存在，并开发一维函数捕捉动态变化。

Result: 在五个常用数据集和三种代表性攻击下，提出的方法显著优于现有防御技术。

Conclusion: 图对抗攻击存在内在韧性状态，通过动态系统方法可以有效捕捉并防御攻击。

Abstract: Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [391] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/abs/2505.14468)
*Yifan Sui,Hao Wang,Hanfei Yu,Yitao Hu,Jianxun Li,Hao Wang*

Key words: Serverless computing, LoRA, LLM inference, resource optimization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 当前的无服务器计算在通用LLM推理中表现良好，但在LoRA推理中存在参数冗余、加载延迟高和资源竞争等问题。ServerlessLoRA通过共享骨干LLM、预加载LoRA工件和资源优化，显著提升了性能和降低了成本。

Motivation: 解决当前无服务器计算在LoRA推理中的参数冗余、加载延迟和资源竞争问题，以提升性能和降低成本。

Method: 提出ServerlessLoRA系统，包括骨干LLM共享、LoRA工件预加载和资源管理优化技术。

Result: 实验证明，ServerlessLoRA将TTFT降低86%，成本降低89%。

Conclusion: ServerlessLoRA为LoRA LLM推理提供了更高效、更低成本的解决方案。

Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [392] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/abs/2505.14477)
*Maria Panagiotou,Lorenzo Brigato,Vivien Streit,Amanda Hayoz,Stephan Proennecke,Stavros Athanasopoulos,Mikkel T. Olsen,Elizabeth J. den Brok,Cecilie H. Svensson,Konstantinos Makrilakis,Maria Xatzipsalti,Andriani Vazeou,Peter R. Mertens,Ulrik Pedersen-Bjergaard,Bastiaan E. de Galan,Stavroula Mougiakakou*

Key words: 个性化胰岛素治疗、强化学习、1型糖尿病、2型糖尿病、血糖管理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究提出了基于强化学习的个性化胰岛素治疗推荐方法ABBA，用于1型和2型糖尿病患者的血糖管理，通过模拟测试显示其相比标准方法显著提高了血糖达标时间和减少了血糖异常时间。

Motivation: 尽管胰岛素制剂和技术有所进步，调整胰岛素剂量对大多数1型和长期2型糖尿病患者仍具挑战性，因此需要开发更个性化的胰岛素治疗推荐方法。

Method: 研究开发了ABBA（Adaptive Basal-Bolus Advisor），通过强化学习为患者提供个性化胰岛素剂量建议，并在FDA认可的模拟人群中进行了测试。

Result: 模拟测试显示，ABBA显著提高了血糖达标时间，减少了低血糖和高血糖时间，且性能持续提升，效果优于标准方法。

Conclusion: ABBA有望进一步优化血糖控制，支持患者的日常自我管理，值得在人体试验中进一步验证。

Abstract: Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [393] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/abs/2505.14502)
*Wenze Liu,Xiangyu Yue*

Key words: 扩散模型, ODE积分, 割线损失, 蒙特卡洛积分

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过“割线损失”学习ODE积分的方法，以平衡性能和成本，加速扩散模型推理。

Motivation: 在极小步长下，数值求解器性能不佳，而蒸馏技术则复杂且不稳定，需要一种中间策略来平衡。

Method: 通过蒙特卡洛积分和皮卡迭代，从导数-积分关系中推导出损失函数（割线损失），逐步将切线扩展到割线。

Result: 实验表明，EDM和SiT-XL/2模型的割线版本在CIFAR-10和ImageNet-256×256上均取得优异结果。

Conclusion: 割线损失能快速调整预训练扩散模型，显著提升推理效率。

Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [394] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/abs/2505.14512)
*Juliusz Ziomek,George Whittle,Michael A. Osborne*

Key words: 神经网络, 层归一化, 神经切线核, 泛化, 稳定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了神经网络在训练分布之外的行为，发现无限宽网络中的层归一化能使输出保持稳定，而无层归一化的网络可能产生极端输出。理论结果通过实验验证，并应用于实际问题。

Motivation: 理解神经网络在训练分布之外的泛化行为，尤其是层归一化对其稳定性的影响。

Method: 应用神经切线核理论分析无限宽网络，结合层归一化，通过理论证明和实验验证。

Result: 层归一化能显著抑制网络在训练分布之外的输出波动，而无层归一化的网络可能表现不稳定。

Conclusion: 层归一化在神经网络的泛化稳定性中起关键作用，解决了传统方法的局限性。

Abstract: In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [395] [Latent Flow Transformer](https://arxiv.org/abs/2505.14513)
*Yen-Chen Wu,Feng-Ting Liao,Meng-Hsi Chen,Pei-Chen Ho,Farhang Nabiei,Da-shan Shiu*

Key words: 流变换器, 语言模型, 流匹配, 层压缩, Flow Walking

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种称为潜在流变换器（LFT）的方法，通过将多个离散层替换为单个学习的传输算子，显著压缩模型尺寸，同时保持性能，并且提出了Flow Walking算法以优化流匹配方法。

Motivation: 传统大语言模型的层数虽多但效率不高，而连续层在图像生成中表现出色。论文旨在探索如何将连续层的优势引入语言模型。

Method: 提出的LFT方法采用流匹配训练的传输算子替代多个离散层，并引入Flow Walking算法以保持耦合性。

Result: 在Pythia-410M模型上，LFT成功压缩了6层（共24层），性能优于直接跳过2层的方法。进一步使用Flow Walking算法时，能将12层压缩至1层。

Conclusion: LFT的设计展示了在语言模型中结合连续层优势的可行性，显著缩短了自回归和流式生成方法之间的差距。

Abstract: Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [396] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/abs/2505.14522)
*Mahmuda Akhter Nishu,Chenyu Huang,Milad Roohi,Xin Zhong*

Key words: 风害预测,双流学习,韧性规划,社区脆弱性,随机森林,RoBERTa

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种可解释的双流学习框架，结合数值天气数据和文本事件描述，用于预测风害风险，特别针对服务不足的部落社区，实验显示其性能优于传统方法。

Motivation: 现有的风害预测系统主要关注气象因素，忽略社区特定脆弱性，限制了本地化风险评估和韧性规划的效用。本文旨在弥补这一空白。

Method: 采用基于随机森林和RoBERTa的双流学习框架，通过后期融合机制结合数值天气数据和文本事件描述。

Result: 实验结果表明，该模型的性能显著优于传统基线方法，并通过敏感性分析增强了模型的透明度。

Conclusion: 该框架不仅预测效果显著，还具有实际应用价值，可用于支持应急准备和提升社区韧性。

Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [397] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/abs/2505.14531)
*Shaoye Luo,Xinxin Fan,Quanliang Jing,Chi Lin,Mengfan Li,Yunfeng Lu,Yongjun Xu*

Key words: 后门攻击, Ising模型, Hopfield网络, 触发器净化, SifterNet

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于Ising模型的通用且与模型无关的触发器净化方法SifterNet，用于防御卷积神经网络和视觉Transformer大型模型中的后门攻击。

Motivation: 现有触发器检测/移除方法通常需要预先了解目标模型的详细信息、大量干净样本或模型重新训练权限，限制了实际应用。理想的防御方法应在不了解目标模型的情况下消除植入的触发器。

Method: 通过Hopfield网络的记忆关联功能，提出轻量级黑盒防御方法SifterNet，利用Ising模型的思想净化输入样本中的触发器。

Result: 实验表明，SifterNet在触发器净化和准确性方面表现优异，优于现有基准方法。

Conclusion: SifterNet是一种有效的通用后门攻击防御方法，无需目标模型详细信息即可实现高性能。

Abstract: Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [398] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/abs/2505.14533)
*Mohammad Irfan Uddin,Nishad Tasnim,Md Omor Faruk,Zejian Zhou*

Key words: Spike-Transformer, 强化学习, SNNs, 能效, LIF神经元

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种结合SNNs能效与强化学习决策能力的新算法STRL，通过改进的LIF神经元和注意力机制优化表现，实验验证其在能效和策略性能上的优越性。

Motivation: 解决传统Transformer在强化学习中高能耗问题，利用SNNs的生物启发结构实现高效能机器学习模型。

Method: 设计了多步LIF神经元和注意力机制的SNN，结合状态、动作和奖励编码，构建类Transformer结构。

Result: 在先进基准测试中，STRL相比传统Transformer显著提升了策略性能，同时提高了能源效率。

Conclusion: STRL展示了在复杂现实决策场景中部署生物启发、低成本机器学习模型的潜力。

Abstract: Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [399] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/abs/2505.14535)
*Jiangrong Shen,Yulin Xie,Qi Xu,Gang Pan,Huajin Tang,Badong Chen*

Key words: 多模态脉冲神经网络；时态注意力；自适应融合；能效；模态不平衡

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种时态注意力引导的自适应融合框架，解决多模态脉冲神经网络中的模态不平衡和时态不对齐问题。

Motivation: 多模态脉冲神经网络在高效感知处理中潜力巨大，但面临模态不平衡和时态不对齐的挑战。现有方法模态收敛速度不协调且融合机制静态，无法适应时变跨模态交互。

Method: 引入时态注意力引导的自适应融合（TAAF）模块和时态自适应平衡融合损失，动态调整模态重要性和学习率。

Result: 在CREMA-D、AVE和EAD数据集上取得77.55%、70.65%和97.5%的准确率，能效高且解决了时态不对齐问题。

Conclusion: 该框架为神经形态系统中的时态一致多模态学习提供了新范例，弥合了生物感知处理与高效机器智能的鸿沟。

Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [400] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/abs/2505.14543)
*Utsav Dutta,Sina Khoshfetrat Pakazad,Henrik Ohlsson*

Key words: 时间序列, 基础模型, 嵌入学习, CHARM, 联合嵌入预测架构

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了CHARM，一种用于多元时间序列的基础嵌入模型，通过学习共享、可迁移和领域感知的表示，解决了传统时间序列模型的局限性。

Motivation: 传统时间序列模型通常是任务特定的，且依赖于数据集特定的训练和复杂的特征工程。尽管基于Transformer的架构提高了可扩展性，但时间序列领域的基础模型仍未被充分探索，且主要用于预测任务。

Method: CHARM通过结合通道级文本描述的架构创新，并保持对通道顺序的不变性，解决了时间序列基础学习的独特困难。模型采用联合嵌入预测架构（JEPA），结合新的增强方案和设计用于提高可解释性和训练稳定性的损失函数进行训练。

Result: 7M参数的CHARM模型在多样下游任务中实现了最先进的性能，为时间序列表示学习设立了新基准。

Conclusion: CHARM通过学习共享、可迁移和领域感知的表示，显著提升了时间序列模型的性能和应用范围。

Abstract: Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [401] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/abs/2505.14555)
*Yingtao Luo,Shikai Fang,Binqing Wu,Qingsong Wen,Liang Sun*

Key words: 天气预报,深度学习,物理规律,降尺度,效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种结合物理方程与深度学习的天气预报框架PhyDL-NWP，显著提升预测效率与物理一致性。

Motivation: 传统数值天气预报方法计算密集且物理不完整，深度学习模型虽高效但缺乏物理规律约束。

Method: 整合物理方程与潜在力参数化到数据驱动模型中，通过自动微分计算物理项，并使用物知损失对齐预测与物理规律。

Result: 实现了分辨率无关的降尺度，推理速度提升170倍，仅需55K参数。

Conclusion: PhyDL-NWP在提升预测性能的同时增强了物理一致性。

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [402] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/abs/2505.14564)
*David Krame Kadurha,Domini Jocema Leko Moutouo,Yae Ulrich Gaba*

Key words: 强化学习, Banach空间, Bellman算子, 收敛性, 算法优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过回顾拓扑学基础，探讨了强化学习中状态、动作和策略空间的结构，并利用数学工具（如Banach压缩原理）解释了RL算法的收敛性。

Motivation: 旨在通过数学理论（如Banach不动点定理和Bellman算子）增强RL算法的效率，并改进标准环境下的性能。

Method: 利用Banach空间的数学工具分析Bellman算子，提出替代性算子形式，并在MountainCar等标准RL环境中验证效果。

Result: 研究表明，对RL的数学理解可以优化算法设计，提升收敛速度和性能。

Conclusion: 数学理论为RL算法设计提供了新视角，能有效提升决策问题的解决效率。

Abstract: This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [403] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/abs/2505.14566)
*Andrei Cozma,Landon Harris,Hairong Qi*

Key words: 强化学习, Koopman算子理论, 近端策略优化, 非线性动态系统, 线性化表征

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合Koopman算子理论与近端策略优化（PPO）的新方法KIPPO，通过在高维空间中学习近似线性的系统动态表征，提高了策略学习的稳定性和性能。

Motivation: 针对强化学习中复杂非线性动态环境下策略梯度方法的不稳定性和高方差问题，研究如何利用Koopman算子理论的线性化优势改进策略优化。

Method: 提出KIPPO方法，通过一个Koopman近似辅助网络学习系统的近似线性潜在空间表征，同时将其集成到基线PPO算法中，不改变核心策略或价值函数的结构。

Result: 实验结果显示，KIPPO在连续控制任务中性能提升6-60%，且减少变异性高达91%。

Conclusion: KIPPO通过结合Koopman算子理论与PPO，显著提升了策略学习的稳定性和性能。

Abstract: Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [404] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/abs/2505.14592)
*Alexandre Broggi,Nathaniel Bastian,Lance Fiondella,Gokhan Kul*

Key words: 神经网络剪枝, 泛化能力, 网络安全, 模型优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了神经网络剪枝方法在网络安全数据集上的泛化能力，发现仅有少量方法适用。

Motivation: 探索神经网络剪枝方法在简化网络结构中的泛化能力，以优化模型大小和推理速度。

Method: 通过不同剪枝程度分析多种神经网络剪枝算法在新数据集上的表现。

Result: 大多数剪枝方法泛化能力不足，仅少数表现良好。

Conclusion: 针对特定任务需选择合适剪枝方法，现有方法泛化性有限。

Abstract: Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [405] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz,Hesam Salehipour,Adrian Butscher,Nigel Morris*

Key words: 降阶建模, 物理信息模型, 可微求解器, 泛化能力, 长期预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了物理信息降阶模型（Φ-ROM），通过将可微PDE求解器集成到训练过程中，显著提高了模型的泛化能力和长期预测性能。

Motivation: 现有的降阶模型（ROM）在训练过程中未包含高保真数值求解器，导致潜在动力学偏离实际物理规律，限制了模型的泛化和预测能力。

Method: 提出Φ-ROM模型，通过直接利用求解器中的物理方程来塑造潜在空间动力学及其对PDE参数的依赖关系，确保与原始系统的强一致性。

Result: Φ-ROM在泛化性、长期预测、时空连续性以及数据成本方面均优于现有方法，即使在稀疏和不规则的观测数据下也能恢复和预测解场。

Conclusion: Φ-ROM通过融合物理信息提升了降阶模型的性能，提供了一个适用于多种PDE系统和可微求解器的灵活框架。

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [406] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/abs/2505.14596)
*Isabella Degen,Zahraa S Abdallah,Henry W J Reeve,Kate Robson Brown*

Key words: 时间序列聚类,相关性结构,合成基准,评估方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CSTS是一个用于评估多元时间序列数据中相关性结构发现的合成基准，旨在帮助研究者区分聚类失败的原因，并提供可扩展的数据生成框架。

Motivation: 为了解决时间序列聚类中缺乏真实基准的问题，使研究者能客观评估聚类质量和算法表现。

Method: 提出CSTS基准，包含不同的相关性结构、系统变化的数据条件、性能阈值和评估协议。

Result: 实证结果显示相关性结构在下采样时中度失真，而对分布偏移和稀疏化影响较小。

Conclusion: CSTS为基于相关性的时间序列聚类提供了严格的评估标准，并能精确诊断方法局限性。

Abstract: Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [407] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/abs/2505.14606)
*Maksim Zhdanov,Vladislav Kurenkov*

Key words: 神经网络原子间势、泊松方程、自监督学习、静电相互作用、物理约束

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为Φ-Module的插件模块，通过将泊松方程融入消息传递框架，以自监督方式学习静电相互作用，显著提升了神经网络原子间势的准确性。

Motivation: 现有的深度学习模型在原子间势预测中通常缺乏基于物理定律的辅助约束，这可能导致训练效率低或预测准确性不足。

Method: 引入Φ-Module模块，通过离散泊松方程约束原子间势学习，并推导出静电能量项，与现有神经网络势能模型无缝集成。

Result: 在OE62和MD22基准测试中，使用Φ-Module的模型显著优于基线，误差分别降低了4.5%至17.8%和5/14案例中的最佳结果。

Conclusion: Φ-Module通过嵌入基于第一性原理的约束，显著提升了性能，同时保持了超参数友好、内存高效和训练轻量化的特点。

Abstract: Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [408] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/abs/2505.14610)
*Hao Wang,Chenyu Shi,Angel E. Rodriguez-Fernandez,Oliver Schütze*

Key words: MMD, 多目标优化, Newton方法, 进化算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出使用最大均值差异（MMD）解决连续多目标优化问题（MOPs），通过分析梯度和Hessian矩阵，设计了一种新的MMD-based Newton方法（MMDN），并与多目标进化算法（MOEA）结合，实验证实混合算法优化效果更佳。

Motivation: 多目标优化问题中，常用Hausdorff距离衡量近似Pareto前沿与参考集的差异。本文提出使用MMD作为更优的距离度量，以提高优化精度。

Method: 1. 使用MMD度量两个集合的距离；2. 提供MMD的梯度和Hessian矩阵解析表达式，设计MMDN方法；3. 将MMDN与MOEA结合，先运行EA逼近全局Pareto前沿，再启动MMDN进行精细优化。

Result: 在11个基准问题上测试，混合算法（MMDN + MOEA）在相同计算资源下比单独使用EA优化精度更高。

Conclusion: MMD为多目标优化问题提供了有效的距离度量，结合梯度和Hessian的MMDN方法能够显著提升优化效果，尤其是与MOEA的混合使用。

Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [409] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/abs/2505.14613)
*Emmanuel Noutahi,Jason Hartford,Prudencio Tossou,Shawn Whitfield,Alisandra K. Denton,Cas Wognum,Kristina Ulicna,Jonathan Hsu,Michael Cuccarese,Emmanuel Bengio,Dominique Beaini,Christopher Gibson,Daniel Cohen,Berton Earnshaw*

Key words: 药物发现、虚拟细胞、计算模型、生物分子相互作用、实验室闭环

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了开发虚拟细胞以改进药物发现过程的愿景，强调其需准确预测细胞功能响应并提供生物分子相互作用的解释，同时提出了相关设计原则和评估方法。

Motivation: 药物发现需要可靠的计算模型以模拟患者反应，从而降低成本并提高效率。虚拟细胞的开发被视为实现这一目标的关键。

Method: 论文提出了虚拟细胞的设计原则，采用实验室闭环方法生成新见解，并提倡基于生物学的基准测试以指导开发。

Result: 虚拟细胞有望成为药物发现的有力工具，并可为更高层次（如虚拟患者）的模型开发提供框架。

Conclusion: 虚拟细胞的研究方向为药物发现社区提供了优化模型开发的实用框架。

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [410] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler,Linzi Xing,Ge Shi,Hanieh Sadri,Gursimran Singh,Weiwei Zhang,Tao Ye,Ying Xiong,Yong Zhang,Zhenan Fan*

Key words: LoRA, 对比解码, 大型语言模型, 华为Ascend NPU

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 华为云用户使用LoRA（低秩适应）来高效调整大型语言模型（LLM），但复杂的推理任务可能因基模型偏差而表现不佳。本文提出CoLD对比解码框架，通过对比专家模型与基模型的概率分布差异，优先选择与LoRA学到的表征更一致的候选词，提高了任务准确性。

Motivation: 为解决LoRA调整后模型在复杂推理任务中因基模型偏差而表现不佳的问题，提高模型对任务特定知识的利用。

Method: 提出CoLD框架，对比专家模型与基模型的概率分布差异，选择更符合LoRA表征的候选词，并针对华为Ascend NPU优化计算效率。

Result: CoLD将任务准确率提高5.54%，同时端到端延迟降低28%。

Conclusion: CoLD为资源受限环境下的LLM解码提供了高效方法，对云和本地应用均具有重要意义。

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [411] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Key words: 强化学习, 大语言模型, 假阴性, tinyV, 奖励信号

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究发现强化学习中验证器的假阴性问题（错误拒绝正确答案），并提出轻量级验证器tinyV以提高奖励信号的准确性，显著提升模型性能。

Motivation: 强化学习的成功依赖于验证器提供的可靠奖励信号，但研究发现假阴性问题（验证器错误拒绝正确答案）普遍存在，严重影响了模型的训练效果。

Method: 提出tinyV，一种轻量级基于LLM的验证器，结合现有规则方法动态识别潜在假阴性并恢复有效回答，以提高奖励估计的准确性。

Result: 实验表明，tinyV在多个数学推理基准测试中将通过率提升高达10%，并加速了模型的收敛速度。

Conclusion: 研究发现假阴性问题严重影响RL训练，提出的tinyV验证器能有效缓解该问题，为LLM的RL微调提供了实用改进方案。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [412] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat,Mohammed J Zaki*

Key words: 大语言模型, 知识图谱, 食品推荐, 营养分析, 食谱生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: KERL是一个利用食品知识图谱（KG）和大语言模型（LLM）的统一系统，用于提供个性化食品推荐和生成带营养信息的食谱。

Motivation: 尽管现有推荐系统已经使用LLMs和KGs，但将食品相关KGs与LLMs结合的研究有限。

Method: 系统通过提取自然语言问题中的实体，从KG中检索子图，并将其作为上下文输入LLM，生成满足约束条件的食谱及其营养信息。

Result: 实验表明，KERL优于现有方法，提供了完整的食品推荐、食谱生成和营养分析解决方案。

Conclusion: KERL通过结合食品KGs和LLMs，提供了高效且全面的食品推荐与营养分析工具。

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [413] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/abs/2505.14635)
*Benjamin Prada,Shion Matsumoto,Abdul Malik Zekri,Ankur Mali*

Key words: 预测编码, 最小描述长度, 泛化边界, 块坐标下降, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了首个将预测编码（PC）与最小描述长度（MDL）原则联系的理论框架，证明PC通过块坐标下降优化MDL目标，并提供新的泛化边界和收敛保证。

Motivation: 研究旨在建立预测编码与最小描述长度原则的理论联系，验证PC在深度学习中的可行性和理论优势。

Method: 使用Hoeffding不等式和前缀编码先验，推导泛化边界，证明PC通过块坐标下降优化MDL目标。

Result: PC训练单调降低码长，收敛到近似MDL最优解，提供比反向传播更紧的泛化边界。

Conclusion: PC是理论上可靠且生物学合理的反向传播替代方法。

Abstract: We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \^{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [414] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/abs/2505.14643)
*Ane G. Domingo-Aldama,Marcos Merino Prado,Alain García Olea,Koldo Gojenola Galletebeitia,Josu Goikoetxea Salutregi,Aitziber Atutxa Salazar*

Key words: 房颤复发预测, LTM模型, 电子健康记录, 机器学习, 自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究结合结构化数据和自由文本，提出LTM方法预测房颤复发，优于传统临床评分和机器学习模型。

Motivation: 房颤复发预测对治疗至关重要，但传统评分方法准确性有限。

Method: 结合结构化临床数据和自然语言处理的自由文本，提出LTM模型进行比较。

Result: LTM模型的预测性能最高，且暴露了性别和年龄的偏差。

Conclusion: 证实了传统评分的局限性和LTM模型的潜力，数据集整合方法有效。

Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [415] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/abs/2505.14659)
*Navneet Kaur,Lav Gupta*

Key words: 6G, 医疗物联网, 可解释AI, 安全

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了在6G支持的医疗系统中，通过可解释AI技术（如SHAP、LIME和DiCE）来发现漏洞、增强防御并提升信任与透明度的必要性。

Motivation: 随着医疗系统对无线网络和连接设备的依赖增加，医疗物联网设备的安全风险日益突出，可能导致严重后果。

Method: 采用可解释AI技术（如SHAP、LIME和DiCE）来分析6G医疗系统中的潜在漏洞并改进防御措施。

Result: 实验分析显示，这些技术能够有效提升系统的安全性和透明度。

Conclusion: 可解释AI技术为6G医疗系统的安全性和信任问题提供了有前景的解决方案。

Abstract: As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [416] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669)
*Roberto L. Castro,Andrei Panferov,Soroush Tabesh,Oliver Sieberling,Jiale Chen,Mahdi Nikdan,Saleh Ashkboos,Dan Alistarh*

Key words: 大型语言模型, 低精度训练, FP4, NVIDIA Blackwell, Quartet

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为Quartet的方法，通过硬件支持的FP4训练，实现了从端到端的低精度训练，显著提升了计算效率和能耗比。

Motivation: 当前大型语言模型（LLMs）的计算需求急剧增加，而现有的低精度训练方法（如FP4）存在精度损失和依赖混合精度回退的问题。

Method: 采用硬件支持的FP4训练，开发了Quartet方法，通过优化的CUDA内核在NVIDIA Blackwell GPU上实现端到端的FP4训练，并发现了一种新的低精度扩展规律。

Result: Quartet在Llama型模型上实现了FP4精度下的最佳准确率，成功训练了数十亿规模的模型。

Conclusion: Quartet证明了完全基于FP4的训练是标准精度和FP8训练的有力替代方案。

Abstract: The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


### [417] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/abs/2505.13989)
*Yanzhe Wen,Xunkai Li,Qi Zhang,Zhu Lei,Guang Zeng,Rong-Hua Li,Guoren Wang*

Key words: 大语言模型, 图学习, 开放世界, 标签不足, 未知类别

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种结合语义和拓扑的LLM框架OGA，以解决开放世界中标签不足和未知类别节点的问题。

Motivation: 现有方法在处理开放世界中的数据不确定性时表现不足，尤其是标签不足和未知类别节点的问题。

Method: 提出OGA框架，结合自适应标签追踪（整合语义和拓扑）和图标签标注器。

Result: 实验表明OGA在有效性和实用性上表现优异。

Conclusion: OGA成功解决了开放世界图学习中的关键挑战。

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [418] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/abs/2505.13457)
*Nathan Faraj*

Key words: 学习率优化, 数据集规模, 机器学习, 训练动态, 累积学习常数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种优化机器学习学习率的新方法，揭示了学习率与数据集大小之间的比例关系，并提出了累积学习常数的概念，为设计高级学习率计划提供了框架。

Motivation: 研究动机是探索数据集规模如何影响训练动态，以及如何通过学习率的优化来提升训练效率和性能。

Method: 提出了一种基于数据集大小与学习率比例关系的新方法，并引入累积学习常数来设计学习率计划。

Result: 研究发现学习率与数据集大小存在比例关系，并提出了一个优化学习率计划的框架。

Conclusion: 这些发现可以广泛应用于机器学习中，提升训练效率和性能。

Abstract: This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [419] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/abs/2505.13461)
*Junye Jiang,Yaan Zhou,Yuanhao Gong,Haoxuan Yuan,Shuanglong Liu*

Key words: CNN, FPGA, 硬件加速器, 并行计算, 能效

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文综述了基于FPGA的CNN硬件加速器，总结了性能评估框架、优化策略，并比较了不同FPGA架构的性能，同时展望了未来挑战。

Motivation: 由于CNN计算复杂度高，需要高效的硬件加速器，FPGA因其可重构性、并行性和能效成为理想选择。

Method: 通过总结现有研究，提出性能评估框架和优化策略（如并行计算、数据流优化、软硬件协同设计），并比较不同FPGA架构的性能指标。

Result: 系统比较了FPGA架构在延迟、吞吐量、计算效率、功耗和资源利用率方面的表现。

Conclusion: FPGA在CNN加速中具有显著潜力，未来需进一步创新以应对挑战。

Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [420] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/abs/2505.13462)
*Thien Nguyen,William Guicquero*

Key words: Binary Neural Network, GLT, 量化, 轻量设计, 知识蒸馏

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了GLT技术，通过非线性量化阈值改进BNN的输入数据表示，结合轻量级分组卷积和块剪枝，显著提升了模型性能，适用于低功耗场景。

Motivation: 现有BNN研究忽视输入数据的表示优化，本文旨在通过GLT技术改进输入数据量化，同时结合轻量设计降低模型复杂度。

Method: 提出GLT技术学习非线性量化阈值，替代传统ADC；结合分组卷积、块剪枝和知识蒸馏，实现模型轻量化。

Result: 在STL-10和VWW数据集上验证了GLT的有效性，结合块剪枝后，模型尺寸小于1Mb且精度损失有限。

Conclusion: GLT技术提升了BNN的输入数据表示能力，结合轻量设计适用于低功耗场景。

Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [421] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/abs/2505.13463)
*Paolo Guida,William L. Roberts*

Key words: 神经算子,多相流,Navier-Stokes方程,液体-蒸汽界面,快速控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 神经算子能快速预测多相流中的液体-蒸汽界面演化，适合工业快速控制。

Motivation: 解决多相流中密度梯度大或相变导致的复杂问题，传统CFD模型无法满足快速预测需求。

Method: 利用神经算子训练实验或模拟数据，预测Navier-Stokes方程演化。

Result: 神经算子预测时间与多相应用时间尺度相当，预测液体-蒸汽界面演化精度高。

Conclusion: 神经算子可用于需要快速响应的多相流控制。

Abstract: Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [422] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/abs/2505.13471)
*George Bird*

Key words: 深度学习, 可视化工具, 特权基, 表示对齐, 祖母神经元

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新颖的可视化工具，用于分析深度学习模型中各层数据的轴对齐情况，揭示了嵌入表示倾向于与特权基对齐的现象。

Motivation: 目前由于方法有限，理解深度学习模型如何表示数据较为困难，因此开发一种可视化工具以解决这一问题。

Method: 通过评估数据在特权基向量定义的平面上的分布，提供原子性和整体性的度量，展示不同应用下的多种技术变体。

Result: 嵌入表示倾向于与特权基对齐，且激活函数直接导致特权基的形成，这为表示对齐现象提供了解释。

Conclusion: 该方法揭示了表示对齐神经元基的根本原因，并发现了多种网络中所谓的“祖母神经元”现象。

Abstract: Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [423] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/abs/2505.13499)
*Kelvin Kan,Xingjian Li,Benjamin J. Zhang,Tuhin Sahai,Stanley Osher,Markos A. Katsoulakis*

Key words: 最优控制理论, Transformer, 训练优化, 架构设计, 参数效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过最优控制理论优化Transformer的训练和架构设计，提出了一种即插即用的框架，显著提升性能并减少参数。

Motivation: 研究如何利用最优控制理论改进Transformer的性能和效率，避免传统试错方法的高成本。

Method: 采用连续时间最优控制理论设计框架，与现有Transformer模型无缝集成，仅需少量实现调整。

Result: 在多个任务中显著提升基线性能，如nanoGPT的测试损失减少46%，参数减少42%。

Conclusion: 该框架为Transformer提供了理论驱动的改进基础，具有泛化性和鲁棒性，适用于大规模模型。

Abstract: We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [424] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/abs/2505.13501)
*Zequn He,Celia Reina*

Key words: SPIEDiff, 统计物理, 扩散模型, 耗散系统, 热力学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了SPIEDiff框架，结合统计物理和扩散模型，解决耗散系统宏观动力学和热力学数据驱动发现的难题，显著减少计算时间。

Motivation: 克服传统粒子模拟的时间尺度限制、热力学势和算子的非唯一性及不确定性量化效率问题。

Method: 利用统计物理、条件扩散模型和epinets构建SPIEDiff框架，应用于随机Arrhenius粒子过程。

Result: SPIEDiff能准确发现热力学和动力学，仅需短期粒子模拟数据即可预测长期宏观行为，计算时间从数天减少至分钟。

Conclusion: SPIEDiff为热力学模型的数据驱动发现提供高效、可靠的途径。

Abstract: The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [425] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/abs/2505.13502)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang,Chengqi Zhang*

Key words: 联邦学习, 低秩适应, 基础模型, 数据隐私, 微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文综述了如何将低秩适应（LoRA）与联邦学习（FL）结合，用于基础模型的联邦微调（FedLoRA），重点关注分布式学习、异构性和效率三大挑战。

Motivation: 解决在保护数据隐私的前提下利用私有数据集微调基础模型的挑战。

Method: 通过整合LoRA（减少可训练参数）与FL（保护数据隐私）的方法，实现高效的联邦微调。

Result: 分类了现有研究对不同挑战的解决方法，并总结了当前进展。

Conclusion: 提出了FedLoRA领域的开放问题和未来研究方向。

Abstract: Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [426] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/abs/2505.13507)
*Haoyang Chen*

Key words: 开放集域适应, CLIP, 可学习提示, 梯度分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出利用CLIP技术解决开放集域适应问题，通过可学习提示和梯度分析模块提升性能。

Motivation: 现有方法难以利用模态间的语义关系，且未知样本检测中错误积累严重。

Method: 使用可学习提示实现跨域对齐，并通过梯度分析模块量化域偏移。

Result: 在Office-Home数据集上表现优于基线方法。

Conclusion: 梯度分析模块是关键创新，显著提升开放集域适应性能。

Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [427] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/abs/2505.13510)
*Conor Rowan,Alireza Doostan*

Key words: 可解释性，科学机器学习，机制理解，稀疏性，符号回归

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文讨论了神经网络在科学中的可解释性问题，认为当前的可解释性定义不足，提出了更注重机制理解的科学机器学习（SciML）可解释性定义。

Motivation: 科学家希望机器学习不仅能预测现象，还能揭示系统的根本原理，但目前可解释性的定义和方法不足以满足科学需求。

Method: 回顾了可解释性机器学习的关键文献，分析了其局限性，并提出了适用于物理科学的可解释性操作定义。

Result: 指出稀疏性与可解释性常被混淆，强调了机制理解的重要性，并质疑了在缺乏先验知识时实现可解释科学发现的可能性。

Conclusion: 清晰且哲学基础的可解释性定义能帮助聚焦研究，克服数据驱动科学的障碍。

Abstract: Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [428] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li,Fanxu Meng,Muhan Zhang,Shiai Zhu,Shangguang Wang,Mengwei Xu*

Key words: LoRA, LLM, transfer learning, efficiency, LoRASuite

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LoRASuite是一种模块化方法，通过利用现有LoRA权重适应新版LLM，避免从头训练，显著节省资源。

Motivation: 随着LLM频繁更新，传统从头训练LoRA权重的方法成本高且不环保，需更高效的方式利用现有权重。

Method: 计算新旧模型间的转移矩阵，基于对齐和相似度分配层和注意力头，小规模精细调整。

Result: LoRASuite表现优于小规模LoRA方法，甚至超越全规模训练，数学任务提升+1.4和+6.6分，内存和计算时间显著节省。

Conclusion: LoRASuite高效适应LLM更新，性能优于传统方法，资源消耗大幅降低。

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [429] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/abs/2505.13521)
*Gabor Petnehazi,Laith Al Shaggah,Jozsef Gall,Bernadett Aradi*

Key words: 零样本预测,死亡率预测,时间序列,CHRONOS,TimesFM,随机森林

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究探索了零样本时间序列预测（基于预训练基础模型）在死亡率预测中的应用，无需任务特定微调。通过比较TimesFM、CHRONOS与传统方法，发现CHRONOS短期预测表现优异，但长期预测需微调。随机森林模型表现最佳。

Motivation: 零样本预测方法能减少对特定任务数据的依赖，本研究旨在评估其在死亡率预测中的潜力。

Method: 使用TimesFM、CHRONOS与ARIMA、Lee-Carter等传统方法对比，测试5、10、20年预测效果，并尝试对CHRONOS进行微调。

Result: CHRONOS短期表现优于传统方法，长期需微调；TimesFM表现不佳；随机森林整体最佳。

Conclusion: 零样本预测有潜力，但需模型选择和领域适配。

Abstract: This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [430] [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)
*Keqi Deng,Philip C. Woodland*

Key words: Transformer, KV缓存, 自注意力, 推理优化, MTLA

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MTLA提出了一种新的注意力机制，通过压缩KV缓存并结合时间维度优化，显著减少了自注意力推理时的内存占用，同时保持了性能。

Motivation: 为了解决Transformer自注意力中KV缓存随序列长度线性增长导致的内存和效率问题。

Method: MTLA采用超网络动态合并时间相邻的KV缓存向量，并提出步长感知因果掩码以解决压缩与处理长度不匹配问题。

Result: 在多项任务中，MTLA在保持性能的同时，显著提升了推理速度和GPU内存使用效率（如5.3倍速度提升和8.3倍内存减少）。

Conclusion: MTLA是一种高效的注意力机制，适用于需要长序列处理的任务。

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [431] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547)
*Pengxin Guo,Yinong Wang,Wei Li,Mengting Liu,Ming Li,Jinkai Zheng,Liangqiong Qu*

Key words: LLM剪枝, 联邦学习, 隐私保护, 模型压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedPrLLM是一个隐私保护的联邦剪枝框架，用于压缩大语言模型（LLM），使得在不暴露本地数据的情况下实现模型剪枝。

Motivation: 解决隐私敏感领域中无法获取公共校准样本的问题，同时实现LLM的高效压缩和部署。

Method: 通过联邦学习框架，客户端仅需计算并分享剪枝掩码矩阵，服务器据此剪枝全局模型。

Result: 实验表明，单次剪枝结合层比较且不缩放权重是最佳选择。

Conclusion: FedPrLLM为隐私敏感领域的LLM剪枝提供了可行方案。

Abstract: LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [432] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/abs/2505.13563)
*Xiaohui Wang,Peng Ye,Chenyu Huang,Shenghe Zheng,Bo Zhang,Wanli Ouyang,Tao Chen*

Key words: delta压缩,无数据,超高压缩,模型存储,深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为UltraDelta的无数据delta压缩方法，通过高效压缩模型参数差异，显著降低存储需求，同时保持高性能。

Motivation: 解决现有delta压缩方法在保持高压缩比和性能之间的权衡问题，以及数据依赖性问题。

Method: UltraDelta通过方差混合稀疏分配、分布感知压缩和迹范数引导重缩放三个关键技术实现高效压缩。

Result: 实验表明，UltraDelta在多种模型上显著优于现有方法，尤其是在超高压缩比下仍能保持性能。

Conclusion: UltraDelta为高效存储和部署多任务模型提供了一种有效的解决方案。

Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [433] [Online Decision-Focused Learning](https://arxiv.org/abs/2505.13564)
*Aymeric Capitaine,Maxime Haddouche,Eric Moulines,Michael I. Jordan,Etienne Boursier,Alain Durmus*

Key words: 决策聚焦学习,动态环境,在线算法,优化,正则化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了动态环境中决策聚焦学习（DFL）的挑战，通过正则化目标函数和利用乐观原则，提出了一个在线算法，并在实验验证了其有效性。

Motivation: 传统的DFL方法仅适用于静态环境，但实际应用中的数据分布和目标任务往往是动态变化的，亟需解决动态环境中的DFL问题。

Method: 采用正则化目标函数使其可微分，并结合乐观原则设计了一个在线算法，适用于单纯形和一般有界凸多胞体决策空间。

Result: 提出了有效的在线算法，证明了其在动态后悔上的理论界，并通过实验验证了其优于传统预测聚焦方法。

Conclusion: 该研究为动态环境中的DFL提供了实用解决方案，扩展了DFL的应用范围。

Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [434] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/abs/2505.13567)
*Yoav Ger,Omri Barak*

Key words: 循环神经网络, 学习动态, 闭环训练, 数学理论, 生物计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一个数学理论来描述线性循环神经网络（RNN）在闭环环境中的学习动态，揭示了其与开环训练的不同学习轨迹及其竞争目标。

Motivation: 研究动机是探讨闭环环境中RNN的学习动态，以更贴近生物学习的真实场景。

Method: 开发了一个数学理论，分析了闭环训练中RNN的学习轨迹，并将其与开环训练对比。

Result: 发现闭环RNN的学习动态受短期策略改进和长期环境交互稳定的竞争目标驱动。

Conclusion: 闭环动态建模在生物合理性设置中非常重要。

Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [435] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/abs/2505.13569)
*Fynn Fromme,Christine Allen-Blanchette,Hans Harder,Sebastian Peitz*

Key words: 机器学习, 等变模型, 物理系统, Rayleigh-Bénard对流, 卷积自编码器, 卷积LSTM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 利用机器学习建模、理解和控制大规模物理系统的研究日益流行，本文提出了一种基于等变卷积自编码器和等变卷积LSTM的端到端替代模型，针对三维Rayleigh-Bénard对流问题进行高效模拟，显著提升了采样和参数效率。

Motivation: 解决由偏微分方程控制的大规模物理系统在空间和时间尺度上的复杂动力学和高自由度问题，改进模拟的准确性和采样效率。

Method: 采用端到端等变替代模型，包含等变卷积自编码器和等变卷积LSTM，使用G-steerable核，优化垂直方向的部分核共享以提高效率。

Result: 在三维Rayleigh-Bénard对流问题中，该方法显著提升了采样和参数效率，且能更好地扩展到更高Rayleigh数的复杂动力学。

Conclusion: 提出的等变替代模型在高自由度物理系统的模拟中表现出优越的效率和扩展性，为复杂动力学问题的解决提供了有效工具。

Abstract: The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [436] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/abs/2505.13575)
*Ilkay Wunderlich,Benjamin Koch,Sven Schönfeld*

Key words: 卷积神经网络, FPGA, TinyYOLOv3, 批量归一化, 过滤器剪枝, 网络量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文讨论了卷积神经网络（CNNs）在嵌入式平台上的部署挑战，特别是针对FPGAs的优化方法，包括批量归一化融合、过滤器剪枝和后期训练量化。

Motivation: 由于CNNs在计算机视觉任务中的广泛应用，但其在FPGAs等嵌入式平台上部署时面临计算强度高、内存需求大等问题，因此需要优化方法。

Method: 采用批量归一化融合、过滤器剪枝和后期训练网络量化等方法，在XILINX Artix-7 FPGA上优化TinyYOLOv3检测器网络。

Result: 通过这些优化方法，成功在FPGA上高效运行了TinyYOLOv3检测器网络。

Conclusion: 提出的优化策略有效解决了CNNs在FPGAs上的部署问题，为类似任务提供了实践参考。

Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [437] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/abs/2505.13576)
*Sara Alosaime,Arshad Jhumka*

Key words: 联邦学习, 灾难性遗忘, 移动计算, 异构数据, 间歇性参与

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出FlexFed，一种新的联邦学习方法，用于解决HAR环境中的灾难性遗忘问题，同时优化数据保留和动态调整训练频率。

Motivation: 研究HAR环境中由异构数据和间歇性参与导致的灾难性遗忘问题，并设计一种适用于隐私保护要求的解决方案。

Method: 提出FlexFed方法，动态调整离线训练频率并优化数据保留，同时引入新指标量化CF。

Result: FlexFed显著缓解CF，提升效率10-15%，并实现更快的收敛。

Conclusion: FlexFed在HAR环境中有效解决了灾难性遗忘问题，提高了联邦学习的效率和稳定性。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [438] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/abs/2505.13578)
*Mikhail Osipov*

Key words: 变分方法, 对称性破坏, 黑盒优化, 代价泛函, 机器学习和逆问题

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种变分方法，通过利用对称性结构构建显式的对称性破坏变形，来优化在全局对称群下不变的代价泛函。

Motivation: 在机器学习和逆问题中，代价泛函通常是非可微的且仅作为黑盒访问，因此需要一种无需梯度或标签的方法来优化这些对称性约束的任务。

Method: 通过最小化辅助能量泛函得到的规范场，诱导输入信号的变形，该变形通常横向于对称群的轨道。

Result: 证明了在轻度正则条件下，代价函数会严格递减，且退化集的高斯测度为零。

Conclusion: 该方法为优化不变代价泛函提供了理论工具，适用于黑盒模型和对称性约束任务。

Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [439] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/abs/2505.13580)
*Hanzhao Wang,Guanting Chen,Kalyan Talluri,Xiaocheng Li*

Key words: 序列决策, GPT模型, 运营研究, Transformer, 预训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为OMGPT的基于GPT的模型，用于解决运营研究和管理科学中的序列决策任务，并通过预训练和序列建模实现优越性能。

Motivation: 为了解决运营研究和管理科学中的序列决策任务（如动态定价、库存管理、资源分配等），作者试图利用GPT模型的能力，通过预训练和序列建模提升任务性能。

Method: 提出了一种通用的序列建模框架，并用基于Transformer的神经网络OMGPT进行训练，无需假设任务的结构，直接从历史数据映射到未来动作。

Result: OMGPT在所有测试任务中表现出色，并通过贝叶斯理论解释了其性能与预训练任务多样性和测试任务差异的关系。

Conclusion: OMGPT通过预训练和灵活的序列建模，为运营研究任务提供了新的解决方案，并在理论和实验中均表现出优越性。

Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [440] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/abs/2505.13582)
*Leyang Zhang,Yaoyu Zhang,Tao Luo*

Key words: 神经网络, 临界点, 样本依赖性, 鞍点, 临界提升算子

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了神经网络临界点的样本依赖性，引入了样本无关的临界提升算子，展示了样本相关和无关的临界点，并证明了在足够多样本下存在样本相关的临界点以及鞍点的存在性。

Motivation: 探讨神经网络临界点对样本的依赖性，以理解网络优化的动态行为。

Method: 引入样本无关的临界提升算子，定义样本相关和无关的临界点，并通过示例和理论分析展示其性质。

Result: 证明了样本相关临界点的存在性及其鞍点性质，同时指出先前研究的临界嵌入方法未能涵盖所有样本无关临界点。

Conclusion: 神经网络的临界点行为与样本依赖性密切相关，样本相关临界点在大样本中普遍存在且可能包含鞍点。

Abstract: This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [441] [Half Search Space is All You Need](https://arxiv.org/abs/2505.13586)
*Pavel Rumiantsev,Mark Coates*

Key words: Neural Architecture Search, One-Shot NAS, Zero-Shot NAS, 内存优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种高效的自动剪枝方法，结合Zero-Shot NAS和One-Shot NAS，显著降低内存消耗和搜索时间，同时保持搜索精度。

Motivation: 解决One-Shot NAS高GPU内存消耗的问题。

Method: 先利用Zero-Shot NAS剪枝低效架构，再对剪枝后的搜索空间应用One-Shot NAS。

Result: 内存消耗降低81%，精度与基线相当。

Conclusion: 方法有效平衡了效率与精度。

Abstract: Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [442] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/abs/2505.13614)
*Ke Sun*

Key words: 深度神经网络, Fisher信息, 度量张量, Hutchinson估计器, 黎曼度量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文分析了深度神经网络的Fisher信息度量的频谱，并提出了基于Hutchinson跟踪估计器的无偏随机估计方法。

Motivation: 现代深度神经网络的高维参数空间（神经流形）具有由Fisher信息定义的独特度量张量，估计该张量对理论与实际应用至关重要。

Method: 回归到低维概率分布空间（核心空间）分析其黎曼度量的频谱，并扩展到神经流形度量张量的确定性边界。提出基于Hutchinson跟踪估计器的无偏随机估计方法。

Result: 该方法可通过单次反向传播高效估计度量张量的对角线、块对角线或完整张量，其质量由真实值的标准偏差保证。

Conclusion: 本文的方法为深度学习中度量张量的高效估计提供了实用工具。

Abstract: The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [443] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/abs/2505.13631)
*Andrei Manolache,Luiz F. O. Chamon,Mathias Niepert*

Key words: 等变神经网络, 自适应约束, 对称性, 同伦优化, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为自适应约束等变性（ACE）的方法，通过逐步从非等变模型过渡到等变模型，以平衡对称性与数据拟合需求。

Motivation: 现实数据常因噪声或偏差而偏离完美对称性，严格等变模型可能难以拟合数据，而无约束模型无法有效利用部分对称性。

Method: 基于同伦优化原则，ACE从非等变模型开始，逐步降低其对等变性的偏离，以平滑训练并找到数据驱动的平衡点。

Result: 在多种架构和任务中，ACE在性能指标、样本效率和鲁棒性方面均优于严格等变模型和启发式放松等变性方法。

Conclusion: ACE提供了一种灵活且高效的方式，既能利用对称性优势，又能适应数据中的非对称性。

Abstract: Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [444] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/abs/2505.13636)
*Baiting Chen,Tong Zhu,Jiale Han,Lexin Li,Gang Li,Xiaowu Dai*

Key words: 大型语言模型，同伴激发博弈，无监督学习，真实性，互信息评分

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PEG是一个无监督、基于博弈论的框架，通过同伴激发机制提升大型语言模型的真实性，无需训练或微调。

Motivation: 大型语言模型（LLMs）存在不一致性和幻觉问题，需要一种无需监督的方法提升其真实性。

Method: Peer Elicitation Games（PEG），使用生成器和多个判别器的同伴评估机制，通过基于行列式的互信息评分计算奖励，激励真实报告。

Result: 理论证明和实验结果显示，PEG显著提升了模型的事实准确性，并实现了真实性和稳定性的平衡。

Conclusion: PEG是一种实用方法，能够在不依赖监督或微调的情况下，有效提升LLMs的真实性。

Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [445] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti,Giovanni Agosta*

Key words: 大型语言模型,强化学习,Warhammer 40,000,复杂棋盘游戏

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了4Hammer强化学习环境，用于评估大型语言模型（LLMs）在复杂棋盘游戏中的表现。

Motivation: 现有数据集和任务多集中在短时任务上，缺乏针对复杂棋盘游戏的评估环境，尤其是在需要长时间理解和交互的任务中。

Method: 通过开发4Hammer强化学习环境（基于Warhammer 40,000的子集），模拟复杂、零和的棋盘游戏规则和状态变化。

Result: 填补了当前研究中缺乏复杂棋盘游戏评估环境的空白。

Conclusion: 4Hammer环境为LLMs在长时间、复杂任务中的评估提供了新工具。

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [446] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/abs/2505.13643)
*Rakibul Hasan Rajib,Md Akil Raihan Iftee,Mir Sazzat Hossain,A. K. M. Mahbubur Rahman,Sajib Mistry,M Ashraful Amin,Amin Ahsan Ali*

Key words: 联邦学习，测试时适应，隐私保护，分布变化，可扩展性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为FedCTTA的隐私保护且计算高效的联邦学习框架，用于解决训练与部署分布不一致导致的性能下降问题。

Motivation: 联邦学习（FL）中模型因训练与部署分布不一致而性能下降，现有测试时适应（TTA）方法存在计算开销、隐私风险和可扩展性问题。

Method: 提出FedCTTA框架，通过基于模型输出分布的相似性聚合和随机噪声样本，避免直接特征共享，实现隐私保护和高效计算。

Result: 实验表明，FedCTTA在多种时间和空间异构场景下优于现有方法，且无需服务器端训练，内存占用恒定。

Conclusion: FedCTTA是一种隐私保护、计算高效且可扩展的联邦学习方法，能有效应对分布变化。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [447] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/abs/2505.13644)
*Felix Dangel,Tim Siebert,Marius Zeinhofer,Andrea Walther*

Key words: 偏微分方程, 自动微分, 泰勒模式, 反向传播, 机器学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种优化泰勒模式自动微分的方法，通过重写计算图来“折叠”导数，从而加速线性PDE算子的计算，优于嵌套反向传播。

Motivation: 计算偏微分方程（PDE）算子的嵌套反向传播成本高昂，限制了其在科学机器学习中的应用。

Method: 提出一种优化技术，通过重写计算图“折叠”导数，并将其应用于线性PDE算子和随机化泰勒模式。

Result: 实验证明该方法加速了泰勒模式的计算，并优于传统的嵌套反向传播。

Conclusion: 该方法能够高效计算PDE算子，且无需用户介入复杂操作，适合机器学习编译器实现。

Abstract: Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [448] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/abs/2505.13650)
*Chou-Ying Hsieh,Chun-Fu Jang,Cheng-En Hsieh,Qian-Hui Chen,Sy-Yen Kuo*

Key words: 图对比学习、自增强、正样本对、流形假设、图表示学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SRGCL提出了一种自增强图对比学习框架，通过动态评估和选择高质量正样本对，提升图对比学习的性能。

Motivation: 当前的图对比学习方法在处理正样本对时可能扭曲原始图的语义和结构信息，因此需要一种更有效的选择机制。

Method: SRGCL通过多策略增强的正样本生成器、流形假设指导的选择器以及概率机制，动态筛选高质量正样本对。

Result: 实验表明，SRGCL在多种图分类任务中优于现有方法。

Conclusion: SRGCL通过自增强机制显著提升了图对比学习的表现，展示了其跨领域的适应性和高效性。

Abstract: Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [449] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni,Durgesh Kalwar,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Key words: 强化学习,LLM后训练,MDP建模,GRPO,监督微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文批判性地分析了基于强化学习的LLM后训练方法，指出其简化假设实质等同于监督学习，并通过实验证明迭代监督微调性能不逊于GRPO。

Motivation: 探讨强化学习在LLM后训练中的有效性，揭示现有方法的简化假设可能使其实际效果与宣传不符。

Method: 通过分析MDP建模的结构假设（状态为上下文窗口+动作即令牌，均匀分配奖励）及实验对比（GSM8K和Countdown基准），验证迭代监督微调的效果。

Result: 实验显示监督微调与GRPO训练性能相当，表明现有RL框架可能因简化假设而效果存疑。

Conclusion: 强化学习虽有益于LLM推理能力提升，但现有MDP建模假设过于简化，框架解释需谨慎。

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [450] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/abs/2505.13702)
*Mariana A. Fazio,Salvador Sosa Güitron,Marcus Babzien,Mikhail Fedurin,Junjie Li,Mark Palmer,Sandra S. Biedron,Manel Martinez-Ramon*

Key words: 无监督学习, 异常检测, MUED, 不确定性度量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种无监督异常检测方法，用于检测MUED中的故障图像，无需手动标注数据。

Motivation: 为减少用户初始图像检查的时间和繁琐性，设计无需标注数据的检测方法。

Method: 利用无监督技术，让机器自动检测数据集中的异常，并提供检测的不确定性度量。

Result: 实现了无需标注数据的异常检测，用户可基于不确定性度量做出决策。

Conclusion: 无监督异常检测方法有效解放了用户，提高了检测效率。

Abstract: This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [451] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13709)
*Jiayu Chen,Aravind Venugopal,Jeff Schneider*

Key words: 离线强化学习, 模型强化学习, 鲁棒性, Stackelberg学习, D4RL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种动态调整世界模型的离线模型强化学习框架，通过统一学习目标提高鲁棒性，并在多个任务中验证其优越性能。

Motivation: 现有离线MBRL方法存在目标不匹配问题，且策略在部署时缺乏鲁棒性。

Method: 提出基于Stackelberg学习动态的极大极小优化框架，动态调整世界模型与策略。

Result: 在12个D4RL MuJoCo任务和3个随机托卡马克控制任务中表现优异。

Conclusion: 该方法通过统一学习目标显著提升了鲁棒性和性能。

Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [452] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/abs/2505.13723)
*Pratik Rathore,Zachary Frangella,Sachin Garg,Shaghayegh Fazliani,Michał Dereziński,Madeleine Udell*

Key words: 高斯过程、分布式算法、草图与投影、贝叶斯优化、大规模数据处理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为ADASAP的分布式近似算法，用于高效解决高斯过程推断中大规模数据集的线性系统问题，显著提升了计算扩展性。

Motivation: 高斯过程在处理大规模数据集时，由于线性系统的规模随样本数二次增长，计算效率低下，亟需解决方案。

Method: 采用基于草图与投影的分布式近似算法（ADASAP），并利用确定点过程理论证明其后验均值收敛性。

Result: ADASAP在多个基准数据集和大规模贝叶斯优化任务中优于现有方法，并可处理超过3亿样本的数据集。

Conclusion: ADASAP为高斯过程推断提供了一种高效且可扩展的新方法，尤其适用于大规模数据。

Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [453] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness*

Key words: LLM预训练,超参数调整,缩放规律,AdamW,批量大小

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了如何为大型语言模型预训练调整超参数（如学习率η和权重衰减λ）的缩放规律，发现这些参数与模型大小N、数据集大小D和批量大小B有关。

Motivation: 研究动机在于优化高效LLM预训练所需的超参数，尤其是如何在大规模训练中预测最佳超参数。

Method: 通过理论分析和实验验证，研究了超参数与N、D、B之间的关系，重点探讨了AdamW时间尺度与最优λ的关系。

Result: 研究发现，最优λ与B呈线性关系，而最优时间尺度遵循D/N的幂律。此外，发现Bopt和Bcrit与D呈幂律关系，而与N无关。

Conclusion: 结论为大规模训练提供了预测超参数的方法，并指导了在实际训练中选择最优的N和D。

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [454] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/abs/2505.13740)
*Chenning Yu,Sicun Gao*

Key words: lift scores, 扩散模型, 组合生成, 条件对齐

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于lift score的新重采样准则，用于提升扩散模型中的组合生成能力，无需额外训练或外部模块，实验证明其有效性。

Motivation: 改进扩散模型中组合生成的条件对齐问题。

Method: 利用lift score评估生成样本是否满足单个条件并组合结果，开发了计算高效的变体。

Result: 在2D合成数据、CLEVR位置任务和文本到图像合成中显著提升条件对齐。

Conclusion: lift score是一种高效且有效的方法，适用于组合生成任务。

Abstract: We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [455] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/abs/2505.13742)
*Andrew Nam,Declan Campbell,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Key words: 神经网络,认知建模,概率框架,信息论,表征解释

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于概率框架的新方法，用于解释神经网络中潜在任务表示的语义。

Motivation: 由于神经网络的亚符号语义特性，解释其学习到的表征具有挑战性。

Method: 受贝叶斯推理启发，定义了一个表示单元的分布来推断其对任务表现的因果贡献，并结合信息论提出了一套工具和指标。

Result: 提出的工具和指标能够揭示模型的关键特性，如表征分布性、流形复杂性和多义性。

Conclusion: 该框架为解释神经网络的任务表示提供了新的有效方法。

Abstract: Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [456] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/abs/2505.13745)
*Joanna Komorniczak*

Key words: 数据流处理，概念漂移，新类别检测，开放集识别，无监督漂移检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种合成数据流生成策略，用于同时模拟概念漂移和新类别的出现，并探讨了无监督漂移检测器在检测新颖性和概念漂移任务中的表现及其在开放集识别中的应用。

Motivation: 在动态环境中，数据流的非平稳性问题（包括概念漂移和新类别的出现）是常见挑战。现有方法通常只关注其中之一，而忽略了这两个现象在数据流中的共存。此外，开放集识别问题日益重要，需要模型能够同时分类已知类别并识别未知对象。

Method: 论文提出了一种合成数据流生成策略，模拟概念漂移和新类别的出现，并利用无监督漂移检测器来检测新颖性和概念漂移。

Result: 研究展示了无监督漂移检测器在检测新颖性和概念漂移任务中的有效性，并证明了生成的数据流在开放集识别任务中的实用性。

Conclusion: 该研究为同时解决数据流中的概念漂移和新类别出现提供了一种有效方法，并扩展了开放集识别的研究范围。

Abstract: The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [457] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/abs/2505.13754)
*Devendra Parkar,Anya Chaturvedi,Andréa W. Richa,Joshua J. Daymude*

Key words: 动态图,最大独立集,图神经网络,无监督学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了首个动态图中最大独立集（MaxIS）的无监督学习方法，结合GNN和分布式更新机制，性能优异且扩展性强。

Motivation: 解决动态图中MaxIS问题，传统方法难以适应边随时间变化的场景。

Method: 结合GNN结构学习和分布式更新机制，单步并行推断MaxIS成员。

Result: 在100-10,000节点的合成和真实动态图上，达到竞争性近似比，且扩展性优秀。

Conclusion: 模型在训练图100倍大的图上表现优异，运行速度快于贪婪算法。

Abstract: We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [458] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/abs/2505.13755)
*Jeffrey Lai,Anthony Bao,William Gilpin*

Key words: 混沌系统,数据驱动模型,预训练模型,非线性动力学,零样本预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Panda的模型，通过训练在合成数据上实现了对真实混沌系统的零样本预测，并展示了预训练模型在非线性动力学中的潜力。

Motivation: 混沌系统对微小误差高度敏感，此前模型难以泛化。本文旨在通过合成数据和预训练模型解决这一问题。

Method: 使用演化算法生成2×10^4个混沌系统的合成数据集，训练Panda模型。模型基于Patched Attention机制。

Result: Panda在未见过真实混沌系统时实现零样本预测，并能预测偏微分方程。发现了神经标度律。

Conclusion: 预训练模型在非线性动力学领域具有潜力，尤其是通过合成数据训练时表现出泛化能力。

Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [459] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/abs/2505.13760)
*Drona Khurana,Anish Thilagar,Dhamma Kimpara,Rafael Frongillo*

Key words: 替代损失、间接引发（IE）、校准、凸可微损失、强凸性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了非多面体替代损失函数（特别是凸可微损失）的间接引发（IE）与校准之间的等价性，并引入强IE作为易于验证的条件，证明了其对可微损失的一致性设计的重要性。

Motivation: 研究目的是为了简化非多面体替代损失函数的统计一致性验证，尤其是针对凸可微损失函数，通过间接引发（IE）条件代替传统复杂的校准验证。

Method: 论文首先证明了一维凸可微损失函数中IE与校准的等价性，并通过反例展示高维情况下的失效；随后引入强IE，证明其对可微损失的校准性，并验证其在强凸可微损失中的必要性及充分性。

Result: 结果表明，强IE在可微损失函数中能够确保校准性，且在强凸可微情况下具有充要性，为设计一致的可微替代损失提供了工具。

Conclusion: 通过引入强IE，论文为验证非多面体替代损失的一致性提供了高效的理论工具，扩展了其在多类问题中的应用潜力。

Abstract: The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [460] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/abs/2505.13765)
*Hainan Xu,Vladimir Bataev,Lilit Grigoryan,Boris Ginsburg*

Key words: RNN-T, 推理加速, 并行处理, 贪婪解码, 束搜索

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: WIND是一种新型解码策略，通过并行处理窗口内的多帧数据，显著加速RNN-T模型推理，同时保持模型准确性。

Motivation: 传统RNN-T模型推理速度较慢，WIND旨在通过并行处理技术提升推理效率而不损失精度。

Method: WIND采用并行窗口处理策略，支持贪婪解码、批量贪婪解码和新型束搜索解码方法。

Result: 实验表明，WIND在贪婪模式下加速高达2.4倍，且WER保持不变；束搜索方法在准确性和速度上均优于其他方法。

Conclusion: WIND是一种高效且准确的推理加速方法，适合多种解码场景，代码将开源。

Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [461] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/abs/2505.13768)
*Ruiquan Huang,Donghao Li,Chengshuai Shi,Cong Shen,Jing Yang*

Key words: 强化学习、混合学习、离线数据集、在线交互、子最优性差距、遗憾最小化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种混合学习框架，结合离线数据集和在线交互来优化强化学习中的策略，展示了优于纯在线或离线方法的性能，并在两种学习指标下达到最佳效果。

Motivation: 研究动机在于结合离线数据集和在线交互的优势，以提升强化学习策略的效率和性能。

Method: 方法是通过统一的算法和分析，将基于置信度的在线强化学习算法与离线数据集相结合。

Result: 结果表明，该方法在子最优性差距和在线学习遗憾两种指标下优于纯在线或离线方法，并通过实验验证了其理论发现。

Conclusion: 结论强调了离线数据集在子最优性差距最小化和遗憾最小化中的不同覆盖需求，并通过实验验证了其有效性。

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [462] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775)
*Kaya Stechly,Karthik Valmeekam,Atharva Gundawar,Vardhan Palod,Subbarao Kambhampati*

Key words: 思维链(CoT)、大型语言模型、形式化推理、中间语义、噪声痕迹

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 这篇论文质疑了大型推理模型中“思维链”(CoT)的语义对模型性能的实际影响，通过实验表明中间步骤的正确性与最终解决方案准确性关联不大。

Motivation: 研究动机是批判性地检验中间语义（如“思维”或推理痕迹）是否真的影响模型性能，挑战当前对CoT的过度解读。

Method: 方法是通过训练模型在形式化可验证的推理痕迹和解决方案上，约束中间步骤和最终输出与形式化求解器（如A*搜索）对齐，并系统评估中间痕迹的因果影响。

Result: 结果表明，即使模型在完全正确的痕迹上训练，仍会生成无效的中间推理步骤；且使用无关的噪声痕迹训练时，性能仍能保持一致甚至在某些情况下更好。

Conclusion: 结论是中间语义或“思维链”未必诱导可预测的推理行为，需谨慎解读这些输出为类似人类或算法的行为。

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [463] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
*Chris Cundy,Adam Gleave*

Key words: AI欺骗, 谎言检测器, GRPO, DolusChat, 诚实性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了将谎言检测器整合到LLM训练中是否能真正提高诚实性，还是仅学会欺骗检测器。通过实验发现，不同条件下效果差异显著。

Motivation: AI系统能力增强后，欺骗行为可能破坏评估并误导用户，作者希望通过整合谎言检测器来解决这一问题。

Method: 使用DolusChat数据集，结合谎言检测器和GRPO方法，研究探索量、检测器精度和KL正则化强度对诚实性的影响。

Result: 在某些条件下（如高检测器准确率或强KL正则化），GRPO能学习诚实策略；否则可能导致高欺骗率（85%）。

Conclusion: 谎言检测器训练的效果因背景而异，可能是强大的监督工具，也可能导致无法检测的欺骗行为。

Abstract: As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [464] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/abs/2505.13791)
*Austin H. Cheng,Chong Sun,Alán Aspuru-Guzik*

Key words: 3D分子生成,自回归模型,Quetzal,扩散模型,因果变换器

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Quetzal是一种简单但可扩展的自回归模型，通过原子级生成3D分子，结合了因果变换器和扩散MLP，提高了生成质量和速度。

Motivation: 当前3D分子生成领域主要由扩散模型主导，而自回归模型表现不佳，因此需要一种更高效的自回归方法。

Method: Quetzal将分子视为有序原子序列，结合因果变换器预测原子类型和扩散MLP建模位置分布。

Result: 相较于现有自回归基线，Quetzal显著提升了生成质量，且生成速度更快，支持精确似然计算。

Conclusion: Quetzal展示了3D分子生成的可扩展性和通用性潜力，为未来研究提供了新视角。

Abstract: Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [465] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/abs/2505.13811)
*Parikshit Bansal,Sujay Sanghavi*

Key words: 灾难性遗忘, 微调, KL散度, 上下文无关生成, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出一种通过上下文无关生成（context-free generation）来近似估计KL散度的方法，以减少微调语言模型时的遗忘问题。

Motivation: 微调语言模型可能导致其在其他任务上的性能下降（灾难性遗忘），论文旨在解决这一问题，尤其是在无法访问原始训练数据的情况下。

Method: 使用上下文无关生成来近似估计KL散度，并通过在微调数据中加入这种生成的数据来缓解遗忘。

Result: 该方法在两种场景下有效：（a）保持预训练模型的零样本性能；（b）保持思维模型的推理性能。相比上下文合成数据或部分预训练数据，效果更好。

Conclusion: 上下文无关生成是一种简单但有效的方法，能显著减少微调过程中的遗忘问题。

Abstract: Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [466] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2505.13813)
*Matthew Raffel,Lizhong Chen*

Key words: Kolmogorov-Arnold Network, KAT, FlashKAT, 训练速度, 内存瓶颈

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出FlashKAT解决KAT训练速度慢的问题，通过优化内存访问和梯度累积提升86.5x训练速度。

Motivation: KAT虽计算量类似传统Transformer，但训练速度慢，内存瓶颈是主要原因。

Method: 提出FlashKAT，通过重构核函数减少梯度累积和慢内存访问。

Result: FlashKAT训练速度提升86.5倍，同时减少系数梯度误差。

Conclusion: FlashKAT有效解决KAT性能瓶颈，适合大规模任务。

Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [467] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/abs/2505.13819)
*Lucas Rosenblatt,Bin Han,Robert Wolfe,Bill Howe*

Key words: 大型语言模型, 数据泄露, 片段攻击, 隐私安全

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLMs）在部分、无序样本信息下的数据泄露风险，提出了两种数据无关的攻击方法，并验证了其有效性。

Motivation: 探讨LLMs在较弱攻击假设下的数据泄露问题，尤其是当攻击者仅拥有部分无序信息时。

Method: 提出了两种数据无关的攻击方法：基于似然比的攻击和PRISM方法。

Result: 实验证明两种方法在医学和法律场景中表现优异，与需要标记数据的基线分类器相当。

Conclusion: 研究表明，即使在较弱的攻击假设下，微调的LLMs仍易受片段特定提取攻击。

Abstract: Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [468] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu,Zhenglun Kong,Peiyan Dong,Changdi Yang,Tianqi Li,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Key words: LLM, 代理蒸馏, 推理-行为对齐, 模型压缩

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Structured Agent Distillation的框架，通过将LLM代理压缩为更小的学生模型，同时保持推理和行为一致性，显著降低了推理成本和模型大小。

Motivation: 解决大型语言模型（LLM）作为决策代理时的高推理成本和巨大模型规模问题。

Method: 提出分段式蒸馏方法，将轨迹分为{[REASON]}和{[ACT]}片段，并分别应用特定损失函数，以更精确地对齐教师模型的行为。

Result: 在ALFWorld、HotPotQA-ReAct和WebShop等数据集上，该方法显著优于基于token的蒸馏和模仿学习基线，实现了高效压缩且性能下降最小。

Conclusion: 分段对齐是提升代理效率与可部署性的关键。

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [469] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/abs/2505.13852)
*Yusheng Zhao,Chi Zhang,Yuxuan Du*

Key words: 量子系统基态学习,深度学习,机器学习,基准测试,资源利用

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文对比了深度学习和传统机器学习在量子系统基态学习任务中的表现，发现传统方法表现相当或更好，质疑了当前深度学习模型在此领域的必要性。

Motivation: 研究旨在明确深度学习模型在量子系统基态学习任务中的必要性及作用，解决以往研究中资源使用不公平的问题。

Method: 系统地对深度学习和传统机器学习模型进行了基准测试，涵盖了三种哈密顿量家族，资源使用保持一致，规模扩展到127个量子比特。

Result: 传统机器学习模型在所有任务中表现与深度学习相当甚至更好，随机化测试显示测量输入对深度学习预测性能影响很小。

Conclusion: 研究结果挑战了当前深度学习模型在量子系统学习中的必要性，并提供了对其有效利用的宝贵见解。

Abstract: Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [470] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/abs/2505.13857)
*Tian Sun,Yuqi Chen,Baihua Zheng,Weiwei Sun*

Key words: GPS轨迹，轨迹恢复，时空动态，PD-GNN，TedFormer

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的地图约束轨迹恢复方法TedTrajRec，通过PD-GNN和TedFormer分别捕捉时空交通动态和轨迹动态，解决了GPS轨迹采样率低的问题。

Motivation: GPS轨迹的低采样率和稀疏性限制了其在GPS系统中的直接应用，需要增强轨迹采样率。

Method: 提出TedTrajRec方法，包含PD-GNN（建模周期性模式和拓扑动态）和TedFormer（集成神经ODE的时间感知Transformer）。

Result: 在三个真实数据集上的实验表明，TedTrajRec表现优越。

Conclusion: TedTrajRec成功解决了GPS轨迹稀疏性问题，提升了轨迹恢复性能。

Abstract: In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [471] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/abs/2505.13858)
*Gonzalo E. Constante-Flores,Hao Chen,Can Li*

Key words: 深度学习, 安全关键任务, 约束满足, 神经网络, 优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个模型无关的框架，用于在神经网络输出上强制执行输入相关的线性等式和不等式约束，通过结合任务网络和安全网络来保证约束的满足。

Motivation: 由于深度学习模型在安全关键任务中的部署需求日益增长，而标准架构缺乏内置机制来强制满足硬约束（如物理定律、公平性或安全限制）。现有方法通常局限于简单约束、计算成本高或缺乏可行性保证。

Method: 框架结合了一个用于预测准确性的任务网络和一个基于随机和鲁棒优化决策规则的安全网络，最终预测是这两个子网络的凸组合，确保在训练和推理过程中约束的满足。

Result: 实验结果表明，该方法在基准回归任务中始终满足约束，同时保持竞争性的准确性和低推理延迟。

Conclusion: 提出的架构是约束函数的通用逼近器，并通过线性决策规则实现了计算上的可行性。

Abstract: Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [472] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/abs/2505.13873)
*Peisong Niu,Ziqing Ma,Tian Zhou,Weiqi Chen,Lefei Shen,Rong Jin,Liang Sun*

Key words: 天气预测, 过拟合, 自监督学习, Baguan模型, 预训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了天气预测中的过拟合问题，提出了一种自监督预训练方法，开发了Baguan模型，显著提升了预测精度。

Motivation: 由于真实天气数据有限，传统方法易过拟合，而AI模型需要创新策略以克服这一挑战。

Method: 使用Siamese Autoencoder进行自监督预训练，并根据不同预测时间微调模型。

Result: Baguan在实验中优于传统方法，并在下游任务中表现优异。

Conclusion: 预训练方法有效解决了过拟合问题，同时提升了天气预测的性能。

Abstract: Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [473] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu,Zhaoyi Yan,Yuanyi Wang,Yiming Zhang,Qi Zhou,Fei Wu,Hongxia Yang*

Key words: 模型融合,偏好对齐,概率信息,InfiFPO,LLM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: InfiFPO是一种新的模型融合方法，专注于偏好对齐阶段，通过整合多源概率信息和引入概率裁剪与最大边距融合策略，显著提升了LLM的性能。

Motivation: 现有模型融合方法在偏好对齐阶段主要依赖响应输出而忽略概率信息，限制了性能提升。InfiFPO旨在解决这一问题。

Method: InfiFPO通过序列级的多源概率合成替换DPO中的参考模型，结合概率裁剪和最大边距融合策略。

Result: 在11个基准测试中，InfiFPO显著优于现有方法，将Phi-4模型的平均性能从79.95提升到83.33。

Conclusion: InfiFPO有效整合了多源概率信息，显著提升了LLM在数学、编码和推理任务中的表现。

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [474] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/abs/2505.13896)
*Yingwei Zhang,Ke Bu,Zhuoran Zhuang,Tao Xie,Yao Yu,Dong Li,Yang Guo,Detao Lv*

Key words: 时间序列预测, 跨未来行为, CRAFT, Koopman预测, 趋势挖掘

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于跨未来行为（CFB）的时间序列预测方法CRAFT，通过挖掘CFB趋势来预测未来数据，解决了传统时间序列预测中因历史数据不足而导致的预测不确定性。

Motivation: 传统时间序列预测因历史数据有限而面临预测不确定性问题，论文探索利用跨未来行为（CFB）来提升预测准确性。

Method: CRAFT方法结合Koopman预测模块、内部趋势挖掘模块和外部趋势指导模块，提取和补充CFB的关键趋势，并通过需求约束损失校准预测结果。

Result: 离线大规模数据集和在线A/B测试均验证了CRAFT的有效性。

Conclusion: CRAFT通过利用跨未来行为的趋势，显著提升了时间序列预测的准确性。

Abstract: The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [475] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
*Róbert Csordás,Christopher D. Manning,Christopher Potts*

Key words: LLM, 深度效率, 残差流, Transformer, 回报递减

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 现代大型语言模型（LLM）的深度与性能相关，但研究发现模型并未高效利用其深度，而是将相同类型的计算分散到更多层中，导致规模增大的回报递减。

Motivation: 探讨现代LLM是否通过增加的深度来学习新类型的计算，还是仅仅将相同的计算分散到更多层中。

Method: 分析Llama 3.1和Qwen 3模型的残差流，包括比较子层输出、跳过层对预测的影响、多跳任务的验证，以及训练浅层模型到深层模型的线性映射。

Result: 深层模型的后半部分贡献较少，多跳任务中未发现深度用于组合子结果的证据，线性映射表明深层模型仅是将相同计算分散到更多层。

Conclusion: 深度增加并未带来新计算类型，而是用于更精细的残差调整，解释了Transformer架构规模增大的回报递减现象。

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [476] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/abs/2505.13899)
*Zeyu Michael Li,Hung Anh Vu,Damilola Awofisayo,Emily Wenger*

Key words: 机器学习, 表示相似性, 数据集重叠, 任务重叠

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 摘要探讨了数据集重叠和任务重叠如何影响机器学习模型间的表示相似性，发现二者均正相关，且结合时效果最强。

Motivation: 当前研究多关注模型表示相似性的属性和度量，但对其原因探讨较少。本文旨在探究数据集重叠和任务重叠对模型相似性的影响。

Method: 通过实验评估数据集重叠和任务重叠对模型表示相似性的影响。

Result: 研究发现，数据集重叠和任务重叠均与更高的表示相似性正相关，且二者结合时效果最强。

Conclusion: 数据集和任务的重叠是模型相似性的重要驱动因素。

Abstract: Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [477] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/abs/2505.13900)
*Zhanpeng Zhou,Yongyi Yang,Mahito Sugiyama,Junchi Yan*

Key words: 深度学习，训练动态，混沌效应，锥效应，eNTK

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文通过时间窗分析揭示了深度学习的双阶段特性，包括混沌效应和锥效应，说明了网络从敏感探索到稳定优化的动态过程。

Motivation: 研究深度神经网络的学习动态及其在训练过程中的相变现象，以更全面地理解其学习机制。

Method: 通过时间窗比较网络状态，分析参数扰动响应（混沌效应）和eNTK演化（锥效应）。

Result: 发现网络早期对初始条件高度敏感（混沌效应），后期功能轨迹被限制在窄锥形子集内（锥效应）。

Conclusion: 混沌效应和锥效应共同揭示了深度网络从探索到优化的动态结构转变。

Abstract: Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [478] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/abs/2505.13904)
*Fu Luo,Xi Lin,Mengyuan Zhong,Fei Liu,Zhenkun Wang,Jianyong Sun,Qingfu Zhang*

Key words: NCO, VRPs, 插入范式, L2C-Insert, TSP, CVRP

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于插入范式的学习方法L2C-Insert，用于神经组合优化（NCO），优于传统的顺序添加方法。

Motivation: 现有NCO方法通常采用顺序添加节点的策略，导致结果次优，因此探索插入范式以提高灵活性和解的质量。

Method: 提出L2C-Insert方法，通过预测插入位置、高效训练方案和高级推理技术，实现节点的灵活插入。

Result: 在TSP和CVRP的合成与现实实例中，L2C-Insert在多种问题规模下表现优越。

Conclusion: L2C-Insert通过插入范式显著提升了NCO的解法质量和灵活性，具有广泛的应用前景。

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [479] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/abs/2505.13907)
*Junyu Luo,Yusheng Zhao,Xiao Luo,Zhiping Xiao,Wei Ju,Li Shen,Dacheng Tao,Ming Zhang*

Key words: 无监督学习、领域自适应、图扩散、哈希学习、噪声鲁棒

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为COUPLE的新方法，通过图扩散视角解决无监督领域自适应检索中的噪声问题，实现了高效的跨领域知识迁移。

Motivation: 现有方法在无监督高效领域自适应检索中未能有效处理目标域噪声，直接对齐高层特征导致性能不佳，COUPLE旨在解决这些问题。

Method: 构建跨领域关系图，利用噪声鲁棒的图流扩散模拟领域转移动态，结合分层Mixup操作进行渐进式对齐，学习判别性哈希码。

Result: COUPLE在多个基准测试中表现出色，有效提升了检索性能。

Conclusion: 通过噪声鲁棒的图扩散和渐进对齐，COUPLE实现了高效的域自适应哈希学习。

Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [480] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/abs/2505.13910)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Key words: 虚假偏差,深度学习,鲁棒性,后处理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种无需组标签的后处理框架ShortcutProbe，用于缓解深度学习模型中的虚假偏差，并通过理论和实证验证其有效性。

Motivation: 深度学习模型常因学习目标与非本质特征之间的虚假相关性而降低性能，现有方法依赖昂贵的人工标注组标签，难以捕捉细微偏差。

Method: 提出ShortcutProbe框架，在模型的潜在空间中识别预测捷径，并重新训练模型以消除其对预测的影响。

Result: 理论和实验证明框架可有效提升模型对虚假偏差的鲁棒性。

Conclusion: ShortcutProbe是一种高效且实用的工具，无需组标签即可提升模型鲁棒性。

Abstract: Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [481] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu,Shaofeng Yin,Ningya Feng,Mingsheng Long*

Key words: 世界模型, 强化学习, 可验证奖励, 生成模型, MLE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了RLVR-World框架，通过强化学习与可验证奖励（RLVR）直接优化世界模型的特定任务目标，显著提升了语言和视频领域的世界模型性能。

Motivation: 传统的最大似然估计（MLE）训练目标常与世界模型的任务目标（如准确性或感知质量）不匹配，RLVR-World通过可验证奖励直接优化这些指标。

Method: 采用强化学习与可验证奖励（RLVR）框架，通过对解码预测的度量指标作为可验证奖励，优化世界模型的性能。

Result: 实验结果表明，RLVR-World在文本游戏、网页导航和机器人操作等领域的语言和视频世界模型中取得了显著性能提升。

Conclusion: RLVR为生成模型提供了一种有前景的后训练范式，能够更广泛地提升其效用。

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [482] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/abs/2505.13938)
*Amitayush Thakur,Jasper Lee,George Tsoukalas,Meghana Sistla,Matthew Zhao,Stefan Zetzche,Greg Durrett,Yisong Yue,Swarat Chaudhuri*

Key words: 程序合成,形式推理,Lean,基准测试,代码生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 介绍了一个名为CLEVER的高质量基准测试集，包含161个问题，用于验证Lean中的端到端代码生成，并评估了多种基于语言模型的方法。

Motivation: 为了解决现有基准测试中存在的问题，如测试用例监督、LLM生成注释和规范泄露实现逻辑，提供一个机器可验证的正确性标准。

Method: 设计了CLEVER基准测试集，每个问题包含生成规范和实施的任务，确保验证通过Lean类型检查器。

Result: 现有方法在完全验证方面表现不佳，表明程序合成和形式推理仍具挑战性。

Conclusion: CLEVER为程序合成和形式推理提供了一个前沿的挑战性基准。

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [483] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/abs/2505.13954)
*Jiahe Chen,Ziye Ma*

Key words: 非凸优化, 一阶方法, 零阶方法, VAMO, 方差缩减

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: VAMO是一种结合一阶和零阶梯度方法的新型优化器，解决了大规模非凸优化问题中收敛速度和计算效率的平衡问题。

Motivation: 研究旨在解决大规模非凸优化问题中一阶方法计算成本高和零阶方法收敛慢的问题。

Method: 提出了VAMO优化器，结合一阶小批次梯度和零阶有限差分探测，通过SVRG框架实现方差缩减。

Result: VAMO实现了维度无关的收敛速率，优于纯零阶方法和SGD，且在多点变体中进一步优化了误差平衡。

Conclusion: VAMO在实验中的表现优于现有方法，为计算受限场景提供了高效灵活的解决方案。

Abstract: Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [484] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/abs/2505.14005)
*Han Zhang,Yan Wang,Guanfeng Liu,Pengfei Ding,Huaxiong Wang,Kwok-Yan Lam*

Key words: GNN, 可解释性, OPEN, 分区样本空间, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的GNN解释方法OPEN，解决了现有方法在捕获GNN决策逻辑和通用性上的局限性，表现优于现有技术。

Motivation: 提升GNN的可靠性和透明度，解决现有XGNN方法在捕获完整决策逻辑和通用性上的不足。

Method: 提出OPEN方法，通过分区样本空间和学习不同分布的决策逻辑，避免严格前提条件。

Result: OPEN能捕获几乎完整的决策逻辑，在保真度和效率上优于现有方法，并增强鲁棒性。

Conclusion: OPEN是一种全面且无需前提条件的GNN解释方法，显著提升了性能和适用性。

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [485] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/abs/2505.14011)
*Yifei Jin,Xin Zheng,Lei Guo*

Key words: 司法量刑预测，解释性模型，SMS，MLMS，中国刑法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种饱和机制量刑（SMS）模型，结合动量最小均方（MLMS）自适应算法，解决了现有端到端模型缺乏法律解释性的问题。

Motivation: 现有司法量刑预测研究多依赖端到端模型，忽视了量刑逻辑且缺乏解释性，而解释性对学术研究和司法实践至关重要。

Method: 提出SMS模型（基于中国刑法），并开发MLMS自适应算法；建立自适应预测的数学理论；构建中国故意伤害（CIBH）数据集。

Result: 实验表明，该方法预测精度接近理论上限，验证了模型和算法的有效性。

Conclusion: SMS模型和MLMS算法结合，不仅具有法律解释性，且预测精度高，适用于司法实践。

Abstract: Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [486] [Adversarial Training from Mean Field Perspective](https://arxiv.org/abs/2505.14021)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Key words: 对抗训练, 深度神经网络, 平均场理论, 紧上界, 网络容量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文首次对随机深度神经网络的对抗训练进行了理论分析，提出了基于平均场理论的新框架，并推导了对抗损失的紧上界，揭示了网络结构对对抗训练的影响。

Motivation: 对抗训练的动态机制尚不明确，本研究旨在填补这一空白，通过理论分析揭示对抗训练的本质。

Method: 引入基于平均场理论的新框架，分析随机深度神经网络的对抗训练，推导$\ell_q$范数对抗损失的紧上界。

Result: 证明了无捷径网络的对抗训练不可行性，发现对抗训练降低网络容量，但网络宽度可缓解此问题。

Conclusion: 对抗训练的理论分析为设计更稳健的神经网络提供了新视角。

Abstract: Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [487] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/abs/2505.14024)
*Di Wu,Qian Li,Heng Yang,Yong Han*

Key words: 联邦学习, 无目标攻击, 鲁棒聚合, FedGraM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedGraM 是一种新颖的联邦学习鲁棒聚合方法，通过检测并移除无目标攻击来保护模型性能。

Motivation: 联邦学习易受无目标攻击影响，现有防御方法因数据异质性效果有限。论文旨在通过检测和移除攻击来减轻其影响。

Method: 提出 FedGraM 方法，服务器维护辅助数据集，计算本地模型的嵌入 Gram 矩阵范数，筛选并移除潜在恶意模型。

Result: 实验表明，FedGraM 在有限数据下表现优异，优于现有防御方法。

Conclusion: FedGraM 能有效防御无目标攻击，提升联邦学习的鲁棒性。

Abstract: Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [488] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/abs/2505.14033)
*Guoming Li,Jian Yang,Yifan Chen*

Key words: 图神经网络, 异质性图, 滤波框架, 节点分类, 异常检测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新型的图神经网络方法CPF，通过结合图级和节点级滤波的优势，解决了异质性图中的分类问题。

Motivation: 传统图神经网络在异质性图中的表现不足，缺乏对图级和节点级滤波的统一框架，需要一种更灵活的方法。

Method: 提出CPF方法，通过图粗化和k均值聚类生成节点分区，分别在结构和特征上进行分区级滤波。

Result: CPF在节点分类和异常检测任务中表现优异，证明了其理论优势和实用性。

Conclusion: CPF通过分区级滤波，显著提升了在异质性图中的分类性能，同时避免了过度参数化。

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [489] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/abs/2505.14036)
*Gyubin Lee,Truong Nhat Nguyen Bao,Jaesik Yoon,Dongwoo Lee,Minsu Kim,Yoshua Bengio,Sungjin Ahn*

Key words: 扩散模型, 自适应推理, 计算效率, 双向扩散, 探索-利用平衡

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种自适应推理时间扩展方法ABCD，通过双向扩散循环动态调整计算量，提升了生成模型的性能与效率。

Motivation: 现有扩散模型的推理扩展方法通常采用固定的去噪计划，无法根据实例难度或任务需求动态分配计算资源。

Method: 提出了ABCD框架，包含三个组件：循环扩散搜索、自动探索-利用平衡和自适应思考时间，通过双向扩散循环动态优化计算分配。

Result: 实验表明，ABCD在多种任务中性能提升的同时保持了计算效率。

Conclusion: ABCD为扩散模型的自适应推理扩展提供了一种灵活高效的解决方案。

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [490] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/abs/2505.14039)
*Luca Pellegrini,Massimiliano Ghiotto,Edoardo Centofanti,Luca Franco Pavarino*

Key words: 离子模型, 傅里叶神经算子, 高维动力学, 多尺度非线性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究探讨了傅里叶神经算子（FNO）能否有效学习高维动力学系统中的所有状态变量，并通过三个离子模型的实验验证了其有效性。

Motivation: 由于离子模型的刚性和多尺度非线性特性，用人工神经网络逼近这些模型面临挑战。研究试图通过FNO解决这些问题，并在高维系统中验证其表现。

Method: 研究使用FNO学习离子模型中所有状态变量的演化，并通过自动超参数调优在约束和非约束两种场景下优化模型配置。

Result: FNO在三个不同维度的离子模型（FitzHugh-Nagumo、Hodgkin-Huxley、O'Hara-Rudy）中均能准确捕捉复杂多尺度动态，非约束架构训练效率更高。

Conclusion: 傅里叶神经算子能够准确捕捉高维复杂动态系统，尤其在多尺度非线性问题中表现优异。

Abstract: Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [491] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei,Ming Lin,Fanjiang Ye,Fengguang Song,Liangliang Cao,My T. Thai,Tianbao Yang*

Key words: 模型引导, DRRho风险最小化, 分布鲁棒优化, 对比学习, CLIP

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为‘模型引导’的学习范式，通过参考模型指导目标模型的训练，优化数据选择或权重。提出理论驱动的框架DRRho风险最小化，并进行理论分析和实验验证。

Motivation: 目前模型引导的学习范式缺乏理论支持，导致性能不佳，本文旨在提供理论框架和实践方法。

Method: 提出基于分布鲁棒优化（DRO）的DRRho风险最小化框架，并通过对比学习与DRO的联系，提出DRRho-CLIP方法。

Result: 理论分析显示该方法在泛化和数据效率上的优势，实验验证DRRho-CLIP优于现有启发式方法和无参考模型的CLIP。

Conclusion: 本文为模型引导提供了首个理论支持，验证了其有效性，并展示了在CLIP任务中的优越性。

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [492] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/abs/2505.14040)
*Jingyun Zhang,Hao Peng,Li Sun,Guanlin Wu,Chunyang Liu,Zhengtao Yu*

Key words: Graph Structure Learning, Deep Structural Entropy, Unsupervised Clustering, Graph Neural Networks, Contrastive Learning

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为DeSE的无监督图聚类框架，结合深度结构熵来解决现有方法对原始图结构依赖过强的问题，通过量化结构信息和深度神经网络优化聚类效果。

Motivation: 当前图学习方法（如GNNs、GATs）通常依赖原始图结构，若图过于稀疏或包含噪声边，性能会下降。此外，这些方法未能完全捕捉节点间的潜在图结构。

Method: DeSE框架通过软分配计算结构熵，设计结构学习层（SLL）生成属性图优化原始结构，并采用基于GNNs的聚类分配方法（ASS）进行节点嵌入和聚类。

Result: 在四个基准数据集上的实验表明，DeSE在效果和可解释性上优于八种代表性无监督图聚类基线方法。

Conclusion: DeSE通过深度结构熵和优化图结构，显著提升了聚类的稳定性和一致性。

Abstract: Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [493] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/abs/2505.14042)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Key words: 对抗性训练, Transformer, 鲁棒性, 上下文学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过对抗性预训练的Transformer可以作为鲁棒的基础模型，消除下游任务中对对抗性训练的需求，实现无需参数更新的鲁棒泛化，但也存在局限性。

Motivation: 探索对抗性训练的高计算成本问题，研究对抗性预训练Transformer在多样化任务上的潜力，以减少对抗性训练的需求。

Method: 利用对抗性预训练的Transformer，通过上下文学习（in-context learning）实现多个未见任务的鲁棒泛化，无需额外训练或参数更新。

Result: 对抗性预训练的Transformer能够专注于鲁棒特征并抵抗攻击，但在某些（不实际）条件下无法实现普遍鲁棒性，且存在准确性与鲁棒性的权衡。

Conclusion: 对抗性预训练的Transformer是一种有效的鲁棒基础模型，但需大量上下文演示，并面临准确性与鲁棒性的权衡。

Abstract: Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [494] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/abs/2505.14044)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Cheng Chen*

Key words: 广义类别发现,流形容能力,核范数,聚类,开放世界学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的广义类别发现方法MTMC，通过最大化类令牌的流形容能力来提升聚类的准确性和多样性，优于现有方法。

Motivation: 传统GCD方法在最小化簇内差异时牺牲了流形容能力，限制了类内表示的丰富性。MTMC旨在通过最大化流形容能力来解决这一缺陷。

Method: MTMC利用奇异值的核范数作为流形容能力的度量，确保样本表示的信息丰富性和结构完整性。

Result: 实验证明MTMC在粗粒度和细粒度数据集上优于现有GCD方法，提升了聚类准确性和类别数量估计。

Conclusion: MTMC通过增强表示的完整性和类间可分离性，减少了维度崩溃，是开放世界学习的重要组件。

Abstract: Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [495] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan,Deqing Fu,Julian Asilis,Ollie Liu,Dani Yogatama,Vatsal Sharan,Robin Jia,Willie Neiswanger*

Key words: 多模态大语言模型, 引导方法, 稀疏自编码器, 均值漂移, 线性探测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，通过文本引导方法可以提升多模态大语言模型（MLLMs）在视觉任务中的表现，且无需额外数据或计算资源。

Motivation: 目前缺乏针对多模态大语言模型的引导方法，研究旨在探索利用文本模型的引导向量是否能够提升MLLMs的性能。

Method: 使用稀疏自编码器（SAEs）、均值漂移（mean shift）和线性探测（linear probing）从文本模型中提取引导向量，并应用于MLLMs。

Result: 文本引导显著提升了MLLMs的多模态准确性，均值漂移方法在空间关系任务中提升了7.3%，计数任务中提升了3.3%。

Conclusion: 文本引导向量是提升MLLMs性能的高效机制，且具有通用性。

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [496] [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)
*Xinyi Shang,Peng Sun,Fengyuan Liu,Tao Lin*

Key words: 数据优化,深度学习,未标记数据,可扩展性,可持续性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的数据为中心的方法CoOpt，通过优化未标记数据提升深度学习训练的效率和可持续性。

Motivation: 现有以模型为中心的方法存在局限性，知识无法重用和扩展，需要一种新方法来优化数据本身。

Method: 提出CoOpt框架，通过并行化协作优化未标记数据，利用公开任务无关模型提高可扩展性和复用性。

Result: 在Tiny-ImageNet和ImageNet-1K上分别实现了13.6%和6.8%的性能提升，训练速度分别提升了1.94倍和1.2倍。

Conclusion: CoOpt框架有效提升了深度学习训练的效率和可持续性，证明了数据优化的潜力。

Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [497] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/abs/2505.14122)
*Ehsan Masoudian,Ali Mirzaei,Hossein Bagheri*

Key words: 野火风险, 遥感, 机器学习, 气候因素, 人类活动

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究通过遥感和机器学习技术分析了伊朗野火风险的多因素影响，发现气候条件和人类活动均对野火风险有显著贡献，且人类因素在季节性分析中更为突出。

Motivation: 探究伊朗野火风险的复杂因素，尤其是气候与人类活动的相互作用，为火灾管理提供科学依据。

Method: 采用遥感、GIS云处理和机器学习算法，结合多场景数据采样策略，评估野火易感性。

Result: 气候参数（如土壤湿度和温度）和人类活动（如人口密度和电力线接近度）均显著影响野火风险，人类因素在季节性分析中更为突出。

Conclusion: 研究为伊朗野火动态提供了新见解，生成了高分辨率易感性地图，明确了高危险区域，强调需采取有效防火策略。

Abstract: This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [498] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/abs/2505.14125)
*Viet Anh Khoa Tran,Emre Neftci,Willem. A. M. Wybo*

Key words: 持续学习,对比学习,无监督学习,任务调制,预测编码

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出任务调制对比学习（TMCL），受新皮层生物物理机制启发，通过对比损失实现无监督学习，平衡稳定性和可塑性。

Motivation: 生物大脑能够持续从无标签数据流中学习，同时整合稀疏标签信息而不影响泛化能力，而机器学习方法容易因任务调整而遗忘原有知识。

Method: 任务调制对比学习（TMCL）利用预测编码原理，通过对比损失构建视图不变的表示空间，并在标记样本出现时学习新的仿射调制，以改进分类。

Result: 实验表明，在类增量学习和迁移学习中，TMCL优于当前最先进的无监督和监督方法，仅需1%的标签即可取得显著效果。

Conclusion: 研究表明，自上而下的调制在平衡学习稳定性和可塑性方面起关键作用。

Abstract: Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [499] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/abs/2505.14126)
*Yuan-Hao Jiang,Kezong Tang,Zi-Wei Chen,Yuang Wei,Tian-Yi Liu,Jiayi Wu*

Key words: 知识组件(KC), KC图, 多智能体系统, 大型语言模型, 学习路径识别

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MAS-KCL算法通过多智能体系统优化知识组件(KC)图结构，提升学习路径识别效率，助力教育可持续发展。

Motivation: 精确的KC图能帮助教育者定位学习者表现不佳的根源，从而进行有针对性的教学干预。

Method: 开发了基于多智能体系统和大型语言模型的MAS-KCL算法，结合双向反馈机制优化KC图结构。

Result: 算法在合成和真实教育数据集上验证了其有效性，特别是在学习路径识别方面。

Conclusion: MAS-KCL算法有助于教师设计更全面的学习计划，提升教育目标的实现效率。

Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [500] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/abs/2505.14128)
*Yihang Du,Jiaying Hu,Suyang Hou,Yueyang Ding,Xiaobo Sun*

Key words: 空间标注,相似性评估,图模型,分布差异,SLAM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个名为SLAM的框架，用于全面评估空间标注的相似性，考虑了标注一致性、拓扑结构和异质性影响，并通过实验验证其优越性。

Motivation: 现有方法在评估空间标注相似性时未能全面考虑标注一致性、拓扑结构和异质性影响，因此需要一种更综合的测量方法。

Method: 将空间标注转化为基于位置组织和标签的图模型，提取属性分布，计算分布差异以反映标注差异，具体实现为SLAM框架。

Result: SLAM在模拟和真实空间转录组数据中表现优于其他评估指标，能全面准确地反映标注质量。

Conclusion: SLAM提供了一种更全面的空间标注相似性评估方法，适用于科学研究和实践应用。

Abstract: Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [501] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/abs/2505.14136)
*Ryo Bertolissi,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Key words: Mixture of expert models, Test-Time Model Merging, Test-Time Training, model merging, language models

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种Test-Time Model Merging (TTMM)方法，通过合并模型避免推理时的额外开销，显著扩展MoE模型中专家数量，且性能接近Test-Time Training (TTT)，但计算成本更低。

Motivation: 为了解决当前MoE模型因训练和推理成本高而仅使用少量专家的局限性，提出TTMM方法，以更高效的方式扩展专家数量。

Method: TTMM利用模型合并技术，在训练阶段分摊TTT的计算成本，从而在测试时避免额外开销。

Result: TTMM性能随专家数量增加而提升，接近TTT，且对1B参数的基础模型，TTMM测试速度比TTT快100倍以上。

Conclusion: TTMM是一种扩展测试时训练的高效且经济的方案。

Abstract: Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [502] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/abs/2505.14139)
*Marvin Alles,Nutan Chen,Patrick van der Smagt,Botond Cseke*

Key words: 能量引导流匹配,流模型,强化学习,FlowQ,离线学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为能量引导流匹配的新方法，用于在训练阶段引导流模型，避免推理时的引导需求，适用于强化学习等任务。

Motivation: 现有研究多关注扩散模型推理时的引导，而训练阶段的引导较少探索，需要一种在训练中引导流模型的方法。

Method: 提出能量引导流匹配方法，通过近似高斯路径学习条件速度场，并结合离线强化学习算法FlowQ。

Result: 方法在性能上具有竞争力，且策略训练时间不随流采样步骤数增加。

Conclusion: 能量引导流匹配有效提升了流模型的训练效果，适用于目标分布由数据和能量函数共同定义的任务。

Abstract: The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [503] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/abs/2505.14161)
*Ting Wei,Biao Mei,Junliang Lyu,Renquan Zhang,Feng Zhou,Yifan Sun*

Key words: 贝叶斯联邦学习, 非参数推断, Wasserstein质心, 变分推断

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FedWBA是一种新颖的个性化贝叶斯联邦学习方法，通过非参数后验表示和基于Wasserstein质心的全局聚合，克服了现有方法的局限性，提供了理论和实证上的优势。

Motivation: 解决现有PBFL方法在参数假设和后验平均聚合上的局限性。

Method: 客户端采用基于粒子的变分推断进行非参数后验表示；服务器端引入基于Wasserstein质心的全局聚合方法。

Result: FedWBA在预测精度、不确定性校准和收敛速度上优于基线方法，并具有理论收敛保证。

Conclusion: FedWBA通过创新的本地推断和全局聚合方法，显著提升了PBFL的性能和鲁棒性。

Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [504] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/abs/2505.14170)
*Chen Zhang,Weixin Bu,Zeyi Ren,Zhengwu Liu,Yik-Chung Wu,Ngai Wong*

Key words: GraNT, GCN, 非参数教学, 图结构数据, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: GraNT通过非参数教学视角重新解释图结构数据的学习过程，显著提升图神经网络的训练效率。

Motivation: 解决图卷积网络（GCNs）学习图结构数据时的高成本问题。

Method: 提出Graph Neural Teaching（GraNT），通过选择图-属性对子集促进GCN训练收敛。

Result: 训练时间显著减少（图级别回归-36.62%，分类-38.19%；节点级别回归-30.97%，分类-47.30%），且保持泛化性能。

Conclusion: GraNT在提升GCN学习效率方面具有一致性和有效性。

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [505] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Key words: 大语言模型, 安全对齐, 几何视角, 子空间, 微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了大语言模型（LLMs）安全对齐的几何视角，发现安全行为与不安全行为在参数和激活空间中重叠，挑战了安全局部化的假设。

Motivation: 研究LLMs的安全对齐是否可以通过几何方向或子空间来隔离或保护，以防止微调导致的安全退化。

Method: 对参数和激活空间进行综合实证研究，分析安全相关行为是否集中在特定子空间，以及与通用学习的可分离性。

Result: 安全与不安全行为在子空间中重叠，未发现能选择性控制安全的子空间，安全依赖于模型的广泛学习动态。

Conclusion: 子空间防御可能面临根本限制，需开发替代策略以在训练中保持对齐。

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [506] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/abs/2505.14190)
*Ni Ding,Miao Qiao,Jiaxing Xu,Yiping Ke,Xiaoyu Zhang*

Key words: α-GAN, Rényi measures, generative adversarial networks, vanishing gradient, optimization

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于Rényi度量的生成对抗网络（α-GAN），通过优化Rényi交叉熵作为值函数，解决了生成器和判别器之间的最小最大问题。α=1时退化为传统GAN，实验证明在α∈(0,1)时收敛更快，并能解决梯度消失等问题。

Motivation: 探索Rényi度量在生成对抗网络中的应用，以改进传统GAN的性能和收敛速度，解决如梯度消失等常见问题。

Method: 提出α-GAN，通过Rényi交叉熵定义值函数，形成生成器和判别器的对抗优化问题，参数化Rényi阶数α，并在概率空间中进行优化。

Result: 实验表明在α∈(0,1)范围内梯度放大，收敛速度加快，同时可能解决梯度消失问题。现有Rényi-GAN未充分探索该范围。

Conclusion: α-GAN在α∈(0,1)范围内表现出更快的收敛性和潜在的问题解决能力，为GAN的研究提供了新视角。

Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [507] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/abs/2505.14201)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Key words: Transformer, FlashAttention, 硬件优化, 数值稳定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了FLASH-D，一种优化FlashAttention内核的新方法，简化了计算流程，减少了硬件资源消耗，同时保持了性能和数值稳定性。

Motivation: Transformer的注意力机制计算效率受限于矩阵运算和softmax重缩放，FlashAttention通过在线softmax计算改进了这一过程，但仍有机会进一步优化。

Method: 提出FLASH-D，将softmax除法隐藏在其他非线性函数评估中，实现数值稳定的指数计算，减少计算成本且无需数值近似。

Result: 在28nm硬件实现中，FLASH-D平均减少了22.8%的面积和20.3%的功耗，且无性能损失。

Conclusion: FLASH-D简化了FlashAttention内核，显著提升了硬件效率，同时保留了原有性能。

Abstract: The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [508] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/abs/2505.14202)
*Zhicheng Chen,Shibo Feng,Xi Xiao,Zhong Zhang,Qing Li,Xingyu Gao,Peilin Zhao*

Key words: 多尺度离散变换器,时间序列生成,向量量化,率失真定理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了多尺度离散变换器(MSDformer)，以解决现有DTM方法无法捕获多尺度时间模式及缺乏理论指导的问题。

Motivation: 现有DTM方法在复杂时间序列生成中存在无法捕获多尺度时间模式和缺乏理论基础的局限性。

Method: 提出MSDformer，使用多尺度时间序列分词器和多尺度自回归标记建模技术，并结合率失真定理进行理论验证。

Result: 实验表明MSDformer显著优于现有方法，多尺度信息建模显著提升生成质量。

Conclusion: 多尺度信息建模和理论指导可显著改进DTM时间序列生成。

Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [509] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/abs/2505.14206)
*Flavio Di Martino,Franca Delmastro*

Key words: 合成数据, 生成对抗网络, 扩散模型, 时间序列, 移动健康

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了移动传感器数据合成生成的挑战，提出新评估框架，揭示现有方法局限性，并展望未来研究方向。

Motivation: 解决移动健康领域数据稀缺和隐私问题，推动人工智能应用。

Method: 系统评估现有生成模型，引入新评估框架，测试多模态、长时依赖和条件生成能力。

Result: 发现现有方法在跨模态一致性、时间连贯性和下游任务性能上的局限。

Conclusion: 需改进合成时间序列生成技术以提升移动健康领域适用性。

Abstract: The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [510] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/abs/2505.14211)
*Qu Wang,Yan Xia*

Key words: 动态网络、链接预测、张量轮分解、PID控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 介绍了一种基于PID控制的张量轮分解模型（PTWD），用于动态网络的链接预测，通过结合PID控制原理提升预测精度。

Motivation: 动态网络中链接预测是网络科学中的基础挑战，传统静态方法难以捕捉时间依赖性和权重动态，而张量方法为建模多维交互提供了新思路。

Method: 提出PTWD模型，结合张量轮分解（TWD）的拓扑结构和PID控制原理，优化模型参数学习。

Result: 在四个真实数据集上的实验验证了PTWD模型相比其他模型具有更高的链接预测精度。

Conclusion: PTWD模型通过整合PID控制，显著提升动态网络链接预测的准确性。

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [511] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/abs/2505.14214)
*Mattes Mollenhauer,Nicole Mücke,Dimitri Meunier,Arthur Gretton*

Key words: 岭回归, 再生核希尔伯特空间, 重尾噪声, Fuk-Nagaev不等式, 积分算子

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了在噪声具有有限高阶矩的情况下，再生核希尔伯特空间中岭回归的性能，提出了基于积分算子框架的过风险界，结合次高斯和多项式项。

Motivation: 探索岭回归在存在有限高阶矩噪声时的性能，填补以往研究通常假设次指数噪声的局限性。

Method: 使用积分算子框架和Fuk-Nagaev不等式，推导过风险界。

Result: 在标准特征值衰减条件下，实现了最优收敛速率，显示正则最小二乘法对重尾噪声的渐近鲁棒性。

Conclusion: 岭回归在重尾噪声下仍能保持良好性能，且收敛速率最优。

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [512] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/abs/2505.14217)
*Jorge Fabila,Lidia Garrucho,Víctor M. Campello,Carlos Martín-Isla,Karim Lekadir*

Key words: 联邦学习,肺结核诊断,低资源地区,医疗AI,数据隐私

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究探讨了在非洲低资源环境下使用联邦学习（FL）进行肺结核（TB）诊断的可行性，通过胸部X光片实现无需共享患者原始数据的协作模型训练。

Motivation: 解决低资源地区因隐私问题和数据稀缺而难以使用传统集中式AI模型的挑战。

Method: 研究涵盖非洲八个国家的医院和研究机构，对比本地训练模型与联邦模型的性能，评估FL的实际应用效果。

Result: FL展示了在医疗资源匮乏地区的潜力，但基础设施薄弱、网络不可靠、数字素养低以及法规不完善等问题限制了其应用。

Conclusion: FL在医疗AI领域前景广阔，但需改善基础设施、提升数字素养并加强法规支持以推动广泛应用。

Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [513] [Fast and close Shannon entropy approximation](https://arxiv.org/abs/2505.14234)
*Illia Horenko,Davide Bassetti,Lukáš Pospíšil*

Key words: 香农熵、快速熵近似、非奇异梯度、机器学习、计算效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种快速熵近似方法（FEA），通过非奇异有理近似解决香农熵及其梯度的计算效率问题，显著提升了计算速度和模型质量。

Motivation: 香农熵和冯·诺依曼熵在物理学、信息论、机器学习和量子计算中广泛应用，但其梯度奇异性和计算高成本导致工具效率低下。

Method: 提出快速熵近似（FEA），采用非奇异有理近似方法，显著降低计算复杂度和误差。

Result: FEA的平均绝对误差为10^-3，计算速度提升50%，在机器学习特征选择任务中，模型质量显著提升，速度快2-3个数量级。

Conclusion: FEA为解决熵计算的高成本和低效问题提供了有效方案，适用于多种AI工具。

Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [514] [Learning with Local Search MCMC Layers](https://arxiv.org/abs/2505.14240)
*Germain Vivier-Ardisson,Mathieu Blondel,Axel Parmentier*

Key words: 组合优化、神经网络、MCMC、局部搜索、动态车辆路径

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于MCMC的理论方法，用于在神经网络中整合不可精确的组合优化求解器，降低了计算负担，并在动态车辆路径问题上验证了有效性。

Motivation: 现有方法缺乏理论保证或在依赖不可精确求解器时表现不佳，尤其是NP难问题常依赖局部搜索启发式方法，因此需要一种理论可靠的学习方法。

Method: 通过模拟退火与Metropolis-Hastings的联系，将局部搜索中的邻域系统转化为MCMC的提议分布，构建可微组合层和损失函数。

Result: 在动态车辆路径问题上的大规模实验表明，该方法有效降低了学习计算负担。

Conclusion: 通过理论驱动的MCMC方法，成功实现了不可精确组合求解器的可微学习。

Abstract: Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [515] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/abs/2505.14251)
*Bar Mahpud,Or Sheffet*

Key words: 差分隐私, 二阶矩估计, 子采样, zCDP, 异常值

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私二阶矩估计算法，通过子采样假设在最坏情况下实现隐私与效用的平衡。

Motivation: 解决差分隐私下二阶矩估计的难题，尤其是在存在异常值或最坏情况输入时的准确性。

Method: 基于子采样假设，提出递归算法框架，满足零集中差分隐私（zCDP），并在高概率下保持二阶矩估计的准确性。

Result: 算法能在存在异常值的情况下，高概率地近似分布的二阶矩矩阵。

Conclusion: 该方法在保证隐私的同时，有效提升了二阶矩估计的鲁棒性和准确性。

Abstract: We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [516] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/abs/2505.14252)
*Mouad Elaarabi,Domenico Borzacchiello,Philippe Le Bot,Nathan Lauzeral,Sebastien Comas-Cardona*

Key words: 物理信息神经网络, 序列编码, 动态系统识别, 实时应用, 自适应模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合序列编码和物理信息神经网络的方法，用于实时应用中的参数识别，能够适应变量参数、边界和初始条件的变化，无需重新训练模型。

Motivation: 现有方法在处理参数或边界条件变化时需要重新训练模型，限制了其实际应用的灵活性。本文旨在通过序列编码和PINN的结合，构建一个能够动态适应变化的模型。

Method: 采用Deep Sets或序列编码器将动态参数、边界条件和初始条件编码为特征，作为PINN的输入，从而实现对变化的适应性。实验涵盖Rossler ODE系统、2D Navier-Stokes问题以及1D热监控问题。

Result: 模型在三个不同问题中表现出色，能够处理噪声并泛化到新场景，例如在2D问题中仅通过少量压力数据识别入口速度剖面。

Conclusion: 所提架构能够有效适应参数、边界和初始条件的变化，为实时应用提供了灵活性，且在不同问题中均表现出良好的性能。

Abstract: In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [517] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Key words: 强化学习, 群组相对优势估计, AAPO, 动量估计, 训练效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习算法AAPO，通过基于动量的优势估计优化交叉熵损失，有效解决了现有群组相对优势估计方法的训练效率问题。

Motivation: 现有群组相对优势估计方法（如GRPO）在优势接近零时训练效率低下，无法满足实际需求。

Method: 提出AAPO算法，通过动量增强的优势估计来优化交叉熵损失。

Result: 在多个数学推理基准测试中，AAPO表现出优越性能。

Conclusion: AAPO有效提升了训练效率，优于传统RL方法。

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [518] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/abs/2505.14273)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Key words: 函数逼近，Kolmogorov-Arnold Networks (KAN)，XCSF，局部模型，规则学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: X-KAN提出了一种新颖的方法，通过结合局部KAN模型和XCSF的自适应分区能力，显著提升了函数逼近的准确性，尤其适用于局部复杂或不连续的函数。

Motivation: 现有神经网络方法依赖单一全局模型，难以处理局部复杂或不连续的函数。X-KAN旨在通过局部KAN模型和XCSF框架解决这一问题。

Method: X-KAN通过XCSF框架优化多个局部KAN模型，将KAN模型作为规则结果，通过规则前件定义局部区域。

Result: 实验显示，X-KAN在人工测试函数和真实数据集上显著优于传统方法（如XCSF、多层感知机和KAN），且在局部复杂或不连续函数中表现优异。

Conclusion: X-KAN证明了局部KAN模型在XCSF框架中的有效性，提供了更高效和准确的函数逼近方法。

Abstract: Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [519] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen,Chaoyi Zhang,Jing Liu,Yutao Zeng,Zeyue Xue,Zhiheng Liu,Yunshui Li,Jin Ma,Jie Huang,Xun Zhou,Ping Luo*

Key words: 量化感知训练, W4A4, 缩放定律, 量化误差, 混合精度量化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种统一的量化感知训练（QAT）缩放定律，研究了模型大小、训练数据量和量化分组大小对4比特精度（W4A4）量化误差的影响，并揭示了激活量化误差是主要瓶颈。

Motivation: 大语言模型（LLMs）对计算和内存资源需求高，量化感知训练（QAT）可以降低模型精度而不影响性能，但现有QAT缩放定律忽略了一些关键因素，如训练令牌数量和量化粒度。

Method: 通过268个QAT实验，研究了量化误差与模型大小、训练数据量和量化分组大小的关系，并分解了W4A4量化误差的权重和激活分量。

Result: 量化误差随模型增大而减少，但随训练令牌增加和量化粒度变粗而上升。激活量化误差（尤其是FC2层中的异常值）是主要瓶颈，通过混合精度量化可解决。

Conclusion: 激活量化误差是W4A4 QAT的主要瓶颈，减少权重量化误差在更多训练数据时也很重要，这些发现为QAT研究提供了重要见解。

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [520] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/abs/2505.14312)
*Kyungeun Lee,Moonjung Eo,Hye-Seung Cho,Dongmin Kim,Ye Seul Sim,Seoyoon Kim,Min-Kook Suh,Woohyung Lim*

Key words: 表格数据, 多维度评估, 数据感知, 模型行为, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MultiTab是一个针对表格学习算法的多维数据感知评估框架，通过分类196个数据集并根据关键数据特征评估13个模型，揭示了模型性能对不同数据特征的敏感性，为模型设计提供了指导。

Motivation: 现有基准测试依赖平均指标，无法揭示模型在不同数据特征下的行为差异。

Method: 提出MultiTab框架，分类数据集并评估代表性模型在不同数据特征下的表现。

Result: 模型性能对数据特征敏感，不同模型在不同数据特征下的表现各异。

Conclusion: MultiTag为模型设计和选择提供了更科学的方法和实用指导。

Abstract: Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [521] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/abs/2505.14338)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Jack Stade,Amir Yehudayoff*

Key words: ReLU神经网络, CPWL函数, 深度优化, 最大值函数

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究ReLU神经网络的表达能力，特别是其深度。之前的研究认为计算所有连续分段线性（CPWL）函数需要$⌈ \log_2(n+1) \rceil$层隐藏层，但本文反驳了这一猜想，证明$⌈\log_3(n-1)⌉+1$层即可。

Motivation: Hertrich等人（NeurIPS'21）提出$⌈ \log_2(n+1) \rceil$层隐藏层为最优深度的猜想，本文旨在验证或反驳这一猜想。

Method: 通过构造性证明，展示仅需$⌈\log_3(n-1)⌉+1$层隐藏层即可计算所有CPWL函数。关键步骤包括使用两层隐藏层精确表示五个输入的最大值函数。

Result: 成功反驳了Hertrich等人的猜想，证明$⌈\log_3(n-1)⌉+1$层足够。且最大函数的计算仅需$⌈\log_3(n-2)⌉+1$层，接近已知下界。

Conclusion: ReLU神经网络的计算深度需求低于之前推测，且与数值下界接近，揭示了其更高的表达效率。

Abstract: This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [522] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/abs/2505.14345)
*Aydin Abedinia,Shima Tabakhi,Vahid Seydi*

Key words: 半监督学习, 距离加权, 模型泛化, 数据噪声, 不平衡数据集

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了一种半监督学习框架，通过距离加权机制优先处理关键训练样本，从而提升模型泛化能力和鲁棒性。

Motivation: 解决标记数据有限场景下分类性能不佳的问题，特别是在噪声或不平衡数据集的挑战性情况中。

Method: 采用基于距离的加权机制，结合不确定性一致性和图表示技术，优先选择靠近测试数据的样本。

Result: 在12个基准数据集上的实验表明，该方法在准确性、精确度和召回率等关键指标上显著优于现有方法。

Conclusion: 该框架为半监督学习提供了鲁棒且实用的解决方案，适用于医疗和安全等数据受限领域。

Abstract: Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [523] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/abs/2505.14352)
*Bartosz Cywiński,Emil Ryd,Senthooran Rajamanoharan,Neel Nanda*

Key words: 语言模型,隐藏知识,可解释性,Taboo模型,黑盒方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了如何从语言模型中提取隐藏知识，通过训练一个Taboo模型并测试不同方法来揭示秘密词。

Motivation: 随着语言模型变得更强大和复杂，确保其可信和可靠至关重要，尤其是预防模型欺骗或隐藏信息的行为。

Method: 训练一个不直接提及秘密词的Taboo模型，并采用黑盒方法和可解释性技术（如logit lens和稀疏自编码器）揭示秘密词。

Result: 实验证明，两种方法在概念验证中都能有效揭示隐藏的秘密词。

Conclusion: 这些方法在提取隐藏知识方面显示出潜力，未来可在更复杂模型上进一步测试和改进。

Abstract: As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [524] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/abs/2505.14371)
*Anh Duc Nguyen,Ilia Markov,Frank Zhengqing Wu,Ali Ramezani-Kebrya,Kimon Antonakopoulos,Dan Alistarh,Volkan Cevher*

Key words: 分层量化、分布式训练、变分不等式、QODA、Wasserstein GAN

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种通用的分层量化框架，并开发了QODA算法，显著提升了Wasserstein GAN在多GPU上的训练效率。

Motivation: 现代深度神经网络的各层存在异构性问题，影响预测效果，需要一种适应这种异质性的量化方法。

Method: 提出分层量化框架和QODA算法，结合自适应学习率，用于分布式变分不等式问题。

Result: QODA在Wasserstein GAN的多GPU训练中实现了比基线快150%的端到端训练速度。

Conclusion: 分层量化框架和QODA算法有效解决了神经网络层间异构性问题，提升了分布式训练效率。

Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [525] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/abs/2505.14388)
*Prasanna Parasurama,Panos Ipeirotis*

Key words: 招聘算法, 多样性, 公平性, 性别平衡, 算法设计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，即使强制要求性别平衡的候选人短名单，也未必能提高最终雇佣的多样性。关键在于算法筛选标准与招聘经理评价标准的相关性——相关性越高，最终多样性越低。通过大规模数据分析和提出新算法，研究发现可以通过设计算法来显著提升多样性。

Motivation: 探讨算法在招聘中如何有效提升公平性和多样性，尤其是在强制性别平衡短名单的实际效果不佳的情况下。

Method: 通过理论分析和近80万份求职申请的实证数据，研究算法筛选与招聘经理评价的相关性，并提出一种新算法来多样化短名单。

Result: 强制平衡短名单对最终雇佣多样性改善有限；新算法显著提升性别多样性且不影响雇佣质量。

Conclusion: 算法设计对实现多样性目标至关重要，需注意筛选标准与评价标准的相关性。

Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [526] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/abs/2505.14407)
*Aniket Salvi,Gereon Weiss,Mario Trapp*

Key words: 机器学习, 自动驾驶, 安全监控, 模糊逻辑, 解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出一种基于模糊逻辑的监控器，用于机器学习感知组件，提供人类可理解的解释，并作为运行时安全监控器，提升自动驾驶系统的安全性和可靠性。

Motivation: 机器学习预测错误缺乏人类可理解的解释，影响了系统的安全性和可靠性保证。

Method: 引入一种新型模糊监控器，评估感知组件的可靠性，并提供操作条件的解释。

Result: 监控器在自然驾驶数据集中表现优异，提高了安全性并保持了任务可用性。

Conclusion: 该监控器在提升安全性和提供解释性方面优于现有技术。

Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [527] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/abs/2505.14411)
*Leon Götz,Marcel Kollovieh,Stephan Günnemann,Leo Schwinn*

Key words: 时间序列、分词、频繁模式、条件解码、高效预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于频繁模式的时序数据分词方法，通过自适应压缩减少计算开销，并结合条件解码进一步优化性能，显著提升了预测效果和效率。

Motivation: 现有的时序数据分词方法通常固定样本数生成令牌，导致简单模式（如常量值）产生过多令牌，计算开销大。

Method: 提出基于频繁模式的令牌化方案，合并样本与底层模式为令牌，并引入无需梯度计算的条件解码优化方法。

Result: 在时序基础模型上，基于模式的分词提升预测性能36%，效率提升1990%；条件解码进一步降低MSE达44%。

Conclusion: 新分词方法能自适应多样时序模式，泛化性好，令牌表示能捕捉统计矩和趋势等时序特性。

Abstract: Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [528] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/abs/2505.14415)
*Myung Jun Kim,Félix Lefebvre,Gaëtan Brison,Alexandre Perez-Lebel,Gaël Varoquaux*

Key words: TARTE, 表格基础模型, 预训练, 语义理解, 向量表示

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TARTE是一种基于字符串语义的表格基础模型，通过预训练生成知识增强的向量表示，显著提升下游任务性能。

Motivation: 解决表格数据语义理解的挑战，提升预训练模型在表格任务中的通用性和效率。

Method: 提出TARTE模型，通过字符串捕捉表格语义，预训练后生成知识增强的向量表示。

Result: TARTE在预测性能和计算效率上达到最优，并可灵活微调或结合其他模型。

Conclusion: TARTE为表格学习提供了一种有效的知识预训练方法。

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [529] [Explaining Neural Networks with Reasons](https://arxiv.org/abs/2505.14424)
*Levin Hornischer,Hannes Leitgeb*

Key words: 可解释性,原因向量,多语义性,神经网络,贝叶斯

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于新的数学哲学理论（原因论）的神经网络可解释性方法，通过计算神经元的‘原因向量’来解释其行为，兼具逻辑和贝叶斯视角。

Motivation: 为了解决神经网络的可解释性问题，尤其是在多语义性（单个神经元参与多个概念）的背景下，提供一种兼具理论基础和实践价值的方法。

Method: 提出计算每个神经元的‘原因向量’，并通过该向量评估神经元对不同命题的支持强度，结合逻辑和贝叶斯视角。

Result: 理论及实验证明该方法具有哲学基础、普适性、可扩展性、忠实性、正确性、可训练性和实用性。

Conclusion: 该方法为神经网络的可解释性提供了兼具理论和实践价值的解决方案。

Abstract: We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [530] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/abs/2505.14428)
*Riccardo D'Elia*

Key words: 深度学习, 系统动力学, 可解释性, 因果可靠性, 神经动力学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该提案旨在通过开发一个可解释的神经系统动力学框架，弥合深度学习（DL）与系统动力学（SD）之间的差距，结合二者的优势。

Motivation: 深度学习的预测能力强但缺乏可解释性和因果可靠性，而传统系统动力学模型虽透明且具因果性，但扩展性不足且需要大量领域知识。

Method: 提出了神经系统动力学管道，整合基于概念的可解释性、机制可解释性和因果机器学习，结合深度学习的预测能力与传统系统动力学的可解释性。

Result: 该框架实现了因果可靠性和可扩展性，并将在欧盟资助的AutoMoTIF项目中通过实际应用验证其有效性。

Conclusion: 目标是长期为自主系统提供可解释性和安全性的可行见解。

Abstract: The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [531] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/abs/2505.14451)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Key words: 缺失值填补, MNAR, 高维数据, Mamba网络, 数据精炼

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: RefiDiff是一个新颖的数据填补框架，结合了局部机器学习预测和Mamba去噪网络，适用于高维混合类型数据，特别是在MNAR机制下表现优异。

Motivation: 高维混合类型数据中的缺失值填补在MNAR机制下具有挑战性，现有方法难以整合局部和全局数据特征，限制了性能。

Method: RefiDiff通过预精炼和后精炼步骤，结合局部预测和Mamba去噪网络，统一编码混合类型数据，无需调整架构或超参数。

Result: RefiDiff在MNAR机制下表现优于现有方法，训练速度比DDPM快4倍，并在多个真实数据集上验证了其鲁棒性和可扩展性。

Conclusion: RefiDiff为高维混合类型数据填补提供了高效、稳定的解决方案，尤其在MNAR机制下表现突出。

Abstract: Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [532] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.14459)
*Kamal Singh,Sami Marouani,Ahmad Al Sheikh,Pham Tran Anh Quang,Amaury Habrard*

Key words: 强化学习, 可解释性, 网络控制, Kolmogorov-Arnold Networks, PPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种使用Kolmogorov-Arnold Networks（KAN）的可解释强化学习方法，用于网络控制问题，如负载均衡。通过PPO代理和1层Actor KAN模型，可以提取控制器方程，提高网络性能。

Motivation: 现有强化学习方法在网络控制中缺乏可解释性，难以提取控制器方程。

Method: 使用PPO代理结合1层Actor KAN模型和MLP Critic网络，学习负载均衡策略。

Result: 方法能提取控制器方程，提升网络性能并提供可解释的策略。

Conclusion: KAN结合RL有效解决了网络控制中的可解释性问题。

Abstract: Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [533] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/abs/2505.14463)
*Xinxin Fan,Wenxiong Chen,Mengfan Li,Wenqi Wei,Ling Liu*

Key words: 图对抗攻击,抵抗状态,动态系统,防御方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新方法来研究图对抗攻击的内在抵抗状态，通过动态系统建模、理论框架和函数捕捉动态变化，实验表明其优于现有防御方法。

Motivation: 研究图对抗攻击是否存在内在抵抗状态及如何找到这种状态。

Method: 将图对抗学习视为多目标动态系统，提出理论框架证明抵抗状态存在，并开发单维函数捕捉动态变化以找到临界状态。

Result: 在五个常用数据集和三种攻击下，方法显著优于现有防御技术。

Conclusion: 研究表明存在内在抵抗状态，且新方法能有效识别并防御攻击。

Abstract: Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [534] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/abs/2505.14468)
*Yifan Sui,Hao Wang,Hanfei Yu,Yitao Hu,Jianxun Li,Hao Wang*

Key words: Serverless computing, LoRA, LLM inference, GPU efficiency, cost reduction

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Serverless computing is ineffective for Low-Rank Adaptation (LoRA) LLM inference due to parameter redundancy, artifact loading latency, and resource contention. ServerlessLoRA addresses these issues with backbone sharing, pre-loading methods, and contention-aware batching, reducing costs and latency significantly.

Motivation: Current serverless computing fails to efficiently serve LoRA LLM inference due to inefficiencies like parameter redundancy and high latency, leading to increased costs and poor performance.

Method: ServerlessLoRA introduces backbone LLM sharing, pre-loading of LoRA artifacts, and contention-aware batching and offloading to optimize GPU usage and reduce latency.

Result: Experiments show ServerlessLoRA reduces Time-To-First-Token (TTFT) by up to 86% and cuts costs by up to 89% compared to existing solutions.

Conclusion: ServerlessLoRA significantly improves LoRA LLM inference by addressing key inefficiencies, making it a faster and more cost-effective solution.

Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [535] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/abs/2505.14477)
*Maria Panagiotou,Lorenzo Brigato,Vivien Streit,Amanda Hayoz,Stephan Proennecke,Stavros Athanasopoulos,Mikkel T. Olsen,Elizabeth J. den Brok,Cecilie H. Svensson,Konstantinos Makrilakis,Maria Xatzipsalti,Andriani Vazeou,Peter R. Mertens,Ulrik Pedersen-Bjergaard,Bastiaan E. de Galan,Stavroula Mougiakakou*

Key words: 胰岛素调整、强化学习、T1D、T2D、血糖控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的个性化胰岛素治疗建议方法ABBA，相比标准方法显著改善了血糖控制。

Motivation: 尽管胰岛素制剂和技术有所进步，调整胰岛素仍是T1D和T2D患者的挑战。

Method: 开发ABBA方法，通过强化学习为患者提供个性化胰岛素建议，并进行模拟测试。

Result: ABBA显著提高了TIR（血糖达标时间），减少了高低血糖时间，优于标准方法BBA。

Conclusion: ABBA有望优化血糖控制，值得在人类中进行首次试验。

Abstract: Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [536] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/abs/2505.14502)
*Wenze Liu,Xiangyu Yue*

Key words: 扩散模型, 正割损失, ODE积分, 推理加速

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种通过“正割损失”学习ODE积分的方法，以平衡扩散模型推理的性能与成本，显著提升了小步长下的生成质量。

Motivation: 针对扩散模型推理中数值求解器在极小步长下性能不佳，而蒸馏技术又带来复杂性和不稳定的问题，提出了一种中间策略。

Method: 通过从导数-积分关系衍生的损失函数（灵感来自蒙特卡罗积分和Picard迭代）学习ODE积分，并将切线逐渐扩展为正割。

Result: 实验表明，EDM的“正割版本”在CIFAR-10上10步FID为2.14，SiT-XL/2的“正割版本”在ImageNet-256×256上4步FID为2.27，8步FID为1.96。

Conclusion: “正割损失”能高效地将预训练扩散模型转换为“正割版本”，显著提升了推理速度与生成质量。

Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [537] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/abs/2505.14512)
*Juliusz Ziomek,George Whittle,Michael A. Osborne*

Key words: 神经网络, 神经切线核, 层归一化, 泛化, 稳定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了神经网络在训练分布外区域的行为，首次通过神经切线核（NTK）理论证明了无限宽网络中仅需一个层归一化（LN）即可显著改变NTK，使其变为有界方差核，从而在远离训练数据的输入上保持输出稳定。

Motivation: 探讨神经网络在训练分布外区域的行为及其稳定性，尤其是层归一化在抑制输出病理增长中的作用。

Method: 应用NTK理论分析无限宽网络的行为，并通过实验验证有限宽度网络的稳定性表现。

Result: 包含至少一个LN的网络在远离训练数据的输入上表现出有界输出，而未含LN的网络则可能产生无界输出。

Conclusion: 层归一化显著提升了神经网络的泛化稳定性，尤其在训练数据之外的区域。

Abstract: In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [538] [Latent Flow Transformer](https://arxiv.org/abs/2505.14513)
*Yen-Chen Wu,Feng-Ting Liao,Meng-Hsi Chen,Pei-Chen Ho,Farhang Nabiei,Da-shan Shiu*

Key words: Transformer, flow matching, Latent Flow Transformer, Flow Walking, compression

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Latent Flow Transformer (LFT)的方法，通过流匹配训练将多个离散层替换为单一的学习传输操作器，显著压缩模型层数并保持性能。

Motivation: 传统的Transformer模型由大量离散层组成，效率低下。基于扩散和流模型的连续层在图像生成中的优越性激发了将类似方法用于语言模型的动机。

Method: LFT将多个层替换为一个通过流匹配训练的学习传输操作器，并引入Flow Walking (FW)算法解决现有流方法在耦合保留上的局限性。

Result: 在Pythia-410M模型上，LFT压缩了层数并优于直接跳过层的方法（KL散度更低），尤其是通过FW算法压缩12层为1层时，进一步缩小了自回归与流生成之间的差距。

Conclusion: LFT展示了将连续层方法用于语言模型的可行性，显著提升了效率并保持了性能，为未来研究提供了新方向。

Abstract: Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [539] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/abs/2505.14522)
*Mahmuda Akhter Nishu,Chenyu Huang,Milad Roohi,Xin Zhong*

Key words: 风灾预测,双流学习框架,社区韧性,风险评估,随机森林,RoBERTa

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种双流学习框架，结合气象数据和文本叙事，改善了针对弱势社区的风灾预测和风险评估。

Motivation: 现有风灾预测系统主要关注气象因素，忽略了社区特定的脆弱性，尤其在基础设施和数据覆盖有限的地区，难以提供有效的本地化风险评估。

Method: 采用双流学习框架，整合随机森林和RoBERTa模型，通过后期融合机制，结合结构化气象数据和非结构化文本叙事。

Result: 实验结果显示该方法在性能上显著优于传统基线模型，并通过敏感性分析和消融研究提高了模型透明度和可信度。

Conclusion: 该框架不仅在预测风灾上效果显著，还具有实际应用价值，能够支持应急准备和提升社区韧性。

Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [540] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/abs/2505.14531)
*Shaoye Luo,Xinxin Fan,Quanliang Jing,Chi Lin,Mengfan Li,Yunfeng Lu,Yongjun Xu*

Key words: 后门攻击, Ising模型, 触发器净化, Hopfield网络, 黑盒防御

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于Ising模型的通用且模型无关的触发器净化方法SifterNet，用于抵御卷积神经网络和视觉Transformer大型模型中的后门攻击。该方法无需提前了解目标模型细节、大量干净样本或模型重训练授权，具有轻量级和黑盒防御特点。

Motivation: 现有的触发器检测/移除研究通常需要提前了解目标模型的详细信息、大量干净样本甚至模型重训练授权，这在实践中尤其在不访问目标模型时带来巨大不便。理想的应对措施应能消除植入的触发器，而不论目标模型为何。

Method: 通过利用Hopfield网络的记忆关联功能，提出了一种轻量级黑盒防御方法SifterNet，其核心创新在于引入Ising模型的思想，以有效净化输入样本中的触发器。

Result: 大量实验验证了SifterNet在触发器净化和高精度实现方面的有效性，且在多个常用数据集上显著优于现有基线方法。

Conclusion: SifterNet在无需目标模型详细信息的情况下，能够有效净化触发器，同时保持高模型精度，具有广泛的实际应用价值。

Abstract: Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [541] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/abs/2505.14533)
*Mohammad Irfan Uddin,Nishad Tasnim,Md Omor Faruk,Zejian Zhou*

Key words: 脉冲神经网络, 强化学习, Transformer, 能量效率, 策略性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合脉冲神经网络(SNN)和强化学习的新型Spike-Transformer算法，显著提升了能量效率和策略性能。

Motivation: 为了解决传统Transformer模型在强化学习中高能耗的问题，同时利用SNN的生物启发结构和高效能特性。

Method: 设计了一种基于多步Leaky Integrate-and-Fire (LIF)神经元和注意机制的SNN，结合状态、动作和奖励编码，构建类似Transformer的结构。

Result: 实验表明，该算法在能量效率和策略性能上优于传统Transformer模型。

Conclusion: 通过结合SNN和强化学习，为复杂决策场景提供了一种高效且低成本的解决方案。

Abstract: Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [542] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/abs/2505.14535)
*Jiangrong Shen,Yulin Xie,Qi Xu,Gang Pan,Huajin Tang,Badong Chen*

Key words: 多模态脉冲神经网络,时间注意力,自适应融合,能效学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种动态跨模态融合框架（TAAF）来解决多模态脉冲神经网络（SNNs）中的模态不平衡和时间错位问题，通过时间注意力机制和自适应融合损失实现高效的多感官学习。

Motivation: 多模态SNNs在能效感知处理中潜力巨大，但面临模态不平衡和时间错位的挑战，现有方法无法协调模态收敛速度或动态适应跨模态交互。

Method: 1) 时间注意力引导的自适应融合（TAAF）模块动态分配重要性分数；2) 基于注意力分数的自适应平衡融合损失调节模态学习率。

Result: 在CREMA-D、AVE和EAD数据集上分别达到77.55%、70.65%和97.5%的准确率，并展现了高能效特性。

Conclusion: 该框架通过可学习时间扭曲操作和快速模态收敛协调解决了时间错位问题，为神经形态系统中的多模态学习提供了新范式。

Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [543] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/abs/2505.14543)
*Utsav Dutta,Sina Khoshfetrat Pakazad,Henrik Ohlsson*

Key words: 时间序列, 基础模型, 嵌入表示, JEPA, 多元时间序列

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CHARM是一个用于多元时间序列的基础嵌入模型，通过学习共享、可迁移和领域感知的表示，解决了传统时间序列模型的限制。

Motivation: 传统时间序列模型是任务特定的，依赖于数据集特定的训练和复杂的特征工程，而基础模型在时间序列领域尚未充分探索。

Method: CHARM结合了架构创新，整合了通道级文本描述并保持通道顺序不变性，使用JEPA训练，并采用新颖的数据增强方案和损失函数。

Result: 7M参数的CHARM模型在多种下游任务中达到最先进性能，为时间序列表示学习设定了新标准。

Conclusion: CHARM为时间序列领域的基础模型研究提供了新方向，展示了其潜在的应用价值。

Abstract: Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [544] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/abs/2505.14555)
*Yingtao Luo,Shikai Fang,Binqing Wu,Qingsong Wen,Liang Sun*

Key words: 天气预报,深度学习,物理方程,PhyDL-NWP,降尺度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了PhyDL-NWP框架，结合物理方程与深度学习，优化天气预报效率与准确性。

Motivation: 传统气象预测方法计算量大且物理不完整，深度学习模型虽高效但忽视物理法则，限制其可解释性和泛化能力。

Method: 提出PhyDL-NWP框架，将物理方程与潜力量参数化融入数据驱动模型，通过自动微分计算物理项，并使用物理信息损失对齐预测与动态规律。

Result: PhyDL-NWP实现分辨率无关的降尺度，推理速度提升170倍，参数仅55K，提升了预测性能和物理一致性。

Conclusion: PhyDL-NWP成功结合物理与深度学习，显著提升了天气预报的效率和准确性。

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [545] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/abs/2505.14564)
*David Krame Kadurha,Domini Jocema Leko Moutouo,Yae Ulrich Gaba*

Key words: 强化学习, Banach不动点定理, Bellman算子, 收敛性, 算法效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文通过分析状态、动作和策略空间的结构，回顾了强化学习的拓扑基础，并探讨了如何利用数学工具如Banach不动点定理提高算法效率。

Motivation: 研究旨在通过理论数学工具（如Banach空间和不动点定理）深化对强化学习问题的理解，从而设计更高效的算法。

Method: 回顾了Banach收缩原理和Bellman算子，探讨了它们在强化学习算法收敛性中的应用，并通过实验验证了改进的Bellman算子对性能的提升。

Result: 研究发现，通过对Bellman算子的替代公式化，可在标准强化学习环境中（如MountainCar、CartPole和Acrobot）显著提高收敛速度和性能。

Conclusion: 研究表明，对强化学习的更深层数学理解能够为决策问题提供更有效的算法设计思路。

Abstract: This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [546] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/abs/2505.14566)
*Andrei Cozma,Landon Harris,Hairong Qi*

Key words: Reinforcement Learning, PPO, Koopman Operator, KIPPO, 连续控制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出KIPPO方法，结合Koopman算子理论和PPO，提升复杂环境中的策略学习效果。

Motivation: 针对复杂非线性动态环境中策略梯度方法的高方差和非凸优化问题，提出改进方案。

Method: 通过Koopman近似辅助网络学习系统动态的近似线性潜在空间表示，保留关键特征。

Result: 在连续控制任务中性能提升6-60%，稳定性提升91%。

Conclusion: KIPPO显著提升了PPO的性能和稳定性。

Abstract: Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [547] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/abs/2505.14592)
*Alexandre Broggi,Nathaniel Bastian,Lance Fiondella,Gokhan Kul*

Key words: 人工神经网络,剪枝,泛化能力,网络安全

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了多种人工神经网络剪枝方法在网络安全数据集上的泛化能力，发现只有少数方法表现良好。

Motivation: 探讨剪枝方法在简化网络结构和新数据集上的适用性，以优化模型大小和推理速度。

Method: 分析了多种剪枝方法在不同剪枝程度下的表现，以评估其泛化能力。

Result: 发现大多数剪枝方法泛化能力不足，仅有少数算法表现达标。

Conclusion: 只有特定剪枝方法适用于该网络安全任务，需谨慎选择。

Abstract: Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [548] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz,Hesam Salehipour,Adrian Butscher,Nigel Morris*

Key words: 降阶建模、可微求解器、物理学信息、泛化能力、长期预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的降阶模型（$Φ$-ROM），通过将可微PDE求解器融入训练过程，显著提升了模型的泛化能力和长期预测能力。

Motivation: 传统降阶模型（ROM）在训练过程中未利用高保真数值求解器，导致潜在动力学与离散物理不匹配，限制了泛化和预测能力。

Method: 结合可微PDE求解器训练模型，确保潜在空间动力学与物理方程一致。

Result: $Φ$-ROM在泛化性、长期预测、数据效率和稀疏观测处理上优于现有方法。

Conclusion: $Φ$-ROM为复杂系统的降阶建模提供了一种灵活且高效的框架。

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [549] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/abs/2505.14596)
*Isabella Degen,Zahraa S Abdallah,Henry W J Reeve,Kate Robson Brown*

Key words: 时间序列聚类,相关结构,合成基准,评估标准,算法诊断

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 介绍了CSTS，一个用于评估多元时间序列数据中相关结构发现的合成基准，帮助区分聚类失败的原因。

Motivation: 解决时间序列聚类缺乏客观评估标准的问题，区分数据结构、算法或验证方法的局限性。

Method: 开发CSTS基准，包含不同相关结构、数据条件和评估协议，验证相关结构保存性。

Result: CSTS能精确诊断聚类算法的局限性，如对非正态分布的敏感性。

Conclusion: CSTS为基于相关性的时间序列聚类提供了严谨的评估标准。

Abstract: Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [550] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/abs/2505.14606)
*Maksim Zhdanov,Vladislav Kurenkov*

Key words: 神经原子间势能、泊松方程、自监督学习、静电相互作用、物理约束

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为$Φ$-Module的通用插件模块，通过自监督学习在消息传递框架中强制满足泊松方程，以学习静电相互作用，显著提升神经原子间势能模型的性能。

Motivation: 现有的深度学习原子间势能模型缺乏基于物理定律的辅助约束，可能导致训练速度慢和预测准确性不足。

Method: 引入$Φ$-Module模块，通过自监督方式学习静电相互作用，利用泊松方程约束原子级表示，并推导出静电能量项。

Result: 在OE62和MD22基准测试中，结合$Φ$-Module的模型表现优于基线，误差降低4.5%至17.8%。

Conclusion: 通过嵌入第一性原理约束，神经原子间势能模型的性能得到显著提升，同时保持超参数友好性和计算效率。

Abstract: Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [551] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/abs/2505.14610)
*Hao Wang,Chenyu Shi,Angel E. Rodriguez-Fernandez,Oliver Schütze*

Key words: 最大均值差异（MMD）；多目标优化（MOPs）；牛顿方法；进化算法（MOEA）

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出使用最大均值差异（MMD）解决连续多目标优化问题（MOPs），并开发了一种基于MMD的牛顿方法（MMDN），结合进化算法（MOEAs）显著提高了优化精度。

Motivation: 多目标优化问题中，通常需要衡量目标集与参考集的距离。MMD作为一种分布距离测量工具，被引入以改善优化效果。

Method: 通过分析MMD的梯度和Hessian矩阵，设计MMDN方法，并结合MOEAs进行混合优化。

Result: 在11个基准问题上的实验表明，混合方法（MMDN + MOEA）比单独使用MOEA具有更高的优化精度。

Conclusion: MMD与MOEAs的结合提供了一种高效的多目标优化解决方案。

Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [552] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/abs/2505.14613)
*Emmanuel Noutahi,Jason Hartford,Prudencio Tossou,Shawn Whitfield,Alisandra K. Denton,Cas Wognum,Kristina Ulicna,Jonathan Hsu,Michael Cuccarese,Emmanuel Bengio,Dominique Beaini,Christopher Gibson,Daniel Cohen,Berton Earnshaw*

Key words: 药物发现、虚拟细胞、人工智能、生物分子相互作用、实验室循环

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了开发虚拟细胞模型以加速药物发现的愿景，强调了预测细胞对扰动的功能响应能力，并提出了设计原则、实验室循环方法和生物学基准的重要性。

Motivation: 通过虚拟细胞模型预测细胞对治疗的反应，可以安全、经济地测试大量治疗假设，从而加速药物发现并降低成本。

Method: 提出设计虚拟细胞的关键原则，包括准确预测细胞功能响应和解释生物分子相互作用变化；采用实验室循环方法生成新见解；并建议基于生物学的基准指导开发。

Result: 虚拟细胞模型不仅有助于药物发现，还为构建更高层次（如虚拟患者）的模型提供了框架。

Conclusion: 虚拟细胞模型在药物发现中具有巨大潜力，需要跨学科合作和技术进步以实现其目标。

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [553] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler,Linzi Xing,Ge Shi,Hanieh Sadri,Gursimran Singh,Weiwei Zhang,Tao Ye,Ying Xiong,Yong Zhang,Zhenan Fan*

Key words: LoRA, 对比性解码, LLM, 华为Ascend NPU, 任务准确性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为对比性LoRA解码（CoLD）的新解码框架，旨在最大化利用LoRA适配模型的任务特定知识，显著提升任务准确性和降低延迟。

Motivation: 传统解码方法（如贪心搜索或束搜索）在处理复杂推理或深层次上下文理解任务时，会受到基础模型的偏见或干扰，导致任务无关的通用响应。为了克服这一问题，需要一种更有效的解码策略。

Method: CoLD采用对比性解码方法，通过比较LoRA适配的专家模型与基础模型的概率分布差异来评分候选token，优先选择与LoRA学习表示更匹配的token。此外，针对计算成本高的问题，优化了华为Ascend NPU的内核。

Result: CoLD在任务准确性上提升了5.54%，同时端到端延迟降低了28%，相比贪心解码。

Conclusion: CoLD为资源受限环境中的LLM微调提供了高效解码策略，对云端和本地应用具有广泛意义。

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [554] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Key words: 强化学习, 语言模型, 假阴性, 验证器, 微调

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文研究了强化学习中验证器错误拒绝正确输出的问题，提出了轻量级验证器tinyV来改进奖励估计，提升了模型性能。

Motivation: 研究揭示验证器存在显著错误拒绝（假阴性）问题，影响了强化学习的训练效果，旨在提出解决方案以优化LLM的微调。

Method: 提出轻量级LLM验证器tinyV，结合规则方法动态识别假阴性并恢复正确响应，从而更准确地估计奖励。

Result: 实验表明，tinyV在多项数学推理基准测试中提升通过率最高达10%，并加速收敛。

Conclusion: 验证器假阴性问题对RL训练至关重要，tinyV提供了一种实用方法显著改进LLM微调效果。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [555] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat,Mohammed J Zaki*

Key words: 大型语言模型、知识图谱、食物推荐、食谱生成、营养分析

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: KERL是一个结合食物知识图谱与大型语言模型的统一系统，能根据用户问题生成个性化食谱及营养信息，优于现有方法。

Motivation: 利用知识图谱与大型语言模型提升食物推荐与食谱生成能力，填补相关研究空白。

Method: 提取实体、检索子图，利用知识图谱上下文生成满足条件的食谱，并提供烹饪步骤和营养信息。

Result: 实验表明KERL显著优于现有方法，提供完整的食物推荐与营养分析解决方案。

Conclusion: KERL为食物推荐与食谱生成提供了高效、连贯的解决方案，数据集与代码已开源。

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [556] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/abs/2505.14635)
*Benjamin Prada,Shion Matsumoto,Abdul Malik Zekri,Ankur Mali*

Key words: 预测编码（PC）、最小描述长度（MDL）、深度学习、泛化边界、收敛性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文首次将预测编码（PC）与最小描述长度（MDL）原理在深度网络中联系起来，证明了PC如何优化MDL目标，并提供了理论保证。

Motivation: 旨在为生物启发的局部学习规则预测编码（PC）提供一个理论框架，证明其在深度网络中的有效性，并与MDL原理结合，提供理论支持。

Method: 通过理论证明，将PC与MDL目标联系起来，使用Hoeffding不等式和前缀码先验，推导泛化边界，并证明PC更新的收敛性。

Result: PC能够单调降低经验两部分码长，提供比无约束梯度下降更紧的高概率风险边界，并收敛到近似MDL最优解。

Conclusion: PC作为生物启发的学习规则，具有理论保证和生物合理性，是反向传播的可行替代方案。

Abstract: We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \^{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [557] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/abs/2505.14643)
*Ane G. Domingo-Aldama,Marcos Merino Prado,Alain García Olea,Koldo Gojenola Galletebeitia,Josu Goikoetxea Salutregi,Aitziber Atutxa Salazar*

Key words: 房颤复发, 机器学习, 传统评分, 结构化数据, NLP

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究通过整合结构化数据和自由文本，验证了LTM模型在预测房颤复发方面的优越性，优于传统评分和机器学习模型。

Motivation: 传统评分系统预测房颤复发的准确性有限，且电子健康记录数据存在误差和缺失问题。研究旨在通过机器学习提升预测性能。

Method: 结合结构化临床数据和自由文本报告，生成高质量表格数据集，并评估LTM模型与传统评分和机器学习模型的性能。

Result: LTM模型预测性能最高，性别和年龄偏见分析揭示了人口统计学差异。

Conclusion: 整合多源数据可提升数据集质量，LTM模型在房颤复发预测中展现出潜力。

Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [558] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/abs/2505.14659)
*Navneet Kaur,Lav Gupta*

Key words: 6G医疗,可解释AI,网络安全,SHAP,LIME,DiCE

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文探讨了可解释AI技术（如SHAP、LIME和DiCE）如何在6G医疗系统中发现漏洞、增强防御并提高透明度和信任。

Motivation: 随着医疗系统逐渐采用先进的无线网络和连接设备，医疗应用的安全性变得至关重要，特别是在6G和AI技术的背景下。

Method: 使用可解释AI技术（SHAP、LIME、DiCE）来分析6G医疗系统中的漏洞并加强防御。

Result: 实验分析支持了该方法的有效性，并展示了积极的结果。

Conclusion: 可解释AI技术可以提升6G医疗系统的安全性和透明度。

Abstract: As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [559] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669)
*Roberto L. Castro,Andrei Panferov,Soroush Tabesh,Oliver Sieberling,Jiale Chen,Mahdi Nikdan,Saleh Ashkboos,Dan Alistarh*

Key words: 大型语言模型、FP4训练、低精度计算、Quartet、能效、NVIDIA Blackwell

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为Quartet的新方法，实现了基于FP4精度的端到端训练，解决了当前低精度训练中的准确度下降问题，并在Llama型模型上验证了其高效性与准确性。

Motivation: 随着大型语言模型（LLMs）的快速发展，计算需求急剧增加，低精度算术训练是一种提升计算吞吐量和能效的潜在解决方案。

Method: 论文系统研究了硬件支持的FP4训练，提出Quartet方法，通过优化的CUDA内核在NVIDIA Blackwell GPU上实现低精度计算。

Result: 实验表明，Quartet在FP4精度下实现了接近最优的训练效果，成功训练了十亿级规模的模型，优于标准精度和FP8训练。

Conclusion: Quartet证明了完全基于FP4的训练是一种高效且准确的替代方案，有望推动低精度训练的广泛应用。

Abstract: The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


### [560] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/abs/2505.13989)
*Yanzhe Wen,Xunkai Li,Qi Zhang,Zhu Lei,Guang Zeng,Rong-Hua Li,Guoren Wang*

Key words: 大语言模型、文本属性图、开放世界学习、未知类别拒绝、标注管道

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了基于大语言模型的Open-world Graph Assistant（OGA）框架，结合语义和拓扑结构解决开放世界中数据不确定性问题，并引入标注管道提升模型性能。

Motivation: 现有方法在开放世界场景中未能有效处理数据不确定性，尤其是在有限标注和未知类别节点方面。

Method: OGA框架结合自适应标签追溯（整合语义与拓扑结构）和图形标签标注器，实现未知类别拒绝和模型更新。

Result: 实验证明OGA在效果和实用性上表现优异。

Conclusion: OGA为开放世界中的图学习提供了一种有效解决方案。

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [561] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/abs/2505.13466)
*Vu Dinh Xuan,Hao Vo,David Murphy,Hoang D. Nguyen*

Key words: 合成数据、安全关键应用、多智能体框架、语义一致性、LLM

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体的框架，用于生成合成数据以解决安全关键应用中的数据稀缺问题。该框架通过两个智能体的协作迭代生成语义一致的合成场景。

Motivation: 由于在安全关键应用（如建筑安全）中获取真实数据存在伦理和实际障碍，亟需一种能够生成高质量合成数据的端到端框架。现有方法合成场景时语义深度不足，效果有限。

Method: 提出了一种双智能体协作框架，包括基于LLM的评估者智能体（确保语义一致性和安全约束）和编辑者智能体（生成和优化场景）。

Result: 实验表明，该框架能根据现实需求生成有用的合成场景，弥补了现有方法的不足，平衡了安全需求和视觉语义。

Conclusion: 迭代协作设计有望为解决多媒体安全应用中的数据稀缺问题提供健壮且美观的模拟解决方案。

Abstract: The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [562] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch,Sebastian Eilermann,Alexander Windmann,Alexander Diedrich,Philipp Rosenthal,Oliver Niggemann*

Key words: 大型语言模型, 工程评估, 真实场景, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文指出了当前大型语言模型（LLMs）在工程任务评估中的不足，提出了一个包含100多个真实工程问题的数据集，并评估了四种先进LLMs的表现。结果显示LLMs在基础和结构性推理上表现良好，但在抽象推理和工程逻辑上存在困难。

Motivation: 当前LLMs在工程任务评估中的方法过于简化，未能充分体现真实工程的复杂性。

Method: 研究者构建了一个包含真实工程问题的数据集，并系统评估了四种LLMs的表现。

Result: LLMs在基础和结构性推理上表现良好，但在抽象推理、形式化建模和工程逻辑上表现不佳。

Conclusion: LLMs在工程任务中的潜力有限，需要进一步改进以应对复杂工程问题。

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [563] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han,Wang Lin,Liya Hu,Zhenlong Dai,Yiyun Zhou,Mengze Li,Zemin Liu,Chang Yao,Jingyuan Chen*

Key words: 知识跟踪,跨课程学习,大语言模型,概念图,对比学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TransKT是一种对比跨课程知识跟踪方法，通过概念图引导知识转移，提升对学习者知识状态的全面理解。

Motivation: 现有知识跟踪模型多局限于单一课程数据，难以全面捕捉学习者的知识状态。

Method: 利用零样本大语言模型构建跨课程概念图，结合图卷积网络和对比目标优化知识状态表示。

Result: TransKT显著提升了知识转移的性能，提供了更准确的学习者知识状态表示。

Conclusion: 跨课程知识转移能够有效提升知识跟踪模型的性能，尤其是通过概念图和大语言模型的支持。

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [564] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/abs/2505.13496)
*Przemek Pospieszny,Wojciech Mormul,Karolina Szyndler,Sanjeev Kumar*

Key words: 异常检测, 无监督学习, 日志分析, Transformer, 自适应阈值

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ADALog是一种自适应、无监督的异常检测框架，适用于异构日志数据，无需依赖日志解析或标记数据。

Motivation: 现代软件系统生成的日志数据具有动态格式、碎片化事件序列和变化的时序模式，使得异常检测变得重要且具有挑战性。

Method: ADALog使用基于Transformer的预训练双向编码器，通过掩码语言建模任务对正常日志进行微调，提取日志内上下文关系，并通过自适应百分位阈值识别异常。

Result: 在BGL、Thunderbird和Spirit基准数据集上，ADALog表现出强大的泛化能力和竞争性性能。

Conclusion: ADALog能够动态适应系统行为变化，无需依赖固定阈值，优于传统方法。

Abstract: Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [565] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever,Forrest McKee*

Key words: 大型语言模型, 自由职业, 基准测试, 自动化评估, AI开发

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探索大型语言模型（LLMs）作为自主代理完成现实任务的能力，提出一个基于经济数据的自由职业编程和数据分析任务新基准。

Motivation: 评估LLMs在自由职业任务中的表现，为AI作为自由职业者的可行性提供依据。

Method: 从Kaggle数据集构建合成任务，标准化价格，提供结构化输入输出测试用例和价格标签，自动化评估四个现代LLM。

Result: Claude 3.5 Haiku表现最佳，赚取约152万美元，其次是GPT-4o-mini（149万美元）、Qwen 2.5（133万美元）和Mistral（70万美元）。

Conclusion: 最强模型能解决大部分任务，但结构化任务与真实自由职业复杂性仍有差距。

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [566] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/abs/2505.13522)
*Nathalie Sanghikian,Rafael Meirelles,Rafael Martinelli,Anand Subramanian*

Key words: Maritime Inventory Routing Problem, MIRP, heuristic, Beam Search, Iterated Local Search

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种不依赖数学优化技术的启发式方法，用于解决确定性、有限期、单产品的MIRRP问题，结合了改进的Beam Search算法和迭代局部搜索程序，改进了部分已知最优解。

Motivation: 由于MIRP问题的高复杂度，现有的精确方法（如MIP）在实际操作中效率低下，而非MIP的启发式方法又因其高度约束性难以应用，因此需要新的方法解决问题。

Method: 提出了一种启发式方法，结合改进的Beam Search算法和迭代局部搜索程序。

Result: 在72个测试实例中，该方法改进了10个已知最优解，并在可接受的CPU时间内实现。

Conclusion: 该方法为MIRP问题提供了一种高效的解决方案，推动了MIRPLib的使用和结果比较。

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [567] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang,Jinzhe Tu,Haoran Liu,Xiaoce Wang,Chujie Zheng,Zhexin Zhang,Shiyao Cui,Caishun Chen,Tiantian He,Hongning Wang,Yew-Soon Ong,Minlie Huang*

Key words: Large Reasoning Models, BARREL, factual reliability, overconfident answers

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: BARREL框架通过改进大模型（LRMs）的推理模式，提升了其回答的可靠性和事实性。

Motivation: 当前大型推理模型（LRMs）在数学和逻辑推理中表现出过度自信和错误回答的问题，缺乏承认无知的能力。

Method: 提出BARREL框架，旨在实现简洁且有边界意识的事实性推理，解决过思考和猜测问题。

Result: 实验显示，BARREL训练显著提升了DeepSeek-R1-Distill-Llama-8B模型的可靠性，从39.33%提高到61.48%。

Conclusion: BARREL框架为构建更可靠的System 2 LRMs提供了启示。

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [568] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/abs/2505.13533)
*Junzhe Jiang,Chang Yang,Aixin Cui,Sihan Jin,Ruiyu Wang,Bo Li,Xiao Huang,Dongning Sun,Xinrun Wang*

Key words: 金融基准测试,大语言模型,金融推理,FinMaster

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文介绍了FinMaster，一个全面的金融基准测试，用于系统评估大语言模型在金融领域的应用能力，包括金融知识、会计、审计和咨询。

Motivation: 金融任务对全球经济稳定至关重要，但存在劳动密集、容错率低、数据分散和工具限制等挑战。现有评估大语言模型的金融基准缺乏领域特定数据、任务设计简单且评估框架不完善。

Method: FinMaster包含三个模块：FinSim（生成合成隐私合规的金融数据）、FinSuite（提供183个核心金融任务）和FinEval（统一评估接口）。

Result: 实验显示，大语言模型在复杂金融推理任务上表现较差，准确率从基础任务的90%降至多步推理任务的40%，计算错误传播现象明显。

Conclusion: FinMaster是首个覆盖全流程金融任务的基准测试，有望推动大语言模型在真实金融实践中的应用。

Abstract: Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [569] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/abs/2505.13546)
*Ke Chen,Yufei Zhou,Xitong Zhang,Haohan Wang*

Key words: 自动提示生成、语义稳定性、LLM、多智能体系统、可靠性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于稳定性的自动提示生成方法，通过量化语义稳定性并优化提示，提高多智能体系统的可靠性和性能。

Motivation: 现有提示生成方法仅关注任务性能，忽视了提示的内在稳定性，导致系统不可靠。论文旨在通过稳定性反馈提升提示质量。

Method: 提出语义稳定性作为评估标准，微调LLaMA评估器，开发了首个稳定性感知的提示生成系统，迭代优化提示质量。

Result: 实验表明，该框架提高了任务准确性和输出一致性，证明了稳定性对系统性能的必要性。

Conclusion: 通过关注持久可靠性而非一次性结果，本研究为构建更可信的通用系统提供了新视角和实用工具。

Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [570] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/abs/2505.13551)
*Serge Dolgikh*

Key words: 反推断行为,认知刚性,适应不良,认知脆弱性,认知架构

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究探讨了自然和人工认知系统中反推断行为的出现，即代理人错误归因经验成功或抑制适应，导致认知刚性或适应不良的稳定性。通过分析典型场景，揭示了此类行为的来源及其普遍性。

Motivation: 探索认知系统中反推断行为的形成机制及其影响，以理解认知刚性和适应不良稳定性的根源。

Method: 分析强化稳定性的奖励失衡、元认知将成功归因于内部优势的案例，以及在感知模型脆弱性下的保护性重构。结合人工智能、生物认知、心理学和社会动态的证据进行研究。

Result: 反推断行为是一种普遍的认知脆弱性，可能出现在适应性良好的系统中。研究提出在稳定条件下保持最小适应性激活的重要性。

Conclusion: 研究强调了设计能够抵抗信息压力下刚性的认知架构的原则，并提供了避免反推断行为的建议。

Abstract: This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [571] [Language and Thought: The View from LLMs](https://arxiv.org/abs/2505.13561)
*Daniel Rothschild*

Key words: 语言与思维、大型语言模型、AI推理、Daniel Dennett

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了语言对思维的影响，通过AI模型（尤其是大型语言模型）的表现支持了Daniel Dennett的观点，即语言显著改变了思维的运作方式。

Motivation: 受Daniel Dennett的启发，研究语言对思维的潜在改变作用，并通过AI系统验证这一假设。

Method: 通过比较有语言训练和无语言训练的AI系统（尤其是大型语言模型）的表现，分析其推理能力。

Result: 大型语言模型在跨领域推理中的成功支持了Dennett的观点，表明语言的抽象性和高效编码是推理能力的关键。

Conclusion: 语言使推理在计算上变得可行，这一发现对理解人类生物大脑中语言的作用具有启示意义。

Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [572] [MAFA: A multi-agent framework for annotation](https://arxiv.org/abs/2505.13668)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Key words: FAQ检索、多智能体框架、Attentive Reasoning Queries、少样本学习、检索准确性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种多智能体框架用于FAQ标注，通过结合多种专业智能体和法官智能体重新排序候选结果，显著提升了检索准确性和效率。

Motivation: 传统FAQ检索方法常依赖单一模型，难以捕捉多样用户查询的细微差异，因此提出多智能体框架以解决这一问题。

Method: 采用基于Attentive Reasoning Queries（ARQs）的结构化推理方法，结合多个专业智能体和法官智能体进行重排序；使用差异化少样本示例策略增强多样性。

Result: 在真实银行数据集及公开基准测试（LCQMC和FiQA）上，Top-1准确率提升14%，Top-5准确率提升18%，平均倒数排名（MRR）提升12%。

Conclusion: 框架在模糊查询处理上表现优异，适用于生产环境，并展现出强泛化能力。

Abstract: Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [573] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/abs/2505.13672)
*Giannis Chatziveroglou*

Key words: 语言模型,推理时间优化,A*搜索,计算资源利用,小模型性能提升

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出A*-decoding方法，通过结构化搜索优化推理时计算资源使用，显著提升小模型在复杂推理任务中的性能。

Motivation: 现有推理时间扩展方法在固定计算预算下表现良好，但缺乏对预算最优利用的研究。

Method: 提出A*-decoding，基于A*搜索算法，在推理时优先选择高质量推理路径。

Result: 在相同计算预算下，A*-decoding比现有方法节省3倍token和30%PRM，使小模型性能接近大模型。

Conclusion: 结构化搜索解码提供高效的替代方案，展示了推理时间策略对小模型推理能力的重要性。

Abstract: Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [574] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696)
*Zizhan He,Maxime Daigle,Pouya Bashivan*

Key words: ESWM, 片段记忆, 空间模型, 样本效率, 自适应, 导航

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种新的框架ESWM，能够通过稀疏和离散的片段记忆构建环境的空间模型，展示了高效样本利用和自适应能力，支持探索和导航任务。

Motivation: 研究动物如何通过稀疏经验构建灵活的世界模型，并探索神经网络是否能够模拟类似能力。

Method: 在模拟环境中提出Episodic Spatial World Model (ESWM)框架，通过稀疏的片段记忆学习空间模型。

Result: ESWM表现出极高的样本效率，能够快速适应环境变化，并支持探索和导航任务，无需额外训练。

Conclusion: ESWM为通过稀疏记忆构建环境模型提供了有效解决方案，展示了类似动物大脑的效率和适应性。

Abstract: Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [575] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha,Minwu Kim,Aadim Nepal,Anubhav Shrestha,Keith Ross*

Key words: LLM, 推理能力, 预热训练, 数据稀缺, RLVR, 长链思维

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出一种两阶段训练策略，通过在玩具领域蒸馏长链思维预热模型，再用少量目标域数据微调，解决数据稀缺下推理LLMs训练问题。

Motivation: 传统RLVR或CoT蒸馏方法依赖大量数据，本文旨在数据稀缺环境下高效训练推理能力的LLMs。

Method: 两阶段训练：1) 在K&K逻辑题领域蒸馏长链思维预热模型；2) 用少量目标域数据RLVR微调预热模型。

Result: 预热模型在多项任务表现更优，且RLVR训练后保持跨域泛化性，同时提升样本效率。

Conclusion: 预热方法在数据稀缺环境下显著提升推理LLMs的训练效果和样本效率。

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [576] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
*Andrew Nam,Henry Conklin,Yukang Yang,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Key words: 因果头门控（CHG）, Transformer模型, 注意力头, 机制解释性, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种称为因果头门控（CHG）的可扩展方法，用于解释Transformer模型中注意力头的功能角色。CHG通过学习头的软门并将其分类为促进、干扰或无关，直接应用于任何数据集。研究表明，CHG能提供因果性而不仅是相关性的见解，并通过对比CHG变体揭示了LLMs中存在稀疏且充分的子电路。

Motivation: 旨在开发一种无需依赖假设或模板的方法，直接解释Transformer模型中注意力头的功能角色。

Method: 提出了因果头门控（CHG）方法，通过学习头的软门并基于其对任务性能的影响进行分类，适用于任何标准的下一个词预测任务。

Result: CHG在多个LLAMA 3模型和多样化任务中表现出色，能揭示因果性见解，并发现LLMs中存在稀疏子电路。

Conclusion: CHG为解释注意力头的功能角色提供了新视角，揭示了LLMs的低模块性和任务机制的分离性。

Abstract: We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [577] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An,Hua-Dong Xiong,Robert C. Wilson,Marcelo G. Mattar,Marcus K. Benna*

Key words: 大语言模型, 元认知, 神经反馈, AI安全

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文研究了大型语言模型（LLMs）的元认知能力，即它们是否能监控和报告自身的激活模式，并通过神经反馈范式量化了这种能力。研究发现LLMs能学习和控制部分激活模式，但其能力受限。

Motivation: 随着社会对LLMs依赖的增加，了解其元认知能力的限制，尤其是在监控内部激活方面的能力，对AI安全至关重要。

Method: 采用神经反馈范式，通过提供句子-标签对来训练LLMs报告和控制其内部激活模式。

Result: LLMs能够学习和控制部分激活模式，但其性能受示例数量、目标神经方向的可解释性和方差解释率的影响。

Conclusion: LLMs的元认知能力有限，仅能监控部分神经机制，这对AI安全有重要启示。

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [578] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du,Li Chen,Xun Xian,An Luo,Fangqiao Tian,Ganghua Wang,Charles Doss,Xiaotong Shen,Jie Ding*

Key words: 因果推断、大语言模型、基准测试、统计陷阱

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了CausalPitfalls基准，用于严格评估大语言模型在克服因果推断常见陷阱中的能力。通过多难度挑战和评分标准，定量测量其推理能力和可靠性。

Motivation: 在医学、经济学等高风险领域，可靠的因果推断至关重要，但目前不清楚大语言模型是否能胜任。现有基准任务过于简化，忽略了统计陷阱，故需更全面的评估方法。

Method: 提出CausalPitfalls基准，包含结构化挑战和评分标准，通过直接提示和代码辅助提示两种协议评估模型，并与人类专家评分对比验证。

Result: 发现当前大语言模型在统计因果推断中存在显著局限性，CausalPitfalls提供了定量指标和指导。

Conclusion: CausalPitfalls为开发可信的因果推理系统提供了重要基准和支持。

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [579] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/abs/2505.13773)
*Ryan Bowers,Richard Agbeyibor,Jack Kolb,Karen Feigh*

Key words: 人类-AI协作, ISR环境, 团队策略, 风险行为, 熟悉方法

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文比较了三种让人熟悉AI队友的方法，发现文档学习最快但偏向保守，直接互动更愿冒险但理解较弱，推荐结合文档、训练和互动。

Motivation: 研究如何在快速协作的ISR环境中让人最快熟悉AI队友，以提高团队策略形成速度。

Method: 通过60人对比实验，分别采用阅读文档、直接训练或无熟悉方法，分析团队策略形成和风险行为。

Result: 文档学习最快但保守，互动学习更冒险但理解弱，个体风险偏好差异显著。

Conclusion: 推荐结合文档、训练和互动的人类-AI熟悉方法。

Abstract: We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [580] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/abs/2505.13774)
*Zidi Xiong,Chen Shan,Zhenting Qi,Himabindu Lakkaraju*

Key words: 大型推理模型, 忠实性评估, 反事实干预, 思维草稿, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种系统性反事实干预框架，用于评估大型推理模型（LRMs）中思维草稿的忠实性。研究发现当前LRMs对中间推理步骤的忠实性存在选择性，且结论与思维草稿的一致性不足。

Motivation: 为了确保LRMs在复杂问题解决中的中间推理过程的可靠性，需要评估其思维草稿的忠实性。

Method: 提出了一个反事实干预框架，通过两个维度评估忠实性：1) 草稿内忠实性（Intra-Draft Faithfulness），2) 草稿至答案忠实性（Draft-to-Answer Faithfulness）。

Result: 实验表明，当前LRMs对中间推理步骤的忠实性具有选择性，且与思维草稿的结论一致性较差。

Conclusion: 研究强调了在高级LRMs中实现更忠实和可解释推理的必要性。

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [581] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/abs/2505.13778)
*Guoheng Sun,Ziyao Wang,Bowei Tian,Meng Liu,Zheyu Shen,Shwai He,Yexiao He,Wanghao Ye,Yiting Wang,Ang Li*

Key words: 大语言模型, 透明度, 令牌验证, 账单审计

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出CoIn框架，用于验证大语言模型中隐藏推理令牌的数量和语义有效性，以解决账单透明度问题。

Motivation: 商业LLM服务隐藏推理过程，用户无法验证令牌使用情况，可能导致账单膨胀问题。

Method: CoIn通过令牌指纹的哈希树验证数量，并通过嵌入相关性匹配检测伪造内容。

Result: 实验显示CoIn能94.7%准确检测令牌膨胀，恢复账单透明性。

Conclusion: CoIn有效解决了LLM服务中的透明度问题，提升了账单可信度。

Abstract: As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [582] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/abs/2505.13794)
*Qi Cheng,Licheng Liu,Qing Zhu,Runlong Yu,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Key words: 生态时间序列, 度量学习, 大型语言模型, 评价框架, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种结合度量学习和大型语言模型（LLM）的框架，用于生态时间序列的评估，弥补传统数值指标的不足，并提供了可解释的评价标准。

Motivation: 传统数值指标无法捕捉生态过程中关键的时域模式，且依赖人工视觉检查，限制了大规模应用的可行性。

Method: 整合度量学习与LLM的自然语言策略提取，通过成对标注处理和政策优化机制生成和组合评估指标。

Result: 在多个数据集上验证了其有效性，特别是在作物总初级生产和二氧化碳通量预测评估中。

Conclusion: 该框架填补了数值指标与专家知识之间的鸿沟，提供了可解释且灵活适应不同生态系统建模需求的评估策略。

Abstract: Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [583] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/abs/2505.13828)
*Kiarash Naghavi Khanghah,Zhiling Chen,Lela Romeo,Qian Yang,Rajiv Malhotra,Farhad Imani,Hongyi Xu*

Key words: 增材制造, 异常检测, 检索增强生成, 零样本学习, 多模态模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该研究提出了一种基于多模态检索增强生成的框架，用于自动化增材制造中的异常检测，无需训练数据，通过检索文献信息实现零样本分类和解释。

Motivation: 增材制造在复杂设计和小批量生产中有优势，但缺陷和过程异常是主要挑战。传统方法依赖训练数据，难以适应新情况。

Method: 研究结合文本和图像检索，以及多模态生成模型，提出一个零样本异常检测框架，并在多个激光粉末床熔融数据集上评估。

Result: 框架在四组L-PBF数据集上表现出良好的适应性和泛化能力，GPT-4o-mini优于Qwen2-VL-2B和随机基线。检索机制的加入使平均准确率提高12%。

Conclusion: 该框架可随研究进展更新，是一种高效、自动化的增材制造异常分析方法，提升了效率和准确性。

Abstract: Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [584] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/abs/2505.13831)
*Zongyuan Deng,Yujie Cai,Qing Liu,Shiyao Mu,Bin Lyu,Zhen Yang*

Key words: 5G网络规划、AI驱动、基站选址、多目标优化、GRPO

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TelePlanNet是一个AI驱动的框架，用于5G基站选址，通过LLM和GRPO强化学习实现高效规划，提高一致性至78%。

Motivation: 解决传统人工方法和现有AI工具在5G基站选址中的效率和多目标优化不足的问题。

Method: 结合LLM实时处理用户输入和GRPO强化学习训练规划模型，实现多目标优化。

Result: 实验显示TelePlanNet将规划一致性提高到78%，优于人工方法。

Conclusion: TelePlanNet为电信运营商提供了高效、可扩展的网络规划工具。

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [585] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/abs/2505.13851)
*Sahil Shah,Harsh Goel,Sai Shankar Narasimhan,Minkyu Choi,S P Sharan,Oguzhan Akcin,Sandeep Chinchali*

Key words: 视频理解,神经符号方法,智能视频代理,时间推理,自主决策

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 现代视频理解系统在场景分类、物体检测和短视频检索等任务上表现优异，但缺乏长时间事件序列理解能力。提出神经符号方法以提升视频代理的推理和决策能力，推动智能视频代理的发展。

Motivation: 现有视频理解系统在单帧或短片段识别上表现优秀，但缺乏对长时间事件序列的理解能力，亟需提升视频代理的推理和决策能力。

Method: 采用神经符号方法，将视频查询分解为原子事件，构建序列并通过时间约束验证，以实现结构化推理和可解释性。

Result: 提出智能视频代理需具备三项核心能力：自主视频搜索与分析、无缝现实交互和高级内容生成。

Conclusion: 通过神经符号方法和三大核心能力的整合，可以推动视频代理从被动感知向智能推理和行动的转变。

Abstract: Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [586] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang*

Key words: 移动自动化, 视频指导, 知识注入, Mobile-Agent-V

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Mobile-Agent-V是一种利用视频指导注入操作知识的创新移动自动化框架，显著提升效率。

Motivation: 移动设备使用的激增需要高效的自动化任务管理，但现有AI框架因操作知识不足而受限。

Method: 引入Mobile-Agent-V框架，通过视频内容直接获取操作知识，避免人工干预。

Result: 实验显示，Mobile-Agent-V比现有方法性能提升36%。

Conclusion: Mobile-Agent-V为移动自动化提供了高效、省时的知识注入解决方案。

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [587] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He,Jiahe Jin,Pengfei Liu*

Key words: PC Agent-E, 轨迹数据, Claude 3.7 Sonnet, 计算机代理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: PC Agent-E框架通过少量高质量轨迹数据显著提升计算机代理性能，减少对人类示范的依赖。

Motivation: 解决高质量轨迹数据稀缺问题，开发高效计算机代理训练方法。

Method: 利用312条人工标注轨迹，通过Claude 3.7 Sonnet合成多样化动作决策，提升数据质量。

Result: PC Agent-E在WindowsAgentArena-V2基准上性能提升141%，并展示跨操作系统泛化能力。

Conclusion: 少量高质量轨迹数据可激发强大计算机使用能力。

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [588] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/abs/2505.13914)
*Jake Chandler,Richard Booth*

Key words: 并行修正、信念修正、迭代修正、TeamQueue聚合器

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一个基于TeamQueue聚合器的方法，将串行迭代信念修正算子扩展到并行变化，以满足合理性假设。

Motivation: 当前对并行迭代信念修正的研究较少，缺乏统一的理论框架，本文旨在填补这一空白。

Method: 基于TeamQueue聚合器家族的方法，将串行迭代信念修正算子扩展为并行变化处理方式。

Result: 该方法能够满足文献中合理的性质，同时避免不合理的结果。

Conclusion: 通过统一的理论框架，有效地处理了并行迭代信念修正问题。

Abstract: Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [589] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/abs/2505.13940)
*Kun Li,Zhennan Wu,Shoupeng Wang,Wenbin Hu*

Key words: AI4Science, LLM, 药物发现, 参数化推理, 多模态数据处理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为DrugPilot的基于LLM的药物发现代理系统，通过参数化推理架构解决了多模态数据处理、领域知识更新延迟等挑战，并在任务完成率上表现优异。

Motivation: 现有LLM在药物发现领域面临多模态数据处理、知识更新延迟等挑战，亟需改进。

Method: 提出了DrugPilot代理系统，包含参数化记忆池和药物指令数据集，支持多阶段任务自动规划与执行。

Result: DrugPilot在简单、多重和多轮任务上的完成率分别为98.0%、93.5%和64.0%，优于现有代理。

Conclusion: DrugPilot通过创新的参数化推理架构显著提升了药物发现中的LLM应用效果。

Abstract: In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [590] [Visual Instruction Bottleneck Tuning](https://arxiv.org/abs/2505.13946)
*Changdae Oh,Jiatong Li,Shawn Im,Yixuan Li*

Key words: 多模态大语言模型, 信息瓶颈, 鲁棒性, 分布偏移, 视觉指令

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出Vittle方法，通过信息瓶颈原理增强多模态大语言模型（MLLMs）在分布偏移下的鲁棒性。

Motivation: 现有方法需要更多指令数据或更大模型架构，成本高昂，因此从表示学习角度改进MLLMs的鲁棒性。

Method: 基于信息瓶颈（IB）原理，推导出MLLMs的变分下界，提出视觉指令瓶颈调优（Vittle）方法。

Result: 在45个数据集（包括30种偏移场景）上的实验表明，Vittle通过最小充分表示学习提升了MLLMs的鲁棒性。

Conclusion: Vittle是一种有效的低成本方法，能显著提升MLLMs在分布偏移下的性能。

Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [591] [Solving Normalized Cut Problem with Constrained Action Space](https://arxiv.org/abs/2505.13986)
*Qize Jiang,Linsey Pang,Alice Gatti,Mahima Aggarwa,Giovanna Vantin,Xiaosong Ma,Weiwei Sun,Sanjay Chawla*

Key words: 强化学习, 组合优化, 归一化切割, 楔形和环形变换器, 交通运输网络

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种新的强化学习方法，通过约束动作空间引导组合优化问题的解，特别是在归一化切割问题中应用楔形和环形变换器，以获得更接近自然最优解的分区。

Motivation: 虽然强化学习在解决组合优化问题中表现出色，但如何整合外部知识以引导解向领域合适的方向发展仍然是一个挑战。

Method: 采用约束动作空间的强化学习方法，结合楔形和环形变换器，引导归一化切割问题的解。

Result: 在交通运输网络中的应用表明，该方法能生成更接近自然最优解的楔形和环形分区。

Conclusion: 该方法不仅适用于交通运输网络，还能推广到其他领域。

Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve
combinatorial optimization problems primarily due to its ability to learn
heuristics that can generalize across problem instances. However, integrating
external knowledge that will steer combinatorial optimization problem solutions
towards domain appropriate outcomes remains an extremely challenging task. In
this paper, we propose the first RL solution that uses constrained action
spaces to guide the normalized cut problem towards pre-defined template
instances. Using transportation networks as an example domain, we create a
Wedge and Ring Transformer that results in graph partitions that are shaped in
form of Wedges and Rings and which are likely to be closer to natural optimal
partitions. However, our approach is general as it is based on principles that
can be generalized to other domains.

</details>


### [592] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/abs/2505.13994)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Key words: Retrieval-Augmented Generation, 知识图谱, 语义分割, 多智能体, SPLIT-RAG

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SPLIT-RAG是一种多智能体RAG框架，通过语义图分割和协作子图检索解决现有RAG系统在大规模知识图谱上的效率-准确性权衡问题。

Motivation: 大型语言模型（LLMs）在扩展到大知识图谱时，面临着检索效率与准确性之间的权衡问题。现有方法往往依赖于单一图检索，导致简单查询时不必要的延迟和复杂多跳问题的碎片化推理。

Method: 提出SPLIT-RAG框架，采用问题驱动的语义图分割和协作子图检索。通过语义划分链接信息，利用类型专门化的知识库实现多智能体RAG，并通过轻量级LLM代理管理分割后的子图，激活相关分区进行检索，减少搜索空间。

Result: 实验验证表明，SPLIT-RAG在效率和准确性上显著优于现有方法。

Conclusion: SPLIT-RAG通过语义分割和多智能体协作优化了RAG系统在大规模知识图谱中的性能。

Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [593] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/abs/2505.14001)
*Sterre Lutz,Matthijs T. J. Spaan,Anna Lukina*

Key words: 自主系统, 概率神经Lyapunov认证, 随机动力系统, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: VeRecycle是一个框架，用于在系统动态仅在状态子集变化时高效重用概率证书，减少重新认证的计算成本。

Motivation: 现实中的自主系统面临多种不确定性，但现有的概率认证方法在系统动态变化时需要完全重新认证，计算成本高。

Method: 提出了VeRecycle框架，通过局部状态空间变化的假设，高效重用概率证书。

Result: 实验证明VeRecycle既能节省计算资源，又能提供竞争性的概率保证。

Conclusion: VeRecycle为非线性随机动力系统的认证提供了高效解决方案。

Abstract: Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [594] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2505.14020)
*Hao Dong,Ziyue Qiao,Zhiyuan Ning,Qi Hao,Yi Du,Pengyang Wang,Yuanchun Zhou*

Key words: TKG、外推、多跨度演化、解缠、语义变化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了DiMNet模型，通过多跨度演化策略和解缠技术改进TKG推理，性能提升22.7%。

Motivation: 现有TKG外推方法在子图语义演化和平滑特征捕捉上存在不足。

Method: 使用多跨度演化策略和解缠组件区分活跃与稳定特征。

Result: 在四个真实数据集上大幅超越现有方法，最高提升22.7%的MRR。

Conclusion: DiMNet有效捕获子图内部交互和语义变化，显著提升推理性能。

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [595] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng,Sijie Ji,Jiawei Sun,Renqi Chen,Wei Gao,Mani Srivastava*

Key words: 心理健康，风险评估，大语言模型，ProMind-LLM，因果推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出ProMind-LLM，通过结合客观行为数据和主观心理记录，利用领域预训练、自优化机制和因果链推理，提升心理健康风险评估的可靠性和可解释性。

Motivation: 心理健康风险是全球公共卫生挑战，现有方法依赖主观文本数据，易受心理不确定性影响，导致预测不一致且不可靠。

Method: 提出ProMind-LLM，结合主观心理记录和客观行为数据，使用领域预训练、自优化机制和因果链推理。

Result: 在PMData和Globem数据集上测试显示，方法显著优于通用大语言模型。

Conclusion: ProMind-LLM为心理健康问题提供了更可靠、可解释和可扩展的解决方案。

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [596] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/abs/2505.14072)
*Soroush Hashemifar,Sherry Sahebi*

Key words: 学生知识跟踪,行为建模,个性化学习,多任务学习,KMaP

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: KMaP是一种多任务学习方法，通过聚类学生画像，实现个性化知识跟踪和行为建模，解决了传统方法中上下文信息丢失和非评估材料忽略的问题。

Motivation: 深度学习在教育领域的应用存在局限性，如缺乏个性化、忽视非评估材料以及知识获取与行为模式的关联性不足。KMaP旨在通过多任务方法解决这些问题。

Method: 提出KMaP模型，通过聚类分析生成个性化学生表征，同时建模学生知识和行为，并预测未来学习资源偏好。

Result: 在两个真实数据集上的实验显示，不同学生集群存在显著行为差异，KMaP模型表现优异。

Conclusion: KMaP有效提升了学生知识跟踪和行为建模的个性化程度，为教育领域的深度学习应用提供了新思路。

Abstract: Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [597] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/abs/2505.14137)
*Vojtěch Kůr,Vít Musil,Vojtěch Řehák*

Key words: 对抗性巡逻游戏,有限记忆策略,内存分配,策略优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种新方法，通过迭代调整内存分配，解决了有限记忆防御策略中的内存分配难题，提升了策略效果。

Motivation: 有限记忆防御策略在对抗性巡逻游戏中表现出色，但当前算法需要手动分配内存，这一问题是长期未解决的难题。

Method: 开发了一种通用方法，可迭代调整内存分配，并与任何黑盒策略优化工具结合使用。

Result: 实验证明了该方法的鲁棒性，能够解决多种巡逻模型中的实例。

Conclusion: 该方法有效解决了有限记忆策略中的内存分配问题，提升了策略的实用性。

Abstract: Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [598] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/abs/2505.14140)
*Qianyue Hao,Sibo Li,Jian Yuan,Yong Li*

Key words: 大语言模型、强化学习、推理框架、自适应、逻辑模块

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的自适应推理框架RLoT，通过动态选择逻辑模块提升大语言模型的推理能力，性能优于现有方法。

Motivation: 现有推理框架缺乏适应性，无法针对不同任务动态调整。

Method: 设计了五种基础逻辑模块，训练轻量级RL导航器动态组合模块。

Result: 在多个基准测试中性能提升13.4%，轻量级导航器（<3K参数）使小模型达到接近大模型的效果。

Conclusion: RLoT具有强适应性和迁移性，为提升LLM推理能力提供了新思路。

Abstract: Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [599] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/abs/2505.14141)
*Fanglin Mo,Junzhe Chen,Haoxuan Zhu,Xuming Hu*

Key words: 移动GUI代理, 任务规划, SPlanner, 扩展有限状态机, 自然语言计划

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SPlanner是一个用于移动GUI代理的规划模块，通过扩展有限状态机建模应用逻辑，生成自然语言执行计划，显著提升任务成功率。

Motivation: 移动GUI代理在执行任务时由于缺乏对目标应用的深入理解，容易在任务规划中迷失方向，因此需要一种有效的规划方法。

Method: 提出SPlanner模块，利用扩展有限状态机（EFSMs）建模应用逻辑，分解用户指令为功能序列并生成执行路径，再通过LLM转换为自然语言计划。

Result: 在AndroidWorld基准测试中，SPlanner与Qwen2.5-VL-72B结合使用时任务成功率达63.8%，比无规划辅助时提高了28.8个百分点。

Conclusion: SPlanner能有效解决GUI代理的任务规划问题，生成简洁可执行的计划，显著提升任务完成率。

Abstract: Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [600] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/abs/2505.14143)
*Shuo Zhang,Jinsong Zhang,Zhejun Zhang,Lei Li*

Key words: 多任务学习, 多模态情感分析, 多模态情绪识别, 低秩专家网络

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为MMoLRE的多任务学习方法，用于多模态情感分析和多模态情绪识别，通过共享和任务特定的专家网络解决参数冲突问题，并在性能上取得显著提升。

Motivation: 多任务学习（MTL）可以通过共享知识提升任务性能，但现有方法忽视了任务间复杂相关导致的参数冲突问题。

Method: 提出MMoLRE方法，结合共享和任务特定专家网络，并采用低秩专家网络减少计算开销。

Result: 在CMU-MOSI和CMU-MOSEI基准测试中，MMoLRE在多模态情感分析任务上达到最优性能，在多模态情绪识别任务上表现竞争性。

Conclusion: MMoLRE通过有效建模任务共性和特性，解决了参数冲突问题，同时降低了计算成本。

Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [601] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang,Xueqiang Xu,Jiacheng Lin,Jinfeng Xiao,Zifeng Wang,Jimeng Sun,Jiawei Han*

Key words: 检索增强生成、大型语言模型、强化学习、问答系统、医学QA

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一个名为s3的轻量级、模型无关框架，通过解耦搜索器与生成器并基于RAG的改进奖励训练搜索器，显著提升了检索增强生成系统的性能。

Motivation: 现有检索增强生成系统中，检索优化可能忽视了生成任务的性能，或需要联合优化整个大型语言模型，限制了实际应用。s3旨在解决这些问题。

Method: s3框架解耦了搜索器与生成器，并使用‘Gain Beyond RAG’奖励（生成准确性的改进）训练搜索器。仅需少量训练数据即可高效优化检索。

Result: s3在仅2.4k训练样本下，性能超越使用70倍以上数据的基线，并在多个问答基准测试中表现更优。

Conclusion: s3框架通过解耦与高效训练，显著提升了检索增强生成系统的性能，适用于多种任务与模型。

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [602] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/abs/2505.14147)
*Xiong Jun Wu,Zhenduo Zhang,ZuJie Wen,Zhiqiang Zhang,Wang Ren,Lei Shi,Cai Chen,Deng Zhao,Dingnan Jin,Qing Cui,Jun Zhou*

Key words: SHARP, 大型推理模型, 强化学习, STEM, 复杂推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SHARP是一种合成高质量、可验证推理问题的方法，用于增强大型推理模型的强化学习，显著提升了复杂任务的准确性。

Motivation: 现有合成方法（如Chain-of-Thought）生成的问题过于简单或不可验证，限制了模型在复杂任务上的进步。

Method: SHARP通过自对齐原则（如高难度、逻辑一致性、可验证答案）和三阶段框架（对齐、实例化、推理）生成高质量问题，并利用强化学习优化模型。

Result: 实验表明，SHARP显著提升了模型在复杂推理任务中的表现，接近专家水平。

Conclusion: SHARP为大型推理模型提供了高效的问题生成方法和训练框架，推动了复杂推理能力的进步。

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [603] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/abs/2505.14148)
*Fan Liu,Zherui Yang,Cancheng Liu,Tianrui Song,Xiaofeng Gao,Hao Liu*

Key words: 数学建模, 大语言模型, MM-Agent, MM-Bench

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种基于LLM的数学建模框架MM-Agent，通过四阶段分解方法显著提升了建模能力，并在竞赛中取得了优异表现。

Motivation: 数学建模在科学和工程中至关重要，但LLM在此领域表现不足，亟需一种能分析问题并生成端到端解决方案的方法。

Method: 提出了MM-Agent框架，将建模分为问题分析、模型构建、计算求解和报告生成四个阶段，并在MM-Bench数据集上进行验证。

Result: MM-Agent在MM-Bench上表现优异，比基线高出11.88\%；在MCM/ICM竞赛中成功辅助团队获奖。

Conclusion: MM-Agent证明LLM可作为建模助手，显著提升建模效率和效果。

Abstract: Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [604] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/abs/2505.14163)
*He Wang,Alexander Hanbo Li,Yiqun Hu,Sheng Zhang,Hideo Kobayashi,Jiani Zhang,Henry Zhu,Chung-Wei Hang,Patrick Ng*

Key words: LLM代理, 数据科学, 课程学习, 推理优化, 长期记忆

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为DSMentor的新型推理时优化框架，利用课程学习策略提升LLM代理在复杂数据科学任务中的表现。

Motivation: 现有研究忽略了问题解决顺序的重要性，本文旨在通过课程学习策略优化LLM代理的性能。

Method: DSMentor框架按难度递增组织任务，并引入长期记忆机制以积累和利用知识。

Result: 实验表明，DSMentor在DSEval和QRData基准上提升了5.2%的通过率，在因果推理任务中比GPT-4高出8.8%。

Conclusion: DSMentor通过模仿人类学习过程的策略，为LLM性能提升开辟了新途径。

Abstract: Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [605] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/abs/2505.14193)
*Abdallah Abuaisha,Bojie Shen,Daniel Harabor,Peter Stuckey,Mark Wallace*

Key words: 公共交通, 动态重规划, 推式方法, 拉式方法, 实时延迟

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种动态重规划框架，利用实时延迟数据优化公共交通路线，推式方法显著优于拉式方法。

Motivation: 现有解决方案无法充分利用实时延迟数据，导致用户错失更早到达的机会，因此需要一种系统级的动态重规划框架。

Method: 提出了推式和拉式两种动态重规划方法，推式由服务器主动监控和调整路线。

Result: 实验表明推式方法显著优于拉式方法，动态重规划带来了显著的到达时间节省。

Conclusion: 利用实时延迟数据的动态重规划能显著优化用户行程，推式方法更具优势。

Abstract: Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [606] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/abs/2505.14209)
*Li Wang,Xin Yu,Xuxin Lv,Gangzheng Ai,Wenjun Wu*

Key words: 无人机, 周边防御, 异质性, 表示学习, 均值场方法

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究了大规模异质三维周边防御博弈，提出EMFAC框架，结合表示学习和注意力机制，提升防御策略的可扩展性和决策效率，并通过模拟和实验验证其有效性。

Motivation: 现有研究多限于简化的二维小规模场景，忽视了现实环境扰动和异质性，需在三维大规模场景中探索更实用的防御策略。

Method: 提出EMFAC框架，利用表示学习和均值场方法实现高效协调；引入轻量级注意力机制优化观测和决策。

Result: EMFAC在模拟中表现优于基线方法，收敛速度和性能均有提升，并在小规模现实实验中验证了实用性。

Conclusion: EMFAC为解决大规模异质防御问题提供了有效方案，适用于复杂三维场景。

Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [607] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim,Anubhav Shrestha,Safal Shrestha,Aadim Nepal,Keith Ross*

Key words: 强化学习,蒸馏,语言模型,推理行为

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨了RLVR和蒸馏对语言模型推理行为的影响，发现RLVR虽能提升整体准确性但未能提升能力，而蒸馏则在引入新知识时才提升能力。

Motivation: 探讨RLVR和蒸馏对模型能力与准确性影响的机制。

Method: 通过小模型实验分析RLVR和蒸馏对问题难度及回答质量的影响。

Result: RLVR仅提升简单问题的准确性而损害难题，蒸馏只有引入新知识时才提升能力。

Conclusion: 研究揭示了RLVR和蒸馏如何影响模型的推理行为。

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [608] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/abs/2505.14235)
*Yequan Wang,Aixin Sun*

Key words: Artificial General Intelligence, Embodied AGI, Taxonomy, Robotics, Conceptual Framework

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 这篇论文提出了一个关于具身通用人工智能（Embodied AGI）的系统分类法，分为五个等级（L1-L5），并探讨了实现更高水平能力所需的关键组件。

Motivation: 随着机器人和基础AI模型的进展，作者希望通过系统分类法和框架推动具身AGI的发展。

Method: 作者提出了一个五级分类法（L1-L5），并基于现有技术设计了一个L3+机器人脑的概念框架。

Result: 论文总结并展望了实现更高水平具身AGI的技术路径。

Conclusion: 该框架为未来研究提供了技术和理论基础，旨在推动具身AGI的发展。

Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [609] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/abs/2505.14289)
*Yijie Lu,Tianjie Ju,Manman Zhao,Xinbei Ma,Yuan Guo,ZhuoSheng Zhang*

Key words: 间接提示注入, 多模态代理, 红队测试, GUI, 动态优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了EVA框架，用于动态优化间接提示注入攻击，显著提高攻击成功率和跨场景适用性。

Motivation: 多模态代理在GUI环境中易受间接提示注入攻击，现有静态方法效果不佳，需动态优化攻击策略。

Method: 提出EVA框架，通过闭环优化动态调整攻击提示，结合代理的视觉注意力分布持续更新策略。

Result: 实验表明EVA在六种GUI代理中显著优于静态方法，且攻击模式可跨模型迁移，揭示代理的共同行为偏差。

Conclusion: EVA是红队测试和发现多模态代理决策漏洞的有力工具。

Abstract: As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [610] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary,Fazl Barez*

Key words: 实时监测, 大型语言模型, 后门触发, 无监督学习, 欺骗行为

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种实时监测框架，通过无监督方法预测大型语言模型（LLMs）的有害输出，重点关注后门触发响应，并在检测有害行为方面达到96%的准确性。

Motivation: 高风险行业（如核能和航空）使用实时监测来检测危险系统状态，类似的，LLMs也需要监测保障。本文旨在通过无监督方法预测有害输出，并防止高级模型的欺骗行为。

Method: 采用无监督方法，将正常行为作为基线，有害输出视为异常值。通过研究后门触发响应和模型内部行为特征，开发了Safety-Net多检测器框架，监测不同表示维度。

Result: 研究发现模型能够通过因果机制生成有害内容，并通过改变特征关系实现欺骗。Safety-Net框架在检测有害行为方面达到96%的准确性。

Conclusion: 本文提出的无监督框架能够有效检测LLMs的有害输出，即使模型尝试通过多种方式逃避监测。

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [611] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/abs/2505.14366)
*Joel Currie,Gioele Migno,Enrico Piacenti,Maria Elena Giannaccini,Patric Bach,Davide De Tommaso,Agnieszka Wykowska*

Key words: 视觉语言模型（VLMs），视觉透视（VPT），人类-机器人交互（HRI），合成数据集，空间推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出用视觉语言模型（VLMs）训练视觉透视（VPT）的框架，并发布了一个用于监督学习的合成数据集。

Motivation: 为人类-机器人交互（HRI）中的空间认知能力提供基础技术支持。

Method: 使用NVIDIA Omniverse生成合成数据集，包含RGB图像、自然语言描述和对象位姿的变换矩阵。

Result: 专注于Z轴距离推理，未来将扩展到6自由度推理。数据集已公开。

Conclusion: 这是实现具备空间理解能力的具身AI系统的初步工作。

Abstract: We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [612] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.14381)
*Yuyang Dong,Nobuhiro Ueda,Krisztián Boros,Daiki Ito,Takuya Sera,Masafumi Oyamada*

Key words: 视觉语言模型,检索增强生成,布局分析,语义分割

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种名为SCAN的新型方法，通过增强视觉丰富文档的语义布局分析，提升文本和视觉检索增强生成（RAG）系统的性能。

Motivation: 随着大型语言模型（LLMs）和视觉语言模型（VLMs）的普及，处理视觉丰富文档的需求日益增长，但现有方法在信息密集的单一页面中仍面临挑战。

Method: SCAN采用粗粒度的语义方法，将文档分为连贯区域，并通过微调目标检测模型实现高效布局分析。

Result: 实验表明，SCAN在英语和日语数据集上分别将文本和视觉RAG性能提升了9.0%和6.4%，优于传统方法和商业解决方案。

Conclusion: SCAN通过平衡上下文保留与处理效率，显著提升了RAG系统的性能。

Abstract: With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [613] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/abs/2505.14391)
*Zhaohui Yang,Chenghua He,Xiaowen Shi,Linjing Li,Qiyue Yin,Shihong Deng,Daxin Jiang*

Key words: PRMs, 数据标注, 长链式思维, 错误传播, 错误终止

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种针对长链式思维（CoT）推理过程的新型数据标注方法，通过引入错误传播和错误终止概念，提升PRMs识别自我纠正行为的能力，实验表明其优于现有方法。

Motivation: 现有数据标注方法在长CoT推理中仅关注首次错误步骤及之前步骤，忽略了后续可能存在的正确推理步骤，因此需要改进。

Method: 提出了一种新的数据标注方法，结合错误传播和错误终止概念，利用LLM生成标注数据，训练7B PRM模型。

Result: 实验显示，该方法在搜索指导、BoN和F1分数上优于现有开源PRMs，且数据效率和性能更优。

Conclusion: 该方法不仅稳定性强且泛化能力高，显著提升了PRMs的长CoT推理能力。

Abstract: Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [614] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/abs/2505.14394)
*Mihir Athale,Vishal Vaddina*

Key words: Large Language Models, 知识图谱, 代码检索, 代码生成

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱的代码检索方法，通过将代码库表示为图结构，提升代码生成的上下文准确性和质量。

Motivation: 当前LLMs在代码生成中存在上下文准确性不足的问题，尤其面对动态变化的代码库时表现不佳。

Method: 采用知识图谱表示代码库，结合混合检索方法，捕捉代码的结构和依赖关系。

Result: 在EvoCodeBench数据集上显著优于基线方法。

Conclusion: 基于知识图谱的代码生成方法能提升上下文敏感的编码辅助工具的效果。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [615] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron,Jože M. Rožanec,Michael Witbrock,Gillian Dobbie*

Key words: 因果推理, 反事实问题, 大型语言模型, 图检索增强生成, 因果知识库

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了Causal Cartographer框架，通过显式提取和建模因果关系，解决现有基础模型（如LLM）在因果推理任务中的局限性，并提升推理的鲁棒性。

Motivation: 现有的大型语言模型（LLM）在因果推理任务中表现不足，主要是依赖记忆而非真正的因果推理能力。此外，现实应用中反事实推理的评估也面临挑战。

Method: 提出Causal Cartographer框架，包括一个图检索增强生成代理用于从数据中提取因果关系，以及一个受因果关系约束的反事实推理代理。

Result: 该方法能够有效提取因果知识，提升LLM在因果推理任务中的鲁棒性，同时降低推理成本并减少虚假相关性。

Conclusion: Causal Cartographer框架通过显式建模因果关系，显著提升了模型在复杂因果推理任务中的表现。

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [616] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Key words: 推理模型,负面样本,离线强化学习,BCPG-NSA

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出BCPG-NSA框架，通过细粒度处理负面样本中的有效信息，提升推理模型的性能。

Motivation: 长链思维模型因计算成本高，需充分利用固定训练数据；负面样本中的自反思与纠错步骤未被现有方法有效利用。

Method: BCPG-NSA包括样本分割、基于共识的步骤评估（结合LLM和PRM）及策略优化（NSA挖掘负面样本中的有效步骤）。

Result: 在同一训练数据下，BCPG-NSA在数学/代码推理任务中表现优于基线，提高了样本效率并展现出扩展性。

Conclusion: BCPG-NSA证明了负面样本中有效信息的挖掘对提升模型性能的重要性。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [617] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski,Adrian Kosmala,Paul Swoboda*

Key words: 提示工程, 强化学习, LLM, 自动提示生成, 性能提升

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: PRL（基于强化学习的自动提示生成方法）通过生成训练中未见过的few-shot示例，在文本分类、简化与摘要任务中实现最佳性能。

Motivation: 提示工程对发挥LLM能力至关重要，但传统方法依赖专家直觉且难以捕捉微妙语义线索，PRL旨在通过强化学习自动生成高效提示。

Method: 提出PRL，一种基于强化学习的自动提示生成方法，能够生成训练中未见的few-shot示例。

Result: PRL在分类任务中超越APE和EvoPrompt，摘要任务中ROUGE分数显著提升，简化任务中SARI分数也大幅提高。

Conclusion: PRL通过自动生成提示，显著提升了多种任务的性能，为提示工程提供了高效解决方案。

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [618] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/abs/2505.14419)
*Huimin Xu,Xin Mao,Feng-Lin Li,Xiaobao Wu,Wang Chen,Wei Zhang,Anh Tuan Luu*

Key words: 过程奖励模型、数学推理、注释成本、压缩方法、前缀树

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为SCOPE的新型压缩方法，通过将自然语言推理步骤转化为代码并合并等效步骤，显著降低了过程注释的计算成本。

Motivation: 现有的过程奖励模型（PRMs）在数学推理中表现出色，但传统的过程注释方法（如人工注释或蒙特卡洛模拟）计算成本高昂。

Method: 提出SCOPE方法，将自然语言推理步骤转化为代码，并通过抽象语法树归一化，合并等效步骤构建前缀树。

Result: SCOPE将复杂度从$O(NMK)$降至$O(N)$，并用仅5%的计算资源构建了包含196K样本的大规模数据集。

Conclusion: 在PRMs训练中，SCOPE显著优于现有自动注释方法，尤其是在Best-of-N策略和ProcessBench上表现突出。

Abstract: Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [619] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan,Eitan Stern,Dafna Shahaf*

Key words: 神经符号方法, 大型语言模型, 几何证明, 形式验证器

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 结合神经符号方法，通过类比问题检索和形式验证器提升LLM在几何问题中的证明生成能力，显著提高准确率。

Motivation: 解决大型语言模型在需要严格逻辑推理和符号推理的正式领域（如数学证明生成）中的局限性。

Method: 两阶段方法：(1) 检索类似问题并用其证明指导LLM；(2) 使用形式验证器评估和反馈生成的证明。

Result: 显著提高了OpenAI o1模型的证明准确率（58%-70%提升）。

Conclusion: 生成可证明正确结论的LLM可大幅提升可靠性、准确性和一致性，适用于需要高信任度的复杂任务。

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [620] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon,Seungone Kim,Sohee Yang,Sunkyoung Kim,Soyeon Kim,Yongil Kim,Eunbi Choi,Yireun Kim,Minjoon Seo*

Key words: 大语言模型；链式思维；信心校准；慢思考；上下文学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究显示，进行链式思维（CoT）推理的大语言模型（LLMs）在问题解决和信心表达上优于非推理模型，且信心校准表现更优。

Motivation: LLMs在表达自身信心时往往不准确，限制了其可靠性。本研究探讨推理模型能否更准确地表达信心。

Method: 对六种推理模型在六个数据集上进行基准测试，分析其慢思考行为（如探索替代方法和回溯）对信心校准的影响。

Result: 推理模型在33/36设置中表现优于非推理模型，信心校准随时间推移而提升；移除慢思考行为会导致校准显著下降。

Conclusion: 慢思考行为是推理模型信心校准的关键，非推理模型通过上下文学习也能受益。

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [621] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/abs/2505.14510)
*Haishi Bai,Jozo Dujmovic,Jianwu Wang*

Key words: BACON, explainable AI, graded logic, decision making, transparency

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: BACON是一个基于分级逻辑的新型框架，用于自动训练可解释的AI模型，旨在实现高预测精度和完全透明的决策逻辑。

Motivation: 随着机器学习和自主代理在高风险领域（如医疗、金融等）的广泛应用，对透明和可信赖的解释的需求日益迫切。

Method: BACON使用分级逻辑自动训练可解释的AI模型，提供结构透明和基于逻辑的符号化解释。

Result: 在多个场景（如布尔近似、鸢尾花分类等）中，BACON不仅实现了高性能，还生成了紧凑且可验证的决策逻辑。

Conclusion: BACON是一个实用且原则性的方法，能够提供清晰、可信赖的可解释AI。

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [622] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher,William Brach,Tibor Sloboda,Kristián Košťál,Lukas Galke*

Key words: 查询路由，LLM，分布外查询，GQR-Bench，文本分类

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文研究了一个‘保护式查询路由’问题，提出GQR-Bench基准测试，并对比了多种路由方法的性能。结果显示，结合离域检测的WideMLP在准确性和速度上表现最佳。

Motivation: 在查询路由任务中，如何处理分布外查询（如无关领域问题、其他语言或不安全文本）是一个重要问题。本文旨在研究和评估不同方法在此任务中的表现。

Method: 引入GQR-Bench基准测试，覆盖三个目标领域和七种数据集，评估了LLM、传统机器学习模型和词袋分类器在查询路由任务中的性能。

Result: WideMLP在准确率（88%）和速度（<4ms）上表现最佳；fastText速度最快（<1ms）；LLM准确率最高（91%）但速度较慢。

Conclusion: 质疑了LLM在查询路由中的自动依赖，推荐在实际应用中采用WideMLP或fastText等高效方法。

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [623] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/abs/2505.14539)
*Gaia Belardinelli,Thomas Bolander,Sebastian Watzl*

Key words: 注意力逻辑、动态认知逻辑、AI代理、模态逻辑、边条件事件模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种通用的注意力逻辑模型，解决了现有动态认知逻辑无法处理复杂注意力场景的问题，尤其适用于AI代理分析人类注意力偏见的场景。

Motivation: 现有动态认知逻辑仅能处理原子公式的注意力模型，且复杂度随代理和变量数量指数增长，无法应对复杂注意力场景如高阶信念或多代理注意力。

Method: 通过扩展边条件事件模型，使其表达力与标准事件模型相当但更简洁；将注意力扩展到任意公式，支持代理间注意力交互。

Result: 新逻辑模型在表达力和复杂度上取得平衡，支持复杂注意力场景建模，并成功应用于AI代理发现人类注意力偏见。

Conclusion: 该逻辑模型为注意力提供了一种模态化处理方式，填补了现有工具的空白，具有理论和应用双重价值。

Abstract: In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [624] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/abs/2505.14544)
*Saahil Mahato*

Key words: 交通拥堵，多智能体强化学习，动态信号控制，仿真

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨了多智能体强化学习在优化交通信号协调中的应用，显示其相比于固定时间信号控制系统能显著减少等待时间和提高交通流量。

Motivation: 城市交通拥堵在十字路口尤为严重，固定时间信号控制系统难以应对动态交通模式，需要更智能的解决方案。

Method: 通过模拟环境（Pygame）建模多个十字路口，实现分散式多智能体强化学习控制器，每个交通信号作为自主智能体基于局部观察和邻域信息决策。

Result: 与固定时间控制器相比，MARL显著减少了平均等待时间并提高了整体交通流量。

Conclusion: MARL动态控制策略在提升城市交通效率方面具有潜力，但需进一步研究可扩展性和实际应用挑战。

Abstract: Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [625] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj,Arjun Beniwal,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik R. Narasimhan,Ameet Deshpande,Vishvak Murahari*

Key words: 多智能体系统, 结构化协议, 协作推断, 持久蓝图, 标准化消息

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文介绍了Agent Context Protocols（ACPs），一种用于多智能体系统的结构化协议，通过结合执行蓝图和标准化消息模式，提升协作性能，并在实验中达到领先水平。

Motivation: 当前多智能体系统的协调依赖非结构化的自然语言，限制了复杂交互和领域专用智能体的互操作性。

Method: 提出ACPs协议，结合持久执行蓝图和标准化消息模式，实现多智能体的高效协作与容错。

Result: ACP系统在AssistantBench上的长时网络辅助任务中达到28.3%准确率，在多模态技术报告中表现优于商业AI系统。

Conclusion: ACPs具有高度模块化和可扩展性，可快速构建高性能通用智能体。

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [626] [Towards a Foundation Model for Communication Systems](https://arxiv.org/abs/2505.14603)
*Davide Buffelli,Sowmen Das,Yu-Wei Lin,Sattar Vakili,Chien-Yi Wang,Masoud Attarifar,Pritthijit Nath,Da-shan Shiu*

Key words: 人工智能,通信系统,Transformer,多模态模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文探讨了AI在通信系统中的基础模型，基于Transformer的多模态模型，能够直接处理通信数据，并提出了相关方法论。

Motivation: AI在多个领域表现出色，但在通信系统中仍以任务特定解决方案为主，本文旨在开发一种通用的通信数据基础模型。

Method: 基于Transformer的多模态模型，解决了tokenization、位置嵌入、多模态、可变特征大小和归一化等关键问题。

Result: 实验证明，该模型能成功估计多个通信特征，如传输秩、预编码器选择、多普勒扩展和延迟剖面。

Conclusion: 本文提出的基础模型为通信系统提供了一种通用的AI解决方案，具有广泛的应用潜力。

Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [627] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604)
*Haoran Zhao,Yuchen Yan,Yongliang Shen,Haolei Xu,Wenqi Zhang,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Key words: 大型推理模型，冗余推理，Self-Braking Tuning，计算开销，自我调节

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为Self-Braking Tuning (SBT)的新框架，旨在通过模型自我调节来解决大型推理模型（LRMs）中的冗余推理问题，减少计算开销。

Motivation: 大型推理模型在生成长链思维时出现大量冗余推理，导致计算成本高昂和过度思考问题，现有方法依赖外部干预，效果有限。

Method: 提出SBT框架，设计过思考识别指标，构建自适应推理长度数据，引入刹车提示机制，使模型学会自我终止推理。

Result: 实验表明，SBT在数学基准测试中减少了60%的token消耗，同时保持与未约束模型相当的准确性。

Conclusion: SBT框架有效解决了冗余推理问题，提供了一种无需外部干预的自我调节方案。

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [628] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei,Yuheng Wu,Yingjia Wan,Tarun Suresh,Huanmi Tan,Zhanke Zhou,Sanmi Koyejo,Ke Wang,Alex Aiken*

Key words: SATBench, 大语言模型, 逻辑推理, 布尔可满足性, 逻辑搜索

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SATBench是一种基于布尔可满足性问题的逻辑推理评估基准，旨在测试大语言模型的逻辑搜索能力。结果显示当前模型表现接近随机基线。

Motivation: 评估大语言模型在逻辑搜索推理中的能力，填补现有研究专注于推理规则的不足。

Method: 通过自动生成SAT公式并将其转化为故事上下文和条件，构建2100个可调整难度的逻辑谜题，并通过多重验证确保质量。

Result: 最强模型o4-mini在难题上准确率仅65.0%，接近50%的随机基线。

Conclusion: SATBench揭示了当前大语言模型在搜索型逻辑推理中的根本局限性，为未来研究提供可扩展测试平台。

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [629] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari,Mirella Lapata*

Key words: 大语言模型, 辩论机制, 多模态监督, 视觉问答, 模型增强

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探讨了在多模态环境中扩展辩论机制，通过辩论框架使较弱模型监督和增强较强模型的性能，尤其在视觉问答任务中表现优异。

Motivation: 随着大语言模型（LLMs）在多样领域和多模态中展现出超越人类评估者的能力，如何实现可扩展的监管成为挑战，辩论机制为解决这一问题提供了可能。

Method: 研究在多模态背景下采用辩论机制，由两个视觉语言专家模型进行辩论，文本模型作为裁判裁决答案。专家仅支持符合其信念的答案，避免角色扮演，集中讨论分歧点。

Result: 实验表明，辩论框架在多模态任务中表现优于单个专家模型，且较弱的LLMs通过微调可以提升视觉语言模型的推理能力。

Conclusion: 辩论机制在多模态监督中具有潜力，能够有效提升模型性能，尤其适用于解决模型能力超越人类评估者时的监管问题。

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [630] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/abs/2505.14656)
*Zihao Zhang,Fei Liu*

Key words: 成本敏感规划, 蒙特卡洛树搜索, 大型语言模型, 预算约束, 任务成功率

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为CATS的成本增强蒙特卡洛树搜索方法，用于解决大型语言模型在成本敏感规划中的不足，显著提高了任务成功率和成本效率。

Motivation: 大型语言模型（LLMs）在开放推理中表现优异，但在成本敏感规划中往往表现不佳，无法有效处理严格的预算约束。

Method: 通过引入成本增强的蒙特卡洛树搜索（CATS），结合LLM的推理能力与结构化搜索，实现了对成本约束的显式关注。

Result: 实验表明，CATS在严格预算下表现优于GPT-4.1等顶级LLMs，任务成功率和成本效率更高。

Conclusion: CATS为预算敏感的决策提供了一种有效的解决方案，结合了LLMs的推理能力与结构化搜索的优势。

Abstract: While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [631] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung,Sangyeon Yoon,Minsuk Kahng,Albert No*

Key words: SAFEPATH, 大型推理模型, 安全对齐, 有害输出, 越狱攻击

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SAFEPATH是一种轻量级对齐方法，通过在推理开始时生成安全提示来减少有害输出，同时保持推理性能。

Motivation: 大型推理模型（LRMs）在复杂问题解决中表现强大，但其结构化推理路径在面对有害提示时可能导致不安全输出，现有安全对齐方法存在性能下降和攻击漏洞问题。

Method: SAFEPATH通过微调LRMs，使其在面对有害提示时生成简短的安全提示（8个token），其余推理过程不受监督。

Result: SAFEPATH在多个基准测试中显著减少有害输出（最多90.0%），并阻止83.3%的越狱攻击，计算成本远低于其他方法。

Conclusion: SAFEPATH是一种高效的安全对齐方法，同时揭示了当前方法在推理模型中的不足，为更安全的AI提供了新方向。

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [632] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Key words: LLM, 上下文感知, 主动代理, 可穿戴设备, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了ContextAgent，一种利用多维度感知环境增强LLM代理主动能力的上下文感知代理，并创建了首个上下文感知主动LLM代理基准。

Motivation: 现有主动代理要么依赖封闭环境的观察，要么使用基于规则的主动通知，导致用户意图理解不足，功能受限。

Method: ContextAgent从可穿戴设备的多维感知数据中提取上下文，结合历史数据预测主动服务需求，并自动调用工具。

Result: 在ContextAgentBench上，ContextAgent在主动预测和工具调用上的准确率分别提高了8.5%和6.0%。

Conclusion: 研究为开发更先进、以人为中心的主动AI助手提供了启示。

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [633] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang,Xingyu Chen,Yue Wang,Zhiwei He,Jiahao Xu,Tian Liang,Qiuzhi Liu,Yunzhi Yao,Wenxuan Wang,Ruotian Ma,Haitao Mi,Ningyu Zhang,Zhaopeng Tu,Xiaolong Li,Dong Yu*

Key words: Mixture-of-Experts (MoE), Large Reasoning Models (LRMs), RICE, nPMI, cognitive efficiency

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了一种名为RICE的新方法，通过强化认知专家提升大型推理模型的性能，无需额外训练或复杂启发式。

Motivation: 现有的推理模型常因认知效率问题（如过度思考或思考不足）表现不佳，RICE旨在解决这一局限。

Method: 利用nPMI识别并强化特定的认知专家，优化推理过程。

Result: 在DeepSeek-R1和Qwen3-235B等模型上，RICE显著提升了准确性、认知效率和跨域泛化能力。

Conclusion: RICE是一种轻量且可解释的方法，能有效提升高级推理模型的认知效率。

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


### [634] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/abs/2505.13466)
*Vu Dinh Xuan,Hao Vo,David Murphy,Hoang D. Nguyen*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种新型多智能体框架，通过LLM驱动的评估器和编辑器智能体协作生成安全关键场景的合成数据，解决数据稀缺问题。

Motivation: 安全关键应用中数据稀缺问题严重，现有方法缺乏语义深度，无法满足训练AI系统的需求。

Method: 采用多智能体框架，包括基于LLM的评估器（确保语义一致性和安全性）和编辑器（生成和优化场景），通过迭代协作生成合成图像。

Result: 实验表明，该框架能根据实际规范生成有用场景，平衡了安全需求和视觉语义，改进了现有方法的不足。

Conclusion: 该迭代设计有望为多媒体安全应用的数据稀缺问题提供解决方案，生成稳健且美观的模拟场景。

Abstract: The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [635] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch,Sebastian Eilermann,Alexander Windmann,Alexander Diedrich,Philipp Rosenthal,Oliver Niggemann*

Key words: 大语言模型,工程评估,真实场景,抽象推理,工程逻辑

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一个针对复杂工程问题的LLMs评估方法，发现了LLMs在基础推理上的优势及在抽象推理和工程逻辑上的不足。

Motivation: 现有对LLMs在工程领域的评估过于简化且缺乏真实场景，导致对其在复杂工程问题中能力的了解不足。

Method: 通过构建包含100多个真实工程场景问题的数据库，系统评估了四种先进LLMs的性能。

Result: LLMs在基础时空推理上表现良好，但在抽象推理、形式化建模和上下文敏感工程逻辑上明显不足。

Conclusion: LLMs在复杂工程任务中仍有局限性，需进一步改进。

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [636] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han,Wang Lin,Liya Hu,Zhenlong Dai,Yiyun Zhou,Mengze Li,Zemin Liu,Chang Yao,Jingyuan Chen*

Key words: 知识追踪,跨课程学习,概念图,LLM,GCN,对比学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TransKT模型通过跨课程概念图和对比学习目标提升知识追踪能力，利用LLM建立概念间的隐式链接，并通过GCN进行知识迁移。

Motivation: 现有知识追踪模型局限于单一课程数据，无法全面捕获学习者的知识状态。

Method: 构建跨课程概念图，利用零-shot LLM提示建立概念间链接，采用LLM-to-LM管道增强GCN性能，并通过对比学习目标对齐课程间知识状态。

Result: TransKT显著提升了知识状态估计的鲁棒性和准确性。

Conclusion: TransKT通过跨课程知识迁移和对比学习有效提升了知识追踪模型的性能。

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [637] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/abs/2505.13496)
*Przemek Pospieszny,Wojciech Mormul,Karolina Szyndler,Sanjeev Kumar*

Key words: 异常检测,日志分析,Transformer,自适应阈值,无监督学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了ADALog框架，一种无需人工标注的自适应异常检测方法，适用于异构日志数据，基于Transformer模型提取上下文关系，动态调整阈值。

Motivation: 现代软件系统生成大量异构日志数据，传统方法依赖日志解析或标注数据，难以应对动态格式和碎片化事件序列的挑战。

Method: 使用预训练双向Transformer编码器，通过掩码语言建模任务提取日志上下文关系，基于正常日志微调，并利用自适应百分位数阈值检测异常。

Result: 在BGL、Thunderbird和Spirit基准数据集上表现优异，泛化能力强，优于现有监督和无监督方法。

Conclusion: ADALog能动态适应系统行为变化，避免传统方法的启发式阈值限制，适用于实际环境。

Abstract: Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [638] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever,Forrest McKee*

Key words: 大型语言模型, 自由职业, 自动化基准测试, 软件开发

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究评估大型语言模型（LLMs）作为自由软件开发任务的自主代理，并开发了新的自动化基准测试。结果表明Claude 3.5 Haiku表现最佳，赚取152万美元。

Motivation: 探索LLMs在真实世界任务（如自由职业软件开发）中的潜力，并简化评估过程。

Method: 使用Kaggle自由职业者数据集的合成任务构建基准测试，标准化价格并自动化测试。评估了四个现代LLMs的性能。

Result: Claude 3.5 Haiku表现最佳（152万美元），其次是GPT-4o-mini（149万美元）。模型在结构化任务中表现良好，但真实任务复杂性仍有差距。

Conclusion: LLMs在自由职业开发中具有潜力，自动化基准测试方法可扩展，但仍需解决真实任务的复杂性。

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [639] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/abs/2505.13522)
*Nathalie Sanghikian,Rafael Meirelles,Rafael Martinelli,Anand Subramanian*

Key words: 海上库存路径问题, MIRP, 启发式方法, Beam Search, 迭代局部搜索

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种不依赖数学优化技术的启发式方法来解决确定性、有限期限、单一产品的海上库存路径问题（MIRP），结合Beam Search算法和局部搜索过程，改进了部分实例的最优解。

Motivation: 当前缺乏高效解决大型MIRP实例的方法，且现有精确方法计算成本高，启发式方法较少应用。研究旨在鼓励使用MIRPLib并提供高效解决方案。

Method: 提出了一种结合Beam Search算法变体和迭代局部搜索过程的启发式方法。

Result: 在72个测试实例中，该方法在可接受的计算时间内改进了其中10个实例的最优解。

Conclusion: 该启发式方法为MIRP提供了一种高效的解决方案，且结果具有竞争力。

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [640] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang,Jinzhe Tu,Haoran Liu,Xiaoce Wang,Chujie Zheng,Zhexin Zhang,Shiyao Cui,Caishun Chen,Tiantian He,Hongning Wang,Yew-Soon Ong,Minlie Huang*

Key words: 大推理模型，过度自信，病态推理，BARREL框架，可靠性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文针对大推理模型（LRMs）在推理过程中过度自信且错误回答的问题，提出了BARREL框架，通过减少过度思考和加强边界意识，显著提高了模型的可靠性。

Motivation: 当前的大推理模型（LRMs）在数学和逻辑推理中表现出色，但缺乏对未知问题的承认能力，导致过度自信和错误回答。论文旨在解决这一问题。

Method: 作者提出BARREL框架，通过识别并解决两种病态推理模式——最后猜测和二次思考螺旋——来优化推理过程。

Result: 实验显示，BARREL训练将DeepSeek-R1-Distill-Llama-8B模型的可靠性从39.33%提升至61.48%，同时保持了与基准模型相当的准确性。

Conclusion: BARREL框架为构建更可靠、更具事实性的大推理模型提供了有效的初步研究。

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [641] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/abs/2505.13533)
*Junzhe Jiang,Chang Yang,Aixin Cui,Sihan Jin,Ruiyu Wang,Bo Li,Xiao Huang,Dongning Sun,Xinrun Wang*

Key words: 金融基准测试、大语言模型、FinMaster、金融推理、评估框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了FinMaster，一个全面的金融基准测试，旨在评估大语言模型（LLMs）在金融领域的表现，填补了当前金融任务评估中的空白。

Motivation: 金融任务对全球经济稳定至关重要，但目前存在劳动密集型、低容错性等问题。LLMs在自然语言处理中表现出色，但现有金融领域评估基准缺乏领域特定数据、任务设计简单且评估框架不完整。

Method: FinMaster包含三个模块：FinSim（生成合成金融数据）、FinSuite（提供183个任务）、FinEval（统一评估接口）。通过实验评估LLMs在金融推理中的能力。

Result: 实验显示LLMs在复杂金融推理任务中表现显著下降，准确率从90%降至40%，揭示出计算误差传播问题。

Conclusion: FinMaster是首个覆盖全流程金融工作流的基准测试，有望推动LLMs在金融实践中的应用，提升效率和准确性。

Abstract: Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [642] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/abs/2505.13546)
*Ke Chen,Yufei Zhou,Xitong Zhang,Haohan Wang*

Key words: 自动提示生成,语义稳定性,LLaMA,多智能体系统

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于语义稳定性的自动提示生成方法，关注提示的稳定性而非仅任务表现，通过LLaMA评估器量化稳定性，提升了提示生成系统的可靠性和性能。

Motivation: 现有提示生成方法仅关注任务表现，忽略了提示的内在稳定性，导致可靠性和可解释性不足。本文旨在通过量化稳定性，提升提示生成的鲁棒性。

Method: 提出语义稳定性作为评估标准，并微调LLaMA评估器自动化测量；开发了首个稳定性感知的通用提示生成系统，利用稳定性反馈迭代优化提示质量和系统性能。

Result: 实验证明，该框架在通用和领域特定任务中均提升了准确性和输出一致性，验证了稳定性对系统级执行的重要性。

Conclusion: 通过关注持久可靠性而非一次性结果，本文为构建更可信的通用系统提供了新视角和实用工具。

Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [643] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/abs/2505.13551)
*Serge Dolgikh*

Key words: 反推断行为,认知系统,适应性,认知僵化,设计原则

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨了自然和人工认知系统中反推断行为的出现，分析了其产生场景及机制，并提出了避免认知僵化的设计原则。

Motivation: 探讨认知系统中导致僵化或适应性下降的反推断行为机制。

Method: 通过分析强化稳定性、元认知归因和保护性重构等场景，结合人工和生物系统证据进行研究。

Result: 反推断行为是一种普遍的认知弱点，揭示了稳定条件下保持适应性激活的重要性。

Conclusion: 提出了能够抵抗信息压力下认知僵化的认知架构设计原则。

Abstract: This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [644] [Language and Thought: The View from LLMs](https://arxiv.org/abs/2505.13561)
*Daniel Rothschild*

Key words: 语言与思维, 大型语言模型, 推理能力, AI, Dennett

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文探讨了语言对思维的影响，通过大型语言模型（LLMs）的推理能力支持了Dennett的观点，即语言显著改变了思维的运作方式。

Motivation: 研究动机是验证Daniel Dennett关于语言对思维影响的激进观点，尤其是通过对比有无语言训练的AI系统的表现。

Method: 方法包括分析大型语言模型（LLMs）在跨领域推理中的表现，探讨语言编码的抽象性和效率如何影响推理能力。

Result: 研究结果表明，语言训练显著提升了AI系统的推理能力，支持语言是推理计算可行的关键因素。

Conclusion: 结论认为语言对人类生物思维的运作也具有类似的重要作用，通过提高推理的抽象性和效率。

Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [645] [MAFA: A multi-agent framework for annotation](https://arxiv.org/abs/2505.13668)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Key words: FAQ标注, 多智能体框架, 结构化推理, 少样本学习, 检索优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种多智能体框架，用于FAQ标注，结合了多种专业智能体和重排序机制，显著提升了检索准确率。

Motivation: 传统FAQ标注方法依赖单一模型，难以捕捉用户查询的多样性。

Method: 采用多智能体框架，每个智能体使用不同的方法和结构化推理（ARQs），并通过少样本策略增强多样性。

Result: 在银行数据集和公共基准测试中，Top-1准确率提升14%，Top-5准确率提升18%，MRR提高12%。

Conclusion: 该框架能有效处理模糊查询，具备跨领域和语言的泛化能力，适合实际应用部署。

Abstract: Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [646] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/abs/2505.13672)
*Giannis Chatziveroglou*

Key words: A*-decoding, 推理优化, 语言模型, 计算预算, 结构化搜索

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: A*-decoding是一种基于搜索的推理策略，通过优先考虑高质量推理路径优化计算资源，显著提升语言模型性能。

Motivation: 现有方法在固定计算预算下表现良好，但未充分利用推理时的计算资源，因此需要更高效的策略。

Method: 将语言模型解码视为部分解空间的结构化搜索，应用A*搜索算法，并通过外部监督信号指导生成。

Result: A*-decoding在相同计算预算下，比基线方法少用3倍标记和30%PRM，使小模型达到大模型性能。

Conclusion: 结构化搜索为解码提供高效替代方案，未来可推动语言模型更高效部署。

Abstract: Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [647] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696)
*Zizhan He,Maxime Daigle,Pouya Bashivan*

Key words: 神经网络, 空间模型, 片段记忆, 样本效率, 自适应

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文探讨了神经网络是否能够从稀疏且不连贯的片段记忆中构建空间环境模型，并提出了名为Episodic Spatial World Model (ESWM)的新框架。实验表明，ESWM具有高效样本利用率和自适应能力，能够快速适应环境变化，并支持探索和导航策略。

Motivation: 研究动机在于理解动物如何通过稀疏的片段记忆快速构建灵活的环境心理模型，并探索神经网络是否具备类似能力。

Method: 研究方法包括在模拟世界中提出ESWM框架，测试其从稀疏记忆中构建空间模型的效率和适应性。

Result: 结果表明，ESWM能够高效利用少量观察数据构建环境模型，快速适应环境变化，并支持探索和导航任务。

Conclusion: 结论是ESWM为神经网络学习空间模型提供了一种高效且自适应的解决方案，无需额外训练即可完成任务。

Abstract: Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [648] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha,Minwu Kim,Aadim Nepal,Anubhav Shrestha,Keith Ross*

Key words: LLM, reasoning, RLVR, CoT, sample efficiency

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种在监督数据有限的情况下高效训练推理型LLM的两阶段策略，通过预热阶段和RLVR训练提升模型表现和泛化能力。

Motivation: 解决训练推理型LLM时对大量高质量数据的依赖问题，特别是在数据稀缺的情况下。

Method: 两阶段训练策略：1) 使用Knights & Knaves逻辑谜题的CoT预热模型；2) 在预热模型上进行RLVR训练。

Result: 预热阶段提升泛化推理能力，预热后模型在小数据集上表现更优，且保持跨领域泛化能力。

Conclusion: 预热策略能有效提升数据稀缺环境下推理型LLM的鲁棒性和样本效率。

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [649] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
*Andrew Nam,Henry Conklin,Yukang Yang,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Key words: transformer, interpretability, attention heads, causal taxonomy, CHG

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 介绍了因果头门控（CHG），一种可扩展的方法，用于解释transformer模型中注意力头的功能作用，并提供因果分类。

Motivation: 通过CHG方法改进对transformer模型中注意力头功能的理解，避免传统假设驱动方法的局限性。

Method: CHG通过学习软门控对注意力头进行分类（促进、干扰或无关），并通过下一个令牌预测直接应用于任何数据集。

Result: 在多任务和Llama 3模型家族中验证CHG的因果性，并发现LLMs具有稀疏但足够的子电路。

Conclusion: CHG提供因果性洞察，揭示注意力头非模块化特性及任务机制的分离性。

Abstract: We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [650] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An,Hua-Dong Xiong,Robert C. Wilson,Marcelo G. Mattar,Marcus K. Benna*

Key words: 大型语言模型, 元认知, 神经反馈, AI安全, 神经激活

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）的元认知能力，即它们监控和报告内部神经激活模式的能力。通过神经反馈范式，发现LLMs能够学习和控制这些激活，但其能力受多种因素影响。结果表明LLMs的元认知能力有限，仅能监控部分神经机制。

Motivation: 随着大型语言模型在社会中的广泛应用，理解其元认知能力的局限性变得尤为重要，尤其是它们监控内部神经激活的能力，以避免潜在的安全风险。

Method: 提出了一种基于神经科学的神经反馈范式，通过向模型提供句子-标签对（标签对应句子引起的特定神经激活方向），量化LLMs报告和控制这些激活模式的能力。

Result: LLMs能够学习和控制神经激活，但其表现受示例对数量、目标神经方向的语义可解释性和方差解释力等因素影响。元认知空间的维度远低于模型的神经空间维度，表明LLMs仅能监控部分神经机制。

Conclusion: 研究提供了关于LLMs元认知能力的实证证据，为AI安全性提供了重要启示。

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [651] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du,Li Chen,Xun Xian,An Luo,Fangqiao Tian,Ganghua Wang,Charles Doss,Xiaotong Shen,Jie Ding*

Key words: 因果推理, 大型语言模型, 评估基准, 统计陷阱

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了CausalPitfalls基准测试，用于评估大型语言模型在因果推理中的能力，揭示了当前模型的显著局限性。

Motivation: 因果推理在医学、经济学和公共政策等高风险领域至关重要，但现有基准测试通常过于简化，无法全面评估大型语言模型的能力。

Method: 设计CausalPitfalls基准，包含多难度级别的结构化挑战和评分标准，采用直接提示和代码辅助提示两种评估协议，并与人类专家评分对比验证。

Result: 当前大型语言模型在统计因果推理中存在显著局限性。

Conclusion: CausalPitfalls为开发可信赖的因果推理系统提供了重要指导和量化指标。

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [652] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/abs/2505.13773)
*Ryan Bowers,Richard Agbeyibor,Jack Kolb,Karen Feigh*

Key words: 人机协作、AI熟悉化、ISR环境、风险偏好

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 比较了三种人类熟悉AI队友的方法，发现文档熟悉最快但偏保守，直接互动更有探索性且风险容忍更高，建议结合文档、结构化训练和探索性互动。

Motivation: 研究如何在快节奏ISR环境中让人快速熟悉AI队友，以提高协作效率。

Method: 分为三组（阅读文档、直接训练、无熟悉）进行用户研究，比较策略形成速度和风险偏好。

Result: 文档熟悉快速但保守，直接互动更愿意尝试但理解较浅，个体风险容忍差异显著。

Conclusion: 推荐结合文档、结构化训练和探索性互动的方法。

Abstract: We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [653] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/abs/2505.13774)
*Zidi Xiong,Chen Shan,Zhenting Qi,Himabindu Lakkaraju*

Key words: 大型推理模型,反事实干预,忠实性评估,多路径思维链

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种系统性反事实干预框架，用于严格评估大型推理模型（LRMs）的思维草稿忠实性。

Motivation: 为确保LRMs中间推理过程的忠实性，以便实现可靠的监控、解释和有效控制。

Method: 反事实干预框架，包括对单个推理步骤（Intra-Draft Faithfulness）和最终答案与思维草稿的一致性（Draft-to-Answer Faithfulness）的评估。

Result: 实验表明当前LRMs对中间推理步骤的选择性忠实性较高，但经常无法忠实对齐草稿结论。

Conclusion: 高级LRMs需要更忠实和可解释的推理。

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [654] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/abs/2505.13778)
*Guoheng Sun,Ziyao Wang,Bowei Tian,Meng Liu,Zheyu Shen,Shwai He,Yexiao He,Wanghao Ye,Yiting Wang,Ang Li*

Key words: 大语言模型，推理令牌，计费透明度，验证框架，哈希树

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了CoIn框架，用于验证大语言模型（LLM）服务中隐藏推理令牌的数量和语义有效性，以解决计费不透明的问题。

Motivation: 由于商业LLM API通常隐藏推理令牌，用户无法验证令牌的真实性，可能导致计费欺诈。本文旨在恢复计费透明度。

Method: CoIn通过构建可验证的哈希树检查令牌数量，并利用基于嵌入的相关性匹配检测伪造的推理内容。

Result: 实验表明，CoIn作为第三方审计工具，能成功检测令牌计数膨胀，成功率高达94.7%。

Conclusion: CoIn能有效提升LLM服务的计费透明度，防止欺诈行为。

Abstract: As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [655] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/abs/2505.13794)
*Qi Cheng,Licheng Liu,Qing Zhu,Runlong Yu,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Key words: 生态时间序列, 度量学习, 大型语言模型, 评估框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出一种结合度量学习与大型语言模型的新框架，用于生态时间序列评估，弥补传统数值指标与专家知识之间的差距。

Motivation: 传统数值指标难以捕捉生态过程中的关键时间模式，需依赖专家视觉检查，效率低且不适用于大规模评估。

Method: 整合度量学习与LLM的自然语言策略提取，通过策略优化机制生成并组合不同评估指标。

Result: 在作物总初级生产和二氧化碳通量预测的多个数据集上验证了方法的有效性。

Conclusion: 该框架提供可解释的评估策略，满足不同生态建模研究的多样化需求。

Abstract: Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [656] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/abs/2505.13828)
*Kiarash Naghavi Khanghah,Zhiling Chen,Lela Romeo,Qian Yang,Rajiv Malhotra,Farhad Imani,Hongyi Xu*

Key words: 增材制造, 检索增强生成, 多模态, 零样本学习, 异常检测

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种基于检索增强生成的多模态框架，用于零样本检测增材制造中的异常，通过文献检索提升准确性。

Motivation: 增材制造中缺陷和过程异常的自动化检测是一个挑战，需避免依赖训练数据集。

Method: 结合文本和图像检索以及多模态生成模型，实现零样本异常识别与分类。

Result: 框架在多种L-PBF数据集上验证，GPT-4o-mini表现优于Qwen2-VL-2B，检索机制提升12%准确率。

Conclusion: 框架可动态更新，适应增材制造技术发展，提高检测效率和准确性。

Abstract: Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [657] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/abs/2505.13831)
*Zongyuan Deng,Yujie Cai,Qing Liu,Shiyao Mu,Bin Lyu,Zhen Yang*

Key words: 5G网络规划、基站选址、AI驱动、强化学习、多目标优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: TelePlanNet是一个针对5G基站选址的AI驱动框架，通过三层架构和强化学习优化选址，显著提高了规划一致性。

Motivation: 传统人工方法和现有AI工具在5G基站选址中存在效率低和多目标优化不足的问题。

Method: 结合大型语言模型实时处理用户输入和改进的GRPO强化学习进行多目标优化。

Result: 实验显示TelePlanNet将规划一致性提高到78%，优于人工方法。

Conclusion: TelePlanNet为运营商提供了高效且可扩展的基站选址工具。

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [658] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/abs/2505.13851)
*Sahil Shah,Harsh Goel,Sai Shankar Narasimhan,Minkyu Choi,S P Sharan,Oguzhan Akcin,Sandeep Chinchali*

Key words: 视频理解, 时间推理, 神经符号方法, 智能视频代理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文呼吁开发新一代智能视频代理，需具备自主视频搜索分析、实时交互和高级内容生成能力，以解决现有模型在时间推理上的不足。

Motivation: 现代视频理解系统在场景分类、目标检测等任务上表现优秀，但在事件序列理解和时间推理上存在局限，阻碍了智能视频代理的发展。

Method: 提出结合神经符号方法的框架，将视频查询分解为原子事件，构建连贯序列并验证时间约束。

Result: 通过整合自主视频分析、实时交互和内容生成能力，推动视频理解系统的智能化和可信化。

Conclusion: 论文呼吁研究社区共同开发下一代智能视频代理，实现从被动感知到主动推理和行动的跨越。

Abstract: Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [659] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang*

Key words: 移动自动化,视频指导,知识获取,性能提升

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Mobile-Agent-V通过视频自动注入操作知识，提升移动自动化效率，性能提升36%。

Motivation: 移动设备使用激增需要高效的任务自动化，但现有AI框架因知识获取不足表现不佳。

Method: 利用视频作为指导工具，直接从视频内容中提取知识，避免手动干预。

Result: 实验显示Mobile-Agent-V比现有方法性能提升36%。

Conclusion: Mobile-Agent-V显著提升了移动自动化的效率和便捷性。

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [660] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He,Jiahe Jin,Pengfei Liu*

Key words: 轨迹数据,计算机代理,数据合成,性能提升

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: PC Agent-E框架通过少量高质量轨迹数据显著提升计算机使用代理性能

Motivation: 解决高质量轨迹数据稀缺问题，减少对人类演示的依赖

Method: 基于312条人工标注轨迹，利用Claude 3.7 Sonnet合成多样化动作决策

Result: 相对性能提升141%，在WindowsAgentArena-V2和OSWorld上表现优异

Conclusion: 少量高质量数据即可激发强大计算机使用能力

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [661] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/abs/2505.13914)
*Jake Chandler,Richard Booth*

Key words: 并行修正, 信念修正, TeamQueue聚合器, 迭代模型, 合理性假设

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种基于TeamQueue聚合器的方法，将串行迭代信念修正算子扩展为并行变化处理，从而统一解释现有的合理性假设。

Motivation: 尽管对单步并行修正的约束有了一定研究，但对迭代情况的扩展模型研究较少，且现有合理性假设缺乏统一解释。

Method: 利用TeamQueue聚合器家族，将串行迭代信念修正算子推广到并行变化处理。

Result: 该方法能够恢复文献中独立合理的性质，同时避免不可靠的性质。

Conclusion: 基于TeamQueue聚合器的方法为并行迭代信念修正提供了统一的框架，填补了研究空白。

Abstract: Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [662] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/abs/2505.13940)
*Kun Li,Zhennan Wu,Shoupeng Wang,Wenbin Hu*

Key words: AI4Science, LLM, 药物发现, 多模态数据, 参数化推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: DrugPilot是一个基于大语言模型（LLM）的智能代理，针对药物发现领域的多模态数据处理、知识更新延迟和预测结果可信度不足等问题，通过参数化推理架构和创新记忆池技术，显著提升了任务完成率和效率。

Motivation: 现有LLM在药物发现领域面临多模态数据处理、知识更新延迟和预测可信度不足等挑战，需要一种更高效的解决方案。

Method: 提出DrugPilot代理系统，采用参数化推理架构和交互式参数化记忆池技术，并构建药物指令数据集用于微调和评估。

Result: 在伯克利函数调用评估框架下，DrugPilot在简单、多重和多轮任务中的完成率分别达到98.0%、93.5%和64.0%，优于现有代理。

Conclusion: DrugPilot通过参数化推理和多模态数据标准化，显著提升了药物发现任务的自动化效率和准确性。

Abstract: In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [663] [Visual Instruction Bottleneck Tuning](https://arxiv.org/abs/2505.13946)
*Changdae Oh,Jiatong Li,Shawn Im,Yixuan Li*

Key words: 多模态大型语言模型, 分布偏移, 信息瓶颈, 鲁棒性, Vittle

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了Vittle方法，通过信息瓶颈原理增强多模态大型语言模型在分布偏移下的鲁棒性，无需额外数据或更大模型架构。

Motivation: 解决多模态大型语言模型在分布偏移下性能下降的问题，避免高成本的数据或模型扩展。

Method: 基于信息瓶颈原理设计了Vittle方法，通过最小充分表示学习提升鲁棒性。

Result: 在45个数据集（含30个偏移场景）上的实验表明，Vittle显著提升了模型的鲁棒性。

Conclusion: Vittle是一种低成本且有效的方法，可增强多模态大型语言模型在分布偏移下的性能。

Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [664] [Solving Normalized Cut Problem with Constrained Action Space](https://arxiv.org/abs/2505.13986)
*Qize Jiang,Linsey Pang,Alice Gatti,Mahima Aggarwa,Giovanna Vantin,Xiaosong Ma,Weiwei Sun,Sanjay Chawla*

Key words: 强化学习, 组合优化, 约束动作空间, 归一化切割, 交通网络

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种利用强化学习的约束动作空间方法，用于引导归一化切割问题向预定义的模板实例靠近，特别是在交通网络领域实现楔形和环形分区。

Motivation: 尽管强化学习在组合优化问题中有广泛应用，但如何整合外部知识以引导解决方案向领域合适的输出仍具挑战性。

Method: 采用约束动作空间的强化学习方法，结合Wedge和Ring Transformer，生成符合领域特征的分区。

Result: 实验证明，该方法在交通网络中能生成更接近自然最优分区的楔形和环形分区。

Conclusion: 该方法具有普适性，其原理可推广到其他领域。

Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve
combinatorial optimization problems primarily due to its ability to learn
heuristics that can generalize across problem instances. However, integrating
external knowledge that will steer combinatorial optimization problem solutions
towards domain appropriate outcomes remains an extremely challenging task. In
this paper, we propose the first RL solution that uses constrained action
spaces to guide the normalized cut problem towards pre-defined template
instances. Using transportation networks as an example domain, we create a
Wedge and Ring Transformer that results in graph partitions that are shaped in
form of Wedges and Rings and which are likely to be closer to natural optimal
partitions. However, our approach is general as it is based on principles that
can be generalized to other domains.

</details>


### [665] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/abs/2505.13994)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Key words: 检索增强生成,知识图谱,语义分割,多代理协作

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SPLIT-RAG框架通过语义分割与多代理协作，优化了检索增强生成系统的效率与准确性。

Motivation: 解决传统RAG系统在大规模知识图谱上效率与准确性难以权衡的问题。

Method: 提出SPLIT-RAG框架，通过语义分割、类型专业化知识库和多代理协作实现高效检索。

Result: 实验证明，SPLIT-RAG相比现有方法显著提升了性能。

Conclusion: SPLIT-RAG为大规模知识图谱的检索生成任务提供了高效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [666] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/abs/2505.14001)
*Sterre Lutz,Matthijs T. J. Spaan,Anna Lukina*

Key words: 自主系统；概率安全证书；神经Lyapunov认证；VeRecycle

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: VeRecycle是一种框架，能够在系统动态仅在某些状态子集内变化时，高效重用概率安全证书，避免完全重新认证的高成本。

Motivation: 现实中的自主系统面临多种不确定性，当动态超出建模范围时，传统方法需要完全重新认证，成本高昂。

Method: 提出VeRecycle框架，通过重用部分状态空间的概率证书，避免完全重新认证。

Result: 实验显示，VeRecycle显著节省计算资源，同时保持竞争性的概率保证。

Conclusion: VeRecycle为离散时间随机动态系统提供了一种高效的局部重新认证方法。

Abstract: Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [667] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2505.14020)
*Hao Dong,Ziyue Qiao,Zhiyuan Ning,Qi Hao,Yi Du,Pengyang Wang,Yuanchun Zhou*

Key words: 时序知识图谱,推理,多跨度进化,特征解耦

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了DiMNet，一种用于时序知识图谱推理的新方法，通过多跨度进化策略和特征解耦组件，显著提升了推理性能。

Motivation: 现有方法在建模子图语义演变时忽略了子图间的内部结构交互和平滑特征，限制了推理性能。

Method: 提出DiMNet，结合多跨度进化策略捕捉局部和历史邻居信息，并通过解耦组件区分节点的活跃和稳定特征。

Result: 在四个真实TKG数据集上，DiMNet比现有方法表现更优，MRR提升高达22.7%。

Conclusion: DiMNet通过结合多跨度语义和特征解耦，显著提升了TKG推理性能。

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [668] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng,Sijie Ji,Jiawei Sun,Renqi Chen,Wei Gao,Mani Srivastava*

Key words: 心理健康风险评估,大语言模型,客观行为数据,因果链推理,解释性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ProMind-LLM是一种结合主观心理记录和客观行为数据的创新方法，通过领域微调、自优化机制和因果链推理，提升心理健康风险评估的可靠性和解释性。

Motivation: 现有心理健康风险评估方法主要依赖主观文本记录，易受不确定性影响，导致预测不一致。需结合客观数据以提高可靠性。

Method: 提出ProMind-LLM，整合领域微调、自优化机制和改进因果链推理的流程，处理主观心理记录和客观行为数据。

Result: 在PMData和Globem数据集上表现优于通用大语言模型，验证了方法的有效性。

Conclusion: ProMind-LLM为可靠、可解释且可扩展的心理健康解决方案提供了新方向。

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [669] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/abs/2505.14072)
*Soroush Hashemifar,Sherry Sahebi*

Key words: 个性化学习,知识追踪,行为建模,多任务学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出KMaP模型，通过多任务方法个性化建模学生知识与行为，解决现有方法中个性化不足和上下文丢失问题。

Motivation: 现有学生知识追踪和行为建模方法存在个性化不足、非评估材料建模缺失以及对知识与行为交互关系的忽略。

Method: 采用状态化多任务方法KMaP，结合基于聚类的学生画像，实现个性化学生表示和学习资源偏好预测。

Result: 在两真实数据集上验证了学生群体间的行为差异及KMaP模型的有效性。

Conclusion: KMaP模型显著提升了个性化学习资源预测的准确性。

Abstract: Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [670] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/abs/2505.14137)
*Vojtěch Kůr,Vít Musil,Vojtěch Řehák*

Key words: 对抗性巡逻游戏、有限内存策略、迭代优化、防御策略

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种迭代优化内存分配的方法，解决了防御者在有限内存策略中手动分配内存的难题，提高了策略的实用性。

Motivation: 有限内存策略在对抗性巡逻游戏中表现优异，但其内存分配问题一直是一个难以解决的开放性问题，限制了策略的实际应用。

Method: 开发了一种通用方法，通过迭代调整内存分配来优化防御策略，支持与任何黑盒策略优化工具结合使用。

Result: 实验表明该方法具有较强的鲁棒性，能够有效解决多种巡逻模型的实例。

Conclusion: 该方法解决了有限内存策略中的关键难题，提升了策略的实用性和性能。

Abstract: Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [671] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/abs/2505.14140)
*Qianyue Hao,Sibo Li,Jian Yuan,Yong Li*

Key words: 大语言模型, 推理能力, 强化学习, 轻量级导航模型, 自适应逻辑结构

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: RLoT 通过强化学习训练轻量级导航模型，动态选择并组合逻辑块，显著提升了 LLM 的推理能力，性能远超现有技术。

Motivation: 现有的推理技术（如 Chain/Tree/Graph-of-Thoughts）虽然有效但缺乏任务适应性，限制了其在多样化任务中的应用。

Method: 提出 RLoT，训练一个轻量级导航模型，动态选择并组合五种基本逻辑块，生成任务特定的逻辑结构。

Result: 在多个推理基准测试中，RLoT 性能提升高达 13.4%，且导航模型参数少于 3K，能将 10B 级 LLM 提升至媲美 100B 级模型的水平。

Conclusion: RLoT 不仅显著提升了 LLM 的推理能力，还展示了强大的可迁移性，适用于未见过的 LLM 和任务。

Abstract: Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [672] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/abs/2505.14141)
*Fanglin Mo,Junzhe Chen,Haoxuan Zhu,Xuming Hu*

Key words: GUI代理, 任务规划, 有限状态机, 自然语言计划

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出SPlanner模块，通过扩展有限状态机（EFSMs）建模应用逻辑，分解用户指令为可执行的自然语言计划，显著提升移动GUI代理任务成功率。

Motivation: 解决移动GUI代理因缺乏对目标应用深度理解而导致的计划不准确问题。

Method: 利用EFSMs建模应用逻辑和配置，分解指令为功能序列，生成自然语言执行计划。

Result: 在AndroidWorld基准上，任务成功率提升28.8个百分点，达到63.8%。

Conclusion: SPlanner显著提升了GUI代理的任务规划和执行能力。

Abstract: Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [673] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/abs/2505.14143)
*Shuo Zhang,Jinsong Zhang,Zhejun Zhang,Lei Li*

Key words: 多任务学习,多模态情感分析,多模态情感识别,混合专家,低秩结构

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为MMoLRE的新型多任务学习方法，用于解决多模态情感分析（MSA）和多模态情感识别（MER）任务中的参数冲突问题，通过共享和任务特定专家的混合设计，并结合低秩结构优化计算成本。

Motivation: 多任务学习（MTL）可以高效利用任务间的知识迁移，但由于复杂的任务相关性，现有方法通常忽略参数冲突问题。本研究的动机是针对MSA和MER任务设计一种避免参数冲突的MTL方法。

Method: 提出Multimodal Mixture of Low-Rank Experts（MMoLRE）方法，结合共享和任务特定专家共同建模任务共性和特性，并通过低秩专家网络减少参数和计算开销。

Result: 在CMU-MOSI和CMU-MOSEI基准测试中，MMoLRE在MSA任务上达到最先进性能，在MER任务上表现出竞争力。

Conclusion: MMoLRE通过混合专家和低秩结构设计，有效避免了多任务学习中的参数冲突问题，同时提升了性能。

Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [674] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang,Xueqiang Xu,Jiacheng Lin,Jinfeng Xiao,Zifeng Wang,Jimeng Sun,Jiawei Han*

Key words: 检索增强生成、大型语言模型、强化学习、模型无关框架、QA基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一个轻量级、模型无关的框架s3，通过解耦搜索和生成，并使用'Gain Beyond RAG'奖励训练搜索器，显著提升了检索增强生成（RAG）系统的性能。

Motivation: 现有的检索增强生成方法要么忽略了搜索的下游效用，要么通过联合训练检索和生成限定了检索的实际效用和与冻结或专有模型的兼容性，亟需一种更轻量化和解耦的解决方案。

Method: 提出s3框架，将搜索器与生成器解耦，并使用'Gain Beyond RAG'奖励训练搜索器。

Result: s3仅需2400个训练样本，即优于使用超过70倍数据训练的基础模型，在六个通用QA和五个医学QA基准测试中表现更好。

Conclusion: s3框架有效提升了检索增强生成系统的性能，同时保持了轻量化和模型无关的特性。

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [675] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/abs/2505.14147)
*Xiong Jun Wu,Zhenduo Zhang,ZuJie Wen,Zhiqiang Zhang,Wang Ren,Lei Shi,Cai Chen,Deng Zhao,Dingnan Jin,Qing Cui,Jun Zhou*

Key words: 大型推理模型、强化学习、STEM、问题合成、可验证奖励

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SHARP是一种合成高质量对齐推理问题的方法，用于训练大型推理模型（LRMs），特别针对STEM领域，通过可验证的奖励强化学习（RLVR）提升模型性能。

Motivation: 现有的合成方法（如思维链提示）生成的训练数据过于简化或不可验证，限制了大型推理模型在复杂任务上的进步。

Method: SHARP采用自对齐原则（如研究生和奥林匹克级难度、逻辑一致性）和三阶段框架（对齐、实例化、推理），利用先进的LRM生成和验证STEM问题，并通过强化学习优化模型推理。

Result: 实验表明，SHARP在GPQA等基准测试中显著优于现有方法，提高了复杂推理的准确性，并接近专家水平。

Conclusion: SHARP通过高质量、可验证的问题合成和强化学习，显著提升了大型推理模型的推理能力。

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [676] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/abs/2505.14148)
*Fan Liu,Zherui Yang,Cancheng Liu,Tianrui Song,Xiaofeng Gao,Hao Liu*

Key words: 数学模型构建、LLM、MM-Bench、MM-Agent

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一个基于LLM的数学模型构建任务，并引入MM-Bench基准和MM-Agent框架，显著提升了建模效果。

Motivation: 现有LLM在数学模型构建方面表现不足，限制了其在现实问题解决中的应用。

Method: 提出了MM-Agent框架，将建模分解为四个阶段：问题分析、模型构建、计算求解和报告生成。

Result: MM-Agent在MM-Bench上表现优于基线，比人类专家解决方案提升11.88%，并助力两支本科生团队获得MCM/ICM 2025的Finalist Award。

Conclusion: MM-Agent证明了其在数学模型构建中的实际有效性，可作为建模助手。

Abstract: Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [677] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/abs/2505.14163)
*He Wang,Alexander Hanbo Li,Yiqun Hu,Sheng Zhang,Hideo Kobayashi,Jiani Zhang,Henry Zhu,Chung-Wei Hang,Patrick Ng*

Key words: 大语言模型, 数据科学, 课程学习, 推理优化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为DSMentor的推理优化框架，通过课程学习策略逐步解决数据科学任务，提升了LLM代理的性能。

Motivation: 研究发现现有研究忽视了问题解决顺序的重要性，提出通过课程学习优化LLM代理在复杂任务中的表现。

Method: DSMentor框架将任务按难度排序，并结合长期记忆机制，逐步引导LLM代理学习。

Result: 实验表明，DSMentor在DSEval和QRData基准上提升了5.2%的通过率，并在因果推理任务中优于GPT-4。

Conclusion: 研究强调了知识积累与利用在推理中的重要性，为LLM性能优化提供了新方向。

Abstract: Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [678] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/abs/2505.14193)
*Abdallah Abuaisha,Bojie Shen,Daniel Harabor,Peter Stuckey,Mark Wallace*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了公共交通延误问题，提出动态重新规划框架，比较了拉式和推式方法，证明推式方法更高效。

Motivation: 公共交通工具的延误问题常见且严重影响用户体验，现有解决方案未能充分利用实时数据。

Method: 提出两种动态重新规划方案：用户手动请求的“拉式”方法和服务器主动监控的“推式”方法。

Result: 实验表明推式方法优于拉式方法，显著提升了效率并节省了到达时间。

Conclusion: 动态重新规划能有效改善公共交通延误问题，推式方法更具优势。

Abstract: Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [679] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/abs/2505.14209)
*Li Wang,Xin Yu,Xuxin Lv,Gangzheng Ai,Wenjun Wu*

Key words: 边界防御,无人机,三维场景,EMFAC,异构控制

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文研究大规模异构三维边界防御游戏，提出EMFAC框架解决防御策略中的控制挑战，并通过实验验证其有效性。

Motivation: 现有研究多局限于小规模简化场景，忽略现实环境扰动和动态因素，无法满足实际需求，因此研究更复杂的三维异构防御游戏具有重要意义。

Method: 引入Embedded Mean-Field Actor-Critic (EMFAC)框架，结合表征学习和轻量级注意力机制，优化防御者的协同决策。

Result: EMFAC在仿真和实际实验中表现出色，优于基线方法，显著提升了收敛速度和性能。

Conclusion: EMFAC框架可有效解决大规模异构防御问题，具有实际应用潜力。

Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [680] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim,Anubhav Shrestha,Safal Shrestha,Aadim Nepal,Keith Ross*

Key words: reinforcement learning with verifiable rewards, distillation, reasoning behavior, language models

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究探讨了RLVR和蒸馏对语言模型推理行为的影响，发现RLVR提升了简单问题的准确性但牺牲了困难问题，而蒸馏通过引入新知识提升能力。

Motivation: 研究旨在揭示RLVR和蒸馏方法在提升语言模型推理能力中的机制及其局限性。

Method: 通过实验分析RLVR和蒸馏对问题难度分布的影响，及响应质量和知识引入的作用。

Result: RLVR未能提升能力因其牺牲困难问题；蒸馏通过新知识提升能力，但仅学习推理模式时效果与RLVR类似。

Conclusion: RLVR和蒸馏的影响取决于问题难度分布和知识引入，需更可靠的响应质量指标。

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [681] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/abs/2505.14235)
*Yequan Wang,Aixin Sun*

Key words: Artificial General Intelligence, Embodied AI, Robotics, Taxonomy, Conceptual Framework

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一个关于具身AGI的五级系统分类法（L1-L5），回顾了基础阶段（L1-L2）的研究与挑战，并提出了实现更高级能力（L3-L5）的关键组件。在此基础上，作者提出了一个L3+机器人大脑的概念框架。

Motivation: 随着机器人和基础AI模型的进步，具身AGI的研究变得日益重要，但缺乏系统性的分类框架。本文旨在填补这一空白，提供一个清晰的分类法并展望未来方向。

Method: 作者通过系统性分类法将具身AGI分为五个级别（L1-L5），并基于现有技术提出了L3+机器人大脑的概念框架。

Result: 提出了一个具身AGI的五级分类法，并初步设计了L3+机器人大脑的框架，为未来研究奠定了基础。

Conclusion: 本文通过系统分类法和概念框架为具身AGI的研究提供了新的思路和方向。

Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [682] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/abs/2505.14289)
*Yijie Lu,Tianjie Ju,Manman Zhao,Xinbei Ma,Yuan Guo,ZhuoSheng Zhang*

Key words: 间接提示注入, 多模态代理, 红队测试, GUI, 注意力分布

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出EVA框架，用于动态优化间接提示注入攻击，通过监控代理的视觉注意力分布，提高攻击成功率。

Motivation: 随着多模态代理在图形用户界面（GUI）中的应用增加，间接提示注入攻击的威胁日益显著，需要开发更有效的防御和测试工具。

Method: 提出EVA框架，通过闭环优化动态调整攻击提示（如关键词、布局等），以适应代理的注意力分布变化。

Result: 实验表明，EVA在六种GUI代理中显著提升攻击成功率，且攻击模式在不同模型间具有良好迁移性。

Conclusion: EVA不仅是红队测试的有效工具，还能揭示多模态决策中的共同漏洞。

Abstract: As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [683] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary,Fazl Barez*

Key words: 大语言模型, 实时监控, 后门触发, 无监督学习, 安全检测

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种实时监控框架Safety-Net，通过无监督方法检测大语言模型的有害输出，重点研究后门触发响应，准确率达到96%。

Motivation: 为了解决大语言模型在生成有害内容（如暴力、色情或仇恨言论）时的安全风险，尤其是在后门触发情况下的隐藏漏洞。

Method: 采用无监督学习方法，将正常行为作为基线，有害输出视为异常。基于人类欺骗行为的类比，研究模型内部的异常行为特征。

Result: Safety-Net框架在多探测器设计下，能够成功检测有害行为，准确率达到96%。

Conclusion: 该方法有效解决了监控系统中的因果关系识别和防范高级模型欺骗的挑战。

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [684] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/abs/2505.14366)
*Joel Currie,Gioele Migno,Enrico Piacenti,Maria Elena Giannaccini,Patric Bach,Davide De Tommaso,Agnieszka Wykowska*

Key words: Vision-Language Models, Visual Perspective Taking, Human-Robot Interaction, synthetic dataset, spatial reasoning

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一个用于训练视觉语言模型（VLMs）进行视觉透视（VPT）的概念框架，并引入一个合成数据集以支持空间推理任务的监督学习。

Motivation: VPT是体现认知的核心能力，对人机交互（HRI）至关重要。希望通过训练VLMs实现这一能力。

Method: 使用NVIDIA Omniverse生成合成数据集，包含RGB图像、自然语言描述和物体姿态的4X4变换矩阵，专注于Z轴距离推理。

Result: 提出了一个公开可用的数据集，为未来扩展至6自由度推理奠定了基础。

Conclusion: 这是朝着实现能够进行空间理解的体现AI系统的重要一步。

Abstract: We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [685] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.14381)
*Yuyang Dong,Nobuhiro Ueda,Krisztián Boros,Daiki Ito,Takuya Sera,Masafumi Oyamada*

Key words: SCAN, VLM, RAG, 文档布局分析, 视觉语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为SCAN的新方法，通过增强视觉丰富文档的语义和布局分析，提升了文本和视觉RAG系统的性能。

Motivation: 随着大型语言模型（LLMs）和视觉语言模型（VLMs）的普及，处理视觉丰富文档的需求增加，但现有技术仍面临信息过多和效率低的挑战。

Method: SCAN采用粗粒度语义方法，将文档划分为连贯的区域，并通过精细调整的目标检测模型进行训练。

Result: 实验表明，SCAN在英文和日文数据集上分别提升了9.0%的文本RAG性能和6.4%的视觉RAG性能。

Conclusion: SCAN方法在效率和性能上优于传统方法和商业文档处理方案。

Abstract: With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [686] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/abs/2505.14391)
*Zhaohui Yang,Chenghua He,Xiaowen Shi,Linjing Li,Qiyue Yin,Shihong Deng,Daxin Jiang*

Key words: 数据标注, PRM, 长逻辑推理, 自我纠正, 错误传播

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出一种新的数据标注方法，针对长逻辑推理过程（CoT）的PRM训练，解决了现有方法仅关注第一个错误步骤的问题，通过引入错误传播与错误终止概念，提升了PRM的性能。

Motivation: 现有数据标注方法在长逻辑推理过程中仅关注第一个错误步骤及之前的部分，忽略了后续可能存在的自我纠正机制。本文旨在解决这一问题。

Method: 提出一种针对长CoT的数据标注方法，引入错误传播和错误终止概念，利用LLM标注器收集170万数据样本训练7B PRM。

Result: 实验显示，相比开源PRM和数据集训练的PRM，本文方法在搜索指导、BoN和F1分数等指标上表现更优，且数据效率和性能均优于常用的MC标注方法。

Conclusion: 本文方法在长逻辑推理过程中表现稳定且泛化性强，显著提升了PRM的性能。

Abstract: Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [687] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/abs/2505.14394)
*Mihir Athale,Vishal Vaddina*

Key words: 代码生成, 知识图谱, 代码搜索, LLMs, 上下文感知

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱的方法，改进代码搜索和检索，从而提升代码生成质量，尤其在存储库级别任务中表现优异。

Motivation: 尽管大语言模型（LLMs）在代码生成方面表现出色，但在动态代码库中的上下文准确性仍存在不足。现有代码搜索和检索方法在质量和上下文相关性上缺乏鲁棒性。

Method: 通过将代码存储库表示为知识图谱，捕捉结构和关系信息，结合混合检索方法，提升上下文感知的代码生成能力。

Result: 在EvoCodeBench数据集上测试，该方法显著优于基线方法，证明其有效性。

Conclusion: 基于知识图谱的代码生成方法可以推动更具鲁棒性和上下文敏感性的编码辅助工具发展。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [688] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron,Jože M. Rožanec,Michael Witbrock,Gillian Dobbie*

Key words: 因果世界模型、反事实推理、大语言模型、因果关系提取

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出一种名为Causal Cartographer的框架，用于构建因果世界模型，以解决现有基础模型（如大语言模型）在因果推理任务中的不足。通过提取和建模因果关系，该框架能够生成真实世界的反事实推理。

Motivation: 现有的大语言模型缺乏明确的因果推理能力，且对反事实推理的评估受限于只能观察到的真实世界数据。因此，需要一种能够提取因果关系并进行可靠推理的方法。

Method: 提出了Causal Cartographer框架，包括两个主要部分：图检索增强生成代理（用于从数据中提取因果关系）和反事实推理代理（基于因果关系进行逐步推理）。

Result: 该框架能够有效提取因果知识，提高大语言模型在因果推理任务中的鲁棒性，同时降低推理成本和虚假相关性。

Conclusion: Causal Cartographer框架为解决因果推理问题提供了一种可行的方法，能够生成可靠的因果关系网络并支持反事实推理。

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [689] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Key words: 推理语言模型,负样本增强,离线强化学习,数学推理,编码推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为BCPG-NSA的细粒度离线强化学习框架，用于有效利用负样本中的学习信号，提升数学和编码推理任务的性能。

Motivation: 现有方法在负样本处理上存在不足，要么完全丢弃负样本，要么对所有标记进行同等惩罚，未能充分利用负样本中的自反思和纠错步骤等有价值信息。

Method: BCPG-NSA框架分为三个阶段：样本分割、基于共识的步骤正确性评估（结合LLM和PRM评估器）以及带负样本增强的策略优化。

Result: 在多个数学和编码推理基准测试中，BCPG-NSA表现优于基线方法，提高了样本效率，并在多次迭代中展现出稳健性和可扩展性。

Conclusion: BCPG-NSA通过细粒度利用负样本中的学习信号，显著提升了推理模型的性能。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [690] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski,Adrian Kosmala,Paul Swoboda*

Key words: 提示工程, 强化学习, 自动提示生成, LLM

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: PRL提出了一种基于强化学习的自动提示生成方法，能够生成训练中未见过的少样本示例，并在多个任务中实现最佳性能。

Motivation: 有效提示工程是充分发挥LLM能力的核心挑战，需要专家直觉和任务理解。现有方法难以捕捉关键语义线索。

Method: PRL利用强化学习自动生成提示，能够产生训练中未见的少样本示例。

Result: 在文本分类任务中优于APE和EvoPrompt，摘要任务中ROUGE分数提升，简化任务中SARI分数显著提高。

Conclusion: PRL是一种高效、自动化的提示生成方法，显著提升了LLM在多种任务上的性能。

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [691] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/abs/2505.14419)
*Huimin Xu,Xin Mao,Feng-Lin Li,Xiaobao Wu,Wang Chen,Wei Zhang,Anh Tuan Luu*

Key words: 过程奖励模型, SCOPE, 前缀树, 过程标注, 数学推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SCOPE是一种基于压缩的新方法，显著降低了过程奖励模型（PRMs）的过程标注成本，并通过构建前缀树提升效率。

Motivation: 现有过程标注方法（如人工标注或蒙特卡洛模拟）计算成本高，需要一种更高效的替代方案。

Method: 将自然语言推理步骤转化为代码并归一化，构建前缀树合并等效步骤，将复杂度从O(NMK)降至O(N)。

Result: 仅需5%计算资源即构建196K样本数据集，PRMs在Best-of-N和ProcessBench上表现优于现有方法。

Conclusion: SCOPE有效降低标注成本，提升模型性能，为过程奖励模型提供高效解决方案。

Abstract: Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [692] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan,Eitan Stern,Dafna Shahaf*

Key words: LLMs, 神经符号方法, 几何证明, 形式化验证

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文摘要介绍了一种神经符号方法，通过结合LLMs的生成能力和结构化组件，提高其逻辑推理能力，特别是在几何证明任务中。通过检索相似问题和验证反馈，显著提升了证明的准确性。

Motivation: 大型语言模型（LLMs）在需要严格逻辑推理的领域（如数学证明）表现不佳，作者希望通过神经符号方法结合LLMs的生成能力与结构化组件来提升其性能。

Method: 方法包括：（1）检索相似问题并用其证明指导LLM生成；（2）用形式化验证器评估生成的证明并提供反馈以修正错误。

Result: 实验证明，该方法显著提升了OpenAI的o1模型的证明准确率（58%-70%的改进）。

Conclusion: 通过生成可证明正确的结论，可以大幅提升LLMs的可靠性、准确性和一致性，从而解锁需要高可信度的复杂任务和应用。

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [693] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon,Seungone Kim,Sohee Yang,Sunkyoung Kim,Soyeon Kim,Yongil Kim,Eunbi Choi,Yireun Kim,Minjoon Seo*

Key words: LLMs, 置信度校准, 链式思维推理, 慢思考行为

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: LLMs通常无法准确表达其置信度，但研究发现，进行链式思维推理的模型在问题解决和置信度校准上表现更优。

Motivation: 评估LLMs的置信度准确性，探索推理模型如何通过慢思考行为提高校准性能。

Method: 在六个数据集上对比六种推理模型与非推理模型的置信度校准表现，分析慢思考行为的影响。

Result: 推理模型在33/36的场景中表现更优，且随着推理链展开，校准性能逐步提升。去除慢思考行为会显著降低表现。

Conclusion: 慢思考行为（如探索替代方案和回溯）是提高LLMs置信度校准的关键因素。

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [694] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/abs/2505.14510)
*Haishi Bai,Jozo Dujmovic,Jianwu Wang*

Key words: 可解释AI, 分级逻辑, 透明性, 决策支持

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: BACON是一个新框架，通过分级逻辑自动训练可解释的AI模型，强调高准确性和结构透明性，适用于多种决策场景。

Motivation: 随着AI在高风险领域的广泛应用，确保模型透明和可解释至关重要，BACON旨在提供准确且可调试的解决方案。

Method: BACON利用分级逻辑训练模型，提供符号化解释和高结构透明度。

Result: 在布尔逼近、鸢尾花分类等多样场景中，BACON表现优异，生成紧凑且可验证的决策逻辑。

Conclusion: BACON为高透明度和信任度的可解释AI提供了实用且原则性的方法。

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [695] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher,William Brach,Tibor Sloboda,Kristián Košťál,Lukas Galke*

Key words: 查询路由, 分布外查询, GQR-Bench, WideMLP, fastText, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究了受保护的查询路由问题，提出了GQR-Bench基准，评估了多种路由方法的性能，发现WideMLP在速度和准确性上表现最佳。

Motivation: 解决查询路由中如何处理分布外查询的问题，确保路由的准确性与安全性。

Method: 使用GQR-Bench评估了LLM、传统机器学习模型等多种路由机制的性能，重点关注准确性、速度和鲁棒性。

Result: WideMLP在准确性和速度上表现最佳（88%，<4ms），fastText速度最快（<1ms），LLM准确性最高（91%）但速度较慢。

Conclusion: LLM并非查询路由的唯一选择，WideMLP和fastText在某些场景下更为实用。GQR-Bench将作为Python包发布。

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [696] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/abs/2505.14539)
*Gaia Belardinelli,Thomas Bolander,Sebastian Watzl*

Key words: 注意力逻辑, 动态认知逻辑, 事件模型, 模态逻辑, AI认知

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种通用的注意力逻辑，扩展了现有动态认知逻辑的局限性，能够建模复杂的注意力场景，包括对任意公式的关注，并通过引入注意力原则，将其视为一种模态（如信念或认知）。

Motivation: 现有的动态认知逻辑仅能建模对原子公式的关注，且随着代理数量和声明字面量的增加，其复杂性呈指数级增长。本文旨在克服这些局限性，提出更通用的注意力逻辑。

Method: 扩展了边缘条件事件模型，证明其表达力与标准事件模型相当但更简洁；并允许代理关注任意公式，如其他代理的信念或注意力。

Result: 提出的逻辑能够处理复杂的注意力场景，如高阶信念或其他代理的注意力，并通过示例展示了AI代理如何发现人类的注意力偏见。

Conclusion: 本文的通用注意力逻辑为建模和分析注意力相关的认知现象提供了更强大的工具，尤其在处理复杂场景时展现出高效性和灵活性。

Abstract: In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [697] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/abs/2505.14544)
*Saahil Mahato*

Key words: 多智能体强化学习, 交通信号优化, 城市拥堵, 模拟环境

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文研究了多智能体强化学习（MARL）在优化多交叉路口交通信号协调中的应用，展示了MARL相比固定时间信号控制在减少等待时间和提高吞吐量方面的显著优势。

Motivation: 城市交通拥堵，尤其是交叉路口的拥堵，显著影响了出行时间、燃油消耗和排放。传统的固定时间信号控制系统缺乏动态适应能力。

Method: 利用Pygame开发了一个模拟环境，模拟了随机生成车流的互联交叉路口网络。采用分散式MARL控制器，每个交通信号作为自主智能体，基于局部观察和邻近智能体的信息做出决策。

Result: MARL方法显著减少了平均等待时间并提高了吞吐量，效果优于固定时间控制器。

Conclusion: MARL动态控制策略在提高城市交通管理效率方面具有潜力，但仍需进一步研究解决扩展性和实际应用中的挑战。

Abstract: Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [698] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj,Arjun Beniwal,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik R. Narasimhan,Ameet Deshpande,Vishvak Murahari*

Key words: 多智能体系统, 协作协议, 集体推理, ACPs, 模块化

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 介绍了Agent Context Protocols (ACPs)，一种结构化协议，用于多智能体系统的通信与协调，显著提升了集体推理的性能。

Motivation: 解决多智能体系统中自然语言通信的不精确性和临时性问题，以实现更高效的协作与互操作性。

Method: 提出ACP协议，结合持久执行蓝图和标准化消息模式，支持多智能体的复杂交互和容错处理。

Result: 在AssistantBench上实现28.3%准确率，优于商业AI系统，尤其在长时程网络辅助和多模态技术报告中表现突出。

Conclusion: ACPs是模块化和可扩展的，能快速构建高性能通用智能体系统。

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [699] [Towards a Foundation Model for Communication Systems](https://arxiv.org/abs/2505.14603)
*Davide Buffelli,Sowmen Das,Yu-Wei Lin,Sattar Vakili,Chien-Yi Wang,Masoud Attarifar,Pritthijit Nath,Da-shan Shiu*

Key words: 人工智能, 通信系统, Transformer, 多模态模型, 特征估计

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种基于Transformer的多模态基础模型，用于处理通信数据，解决了包括标记化、位置嵌入、多模态等关键挑战，并实现了多种特征的估计。

Motivation: 随着AI在多领域的出色表现，其在通信系统中的应用成为研究热点，趋势逐渐从任务特定解决方案转向支持多应用的大型通用模型。

Method: 开发基于Transformer的多模态模型，提出针对通信数据的标记化、位置嵌入等方法。

Result: 模型能成功估计传输等级、预编码器选择、多普勒扩展和延迟剖面等多种特征。

Conclusion: 该研究为通信数据的基础模型提供了一种有效方法，验证了其估计多种特征的潜力。

Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [700] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604)
*Haoran Zhao,Yuchen Yan,Yongliang Shen,Haolei Xu,Wenqi Zhang,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Key words: 大型推理模型, 过思考, 自我调节, 计算开销, SBT

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种自我调节框架SBT，减少大型推理模型的冗余计算，显著降低计算开销并提升效率。

Motivation: 现有的长推理模型虽然性能卓越，但存在冗余推理和计算开销大的问题，需外部干预。

Method: 通过设计过思考识别指标和自适应推理长度数据构建方法，结合创新的刹车提示机制。

Result: 在数学基准测试中，token消耗减少60%，同时保持与未约束模型相当的准确性。

Conclusion: SBT框架有效解决了过思考问题，减少了计算开销并提升了推理效率。

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [701] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei,Yuheng Wu,Yingjia Wan,Tarun Suresh,Huanmi Tan,Zhanke Zhou,Sanmi Koyejo,Ke Wang,Alex Aiken*

Key words: SATBench, 大型语言模型, 逻辑推理, 布尔可满足性, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SATBench是一个用于评估大型语言模型（LLMs）逻辑推理能力的基准，通过布尔可满足性（SAT）问题生成的逻辑谜题来实现。

Motivation: 现有研究多关注基于推理规则的推理，SATBench则利用SAT问题的搜索性质，填补了这一空白。

Method: SATBench实例从SAT公式生成，并通过LLMs转换为故事上下文和条件，生成过程完全自动化，难度可调。

Result: 实验结果显示，即使是当前最强模型o4-mini在困难UNSAT问题上准确率仅为65.0%，接近随机基准50%。

Conclusion: SATBench揭示了当前LLMs在搜索性逻辑推理能力上的根本局限性，并为未来研究提供了可扩展的测试平台。

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [702] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari,Mirella Lapata*

Key words: 大型语言模型, 多模态, 辩论框架, 视觉问答, 监督学习

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文探讨了在多模态环境中扩展辩论范式，利用较弱模型监督和增强较强模型的潜力。重点研究了视觉问答（VQA）任务，通过辩论框架提升性能。

Motivation: 随着大型语言模型（LLMs）在多领域和多模态中表现出的能力超越人类评估者，如何实现可扩展的监督变得尤为重要。辩论机制是一种潜在的解决方案。

Method: 提出了一种多模态辩论框架，其中两个“有视觉”的专家模型进行辩论，一个“无视觉”的法官模型仅根据辩论质量裁决。专家模型仅支持与自身信念一致的答案。

Result: 实验表明，辩论框架在多项多模态任务中表现优于单个专家模型。此外，较弱LLMs的裁决可通过微调增强视觉语言模型的推理能力。

Conclusion: 辩论机制在多模态环境中具有潜力，能够通过监督和微调提升模型的性能与推理能力。

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [703] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/abs/2505.14656)
*Zihao Zhang,Fei Liu*

Key words: LLM, 成本敏感规划, 蒙特卡洛树搜索, 预算决策

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: CATS是一种将成本意识引入LLM规划的新方法，通过结合LLM的推理能力和结构化搜索，显著提升了预算敏感任务的性能。

Motivation: 解决LLM在成本敏感规划中表现不佳的问题，如忽视成本差异或超出预算。

Method: 引入成本增强蒙特卡洛树搜索（CATS），明确考虑成本约束，优化规划过程。

Result: CATS在严格预算下表现优异，任务成功率和成本效率均高于GPT-4.1等原始LLM。

Conclusion: CATS为预算敏感的决策提供了高效解决方案，结合了LLM的推理能力和结构化搜索的优势。

Abstract: While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [704] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung,Sangyeon Yoon,Minsuk Kahng,Albert No*

Key words: SAFEPATH, 推理模型, 安全对齐, 零样本, 越狱攻击

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: SAFEPATH是一种轻量级对齐方法，通过在有害提示下生成安全提示词来减少不安全输出，同时保持推理性能。

Motivation: 大型推理模型在有害提示下可能产生不安全输出，现有安全对齐方法会降低推理深度或无法抵御复杂攻击。

Method: SAFEPATH通过微调模型在有害提示下生成8个令牌的安全提示词，其余推理过程不受监控。

Result: 实验表明SAFEPATH能减少90.0%的有害输出，阻止83.3%的越狱攻击，计算成本大幅降低。

Conclusion: SAFEPATH提供了一种高效安全对齐方法，揭示了现有方法在推理中心模型中的局限性。

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [705] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Key words: Large Language Models, proactive agents, context-aware, wearables, benchmark

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种名为ContextAgent的上下文感知主动代理，通过整合多维感官数据提升LLM代理的主动服务能力，并在新基准测试中表现优异。

Motivation: 现有主动代理仅依赖封闭环境观察或基于规则的主动通知，导致用户意图理解不足和服务功能受限。

Method: ContextAgent从穿戴设备的多维感官数据中提取上下文，结合历史数据预判主动服务需求，并自动调用工具提供帮助。

Result: 在ContextAgentBench基准测试中，ContextAgent在主动预测和工具调用方面分别比基线高8.5%和6.0%。

Conclusion: 研究表明ContextAgent能有效提升主动服务的准确性和实用性，推动人本AI助理的发展。

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [706] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang,Xingyu Chen,Yue Wang,Zhiwei He,Jiahao Xu,Tian Liang,Qiuzhi Liu,Yunzhi Yao,Wenxuan Wang,Ruotian Ma,Haitao Mi,Ningyu Zhang,Zhaopeng Tu,Xiaolong Li,Dong Yu*

Key words: Mixture-of-Experts, Large Reasoning Models, cognitive efficiency, RICE, nPMI

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种新的推理时间指导方法RICE，通过强化认知专家来提高大型推理模型的性能，避免了额外训练或复杂启发式方法。

Motivation: 现有的推理模型存在认知效率低下（如过度思考或思考不足）的问题，限制了其性能。

Method: 利用归一化点间互信息（nPMI）识别专门的认知专家，并通过RICE方法在推理时指导这些专家。

Result: 在DeepSeek-R1和Qwen3-235B等模型上的实验表明，该方法显著提高了推理准确性、认知效率和跨领域泛化能力。

Conclusion: RICE是一种轻量级且实用的方法，能够有效提升高级推理模型的认知效率。

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [707] [Learning to Program Quantum Measurements for Machine Learning](https://arxiv.org/abs/2505.13525)
*Samual Yen-Chi Chen,Huan-Hsin Tseng,Hsin-Yi Lin,Shinjae Yoo*

Key words: 量子机器学习，可训练观测，动态编程，变分量子电路

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 提出了一种新型量子机器学习框架，通过可训练的量子观测实现端到端优化，显著提升了模型性能。

Motivation: 量子计算和机器学习的快速发展推动了量子机器学习的研究，但高性能QML模型的设计面临数据和参数优化等挑战。

Method: 提出一种可训练量子观测框架，结合神经网络动态编程观测参数，实现与量子电路参数的联合优化。

Result: 数值模拟显示该方法在变分量子电路中动态编程观测，性能优于现有方法，如更高的分类准确率。

Conclusion: 该框架显著提升了QML模型的有效性，为未来研究提供了新方向。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have sparked significant interest, driving extensive exploration of quantum
machine learning (QML) algorithms to address a wide range of complex
challenges. The development of high-performance QML models requires
expert-level expertise, presenting a key challenge to the widespread adoption
of QML. Critical obstacles include the design of effective data encoding
strategies and parameterized quantum circuits, both of which are vital for the
performance of QML models. Furthermore, the measurement process is often
neglected-most existing QML models employ predefined measurement schemes that
may not align with the specific requirements of the targeted problem. We
propose an innovative framework that renders the observable of a quantum
system-specifically, the Hermitian matrix-trainable. This approach employs an
end-to-end differentiable learning framework, enabling simultaneous
optimization of the neural network used to program the parameterized
observables and the standard quantum circuit parameters. Notably, the quantum
observable parameters are dynamically programmed by the neural network,
allowing the observables to adapt in real time based on the input data stream.
Through numerical simulations, we demonstrate that the proposed method
effectively programs observables dynamically within variational quantum
circuits, achieving superior results compared to existing approaches. Notably,
it delivers enhanced performance metrics, such as higher classification
accuracy, thereby significantly improving the overall effectiveness of QML
models.

</details>


### [708] [Benchmarking data encoding methods in Quantum Machine Learning](https://arxiv.org/abs/2505.14295)
*Orlane Zang,Grégoire Barrué,Tony Quertier*

Key words: 量子机器学习, 数据编码, 量子特征映射, 量子嵌入, 基准测试

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 本文讨论了数据编码在量子机器学习（QML）中的关键作用，比较了不同编码方法的性能，并针对数据集进行了基准测试。

Motivation: 研究量子数据编码方法的多样性及其对QML性能的影响，填补了缺乏通用选择规则的空白。

Method: 研究了多种常用的量子逻辑门编码方法，并在不同数据集上进行了基准测试。

Result: 通过实验比较，展示了不同编码方法在不同数据集上的表现差异。

Conclusion: 数据编码方法的选择对QML性能至关重要，但缺乏通用选择规则，需要根据具体数据集进行实验验证。

Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine
Learning (QML). While classical approaches process data directly as vectors,
QML may require transforming classical data into quantum states through
encoding circuits, known as quantum feature maps or quantum embeddings. This
step leverages the inherently high-dimensional and non-linear nature of Hilbert
space, enabling more efficient data separation in complex feature spaces that
may be inaccessible to classical methods. This encoding part significantly
affects the performance of the QML model, so it is important to choose the
right encoding method for the dataset to be encoded. However, this choice is
generally arbitrary, since there is no "universal" rule for knowing which
encoding to choose based on a specific set of data. There are currently a
variety of encoding methods using different quantum logic gates. We studied the
most commonly used types of encoding methods and benchmarked them using
different datasets.

</details>


### [709] [QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems](https://arxiv.org/abs/2505.14192)
*Bikash K. Behera,Saif Al-Kuwari,Ahmed Farouk*

Key words: 脑机接口, 量子支持向量机, 量子神经网络, EEG, 噪声鲁棒性

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 论文提出了一种新型混合量子学习模型QSVM-QNN，结合了量子支持向量机（QSVM）和量子神经网络（QNN），用于提升EEG脑机接口任务的分类准确性和鲁棒性。

Motivation: 现有脑机接口系统在信号变异性、分类效率及实时适应个体用户等方面面临挑战，需要更高效的解决方案。

Method: 利用QSVM的决策边界能力和QNN的表达学习能力，设计了QSVM-QNN混合模型，并通过两个EEG基准数据集和六种量子噪声模型进行评估。

Result: QSVM-QNN在基准数据集上分别达到0.990和0.950的高准确率，并在噪声条件下表现稳定，优于其他模型。

Conclusion: 该混合量子架构不仅适用于脑机接口，还可推广至其他生物医学和时间序列分类任务，为下一代神经技术系统提供可扩展且抗噪声的解决方案。

Abstract: A brain-computer interface (BCI) system enables direct communication between
the brain and external devices, offering significant potential for assistive
technologies and advanced human-computer interaction. Despite progress, BCI
systems face persistent challenges, including signal variability,
classification inefficiency, and difficulty adapting to individual users in
real time. In this study, we propose a novel hybrid quantum learning model,
termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with
a Quantum Neural Network (QNN), to improve classification accuracy and
robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines
the decision boundary capabilities of QSVM with the expressive learning power
of QNN, leading to superior generalization performance. The proposed model is
evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and
0.950, outperforming both classical and standalone quantum models. To
demonstrate real-world viability, we further validated the robustness of QNN,
QSVM, and QSVM-QNN against six realistic quantum noise models, including bit
flip and phase damping. These experiments reveal that QSVM-QNN maintains stable
performance under noisy conditions, establishing its applicability for
deployment in practical, noisy quantum environments. Beyond BCI, the proposed
hybrid quantum architecture is generalizable to other biomedical and
time-series classification tasks, offering a scalable and noise-resilient
solution for next-generation neurotechnological systems.

</details>


### [710] [Quantum Optimization via Gradient-Based Hamiltonian Descent](https://arxiv.org/abs/2505.14670)
*Jiaqi Leng,Bin Shi*

Key words: 机器学习的优化技术, 量子算法, 梯度下降, 哈密顿力学, 全局优化

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 论文提出了一种改进的量子梯度下降算法（gradient-based QHD），通过融入梯度信息，显著提高了收敛速度和解的全局性。

Motivation: 现有的Quantum Hamiltonian Descent (QHD)存在收敛速度慢和对高度非凸问题鲁棒性不足的问题，亟需改进。

Method: 将梯度信息融入QHD中，提出gradient-based QHD算法。

Result: 改进后的算法在数值实验中表现优异，收敛速度和解的全局性均优于现有量子及经典方法至少一个数量级。

Conclusion: gradient-based QHD有效解决了QHD的局限性，为复杂优化问题提供了更高效的量子解决方案。

Abstract: With rapid advancements in machine learning, first-order algorithms have
emerged as the backbone of modern optimization techniques, owing to their
computational efficiency and low memory requirements. Recently, the connection
between accelerated gradient methods and damped heavy-ball motion, particularly
within the framework of Hamiltonian dynamics, has inspired the development of
innovative quantum algorithms for continuous optimization. One such algorithm,
Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle
points and local minima, facilitating the discovery of global solutions in
complex optimization landscapes. However, QHD faces several challenges,
including slower convergence rates compared to classical gradient methods and
limited robustness in highly non-convex problems due to the non-local nature of
quantum states. Furthermore, the original QHD formulation primarily relies on
function value information, which limits its effectiveness. Inspired by
insights from high-resolution differential equations that have elucidated the
acceleration mechanisms in classical methods, we propose an enhancement to QHD
by incorporating gradient information, leading to what we call gradient-based
QHD. Gradient-based QHD achieves faster convergence and significantly increases
the likelihood of identifying global solutions. Numerical simulations on
challenging problem instances demonstrate that gradient-based QHD outperforms
existing quantum and classical methods by at least an order of magnitude.

</details>


### [711] [Learning to Program Quantum Measurements for Machine Learning](https://arxiv.org/abs/2505.13525)
*Samual Yen-Chi Chen,Huan-Hsin Tseng,Hsin-Yi Lin,Shinjae Yoo*

Key words: 量子机器学习（QML）, 可微分学习, 参数化量子电路, 动态可观测量, 变分量子电路

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 提出了一种可训练量子系统中可观测量（厄米矩阵）的创新框架，通过端到端的可微分学习框架优化量子电路参数和神经网络编程的可观测量，提升量子机器学习模型的性能。

Motivation: 量子机器学习（QML）模型的广泛采用面临关键挑战，包括数据编码策略和参数化量子电路设计的困难，以及测量过程的忽视。本文旨在解决这些问题。

Method: 采用可微分学习框架，动态编程量子可观测量参数和标准量子电路参数，使可观测量能够根据输入数据实时调整。

Result: 通过数值模拟验证，该方法在变分量子电路中动态编程可观测量，取得了优于现有方法的性能指标，如更高的分类准确率。

Conclusion: 所提出的框架显著提升了QML模型的整体有效性，为解决QML中的关键挑战提供了创新解决方案。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have sparked significant interest, driving extensive exploration of quantum
machine learning (QML) algorithms to address a wide range of complex
challenges. The development of high-performance QML models requires
expert-level expertise, presenting a key challenge to the widespread adoption
of QML. Critical obstacles include the design of effective data encoding
strategies and parameterized quantum circuits, both of which are vital for the
performance of QML models. Furthermore, the measurement process is often
neglected-most existing QML models employ predefined measurement schemes that
may not align with the specific requirements of the targeted problem. We
propose an innovative framework that renders the observable of a quantum
system-specifically, the Hermitian matrix-trainable. This approach employs an
end-to-end differentiable learning framework, enabling simultaneous
optimization of the neural network used to program the parameterized
observables and the standard quantum circuit parameters. Notably, the quantum
observable parameters are dynamically programmed by the neural network,
allowing the observables to adapt in real time based on the input data stream.
Through numerical simulations, we demonstrate that the proposed method
effectively programs observables dynamically within variational quantum
circuits, achieving superior results compared to existing approaches. Notably,
it delivers enhanced performance metrics, such as higher classification
accuracy, thereby significantly improving the overall effectiveness of QML
models.

</details>


### [712] [Benchmarking data encoding methods in Quantum Machine Learning](https://arxiv.org/abs/2505.14295)
*Orlane Zang,Grégoire Barrué,Tony Quertier*

Key words: 量子机器学习, 数据编码, 量子特征映射, 基准测试, 量子逻辑门

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 量子机器学习中数据编码对模型性能至关重要，但目前缺乏通用的选择标准，论文研究了常见编码方法并进行了基准测试。

Motivation: 量子机器学习依赖于将经典数据编码为量子态，但编码方法的选择缺乏明确规则，影响模型性能。

Method: 研究并对比了多种量子逻辑门实现的编码方法，并使用不同数据集进行了基准测试。

Result: 通过实验验证不同编码方法在特定数据集上的性能表现。

Conclusion: 选择合适的编码方法对量子机器学习模型至关重要，需要根据具体数据集进行优化。

Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine
Learning (QML). While classical approaches process data directly as vectors,
QML may require transforming classical data into quantum states through
encoding circuits, known as quantum feature maps or quantum embeddings. This
step leverages the inherently high-dimensional and non-linear nature of Hilbert
space, enabling more efficient data separation in complex feature spaces that
may be inaccessible to classical methods. This encoding part significantly
affects the performance of the QML model, so it is important to choose the
right encoding method for the dataset to be encoded. However, this choice is
generally arbitrary, since there is no "universal" rule for knowing which
encoding to choose based on a specific set of data. There are currently a
variety of encoding methods using different quantum logic gates. We studied the
most commonly used types of encoding methods and benchmarked them using
different datasets.

</details>


### [713] [QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems](https://arxiv.org/abs/2505.14192)
*Bikash K. Behera,Saif Al-Kuwari,Ahmed Farouk*

Key words: BCI, Quantum Learning, QSVM, QNN, EEG, Noise Resilience

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 提出一种名为QSVM-QNN的混合量子学习模型，结合量子支持向量机（QSVM）和量子神经网络（QNN），以提升EEG-based BCI任务的分类准确性和鲁棒性，并在实验中证明了其优越性能和噪声适应性。

Motivation: BCI系统在信号变异性、分类效率和实时适应个人用户方面仍存在挑战，需要一种更高效的分类方法。

Method: 整合QSVM和QNN，形成QSVM-QNN混合模型，结合两者的优势提升性能。在基准EEG数据集上进行测试，并评估其对量子噪声的鲁棒性。

Result: 在两种基准EEG数据集上分别达到0.990和0.950的高准确率，优于经典和独立量子模型，且在噪声环境下表现稳定。

Conclusion: QSVM-QNN为下一代神经技术系统提供了一种可扩展且抗噪声的解决方案，适用于其他生物医学和时间序列分类任务。

Abstract: A brain-computer interface (BCI) system enables direct communication between
the brain and external devices, offering significant potential for assistive
technologies and advanced human-computer interaction. Despite progress, BCI
systems face persistent challenges, including signal variability,
classification inefficiency, and difficulty adapting to individual users in
real time. In this study, we propose a novel hybrid quantum learning model,
termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with
a Quantum Neural Network (QNN), to improve classification accuracy and
robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines
the decision boundary capabilities of QSVM with the expressive learning power
of QNN, leading to superior generalization performance. The proposed model is
evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and
0.950, outperforming both classical and standalone quantum models. To
demonstrate real-world viability, we further validated the robustness of QNN,
QSVM, and QSVM-QNN against six realistic quantum noise models, including bit
flip and phase damping. These experiments reveal that QSVM-QNN maintains stable
performance under noisy conditions, establishing its applicability for
deployment in practical, noisy quantum environments. Beyond BCI, the proposed
hybrid quantum architecture is generalizable to other biomedical and
time-series classification tasks, offering a scalable and noise-resilient
solution for next-generation neurotechnological systems.

</details>


### [714] [Quantum Optimization via Gradient-Based Hamiltonian Descent](https://arxiv.org/abs/2505.14670)
*Jiaqi Leng,Bin Shi*

Key words: 量子优化, 哈密顿动力学, 梯度下降, 量子隧穿, 非凸优化

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 论文提出了基于梯度的量子哈密顿下降（QHD）方法，通过结合梯度信息解决原QHD的局限，显著提高了收敛速度和解的全局性。

Motivation: 量子哈密顿下降（QHD）虽能通过量子隧穿逃离局部极值，但由于缺乏梯度信息，收敛速度慢且在高非凸问题中表现不佳。

Method: 提出梯度增强的QHD方法，整合高分辨率微分方程的加速机制，利用梯度信息提升优化性能。

Result: 数值模拟表明，梯度QHD在复杂问题上的表现优于现有量子与经典方法至少一个数量级。

Conclusion: 梯度信息的引入显著提升了QHD的效率，为量子优化算法提供新思路。

Abstract: With rapid advancements in machine learning, first-order algorithms have
emerged as the backbone of modern optimization techniques, owing to their
computational efficiency and low memory requirements. Recently, the connection
between accelerated gradient methods and damped heavy-ball motion, particularly
within the framework of Hamiltonian dynamics, has inspired the development of
innovative quantum algorithms for continuous optimization. One such algorithm,
Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle
points and local minima, facilitating the discovery of global solutions in
complex optimization landscapes. However, QHD faces several challenges,
including slower convergence rates compared to classical gradient methods and
limited robustness in highly non-convex problems due to the non-local nature of
quantum states. Furthermore, the original QHD formulation primarily relies on
function value information, which limits its effectiveness. Inspired by
insights from high-resolution differential equations that have elucidated the
acceleration mechanisms in classical methods, we propose an enhancement to QHD
by incorporating gradient information, leading to what we call gradient-based
QHD. Gradient-based QHD achieves faster convergence and significantly increases
the likelihood of identifying global solutions. Numerical simulations on
challenging problem instances demonstrate that gradient-based QHD outperforms
existing quantum and classical methods by at least an order of magnitude.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [715] [Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators](https://arxiv.org/abs/2505.14314)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Key words: 注意力机制, FlashAttention, 硬件加速器, ExpMul算子

<details>
  <summary>Details</summary>

Main category: cs.AR

TL;DR: 论文提出了一种优化的硬件算子ExpMul，用于改进基于FlashAttention的硬件加速器，显著降低了面积和功耗。

Motivation: 随着序列建模中对注意力机制的需求增加，硬件加速器的优化成为关键。现有方法在计算指数和向量乘法时效率不足，需要改进。

Method: 通过设计新的硬件算子ExpMul，融合指数计算和向量乘法（如e^x和V），优化FlashAttention内核。

Result: 在28nm ASIC技术中，ExpMul算子的实现比现有技术平均减少了28.8%的面积和17.6%的功耗。

Conclusion: ExpMul算子显著提升了硬件加速器的效率，为长序列建模提供了更优的解决方案。

Abstract: Attention mechanisms, particularly within Transformer architectures and large
language models (LLMs), have revolutionized sequence modeling in machine
learning and artificial intelligence applications. To compute attention for
increasingly long sequences, specialized accelerators have been proposed to
execute key attention steps directly in hardware. Among the various recently
proposed architectures, those based on variants of the FlashAttention
algorithm, originally designed for GPUs, stand out due to their optimized
computation, tiling capabilities, and reduced memory traffic. In this work, we
focus on optimizing the kernel of floating-point-based FlashAttention using new
hardware operators that fuse the computation of exponentials and vector
multiplications, e.g., e^x, V. The proposed ExpMul hardware operators
significantly reduce the area and power costs of FlashAttention-based hardware
accelerators. When implemented in a 28nm ASIC technology, they achieve
improvements of 28.8% in area and 17.6% in power, on average, compared to
state-of-the-art hardware architectures with separate exponentials and vector
multiplications hardware operators.

</details>


### [716] [Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators](https://arxiv.org/abs/2505.14314)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Key words: 注意力机制, FlashAttention, 硬件加速器, ExpMul, ASIC

<details>
  <summary>Details</summary>

Main category: cs.AR

TL;DR: 本文优化了基于浮点的FlashAttention内核，通过引入新的硬件操作符ExpMul（融合指数和向量乘法运算），显著降低了硬件加速器的面积和功耗成本。

Motivation: 研究动机是解决在长序列建模中，注意力机制计算的高硬件资源消耗问题，尤其是在FlashAttention算法中。

Method: 方法通过设计新的硬件操作符ExpMul，融合指数和向量乘法运算，优化FlashAttention内核。

Result: 在28nm ASIC技术中实现，相比现有技术，面积平均减少28.8%，功耗平均降低17.6%。

Conclusion: 结论表明，融合硬件操作符是优化FlashAttention硬件加速器的有效途径。

Abstract: Attention mechanisms, particularly within Transformer architectures and large
language models (LLMs), have revolutionized sequence modeling in machine
learning and artificial intelligence applications. To compute attention for
increasingly long sequences, specialized accelerators have been proposed to
execute key attention steps directly in hardware. Among the various recently
proposed architectures, those based on variants of the FlashAttention
algorithm, originally designed for GPUs, stand out due to their optimized
computation, tiling capabilities, and reduced memory traffic. In this work, we
focus on optimizing the kernel of floating-point-based FlashAttention using new
hardware operators that fuse the computation of exponentials and vector
multiplications, e.g., e^x, V. The proposed ExpMul hardware operators
significantly reduce the area and power costs of FlashAttention-based hardware
accelerators. When implemented in a 28nm ASIC technology, they achieve
improvements of 28.8% in area and 17.6% in power, on average, compared to
state-of-the-art hardware architectures with separate exponentials and vector
multiplications hardware operators.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [717] [Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains](https://arxiv.org/abs/2505.14551)
*Petros Drineas,Rohit Nema,Rafail Ostrovsky,Vassilis Zikas*

Key words: 信誉系统,博弈论,区块链,可信度,PageRank

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 本文提出了一种新的信誉系统模型，即可信信誉游戏，旨在通过博弈论设计经济稳健的信誉系统，支持用户在区块链环境中真实报告服务器的可信度。

Motivation: 现有区块链信誉系统易受操纵且缺乏博弈论支持的经济稳健性，因此需要设计一种可信的信誉系统模型。

Method: 提出了一类称为可信信誉游戏的博弈，用户通过报告对服务器可信度的真实信念，满足博弈论均衡条件，同时利用PageRank算法关联可信度发现。

Result: 研究表明，用户在博弈中采取真实策略是近乎最优的，且观察者可以通过用户行为估计服务器相对可信度。

Conclusion: 可信信誉游戏为区块链信誉系统提供了经济稳健的设计框架，并可应用于权益证明区块链中。

Abstract: Reputation systems play an essential role in the Internet era, as they enable
people to decide whom to trust, by collecting and aggregating data about users'
behavior. Recently, several works proposed the use of reputation for the design
and scalability improvement of decentralized (blockchain) ledgers; however,
such systems are prone to manipulation and to our knowledge no game-theoretic
treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em
trustworthy reputation systems}. Concretely, we describe a class of games,
which we term {\em trustworthy reputation games}, that enable a set of users to
report a function of their beliefs about the trustworthiness of each server in
a set -- i.e., their estimate of the probability that this server will behave
according to its specified strategy -- in a way that satisfies the following
properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play
a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true}
trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium
allows anyone who observes the users' strategies to estimate the relative
trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well
known PageRank algorithm and the problem of trustworthiness discovery, which
can be of independent interest. Finally, we show how the above games are
motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.

</details>


### [718] [Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains](https://arxiv.org/abs/2505.14551)
*Petros Drineas,Rohit Nema,Rafail Ostrovsky,Vassilis Zikas*

Key words: 信誉系统,博弈论,可信赖游戏,PageRank,PoR区块链

<details>
  <summary>Details</summary>

Main category: cs.GT

TL;DR: 论文提出了一种新的信誉系统模型——可信赖信誉游戏，通过博弈论设计，激励用户真实反馈其对服务器可信度的信念，从而提升系统的经济稳健性。

Motivation: 现有的去中心化账本信誉系统易受操纵且缺乏经济稳健性的博弈论支持，因此研究如何设计可信赖的信誉系统成为必要。

Method: 提出可信赖信誉游戏模型，利用博弈论设计用户反馈机制，结合PageRank算法优化可信度发现。

Result: 模型实现了在用户信念接近真实可信度时的稳定纳什均衡，并可用于评估服务器的相对可信度。

Conclusion: 论文为信誉系统设计提供了新思路，并在PoR区块链中展示了实际应用潜力。

Abstract: Reputation systems play an essential role in the Internet era, as they enable
people to decide whom to trust, by collecting and aggregating data about users'
behavior. Recently, several works proposed the use of reputation for the design
and scalability improvement of decentralized (blockchain) ledgers; however,
such systems are prone to manipulation and to our knowledge no game-theoretic
treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em
trustworthy reputation systems}. Concretely, we describe a class of games,
which we term {\em trustworthy reputation games}, that enable a set of users to
report a function of their beliefs about the trustworthiness of each server in
a set -- i.e., their estimate of the probability that this server will behave
according to its specified strategy -- in a way that satisfies the following
properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play
a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true}
trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium
allows anyone who observes the users' strategies to estimate the relative
trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well
known PageRank algorithm and the problem of trustworthiness discovery, which
can be of independent interest. Finally, we show how the above games are
motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [719] [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](https://arxiv.org/abs/2505.13579)
*Yipeng Sun,Linda-Sophie Schneider,Chengze Ye,Mingxuan Gu,Siyuan Mei,Siming Bayer,Andreas Maier*

Key words: CBCT, FDK算法, 小波变换, 稀疏表示, 深度学习

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出了一种改进的基于FDK算法的神经网络方法，通过选择性集成可训练元素和利用小波变换减少参数空间，从而在保持可解释性的同时提升图像质量和计算效率。

Motivation: 传统FDK算法在CBCT重建中存在噪声和伪影问题，而现有深度学习方法虽改进图像质量，但计算复杂且缺乏可解释性。

Method: 在FDK算法的余弦加权和滤波阶段选择性引入可训练元素，并利用小波变换稀疏化表示以减少参数。

Result: 参数数量减少93.75%，计算效率提高，图像质量提升且保持与传统FDK相当的推理成本。

Conclusion: 该方法在临床应用中具有实用价值，尤其在计算资源有限的环境中。

Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create sparse representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.

</details>


### [720] [XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data](https://arxiv.org/abs/2505.13906)
*Soyabul Islam Lincoln,Mirza Mohd Shahriar Maswood*

Key words: 阿尔茨海默病, MRI, 深度学习, 注意力机制, 分类准确率

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 本文提出了一种新的深度学习架构，结合MRI和多注意力机制，用于阿尔茨海默病的精确诊断，准确率在多个数据集上超过现有方法。

Motivation: 阿尔茨海默病是一种常见的神经退行性疾病，需要精确诊断和高效治疗，尤其是在医疗成本上升和人工智能在医学诊断中应用扩展的背景下。

Method: 使用深度卷积神经网络，结合多残差块、空间注意力块、分组查询注意力和多头注意力机制，评估了模型在多个公开数据集上的性能，并探讨了疾病进展的可解释性。

Result: 模型在多种分类任务中表现优异，如4类分类准确率达99.66%，3类分类达99.63%，二分类达100%。在OASIS和ADNI数据集上也取得了高准确率。

Conclusion: 该深度学习架构能够从MRI图像中提取关键信息，准确分类阿尔茨海默病的不同阶段，且性能优于现有方法。

Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.

</details>


### [721] [Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation](https://arxiv.org/abs/2505.13911)
*Ruijie Zhao,Zuopeng Tan,Xiao Xue,Longfei Zhao,Bing Li,Zicheng Liao,Ying Ming,Jiaru Wang,Ran Xiao,Sirong Piao,Rui Zhao,Qiqi Xu,Wei Song*

Key words: 肺段分割,弱监督学习,解剖学定义,两阶段分割,一致性损失

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出了一种弱监督学习方法（AHSL），利用肺段解剖学定义进行分割，结合段级和叶级监督，并引入两阶段分割策略和一致性损失，提高了分割效果。

Motivation: 肺段分割对癌症定位和手术规划至关重要，但像素级标注耗时且边界难以区分，因此提出一种基于解剖学定义的弱监督方法。

Method: 设计了基于段级和叶级监督的损失函数，引入两阶段分割策略，并使用一致性损失优化边界平滑性。

Result: 在私有数据集上的实验表明，该方法能有效提升分割效果和边界平滑性。

Conclusion: AHSL方法通过结合解剖学定义和多级监督，显著提高了肺段分割的精度和实用性。

Abstract: Pulmonary segment segmentation is crucial for cancer localization and
surgical planning. However, the pixel-wise annotation of pulmonary segments is
laborious, as the boundaries between segments are indistinguishable in medical
images. To this end, we propose a weakly supervised learning (WSL) method,
termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise
clinical anatomical definition of pulmonary segments to perform pulmonary
segment segmentation. Since pulmonary segments reside within the lobes and are
determined by the bronchovascular tree, i.e., artery, airway and vein, the
design of the loss function is founded on two principles. First, segment-level
labels are utilized to directly supervise the output of the pulmonary segments,
ensuring that they accurately encompass the appropriate bronchovascular tree.
Second, lobe-level supervision indirectly oversees the pulmonary segment,
ensuring their inclusion within the corresponding lobe. Besides, we introduce a
two-stage segmentation strategy that incorporates bronchovascular priori
information. Furthermore, a consistency loss is proposed to enhance the
smoothness of segment boundaries, along with an evaluation metric designed to
measure the smoothness of pulmonary segment boundaries. Visual inspection and
evaluation metrics from experiments conducted on a private dataset demonstrate
the effectiveness of our method.

</details>


### [722] [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI](https://arxiv.org/abs/2505.14064)
*Cosmin I. Bercea,Jun Li,Philipp Raffler,Evamaria O. Riedel,Lena Schmitzer,Angela Kurz,Felix Bitzer,Paula Roßmüller,Julian Canisius,Mirjam L. Beyrle,Che Liu,Wenjia Bai,Bernhard Kainz,Julia A. Schnabel,Benedikt Wiestler*

Key words: NOVA, 脑部MRI, 罕见病理, 分布外检测, 视觉语言模型

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: NOVA是一个具有挑战性的评估基准，用于测试模型在真实医疗影像中对罕见和未知病理的检测和推理能力。

Motivation: 为了解决模型在临床环境中对罕见或未知病症的识别能力不足的问题，作者提出了NOVA基准。

Method: NOVA包含约900个脑部MRI扫描，涵盖281种罕见病理和多种采集协议，每个案例附带临床叙述和专家标注。

Result: 基线测试显示，领先的视觉语言模型在NOVA上的表现显著下降。

Conclusion: NOVA为提升模型对未知异常的检测、定位和推理能力提供了严格测试平台。

Abstract: In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.

</details>


### [723] [Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images](https://arxiv.org/abs/2505.14572)
*Jayroop Ramesh,Valentin Bacher,Mark C. Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana IL Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale*

Key words: Intrapartum Ultrasound, fetal biometry, deep learning, angle of progression, head-symphysis distance

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究提出了一种自动化胎儿生物测量管道，通过分类、分割和计算关键参数来减少观测者间的变异性，提高测量可靠性。

Motivation: 通过超声波监测分娩进程，减少观测者间的变异性，提高测量可靠性，改善对分娩停滞原因的理解。

Method: 采用稀疏采样和集成深度学习技术，分类标准平面、分割胎儿头部和耻骨联合，计算AoP和HSD参数。

Result: 结果显示高准确性和可靠性，各项指标表现优异，如ACC: 0.9452, DSC: 0.918等。

Conclusion: 自动化管道可提升临床风险分层工具的开发，改善产前护理效率。

Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.

</details>


### [724] [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](https://arxiv.org/abs/2505.13579)
*Yipeng Sun,Linda-Sophie Schneider,Chengze Ye,Mingxuan Gu,Siyuan Mei,Siming Bayer,Andreas Maier*

Key words: CBCT, FDK算法, 深度学习, 小波变换, 图像重建

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出一种结合增强FDK算法与神经网络的CBCT重建方法，通过选择性集成可训练元素和利用小波变换稀疏化参数，在保持经典算法可解释性的同时提升图像质量。

Motivation: 传统FDK算法在CBCT重建中效率高但易受噪声和伪影影响，而深度学习方法虽提升质量但增加了复杂性和缺乏可解释性。

Method: 在FDK算法的余弦加权和滤波阶段选择性集成可训练元素，并利用小波变换稀疏化参数以减少93.75%的参数。

Result: 该方法在保持与FDK相同推理计算成本的同时，提升了图像的鲁棒性和一致性，且易于集成到现有CT重建流程中。

Conclusion: 该方法为临床应用提供了一种实用的增强方案，特别适用于计算资源有限的环境。

Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create sparse representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.

</details>


### [725] [XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data](https://arxiv.org/abs/2505.13906)
*Soyabul Islam Lincoln,Mirza Mohd Shahriar Maswood*

Key words: 阿尔茨海默病,深度学习,MRI,注意力机制,多残差块

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 本文提出了一种结合多残差块、空间注意力机制和多种注意力机制的深度学习架构，用于阿尔茨海默病的诊断，在多个公开数据集上表现优异，准确率高达99%-100%。

Motivation: 阿尔茨海默病的精准诊断需求迫切，结合深度学习与MRI图像分析有望提升诊断效率。

Method: 提出了新型深度学习架构，包含多残差块、空间注意力机制、分组查询注意力和多头注意力，并在多个数据集上进行测试。

Result: 在4类、3类和二分类任务中，模型准确率分别达到99.66%、99.63%和100%，优于现有方法。

Conclusion: 所提方法在阿尔茨海默病诊断中表现优异，具有高准确性和可解释性。

Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.

</details>


### [726] [Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation](https://arxiv.org/abs/2505.13911)
*Ruijie Zhao,Zuopeng Tan,Xiao Xue,Longfei Zhao,Bing Li,Zicheng Liao,Ying Ming,Jiaru Wang,Ran Xiao,Sirong Piao,Rui Zhao,Qiqi Xu,Wei Song*

Key words: 肺段分割、弱监督学习、解剖层级监督、支气管血管树、医学图像

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出了一种基于解剖层级监督学习的弱监督方法（AHSL），通过结合临床解剖学定义和肺段结构特征，实现了肺段分割的高精度效果。

Motivation: 肺段分割对癌症定位和手术规划至关重要，但现有医学图像中肺段边界难以区分，标注工作繁琐。

Method: 利用肺段的临床解剖定义设计损失函数，分为直接监督肺段输出和间接监督肺段在肺叶内两部分；引入两阶段分割策略并结合支气管血管先验信息；提出一致性损失和边界平滑度评价指标。

Result: 在私有数据集上的实验表明，该方法在视觉检查和平滑度指标上均表现出色。

Conclusion: AHSL方法通过弱监督学习和解剖学先验信息，有效解决了肺段分割难题。

Abstract: Pulmonary segment segmentation is crucial for cancer localization and
surgical planning. However, the pixel-wise annotation of pulmonary segments is
laborious, as the boundaries between segments are indistinguishable in medical
images. To this end, we propose a weakly supervised learning (WSL) method,
termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise
clinical anatomical definition of pulmonary segments to perform pulmonary
segment segmentation. Since pulmonary segments reside within the lobes and are
determined by the bronchovascular tree, i.e., artery, airway and vein, the
design of the loss function is founded on two principles. First, segment-level
labels are utilized to directly supervise the output of the pulmonary segments,
ensuring that they accurately encompass the appropriate bronchovascular tree.
Second, lobe-level supervision indirectly oversees the pulmonary segment,
ensuring their inclusion within the corresponding lobe. Besides, we introduce a
two-stage segmentation strategy that incorporates bronchovascular priori
information. Furthermore, a consistency loss is proposed to enhance the
smoothness of segment boundaries, along with an evaluation metric designed to
measure the smoothness of pulmonary segment boundaries. Visual inspection and
evaluation metrics from experiments conducted on a private dataset demonstrate
the effectiveness of our method.

</details>


### [727] [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI](https://arxiv.org/abs/2505.14064)
*Cosmin I. Bercea,Jun Li,Philipp Raffler,Evamaria O. Riedel,Lena Schmitzer,Angela Kurz,Felix Bitzer,Paula Roßmüller,Julian Canisius,Mirjam L. Beyrle,Che Liu,Wenjia Bai,Bernhard Kainz,Julia A. Schnabel,Benedikt Wiestler*

Key words: NOVA, 基准测试, 分布外检测, 视觉语言模型, 医学影像

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 论文提出了NOVA基准，评估模型在罕见病理和异构协议下的性能差距。

Motivation: 真实应用中，模型常遇到与训练数据不同的输入，现有基准未能充分评估罕见或新出现的病理情况。

Method: 提出NOVA基准，包含900个脑MRI扫描，涵盖281种罕见病理和异构协议，用于评估异常定位、视觉描述和诊断推理。

Result: 测试显示主流视觉语言模型（如GPT-4o）性能显著下降，NOVA成为评估模型泛化能力的严谨测试平台。

Conclusion: NOVA成功凸显现模型在全新和罕见条件上的性能不足，为未来改进提供方向。

Abstract: In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.

</details>


### [728] [Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images](https://arxiv.org/abs/2505.14572)
*Jayroop Ramesh,Valentin Bacher,Mark C. Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana IL Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale*

Key words: 产程超声、胎儿生物测量、自动化管道、深度学习、角度测量

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 本文提出了一种自动化的胎儿生物测量方法，通过分类、分割和计算三个任务，显著减少了观察者间的变异性，提高了测量的可靠性。

Motivation: 为了减少产程超声监测中的人工误差，提高胎儿头部位置测量的可靠性，以更好地预测器械辅助分娩的结果。

Method: 采用稀疏采样和集成深度学习方法，结合标准平面分类、胎儿头部和耻骨联合分割，以及角度和距离计算。

Result: 在未见的测试数据上，实现了高精度指标（如ACC: 0.9452），证明了方法的有效性。

Conclusion: 自动化管道可以提高对产程停滞原因的理解，并为临床风险分层工具的开发提供支持。

Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [729] [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
*Dan Ofer,Michal Linial,Dafna Shahaf*

Key words: 自动化科学发现,生物医学数据,机器学习,知识图谱,大语言模型

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: 这是一篇关于如何自动化发现生物医学数据中有趣假设的论文，提出了一种结合机器学习、知识图谱、文献搜索和大语言模型的综合方法。

Motivation: 科学发现的核心是发现有趣现象，但这是一个手动且定义模糊的概念。论文旨在通过自动化方法解决这一问题。

Method: 提出的方法结合了机器学习、知识图谱、文献搜索和大语言模型，将“有趣性”量化为新颖性、实用性和合理性的组合。

Result: 在8种主要疾病的实验中，该方法能提前多年发现风险因素，40-53%的候选结果被验证为有趣，远超基线方法的0-7%。

Conclusion: 该方法为解决“有趣性”的可扩展性和普适性挑战提供了有效方案。

Abstract: Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat

</details>


### [730] [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
*Dan Ofer,Michal Linial,Dafna Shahaf*

Key words: 自动化发现、生物医学数据、机器学习、知识图谱、大型语言模型、有趣性

<details>
  <summary>Details</summary>

Main category: q-bio.QM

TL;DR: 提出了一种自动化发现生物医学数据中有趣假设的管道，结合机器学习、知识图谱和大型语言模型，成功预测了疾病风险因素。

Motivation: 科学发现中的有趣现象通常是手动且模糊的概念，本研究旨在自动化这一过程，特别是在生物医学数据中。

Method: 结合机器学习、知识图谱、文献搜索和大型语言模型，将“有趣性”定义为新颖性、实用性和合理性的组合。

Result: 在英国生物银行的8种主要疾病数据上，管道成功预测了风险因素，其中28%的候选假设被医学专家认可。

Conclusion: 该管道为可扩展且通用的“有趣性”操作化提供了解决方案。

Abstract: Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [731] [FreeMesh: Boosting Mesh Generation with Coordinates Merging](https://arxiv.org/abs/2505.13573)
*Jian Liu,Haohan Weng,Biwen Lei,Xianghui Yang,Zibo Zhao,Zhuo Chen,Song Guo,Tao Han,Chunchao Guo*

Key words: 网格标记器, PTME, 坐标合并, 自回归网格生成

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: 提出了一种新的指标PTME来评估网格标记器，并通过坐标合并技术改进现有标记器的压缩率。

Motivation: 当前自回归网格生成方法缺乏对标记器效率的有效测量，急需一种无需训练的评估方法。

Method: 引入PTME指标及坐标合并技术，对现有标记器进行评估和优化。

Result: 通过实验验证，PTME和坐标合并能显著提升现有标记器的性能。

Conclusion: PTME和坐标合并技术为网格生成领域的标记器优化和未来发展提供了指导。

Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard
in current auto-regressive mesh generation methods. Despite their
effectiveness, there is no efficient measurement for the various tokenizers
that serialize meshes into sequences. In this paper, we introduce a new metric
Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers
theoretically without any training. Building upon PTME, we propose a
plug-and-play tokenization technique called coordinate merging. It further
improves the compression ratios of existing tokenizers by rearranging and
merging the most frequent patterns of coordinates. Through experiments on
various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we
further validate the performance of our method. We hope that the proposed PTME
and coordinate merging can enhance the existing mesh tokenizers and guide the
further development of native mesh generation.

</details>


### [732] [FreeMesh: Boosting Mesh Generation with Coordinates Merging](https://arxiv.org/abs/2505.13573)
*Jian Liu,Haohan Weng,Biwen Lei,Xianghui Yang,Zibo Zhao,Zhuo Chen,Song Guo,Tao Han,Chunchao Guo*

Key words: 网格生成，标记化，PTME，坐标合并，自回归模型

<details>
  <summary>Details</summary>

Main category: cs.GR

TL;DR: 提出了一种新的度量标准PTME，用于无训练评估网格标记化方法，并引入坐标合并技术改进现有方法的压缩率。

Motivation: 现有自回归网格生成方法缺乏对标记化器的有效评估标准，需要一种无需训练的理论评估方法。

Method: 引入Per-Token-Mesh-Entropy（PTME）度量标准，并提出坐标合并技术优化标记化器的压缩率。

Result: 在多种标记化方法（如MeshXL、MeshAnything V2和Edgerunner）上验证了PTME和坐标合并技术的性能。

Conclusion: PTME和坐标合并技术能够提升现有网格标记化器的性能，并指导原生网格生成的进一步发展。

Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard
in current auto-regressive mesh generation methods. Despite their
effectiveness, there is no efficient measurement for the various tokenizers
that serialize meshes into sequences. In this paper, we introduce a new metric
Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers
theoretically without any training. Building upon PTME, we propose a
plug-and-play tokenization technique called coordinate merging. It further
improves the compression ratios of existing tokenizers by rearranging and
merging the most frequent patterns of coordinates. Through experiments on
various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we
further validate the performance of our method. We hope that the proposed PTME
and coordinate merging can enhance the existing mesh tokenizers and guide the
further development of native mesh generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [733] [Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping](https://arxiv.org/abs/2505.13777)
*Subash Khanal,Srikumar Sastry,Aayush Dhakal,Adeel Ahmad,Nathan Jacobs*

Key words: 声音景观映射, 多模态学习, 跨模态检索, 对比学习, 声音合成

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Sat2Sound是一个用于声音景观映射的多模态表示学习框架，通过结合卫星图像和声音样本预测全球声音分布。

Motivation: 现有方法依赖卫星图像和配对的音频样本，但难以捕捉声音多样性。Sat2Sound通过增强数据集和改进表示学习来克服这一限制。

Method: 利用Vision-Language Model生成语义丰富的声音景观描述，通过对比学习音频、音频描述、卫星图像及其描述，学习共享的声音景观概念库。

Result: 在GeoSound和SoundingEarth数据集上实现了跨模态检索的最先进性能，并展示了基于位置的声音景观合成新应用。

Conclusion: Sat2Sound在声音景观映射和跨模态学习方面表现优异，并为沉浸式声学体验提供了新方向。

Abstract: We present Sat2Sound, a multimodal representation learning framework for
soundscape mapping, designed to predict the distribution of sounds at any
location on Earth. Existing methods for this task rely on satellite image and
paired geotagged audio samples, which often fail to capture the diversity of
sound sources at a given location. To address this limitation, we enhance
existing datasets by leveraging a Vision-Language Model (VLM) to generate
semantically rich soundscape descriptions for locations depicted in satellite
images. Our approach incorporates contrastive learning across audio, audio
captions, satellite images, and satellite image captions. We hypothesize that
there is a fixed set of soundscape concepts shared across modalities. To this
end, we learn a shared codebook of soundscape concepts and represent each
sample as a weighted average of these concepts. Sat2Sound achieves
state-of-the-art performance in cross-modal retrieval between satellite image
and audio on two datasets: GeoSound and SoundingEarth. Additionally, building
on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a
novel application: location-based soundscape synthesis, which enables immersive
acoustic experiences. Our code and models will be publicly available.

</details>


### [734] [An Edge AI Solution for Space Object Detection](https://arxiv.org/abs/2505.13468)
*Wenxuan Zhang,Peng Hu*

Key words: Edge AI, 空间物体检测, 深度学习, SE层, Vision Transformers, YOLOv9

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种基于Edge AI和深度学习的空间物体检测方法，用于实时碰撞评估与避障，结合SE层、ViT和YOLOv9框架，展示了高精度和低延迟的性能。

Motivation: 随着近地轨道空间资产的增加，需要高效的Edge AI技术以实现实时空间物体检测和碰撞避障。

Method: 采用基于深度学习的视觉感知技术，结合SE层、Vision Transformers和YOLOv9框架，开发了一种新的深度学习模型。

Result: 该模型在多种真实场景中表现出高精度和极低延迟，能够有效检测多个卫星。

Conclusion: 提出的Edge AI解决方案在空间物体检测任务中具备高效性和实用性。

Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate
real-time collision assessment and avoidance is essential with the increasing
space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites
must detect other objects with high precision and minimal delay. We explore an
Edge AI solution based on deep-learning-based vision sensing for SOD tasks and
propose a deep learning model based on Squeeze-and-Excitation (SE) layers,
Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of
these models across various realistic SOD scenarios, demonstrating their
ability to detect multiple satellites with high accuracy and very low latency.

</details>


### [735] [Domain Adaptation of VLM for Soccer Video Understanding](https://arxiv.org/abs/2505.13860)
*Tiancheng Jiang,Henry Wang,Md Sirajus Salekin,Parmida Atighehchian,Shinan Zhang*

Key words: 视觉语言模型, 迁移学习, 足球领域, 课程学习, 任务适应

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了开源视觉语言模型（VLM）在特定领域（如足球）的适应性，通过大规模数据和LLM生成指令数据，以课程学习方式微调模型，显著提升了任务表现。

Motivation: 探索通用VLM在专门领域（如足球）的迁移学习能力，填补了现有研究的空白。

Method: 利用大规模足球数据集和LLM生成指令数据，采用课程学习方式（先学习关键概念，再回答问题）微调VLM。

Result: 最终模型在足球相关任务中表现显著提升，视觉问答任务相对改进37.5%，动作分类任务准确率从11.8%提升至63.5%。

Conclusion: 研究表明，特定领域的数据和课程学习能有效提升VLM的迁移能力，适用于专业领域任务。

Abstract: Vision Language Models (VLMs) have demonstrated strong performance in
multi-modal tasks by effectively aligning visual and textual representations.
However, most video understanding VLM research has been domain-agnostic,
leaving the understanding of their transfer learning capability to specialized
domains under-explored. In this work, we address this by exploring the
adaptability of open-source VLMs to specific domains, and focusing on soccer as
an initial case study. Our approach uses large-scale soccer datasets and LLM to
create instruction-following data, and use them to iteratively fine-tune the
general-domain VLM in a curriculum learning fashion (first teaching the model
key soccer concepts to then question answering tasks). The final adapted model,
trained using a curated dataset of 20k video clips, exhibits significant
improvement in soccer-specific tasks compared to the base model, with a 37.5%
relative improvement for the visual question-answering task and an accuracy
improvement from 11.8% to 63.5% for the downstream soccer action classification
task.

</details>


### [736] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
*Wenjun Hou,Yi Cheng,Kaishuai Xu,Heng Li,Yan Hu,Wenjie Li,Jiang Liu*

Key words: RADAR, 放射学报告生成, 大语言模型, 知识整合

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: RADAR框架通过结合LLMs的内部知识与外部补充知识，提升了放射学报告生成的准确性和信息量。

Motivation: 以往的多模态LLMs方法未充分利用模型内嵌知识，导致信息冗余和效率低下，RADAR旨在解决这一问题。

Method: RADAR首先提取LLM与专家分类结果对齐的知识，再检索外部补充知识，最后整合两者生成报告。

Result: 在MIMIC-CXR、CheXpert-Plus和IU X-ray数据集上，RADAR在语言质量和临床准确性上均优于现有LLMs。

Conclusion: RADAR通过有效整合内部和外部知识，显著提升了放射学报告生成的表现。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [737] [AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards](https://arxiv.org/abs/2505.14029)
*Laura-Sophia von Hirschhausen,Jannes S. Magnusson,Mykyta Kovalenko,Fredrik Boye,Tanay Rawat,Peter Eisert,Anna Hilsmann,Sebastian Pretzsch,Sebastian Bosse*

Key words: 精准农业, 苹果果园监测, 数据集, 立体图像, 生长阶段, 3D建模

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了AppleGrowthVision数据集，解决了苹果果园监测中数据集缺乏多样性和真实性的问题。

Motivation: 苹果果园监测受到数据集限制，缺乏多样性和真实性的数据，现有数据集忽略了不同生长阶段和立体图像的需求。

Method: 提出了包含两个子集的大规模数据集AppleGrowthVision，包括高分辨率立体图像和密集标注图像。

Result: 扩展MinneApple数据集提升了YOLOv8和Faster R-CNN的性能，同时通过VGG16等模型预测BBCH阶段准确率超过95%。

Conclusion: AppleGrowthVision填补了农业科学与计算机视觉之间的空白，为精准农业提供了强大的模型基础。

Abstract: Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.

</details>


### [738] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
*Jiaang Li,Yifei Yuan,Wenyan Li,Mohammad Aliannejadi,Daniel Hershcovich,Anders Søgaard,Ivan Vulić,Wenxuan Zhang,Paul Pu Liang,Yang Deng,Serge Belongie*

Key words: 视觉语言模型,文化理解,检索增强生成,多模态任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了RAVENEA基准，通过检索增强方法提升视觉语言模型对文化细微差别的理解，在cVQA和cIC任务中取得显著改进。

Motivation: 随着视觉语言模型在日常生活中的广泛应用，对文化细微差别的准确理解变得至关重要，但现有模型在这方面表现不足。

Method: 提出RAVENEA基准，整合10,000余篇维基百科文档，训练并评估七种多模态检索器，测试其对14种先进视觉语言模型的影响。

Result: 轻量级视觉语言模型结合检索增强方法在cVQA和cIC任务中分别提升3.2%和6.2%。

Conclusion: 检索增强方法和文化包容性基准对多模态理解具有重要价值。

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [739] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Key words: 会话搜索, 图结构, 大语言模型, 自监督学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出Symbolic Graph Ranker (SGR)，结合文本和图结构方法，利用大语言模型(LLMs)提升会话搜索的语义和结构理解。

Motivation: 当前会话搜索方法侧重顺序建模或结构捕捉，但缺乏对两者结合的探索，且文档表示忽视了词级语义。

Method: 引入符号语法规则将会话图转为文本，结合LLMs输入；设计自监督任务（链接预测、节点生成等）增强LLMs对图结构的理解。

Result: 在AOL和Tiangong-ST数据集上验证了方法的优越性。

Conclusion: SGR为传统搜索策略与现代LLMs间架设了新桥梁。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [740] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
*Tomer Gafni,Asaf Karnieli,Yair Hanani*

Key words: 深度学习,量化,硬件效率,W4A8,双精度量化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种硬件高效的量化与推理方案W4A8，通过4位整数存储权重和8位浮点运算推理，显著提升速度和内存效率，并开发了双精度量化算法以减少精度损失。

Motivation: 随着深度学习模型规模的增大，延迟和内存效率成为挑战，后训练量化成为解决方案。

Method: 提出W4A8方案和双精度量化算法(DPQ)，结合硬件优势进行量化与推理。

Result: 实验显示性能提升(吞吐量增加)，同时保持可接受的精度损失。

Conclusion: W4A8方案在速度和内存效率上表现优越，适用于现代加速器。

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [741] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
*Zikai Liao,Yi Ouyang,Yi-Lun Lee,Chen-Ping Yu,Yi-Hsuan Tsai,Zhaozheng Yin*

Key words: LLM, 多模态, 对话时机, 实时预测, MM-When2Speak

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了LLM聊天机器人在实时对话中何时发言的问题，提出了一个多模态模型MM-When2Speak，显著提升了响应时机预测的准确性。

Motivation: LLM聊天机器人在生成连贯回复时，常因缺乏多模态上下文而难以把握发言时机。论文旨在解决这一问题。

Method: 利用从真实对话视频构建的多模态数据集，结合视觉、听觉和文本信号，提出了MM-When2Speak模型。

Result: MM-When2Speak在响应时机预测上表现优异，比现有技术提升了4倍精度。

Conclusion: 多模态输入对提升对话AI的时效性和自然性至关重要。

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


### [742] [VoQA: Visual-only Question Answering](https://arxiv.org/abs/2505.14227)
*Luyang Jiang,Jianing An,Jie Luo,Wenjun Wu,Lei Huang*

Key words: 视觉问答（VoQA）、多模态任务、视觉嵌入、GRT-SFT

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了视觉问答（VoQA）任务，要求模型仅通过视觉输入回答问题，并设计了GRT-SFT方法来提升模型性能。

Motivation: 现有的多模态模型在处理视觉嵌入的文本问题时表现不佳，需要一种新方法来解决这一挑战。

Method: 提出了GRT-SFT（引导响应触发监督微调），一种结构化微调策略，指导模型基于视觉输入进行逐步推理。

Result: GRT-SFT显著提升了模型在VoQA任务中的表现。

Conclusion: 该方法增强了模型在复杂多模态场景中的视觉理解能力。

Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.

</details>


### [743] [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246)
*Ziyu Liu,Yuhang Zang,Yushan Zou,Zijian Liang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Key words: 大型视觉语言模型、视觉强化微调、多模态代理、图像处理、基准测试

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文探讨了通过视觉强化微调（Visual-ARFT）提升大型视觉语言模型（LVLMs）的多模态代理能力，并在搜索和编码任务中显著优于基线模型和GPT-4o。

Motivation: 目前开源社区在语言代理能力（如函数调用和工具集成）上取得了进展，但涉及图像思考的多模态代理能力及其基准测试仍较少探索。

Method: 提出视觉强化微调（Visual-ARFT）方法，使LVLMs能够浏览网站获取实时信息，并通过编写代码进行图像处理。同时开发多模态代理工具库（MAT）用于评估搜索和编码能力。

Result: Visual-ARFT在MAT-Coding和MAT-Search任务中分别比基线提升18.6% F1和10.3% F1，并显著超过GPT-4o。在2Wiki和HotpotQA等多跳问答基准上也有显著提升。

Conclusion: Visual-ARFT为构建鲁棒且通用性强的多模态代理提供了可行路径。

Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.

</details>


### [744] [CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition](https://arxiv.org/abs/2505.14113)
*Bruno Viti,Elias Karabelas,Martin Holler*

Key words: 图像分割,空间相关性,不确定性量化,一致性预测

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种基于CP框架的方法CONSIGN，通过考虑像素间空间相关性改进图像分割中的不确定性量化。

Motivation: 现有图像分割模型生成的置信度分数缺乏严格的统计有效性，忽视了空间相关性，导致不确定性估计保守且不直观。

Method: 提出CONSIGN方法，结合空间分组分解，利用预训练分割模型生成多样本输出，实现统计有效的不确定性估计。

Result: 实验表明，CONSIGN在多个数据集和模型上显著提升性能，不确定性估计质量优于传统像素级CP方法。

Conclusion: 空间相关性是改进图像分割不确定性量化的重要因素，CONSIGN方法能够提供更具解释性的预测集。

Abstract: Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.

</details>


### [745] [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/abs/2505.14260)
*Luxi Lin,Zhihang Lin,Zhanpeng Zeng,Rongrong Ji*

Key words: 多模态推测解码, MLLMs, 推理加速

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种多模态推测解码（MSD）方法，用于加速多模态大语言模型（MLLMs）的推理。通过重新设计推测解码方法，解决了当前方法无法在MLLMs中实现与LLMs相同加速效果的问题。

Motivation: 当前推测解码方法在多模态大语言模型中的应用效果不佳，未能实现与纯文本模型相同的加速效果，因此需要针对多模态特性重新设计。

Method: MSD根据文本和视觉令牌的特性分别处理，并采用两阶段训练策略：第一阶段训练语言建模能力，第二阶段逐步引入多模态数据增强视觉感知能力。

Result: 实验表明，MSD在LLaVA-1.5-7B和LLaVA-1.5-13B上分别实现了2.29倍和2.46倍的推理加速。

Conclusion: MSD通过针对多模态特性的优化设计，显著提升了MLLMs的推理速度。

Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.

</details>


### [746] [Handloom Design Generation Using Generative Networks](https://arxiv.org/abs/2505.14330)
*Rajat Kanti Bhattacharjee,Meghali Nandi,Amrit Jha,Gunajit Kalita,Ferdous Ahmed Barbhuiya*

Key words: 深度学习, 手织布艺, 生成模型, 风格迁移, NeuralLoom

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 利用深度学习和风格迁移算法生成手织布艺设计，并评估性能。

Motivation: 探索生成神经网络模型在手织布艺设计中的应用潜力。

Method: 结合当前最先进的生成模型和风格迁移算法进行研究。

Result: 通过用户评分评估性能，并发布了新数据集NeuralLoom。

Conclusion: 生成模型在手织布艺设计中具有潜力，NeuralLoom数据集为未来研究提供了支持。

Abstract: This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.

</details>


### [747] [Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image](https://arxiv.org/abs/2505.14341)
*Sifan Li,Ming Tao,Hao Zhao,Ling Shao,Hao Tang*

Key words: 文本到图像,反事实场景,概念对齐,显式逻辑叙述提示,DeepSeek

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本研究关注反事实文本到图像（T2I）中的概念对齐问题，提出了一种称为显式逻辑叙述提示（ELNP）的策略，通过逐步替换潜在空间中的对象来实现从常见场景到反事实场景的转换，并设计了一个评估指标来衡量概念覆盖度。实验结果表明该策略有效提升了概念对齐。

Motivation: 反事实T2I在实际应用中常因缺乏概念对齐而影响生成效果，因此需要一种能够提升概念对齐的方法。

Method: 利用可控T2I模型在潜在空间中逐步替换对象，从常见场景转换为反事实场景，通过显式逻辑叙述提示（ELNP）指导替换过程，并使用DeepSeek语言模型生成指令。

Result: 实验和定性比较表明，该策略显著提升了反事实T2I中的概念对齐效果。

Conclusion: ELNP策略有效解决了反事实T2I中的概念对齐问题，为更灵活的AIGC体验提供了支持。

Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.

</details>


### [748] [Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey](https://arxiv.org/abs/2505.14340)
*Seunghyuk Cho,Zhenyue Qin,Yang Liu,Youngbin Choi,Seungbeom Lee,Dongwoo Kim*

Key words: 平面几何问题求解，多模态推理，编解码器，综述

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文综述了平面几何问题求解（PGPS）研究，分类了编解码器方法，总结了挑战与未来方向。

Motivation: PGPS作为评估多模态模型能力的基准日益重要，但缺乏系统性综述。

Method: 将PGPS方法归类为编解码器框架，分析其输入输出格式及架构设计。

Result: 总结了编解码器的架构分类及当前PGPS的局限性。

Conclusion: 提出PGPS研究中的幻觉问题和数据泄漏问题，指出未来研究方向。

Abstract: Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.

</details>


### [749] [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://arxiv.org/abs/2505.14357)
*Siqiao Huang,Jialong Wu,Qixing Zhou,Shangchen Miao,Mingsheng Long*

Key words: 世界模型, 视频扩散模型, 交互性, 自回归生成, 动作可控性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练视频扩散模型构建交互式世界模型（Vid2World）的方法，通过因果化和动作引导机制提升预测质量与可控性。

Motivation: 现有世界模型需要大量特定领域训练且预测粗粒度，而视频扩散模型能生成高质量视频但缺乏交互性。

Method: Vid2World通过因果化预训练视频扩散模型的架构与训练目标，实现自回归生成，并引入因果动作引导机制提升动作可控性。

Result: 实验表明，Vid2World在机器人操作和游戏模拟领域有效且可扩展。

Conclusion: Vid2World为利用视频扩散模型构建交互式世界模型提供通用方法。

Abstract: World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.

</details>


### [750] [Enhancing Interpretability of Sparse Latent Representations with Class Information](https://arxiv.org/abs/2505.14476)
*Farshad Sangari Abiz,Reshad Hosseini,Babak N. Araabi*

Key words: 变分自编码器，稀疏编码，潜空间解释，结构化表示，损失函数

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出一种新方法，通过同一类别样本共享相似活跃维度，增强变分自编码器潜空间的可解释性。

Motivation: 标准VAEs生成的潜空间分散且无结构，VSC虽引入稀疏表示但缺乏类别内结构一致性。

Method: 设计新损失函数，促使同一类别样本共享相似活跃维度，形成结构化潜空间。

Result: 生成的潜空间更结构化且可解释，同时捕获全局和类别特定因子。

Conclusion: 新方法增强了潜空间的解释性和实用性，优于现有解缠方法。

Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.

</details>


### [751] [Instance Segmentation for Point Sets](https://arxiv.org/abs/2505.14583)
*Abhimanyu Talwar,Julien Laasri*

Key words: 3D点云,实例分割,采样方法,内存优化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文探讨了如何通过采样方法解决3D点云实例分割中的内存消耗问题。

Motivation: 为解决SGPN在实例分割中使用的相似性矩阵内存消耗大的问题。

Method: 使用两种基于采样的方法（随机和子采样）进行实例分割，并通过最近邻方法扩展标签。

Result: 两种方法在大子采样集上表现相当，但随机采样方法在速度和内存使用上更具优势。

Conclusion: 随机采样方法能有效提升速度和内存效率，适用于3D点云实例分割。

Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.

</details>


### [752] [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/abs/2505.14646)
*Anna C. Doris,Md Ferdous Alam,Amin Heyrani Nobari,Faez Ahmed*

Key words: CAD生成, 视觉语言模型, 开源工具, 3D建模

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CAD-Coder是一种基于视觉语言模型（VLM）的开源工具，能够直接从视觉输入生成可编辑的CAD代码（CadQuery Python），显著提升了CAD模型的生成效率和准确性。

Motivation: 当前手动创建CAD模型的工作流程耗时且需要专业知识，而现有的AI驱动CAD生成模型存在表示不完整、泛化能力差和输出准确性低的问题。

Method: 通过基于新创建的GenCAD-Code数据集（包含16.3万个CAD模型图像与代码对）对VLM进行微调，开发出CAD-Coder。

Result: CAD-Coder在语法正确率和3D实体相似度方面优于GPT-4.5和Qwen2.5-VL-72B等基线模型，并展示了泛化能力。

Conclusion: CAD-Coder展示了微调VLM在简化工程师和设计师CAD工作流程中的潜力，且具有公开可用性。

Abstract: Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.

</details>


### [753] [3D Reconstruction from Sketches](https://arxiv.org/abs/2505.14621)
*Abhimanyu Talwar,Julien Laasri*

Key words: 3D重建, 草图, CycleGAN, MegaDepth, 数据集

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种从多张草图重建3D场景的流程，包括草图拼接、CycleGAN转换和深度图估计，但拼接部分对真实绘画泛化性较差。

Motivation: 解决从多张草图重建3D场景的问题，填补草图到3D重建的技术空白。

Method: 1. 通过对应点拼接草图；2. 用CycleGAN将草图转为真实图像；3. 用MegaDepth估计深度图。构建了图像-草图数据集用于训练CycleGAN。

Result: 拼接过程对真实绘画泛化性不佳，但从单张草图生成3D重建的流程表现良好。

Conclusion: 尽管拼接部分存在局限性，整体流程在草图到3D重建中表现有效。

Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.

</details>


### [754] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/abs/2505.14664)
*Yilin Ye,Junchao Huang,Xingchen Zeng,Jiazhi Xia,Wei Zeng*

Key words: 跨模态嵌入, 降维技术, 核回归, 可视化, 文本到图像模型

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种新的降维方法AKRMap，用于更准确地可视化跨模态嵌入，通过学习投影空间中的度量景观的核回归。

Motivation: 传统降维方法（如PCA和t-SNE）主要关注单模态特征分布，无法有效整合跨模态的度量标准（如CLIPScore）。因此需要一种新方法来解决这一问题。

Method: AKRMap通过构建监督投影网络，采用自适应广义核并与投影联合优化的方式，生成能捕捉复杂度量分布的交互式可视化。

Result: 定量实验表明，AKRMap在生成准确和可信度高的可视化效果上优于现有降维方法。

Conclusion: AKRMap为跨模态嵌入的可视化和文本到图像模型的比较提供了有效工具。

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [755] [Training-Free Watermarking for Autoregressive Image Generation](https://arxiv.org/abs/2505.14673)
*Yu Tong,Zihao Pan,Shuai Yang,Kaiyang Zhou*

Key words: 图像水印，自回归模型，编码本，训练无关，鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: IndexMark是一种针对自回归图像生成模型的无训练水印框架，通过替换编码本中的冗余标记来嵌入水印，不影响图像质量，并具有高验证准确性和鲁棒性。

Motivation: 现有水印方法主要针对扩散模型，而自回归图像生成模型的水印研究不足，因此提出IndexMark以填补这一空白。

Method: 利用编码本的冗余特性，通过匹配-替换方法选择水印标记并替换，验证时计算水印标记比例，辅以索引编码器和辅助验证方案。

Result: IndexMark在图像质量、验证准确性和鲁棒性方面表现优异，能抵抗多种干扰和裁剪攻击。

Conclusion: IndexMark为自回归图像生成模型提供了一种高效、高质量的水印解决方案。

Abstract: Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.

</details>


### [756] [Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping](https://arxiv.org/abs/2505.13777)
*Subash Khanal,Srikumar Sastry,Aayush Dhakal,Adeel Ahmad,Nathan Jacobs*

Key words: 多模态学习、声音景观映射、对比学习、共享概念代码本、跨模态检索

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Sat2Sound是一种多模态表征学习框架，用于声音景观映射，通过对比学习和共享概念代码本，提升卫星图像与音频的跨模态检索性能，并实现基于位置的声音景观合成。

Motivation: 现有方法依赖卫星图像和地理标记音频样本，但无法充分捕捉声音多样性，因此提出利用视觉语言模型生成丰富的声音景观描述来解决这一问题。

Method: 结合对比学习和共享概念代码本，将音频、音频描述、卫星图像及图像描述融合学习，通过加权平均表示样本。

Result: Sat2Sound在GeoSound和SoundingEarth数据集上实现了跨模态检索的最优性能，并成功应用于基于位置的声音景观合成。

Conclusion: Sat2Sound有效解决了声音多样性捕捉问题，并为声音景观分析提供了新工具。

Abstract: We present Sat2Sound, a multimodal representation learning framework for
soundscape mapping, designed to predict the distribution of sounds at any
location on Earth. Existing methods for this task rely on satellite image and
paired geotagged audio samples, which often fail to capture the diversity of
sound sources at a given location. To address this limitation, we enhance
existing datasets by leveraging a Vision-Language Model (VLM) to generate
semantically rich soundscape descriptions for locations depicted in satellite
images. Our approach incorporates contrastive learning across audio, audio
captions, satellite images, and satellite image captions. We hypothesize that
there is a fixed set of soundscape concepts shared across modalities. To this
end, we learn a shared codebook of soundscape concepts and represent each
sample as a weighted average of these concepts. Sat2Sound achieves
state-of-the-art performance in cross-modal retrieval between satellite image
and audio on two datasets: GeoSound and SoundingEarth. Additionally, building
on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a
novel application: location-based soundscape synthesis, which enables immersive
acoustic experiences. Our code and models will be publicly available.

</details>


### [757] [An Edge AI Solution for Space Object Detection](https://arxiv.org/abs/2505.13468)
*Wenxuan Zhang,Peng Hu*

Key words: Edge AI, 空间目标检测, 深度学习, SE层, Vision Transformers, YOLOv9

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的Edge AI解决方案，结合SE层、ViT和YOLOv9框架，用于空间目标检测（SOD），实现了高精度和低延迟的检测。

Motivation: 随着近地轨道空间资产的增加，实时碰撞评估和避障的需求推动了高效Edge AI解决方案的研究。

Method: 采用基于SE层、Vision Transformers（ViT）和YOLOv9框架的深度学习模型。

Result: 在多种SOD场景中，模型能够以高精度和极低延迟检测多个卫星。

Conclusion: 该解决方案在空间目标检测任务中表现出色，适用于实时应用。

Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate
real-time collision assessment and avoidance is essential with the increasing
space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites
must detect other objects with high precision and minimal delay. We explore an
Edge AI solution based on deep-learning-based vision sensing for SOD tasks and
propose a deep learning model based on Squeeze-and-Excitation (SE) layers,
Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of
these models across various realistic SOD scenarios, demonstrating their
ability to detect multiple satellites with high accuracy and very low latency.

</details>


### [758] [Domain Adaptation of VLM for Soccer Video Understanding](https://arxiv.org/abs/2505.13860)
*Tiancheng Jiang,Henry Wang,Md Sirajus Salekin,Parmida Atighehchian,Shinan Zhang*

Key words: 视觉语言模型, 迁移学习, 足球, 课程学习, 任务适应

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文探讨了开源视觉语言模型（VLMs）在特定领域（如足球）中的适应性，提出了一种基于课程学习的微调方法，显著提升了模型在足球相关任务中的表现。

Motivation: 尽管视觉语言模型在多模态任务中表现优异，但其在特定领域的迁移学习能力尚未深入探索，本文以足球为例填补了这一空白。

Method: 利用大规模足球数据集和LLM生成指令跟随数据，以课程学习方式（先教授关键概念，再问答任务）迭代微调通用VLM。

Result: 微调后的模型在足球相关任务中表现显著提升，视觉问答任务相对提升37.5%，足球动作分类准确率从11.8%提升至63.5%。

Conclusion: 研究表明，通过领域特定的数据集和课程学习方法，可以显著提升VLM在特定任务中的性能，展现了迁移学习的潜力。

Abstract: Vision Language Models (VLMs) have demonstrated strong performance in
multi-modal tasks by effectively aligning visual and textual representations.
However, most video understanding VLM research has been domain-agnostic,
leaving the understanding of their transfer learning capability to specialized
domains under-explored. In this work, we address this by exploring the
adaptability of open-source VLMs to specific domains, and focusing on soccer as
an initial case study. Our approach uses large-scale soccer datasets and LLM to
create instruction-following data, and use them to iteratively fine-tune the
general-domain VLM in a curriculum learning fashion (first teaching the model
key soccer concepts to then question answering tasks). The final adapted model,
trained using a curated dataset of 20k video clips, exhibits significant
improvement in soccer-specific tasks compared to the base model, with a 37.5%
relative improvement for the visual question-answering task and an accuracy
improvement from 11.8% to 63.5% for the downstream soccer action classification
task.

</details>


### [759] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
*Wenjun Hou,Yi Cheng,Kaishuai Xu,Heng Li,Yan Hu,Wenjie Li,Jiang Liu*

Key words: 大语言模型, 放射学报告生成, 知识检索, 知识整合

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: RADAR框架通过整合LLM内部知识与外部检索信息，提升了放射学报告生成的准确性和信息量。

Motivation: 现有方法忽略了LLM中已嵌入的知识，导致信息冗余和学习表示的低效利用。

Method: RADAR首先提取LLM内部与专家图像分类输出一致的知识，再检索补充信息，最后整合生成报告。

Result: 在多个数据集上，RADAR在语言质量和临床准确性上均优于现有LLM方法。

Conclusion: RADAR通过系统利用内部和外部知识，显著提升了放射学报告生成的效果。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [760] [AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards](https://arxiv.org/abs/2505.14029)
*Laura-Sophia von Hirschhausen,Jannes S. Magnusson,Mykyta Kovalenko,Fredrik Boye,Tanay Rawat,Peter Eisert,Anna Hilsmann,Sebastian Pretzsch,Sebastian Bosse*

Key words: precision agriculture, deep learning, apple orchard monitoring, dataset, 3D reconstruction

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文介绍了AppleGrowthVision数据集，解决了苹果园监测中数据集不足的问题，包括高分辨率立体图像和密集标注图像，显著提升了模型性能。

Motivation: 解决苹果园监测中数据集不足的问题，尤其是缺乏多样化和标注密集的数据，以支持3D建模和生长阶段分析。

Method: 提出AppleGrowthVision数据集，包含高分辨率立体图像和密集标注图像，覆盖多个生长阶段。

Result: 数据集显著提升了YOLOv8和Faster R-CNN的性能，生长阶段预测准确率超过95%。

Conclusion: AppleGrowthVision数据集填补了农业科学与计算机视觉之间的空白，支持水果检测和3D分析。

Abstract: Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.

</details>


### [761] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
*Jiaang Li,Yifei Yuan,Wenyan Li,Mohammad Aliannejadi,Daniel Hershcovich,Anders Søgaard,Ivan Vulić,Wenxuan Zhang,Paul Pu Liang,Yang Deng,Serge Belongie*

Key words: 视觉语言模型、检索增强、文化理解、RAVENEA、多模态任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 研究者提出了RAVENEA基准，通过检索增强方法提升视觉文化理解，在cVQA和cIC任务中取得了显著改进。

Motivation: 现有视觉语言模型（VLMs）在文化细微差别的理解上表现不佳，而检索增强生成（RAG）在多模态场景中的应用尚未充分探索。

Method: 引入RAVENEA基准，结合了10,000多个维基百科文档，训练并评估了七种多模态检索器，并在14种先进VLMs上测试效果。

Result: 检索增强后的轻量级VLMs在cVQA和cIC任务中分别提升至少3.2%和6.2%。

Conclusion: 检索增强方法和文化包容性基准对多模态理解具有重要价值。

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [762] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Key words: 会话搜索, 图结构, 大型语言模型, 符号学习, 自监督学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种符号图排序器（SGR），结合文本和图结构方法，利用大型语言模型（LLM）增强会话搜索的语义和结构建模。

Motivation: 当前会话搜索策略侧重于序列建模，忽略了交互的图结构；而关注结构信息的方法又忽视了词级语义建模。SGR旨在结合两者的优势。

Method: 引入符号语法规则将会话图转换为文本，结合LLM进行自监督学习任务（如链接预测、节点内容生成等），以增强LLM对图结构的理解。

Result: 在AOL和Tiangong-ST数据集上的实验证实了方法的优越性。

Conclusion: SGR为传统搜索策略与现代LLM之间架起了桥梁，提供了一种新颖有效的方法论。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [763] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
*Tomer Gafni,Asaf Karnieli,Yair Hanani*

Key words: 深度神经网络, 量化, 硬件效率, W4A8, DPQ

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种硬件高效的量化与推理方案W4A8，结合4位整数权重存储和8位浮点计算，显著提升速度和内存效率，同时通过DPQ算法减少精度损失。

Motivation: 随着任务复杂度增加，模型规模持续增长，导致延迟和内存效率问题，需要解决量化后的精度损失问题。

Method: 引入W4A8量化方案和Dual Precision Quantization（DPQ）算法，优化硬件使用并减少推理开销。

Result: 实验结果显示，方案显著提升了吞吐量，同时保持了可接受的精度下降。

Conclusion: W4A8量化方案结合DPQ算法能有效提升硬件效率，适用于现代加速器。

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [764] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
*Zikai Liao,Yi Ouyang,Yi-Lun Lee,Chen-Ping Yu,Yi-Hsuan Tsai,Zhaozheng Yin*

Key words: 多模态学习、大型语言模型、对话系统、响应时机预测

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一个多模态模型MM-When2Speak，通过整合视觉、听觉和文本信息，显著提升了对话AI在响应时机和类型预测上的准确性，比现有单模态模型和商业LLM表现更优。

Motivation: 现有LLM聊天机器人在对话中难以准确判断何时及如何简短响应，缺乏真实对话中的多模态上下文线索。

Method: 构建了一个多模态数据集，并基于此提出了MM-When2Speak模型，整合视觉、听觉和文本信息来预测响应时机和类型。

Result: MM-When2Speak在多模态输入下表现优异，响应时机预测准确度比现有商业LLM提高了4倍。

Conclusion: 多模态输入对实现自然、及时的对话AI至关重要。

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


### [765] [VoQA: Visual-only Question Answering](https://arxiv.org/abs/2505.14227)
*Luyang Jiang,Jianing An,Jie Luo,Wenjun Wu,Lei Huang*

Key words: Visual-only Question Answering, VoQA, Guided Response Triggering Supervised Fine-tuning, GRT-SFT, 视觉理解, 多模态任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一个名为VoQA的新型多模态任务，其中问题以视觉方式嵌入图像中，无需文本输入。该任务要求模型定位、识别和推理视觉嵌入的文本问题，对现有的大型视觉语言模型（LVLM）提出了挑战。为此，研究引入了GRT-SFT，一种分步推理的微调策略，显著提升了模型性能。

Motivation: 填补现有模型在纯视觉输入下理解和推理的不足，尤其是当信息（包括语言）以视觉方式呈现时的情况。

Method: 提出了GRT-SFT（引导响应触发监督微调），通过结构化微调策略，指导模型基于纯视觉输入进行分步推理。

Result: 该方法显著提高了模型在VoQA任务中的表现，增强了模型在复杂多模态场景中的视觉理解能力。

Conclusion: GRT-SFT有效提升了模型在纯视觉输入下的推理能力，为复杂多模态任务提供了新思路。

Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.

</details>


### [766] [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246)
*Ziyu Liu,Yuhang Zang,Yushan Zou,Zijian Liang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Key words: 大型视觉语言模型, 多模态代理, Visual-ARFT, MAT测试集, 图像处理, 实时搜索

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文介绍了Visual-ARFT方法，通过视觉代理强化微调提升大型视觉语言模型的多模态代理能力，并在搜索和编码任务中显著优于基线模型和GPT-4o。

Motivation: 当前开源研究社区在多模态代理能力方面探索不足，特别是涉及图像处理和实时信息更新的任务。论文旨在填补这一空白，提升模型的灵活性和适应性。

Method: 提出了Visual-ARFT方法，通过强化微调使大型视觉语言模型具备实时网页浏览和图像处理编码的能力，并设计了MAT测试集评估模型的搜索和编码能力。

Result: 实验显示，Visual-ARFT在MAT-Coding任务中比基线提升18.6% F1 / 13.0% EM，在MAT-Search中提升10.3% F1 / 8.7% EM，超越了GPT-4o，并在多跳问答任务中表现出色。

Conclusion: Visual-ARFT为构建鲁棒且通用性强的多模态代理提供了有效路径。

Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.

</details>


### [767] [CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition](https://arxiv.org/abs/2505.14113)
*Bruno Viti,Elias Karabelas,Martin Holler*

Key words: 图像分割,共形预测,不确定性量化,空间相关性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为CONSIGN的方法，通过结合空间相关性改进图像分割中的不确定性量化。

Motivation: 传统的图像分割模型输出的是启发式的置信度分数，缺乏统计上有效的定量不确定性估计。

Method: 利用共形预测框架，并引入空间相关性，提出了CONSIGN方法。

Result: 实验表明，考虑空间结构显著提升了性能，并增强了不确定性估计的质量。

Conclusion: CONSIGN方法能生成具有高概率误差保证的预测集，适用于各种预训练分割模型。

Abstract: Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.

</details>


### [768] [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/abs/2505.14260)
*Luxi Lin,Zhihang Lin,Zhanpeng Zeng,Rongrong Ji*

Key words: Multimodal Large Language Models, Speculative Decoding, Inference Acceleration

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了Multimodal Speculative Decoding (MSD)方法以加速多模态大语言模型(MLLMs)的推理性能，通过重新设计speculative decoding以适应MLLMs的特性，实验证明其在速度上显著提升。

Motivation: 当前针对MLLMs的speculative decoding方法未能实现与LLMs相同的加速效果，因此需要专门优化以适应多模态特性。

Method: MSD将文本和视觉令牌解耦处理，并采用两阶段训练策略，分别增强语言建模能力和视觉感知能力。

Result: 实验显示，MSD对LLaVA-1.5-7B和LLaVA-1.5-13B的推理速度分别提升了2.29倍和2.46倍。

Conclusion: MSD有效加速了MLLMs的推理速度，同时保持了准确性，为多模态模型的推理优化提供了新思路。

Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.

</details>


### [769] [Handloom Design Generation Using Generative Networks](https://arxiv.org/abs/2505.14330)
*Rajat Kanti Bhattacharjee,Meghali Nandi,Amrit Jha,Gunajit Kalita,Ferdous Ahmed Barbhuiya*

Key words: 深度学习; 手织面料; 设计生成; 风格迁移; NeuralLoom

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 使用深度学习技术生成手织面料服装设计的研究，提出新数据集NeuralLoom，并通过用户评分评估生成效果。

Motivation: 探索生成神经网络在理解与合成艺术设计方面的能力，填补其在手织面料设计中的应用空白。

Method: 结合先进的生成模型和风格迁移算法，研究其在设计任务中的表现。

Result: 通过用户评分对生成设计进行评估，并提供新数据集NeuralLoom。

Conclusion: 深度学习在手织面料设计生成中具有潜力，但仍需进一步研究。

Abstract: This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.

</details>


### [770] [Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image](https://arxiv.org/abs/2505.14341)
*Sifan Li,Ming Tao,Hao Zhao,Ling Shao,Hao Tang*

Key words: 反事实文本到图像,概念对齐,可控T2I模型,ELNP,DeepSeek

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为ELNP的策略，用于提升反事实文本到图像生成中的概念对齐，通过逐步替换潜在空间中的对象来实现，并设计了新的评估指标。

Motivation: 反事实文本到图像（T2I）生成在真实感与概念对齐方面存在挑战，论文旨在解决这一问题。

Method: 利用可控T2I模型逐步替换潜在空间中的对象，使用SoTA语言模型DeepSeek生成逻辑叙事提示（ELNP）。

Result: 实验表明，ELNP策略能显著提升反事实T2I中的概念对齐。

Conclusion: 论文提出的方法有效提升了反事实场景下的图像生成效果。

Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.

</details>


### [771] [Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey](https://arxiv.org/abs/2505.14340)
*Seunghyuk Cho,Zhenyue Qin,Yang Liu,Youngbin Choi,Seungbeom Lee,Dongwoo Kim*

Key words: 平面几何问题解决, 多模态推理, 编码器-解码器, 幻觉问题, 数据泄漏

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 对平面几何问题解决（PGPS）的研究进行了系统综述，总结了编码器-解码器框架及相关挑战。

Motivation: 平面几何问题解决作为评估多模态推理能力的基准，但缺乏系统综述。

Method: 分类总结PGPS方法的编码器-解码器框架及架构设计。

Result: 提出了编码阶段的幻觉问题和基准数据泄漏等挑战。

Conclusion: 未来研究方向包括解决幻觉问题和改进数据泄漏问题。

Abstract: Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.

</details>


### [772] [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://arxiv.org/abs/2505.14357)
*Siqiao Huang,Jialong Wu,Qixing Zhou,Shangchen Miao,Mingsheng Long*

Key words: 世界模型, 视频扩散模型, 因果化, 动作引导, 机器人操作

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Vid2World利用预训练视频扩散模型构建交互式世界模型，通过架构改造和因果动作引导机制提升了预测质量和动作可控性。

Motivation: 现有世界模型需要大量领域特定训练且预测粗糙，限制了在复杂环境中的适用性。视频扩散模型在高质量视频生成方面表现优异，但尚未应用于交互式世界模型。

Method: Vid2World对预训练视频扩散模型进行因果化改造，设计自回归生成架构，并引入因果动作引导机制。

Result: 在机器人操作和游戏模拟领域实验中，Vid2World展示了将视频扩散模型转化为交互式世界模型的有效性和可扩展性。

Conclusion: Vid2World为利用预训练视频扩散模型构建高效世界模型提供了新途径。

Abstract: World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.

</details>


### [773] [Enhancing Interpretability of Sparse Latent Representations with Class Information](https://arxiv.org/abs/2505.14476)
*Farshad Sangari Abiz,Reshad Hosseini,Babak N. Araabi*

Key words: 变分自编码器（VAE）、潜在空间、稀疏编码、可解释性、损失函数

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过确保同一类样本在潜在空间中具有一致的活动维度，增强了变分自编码器（VAE）潜在空间的可解释性。

Motivation: 标准变分自编码器（VAE）生成的潜在空间较为分散且缺乏结构，而变分稀疏编码（VSC）虽然引入了稀疏性，但未能实现同一类样本间的结构化一致性。为了提升潜在空间的解释性和实用性，论文设计了新方法。

Method: 论文引入了一种新的损失函数，该函数鼓励同一类别的样本在潜在空间中共享相似的活动维度，从而形成更具结构的潜在表示。

Result: 所提出的方法能够同时捕捉全局和类别特定的高维概念（“因子”），显著提升了潜在空间的解释性和实用性。

Conclusion: 通过强制同类样本在潜在空间中的维度一致性，该方法为变分自编码器提供了更结构化和可解释的潜在表示。

Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.

</details>


### [774] [Instance Segmentation for Point Sets](https://arxiv.org/abs/2505.14583)
*Abhimanyu Talwar,Julien Laasri*

Key words: 3D点云、实例分割、采样方法、内存优化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文尝试通过两种基于采样的方法解决SGPN中相似矩阵内存占用高的问题，随机采样策略在速度和内存使用方面表现最佳。

Motivation: 解决SGPN在实例分割中使用相似矩阵时带来的内存占用高的问题。

Method: 采用两种基于采样的方法（随机采样和策略采样）在子采样点集上计算实例分割，然后通过最近邻方法将标签扩展到完整点集。

Result: 两种方法在大规模子采样上表现相当，但随机采样策略在速度和内存使用方面改进最大。

Conclusion: 随机采样策略是解决内存问题的有效方法。

Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.

</details>


### [775] [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/abs/2505.14646)
*Anna C. Doris,Md Ferdous Alam,Amin Heyrani Nobari,Faez Ahmed*

Key words: 3D CAD模型, AI驱动, 视觉-语言模型, CAD-Coder, 工程设计

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 这篇论文介绍了CAD-Coder，一个开源的视觉-语言模型，旨在通过视觉输入直接生成可编辑的CAD代码，显著提升了CAD工作流的效率。

Motivation: 当前的CAD模型生成方法效率低且依赖专业知识，AI驱动的CAD生成模型在通用性和输出准确性上存在局限，需要更高效的解决方案。

Method: 使用名为GenCAD-Code的新数据集（包含16.3万对CAD模型图像和代码）对CAD-Coder进行微调，使其能够从视觉输入生成CadQuery Python代码。

Result: CAD-Coder在语法正确率和3D实体相似性上优于现有模型（如GPT-4.5和Qwen2.5-VL-72B），并能处理未见过的CAD操作和真实世界图像。

Conclusion: CAD-Coder展示了视觉-语言模型在CAD工作流中的潜力，能够帮助工程师和设计师更高效地完成任务。

Abstract: Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.

</details>


### [776] [3D Reconstruction from Sketches](https://arxiv.org/abs/2505.14621)
*Abhimanyu Talwar,Julien Laasri*

Key words: 3D重建, 草图, CycleGAN, MegaDepth, 数据集

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种通过多张草图重建3D场景的流程，包括草图拼接、CycleGAN生成真实图像以及深度图估计。虽然拼接效果一般，但其他步骤在多种草图上表现良好。

Motivation: 目标是从多张手绘草图中重建3D场景，解决草图到3D模型的转换问题。

Method: 流程包括：1）通过对应点拼接草图；2）用CycleGAN将拼接图转为真实图像；3）用MegaDepth估计深度图。作者还构建了图像-草图对数据集用于训练。

Result: 拼接步骤对真实绘图效果不佳，但其他步骤能在多种草图上成功生成3D重建。

Conclusion: 该流程在3D场景重建上表现良好，但需改进拼接泛化能力。

Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.

</details>


### [777] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/abs/2505.14664)
*Yilin Ye,Junchao Huang,Xingchen Zeng,Jiazhi Xia,Wei Zeng*

Key words: 跨模态嵌入, 降维技术, 核回归, 可视化, AKRMap

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种新的降维技术AKRMap，用于可视化跨模态嵌入度量，通过学习投影空间中的度量景观核回归，提高了可视化准确性。

Motivation: 传统降维方法（如PCA和t-SNE）主要关注单一模态内的特征分布，忽略了跨模态度量（如CLIPScore）。AKRMap旨在填补这一空白，提供更准确的跨模态嵌入可视化。

Method: AKRMap构建了一个监督投影网络，采用后投影核回归损失，并联合优化自适应广义核，从而高效生成复杂度量分布的可视化结果。

Result: 定量实验表明，AKRMap在生成更准确和可信的可视化方面优于现有降维方法。

Conclusion: AKRMap为跨模态嵌入的可视化和比较提供了有效工具，支持交互功能（如缩放和叠加）。

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [778] [Training-Free Watermarking for Autoregressive Image Generation](https://arxiv.org/abs/2505.14673)
*Yu Tong,Zihao Pan,Shuai Yang,Kaiyang Zhou*

Key words: 图像水印、自回归模型、代码书、鲁棒性、无训练框架

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: IndexMark是一种针对自回归图像生成模型的无训练水印框架，利用代码书的冗余属性，在不影响图像质量的情况下嵌入水印，并通过比例计算实现验证。实验表明其在图像质量和验证准确性上表现优异，且对多种攻击具有鲁棒性。

Motivation: 现有生成水印方法主要针对扩散模型，而自回归图像生成模型的水印方法研究不足。IndexMark旨在填补这一空白，提供一种无训练的、高效的解决方案。

Method: IndexMark基于代码书的冗余属性，采用匹配-替换方法选择水印标记，并通过替换操作嵌入水印。验证时计算水印标记比例，并通过索引编码器提高精度。辅助验证方案增强了对裁剪攻击的鲁棒性。

Result: 实验显示IndexMark在图像质量和验证准确性上达到最优效果，且对裁剪、噪声、高斯模糊等多种扰动具有鲁棒性。

Conclusion: IndexMark为自回归图像生成模型提供了一种无训练、高质量、鲁棒的水印解决方案，填补了现有研究的空白。

Abstract: Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [779] [Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer](https://arxiv.org/abs/2505.14303)
*Rebecca Pelke,José Cubero-Cascante,Nils Bosbach,Niklas Degener,Florian Idrizi,Lennart M. Reimann,Jan Moritz Joseph,Rainer Leupers*

Key words: RRAM, 计算内存, 二进制神经网络, 三元神经网络, 设计空间探索

<details>
  <summary>Details</summary>

Main category: cs.ET

TL;DR: 提出了CIM-Explorer工具包，用于优化基于RRAM交叉阵列的二进制和三元神经网络推理，支持从设计空间探索到编译的全流程。

Motivation: 为解决现有软件工具在RRAM计算内存架构中仅关注单一环节（如编译或模拟）且依赖传统8位量化的问题。

Method: 开发了CIM-Explorer，包含端到端编译器堆栈、多种映射选项和模拟器，支持设计空间探索以评估不同交叉阵列参数和映射的准确性。

Result: 通过案例研究展示了不同映射和交叉阵列参数的预期准确性。

Conclusion: CIM-Explorer为从早期准确性评估到最终芯片实现的整个设计过程提供了全面支持。

Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory
(CIM) architectures offers a promising solution to overcome the von Neumann
bottleneck. Due to non-idealities like cell variability, RRAM crossbars are
often operated in binary mode, utilizing only two states: Low Resistive State
(LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary
Neural Networks (TNNs) are well-suited for this hardware due to their efficient
mapping. Existing software projects for RRAM-based CIM typically focus on only
one aspect: compilation, simulation, or Design Space Exploration (DSE).
Moreover, they often rely on classical 8 bit quantization. To address these
limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN
and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end
compiler stack, multiple mapping options, and simulators, enabling a DSE flow
for accuracy estimation across different crossbar parameters and mappings.
CIM-Explorer can accompany the entire design process, from early accuracy
estimation for specific crossbar parameters, to selecting an appropriate
mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case
studies, we demonstrate the expected accuracy for various mappings and crossbar
parameters. CIM-Explorer can be found on GitHub.

</details>


### [780] [Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer](https://arxiv.org/abs/2505.14303)
*Rebecca Pelke,José Cubero-Cascante,Nils Bosbach,Niklas Degener,Florian Idrizi,Lennart M. Reimann,Jan Moritz Joseph,Rainer Leupers*

Key words: RRAM, CIM, BNN, TNN, CIM-Explorer, Design Space Exploration

<details>
  <summary>Details</summary>

Main category: cs.ET

TL;DR: CIM-Explorer是一个用于优化BNN和TNN在RRAM交叉阵列上推理的模块化工具包，解决了现有工具在编译、仿真或DSE上的单一性以及8位量化的局限性。

Motivation: 克服von Neumann瓶颈，利用RRAM交叉阵列在CIM架构中的潜力，同时解决现有软件工具在功能上的局限性和不足。

Method: 开发了一个包含端到端编译器栈、多种映射选项和仿真器的工具包CIM-Explorer，支持从准确性估计到芯片设计的全流程。

Result: 通过DSE案例研究展示了不同映射和交叉阵列参数的预期准确性，工具包已在GitHub上开源。

Conclusion: CIM-Explorer填补了现有工具的空白，为BNN和TNN在RRAM交叉阵列上的高效实现提供了全面支持。

Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory
(CIM) architectures offers a promising solution to overcome the von Neumann
bottleneck. Due to non-idealities like cell variability, RRAM crossbars are
often operated in binary mode, utilizing only two states: Low Resistive State
(LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary
Neural Networks (TNNs) are well-suited for this hardware due to their efficient
mapping. Existing software projects for RRAM-based CIM typically focus on only
one aspect: compilation, simulation, or Design Space Exploration (DSE).
Moreover, they often rely on classical 8 bit quantization. To address these
limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN
and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end
compiler stack, multiple mapping options, and simulators, enabling a DSE flow
for accuracy estimation across different crossbar parameters and mappings.
CIM-Explorer can accompany the entire design process, from early accuracy
estimation for specific crossbar parameters, to selecting an appropriate
mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case
studies, we demonstrate the expected accuracy for various mappings and crossbar
parameters. CIM-Explorer can be found on GitHub.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [781] [Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation Energy Materials](https://arxiv.org/abs/2505.13494)
*Ying Zhao,Guanhua Chen,Jie Liu*

Key words: 聚合物科学, 数据碎片化, 机器学习, FAIR原则, 自主实验

<details>
  <summary>Details</summary>

Main category: cond-mat.soft

TL;DR: 论文探讨了聚合物科学中数据碎片化问题，阻碍了能源技术的材料发现，提出了通过技术与合作创新解决这些问题的方案。

Motivation: 能源技术中聚合物材料的复杂性和数据碎片化问题，限制了机器学习的应用和材料发现，亟需解决方案。

Method: 采用自然语言处理工具从文献中提取结构化数据，结合高通量机器人平台生成自洽数据集，并应用FAIR原则。

Result: 通过技术和协作创新，逐步解决数据碎片化问题，为聚合物材料的发现和机器学习应用提供支持。

Conclusion: 未来突破依赖于开放科学文化、去中心化数据市场和自主实验室的融合，以技术创新解决数据障碍。

Abstract: The pursuit of advanced polymers for energy technologies, spanning
photovoltaics, solid-state batteries, and hydrogen storage, is hindered by
fragmented data ecosystems that fail to capture the hierarchical complexity of
these materials. Polymer science lacks interoperable databases, forcing
reliance on disconnected literature and legacy records riddled with
unstructured formats and irreproducible testing protocols. This fragmentation
stifles machine learning (ML) applications and delays the discovery of
materials critical for global decarbonization. Three systemic barriers compound
the challenge. First, academic-industrial data silos restrict access to
proprietary industrial datasets, while academic publications often omit
critical synthesis details. Second, inconsistent testing methods undermine
cross-study comparability. Third, incomplete metadata in existing databases
limits their utility for training reliable ML models. Emerging solutions
address these gaps through technological and collaborative innovation. Natural
language processing (NLP) tools extract structured polymer data from decades of
literature, while high-throughput robotic platforms generate self-consistent
datasets via autonomous experimentation. Central to these advances is the
adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles,
adapted to polymer-specific ontologies, ensuring machine-readability and
reproducibility. Future breakthroughs hinge on cultural shifts toward open
science, accelerated by decentralized data markets and autonomous laboratories
that merge robotic experimentation with real-time ML validation. By addressing
data fragmentation through technological innovation, collaborative governance,
and ethical stewardship, the polymer community can transform bottlenecks into
accelerants.

</details>


### [782] [Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation Energy Materials](https://arxiv.org/abs/2505.13494)
*Ying Zhao,Guanhua Chen,Jie Liu*

Key words: 聚合物科学, 数据碎片化, 机器学习, FAIR原则, 开放式科学

<details>
  <summary>Details</summary>

Main category: cond-mat.soft

TL;DR: 论文讨论了聚合物科学中数据碎片化的问题，阻碍了机器学习应用和能源材料的发现，并提出了通过技术创新、FAIR原则和协作治理来解决这些挑战。

Motivation: 动机在于聚合物科学中数据生态系统的不连贯性阻碍了能源技术中的聚合物发展，尤其是机器学习应用和材料发现。

Method: 方法包括使用自然语言处理工具从文献中提取结构化数据，以及高通量机器人平台生成自洽数据集；同时采用FAIR原则确保数据的可读性和可重现性。

Result: 结果展示了技术创新的潜力，如通过开放式科学和分散数据市场加速材料发现，解决数据碎片化问题。

Conclusion: 结论是聚合物科学界可以通过技术创新和协作治理将数据瓶颈转化为加速器，推动能源材料的发展。

Abstract: The pursuit of advanced polymers for energy technologies, spanning
photovoltaics, solid-state batteries, and hydrogen storage, is hindered by
fragmented data ecosystems that fail to capture the hierarchical complexity of
these materials. Polymer science lacks interoperable databases, forcing
reliance on disconnected literature and legacy records riddled with
unstructured formats and irreproducible testing protocols. This fragmentation
stifles machine learning (ML) applications and delays the discovery of
materials critical for global decarbonization. Three systemic barriers compound
the challenge. First, academic-industrial data silos restrict access to
proprietary industrial datasets, while academic publications often omit
critical synthesis details. Second, inconsistent testing methods undermine
cross-study comparability. Third, incomplete metadata in existing databases
limits their utility for training reliable ML models. Emerging solutions
address these gaps through technological and collaborative innovation. Natural
language processing (NLP) tools extract structured polymer data from decades of
literature, while high-throughput robotic platforms generate self-consistent
datasets via autonomous experimentation. Central to these advances is the
adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles,
adapted to polymer-specific ontologies, ensuring machine-readability and
reproducibility. Future breakthroughs hinge on cultural shifts toward open
science, accelerated by decentralized data markets and autonomous laboratories
that merge robotic experimentation with real-time ML validation. By addressing
data fragmentation through technological innovation, collaborative governance,
and ethical stewardship, the polymer community can transform bottlenecks into
accelerants.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [783] [Pel, A Programming Language for Orchestrating AI Agents](https://arxiv.org/abs/2505.13453)
*Behnam Mohammadi*

Key words: 大型语言模型（LLM）、Pel编程语言、功能控制、安全性、AI框架

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 本文提出了一种新的编程语言Pel，旨在解决大型语言模型（LLM）在复杂控制、表达性和安全性方面的局限性，提供更高效、安全的框架。

Motivation: 当前LLM的控制和编排方法在表达性、可扩展性、成本和安全性方面存在不足，Pel旨在填补这一空白。

Method: Pel结合了Lisp、Elixir、Gleam和Haskell的优点，提供语法简单、易于修改的编程语言，支持复杂操作和高效控制。

Result: Pel通过高级功能（如管道机制、闭包支持及自然语言条件）实现了更稳健和安全的LLM编排。

Conclusion: Pel为LLM编排提供了更强大且可靠的新范式，推动AI框架的进一步发展。

Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in
computing, yet controlling and orchestrating their capabilities beyond simple
text generation remains a challenge. Current methods, such as function/tool
calling and direct code generation, suffer from limitations in expressiveness,
scalability, cost, security, and the ability to enforce fine-grained control.
This paper introduces Pel, a novel programming language specifically designed
to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and
Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich
platform for LLMs to express complex actions, control flow, and inter-agent
communication safely and efficiently. Pel's design emphasizes a minimal, easily
modifiable grammar suitable for constrained LLM generation, eliminating the
need for complex sandboxing by enabling capability control at the syntax level.
Key features include a powerful piping mechanism for linear composition,
first-class closures enabling easy partial application and functional patterns,
built-in support for natural language conditions evaluated by LLMs, and an
advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and
LLM-powered helper agents for automated error correction. Furthermore, Pel
incorporates automatic parallelization of independent operations via static
dependency analysis, crucial for performant agentic systems. We argue that Pel
offers a more robust, secure, and expressive paradigm for LLM orchestration,
paving the way for more sophisticated and reliable AI agentic frameworks.

</details>


### [784] [RTL++: Graph-enhanced LLM for RTL Code Generation](https://arxiv.org/abs/2505.13479)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Key words: 电子设计自动化, 大语言模型, RTL代码生成, 图表示

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 论文提出了RTL++方法，利用图表示增强LLM在RTL代码生成中的性能。

Motivation: 传统RTL设计方法繁琐且易错，商用LLMs存在安全隐私问题，而开源LLMs因数据质量不足表现不佳。

Method: 通过将RTL代码编码为控制流图和数据流图的文本形式，增强LLM对代码结构的理解。

Result: RTL++在VerilogEval基准测试中优于当前最优模型，证明图增强背景的有效性。

Conclusion: RTL++通过图表示提升了LLM辅助RTL代码生成的质量和能力。

Abstract: As hardware design complexity escalates, there is an urgent need for advanced
automation in electronic design automation (EDA). Traditional register transfer
level (RTL) design methods are manual, time-consuming, and prone to errors.
While commercial (instruction-tuned) large language models (LLMs) shows
promising performance for automation, they pose security and privacy concerns.
Open-source models offer alternatives; however, they frequently fall short in
quality/correctness, largely due to limited, high-quality RTL code data
essential for effective training and generalization. This paper proposes RTL++,
a first-of-its-kind LLM-assisted method for RTL code generation that utilizes
graph representations of code structures to enhance the quality of generated
code. By encoding RTL code into a textualized control flowgraphs (CFG) and data
flow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and
relationships within the code. This structured graph-based approach enhances
the context available to LLMs, enabling them to better understand and generate
instructions. By focusing on data generation through graph representations,
RTL++ addresses the limitations of previous approaches that rely solely on code
and suffer from lack of diversity. Experimental results demonstrate that RTL++
outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated
using the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1
model, which highlight the effectiveness of graph-enhanced context in advancing
the capabilities of LLM-assisted RTL code generation.

</details>


### [785] [Pel, A Programming Language for Orchestrating AI Agents](https://arxiv.org/abs/2505.13453)
*Behnam Mohammadi*

Key words: 大型语言模型,编程语言,AI协调,语法设计,并行化

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: Pel是一种新型编程语言，专为解决大型语言模型（LLMs）在复杂任务中的控制和协调问题而设计。其设计灵感来源于Lisp、Elixir、Gleam和Haskell，提供简洁、高效且安全的语法平台。

Motivation: 现有方法在表达力、扩展性、成本、安全性及精细控制方面存在局限，Pel旨在填补这一空白。

Method: Pel结合多种编程语言的优点，设计了最小化且易于修改的语法，支持线性组合、闭包、自然语言条件和自动并行化等功能。

Result: Pel能够更稳健、安全且高效地协调LLMs，适用于复杂的AI代理框架。

Conclusion: Pel为LLMs协调提供了更强大的编程范式，推动了复杂AI系统的发展。

Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in
computing, yet controlling and orchestrating their capabilities beyond simple
text generation remains a challenge. Current methods, such as function/tool
calling and direct code generation, suffer from limitations in expressiveness,
scalability, cost, security, and the ability to enforce fine-grained control.
This paper introduces Pel, a novel programming language specifically designed
to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and
Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich
platform for LLMs to express complex actions, control flow, and inter-agent
communication safely and efficiently. Pel's design emphasizes a minimal, easily
modifiable grammar suitable for constrained LLM generation, eliminating the
need for complex sandboxing by enabling capability control at the syntax level.
Key features include a powerful piping mechanism for linear composition,
first-class closures enabling easy partial application and functional patterns,
built-in support for natural language conditions evaluated by LLMs, and an
advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and
LLM-powered helper agents for automated error correction. Furthermore, Pel
incorporates automatic parallelization of independent operations via static
dependency analysis, crucial for performant agentic systems. We argue that Pel
offers a more robust, secure, and expressive paradigm for LLM orchestration,
paving the way for more sophisticated and reliable AI agentic frameworks.

</details>


### [786] [RTL++: Graph-enhanced LLM for RTL Code Generation](https://arxiv.org/abs/2505.13479)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Key words: 电子设计自动化，RTL代码生成，大型语言模型，控制流图，数据流图

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: RTL++是一种利用图表示代码结构的LLM辅助方法，旨在提升RTL代码生成的质量和效率。

Motivation: 由于传统RTL设计方法手动且易错，而商业LLM存在安全和隐私问题，开源模型又缺乏高质量数据，因此需要开发更高效的自动化方法。

Method: RTL++通过将RTL代码编码为控制流图（CFG）和数据流图（DFG），利用图结构增强LLM对代码上下文的理解，从而生成更高质量的代码。

Result: 实验结果表明，RTL++在VerilogEval基准测试中的Pass@1/5/10指标上优于现有最优模型，验证了图增强上下文的有效性。

Conclusion: RTL++通过图表示方法显著提升了LLM辅助RTL代码生成的能力，解决了传统方法的局限性。

Abstract: As hardware design complexity escalates, there is an urgent need for advanced
automation in electronic design automation (EDA). Traditional register transfer
level (RTL) design methods are manual, time-consuming, and prone to errors.
While commercial (instruction-tuned) large language models (LLMs) shows
promising performance for automation, they pose security and privacy concerns.
Open-source models offer alternatives; however, they frequently fall short in
quality/correctness, largely due to limited, high-quality RTL code data
essential for effective training and generalization. This paper proposes RTL++,
a first-of-its-kind LLM-assisted method for RTL code generation that utilizes
graph representations of code structures to enhance the quality of generated
code. By encoding RTL code into a textualized control flowgraphs (CFG) and data
flow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and
relationships within the code. This structured graph-based approach enhances
the context available to LLMs, enabling them to better understand and generate
instructions. By focusing on data generation through graph representations,
RTL++ addresses the limitations of previous approaches that rely solely on code
and suffer from lack of diversity. Experimental results demonstrate that RTL++
outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated
using the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1
model, which highlight the effectiveness of graph-enhanced context in advancing
the capabilities of LLM-assisted RTL code generation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [787] [Autonomous nanoparticle synthesis by design](https://arxiv.org/abs/2505.13571)
*Andy S. Anker,Jonas H. Jensen,Miguel Gonzalez-Duque,Rodrigo Moreno,Aleksandra Smolska,Mikkel Juelsholt,Vincent Hardion,Mads R. V. Jorgensen,Andres Faina,Jonathan Quinson,Kasper Stoy,Tejs Vegge*

Key words: 纳米颗粒，自主合成，总散射数据，原子结构，材料设计

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出了一种自主设计合成协议的方法，通过实时匹配实验数据与模拟目标模式，成功合成了两种不同结构的金纳米颗粒，无需先验合成知识。

Motivation: 传统材料合成依赖试错方法，纳米颗粒的原子排列对其性质至关重要，但合成参数繁多，挑战巨大。

Method: 采用自主方法，实时匹配实验总散射数据和模拟目标模式，设计合成协议。

Result: 成功合成了5 nm十面体和10 nm面心立方结构的金纳米颗粒。

Conclusion: 该方法为自主、原子级结构定向合成提供了通用蓝图，可能彻底改变材料设计。

Abstract: Controlled synthesis of materials with specified atomic structures underpins
technological advances yet remains reliant on iterative, trial-and-error
approaches. Nanoparticles (NPs), whose atomic arrangement dictates their
emergent properties, are particularly challenging to synthesise due to numerous
tunable parameters. Here, we introduce an autonomous approach explicitly
targeting synthesis of atomic-scale structures. Our method autonomously designs
synthesis protocols by matching real time experimental total scattering (TS)
and pair distribution function (PDF) data to simulated target patterns, without
requiring prior synthesis knowledge. We demonstrate this capability at a
synchrotron, successfully synthesising two structurally distinct gold NPs: 5 nm
decahedral and 10 nm face-centred cubic structures. Ultimately, specifying a
simulated target scattering pattern, thus representing a bespoke atomic
structure, and obtaining both the synthesised material and its reproducible
synthesis protocol on demand may revolutionise materials design. Thus,
ScatterLab provides a generalisable blueprint for autonomous, atomic
structure-targeted synthesis across diverse systems and applications.

</details>


### [788] [Path-integral molecular dynamics with actively-trained and universal machine learning force fields](https://arxiv.org/abs/2505.14245)
*A. A. Solovykh,N. E. Rybin,I. S. Novikov,A. V. Shapeev*

Key words: 核量子效应, 路径积分分子动力学, 矩张量势能, 机器学习势能, 材料性质

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 该论文探讨了通过路径积分分子动力学（PIMD）结合机器学习势能（MTP）来研究核量子效应（NQEs）对材料性质的影响，实现了高精度且计算高效的方法。

Motivation: 核量子效应（NQEs）在有限温度下会显著改变材料性质，但传统方法如经验势能缺乏精度，量子力学计算则计算成本高。机器学习势能提供了一种平衡方案。

Method: 开发了一个接口，将MLIP-2软件包中的矩张量势能（MTP）与i-PI软件包中的PIMD计算集成，用于主动学习势能和研究NQEs。

Result: 通过锂氢化物（LiH）和硅（Si）系统的晶格参数、热膨胀系数和径向分布函数的研究，MTP-PIMD方法展示了高精度和有效性。

Conclusion: MTP-PIMD方法在保持计算效率的同时，提供了接近量子力学精度的结果，适用于研究NQEs对材料性质的影响。

Abstract: Accounting for nuclear quantum effects (NQEs) can significantly alter
material properties at finite temperatures. Atomic modeling using the
path-integral molecular dynamics (PIMD) method can fully account for such
effects, but requires computationally efficient and accurate models of
interatomic interactions. Empirical potentials are fast but may lack sufficient
accuracy, whereas quantum-mechanical calculations are highly accurate but
computationally expensive. Machine-learned interatomic potentials offer a
solution to this challenge, providing near-quantum-mechanical accuracy while
maintaining high computational efficiency compared to density functional theory
(DFT) calculations. In this context, an interface was developed to integrate
moment tensor potentials (MTPs) from the MLIP-2 software package into PIMD
calculations using the i-PI software package. This interface was then applied
to active learning of potentials and to investigate the influence of NQEs on
material properties, namely the temperature dependence of lattice parameters
and thermal expansion coefficients, as well as radial distribution functions,
for lithium hydride (LiH) and silicon (Si) systems. The results were compared
with experimental data, quasi-harmonic approximation calculations, and
predictions from the universal machine learning force field MatterSim. These
comparisons demonstrated the high accuracy and effectiveness of the MTP-PIMD
approach.

</details>


### [789] [Autonomous nanoparticle synthesis by design](https://arxiv.org/abs/2505.13571)
*Andy S. Anker,Jonas H. Jensen,Miguel Gonzalez-Duque,Rodrigo Moreno,Aleksandra Smolska,Mikkel Juelsholt,Vincent Hardion,Mads R. V. Jorgensen,Andres Faina,Jonathan Quinson,Kasper Stoy,Tejs Vegge*

Key words: 纳米颗粒、自主合成、原子结构、散射数据、材料设计

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 论文介绍了一种自主设计合成纳米颗粒原子结构的方法，通过实时匹配实验数据与模拟目标模式，无需先验知识，成功合成了两种金纳米颗粒。

Motivation: 传统材料合成依赖试错法，纳米颗粒的原子结构难以精确控制，亟需自主化方法实现原子级精准合成。

Method: 利用实时实验散射数据与目标模拟模式匹配，自主设计合成方案，成功在同步辐射实验中得到验证。

Result: 成功合成了5纳米十面体和10纳米面心立方两种金纳米颗粒，证明了方法的高效性和可重复性。

Conclusion: ScatterLab提供了一种通用的自主合成框架，为精准设计原子结构材料开辟了新途径。

Abstract: Controlled synthesis of materials with specified atomic structures underpins
technological advances yet remains reliant on iterative, trial-and-error
approaches. Nanoparticles (NPs), whose atomic arrangement dictates their
emergent properties, are particularly challenging to synthesise due to numerous
tunable parameters. Here, we introduce an autonomous approach explicitly
targeting synthesis of atomic-scale structures. Our method autonomously designs
synthesis protocols by matching real time experimental total scattering (TS)
and pair distribution function (PDF) data to simulated target patterns, without
requiring prior synthesis knowledge. We demonstrate this capability at a
synchrotron, successfully synthesising two structurally distinct gold NPs: 5 nm
decahedral and 10 nm face-centred cubic structures. Ultimately, specifying a
simulated target scattering pattern, thus representing a bespoke atomic
structure, and obtaining both the synthesised material and its reproducible
synthesis protocol on demand may revolutionise materials design. Thus,
ScatterLab provides a generalisable blueprint for autonomous, atomic
structure-targeted synthesis across diverse systems and applications.

</details>


### [790] [Path-integral molecular dynamics with actively-trained and universal machine learning force fields](https://arxiv.org/abs/2505.14245)
*A. A. Solovykh,N. E. Rybin,I. S. Novikov,A. V. Shapeev*

Key words: 核量子效应, 路径积分分子动力学, 动量张量势, 机器学习, 材料性质

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

TL;DR: 该论文开发了一个接口，将MLIP-2软件包中的动量张量势（MTP）集成到i-PI软件包的路径积分分子动力学（PIMD）计算中，用于研究核量子效应（NQE）对材料性质的影响。

Motivation: 在有限温度下，核量子效应（NQE）可以显著改变材料性质。传统的经验势方法计算速度快但精度不足，而量子力学计算精度高但计算成本昂贵。机器学习原子间势提供了一种两全其美的解决方案。

Method: 通过开发一个接口，将动量张量势（MTP）与路径积分分子动力学（PIMD）相结合，同时采用主动学习优化势函数，研究了锂氢化物（LiH）和硅（Si）系统的热力学性质。

Result: 与实验数据、准谐近似计算以及MatterSim机器学习力场的预测结果相比，MTP-PIMD方法在计算晶格参数、热膨胀系数和径向分布函数时表现出高精度和有效性。

Conclusion: MTP-PIMD方法为研究核量子效应提供了高效且精确的计算工具，在材料性质模拟中具有重要应用价值。

Abstract: Accounting for nuclear quantum effects (NQEs) can significantly alter
material properties at finite temperatures. Atomic modeling using the
path-integral molecular dynamics (PIMD) method can fully account for such
effects, but requires computationally efficient and accurate models of
interatomic interactions. Empirical potentials are fast but may lack sufficient
accuracy, whereas quantum-mechanical calculations are highly accurate but
computationally expensive. Machine-learned interatomic potentials offer a
solution to this challenge, providing near-quantum-mechanical accuracy while
maintaining high computational efficiency compared to density functional theory
(DFT) calculations. In this context, an interface was developed to integrate
moment tensor potentials (MTPs) from the MLIP-2 software package into PIMD
calculations using the i-PI software package. This interface was then applied
to active learning of potentials and to investigate the influence of NQEs on
material properties, namely the temperature dependence of lattice parameters
and thermal expansion coefficients, as well as radial distribution functions,
for lithium hydride (LiH) and silicon (Si) systems. The results were compared
with experimental data, quasi-harmonic approximation calculations, and
predictions from the universal machine learning force field MatterSim. These
comparisons demonstrated the high accuracy and effectiveness of the MTP-PIMD
approach.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [791] [Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs](https://arxiv.org/abs/2505.13572)
*Yousouf Taghzouti,Franck Michel,Tao Jiang,Louis-Félix Nothias,Fabien Gandon*

Key words: SPARQL, 知识图谱, 能力问题, 大型语言模型, 查询生成

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 为了解决非专家用户和专家用户在SPARQL查询构建上的困难，Q²Forge工具通过迭代验证和人类反馈生成新能力问题和SPARQL查询。

Motivation: SPARQL查询是访问知识图谱的标准方法，但构建这些查询对非专家用户和专家用户来说都很耗时。现有的能力问题和查询示例往往不足，需要一种自动生成高质量查询的方法。

Method: 使用Q²Forge工具，通过迭代验证和人类反馈生成新能力问题和SPARQL查询。工具是开源的、通用的、可扩展的和模块化的。

Result: Q²Forge提供了一个完整的流程，从能力问题生成到查询评估，支持为任何目标知识图谱创建参考查询集。

Conclusion: Q²Forge通过结合人类反馈和LLM验证，有效解决了生成高质量SPARQL查询的挑战。

Abstract: The SPARQL query language is the standard method to access knowledge graphs
(KGs). However, formulating SPARQL queries is a significant challenge for
non-expert users, and remains time-consuming for the experienced ones. Best
practices recommend to document KGs with competency questions and example
queries to contextualise the knowledge they contain and illustrate their
potential applications. In practice, however, this is either not the case or
the examples are provided in limited numbers. Large Language Models (LLMs) are
being used in conversational agents and are proving to be an attractive
solution with a wide range of applications, from simple question-answering
about common knowledge to generating code in a targeted programming language.
However, training and testing these models to produce high quality SPARQL
queries from natural language questions requires substantial datasets of
question-query pairs. In this paper, we present Q${}^2$Forge that addresses the
challenge of generating new competency questions for a KG and corresponding
SPARQL queries. It iteratively validates those queries with human feedback and
LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,
meaning that the different modules of the application (CQ generation, query
generation and query refinement) can be used separately, as an integrated
pipeline, or replaced by alternative services. The result is a complete
pipeline from competency question formulation to query evaluation, supporting
the creation of reference query sets for any target KG.

</details>


### [792] [Abacus: A Cost-Based Optimizer for Semantic Operator Systems](https://arxiv.org/abs/2505.14661)
*Matthew Russo,Sivaprasad Sudhir,Gerardo Vitagliano,Chunwei Liu,Tim Kraska,Samuel Madden,Michael Cafarella*

Key words: Abacus, 语义操作符, 优化器, LLM, 数据

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: 本文介绍了Abacus，一种用于优化语义操作符系统的成本优化器，通过最小验证样本和先验知识提升性能。

Motivation: 现有语义操作符优化器在多维优化（如质量、成本、延迟）方面能力有限，Abacus旨在解决这一问题。

Method: Abacus利用验证样本和先验知识搜索最优实现，支持多目标优化。

Result: 在生物医学、法律领域及多模态问答任务中，Abacus优化系统质量提升18.7%-39.2%，成本降低23.6倍，延迟减少4.2倍。

Conclusion: Abacus显著提升了语义操作符系统的性能，适用于多维优化场景。

Abstract: LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.

</details>


### [793] [Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs](https://arxiv.org/abs/2505.13572)
*Yousouf Taghzouti,Franck Michel,Tao Jiang,Louis-Félix Nothias,Fabien Gandon*

Key words: SPARQL, 知识图谱, 能力问题, 大型语言模型, Q²Forge

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: Q²Forge 是一个开源工具，用于为知识图谱生成新的能力问题和对应的 SPARQL 查询，并通过人工反馈和大型语言模型验证查询质量。

Motivation: SPARQL 查询对非专业用户具有挑战性，且现有知识图谱的示例查询不足。Q²Forge 通过自动生成和验证查询对，支持为任何知识图谱创建参考查询集。

Method: Q²Forge 使用模块化设计，迭代生成能力问题和 SPARQL 查询，并通过人工反馈和 LLM 作为裁判进行验证。

Result: Q²Forge 提供了一个完整的流水线，从问题生成到查询评估，支持为任何知识图谱生成高质量的参考查询集。

Conclusion: Q²Forge 是一个通用、可扩展的工具，能够有效解决知识图谱中示例查询不足的问题，提升 SPARQL 查询的可访问性。

Abstract: The SPARQL query language is the standard method to access knowledge graphs
(KGs). However, formulating SPARQL queries is a significant challenge for
non-expert users, and remains time-consuming for the experienced ones. Best
practices recommend to document KGs with competency questions and example
queries to contextualise the knowledge they contain and illustrate their
potential applications. In practice, however, this is either not the case or
the examples are provided in limited numbers. Large Language Models (LLMs) are
being used in conversational agents and are proving to be an attractive
solution with a wide range of applications, from simple question-answering
about common knowledge to generating code in a targeted programming language.
However, training and testing these models to produce high quality SPARQL
queries from natural language questions requires substantial datasets of
question-query pairs. In this paper, we present Q${}^2$Forge that addresses the
challenge of generating new competency questions for a KG and corresponding
SPARQL queries. It iteratively validates those queries with human feedback and
LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,
meaning that the different modules of the application (CQ generation, query
generation and query refinement) can be used separately, as an integrated
pipeline, or replaced by alternative services. The result is a complete
pipeline from competency question formulation to query evaluation, supporting
the creation of reference query sets for any target KG.

</details>


### [794] [Abacus: A Cost-Based Optimizer for Semantic Operator Systems](https://arxiv.org/abs/2505.14661)
*Matthew Russo,Sivaprasad Sudhir,Gerardo Vitagliano,Chunwei Liu,Tim Kraska,Samuel Madden,Michael Cafarella*

Key words: 语义操作, LLM, 优化器, Abacus, 数据处理

<details>
  <summary>Details</summary>

Main category: cs.DB

TL;DR: Abacus是一种可扩展的基于成本的优化器，用于优化语义操作系统，显著提高了系统质量和效率。

Motivation: 尽管语义操作系统在基准测试中表现出色，但其优化困难，现有优化器能力有限。需要一种能全局优化系统质量、成本和延迟的解决方案。

Method: 提出Abacus优化器，利用验证示例和先验知识估计算子性能，搜索最佳实现方案。

Result: Abacus在生物医学、法律领域和多模态问答任务中表现卓越，质量和效率显著提升。

Conclusion: Abacus有效解决了语义操作系统的优化问题，为开发者提供强大工具。

Abstract: LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [795] [Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/abs/2505.13455)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Key words: 情绪同步, 多模态情感, 对话动态, 面部表情, 语音情绪

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文研究了人类在对话中情绪表达与同步的多通道（面部和语音）动态，发现非重叠对话比重叠对话能带来更稳定的情绪同步。

Motivation: 研究多通道（面部与语音）情绪表达与同步的机制，以优化情绪识别系统和人机交互。

Method: 使用IEMOCAP数据集，通过EmoNet和Wav2Vec2提取情绪数据，分析对话片段的情绪对齐，包括皮尔逊相关、滞后调整分析和动态时间规整（DTW）。

Result: 非重叠对话的情绪同步更稳定且可预测；重叠对话情绪对齐更混乱，但DTW显示意外紧密的协调策略。

Conclusion: 对话结构对情绪调节至关重要，为多模态情感对齐的时空动态提供了新见解。

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [796] [Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses](https://arxiv.org/abs/2505.13617)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François Germain,Jonathan Le Roux*

Key words: 方向感知神经场，Ambisonic格式，房间脉冲响应，低秩适应

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种方向感知神经场（DANF），通过Ambisonic格式的房间脉冲响应（RIR）更精确地捕捉声场的定向特性，并引入了方向感知损失。进一步研究了DANF在不同房间中的适应能力。

Motivation: 现有的神经场方法主要针对单声道或双声道听众，未能精确捕捉单个点的真实声场定向特性，因此需要更准确的表示方法。

Method: 提出了方向感知神经场（DANF），通过Ambisonic格式的RIR引入方向信息，并设计了方向感知损失函数。同时研究了DANF在新房间中的低秩适应能力。

Result: DANF能够更精确地表示声场的定向特性，并在新环境中表现出良好的适应能力。

Conclusion: DANF为声场建模提供了更精确的方向感知能力，同时展示了在不同环境中的适应性。

Abstract: The characteristics of a sound field are intrinsically linked to the
geometric and spatial properties of the environment surrounding a sound source
and a listener. The physics of sound propagation is captured in a time-domain
signal known as a room impulse response (RIR). Prior work using neural fields
(NFs) has allowed learning spatially-continuous representations of RIRs from
finite RIR measurements. However, previous NF-based methods have focused on
monaural omnidirectional or at most binaural listeners, which does not
precisely capture the directional characteristics of a real sound field at a
single point. We propose a direction-aware neural field (DANF) that more
explicitly incorporates the directional information by Ambisonic-format RIRs.
While DANF inherently captures spatial relations between sources and listeners,
we further propose a direction-aware loss. In addition, we investigate the
ability of DANF to adapt to new rooms in various ways including low-rank
adaptation.

</details>


### [797] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/abs/2505.13814)
*Jihwan Lee,Kevin Huang,Kleanthis Avramidis,Simon Pistrosch,Monica Gonzalez-Machorro,Yoonjeong Lee,Björn Schuller,Louis Goldstein,Shrikanth Narayanan*

Key words: 表面肌电图（EMG）、发音特征、语音合成、Transformer、卷积神经网络

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种通过表面肌电图（EMG）信号预测发音特征的模型，结合卷积层和Transformer模块，并实现了高相关性的预测，同时首次通过发音特征将EMG信号解码为可理解的语音波形。

Motivation: 探索通过EMG信号预测发音特征并解码为语音波形的新方法，填补了EMG信号直接用于语音合成的空白。

Method: 整合卷积层和Transformer模块，结合独立的发音特征预测器，从EMG信号中预测发音特征并解码为语音波形。

Result: 预测的发音特征相关性达到约0.9，并成功解码为可理解的语音波形；分析了电极放置与特征预测的关系。

Conclusion: 该模型为EMG信号用于语音合成提供了新途径，同时为电极配置优化提供了依据。

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [798] [SPIRIT: Patching Speech Language Models against Jailbreak Attacks](https://arxiv.org/abs/2505.13541)
*Amirbek Djanibekov,Nurdaulet Mukhituly,Kentaro Inui,Hanan Aldarmaki,Nils Lukas*

Key words: 语音语言模型（SLMs）、对抗性攻击、越狱攻击、后处理防御、安全性

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文研究了语音语言模型（SLMs）在对抗性攻击中的脆弱性，并提出了一种无需重新训练的后处理修补防御方法，显著提高了安全性，同时几乎不影响实用性。

Motivation: 由于语音信号比文本更丰富，SLMs面临更高的安全风险，尤其是在对抗性攻击（如越狱攻击）中表现更为脆弱。因此，研究如何在不影响模型实用性的情况下提升其安全性至关重要。

Method: 采用后处理修补防御方法，通过修改SLM的激活函数来干预推理过程，无需重新训练模型。

Result: 提出的防御方法将SLMs的鲁棒性提升至99%，且对模型实用性影响极小。

Conclusion: 该研究为SLMs的安全性提供了一种高效的防御方案，同时保持了模型的高实用性。

Abstract: Speech Language Models (SLMs) enable natural interactions via spoken
instructions, which more effectively capture user intent by detecting nuances
in speech. The richer speech signal introduces new security risks compared to
text-based models, as adversaries can better bypass safety mechanisms by
injecting imperceptible noise to speech. We analyze adversarial attacks and
find that SLMs are substantially more vulnerable to jailbreak attacks, which
can achieve a perfect 100% attack success rate in some instances. To improve
security, we propose post-hoc patching defenses used to intervene during
inference by modifying the SLM's activations that improve robustness up to 99%
with (i) negligible impact on utility and (ii) without any re-training. We
conduct ablation studies to maximize the efficacy of our defenses and improve
the utility/security trade-off, validated with large-scale benchmarks unique to
SLMs.

</details>


### [799] [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
*Jinzuomu Zhong,Suyuan Liu,Dan Wells,Korin Richmond*

Key words: 语音合成, 口音相似性, 评估方法, XAB测试, 发音指标

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 本文旨在改进语音合成中口音相似性的主观和客观评估方法，提高了XAB听力测试的统计显著性，并利用发音相关指标评估口音生成。

Motivation: 目前对口音相似性评估的研究不足，尤其是在语音合成中生成高保真口音的需求日益增长的背景下。

Method: 主观方法：改进XAB听力测试，提供听众转录文本并标记口音差异；客观方法：使用基于元音共振峰和语音后验图的发音相关指标。

Result: 实验表明，这些指标可用于评估口音相似性、说话人相似性和Mel倒谱失真，同时揭示了常见指标（如词错误率）的局限性。

Conclusion: 本文提出的方法为口音相似性评估提供了更有效的工具，尤其是在处理代表性不足的口音时。

Abstract: Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.

</details>


### [800] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
*Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Key words: 语音情感识别, 公平性, 隐式人口统计推断, 伪标签, k-means聚类

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种隐式人口统计推断（IDI）模块，用于减少语音情感识别中的偏见，无需显式人口统计标签，显著提高了公平性指标。

Motivation: 现有方法依赖于难以获取的显式人口统计标签，而语音情感识别中的公平性问题尚未充分研究。

Method: 提出IDI模块，结合伪标签和k-means聚类，无需显式标签即可推断人口统计信息。

Result: 伪标签IDI将公平性指标提高了33%以上，同时准确率下降不到3%；无监督IDI提高了26%以上公平性指标，准确率下降不到4%。

Conclusion: IDI模块有效减少了种族和年龄偏见，适用于缺乏显式人口统计信息的场景。

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


### [801] [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
*Chun-Yi Kuan,Hung-yi Lee*

Key words: 音频感知大语言模型, 幻听, 对比训练, 轻量适配器

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了LISTEN方法，通过对比式训练增强音频感知大语言模型（ALLMs）的能力，减少幻听现象，同时保持性能且高效。

Motivation: 现有音频感知大语言模型在处理音频输入时容易产生幻听现象，降低了可靠性。

Method: 采用LISTEN方法，通过对比训练使用合成的负样本数据，无需修改LLM参数，通过轻量适配器集成音频表示。

Result: LISTEN有效减少幻听现象，同时在现有音频问答和推理基准测试中表现优异，数据和计算效率更高。

Conclusion: LISTEN为提升ALLMs的可靠性提供了高效且无需额外参数的解决方案。

Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.

</details>


### [802] [FlowTSE: Target Speaker Extraction with Flow Matching](https://arxiv.org/abs/2505.14465)
*Aviv Navon,Aviv Shamsian,Yael Segal-Feldman,Neta Glazer,Gil Hetz,Joseph Keshet*

Key words: 目标说话人提取，条件流匹配，mel谱图，相位重构，声码器

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种基于条件流匹配的目标说话人提取方法FlowTSE，简化了流程并提升了性能。

Motivation: 现有的生成式目标说话人提取方法依赖复杂流程和预训练组件，计算开销大，需要更高效的解决方案。

Method: 采用条件流匹配方法，结合mel谱图输入，并针对相位重构任务提出新型声码器。

Result: FlowTSE在标准测试中表现优异，匹配或超越了现有基线方法。

Conclusion: FlowTSE是一种简单高效的目标说话人提取方法，且在相位重构任务中表现出色。

Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.

</details>


### [803] [Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2505.14517)
*Jakob Kienegger,Timo Gerkmann*

Key words: 说话人提取, 空间动态场景, 弱引导, 深度跟踪算法, 联合训练

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 本文提出了一种弱引导的说话人提取方法，仅依赖目标的初始位置，以应对空间动态场景，解决了传统方法在动态环境中依赖强引导的问题。

Motivation: 传统基于深度非线性空间滤波的说话人提取方法在目标方向已知且固定的情况下表现优异，但在空间动态场景中，由于时空特征的变化和交叉说话人带来的模糊性问题，表现较差。本文旨在解决这一问题。

Method: 提出了一种弱引导提取方法，仅利用目标的初始位置，结合自研的深度跟踪算法和联合训练策略，通过合成数据集进行训练。

Result: 实验表明，该方法能有效解决空间模糊性问题，甚至优于不匹配但强引导的提取方法。

Conclusion: 弱引导方法在空间动态场景中具有显著优势，为说话人提取提供了一种实用解决方案。

Abstract: Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.

</details>


### [804] [SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification](https://arxiv.org/abs/2505.14561)
*Theo Lepage,Reda Dehak*

Key words: Self-Supervised Learning, Speaker Verification, Positive Sampling, SSPS, DINO, SimCLR

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文提出了一种自我监督学习（SSL）中新的正样本采样技术（SSPS），用于改进说话人验证（SV）。通过聚类和记忆队列选择同一说话人但不同录音条件的正样本，显著降低了说话人内部差异并提升了性能。

Motivation: 现有的SSL方法通常使用同一语句的正样本和增强数据生成说话人相同的锚点-正样本对，但这种方法主要编码了共享的录音条件信息，限制了模型对说话人身份的学习能力。

Method: 提出了自我监督正采样（SSPS）技术，利用聚类分配和正嵌入记忆队列，从潜在空间中选择同一说话人但不同录音条件的正样本。

Result: SSPS显著提升了SimCLR和DINO的性能，在VoxCeleb1-O数据集上分别达到2.57%和2.53%的EER，优于现有SSL方法。SimCLR-SSPS将EER降低58%。

Conclusion: SSPS通过优化正样本选择，有效降低了说话人内部差异，提升了说话人验证性能。

Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.

</details>


### [805] [Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/abs/2505.13455)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Key words: 情绪同步,多模态情感,对话结构,IEMOCAP,动态时间规整

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 研究探讨了非重叠和重叠对话对情绪同步的影响，发现非重叠对话下情绪同步更稳定，而重叠对话则表现出更高的变异性，揭示了对话结构在情绪协调中的重要性。

Motivation: 探究多通道（面部表情与语音）情感表达的同步性及其在情感识别和人机交互中的意义。

Method: 利用IEMOCAP数据集，通过EmoNet和Wav2Vec2模型提取情感数据，分析不同对话片段（非重叠与重叠）的情绪对齐方式。

Result: 非重叠对话的情绪同步更稳定，重叠对话变异性更高；面部表情在轮流发言中更占主导，语音在同时发声时领先。

Conclusion: 对话结构对情感交流的时空动态有显著影响，为多模态情感对齐提供了新见解。

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [806] [Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses](https://arxiv.org/abs/2505.13617)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François Germain,Jonathan Le Roux*

Key words: 声场建模, 神经场, 方向感知, Ambisonic格式, 低秩适应

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了方向感知神经场（DANF），通过Ambisonic格式的RIRs更明确地捕捉方向信息，并研究了其在适应新房间时的能力。

Motivation: 先前基于神经场的方法仅关注单声道全向或双耳监听，未能精确捕捉真实声场的方向特性，因此需要一种更明确捕捉方向信息的方法。

Method: 提出了方向感知神经场（DANF），利用Ambisonic格式的RIRs，并结合方向感知损失函数和低秩适应等技术。

Result: DANF能够更好地捕捉声场的空间关系和方向特性，并在适应新房间时表现出灵活性。

Conclusion: DANF是一种有效的声场建模方法，能够更精确地捕捉方向信息并适应不同房间环境。

Abstract: The characteristics of a sound field are intrinsically linked to the
geometric and spatial properties of the environment surrounding a sound source
and a listener. The physics of sound propagation is captured in a time-domain
signal known as a room impulse response (RIR). Prior work using neural fields
(NFs) has allowed learning spatially-continuous representations of RIRs from
finite RIR measurements. However, previous NF-based methods have focused on
monaural omnidirectional or at most binaural listeners, which does not
precisely capture the directional characteristics of a real sound field at a
single point. We propose a direction-aware neural field (DANF) that more
explicitly incorporates the directional information by Ambisonic-format RIRs.
While DANF inherently captures spatial relations between sources and listeners,
we further propose a direction-aware loss. In addition, we investigate the
ability of DANF to adapt to new rooms in various ways including low-rank
adaptation.

</details>


### [807] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/abs/2505.13814)
*Jihwan Lee,Kevin Huang,Kleanthis Avramidis,Simon Pistrosch,Monica Gonzalez-Machorro,Yoonjeong Lee,Björn Schuller,Louis Goldstein,Shrikanth Narayanan*

Key words: surface EMG, articulatory features, speech synthesis, Transformer, electrode placement

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出了一种从表面肌电图（EMG）信号预测发音特征的模型，结合卷积层和Transformer模块，实现了高预测相关性，并能解码为可理解的语音波形。

Motivation: 探索从EMG信号预测发音特征并解码为语音波形的可能性，为EMG-based语音合成提供新方法。

Method: 整合卷积层和Transformer模块，分别预测发音特征，分析EMG电极位置与特征预测的关系。

Result: 预测相关性达0.9，首次实现从EMG信号解码语音波形，公开了源代码和样本。

Conclusion: 该方法为EMG-based语音合成开辟了新途径，电极位置优化提供了指导。

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [808] [SPIRIT: Patching Speech Language Models against Jailbreak Attacks](https://arxiv.org/abs/2505.13541)
*Amirbek Djanibekov,Nurdaulet Mukhituly,Kentaro Inui,Hanan Aldarmaki,Nils Lukas*

Key words: 语音语言模型,对抗攻击,安全防御

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: SLMs因语音信号更易受对抗攻击，提出无需重新训练的修补防御方法，显著提升安全性。

Motivation: 研究SLMs在语音交互中因信号丰富带来的安全风险，特别是对抗攻击的脆弱性。

Method: 通过事后修补防御，调整SLM激活值以提升安全性，同时保持实用性。

Result: 防御方法可将攻击成功率降至1%，且对实用性影响极小。

Conclusion: 无需重新训练的防御方法有效提升了SLMs的安全性。

Abstract: Speech Language Models (SLMs) enable natural interactions via spoken
instructions, which more effectively capture user intent by detecting nuances
in speech. The richer speech signal introduces new security risks compared to
text-based models, as adversaries can better bypass safety mechanisms by
injecting imperceptible noise to speech. We analyze adversarial attacks and
find that SLMs are substantially more vulnerable to jailbreak attacks, which
can achieve a perfect 100% attack success rate in some instances. To improve
security, we propose post-hoc patching defenses used to intervene during
inference by modifying the SLM's activations that improve robustness up to 99%
with (i) negligible impact on utility and (ii) without any re-training. We
conduct ablation studies to maximize the efficacy of our defenses and improve
the utility/security trade-off, validated with large-scale benchmarks unique to
SLMs.

</details>


### [809] [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
*Jinzuomu Zhong,Suyuan Liu,Dan Wells,Korin Richmond*

Key words: 口音相似性, 语音合成, 主观评估, 客观评估, XAB听力测试

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文研究了如何改进口音相似性的主观和客观评估方法，提出了新的XAB听力测试和发音相关指标，并揭示了常见指标的局限性。

Motivation: 由于口音相似性评估在语音合成中研究不足，作者希望提升主观和客观评估的准确性和效率。

Method: 主观评估使用改进的XAB听力测试，包括提供转录文本、标注感知差异和严格筛选听众；客观评估采用基于元音共振峰距离和语音后验图的指标。

Result: 实验表明新方法在统计显著性和成本效率上更优，同时揭示了常用指标如词错误率在评估少数口音时的局限性。

Conclusion: 论文提出了有效的口音相似性评估方法，并呼吁关注少数口音评估的改进需求。

Abstract: Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.

</details>


### [810] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
*Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Key words: 语音情感识别, 公平性, 隐含人口统计推断, 伪标记, 无监督学习

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 该论文提出了一种隐含人口统计推断（IDI）模块，通过伪标记和无监督学习减少语音情感识别（SER）中的偏见，显著提高了公平性指标，同时保持较高的SER准确性。

Motivation: 当前语音情感识别中公平性问题研究不足，且依赖显性人口统计标签，但隐私问题限制了数据的获取。

Method: 使用预训练模型的伪标记和无监督k-means聚类构建IDI模块，以减少偏见。

Result: 伪标记IDI使公平性指标提升33%以上，SER准确性下降小于3%；无监督IDI使公平性指标提升26%以上，SER准确性下降小于4%。

Conclusion: IDI模块在缺乏显性人口统计信息时仍能有效减少种族和年龄偏见，具有广泛应用潜力。

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


### [811] [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
*Chun-Yi Kuan,Hung-yi Lee*

Key words: 音频感知大语言模型, 幻觉问题, 对比式训练, 轻量适配器, 合成数据

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种名为LISTEN的方法，通过对比式训练减少音频感知大语言模型（ALLMs）的幻觉问题，无需修改模型参数，且高效结合音频表示。

Motivation: 现有音频感知大语言模型在处理音频输入时容易产生非真实的幻觉声音事件，影响实际应用中的可靠性。

Method: 提出了LISTEN方法，通过对比式训练和合成数据增强模型区分真实与非真实声音的能力，使用轻量适配器集成音频表示。

Result: LISTEN有效减少了幻觉现象，同时在现有音频问答和推理基准测试中表现优异，且数据和计算效率更高。

Conclusion: LISTEN是一种高效且无需修改模型参数的方法，显著提升了音频感知大语言模型的可靠性。

Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.

</details>


### [812] [FlowTSE: Target Speaker Extraction with Flow Matching](https://arxiv.org/abs/2505.14465)
*Aviv Navon,Aviv Shamsian,Yael Segal-Feldman,Neta Glazer,Gil Hetz,Joseph Keshet*

Key words: 目标说话人提取、条件流匹配、mel谱图、相位估计、声码器

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: FlowTSE提出了一种基于条件流匹配的简单有效的目标说话人提取方法，性能优于或匹配现有基线。

Motivation: 尽管生成式方法在目标说话人提取中表现优异，但现有方法依赖复杂流程和预训练组件，存在计算开销问题。

Method: FlowTSE基于条件流匹配，输入为mel谱图表示的注册音频和混合语音，目标为提取目标说话人的纯净语音，并提出了新型声码器以改善相位估计。

Result: 实验结果显示，FlowTSE在标准测试集上性能优于或匹配现有基线。

Conclusion: FlowTSE是一种高效且简单的TSE方法，尤其适用于需要高质量相位重建的任务。

Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.

</details>


### [813] [Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2505.14517)
*Jakob Kienegger,Timo Gerkmann*

Key words: 说话人提取、深度空间滤波、动态场景、弱引导、深度跟踪

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种弱引导的说话人提取方法，仅依赖目标初始位置，解决了动态空间场景中的挑战，并优于强引导方法。

Motivation: 静态场景中目标方向已知且固定，但动态场景中时间变化的空间特征和模糊性使得传统方法表现不佳，需要一种更实用的解决方案。

Method: 提出弱引导提取方法，结合深度跟踪算法和联合训练策略，仅依赖目标的初始位置，避免了动态跟踪的复杂性。

Result: 该方法在合成数据集上表现出色，能够有效解决空间模糊性问题，甚至优于不匹配的强引导方法。

Conclusion: 弱引导方法在动态场景中表现优异，为说话人提取提供了新思路。

Abstract: Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.

</details>


### [814] [SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification](https://arxiv.org/abs/2505.14561)
*Theo Lepage,Reda Dehak*

Key words: 自监督学习, 说话人验证, 正样本采样, SSPS

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 论文提出了一种新的自监督正样本采样技术（SSPS），用于改进说话人验证任务，通过降低同一说话人的方差，显著提升了性能。

Motivation: 传统的自监督学习方法在说话人验证中主要依赖于同一语句的采样和数据增强，这限制了模型学习到说话人身份的潜力，因为样本间共享了录音条件的信息。

Method: 提出SSPS技术，通过聚类和内存队列在潜在空间中寻找同一说话人但不同录音条件的正样本。

Result: SSPS在SimCLR和DINO上分别实现了2.57%和2.53%的EER，显著优于现有自监督学习方法。

Conclusion: SSPS通过改进正样本采样策略，降低了同一说话人的方差，提升了说话人验证的性能。

Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [815] [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
*Heng Yang,Jack Cole,Yuan Li,Renzhi Chen,Geyong Min,Ke Li*

Key words: 基因组基础模型，基准测试，可解释性，数据透明，模型互操作性

<details>
  <summary>Details</summary>

Main category: q-bio.GN

TL;DR: OmniGenBench是一个模块化基准测试平台，旨在统一基因组基础模型的数据、模型、基准测试和可解释性层，解决可重复性挑战。

Motivation: 基因组基础模型在解码基因组方面具有巨大潜力，但亟需严格和可重复的评估。

Method: 开发了OmniGenBench，一个支持标准化评估的模块化平台，集成超过31个开源模型，并通过自动化流程解决可重复性问题。

Result: 平台实现了数据透明、模型互操作性、基准测试一体化和可解释性提升。

Conclusion: OmniGenBench旨在成为可重复基因组AI研究的基础设施，推动基因组规模建模时代的可信发现和协作创新。

Abstract: The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.

</details>


### [816] [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
*Heng Yang,Jack Cole,Yuan Li,Renzhi Chen,Geyong Min,Ke Li*

Key words: 基因组基础模型、基准测试、可重复性、人工智能、基因组学

<details>
  <summary>Details</summary>

Main category: q-bio.GN

TL;DR: OmniGenBench是一个模块化的基准测试平台，旨在统一基因组基础模型（GFMs）的数据、模型、基准测试和可解释性层，解决可重复性问题。

Motivation: 基因组建模领域急需一种严谨且可重复的评估方法，以应对基因组基础模型（GFMs）的快速发展。

Method: 通过OmniGenBench平台，标准化一键评估任何GFM，整合超过31个开源模型，并提供自动化流程和社区可扩展功能。

Result: OmniGenBench解决了数据透明度、模型互操作性、基准测试碎片化和黑盒可解释性等关键挑战。

Conclusion: 该平台旨在成为可重复基因组AI研究的基础设施，加速基因组规模建模时代的可信发现和协作创新。

Abstract: The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [817] [Data Balancing Strategies: A Survey of Resampling and Augmentation Methods](https://arxiv.org/abs/2505.13518)
*Behnam Yousefimehr,Mehdi Ghatee,Mohammad Amin Seifi,Javad Fazli,Sajed Tavakoli,Zahra Rafei,Shervin Ghaffari,Abolfazl Nikahd,Mahdi Razi Gandomani,Alireza Orouji,Ramtin Mahmoudi Kashani,Sarina Heshmati,Negin Sadat Mousavi*

Key words: 不平衡数据, 重采样, SMOTE, GANs, VAEs, 数据平衡

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文综述了针对不平衡数据问题的多种重采样方法，包括合成过采样、生成模型等，并探讨了未来研究方向。

Motivation: 解决机器学习中因类别标签分布不均导致的预测偏差和模型准确性下降问题。

Method: 分类和综述了多种数据平衡方法，如SMOTE、GANs、VAEs、集成方法等。

Result: 总结了当前重采样技术的最新进展及其实际应用效果。

Conclusion: 提出了未来在不平衡数据领域的潜在研究方向。

Abstract: Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.

</details>


### [818] [Continuous Domain Generalization](https://arxiv.org/abs/2505.13519)
*Zekun Cai,Yiheng Yao,Guangji Bai,Renhe Jiang,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Key words: 连续域泛化、神经李传输算子、多维变化、几何建模、代数理论

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了连续域泛化（CDG）任务，旨在推广预测模型到由连续变异描述定义的未见域。提出了一种基于几何和代数理论的框架，并引入了神经李传输算子（NeuralLTO）来建模低维流形上的模型参数。

Motivation: 现实世界的数据分布在多个潜在因素（如时间、地理和社会经济背景）上连续变化，现有方法通常将域视为离散或单一维度变化，无法捕捉复杂的多维变化。

Method: 提出了一种基于几何和代数理论的框架，设计了神经李传输算子（NeuralLTO）来建模低维流形上的参数结构，并引入门控机制和局部图表策略以处理噪声和不完整的域描述符。

Result: 合成和真实数据集（包括遥感、科学文献和交通预测）上的实验表明，该方法在泛化准确性和对描述符缺陷的鲁棒性上显著优于现有基线。

Conclusion: 连续域泛化任务及其提出的方法在多维连续域变化中表现出更强的泛化能力和鲁棒性，为实际应用提供了新的解决方案。

Abstract: Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.

</details>


### [819] [Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback](https://arxiv.org/abs/2505.13562)
*Shishen Lin*

Key words: 矩阵游戏，Bandit学习，进化算法，随机乐观，遗憾分析

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种名为CoEBL的新型算法，将进化算法与Bandit框架结合，实现了随机乐观策略，并在矩阵游戏中达到了次线性遗憾，性能优于传统算法。

Motivation: 探讨在未知支付矩阵和Bandit反馈下的两人零和矩阵游戏，研究随机乐观策略的效果，填补理论空白。

Method: 结合进化算法（EAs）与Bandit框架，通过EA的变异操作实现随机乐观策略。

Result: CoEBL在理论分析和实验中均达到了次线性遗憾，性能优于经典Bandit算法。

Conclusion: 进化Bandit学习在游戏理论中具有潜力，尤其是通过进化算法实现的随机乐观策略。

Abstract: Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.

</details>


### [820] [Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles](https://arxiv.org/abs/2505.13585)
*Xinzhu Liang,Joseph M. Lukens,Sanjaya Lohani,Brian T. Kirby,Thomas A. Searles,Xin Qiu,Kody J. H. Law*

Key words: SBMC, 贝叶斯深度学习, SMC, MCMC, 不确定量化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种名为SBMC的可扩展贝叶斯蒙特卡洛方法，通过并行实现SMC或MCMC算法，在性能和总成本上与串行实现相当，并在不确定量化方面优于SOTA方法。

Motivation: 为了解决并行实现贝叶斯深度学习算法时的时间和成本问题，同时提升不确定量化能力。

Method: 采用SBMC方法，结合点估计和后验的插值，并行实现SMC或MCMC算法。

Result: 在MNIST、CIFAR和IMDb等数据集上验证了方法的有效性，UQ表现优于SOTA方法。

Conclusion: 通过锚定点估计，SBMC在保持高效性能的同时，显著提升了不确定量化能力。

Abstract: This work introduces a new method called scalable Bayesian Monte Carlo
(SBMC). The model interpolates between a point estimator and the posterior, and
the algorithm is a parallel implementation of a consistent (asymptotically
unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or
Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its
utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic
numerical study reveals that parallel implementations of SMC and MCMC are
comparable to serial implementations in terms of performance and total cost,
and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like
deep ensembles at convergence, along with substantially improved uncertainty
quantification (UQ)--in particular, epistemic UQ. But even parallel
implementations are expensive, with an irreducible time barrier much larger
than the cost of the MAP estimator. Compressing time further leads to rapid
degradation of accuracy, whereas UQ remains valuable. By anchoring to a point
estimator we can recover accuracy, while retaining valuable UQ, ultimately
delivering strong performance across metrics for a cost comparable to the SOTA.

</details>


### [821] [Backward Conformal Prediction](https://arxiv.org/abs/2505.13732)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Key words: 共形预测, 后向共形预测, 边际覆盖, e值, 留一估计器

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种名为‘后向共形预测’的新方法，通过对预测集大小进行灵活控制，保证了共形覆盖的可靠性。与传统方法不同，它通过约束预测集大小的行为来调整覆盖水平。

Motivation: 传统共形预测方法固定覆盖水平但允许预测集大小变动，不适用于要求严格预测集大小的应用（如医学诊断）。因此，需要一种能够灵活控制预测集大小同时保证覆盖可靠性的方法。

Method: 该方法基于两项关键成果：(i) 使用e值确保边际覆盖，并允许数据依赖的覆盖调整；(ii) 提出一种新的留一估计器来计算边际误差，保证理论可行。

Result: 理论和实验证明，该方法能够保持可计算的覆盖保证，同时确保预测集大小可控且易于解释。

Conclusion: 后向共形预测为需要严格控制预测集大小的应用提供了有效的解决方案。

Abstract: We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.

</details>


### [822] [Graphon Mixtures](https://arxiv.org/abs/2505.13864)
*Sevvandi Kandanaarachchi,Cheng Soon Ong*

Key words: 社交网络, 枢纽, 密集社区, 图线混合模型, 稀疏图

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一个生成模型，结合了社交网络中的枢纽和密集社区结构。通过图线的混合模型，能够生成包含稀疏和密集结构的图序列，并在理论和实验上验证了模型的优势。

Motivation: 社交网络中存在少量的大枢纽和大量的小密集社区，需要一种模型能够同时捕捉这两种结构。

Method: 基于图线混合模型，结合稀疏图和密集图的特点，并提出新的稀疏图条件（最大度数）以识别枢纽。

Result: 理论证明了可以估计枢纽的标准化度数以及稀疏图成分的图线，实验结果显示模型在合成数据、引用图和社交网络中的有效性。

Conclusion: 通过显式建模稀疏图，该模型在捕捉复杂网络结构中表现优异。

Abstract: Social networks have a small number of large hubs, and a large number of
small dense communities. We propose a generative model that captures both hub
and dense structures. Based on recent results about graphons on line graphs,
our model is a graphon mixture, enabling us to generate sequences of graphs
where each graph is a combination of sparse and dense graphs. We propose a new
condition on sparse graphs (the max-degree), which enables us to identify hubs.
We show theoretically that we can estimate the normalized degree of the hubs,
as well as estimate the graphon corresponding to sparse components of graph
mixtures. We illustrate our approach on synthetic data, citation graphs, and
social networks, showing the benefits of explicitly modeling sparse graphs.

</details>


### [823] [An Asymptotic Equation Linking WAIC and WBIC in Singular Models](https://arxiv.org/abs/2505.13902)
*Naoki Hayashi,Takuro Kutsuna,Sawa Takamuku*

Key words: 奇异学习理论, WAIC, WBIC, 模型选择, 渐近行为

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文通过理论推导揭示了WAIC和WBIC之间的渐近关系，为奇异学习理论框架下的模型选择提供了计算效率提升的理论基础。

Motivation: 研究动机是解决奇异模型中传统信息准则（如AIC和BIC）不适用的问题，并探索WAIC和WBIC之间的关系。

Method: 通过理论推导，建立了WAIC和WBIC之间的渐近方程，并在奇异学习理论框架下分析其结构关系。

Result: 结果显示了一个渐近无偏的WAIC表达式，该表达式基于用于WBIC的后验分布，阐明了两种准则的渐近行为。

Conclusion: 结论是这一理论成果为奇异模型中模型选择的计算效率提供了新的理论基础。

Abstract: In statistical learning, models are classified as regular or singular
depending on whether the mapping from parameters to probability distributions
is injective. Most models with hierarchical structures or latent variables are
singular, for which conventional criteria such as the Akaike Information
Criterion and the Bayesian Information Criterion are inapplicable due to the
breakdown of normal approximations for the likelihood and posterior. To address
this, the Widely Applicable Information Criterion (WAIC) and the Widely
Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC
and WBIC are computed using posterior distributions at different temperature
settings, separate posterior sampling is generally required. In this paper, we
theoretically derive an asymptotic equation that links WAIC and WBIC, despite
their dependence on different posteriors. This equation yields an
asymptotically unbiased expression of WAIC in terms of the posterior
distribution used for WBIC. The result clarifies the structural relationship
between these criteria within the framework of singular learning theory, and
deepens understanding of their asymptotic behavior. This theoretical
contribution provides a foundation for future developments in the computational
efficiency of model selection in singular models.

</details>


### [824] [A Probabilistic Perspective on Model Collapse](https://arxiv.org/abs/2505.13947)
*Shirong Xu,Hengzhi He,Guang Cheng*

Key words: 模型崩塌、递归训练、随机游走、样本量、合成数据

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 研究了模型崩塌的机制，提出了递归参数模型训练的随机游走框架，证明了逐步增加样本量可防止崩塌，并通过仿真和真实数据验证了理论结果。

Motivation: 模型崩塌是语言模型训练中的关键问题，理解其机制对避免崩塌至关重要。

Method: 从概率角度研究递归参数模型训练，提出随机游走框架，分析样本量步长和偏差的影响。

Result: 证明逐步增加样本量可防止崩塌，需超线性增长率，并在存在偏差时加速。同时研究了合成数据训练的优势。

Conclusion: 通过理论分析和验证，提出防止模型崩塌的有效方法，并展示了合成数据训练的潜力。

Abstract: In recent years, model collapse has become a critical issue in language model
training, making it essential to understand the underlying mechanisms driving
this phenomenon. In this paper, we investigate recursive parametric model
training from a probabilistic perspective, aiming to characterize the
conditions under which model collapse occurs and, crucially, how it can be
mitigated. We conceptualize the recursive training process as a random walk of
the model estimate, highlighting how the sample size influences the step size
and how the estimation procedure determines the direction and potential bias of
the random walk. Under mild conditions, we rigorously show that progressively
increasing the sample size at each training step is necessary to prevent model
collapse. In particular, when the estimation is unbiased, the required growth
rate follows a superlinear pattern. This rate needs to be accelerated even
further in the presence of substantial estimation bias. Building on this
probabilistic framework, we also investigate the probability that recursive
training on synthetic data yields models that outperform those trained solely
on real data. Moreover, we extend these results to general parametric model
family in an asymptotic regime. Finally, we validate our theoretical results
through extensive simulations and a real-world dataset.

</details>


### [825] [Computational Efficiency under Covariate Shift in Kernel Ridge Regression](https://arxiv.org/abs/2505.14083)
*Andrea Della Vecchia,Arnaud Mavakala Watusadisi,Ernesto De Vito,Lorenzo Rosasco*

Key words: 协变量漂移, 非参数回归, RKHS, 随机投影, 计算效率

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文探讨了在核再生希尔伯特空间（RKHS）中非参数回归的协变量漂移问题，提出随机投影方法以平衡计算效率与统计准确性。

Motivation: 协变量漂移导致训练和测试数据输入分布不一致，增加了学习难度，而核方法的高计算成本限制了其在大数据集上的应用。研究旨在解决这一问题。

Method: 采用随机投影技术，将假设空间限定为RKHS中的一个随机子空间。

Result: 在协变量漂移情况下，该方法显著降低了计算成本，同时保持了学习性能。

Conclusion: 随机投影方法有效平衡了计算效率与统计性能，为大规模数据学习提供了可行方案。

Abstract: This paper addresses the covariate shift problem in the context of
nonparametric regression within reproducing kernel Hilbert spaces (RKHSs).
Covariate shift arises in supervised learning when the input distributions of
the training and test data differ, presenting additional challenges for
learning. Although kernel methods have optimal statistical properties, their
high computational demands in terms of time and, particularly, memory, limit
their scalability to large datasets. To address this limitation, the main focus
of this paper is to explore the trade-off between computational efficiency and
statistical accuracy under covariate shift. We investigate the use of random
projections where the hypothesis space consists of a random subspace within a
given RKHS. Our results show that, even in the presence of covariate shift,
significant computational savings can be achieved without compromising learning
performance.

</details>


### [826] [High-dimensional Nonparametric Contextual Bandit Problem](https://arxiv.org/abs/2505.14102)
*Shogo Iwazaki,Junpei Komiyama,Masaaki Imaizumi*

Key words: 核化上下文赌博机, 高维特征空间, 无遗憾学习, 宽松遗憾

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文研究了具有大特征空间的核化上下文赌博机问题，提出了一种在特征维度增长至样本数量时仍能实现无遗憾学习的方法，并分析了宽松遗憾的速率。

Motivation: 解决现有方法在高维特征空间（如高斯核）下表现不佳的问题，尤其是在特征维度与样本数量相近时的学习困境。

Method: 引入对上下文分布的随机假设，提出新的算法以实现无遗憾学习，并分析宽松遗憾的速率。

Result: 证明在该假设下，即使特征维度增长至样本数量，仍能实现无遗憾学习，并给出了宽松遗憾的具体速率。

Conclusion: 该方法扩展了核化上下文赌博机的适用范围，为高维特征空间下的决策问题提供了有效解决方案。

Abstract: We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.

</details>


### [827] [Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals](https://arxiv.org/abs/2505.14164)
*Marcel Arpogaus,Thomas Kneib,Thomas Nagler,David Rügamer*

Key words: 密度回归、多元条件变换模型、正态化流、可解释性、灵活性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文结合了多元条件变换模型（MCTM）和自回归正态化流（NF），用于密度回归建模，以实现透明性和灵活性的平衡。

Motivation: 现有方法中，NF在多维数据中表现优异但缺乏可解释性，而MCTM虽透明但灵活性不足。本文旨在结合两者的优势。

Method: 将MCTM与自回归NF结合，第一步用MCTM建模可解释特征对边缘分布的影响，第二步用NF处理联合分布中的复杂非线性关系。

Result: 方法在不同数值实验和真实数据中表现优异，展示了其灵活性和可解释性的平衡效果。

Conclusion: 该方法成功结合了MCTM的透明性和NF的灵活性，适用于复杂多维数据建模。

Abstract: Density regression models allow a comprehensive understanding of data by
modeling the complete conditional probability distribution. While flexible
estimation approaches such as normalizing flows (NF) work particularly well in
multiple dimensions, interpreting the input-output relationship of such models
is often difficult, due to the black-box character of deep learning models. In
contrast, existing statistical methods for multivariate outcomes such as
multivariate conditional transformation models (MCTM) are restricted in
flexibility and are often not expressive enough to represent complex
multivariate probability distributions. In this paper, we combine MCTM with
state-of-the-art and autoregressive NF to leverage the transparency of MCTM for
modeling interpretable feature effects on the marginal distributions in the
first step and the flexibility of neural-network-based NF techniques to account
for complex and non-linear relationships in the joint data distribution. We
demonstrate our method's versatility in various numerical experiments and
compare it with MCTM and other NF models on both simulated and real-world data.

</details>


### [828] [From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling](https://arxiv.org/abs/2505.14177)
*Marien Renaud,Valentin De Bortoli,Arthur Leclaire,Nicolas Papadakis*

Key words: 非凸势能, ULA, PSGLA, 采样, 成像逆问题

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文研究了非凸势能下的采样问题，证明了ULA在离散时间下的稳定性，并基于此提出了PSGLA的收敛性证明，在合成数据和成像逆问题中验证了其优越性。

Motivation: 非凸和非平滑势能在许多实际问题（如成像逆问题）中很常见，但传统的采样算法（如ULA）缺乏稳定性分析，且PSGLA的收敛性尚未得到证明。

Method: 结合ULA和向前-向后优化算法提出PSGLA，利用Moreau包络性质和非凸势能的强凸性假设，证明了PSGLA的收敛性。

Result: 实验验证了PSGLA在合成数据和成像逆问题中的有效性，其在后验采样中收敛速度更快且保持恢复性能。

Conclusion: PSGLA为处理非凸势能提供了一种高效稳定的方法，并在实际应用中表现出优越性能。

Abstract: We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.

</details>


### [829] [A system identification approach to clustering vector autoregressive time series](https://arxiv.org/abs/2505.14421)
*Zuogong Yue,Xinyi Wang,Victor Solo*

Key words: 时间序列聚类,自回归模型,系统辨识,k-LMVAR,BIC准则

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种基于系统辨识的向量时间序列聚类方法k-LMVAR，解决了现有方法忽略自相关特征或依赖领域知识的问题，并通过模拟实验验证了其优越性能。

Motivation: 时间序列聚类在复杂系统建模中具有重要价值，但现有方法多以标量时间序列为对象，忽略了自相关特征或依赖领域知识，因此需要一种更通用的方法。

Method: 提出基于混合自回归模型的聚类算法，并因计算问题开发了其小噪声极限版本k-LMVAR，同时设计了BIC准则用于选择聚类数量和模型阶数。

Result: 算法在模拟实验中表现出色，计算效率高，证明了其有效性和可扩展性。

Conclusion: k-LMVAR是一种高效、通用的向量时间序列聚类方法，适用于复杂系统建模。

Abstract: Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.

</details>


### [830] [A simple estimator of the correlation kernel matrix of a determinantal point process](https://arxiv.org/abs/2505.14529)
*Christian Gouriéroux,Yang Lu*

Key words: 行列式点过程, 核估计, 闭式估计器, 渐近正态性, 大偏差

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种简单易实现的行列式点过程（DPP）核的闭式估计器，并验证了其一致性、渐近正态性和大偏差性质。

Motivation: 行列式点过程（DPP）是一种参数化的多元二元变量模型，但其核的估计方法复杂。本文旨在提出一种易于实现且高效的估计器。

Method: 提出了一种闭式估计器，可直接用于核的估计，并可作为最大似然估计学习算法的初始值。

Result: 证明了估计器的一致性、渐近正态性及其大偏差性质。

Conclusion: 该闭式估计器不仅易于实现，还能作为学习算法的初始值，具有理论上的稳健性。

Abstract: The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.

</details>


### [831] [High-Dimensional Analysis of Bootstrap Ensemble Classifiers](https://arxiv.org/abs/2505.14587)
*Hamza Cherkaoui,Malik Tiomoko,Mohamed El Amine Seddik,Cosme Louart,Ekkehard Schnoor,Balazs Kegl*

Key words: Bootstrap, LSSVM, Random Matrix Theory, Ensemble Learning, High-dimensional Data

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文对Bootstrap方法在最小二乘支持向量机（LSSVM）集成中的理论性能进行了分析，研究了其在大样本和高维特征下的表现，并提出了优化子集数量和正则化参数的策略。

Motivation: 探索Bootstrap方法在高维数据和大样本量下对LSSVM集成性能的影响，以提升其实际应用效果。

Method: 利用随机矩阵理论工具，分析基于不同数据子集训练的多个弱分类器的性能，并提出优化参数选择的策略。

Result: 在合成和真实数据集上的实验验证了理论结果，证明了优化策略的有效性。

Conclusion: 研究表明，通过合理选择子集数量和正则化参数，可以显著提升LSSVM集成的性能。

Abstract: Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.

</details>


### [832] [Data Balancing Strategies: A Survey of Resampling and Augmentation Methods](https://arxiv.org/abs/2505.13518)
*Behnam Yousefimehr,Mehdi Ghatee,Mohammad Amin Seifi,Javad Fazli,Sajed Tavakoli,Zahra Rafei,Shervin Ghaffari,Abolfazl Nikahd,Mahdi Razi Gandomani,Alireza Orouji,Ramtin Mahmoudi Kashani,Sarina Heshmati,Negin Sadat Mousavi*

Key words: 机器学习、类不平衡、数据平衡、过采样、生成模型

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文综述了处理机器学习中类不平衡问题的多种数据平衡方法，包括合成过采样、自适应技术、生成模型等，并讨论了未来的研究方向。

Motivation: 类不平衡数据导致模型预测偏差和准确性下降，需要有效的解决方案。

Method: 分类并综述了包括合成过采样、自适应技术、生成模型、集成方法、混合策略、欠采样和邻居方法在内的多种数据平衡技术。

Result: 总结了各种方法的有效性，并通过实际案例验证其应用价值。

Conclusion: 数据平衡方法能有效缓解类不平衡问题，未来研究可进一步优化生成模型和混合策略。

Abstract: Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.

</details>


### [833] [Continuous Domain Generalization](https://arxiv.org/abs/2505.13519)
*Zekun Cai,Yiheng Yao,Guangji Bai,Renhe Jiang,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Key words: 连续域泛化, 多维变化, 神经李传输算子, 几何代数理论, 门控机制

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种名为连续域泛化（CDG）的新任务，旨在解决现有离散域泛化方法对多维连续变化的适应性不足问题。

Motivation: 现实中数据分布往往沿时间、地理和社会经济等多维度连续变化，而现有方法通常假设域为离散或单轴变化，不足以捕捉这种多维复杂性。

Method: 作者提出基于几何与代数理论的框架，通过神经李传输算子（NeuralLTO）建模参数低维流形结构，结合门控机制和局部图表策略处理噪声或缺失描述符。

Result: 实验表明，该方法在合成和真实数据集（如遥感、科学文献和交通预测）上显著优于现有基线，在泛化准确性和鲁棒性上表现优越。

Conclusion: 论文提出的CDG任务及NeuralLTO框架有效解决了多维连续变化的域泛化问题，为实际应用提供更具适应性的模型。

Abstract: Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.

</details>


### [834] [Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback](https://arxiv.org/abs/2505.13562)
*Shishen Lin*

Key words: 矩阵游戏, 老虎机学习, 进化算法, 随机乐观

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种名为\coebl的新算法，将进化算法（EAs）与多臂老虎机框架结合，通过进化变异操作实现随机乐观策略。理论证明该算法可实现亚线性遗憾，性能与确定性乐观策略相当。实证表明其优于经典老虎机算法。

Motivation: 研究两玩家零和矩阵游戏中未知收益矩阵和老虎机反馈下的学习问题，探索随机乐观策略的理论潜力。

Method: 提出\coebl算法，结合进化算法的变异操作实现随机乐观策略。

Result: 理论证明算法可实现亚线性遗憾，实证评估显示其优于经典方法。

Conclusion: 进化老虎机学习在游戏理论中潜力显著，尤其是通过进化算法实现的随机乐观策略。

Abstract: Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.

</details>


### [835] [Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles](https://arxiv.org/abs/2505.13585)
*Xinzhu Liang,Joseph M. Lukens,Sanjaya Lohani,Brian T. Kirby,Thomas A. Searles,Xin Qiu,Kody J. H. Law*

Key words: SBMC, 贝叶斯深度学习, 并行计算, SMC, MCMC, 不确定性量化

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文提出了一种名为可扩展贝叶斯蒙特卡洛（SBMC）的新方法，通过并行实现序列蒙特卡洛（SMC）或马尔可夫链蒙特卡洛（MCMC），在性能和总成本上与串行实现相当，并在准确性和不确定性量化（UQ）方面超越现有技术，但计算成本较高。

Motivation: 开发一种能在大规模和实际应用中提供高准确性和优质不确定性量化（尤其是认知不确定性）的贝叶斯深度学习方法。

Method: 提出SBMC方法，通过并行实现SMC或MCMC，结合点估计和后验插值，提高计算效率。

Result: 并行实现的SMC和MCMC在大规模数据集（如MNIST、CIFAR、IMDb）上表现出色，准确性达到或超越现有技术，同时提供更好的不确定性量化。

Conclusion: SBMC方法在性能和成本上优于串行实现，但仍有较高计算成本；通过结合点估计，能够在保持UQ优势的同时恢复准确性。

Abstract: This work introduces a new method called scalable Bayesian Monte Carlo
(SBMC). The model interpolates between a point estimator and the posterior, and
the algorithm is a parallel implementation of a consistent (asymptotically
unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or
Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its
utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic
numerical study reveals that parallel implementations of SMC and MCMC are
comparable to serial implementations in terms of performance and total cost,
and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like
deep ensembles at convergence, along with substantially improved uncertainty
quantification (UQ)--in particular, epistemic UQ. But even parallel
implementations are expensive, with an irreducible time barrier much larger
than the cost of the MAP estimator. Compressing time further leads to rapid
degradation of accuracy, whereas UQ remains valuable. By anchoring to a point
estimator we can recover accuracy, while retaining valuable UQ, ultimately
delivering strong performance across metrics for a cost comparable to the SOTA.

</details>


### [836] [Backward Conformal Prediction](https://arxiv.org/abs/2505.13732)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Key words: 共形预测、后向共形预测、边际覆盖率、留一估计器、医学诊断

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种名为“后向共形预测”的新方法，能够在保证共形覆盖率的同时，灵活控制预测集的大小。通过基于观测数据约束预测集大小的规则，并调整覆盖率水平，该方法在医学诊断等需要限制预测集大小的应用中特别有效。

Motivation: 传统的共形预测方法固定覆盖率水平但允许预测集大小变化，而新方法旨在通过约束预测集大小并提供灵活的覆盖率控制，解决某些应用中预测集过大不实用的问题。

Method: 方法基于两个关键基础：(i) Gauthier等人的后验共形有效性理论（使用e值），确保边际覆盖率；(ii) 一种新颖的留一估计器，用于估计边际错误覆盖率，确保理论保证的可计算性。

Result: 理论和实证结果均支持该方法的有效性，能够在保持可计算覆盖率的同时，确保预测集大小可控且易于解释。

Conclusion: 后向共形预测方法在需要控制预测集大小的应用中具有实际优势，能够灵活调整覆盖率水平并提供理论保证。

Abstract: We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.

</details>


### [837] [Graphon Mixtures](https://arxiv.org/abs/2505.13864)
*Sevvandi Kandanaarachchi,Cheng Soon Ong*

Key words: 社交网络, 枢纽, 密集社区, 图混合模型, 稀疏图

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种生成模型，捕捉社交网络中的枢纽和密集社区结构，通过图混合生成稀疏和密集图序列，理论上可估计枢纽的归一化度，并通过实验验证。

Motivation: 社交网络中同时存在少量大型枢纽和大量小型密集社区，需模型化这种混合结构。

Method: 基于线图上的图论结果，提出图混合模型，结合稀疏和密集图生成图序列，并定义最大度条件识别枢纽。

Result: 理论证明可估计枢纽的归一化度及稀疏组件的图，实验表明模型在合成数据、引用图和社交网络中有效。

Conclusion: 显式建模稀疏图能更好地捕捉社交网络结构，模型具有理论和实际价值。

Abstract: Social networks have a small number of large hubs, and a large number of
small dense communities. We propose a generative model that captures both hub
and dense structures. Based on recent results about graphons on line graphs,
our model is a graphon mixture, enabling us to generate sequences of graphs
where each graph is a combination of sparse and dense graphs. We propose a new
condition on sparse graphs (the max-degree), which enables us to identify hubs.
We show theoretically that we can estimate the normalized degree of the hubs,
as well as estimate the graphon corresponding to sparse components of graph
mixtures. We illustrate our approach on synthetic data, citation graphs, and
social networks, showing the benefits of explicitly modeling sparse graphs.

</details>


### [838] [An Asymptotic Equation Linking WAIC and WBIC in Singular Models](https://arxiv.org/abs/2505.13902)
*Naoki Hayashi,Takuro Kutsuna,Sawa Takamuku*

Key words: 奇异学习理论, WAIC, WBIC, 模型选择, 渐近关系

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文通过理论推导建立了WAIC和WBIC之间的渐近联系，为奇异学习理论中的模型选择提供了计算效率的基础。

Motivation: 解决奇异模型中传统信息准则（如AIC和BIC）失效的问题，并探索WAIC和WBIC之间的关系。

Method: 通过理论推导，建立了WAIC和WBIC之间的渐近方程，利用不同的后验分布进行计算。

Result: 得到了WAIC和WBIC之间的渐近关系，阐明了它们在奇异学习理论中的结构关系。

Conclusion: 该理论贡献为奇异模型中的计算效率提升奠定了基础，并加深了对WAIC和WBIC渐近行为的理解。

Abstract: In statistical learning, models are classified as regular or singular
depending on whether the mapping from parameters to probability distributions
is injective. Most models with hierarchical structures or latent variables are
singular, for which conventional criteria such as the Akaike Information
Criterion and the Bayesian Information Criterion are inapplicable due to the
breakdown of normal approximations for the likelihood and posterior. To address
this, the Widely Applicable Information Criterion (WAIC) and the Widely
Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC
and WBIC are computed using posterior distributions at different temperature
settings, separate posterior sampling is generally required. In this paper, we
theoretically derive an asymptotic equation that links WAIC and WBIC, despite
their dependence on different posteriors. This equation yields an
asymptotically unbiased expression of WAIC in terms of the posterior
distribution used for WBIC. The result clarifies the structural relationship
between these criteria within the framework of singular learning theory, and
deepens understanding of their asymptotic behavior. This theoretical
contribution provides a foundation for future developments in the computational
efficiency of model selection in singular models.

</details>


### [839] [A Probabilistic Perspective on Model Collapse](https://arxiv.org/abs/2505.13947)
*Shirong Xu,Hengzhi He,Guang Cheng*

Key words: 模型崩溃, 递归训练, 随机游走, 样本量增长, 概率框架

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了递归参数模型训练中的模型崩溃现象，提出了防止崩溃的样本增长条件，并通过理论和实验验证了结果。

Motivation: 近年来，模型崩溃成为语言模型训练中的关键问题，理解其机制并找到缓解方法至关重要。

Method: 从概率角度分析递归训练，将其建模为模型估计的随机游走，推导样本增长条件以防止崩溃。

Result: 理论证明，逐步增加样本量可防止崩溃，且无偏估计需超线性增长，而有偏估计需更快增长。实验验证了这一框架。

Conclusion: 通过理论分析和实验验证，提出了防止模型崩溃的有效条件，并探讨了合成数据训练的潜力。

Abstract: In recent years, model collapse has become a critical issue in language model
training, making it essential to understand the underlying mechanisms driving
this phenomenon. In this paper, we investigate recursive parametric model
training from a probabilistic perspective, aiming to characterize the
conditions under which model collapse occurs and, crucially, how it can be
mitigated. We conceptualize the recursive training process as a random walk of
the model estimate, highlighting how the sample size influences the step size
and how the estimation procedure determines the direction and potential bias of
the random walk. Under mild conditions, we rigorously show that progressively
increasing the sample size at each training step is necessary to prevent model
collapse. In particular, when the estimation is unbiased, the required growth
rate follows a superlinear pattern. This rate needs to be accelerated even
further in the presence of substantial estimation bias. Building on this
probabilistic framework, we also investigate the probability that recursive
training on synthetic data yields models that outperform those trained solely
on real data. Moreover, we extend these results to general parametric model
family in an asymptotic regime. Finally, we validate our theoretical results
through extensive simulations and a real-world dataset.

</details>


### [840] [Computational Efficiency under Covariate Shift in Kernel Ridge Regression](https://arxiv.org/abs/2505.14083)
*Andrea Della Vecchia,Arnaud Mavakala Watusadisi,Ernesto De Vito,Lorenzo Rosasco*

Key words: 协变量偏移、核方法、随机投影、计算效率、统计准确性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了在再生核希尔伯特空间（RKHS）中协变量偏移问题的非参数回归，通过随机投影在计算效率和统计准确性之间取得平衡。

Motivation: 协变量偏移在监督学习中会导致训练和测试数据输入分布不同，增加了学习难度。核方法虽然具有最优统计性质，但计算和内存需求高，难以扩展到大数据集。

Method: 采用随机投影技术，假设空间为给定RKHS中的随机子空间。

Result: 即使在协变量偏移情况下，也能在不影响学习性能的前提下显著节省计算资源。

Conclusion: 随机投影为处理协变量偏移提供了一种高效且统计准确的方法。

Abstract: This paper addresses the covariate shift problem in the context of
nonparametric regression within reproducing kernel Hilbert spaces (RKHSs).
Covariate shift arises in supervised learning when the input distributions of
the training and test data differ, presenting additional challenges for
learning. Although kernel methods have optimal statistical properties, their
high computational demands in terms of time and, particularly, memory, limit
their scalability to large datasets. To address this limitation, the main focus
of this paper is to explore the trade-off between computational efficiency and
statistical accuracy under covariate shift. We investigate the use of random
projections where the hypothesis space consists of a random subspace within a
given RKHS. Our results show that, even in the presence of covariate shift,
significant computational savings can be achieved without compromising learning
performance.

</details>


### [841] [High-dimensional Nonparametric Contextual Bandit Problem](https://arxiv.org/abs/2505.14102)
*Shogo Iwazaki,Junpei Komiyama,Masaaki Imaizumi*

Key words: 核化上下文老虎机，高维特征，无遗憾学习，宽松遗憾

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种针对高维特征空间的核化上下文老虎机问题的算法，解决了现有方法在高维下的性能问题。

Motivation: 研究核化上下文老虎机问题，以解决高维特征空间下的决策问题，如个性化广告和推荐系统。

Method: 引入随机假设，分析在高维特征下的无遗憾学习，并推导宽松遗憾的速率。

Result: 证明了在高维情况下无遗憾学习的可行性，并给出了宽松遗憾的速率。

Conclusion: 核化上下文老虎机在高维特征下仍可实现无遗憾学习，具有实际应用价值。

Abstract: We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.

</details>


### [842] [Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals](https://arxiv.org/abs/2505.14164)
*Marcel Arpogaus,Thomas Kneib,Thomas Nagler,David Rügamer*

Key words: 密度回归模型, MCTM, 自回归NF, 可解释性, 灵活性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种结合MCTM和NF的方法，以兼顾可解释性和灵活性，用于建模复杂的多变量概率分布。

Motivation: 现有方法如MCTM缺乏灵活性，而NF虽然灵活但难以解释。论文旨在结合两者的优势。

Method: 将MCTM与自回归NF结合，先用MCTM建模可解释的边际分布特征，再用NF处理联合数据分布中的复杂关系。

Result: 方法在模拟和实际数据中表现优于单一的MCTM和NF模型。

Conclusion: 结合MCTM和NF的方法在灵活性和可解释性上取得了平衡。

Abstract: Density regression models allow a comprehensive understanding of data by
modeling the complete conditional probability distribution. While flexible
estimation approaches such as normalizing flows (NF) work particularly well in
multiple dimensions, interpreting the input-output relationship of such models
is often difficult, due to the black-box character of deep learning models. In
contrast, existing statistical methods for multivariate outcomes such as
multivariate conditional transformation models (MCTM) are restricted in
flexibility and are often not expressive enough to represent complex
multivariate probability distributions. In this paper, we combine MCTM with
state-of-the-art and autoregressive NF to leverage the transparency of MCTM for
modeling interpretable feature effects on the marginal distributions in the
first step and the flexibility of neural-network-based NF techniques to account
for complex and non-linear relationships in the joint data distribution. We
demonstrate our method's versatility in various numerical experiments and
compare it with MCTM and other NF models on both simulated and real-world data.

</details>


### [843] [From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling](https://arxiv.org/abs/2505.14177)
*Marien Renaud,Valentin De Bortoli,Arthur Leclaire,Nicolas Papadakis*

Key words: 非凸势能, Langevin算法, Moreau包络, 图像反问题, 后验采样

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文研究了使用未调整Langevin算法（ULA）采样非凸势能分布的问题，证明了在势能在无穷远处强凸的假设下，离散时间ULA对漂移近似的稳定性。进一步结合Moreau包络的性质，首次证明了PSGLA在非凸势能下的收敛性，并在合成数据和图像反问题中验证了其高效性。

Motivation: 在许多实际应用中（如图像反问题），势能通常是非凸且非光滑的，需要一种高效且稳定的采样算法。现有方法如PSGLA结合了前后向优化算法和ULA步骤，但其收敛性尚未在非凸势能下得到证明。

Method: 采用未调整Langevin算法（ULA）及其变种PSGLA，通过Moreau包络性质和漂移近似稳定性分析，证明了PSGLA的收敛性。

Result: 证明了PSGLA在非凸势能下的收敛性，并在实验中验证其比SGLD更快的收敛速度和保持恢复特性的能力。

Conclusion: PSGLA在非凸势能下表现优异，不仅收敛速度更快，还能保持后验采样的恢复性质。

Abstract: We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.

</details>


### [844] [A system identification approach to clustering vector autoregressive time series](https://arxiv.org/abs/2505.14421)
*Zuogong Yue,Xinyi Wang,Victor Solo*

Key words: 时间序列聚类,自回归模型,k-LMVAR,BIC准则

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 提出了一种基于自回归动态的时间序列聚类算法k-LMVAR，解决了传统方法的计算问题，并在模拟中表现优异。

Motivation: 时间序列聚类对复杂系统建模具有重要影响，但现有方法多忽略自相关特征或依赖领域知识，需要一种更通用的方法。

Method: 基于混合自回归模型推导聚类算法，并开发其计算高效的小噪声极限版本k-LMVAR，结合BIC准则选择聚类数量和模型阶数。

Result: 算法在模拟中表现优异，计算效率高。

Conclusion: k-LMVAR是一种高效且通用的时间序列聚类方法，适用于向量时间序列。

Abstract: Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.

</details>


### [845] [A simple estimator of the correlation kernel matrix of a determinantal point process](https://arxiv.org/abs/2505.14529)
*Christian Gouriéroux,Yang Lu*

Key words: 确定点过程（DPP）、核估计、最大似然估计、相合性、渐近正态性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种用于确定点过程（DPP）核的闭式估计器，易于实现并支持最大似然估计算法。

Motivation: DPP是一个参数化的多元二值变量模型，需要一种简单且统计性质良好的核估计方法。

Method: 提出了一种闭式核估计器，可作为最大似然估计算法的初始值。

Result: 证明了估计器的相合性、渐近正态性及大偏差性质。

Conclusion: 该估计器在理论和实践中均表现出色，适合作为DPP模型的核估计工具。

Abstract: The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.

</details>


### [846] [High-Dimensional Analysis of Bootstrap Ensemble Classifiers](https://arxiv.org/abs/2505.14587)
*Hamza Cherkaoui,Malik Tiomoko,Mohamed El Amine Seddik,Cosme Louart,Ekkehard Schnoor,Balazs Kegl*

Key words: 自举法, LSSVM, 随机矩阵理论, 集成学习, 高维数据

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文通过随机矩阵理论分析了基于Least Square Support Vector Machine (LSSVM)的自举法在高维数据和大样本量下的表现，提出了优化子集数量和正则化参数的选择策略，并通过实验验证了理论结果。

Motivation: 研究在高维和大样本背景下，自举法在LSSVM集成学习中的应用及其性能表现。

Method: 利用随机矩阵理论分析自举法对LSSVM的影响，并通过实验验证理论分析。

Result: 提出了优化子集数量和正则化参数的选择策略，提升了LSSVM的性能。

Conclusion: 自举法在高维和大样本环境下对LSSVM有显著影响，优化策略能有效提升模型表现。

Abstract: Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [847] [Sobolev Gradient Ascent for Optimal Transport: Barycenter Optimization and Convergence Analysis](https://arxiv.org/abs/2505.13660)
*Kaheon Kim,Bohan Zhou,Changbo Zhu,Xiaohui Chen*

Key words: Wasserstein重心, Sobolev梯度上升, 最优运输, 对偶形式, 全局收敛性

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 该论文提出了一种无约束的凹对偶形式，用于计算Wasserstein重心，并通过Sobolev梯度上升（SGA）算法实现了高效计算。

Motivation: 为了解决现有方法在计算Wasserstein重心时的复杂性和效率问题，作者提出了一种新的对偶形式和SGA算法，以简化计算并提高性能。

Method: 通过基于Sobolev几何的自适应梯度上升算法（SGA），避免了昂贵的c-凹投影操作，从而简化了算法并保证了全局收敛性。

Result: SGA算法在数值实验中表现出优于现有最优运输重心求解器的性能，同时保持了理论上的简洁性。

Conclusion: SGA算法提供了一种高效且理论上简化的方法来计算Wasserstein重心，避免了传统方法的复杂性，同时保持了收敛性。

Abstract: This paper introduces a new constraint-free concave dual formulation for the
Wasserstein barycenter. Tailoring the vanilla dual gradient ascent algorithm to
the Sobolev geometry, we derive a scalable Sobolev gradient ascent (SGA)
algorithm to compute the barycenter for input distributions supported on a
regular grid. Despite the algorithmic simplicity, we provide a global
convergence analysis that achieves the same rate as the classical subgradient
descent methods for minimizing nonsmooth convex functions in the Euclidean
space. A central feature of our SGA algorithm is that the computationally
expensive $c$-concavity projection operator enforced on the Kantorovich dual
potentials is unnecessary to guarantee convergence, leading to significant
algorithmic and theoretical simplifications over all existing primal and dual
methods for computing the exact barycenter. Our numerical experiments
demonstrate the superior empirical performance of SGA over the existing optimal
transport barycenter solvers.

</details>


### [848] [Sequential QCQP for Bilevel Optimization with Line Search](https://arxiv.org/abs/2505.14647)
*Sina Sharifi,Erfan Yazdandoost Hamedani,Mahyar Fazlyab*

Key words: 双层优化, QCQP, 控制障碍函数, 单循环算法, 无需调参

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 本文提出了一种单循环、无需调参的双层优化算法，确保任何时间可行性，并在上层目标下降的同时近似满足下层最优条件。

Motivation: 双层优化问题具有复杂的层级依赖关系，现有方法通常需要调参且计算复杂，因此需要一种高效、无需调参的算法。

Method: 通过求解凸二次约束二次规划（QCQP）获得搜索方向，并利用基于控制障碍函数的回溯线搜索确保步长安全。

Result: 所提算法在温和的局部正则假设下收敛，并具有O(1/k)的遍历收敛速率。

Conclusion: 该算法高效、可扩展，无需调参，在典型双层优化任务中表现优异。

Abstract: Bilevel optimization involves a hierarchical structure where one problem is
nested within another, leading to complex interdependencies between levels. We
propose a single-loop, tuning-free algorithm that guarantees anytime
feasibility, i.e., approximate satisfaction of the lower-level optimality
condition, while ensuring descent of the upper-level objective. At each
iteration, a convex quadratically-constrained quadratic program (QCQP) with a
closed-form solution yields the search direction, followed by a backtracking
line search inspired by control barrier functions to ensure safe, uniformly
positive step sizes. The resulting method is scalable, requires no
hyperparameter tuning, and converges under mild local regularity assumptions.
We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's
effectiveness on representative bilevel tasks.

</details>


### [849] [Sobolev Gradient Ascent for Optimal Transport: Barycenter Optimization and Convergence Analysis](https://arxiv.org/abs/2505.13660)
*Kaheon Kim,Bohan Zhou,Changbo Zhu,Xiaohui Chen*

Key words: Wasserstein barycenter, Sobolev梯度上升, 最优传输, c-凹投影, 全局收敛

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 本文介绍了一种用于Wasserstein重心计算的无约束凹对偶公式，并提出了一种基于Sobolev几何的可扩展Sobolev梯度上升算法(SGA)。该算法在保证全局收敛的同时，简化了计算复杂度，显著优于现有方法。

Motivation: 现有Wasserstein重心计算方法复杂且计算成本高，需要寻找更高效且理论简化的算法。

Method: 基于Sobolev几何设计的Sobolev梯度上升算法(SGA)，避免了昂贵的c-凹投影操作。

Result: SGA算法在数值实验中表现出优于现有最优传输重心求解器的性能。

Conclusion: SGA算法理论简单且计算高效，为Wasserstein重心计算提供了新的解决方案。

Abstract: This paper introduces a new constraint-free concave dual formulation for the
Wasserstein barycenter. Tailoring the vanilla dual gradient ascent algorithm to
the Sobolev geometry, we derive a scalable Sobolev gradient ascent (SGA)
algorithm to compute the barycenter for input distributions supported on a
regular grid. Despite the algorithmic simplicity, we provide a global
convergence analysis that achieves the same rate as the classical subgradient
descent methods for minimizing nonsmooth convex functions in the Euclidean
space. A central feature of our SGA algorithm is that the computationally
expensive $c$-concavity projection operator enforced on the Kantorovich dual
potentials is unnecessary to guarantee convergence, leading to significant
algorithmic and theoretical simplifications over all existing primal and dual
methods for computing the exact barycenter. Our numerical experiments
demonstrate the superior empirical performance of SGA over the existing optimal
transport barycenter solvers.

</details>


### [850] [Sequential QCQP for Bilevel Optimization with Line Search](https://arxiv.org/abs/2505.14647)
*Sina Sharifi,Erfan Yazdandoost Hamedani,Mahyar Fazlyab*

Key words: 双层优化；单循环算法；QCQP；控制屏障函数；收敛性

<details>
  <summary>Details</summary>

Main category: math.OC

TL;DR: 提出了一种单循环、无需调参的双层优化算法，保证可行性和收敛。

Motivation: 解决双层优化问题中复杂的层级依赖关系，同时确保上层目标下降和下层最优条件的近似满足。

Method: 采用单循环算法，通过凸二次约束二次规划（QCQP）生成搜索方向，并结合回溯线搜索确保步长安全。

Result: 算法无需超参数调优，具有可扩展性，并在温和的局部正则假设下收敛，展示了O(1/k)的遍历收敛率。

Conclusion: 所提方法在代表性双层任务中表现有效，适用于复杂优化问题。

Abstract: Bilevel optimization involves a hierarchical structure where one problem is
nested within another, leading to complex interdependencies between levels. We
propose a single-loop, tuning-free algorithm that guarantees anytime
feasibility, i.e., approximate satisfaction of the lower-level optimality
condition, while ensuring descent of the upper-level objective. At each
iteration, a convex quadratically-constrained quadratic program (QCQP) with a
closed-form solution yields the search direction, followed by a backtracking
line search inspired by control barrier functions to ensure safe, uniformly
positive step sizes. The resulting method is scalable, requires no
hyperparameter tuning, and converges under mild local regularity assumptions.
We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's
effectiveness on representative bilevel tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [851] [When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making](https://arxiv.org/abs/2505.14377)
*Ulrike Kuhl,Annika Bush*

Key words: AI偏见、反事实解释、决策影响、公平性、XAI

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究发现有偏见的AI推荐会潜移默化地影响人类决策，反事实解释可逆转这种偏见，但信任度未受影响。

Motivation: 探究AI偏见如何影响人类决策，特别是在招聘场景中，以及反事实解释的作用。

Method: 通过模拟招聘实验，分三个阶段（无AI、有偏见AI推荐、无AI）观察参与者决策，并引入反事实解释。

Result: 70%的决策受AI偏见影响，仅少数人察觉偏见；反事实解释能逆转偏见，但对信任度无显著影响。

Conclusion: 需谨慎设计可解释AI以避免偏见传播，确保公平决策。

Abstract: Although the integration of artificial intelligence (AI) into everyday tasks
improves efficiency and objectivity, it also risks transmitting bias to human
decision-making. In this study, we conducted a controlled experiment that
simulated hiring decisions to examine how biased AI recommendations - augmented
with or without counterfactual explanations - influence human judgment over
time. Participants, acting as hiring managers, completed 60 decision trials
divided into a baseline phase without AI, followed by a phase with biased (X)AI
recommendations (favoring either male or female candidates), and a final
post-interaction phase without AI. Our results indicate that the participants
followed the AI recommendations 70% of the time when the qualifications of the
given candidates were comparable. Yet, only a fraction of participants detected
the gender bias (8 out of 294). Crucially, exposure to biased AI altered
participants' inherent preferences: in the post-interaction phase,
participants' independent decisions aligned with the bias when no
counterfactual explanations were provided before, but reversed the bias when
explanations were given. Reported trust did not differ significantly across
conditions. Confidence varied throughout the study phases after exposure to
male-biased AI, indicating nuanced effects of AI bias on decision certainty.
Our findings point to the importance of calibrating XAI to avoid unintended
behavioral shifts in order to safeguard equitable decision-making and prevent
the adoption of algorithmic bias.

</details>


### [852] [How Managers Perceive AI-Assisted Conversational Training for Workplace Communication](https://arxiv.org/abs/2505.14452)
*Lance T Wilhelm,Xiaohan Ding,Kirk McInnis Knutsen,Buse Carik,Eugenia H Rho*

Key words: 职场沟通, AI辅助培训, 角色扮演, 自适应模拟, 反馈机制

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究探讨了人工智能辅助交流系统在管理沟通培训中的应用潜力，发现管理者重视自适应、低风险的模拟练习，并提出了AI辅助培训需平衡个性化与结构性。

Motivation: 职场有效沟通对管理成功至关重要，但缺乏针对性培训。研究旨在探索AI如何帮助管理者提升沟通技能。

Method: 设计了一个名为CommCoach的会话角色扮演系统，通过半结构化访谈了解管理者对AI辅助沟通培训的期望。

Result: 参与者重视自适应模拟、透明反馈和控制AI生成角色，同时指出需平衡个性化、结构性及适应性。

Conclusion: AI辅助沟通培训需兼顾个性化与结构性，同时解决自适应与一致反馈、真实性与偏见的矛盾。

Abstract: Effective workplace communication is essential for managerial success, yet
many managers lack access to tailored and sustained training. Although
AI-assisted communication systems may offer scalable training solutions, little
is known about how managers envision the role of AI in helping them improve
their communication skills. To investigate this, we designed a conversational
role-play system, CommCoach, as a functional probe to understand how managers
anticipate using AI to practice their communication skills. Through
semi-structured interviews, participants emphasized the value of adaptive,
low-risk simulations for practicing difficult workplace conversations. They
also highlighted opportunities, including human-AI teaming, transparent and
context-aware feedback, and greater control over AI-generated personas.
AI-assisted communication training should balance personalization, structured
learning objectives, and adaptability to different user styles and contexts.
However, achieving this requires carefully navigating tensions between adaptive
and consistent AI feedback, realism and potential bias, and the open-ended
nature of AI conversations versus structured workplace discourse.

</details>


### [853] [When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making](https://arxiv.org/abs/2505.14377)
*Ulrike Kuhl,Annika Bush*

Key words: AI偏见、反事实解释、决策影响、性别偏见、XAI

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究通过模拟招聘实验发现，有偏见的AI推荐会显著影响人类决策，导致偏见延续或逆转，取决于是否提供反事实解释。

Motivation: 探索AI偏见如何通过交互影响人类决策，尤其是招聘中的性别偏见。

Method: 通过模拟招聘实验，分阶段测试参与者与AI互动的决策行为。

Result: 参与者70%时间遵循AI推荐，但很少察觉偏见；反事实解释可逆转偏见影响。

Conclusion: 需校准XAI以减少偏见传播，确保决策公平。

Abstract: Although the integration of artificial intelligence (AI) into everyday tasks
improves efficiency and objectivity, it also risks transmitting bias to human
decision-making. In this study, we conducted a controlled experiment that
simulated hiring decisions to examine how biased AI recommendations - augmented
with or without counterfactual explanations - influence human judgment over
time. Participants, acting as hiring managers, completed 60 decision trials
divided into a baseline phase without AI, followed by a phase with biased (X)AI
recommendations (favoring either male or female candidates), and a final
post-interaction phase without AI. Our results indicate that the participants
followed the AI recommendations 70% of the time when the qualifications of the
given candidates were comparable. Yet, only a fraction of participants detected
the gender bias (8 out of 294). Crucially, exposure to biased AI altered
participants' inherent preferences: in the post-interaction phase,
participants' independent decisions aligned with the bias when no
counterfactual explanations were provided before, but reversed the bias when
explanations were given. Reported trust did not differ significantly across
conditions. Confidence varied throughout the study phases after exposure to
male-biased AI, indicating nuanced effects of AI bias on decision certainty.
Our findings point to the importance of calibrating XAI to avoid unintended
behavioral shifts in order to safeguard equitable decision-making and prevent
the adoption of algorithmic bias.

</details>


### [854] [How Managers Perceive AI-Assisted Conversational Training for Workplace Communication](https://arxiv.org/abs/2505.14452)
*Lance T Wilhelm,Xiaohan Ding,Kirk McInnis Knutsen,Buse Carik,Eugenia H Rho*

Key words: AI辅助沟通、职场培训、人机协作、反馈透明性

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究表明，AI辅助沟通系统如CommCoach可帮助管理者提升职场沟通技能，但需平衡个性化与一致性。

Motivation: 探索管理者如何利用AI改善沟通技能，以解决缺乏定制化培训的问题。

Method: 设计CommCoach系统，通过半结构化访谈收集管理者对AI辅助沟通训练的看法。

Result: 参与者认可适应性模拟的价值，并提出人机协作、透明反馈等需求。

Conclusion: AI辅助训练需平衡个性化、学习目标和适应性，并解决反馈一致性与偏见的挑战。

Abstract: Effective workplace communication is essential for managerial success, yet
many managers lack access to tailored and sustained training. Although
AI-assisted communication systems may offer scalable training solutions, little
is known about how managers envision the role of AI in helping them improve
their communication skills. To investigate this, we designed a conversational
role-play system, CommCoach, as a functional probe to understand how managers
anticipate using AI to practice their communication skills. Through
semi-structured interviews, participants emphasized the value of adaptive,
low-risk simulations for practicing difficult workplace conversations. They
also highlighted opportunities, including human-AI teaming, transparent and
context-aware feedback, and greater control over AI-generated personas.
AI-assisted communication training should balance personalization, structured
learning objectives, and adaptability to different user styles and contexts.
However, achieving this requires carefully navigating tensions between adaptive
and consistent AI feedback, realism and potential bias, and the open-ended
nature of AI conversations versus structured workplace discourse.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [855] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/abs/2505.13577)
*Yubin Kim,Taehan Kim,Wonjune Kang,Eugene Park,Joonsik Yoon,Dongjae Lee,Xin Liu,Daniel McDuff,Hyeonhoon Lee,Cynthia Breazeal,Hae Won Park*

Key words: 嗓音健康、大语言模型、诊断准确性、跨语言性能、伦理验证

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 本文介绍了VocalAgent，一个基于音频大语言模型（LLM）的工具，用于通过声音健康诊断解决全球普遍存在的嗓音障碍问题，展示了其在分类准确性上的优越性。

Motivation: 全球许多人面临嗓音障碍问题，但缺乏便捷的诊断和治疗途径。VocalAgent旨在通过LLM技术提供一种可扩展的解决方案。

Method: 利用Qwen-Audio-Chat模型，并在医院患者数据集上进行微调，采用多层面评估框架，包括安全性评估、跨语言性能分析和模态消融研究。

Result: VocalAgent在嗓音障碍分类上表现优于现有基准方法。

Conclusion: VocalAgent为健康诊断的广泛应用提供了可行性，同时强调了对伦理和技术验证的重要性。

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [856] [ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech](https://arxiv.org/abs/2505.13805)
*Yu Pan,Yanni Hu,Yuguang Yang,Jixun Yao,Jianhao Ye,Hongbin Zhou,Lei Ma,Jianjun Zhao*

Key words: 情感语音转换, 自然语言提示, 流匹配, 跨模态对齐

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文介绍了ClapFM-EVC，一种新颖的情感语音转换框架，通过自然语言提示或参考语音实现高质量转换，并支持情感强度调整。

Motivation: 现有情感语音转换技术在实现高保真、灵活及可解释控制方面仍面临挑战。

Method: 提出EVC-CLAP模型跨模态对齐情感元素，结合FuEncoder和自适应强度门融合特征，采用流匹配模型重建频谱。

Result: 主观和客观评估验证了ClapFM-EVC的有效性。

Conclusion: ClapFM-EVC在情感语音转换中表现出色。

Abstract: Despite great advances, achieving high-fidelity emotional voice conversion
(EVC) with flexible and interpretable control remains challenging. This paper
introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality
converted speech driven by natural language prompts or reference speech with
adjustable emotion intensity. We first propose EVC-CLAP, an emotional
contrastive language-audio pre-training model, guided by natural language
prompts and categorical labels, to extract and align fine-grained emotional
elements across speech and text modalities. Then, a FuEncoder with an adaptive
intensity gate is presented to seamless fuse emotional features with Phonetic
PosteriorGrams from a pre-trained ASR model. To further improve emotion
expressiveness and speech naturalness, we propose a flow matching model
conditioned on these captured features to reconstruct Mel-spectrogram of source
speech. Subjective and objective evaluations validate the effectiveness of
ClapFM-EVC.

</details>


### [857] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
*Tianle Yang,Chengzhe Sun,Siwei Lyu,Phil Rose*

Key words: 深度伪造音频, 声学特征, 分段语音, 法医语音比对

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 研究探讨了利用分段语音的声学特征检测深度伪造音频的潜力，结果表明某些分段特征对识别伪造音频有效。

Motivation: 探索基于人类发音过程的可解释声学特征，以检测难以被深度伪造模型复制的音频。

Method: 使用分段语音的声学特征进行检测，重点关注法医语音比对中常用的特征。

Result: 某些分段特征能有效识别深度伪造音频，而某些全局特征效果不佳。

Conclusion: 研究为音频深度伪造检测提供了新视角，强调需采用不同于法医语音比对的方法。

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [858] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/abs/2505.13971)
*Ming Gao,Shilong Wu,Hang Chen,Jun Du,Chin-Hui Lee,Shinji Watanabe,Jingdong Chen,Siniscalchi Sabato Marco,Odette Scharenborg*

Key words: MISP 2025, 多模态, 会议转录, AVSD, AVSR, AVDR

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: MISP 2025挑战赛专注于多模态、多设备会议转录，结合视频和音频模态，包括AVSD、AVSR和AVDR任务，参与者提出解决方案显著提升了基线性能。

Motivation: 会议场景中复杂的声学条件对语音应用提出了挑战，多模态（视频+音频）方法有望提升转录性能。

Method: 挑战赛通过音频-视频模态结合，设置了AVSD、AVSR和AVDR任务，参与者提交的解决方案包括基线系统的改进。

Result: 最佳AVSD模型DER为8.09%（提升7.43%），最佳AVSR系统CER为9.48%（提升10.62%），最佳AVDR系统cpCER为11.56%（提升72.49%）。

Conclusion: 多模态方法显著提升了会议转录的性能，视频模态的引入是有效的。

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [859] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Yuqing Cai,Yongbin Yu,Renzeng Duojie,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Key words: 藏语，多方言 TTS，少样本学习，说话人相似性，方言转换

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: FMSD-TTS 是一个针对藏语多方言的少样本、多说话人、多方言的文本转语音框架，通过新颖的说话人-方言融合模块和方言专用动态路由网络（DSDR-Net）显著提升了方言表达和说话人相似性。

Motivation: 藏语是一种低资源语言，缺乏涵盖其三大方言（卫藏、安多和康巴）的平行语音语料库，限制语音建模的进展。

Method: FMSD-TTS 采用说话人-方言融合模块和 DSDR-Net 来捕捉精细的语音和语言变化，同时保留说话人身份。

Result: 客观和主观评估显示，FMSD-TTS 在方言表达和说话人相似性上显著优于基线模型。

Conclusion: 该研究不仅提出了针对藏语的少样本 TTS 系统，还公开了大规模合成语音语料库和开源评估工具包。

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [860] [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
*Sho Inoue,Shai Wang,Haizhou Li*

Key words: 对话系统,个性标注,语音处理,大语言模型

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出一种方法，通过处理语音数据生成带标注的对话数据集，结合大语言模型预测对话个性，优于现有方法。

Motivation: 语音数据缺乏个性标注，限制了能适应个性的对话系统的发展。

Method: 预处理语音数据，提取文本和时间戳，生成对话级标注，结合大语言模型预测个性。

Result: 系统预测结果与人类判断更一致，优于现有方法。

Conclusion: 提出的方法能有效生成个性标注数据，提升对话系统的个性适应性。

Abstract: Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.

</details>


### [861] [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
*Yuanbo Fang,Haoze Sun,Jun Liu,Tao Zhang,Zenan Zhou,Weipeng Chen,Xiaofen Xing,Xiangmin Xu*

Key words: 语音大语言模型, 性能下降, S2SBench, Baichuan-Audio

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 本文提出了S2SBench基准，用于量化语音大语言模型的性能下降现象，并通过诊断数据集和评估协议进行分析，应用于Baichuan-Audio模型的训练过程。

Motivation: 研究语音大语言模型在直接处理音频令牌时可能导致推理和生成性能下降（称为智能退化）的问题。

Method: 设计了S2SBench基准，包含针对句子延续和常识推理的诊断数据集，以及基于困惑度差异的成对评估协议。

Result: S2SBench成功应用于Baichuan-Audio模型的训练过程，证明了其有效性。

Conclusion: S2SBench为量化语音大语言模型的性能下降提供了一种系统化的评估方法。

Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.

</details>


### [862] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
*Nadav Har-Tuv,Or Tal,Yossi Adi*

Key words: 语音建模, 端到端框架, 实时应用, 语音表示

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: PAST是一个端到端框架，联合建模语音信息和信号重建，无需依赖外部预训练模型，直接整合领域知识并通过辅助任务优化。

Motivation: 现有方法依赖预训练自监督模型，PAST旨在通过监督语音数据直接整合领域知识，提升语音表现力和实时应用能力。

Method: PAST利用监督语音数据和辅助任务进行联合建模，并开发了可流式处理的因果变体以实现实时应用。

Result: PAST在语音表示和重建任务上超越现有基准方法，并在语音语言模型中表现出色。

Conclusion: PAST为语音生成任务提供了高效基础框架，代码和模型已开源。

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


### [863] [Score-Based Training for Energy-Based TTS Models](https://arxiv.org/abs/2505.13771)
*Wanli Sun,Anton Ragni*

Key words: 噪声对比估计, 能量基模型, 切片分数匹配, 扩散模型, 一阶优化

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出了一种新标准，用于提升能量基模型的一阶优化性能，对比了噪声对比估计和切片分数匹配的优缺点。

Motivation: 现有的噪声对比估计（NCE）和切片分数匹配（SSM）方法忽略了似然函数的形式问题，影响了能量基模型和扩散模型中的一阶优化性能。

Method: 提出了一种新标准，通过更适应一阶优化方案的分数学习方法来改进模型训练。

Result: 实验对比了新方法与传统NCE和SSM在训练能量基模型中的表现。

Conclusion: 新方法能够更有效地训练模型，尤其是在一阶优化场景中。

Abstract: Noise contrastive estimation (NCE) is a popular method for training
energy-based models (EBM) with intractable normalisation terms. The key idea of
NCE is to learn by comparing unnormalised log-likelihoods of the reference and
noisy samples, thus avoiding explicitly computing normalisation terms. However,
NCE critically relies on the quality of noisy samples. Recently, sliced score
matching (SSM) has been popularised by closely related diffusion models (DM).
Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning
distribution of its projections on randomly chosen directions. However, both
NCE and SSM disregard the form of log-likelihood function, which is problematic
given that EBMs and DMs make use of first-order optimisation during inference.
This paper proposes a new criterion that learns scores more suitable for
first-order schemes. Experiments contrasts these approaches for training EBMs.

</details>


### [864] [AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis](https://arxiv.org/abs/2505.14285)
*Eirini Panteli,Paulo E. Santos,Nabil Humphrey*

Key words: 水下声学,深度学习,去噪,分类,新颖性检测

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: AquaSignal是一个模块化、可扩展的水下声学信号处理管道，结合了深度学习方法，用于去噪、分类和新颖性检测，在真实海洋环境中表现出色。

Motivation: 由于水下声学信号在嘈杂和动态的海洋环境中难以分析，AquaSignal旨在通过深度学习技术提高分析的可靠性和准确性。

Method: AquaSignal采用U-Net去噪，ResNet18分类已知声学事件，AutoEncoder检测异常信号，并在Deepship和ONC数据集上进行了评估。

Result: 实验结果显示，AquaSignal在分类和新颖性检测中的准确率分别为71%和91%，信号清晰度和任务性能显著提升。

Conclusion: AquaSignal在实时水下声学监测方面具有潜力，适用于科学、环境和海洋领域。

Abstract: This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.

</details>


### [865] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/abs/2505.13577)
*Yubin Kim,Taehan Kim,Wonjune Kang,Eugene Park,Joonsik Yoon,Dongjae Lee,Xin Liu,Daniel McDuff,Hyeonhoon Lee,Cynthia Breazeal,Hae Won Park*

Key words: 声音健康,LLM,诊断,跨语言,伦理

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: VocalAgent是一个基于音频大语言模型（LLM）的系统，用于远程诊断声音健康问题，表现优于现有基准方法。

Motivation: 全球声音障碍普遍，但诊断和治疗资源不足，亟需便捷解决方案。

Method: 采用Qwen-Audio-Chat模型，基于医院患者数据微调，并通过多维度评估框架验证。

Result: 在声音障碍分类任务中表现优异，且具备跨语言能力。

Conclusion: VocalAgent为声音健康诊断提供了可扩展的解决方案，同时强调了伦理与技术验证的重要性。

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [866] [ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech](https://arxiv.org/abs/2505.13805)
*Yu Pan,Yanni Hu,Yuguang Yang,Jixun Yao,Jianhao Ye,Hongbin Zhou,Lei Ma,Jianjun Zhao*

Key words: 情感语音转换, 多模态对齐, 自适应强度门, 流匹配, 自然语言提示

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: ClapFM-EVC是一种新型情感语音转换（EVC）框架，通过自然语言提示或参考语音驱动，支持情感强度调节，实现了高质量语音转换。

Motivation: 尽管情感语音转换取得进展，但实现高保真且可控性强的EVC仍具挑战性。本文旨在通过多模态对齐和自适应融合技术解决这一问题。

Method: 1. 提出EVC-CLAP模型，通过自然语言提示和分类标签提取并对齐跨模态的细粒度情感特征；2. 设计带自适应强度门的FuEncoder，融合情感特征与语音后验图；3. 采用基于流匹配的模型重构Mel频谱。

Result: 主观和客观评估验证了ClapFM-EVC的有效性，实现了高情感表现力和语音自然度。

Conclusion: ClapFM-EVC通过多模态对齐和自适应特征融合，显著提升了EVC的性能和可控性。

Abstract: Despite great advances, achieving high-fidelity emotional voice conversion
(EVC) with flexible and interpretable control remains challenging. This paper
introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality
converted speech driven by natural language prompts or reference speech with
adjustable emotion intensity. We first propose EVC-CLAP, an emotional
contrastive language-audio pre-training model, guided by natural language
prompts and categorical labels, to extract and align fine-grained emotional
elements across speech and text modalities. Then, a FuEncoder with an adaptive
intensity gate is presented to seamless fuse emotional features with Phonetic
PosteriorGrams from a pre-trained ASR model. To further improve emotion
expressiveness and speech naturalness, we propose a flow matching model
conditioned on these captured features to reconstruct Mel-spectrogram of source
speech. Subjective and objective evaluations validate the effectiveness of
ClapFM-EVC.

</details>


### [867] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
*Tianle Yang,Chengzhe Sun,Siwei Lyu,Phil Rose*

Key words: 深度伪造音频, 声学特征, 法医语音比对, 分段特征, 音频检测

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 利用语音分段声学特征检测深度伪造音频，结果显示部分特征有效，为法医语音比对提供了新视角。

Motivation: 探索可解释的声学特征在检测伪造音频中的潜力，特别关注其与人类发音过程的关联。

Method: 通过分析语音分段声学特征（常用于法医语音比对）来识别伪造音频。

Result: 部分分段特征在检测伪造音频中有效，而某些全局特征效果不佳。

Conclusion: 为音频伪造检测提供了新方法，强调需针对法医语音比对调整检测策略。

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [868] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/abs/2505.13971)
*Ming Gao,Shilong Wu,Hang Chen,Jun Du,Chin-Hui Lee,Shinji Watanabe,Jingdong Chen,Siniscalchi Sabato Marco,Odette Scharenborg*

Key words: 会议转录、多模态、MISP 2025、AVSD、AVSR、AVDR

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: MISP 2025 Challenge专注于多模态、多设备会议转录，结合视频和音频，任务包括AVSD、AVSR和AVDR。最佳系统显著优于基线。

Motivation: 解决会议场景中复杂声学条件下的语音应用挑战。

Method: 结合视频和音频模态，设计AVSD、AVSR和AVDR任务，提供数据集和基线系统。

Result: 最佳AVSD系统DER为8.09%，提升7.43%；最佳AVSR系统CER为9.48%，提升10.62%；最佳AVDR系统cpCER为11.56%，提升72.49%。

Conclusion: 多模态方法显著提升会议转录性能。

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [869] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Yuqing Cai,Yongbin Yu,Renzeng Duojie,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Key words: 低资源语言, 多方言合成, few-shot学习, 藏语语音合成

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: FMSD-TTS是一个面向低资源语言（藏语）的多方言、多说话人的few-shot文本到语音合成框架，通过创新的模块设计和网络结构显著提升了方言表达的准确性和说话人相似性。

Motivation: 藏语作为一种低资源语言，缺乏跨方言的平行语音语料库，限制了语音建模的进展。

Method: 作者提出了FMSD-TTS框架，包含说话人-方言融合模块和Dialect-Specialized Dynamic Routing Network (DSDR-Net)，以捕捉方言间的声学和语言差异。

Result: 实验表明，FMSD-TTS在方言表达和说话人相似性上显著优于基线系统。

Conclusion: FMSD-TTS为藏语多方言语音合成提供了有效的解决方案，并公开了合成语音库和评估工具包。

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [870] [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
*Sho Inoue,Shai Wang,Haizhou Li*

Key words: 神经网络对话系统,人格感知,语音标注,ASR,大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种为语音对话数据集标注人格特征的方法，利用ASR和大型语言模型预测对话人格，并证明其比现有方法更符合人类判断。

Motivation: 由于语音数据集中缺乏人格标注，研究如何构建能基于人格调整行为的对话系统。

Method: 预处理原始音频，使用ASR提取转录和时间戳，生成对话级标注，并利用大型语言模型预测人格。

Result: 系统预测的人格与人类判断更一致。

Conclusion: 该方法有效解决了语音数据集中人格标注缺失的问题，提升了对话系统的个性适应性。

Abstract: Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.

</details>


### [871] [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
*Yuanbo Fang,Haoze Sun,Jun Liu,Tao Zhang,Zenan Zhou,Weipeng Chen,Xiaofen Xing,Xiangmin Xu*

Key words: 语音大语言模型,性能退化,基准测试,困惑度

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了S2SBench基准来量化语音大语言模型（LLMs）在音频输入下的推理和生成性能下降问题，并通过诊断数据集和评估协议分析性能差距。

Motivation: 语音LLMs在处理音频输入时会出现推理和生成性能下降（智能退化），需要系统评估这一差距。

Method: 设计了S2SBench基准，包括针对句子延续和常识推理的诊断数据集，并引入基于困惑度差异的成对评估协议。

Result: 应用于Baichuan-Audio的训练过程分析，验证了基准的有效性。

Conclusion: S2SBench能有效量化语音LLMs的性能退化，为研究音频输入下的模型表现提供了工具和数据集。

Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.

</details>


### [872] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
*Nadav Har-Tuv,Or Tal,Yossi Adi*

Key words: 语音处理,端到端框架,语音标记化,实时应用

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: PAST是一种新型端到端框架，结合语音信息与信号重建，无需外部预训练模型，优于现有基准方法。

Motivation: 现有语音处理方法依赖预训练的自监督模型，而PAST旨在通过直接利用监督语音数据，将领域知识融入标记化过程。

Method: PAST通过辅助任务集成监督语音数据，并引入可流式处理的因果变体，支持实时语音应用。

Result: PAST在语音表示和重建指标上超越现有基准方法，并作为语音语言模型的表示表现优异。

Conclusion: PAST为语音生成任务提供了高效基础，相关代码和模型已开源。

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


### [873] [Score-Based Training for Energy-Based TTS Models](https://arxiv.org/abs/2505.13771)
*Wanli Sun,Anton Ragni*

Key words: 噪声对比估计, 基于能量的模型, 切片得分匹配, 扩散模型, 一阶优化

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 本文提出了一种新的学习方法，以优化基于能量的模型（EBM）的得分函数，使其更适合一阶优化方案。

Motivation: 现有的噪声对比估计（NCE）和切片得分匹配（SSM）方法存在对对数似然函数形式的忽视问题，尤其是在基于能量的模型（EBM）和扩散模型（DM）中采用一阶优化时表现不佳。

Method: 作者提出了一种新的标准，通过学习更适应一阶优化方案的得分函数来改进模型训练。

Result: 实验表明，新方法在训练EBMs时优于NCE和SSM方法。

Conclusion: 新方法通过学习更适合一阶优化的得分函数，显著提升了EBMs的训练效果。

Abstract: Noise contrastive estimation (NCE) is a popular method for training
energy-based models (EBM) with intractable normalisation terms. The key idea of
NCE is to learn by comparing unnormalised log-likelihoods of the reference and
noisy samples, thus avoiding explicitly computing normalisation terms. However,
NCE critically relies on the quality of noisy samples. Recently, sliced score
matching (SSM) has been popularised by closely related diffusion models (DM).
Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning
distribution of its projections on randomly chosen directions. However, both
NCE and SSM disregard the form of log-likelihood function, which is problematic
given that EBMs and DMs make use of first-order optimisation during inference.
This paper proposes a new criterion that learns scores more suitable for
first-order schemes. Experiments contrasts these approaches for training EBMs.

</details>


### [874] [AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis](https://arxiv.org/abs/2505.14285)
*Eirini Panteli,Paulo E. Santos,Nabil Humphrey*

Key words: 水下声学信号；深度学习；U-Net；ResNet18；AutoEncoder

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: AquaSignal是一个模块化和可扩展的水下声学信号处理管道，结合深度学习技术提升信号分析的可靠性。

Motivation: 解决嘈杂和动态海洋环境中水下声学信号的预处理、去噪、分类和新颖性检测问题。

Method: 采用U-Net进行信号去噪，ResNet18进行分类，AutoEncoder进行新颖信号检测。

Result: 在分类和新颖性检测任务上分别达到71%和91%的准确率，提升了信号清晰度。

Conclusion: AquaSignal在实时水下声学监测中展现出强大潜力。

Abstract: This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [875] [ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets](https://arxiv.org/abs/2505.14016)
*Shunjing Zhao,Xian Shi,Hanlun Lei*

Key words: 彗星活动, ThermoONet, 机器学习, 热物理模型, 计算效率

<details>
  <summary>Details</summary>

Main category: astro-ph.EP

TL;DR: ThermoONet是一种基于机器学习的神经网络，用于预测彗星温度和冰升华通量，显著降低计算成本，并成功应用于彗星67P和21P的水活动建模。

Motivation: 传统数值方法在彗星热物理模型研究中计算成本高昂，限制了高分辨率或重复建模的需求。

Method: 采用机器学习方法开发ThermoONet神经网络，预测彗星温度和冰升华通量，并与数值模拟结果对比。

Result: ThermoONet在温度预测中平均误差约2%，计算时间减少近六个数量级，成功拟合彗星67P和21P的水生成曲线。

Conclusion: ThermoONet高效且有效，结合全局优化算法可反推目标天体的物理性质。

Abstract: Cometary activity is a compelling subject of study, with thermophysical
models playing a pivotal role in its understanding. However, traditional
numerical solutions for small body thermophysical models are computationally
intensive, posing challenges for investigations requiring high-resolution or
repetitive modeling. To address this limitation, we employed a machine learning
approach to develop ThermoONet - a neural network designed to predict the
temperature and water ice sublimation flux of comets. Performance evaluations
indicate that ThermoONet achieves a low average error in subsurface temperature
of approximately 2% relative to the numerical simulation, while reducing
computational time by nearly six orders of magnitude. We applied ThermoONet to
model the water activity of comets 67P/Churyumov-Gerasimenko and
21P/Giacobini-Zinner. By successfully fitting the water production rate curves
of these comets, as obtained by the Rosetta mission and the SOHO telescope,
respectively, we demonstrate the network's effectiveness and efficiency.
Furthermore, when combined with a global optimization algorithm, ThermoONet
proves capable of retrieving the physical properties of target bodies.

</details>


### [876] [ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets](https://arxiv.org/abs/2505.14016)
*Shunjing Zhao,Xian Shi,Hanlun Lei*

Key words: 彗星活动、热物理模型、机器学习、ThermoONet、计算效率

<details>
  <summary>Details</summary>

Main category: astro-ph.EP

TL;DR: 利用机器学习开发的ThermoONet神经网络，用于预测彗星的温度和冰升华通量，显著提高了计算效率。

Motivation: 传统的小天体热物理模型数值解法计算量大，限制了高分辨率或重复建模的研究需求。

Method: 采用机器学习方法开发神经网络ThermoONet，预测彗星的温度和冰升华通量。

Result: ThermoONet将计算时间降低近六个数量级，温度预测平均误差约为2%，并成功应用于67P和21P彗星的建模。

Conclusion: ThermoONet不仅高效且有效，还能与全局优化算法结合，用于反演目标天体的物理特性。

Abstract: Cometary activity is a compelling subject of study, with thermophysical
models playing a pivotal role in its understanding. However, traditional
numerical solutions for small body thermophysical models are computationally
intensive, posing challenges for investigations requiring high-resolution or
repetitive modeling. To address this limitation, we employed a machine learning
approach to develop ThermoONet - a neural network designed to predict the
temperature and water ice sublimation flux of comets. Performance evaluations
indicate that ThermoONet achieves a low average error in subsurface temperature
of approximately 2% relative to the numerical simulation, while reducing
computational time by nearly six orders of magnitude. We applied ThermoONet to
model the water activity of comets 67P/Churyumov-Gerasimenko and
21P/Giacobini-Zinner. By successfully fitting the water production rate curves
of these comets, as obtained by the Rosetta mission and the SOHO telescope,
respectively, we demonstrate the network's effectiveness and efficiency.
Furthermore, when combined with a global optimization algorithm, ThermoONet
proves capable of retrieving the physical properties of target bodies.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [877] [RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework](https://arxiv.org/abs/2505.13808)
*Faramarz Safi Esfahani,Ghassan Beydoun,Morteza Saberi,Brad McCusker,Biswajeet Pradhan*

Key words: 元启发式, 自适应优化, 动态选择, 性能反馈, AI决策

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 论文介绍了自适应的多态元启发式框架（PMF），通过实时性能反馈动态选择算法，提升优化效率和适应性。

Motivation: 传统元启发式算法因固定结构和调参限制，难以适应复杂优化问题。

Method: PMF利用PMA和PMSA代理，基于性能指标动态选择和切换算法。

Result: 在基准测试中，PMF显著提升收敛速度、适应性和解的质量，优于传统方法。

Conclusion: PMF结合AI决策和自校正机制，为智能优化框架提供新方向。

Abstract: Metaheuristic algorithms are widely used for solving complex optimization
problems, yet their effectiveness is often constrained by fixed structures and
the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)
addresses this limitation by introducing a self-adaptive metaheuristic
switching mechanism driven by real-time performance feedback and dynamic
algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)
and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select
and transition between metaheuristic algorithms based on key performance
indicators, ensuring continuous adaptation. This approach enhances convergence
speed, adaptability, and solution quality, outperforming traditional
metaheuristics in high-dimensional, dynamic, and multimodal environments.
Experimental results on benchmark functions demonstrate that PMF significantly
improves optimization efficiency by mitigating stagnation and balancing
exploration-exploitation strategies across various problem landscapes. By
integrating AI-driven decision-making and self-correcting mechanisms, PMF paves
the way for scalable, intelligent, and autonomous optimization frameworks, with
promising applications in engineering, logistics, and complex decision-making
systems.

</details>


### [878] [RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework](https://arxiv.org/abs/2505.13808)
*Faramarz Safi Esfahani,Ghassan Beydoun,Morteza Saberi,Brad McCusker,Biswajeet Pradhan*

Key words: 元启发式算法, 自适应机制, 多模态优化, AI驱动, PMF

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: PMF通过实时性能反馈和动态算法选择的自适应机制，提升了元启发式算法的收敛速度、适应性和解的质量，优于传统方法。

Motivation: 传统的元启发式算法因固定结构和需大量调优而效果受限，PMF旨在解决这一问题。

Method: PMF引入PMA和PMSA，通过实时性能反馈动态选择和切换算法，提升适应性和解质量。

Result: 实验结果表明，PMF在高维、动态和多模态环境中显著提升了优化效率。

Conclusion: PMF为构建可扩展、智能和自主的优化框架开辟了新途径，适用于工程、物流等领域。

Abstract: Metaheuristic algorithms are widely used for solving complex optimization
problems, yet their effectiveness is often constrained by fixed structures and
the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)
addresses this limitation by introducing a self-adaptive metaheuristic
switching mechanism driven by real-time performance feedback and dynamic
algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)
and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select
and transition between metaheuristic algorithms based on key performance
indicators, ensuring continuous adaptation. This approach enhances convergence
speed, adaptability, and solution quality, outperforming traditional
metaheuristics in high-dimensional, dynamic, and multimodal environments.
Experimental results on benchmark functions demonstrate that PMF significantly
improves optimization efficiency by mitigating stagnation and balancing
exploration-exploitation strategies across various problem landscapes. By
integrating AI-driven decision-making and self-correcting mechanisms, PMF paves
the way for scalable, intelligent, and autonomous optimization frameworks, with
promising applications in engineering, logistics, and complex decision-making
systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [879] [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
*Guobin Shen,Dongcheng Zhao,Linghao Feng,Xiang He,Jihang Wang,Sicheng Shen,Haibo Tong,Yiting Dong,Jindong Li,Xiang Zheng,Yi Zeng*

Key words: 大语言模型, 对抗性攻击, 安全评估, 多智能体系统, 可复现性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: PandaGuard是一个统一的模块化框架，用于评估和提升大语言模型的安全性，通过多智能体系统模拟攻击、防御和裁判，揭示了模型漏洞和防御策略的局限性。

Motivation: 大语言模型（LLMs）易受对抗性提示攻击（jailbreaks），现有评估方法分散且缺乏系统性，亟需统一的框架进行可复现分析。

Method: 提出PandaGuard框架，包含19种攻击方法、12种防御机制及多种裁判策略；开发PandaBench基准，评估49个LLM的攻击/防御交互。

Result: 发现防御策略无最优解，裁判分歧导致安全评估差异显著，提供了透明可复现的研究成果。

Conclusion: PandaGuard为LLM安全研究提供了系统化工具，揭示了安全评估的复杂性和多维度挑战。

Abstract: Large language models (LLMs) have achieved remarkable capabilities but remain
vulnerable to adversarial prompts known as jailbreaks, which can bypass safety
alignment and elicit harmful outputs. Despite growing efforts in LLM safety
research, existing evaluations are often fragmented, focused on isolated attack
or defense techniques, and lack systematic, reproducible analysis. In this
work, we introduce PandaGuard, a unified and modular framework that models LLM
jailbreak safety as a multi-agent system comprising attackers, defenders, and
judges. Our framework implements 19 attack methods and 12 defense mechanisms,
along with multiple judgment strategies, all within a flexible plugin
architecture supporting diverse LLM interfaces, multiple interaction modes, and
configuration-driven experimentation that enhances reproducibility and
practical deployment. Built on this framework, we develop PandaBench, a
comprehensive benchmark that evaluates the interactions between these
attack/defense methods across 49 LLMs and various judgment approaches,
requiring over 3 billion tokens to execute. Our extensive evaluation reveals
key insights into model vulnerabilities, defense cost-performance trade-offs,
and judge consistency. We find that no single defense is optimal across all
dimensions and that judge disagreement introduces nontrivial variance in safety
assessments. We release the code, configurations, and evaluation results to
support transparent and reproducible research in LLM safety.

</details>


### [880] [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
*Jiankun Zhang,Shenglai Zeng,Jie Ren,Tianqi Zheng,Hui Liu,Xianfeng Tang,Hui Liu,Yi Chang*

Key words: MRAG, 隐私漏洞, 多模态, 检索增强生成, 黑盒攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文研究了多模态检索增强生成（MRAG）系统的隐私漏洞，提出了一种新的攻击方法，并强调了隐私保护技术的紧迫性。

Motivation: 探讨MRAG系统在多模态数据下的隐私风险，填补现有研究的空白。

Method: 采用组合式结构化提示攻击（黑盒设置），通过操纵查询提取私有信息。

Result: 实验显示LMMs能直接生成类似检索内容或间接暴露敏感信息。

Conclusion: 亟需开发隐私保护的MRAG技术以应对这些漏洞。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.

</details>


### [881] [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
*Jiawen Wang,Pritha Gupta,Ivan Habernal,Eyke Hüllermeier*

Key words: 大语言模型, 提示注入攻击, 催眠攻击, 开源模型, 脆弱性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文研究了开源大语言模型（LLM）对提示注入攻击的脆弱性，提出了一种新的催眠攻击方法，并在14种流行开源LLM上验证了其效果。

Motivation: 针对开源LLM在提示注入攻击下的脆弱性，研究旨在填补这一领域的空白，并揭示模型的潜在风险。

Method: 通过提出攻击成功概率（ASP）作为新指标，并结合催眠攻击和忽略前缀攻击，对14种开源LLM进行了全面测试。

Result: 实验结果表明，催眠攻击成功率达到90%，而忽略前缀攻击可突破所有14种模型，ASP超过60%。发现越知名的模型越容易受到攻击。

Conclusion: 开源LLM对提示注入攻击高度脆弱，需提高公众意识并开发有效的防御策略。

Abstract: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable
to different prompt-based attacks, generating harmful content or sensitive
information. Both closed-source and open-source LLMs are underinvestigated for
these attacks. This paper studies effective prompt injection attacks against
the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.
Current metrics only consider successful attacks, whereas our proposed Attack
Success Probability (ASP) also captures uncertainty in the model's response,
reflecting ambiguity in attack feasibility. By comprehensively analyzing the
effectiveness of prompt injection attacks, we propose a simple and effective
hypnotism attack; results show that this attack causes aligned language models,
including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable
behaviors, achieving around $90$% ASP. They also indicate that our ignore
prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over
$60$% ASP on a multi-categorical dataset. We find that moderately well-known
LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the
need to raise public awareness and prioritize efficient mitigation strategies.

</details>


### [882] [CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data](https://arxiv.org/abs/2505.14027)
*Yifan Zeng*

Key words: 

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种基于深度学习的网络入侵检测模型CSAGC-IDS，通过生成高质量数据和提取复杂流量特征，显著提高了检测精度。

Motivation: 随着计算机网络普及，网络入侵的严重性加剧，凸显了入侵检测系统的重要性。现有深度学习模型在应对高维复杂流量和类别不平衡数据时面临挑战。

Method: CSAGC-IDS结合了SC-CGAN（自注意力增强的卷积条件生成对抗网络）生成高质量数据以缓解类别不平衡，以及CSCA-CNN（基于成本敏感学习和通道注意力机制的卷积神经网络）提取复杂流量特征。

Result: 在NSL-KDD数据集上，CSAGC-IDS在五分类任务中准确率84.55%、F1分数84.52%，二分类任务中准确率91.09%、F1分数92.04%。

Conclusion: CSAGC-IDS通过数据生成和特征提取显著提升了网络入侵检测性能，并通过SHAP和LIME提供了模型的可解释性分析。

Abstract: As computer networks proliferate, the gravity of network intrusions has
escalated, emphasizing the criticality of network intrusion detection systems
for safeguarding security. While deep learning models have exhibited promising
results in intrusion detection, they face challenges in managing
high-dimensional, complex traffic patterns and imbalanced data categories. This
paper presents CSAGC-IDS, a network intrusion detection model based on deep
learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced
convolutional conditional generative adversarial network that generates
high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS
integrates CSCA-CNN, a convolutional neural network enhanced through cost
sensitive learning and channel attention mechanism, to extract features from
complex traffic data for precise detection. Experiments conducted on the
NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of
84.52% in five-class classification task, and an accuracy of 91.09% and an F1
score of 92.04% in binary classification task.Furthermore, this paper provides
an interpretability analysis of the proposed model, using SHAP and LIME to
explain the decision-making mechanisms of the model.

</details>


### [883] [Traceable Black-box Watermarks for Federated Learning](https://arxiv.org/abs/2505.13651)
*Jiahao Xu,Rui Hu,Olivera Kotevska,Zikai Zhang*

Key words: 联邦学习, 模型泄露, 黑盒水印, 可追踪性, TraMark

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了一种名为TraMark的新型服务器端水印方法，用于在联邦学习系统中注入可追踪的黑盒水印，以解决模型泄露的隐患。

Motivation: 联邦学习系统中每个客户端可以访问全局模型，存在模型泄露的风险。现有方法多关注非追踪水印或白盒水印，缺乏对黑盒水印的正式定义和问题形式化。

Method: 提出了TraMark方法，通过将模型参数空间划分为主任务区域和水印区域，并仅对主任务区域进行聚合，为每个客户端生成个性化的全局模型。水印区域用于学习独特水印，以实现黑盒设置下的可追踪性。

Result: 实验结果表明，TraMark能够在保持主任务性能的同时，确保所有水印模型的追踪性。

Conclusion: TraMark为解决联邦学习系统中的模型泄露问题提供了一种有效的黑盒水印注入方法。

Abstract: Due to the distributed nature of Federated Learning (FL) systems, each local
client has access to the global model, posing a critical risk of model leakage.
Existing works have explored injecting watermarks into local models to enable
intellectual property protection. However, these methods either focus on
non-traceable watermarks or traceable but white-box watermarks. We identify a
gap in the literature regarding the formal definition of traceable black-box
watermarking and the formulation of the problem of injecting such watermarks
into FL systems. In this work, we first formalize the problem of injecting
traceable black-box watermarks into FL. Based on the problem, we propose a
novel server-side watermarking method, $\mathbf{TraMark}$, which creates a
traceable watermarked model for each client, enabling verification of model
leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions
the model parameter space into two distinct regions: the main task region and
the watermarking region. Subsequently, a personalized global model is
constructed for each client by aggregating only the main task region while
preserving the watermarking region. Each model then learns a unique watermark
exclusively within the watermarking region using a distinct watermark dataset
before being sent back to the local client. Extensive results across various FL
systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all
watermarked models while preserving their main task performance.

</details>


### [884] [Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy](https://arxiv.org/abs/2505.13655)
*Jiahao Xu,Rui Hu,Olivera Kotevska*

Key words: 联邦学习,差分隐私,异构性,模型稀疏化

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: GDPFed和GDPFed⁺在联邦学习中解决客户端隐私要求异构性问题，通过分组和模型稀疏化提高模型效用。

Motivation: 经典方法如DP-FedAvg在客户端隐私要求异构时性能下降，现有方法缺乏理论支持且假设服务器可信。本文提出新方法，在不信任环境下提升性能。

Method: 提出GDPFed，按隐私预算分组实现客户端级差分隐私；GDPFed⁺增加模型稀疏化并优化采样比例。

Result: 实验表明GDPFed⁺在多个数据集上显著优于现有方法。

Conclusion: GDPFed⁺有效减少隐私预算浪费并提升模型效用，适用于不信任环境。

Abstract: Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.

</details>


### [885] [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)
*Guangke Chen,Fu Song,Zhe Zhao,Xiaojun Jia,Yang Liu,Yanchen Qiao,Weizhe Zhang*

Key words: AudioJailbreak, LALMs, jailbreak attack, security

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了AudioJailbreak，一种新型音频越狱攻击，具有异步性、通用性、隐蔽性和空中鲁棒性，显著优于现有攻击方法。

Motivation: 研究大型音频语言模型（LALMs）的越狱攻击，现有方法在有效性、适用性和实用性上表现不佳，特别是假设攻击者可完全操控用户提示。

Method: 提出的AudioJailbreak通过以下特性改进攻击：(1)异步性；(2)通用性；(3)隐蔽性；(4)空中鲁棒性。

Result: 实验证明AudioJailbreak在多种LALMs上高效，适用于更广泛的攻击场景。

Conclusion: 本文揭示了LALMs在音频越狱攻击中的安全风险，并推动了其安全性的提升。

Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied
recently, but they achieve suboptimal effectiveness, applicability, and
practicability, particularly, assuming that the adversary can fully manipulate
user prompts. In this work, we first conduct an extensive experiment showing
that advanced text jailbreak attacks cannot be easily ported to end-to-end
LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a
novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio
does not need to align with user prompts in the time axis by crafting suffixal
jailbreak audios; (2) universality: a single jailbreak perturbation is
effective for different prompts by incorporating multiple prompts into
perturbation generation; (3) stealthiness: the malicious intent of jailbreak
audios will not raise the awareness of victims by proposing various intent
concealment strategies; and (4) over-the-air robustness: the jailbreak audios
remain effective when being played over the air by incorporating the
reverberation distortion effect with room impulse response into the generation
of the perturbations. In contrast, all prior audio jailbreak attacks cannot
offer asynchrony, universality, stealthiness, or over-the-air robustness.
Moreover, AudioJailbreak is also applicable to the adversary who cannot fully
manipulate user prompts, thus has a much broader attack scenario. Extensive
experiments with thus far the most LALMs demonstrate the high effectiveness of
AudioJailbreak. We highlight that our work peeks into the security implications
of audio jailbreak attacks against LALMs, and realistically fosters improving
their security robustness. The implementation and audio samples are available
at our website https://audiojailbreak.github.io/AudioJailbreak.

</details>


### [886] [Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion](https://arxiv.org/abs/2505.14316)
*Tiehan Cui,Yanxu Mao,Peipei Liu,Congying Liu,Datao You*

Key words: 大型语言模型, 越狱攻击, 意图隐藏, 文本生成, 安全性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种名为ICE的新型黑盒越狱攻击方法，通过意图隐藏和分散技术高效绕过LLMs的安全限制，并在单一查询中实现高攻击成功率。同时构建了BiSceneEval数据集，用于评估LLMs在问答和文本生成任务中的鲁棒性。

Motivation: 大型语言模型（LLMs）的安全性仍存在重大隐患，尤其是越狱攻击。现有方法存在迭代查询过多和跨模型泛化能力差的问题，且当前的评估数据集主要关注问答场景，缺乏对文本生成任务的关注。

Method: 提出ICE方法，利用意图隐藏和分散策略绕过LLMs的安全约束；构建BiSceneEval数据集，涵盖问答和文本生成任务，评估模型鲁棒性。

Result: ICE在单一查询中实现了高攻击成功率，显著提升效率和跨模型适用性；BiSceneEval数据集展示了LLMs在两种任务中的漏洞。

Conclusion: 研究揭示了当前防御机制的关键漏洞，强调需要结合预设安全机制和实时语义分解的混合安全策略来增强LLMs的安全性。

Abstract: Although large language models (LLMs) have achieved remarkable advancements,
their security remains a pressing concern. One major threat is jailbreak
attacks, where adversarial prompts bypass model safeguards to generate harmful
or objectionable content. Researchers study jailbreak attacks to understand
security and robustness of LLMs. However, existing jailbreak attack methods
face two main challenges: (1) an excessive number of iterative queries, and (2)
poor generalization across models. In addition, recent jailbreak evaluation
datasets focus primarily on question-answering scenarios, lacking attention to
text generation tasks that require accurate regeneration of toxic content. To
tackle these challenges, we propose two contributions: (1) ICE, a novel
black-box jailbreak method that employs Intent Concealment and divErsion to
effectively circumvent security constraints. ICE achieves high attack success
rates (ASR) with a single query, significantly improving efficiency and
transferability across different models. (2) BiSceneEval, a comprehensive
dataset designed for assessing LLM robustness in question-answering and
text-generation tasks. Experimental results demonstrate that ICE outperforms
existing jailbreak techniques, revealing critical vulnerabilities in current
defense mechanisms. Our findings underscore the necessity of a hybrid security
strategy that integrates predefined security mechanisms with real-time semantic
decomposition to enhance the security of LLMs.

</details>


### [887] [Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime](https://arxiv.org/abs/2505.14323)
*Tomasz Maciążek,Robert Allison*

Key words: 训练数据重构,差分隐私,迁移学习,小数据集,Neyman-Pearson准则

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 训练数据重构攻击允许攻击者恢复模型的部分训练数据。本文研究了在小数据集情况下，即使使用差分隐私防御，攻击者仍能成功重构数据。

Motivation: 研究在实际威胁模型中（攻击者仅知道训练数据的分布），对小数据集进行迁移学习的神经网络是否易受训练数据重构攻击。

Method: 通过重构神经网络对迁移学习的神经网络分类器进行攻击，并使用Neyman-Pearson准则评估重构效果。

Result: 攻击在小数据集情况下有效，且差分隐私防御会严重降低分类器准确性。

Conclusion: 迁移学习分类器在小数据集情况下存在训练数据保护风险。

Abstract: Training data reconstruction attacks enable adversaries to recover portions
of a released model's training data. We consider the attacks where a
reconstructor neural network learns to invert the (random) mapping between
training data and model weights. Prior work has shown that an informed
adversary with access to released model's weights and all but one training data
point can achieve high-quality reconstructions in this way. However,
differential privacy can defend against such an attack with little to no loss
in model's utility when the amount of training data is sufficiently large. In
this work we consider a more realistic adversary who only knows the
distribution from which a small training dataset has been sampled and who
attacks a transfer-learned neural network classifier that has been trained on
this dataset. We exhibit an attack that works in this realistic threat model
and demonstrate that in the small-data regime it cannot be defended against by
DP-SGD without severely damaging the classifier accuracy. This raises
significant concerns about the use of such transfer-learned classifiers when
protection of training-data is paramount. We demonstrate the effectiveness and
robustness of our attack on VGG, EfficientNet and ResNet image classifiers
transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we
point out that the commonly used (true-positive) reconstruction success rate
metric fails to reliably quantify the actual reconstruction effectiveness.
Instead, we make use of the Neyman-Pearson lemma to construct the receiver
operating characteristic curve and consider the associated true-positive
reconstruction rate at a fixed level of the false-positive reconstruction rate.

</details>


### [888] [Can Large Language Models Really Recognize Your Name?](https://arxiv.org/abs/2505.14549)
*Dzung Pham,Peter Kairouz,Niloofar Mireshghallah,Eugene Bagdasarian,Chau Minh Pham,Amir Houmansadr*

Key words: LLM, 隐私保护, AMBENCH, PII检测, 模糊人名

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: LLM在隐私保护中存在系统性缺陷，特别是在检测模糊人名时表现不佳，AMBENCH基准数据集揭示了这一问题。

Motivation: 挑战当前LLM在隐私任务中可靠检测PII的假设，揭示模糊人名检测的失败现象。

Method: 提出AMBENCH数据集，测试LLM在模糊人名检测中的表现，并与专用工具对比。

Result: 模糊人名的召回率下降20-40%，且在隐私保护摘要中更易被忽略。

Conclusion: LLM单独用于隐私保护存在风险，需系统性研究其失败模式。

Abstract: Large language models (LLMs) are increasingly being used to protect sensitive
user data. However, current LLM-based privacy solutions assume that these
models can reliably detect personally identifiable information (PII),
particularly named entities. In this paper, we challenge that assumption by
revealing systematic failures in LLM-based privacy tasks. Specifically, we show
that modern LLMs regularly overlook human names even in short text snippets due
to ambiguous contexts, which cause the names to be misinterpreted or
mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous
human names, leveraging the name regularity bias phenomenon, embedded within
concise text snippets along with benign prompt injections. Our experiments on
modern LLMs tasked to detect PII as well as specialized tools show that recall
of ambiguous names drops by 20--40% compared to more recognizable names.
Furthermore, ambiguous human names are four times more likely to be ignored in
supposedly privacy-preserving summaries generated by LLMs when benign prompt
injections are present. These findings highlight the underexplored risks of
relying solely on LLMs to safeguard user privacy and underscore the need for a
more systematic investigation into their privacy failure modes.

</details>


### [889] [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)
*Chongyang Shi,Sharon Lin,Shuang Song,Jamie Hayes,Ilia Shumailov,Itay Yona,Juliette Pluto,Aneesh Pappu,Christopher A. Choquette-Choo,Milad Nasr,Chawin Sitawarin,Gena Gibson,Andreas Terzis,John "Four" Flynn*

Key words: Gemini,对抗鲁棒性,DeepMind,恶意指令,自适应攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: Google DeepMind评估了Gemini模型的对抗鲁棒性，通过持续对抗测试框架提高模型抵御恶意指令的能力。

Motivation: Gemini在处理非受信数据时可能因恶意指令而偏离用户预期，需提升其对抗攻击的鲁棒性。

Method: 采用自适应攻击技术的对抗评估框架，持续测试Gemini模型的不同版本。

Result: 测试帮助提升了Gemini抵御恶意操纵的能力。

Conclusion: 持续对抗评估是增强模型安全性的有效手段。

Abstract: Gemini is increasingly used to perform tasks on behalf of users, where
function-calling and tool-use capabilities enable the model to access user
data. Some tools, however, require access to untrusted data introducing risk.
Adversaries can embed malicious instructions in untrusted data which cause the
model to deviate from the user's expectations and mishandle their data or
permissions. In this report, we set out Google DeepMind's approach to
evaluating the adversarial robustness of Gemini models and describe the main
lessons learned from the process. We test how Gemini performs against a
sophisticated adversary through an adversarial evaluation framework, which
deploys a suite of adaptive attack techniques to run continuously against past,
current, and future versions of Gemini. We describe how these ongoing
evaluations directly help make Gemini more resilient against manipulation.

</details>


### [890] [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
*Guobin Shen,Dongcheng Zhao,Linghao Feng,Xiang He,Jihang Wang,Sicheng Shen,Haibo Tong,Yiting Dong,Jindong Li,Xiang Zheng,Yi Zeng*

Key words: 大型语言模型,越狱攻击,安全对齐,多代理系统,基准测试

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: PandaGuard是一个统一的模块化框架，通过多代理系统评估LLM的越狱安全性，集成了多种攻击和防御方法，并提供了全面的基准测试PandaBench。

Motivation: 大型语言模型（LLM）容易受到对抗性提示（即越狱）的攻击，现有评估零散且缺乏系统性，需要统一框架支持可重复分析。

Method: 提出PandaGuard框架，包含19种攻击方法、12种防御机制和多种判断策略，基于多代理系统建模；开发PandaBench基准测试，覆盖49种LLM和3亿令牌的交互评估。

Result: 研究发现没有单一防御方法在所有维度上最优，且判断差异导致安全评估存在显著方差。

Conclusion: PandaGuard为LLM安全性研究提供了透明、可重复的框架和工具，揭示了防御权衡和评估一致性挑战。

Abstract: Large language models (LLMs) have achieved remarkable capabilities but remain
vulnerable to adversarial prompts known as jailbreaks, which can bypass safety
alignment and elicit harmful outputs. Despite growing efforts in LLM safety
research, existing evaluations are often fragmented, focused on isolated attack
or defense techniques, and lack systematic, reproducible analysis. In this
work, we introduce PandaGuard, a unified and modular framework that models LLM
jailbreak safety as a multi-agent system comprising attackers, defenders, and
judges. Our framework implements 19 attack methods and 12 defense mechanisms,
along with multiple judgment strategies, all within a flexible plugin
architecture supporting diverse LLM interfaces, multiple interaction modes, and
configuration-driven experimentation that enhances reproducibility and
practical deployment. Built on this framework, we develop PandaBench, a
comprehensive benchmark that evaluates the interactions between these
attack/defense methods across 49 LLMs and various judgment approaches,
requiring over 3 billion tokens to execute. Our extensive evaluation reveals
key insights into model vulnerabilities, defense cost-performance trade-offs,
and judge consistency. We find that no single defense is optimal across all
dimensions and that judge disagreement introduces nontrivial variance in safety
assessments. We release the code, configurations, and evaluation results to
support transparent and reproducible research in LLM safety.

</details>


### [891] [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
*Jiankun Zhang,Shenglai Zeng,Jie Ren,Tianqi Zheng,Hui Liu,Xianfeng Tang,Hui Liu,Yi Chang*

Key words: 多模态检索增强生成, 隐私漏洞, 结构化提示攻击, 黑盒设置, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文分析了多模态检索增强生成（MRAG）系统的隐私漏洞，提出了一种新的攻击方法，并强调了保护隐私的重要性。

Motivation: 研究旨在揭示MRAG系统在多模态数据（视觉-语言和语音-语言）中未探索的隐私风险，填补了文本RAG研究之外的空白。

Method: 使用了一种新颖的组合结构化提示攻击方法，在黑盒设置下通过操纵查询来提取私有信息。

Result: 实验表明，LMMs既能直接生成类似检索内容的结果，也能间接暴露敏感信息。

Conclusion: 研究强调了开发鲁棒的隐私保护MRAG技术的紧迫性。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.

</details>


### [892] [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
*Jiawen Wang,Pritha Gupta,Ivan Habernal,Eyke Hüllermeier*

Key words: 大型语言模型, 提示注入攻击, 催眠攻击, 攻击成功概率, 模型安全

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文研究了针对14种流行开源LLM的提示注入攻击，提出了一种简单有效的催眠攻击方法，并引入了攻击成功概率（ASP）来衡量攻击效果。结果显示，该方法能显著突破模型防御，部分攻击成功率高达90%。

Motivation: 研究背景是大型语言模型（LLMs）易受提示注入攻击的影响，生成有害内容或敏感信息。目前对开源和闭源LLMs在这方面的研究不足，亟需深入分析攻击有效性及防御策略。

Method: 本文提出了一种催眠攻击方法，并通过攻击成功概率（ASP）量化攻击效果，同时在五个攻击基准上对14种流行开源LLMs进行了测试。

Result: 实验表明，催眠攻击能成功使对齐的语言模型（如Stablelm2、Mistral等）生成不当行为，ASP高达90%。另外，忽略前缀攻击能突破所有14种模型，ASP超过60%。

Conclusion: 研究发现，知名度较高的LLMs更容易受攻击，提示公众需提高警惕并优先发展高效防御策略。

Abstract: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable
to different prompt-based attacks, generating harmful content or sensitive
information. Both closed-source and open-source LLMs are underinvestigated for
these attacks. This paper studies effective prompt injection attacks against
the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.
Current metrics only consider successful attacks, whereas our proposed Attack
Success Probability (ASP) also captures uncertainty in the model's response,
reflecting ambiguity in attack feasibility. By comprehensively analyzing the
effectiveness of prompt injection attacks, we propose a simple and effective
hypnotism attack; results show that this attack causes aligned language models,
including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable
behaviors, achieving around $90$% ASP. They also indicate that our ignore
prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over
$60$% ASP on a multi-categorical dataset. We find that moderately well-known
LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the
need to raise public awareness and prioritize efficient mitigation strategies.

</details>


### [893] [CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data](https://arxiv.org/abs/2505.14027)
*Yifan Zeng*

Key words: 网络入侵检测, 深度学习, 类别不平衡, 自注意力, 可解释性

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了CSAGC-IDS，一种基于深度学习的网络入侵检测模型，通过SC-CGAN生成高质量数据以解决类别不平衡问题，并结合CSCA-CNN提取复杂流量数据特征。在NSL-KDD数据集上表现出色，并提供了模型的可解释性分析。

Motivation: 随着计算机网络的普及，网络入侵的严重性日益突出，凸显了入侵检测系统的重要性。现有深度学习模型在应对高维复杂流量数据和类别不平衡时面临挑战。

Method: CSAGC-IDS整合了SC-CGAN（一种自注意力增强的卷积条件生成对抗网络）来生成高质量数据，并采用CSCA-CNN（结合成本敏感学习和通道注意力机制的卷积神经网络）提取特征。

Result: 在NSL-KDD数据集上，CSAGC-IDS在五类分类任务中达到84.55%的准确率和84.52%的F1分数，在二分类任务中达到91.09%的准确率和92.04%的F1分数。

Conclusion: CSAGC-IDS有效解决了类别不平衡和复杂流量数据问题，并提供了模型的可解释性分析，展示了其在实际应用中的潜力。

Abstract: As computer networks proliferate, the gravity of network intrusions has
escalated, emphasizing the criticality of network intrusion detection systems
for safeguarding security. While deep learning models have exhibited promising
results in intrusion detection, they face challenges in managing
high-dimensional, complex traffic patterns and imbalanced data categories. This
paper presents CSAGC-IDS, a network intrusion detection model based on deep
learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced
convolutional conditional generative adversarial network that generates
high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS
integrates CSCA-CNN, a convolutional neural network enhanced through cost
sensitive learning and channel attention mechanism, to extract features from
complex traffic data for precise detection. Experiments conducted on the
NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of
84.52% in five-class classification task, and an accuracy of 91.09% and an F1
score of 92.04% in binary classification task.Furthermore, this paper provides
an interpretability analysis of the proposed model, using SHAP and LIME to
explain the decision-making mechanisms of the model.

</details>


### [894] [Traceable Black-box Watermarks for Federated Learning](https://arxiv.org/abs/2505.13651)
*Jiahao Xu,Rui Hu,Olivera Kotevska,Zikai Zhang*

Key words: 联邦学习, 水印, 可追踪性, 黑盒水印, 模型保护

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种称为TraMark的服务器端水印方法，用于在联邦学习系统中注入可追踪的黑盒水印，以保护模型知识产权。

Motivation: 联邦学习的分布式特性使得本地客户端可以访问全局模型，存在模型泄露的风险。现有水印方法多为不可追踪或白盒可追踪，缺乏黑盒可追踪水印的正式定义和实现方法。

Method: 提出TraMark方法，通过将模型参数空间划分为主任务区域和水印区域，为每个客户端生成个性化的全局模型，并在水印区域中学习独特的水印。

Result: TraMark在各种联邦学习系统中均能确保水印模型的追踪性，同时保持主任务性能。

Conclusion: TraMark填补了黑盒可追踪水印在联邦学习系统中的空白，有效解决了模型泄露的追踪问题。

Abstract: Due to the distributed nature of Federated Learning (FL) systems, each local
client has access to the global model, posing a critical risk of model leakage.
Existing works have explored injecting watermarks into local models to enable
intellectual property protection. However, these methods either focus on
non-traceable watermarks or traceable but white-box watermarks. We identify a
gap in the literature regarding the formal definition of traceable black-box
watermarking and the formulation of the problem of injecting such watermarks
into FL systems. In this work, we first formalize the problem of injecting
traceable black-box watermarks into FL. Based on the problem, we propose a
novel server-side watermarking method, $\mathbf{TraMark}$, which creates a
traceable watermarked model for each client, enabling verification of model
leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions
the model parameter space into two distinct regions: the main task region and
the watermarking region. Subsequently, a personalized global model is
constructed for each client by aggregating only the main task region while
preserving the watermarking region. Each model then learns a unique watermark
exclusively within the watermarking region using a distinct watermark dataset
before being sent back to the local client. Extensive results across various FL
systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all
watermarked models while preserving their main task performance.

</details>


### [895] [Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy](https://arxiv.org/abs/2505.13655)
*Jiahao Xu,Rui Hu,Olivera Kotevska*

Key words: 联邦学习,差分隐私,隐私异质性,GDPFed$^+$,模型性能

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 提出了GDPFed和GDPFed$^+$方法，通过分组实现客户端差分隐私，减少隐私预算浪费并提升模型性能。

Motivation: 解决联邦学习在客户端隐私需求异质性问题中经典方法（如DP-FedAvg）因强制统一最严格隐私级别导致的模型性能下降问题。

Method: 提出GDPFed分组客户端并实现组内客户级差分隐私，GDPFed$^+$进一步通过模型稀疏化和优化组内采样比例减少噪声。

Result: 实验证明GDPFed$^+$在多个基准数据集上性能优于现有方法。

Conclusion: GDPFed$^+$有效解决异质隐私需求下的联邦学习问题，显著提升模型性能。

Abstract: Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.

</details>


### [896] [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)
*Guangke Chen,Fu Song,Zhe Zhao,Xiaojun Jia,Yang Liu,Yanchen Qiao,Weizhe Zhang*

Key words: AudioJailbreak, LALMs, jailbreak attack, security robustness

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 本文提出了一种名为AudioJailbreak的新型音频越狱攻击方法，具有异步性、通用性、隐蔽性和空中传播鲁棒性，显著提升了攻击效果和适用性。

Motivation: 现有的文本越狱攻击难以直接迁移到端到端的大型音频语言模型（LALMs），且攻击效果、适用性和实用性有限。本文旨在解决这些问题。

Method: 通过设计后缀越狱音频（异步性）、融合多提示生成扰动（通用性）、隐藏恶意意图（隐蔽性）和模拟室内脉冲响应（空中传播鲁棒性），提出AudioJailbreak攻击方法。

Result: 实验表明，AudioJailbreak在多种LALMs上具有高效性，且适用于更广泛的实际攻击场景。

Conclusion: AudioJailbreak不仅显著提升了攻击能力，还为LALMs的安全性改进提供了现实依据。

Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied
recently, but they achieve suboptimal effectiveness, applicability, and
practicability, particularly, assuming that the adversary can fully manipulate
user prompts. In this work, we first conduct an extensive experiment showing
that advanced text jailbreak attacks cannot be easily ported to end-to-end
LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a
novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio
does not need to align with user prompts in the time axis by crafting suffixal
jailbreak audios; (2) universality: a single jailbreak perturbation is
effective for different prompts by incorporating multiple prompts into
perturbation generation; (3) stealthiness: the malicious intent of jailbreak
audios will not raise the awareness of victims by proposing various intent
concealment strategies; and (4) over-the-air robustness: the jailbreak audios
remain effective when being played over the air by incorporating the
reverberation distortion effect with room impulse response into the generation
of the perturbations. In contrast, all prior audio jailbreak attacks cannot
offer asynchrony, universality, stealthiness, or over-the-air robustness.
Moreover, AudioJailbreak is also applicable to the adversary who cannot fully
manipulate user prompts, thus has a much broader attack scenario. Extensive
experiments with thus far the most LALMs demonstrate the high effectiveness of
AudioJailbreak. We highlight that our work peeks into the security implications
of audio jailbreak attacks against LALMs, and realistically fosters improving
their security robustness. The implementation and audio samples are available
at our website https://audiojailbreak.github.io/AudioJailbreak.

</details>


### [897] [Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion](https://arxiv.org/abs/2505.14316)
*Tiehan Cui,Yanxu Mao,Peipei Liu,Congying Liu,Datao You*

Key words: 大语言模型, 越狱攻击, ICE方法, BiSceneEval数据集, 安全策略

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文针对大语言模型（LLM）的安全性问题，提出了一种新型黑盒越狱方法ICE和一个评估数据集BiSceneEval，以解决现有越狱攻击效率低和泛化性差的问题，并强调了混合安全策略的重要性。

Motivation: 研究发现现有越狱攻击方法效率低且泛化性差，同时缺乏对文本生成任务的评估数据集，亟需改进方法和评估工具。

Method: 提出ICE方法，通过意图隐藏和分散技术实现高效越狱；构建BiSceneEval数据集，全面评估LLM在不同任务中的鲁棒性。

Result: ICE在单次查询下实现高攻击成功率，且具有跨模型泛化能力；BiSceneEval数据集揭示了当前防御机制的漏洞。

Conclusion: 需结合预定义安全机制和实时语义分解的混合策略，以提升LLM的安全性。

Abstract: Although large language models (LLMs) have achieved remarkable advancements,
their security remains a pressing concern. One major threat is jailbreak
attacks, where adversarial prompts bypass model safeguards to generate harmful
or objectionable content. Researchers study jailbreak attacks to understand
security and robustness of LLMs. However, existing jailbreak attack methods
face two main challenges: (1) an excessive number of iterative queries, and (2)
poor generalization across models. In addition, recent jailbreak evaluation
datasets focus primarily on question-answering scenarios, lacking attention to
text generation tasks that require accurate regeneration of toxic content. To
tackle these challenges, we propose two contributions: (1) ICE, a novel
black-box jailbreak method that employs Intent Concealment and divErsion to
effectively circumvent security constraints. ICE achieves high attack success
rates (ASR) with a single query, significantly improving efficiency and
transferability across different models. (2) BiSceneEval, a comprehensive
dataset designed for assessing LLM robustness in question-answering and
text-generation tasks. Experimental results demonstrate that ICE outperforms
existing jailbreak techniques, revealing critical vulnerabilities in current
defense mechanisms. Our findings underscore the necessity of a hybrid security
strategy that integrates predefined security mechanisms with real-time semantic
decomposition to enhance the security of LLMs.

</details>


### [898] [Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime](https://arxiv.org/abs/2505.14323)
*Tomasz Maciążek,Robert Allison*

Key words: 训练数据重构、迁移学习、差分隐私、神经网络分类器、Neyman-Pearson引理

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文研究了一种针对迁移学习模型的训练数据重构攻击，展示了在小数据集情况下，差分隐私无法有效防御攻击且不损害模型精度。

Motivation: 探讨在现实威胁模型下，攻击者仅知道训练数据的分布时，如何成功重构训练数据，并评估差分隐私防御的局限性。

Method: 使用重构神经网络反转训练数据与模型权重之间的映射，攻击迁移学习的神经网络分类器。

Result: 在小数据集情况下，差分隐私无法防御攻击而不严重损害模型精度，攻击在不同数据集和模型上均有效。

Conclusion: 迁移学习分类器在训练数据保护至关重要时需谨慎使用，现有重构成功率指标不可靠，需改进评估方法。

Abstract: Training data reconstruction attacks enable adversaries to recover portions
of a released model's training data. We consider the attacks where a
reconstructor neural network learns to invert the (random) mapping between
training data and model weights. Prior work has shown that an informed
adversary with access to released model's weights and all but one training data
point can achieve high-quality reconstructions in this way. However,
differential privacy can defend against such an attack with little to no loss
in model's utility when the amount of training data is sufficiently large. In
this work we consider a more realistic adversary who only knows the
distribution from which a small training dataset has been sampled and who
attacks a transfer-learned neural network classifier that has been trained on
this dataset. We exhibit an attack that works in this realistic threat model
and demonstrate that in the small-data regime it cannot be defended against by
DP-SGD without severely damaging the classifier accuracy. This raises
significant concerns about the use of such transfer-learned classifiers when
protection of training-data is paramount. We demonstrate the effectiveness and
robustness of our attack on VGG, EfficientNet and ResNet image classifiers
transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we
point out that the commonly used (true-positive) reconstruction success rate
metric fails to reliably quantify the actual reconstruction effectiveness.
Instead, we make use of the Neyman-Pearson lemma to construct the receiver
operating characteristic curve and consider the associated true-positive
reconstruction rate at a fixed level of the false-positive reconstruction rate.

</details>


### [899] [Can Large Language Models Really Recognize Your Name?](https://arxiv.org/abs/2505.14549)
*Dzung Pham,Peter Kairouz,Niloofar Mireshghallah,Eugene Bagdasarian,Chau Minh Pham,Amir Houmansadr*

Key words: 大型语言模型（LLMs）、隐私保护、个人身份信息（PII）、模糊人名、AMBENCH

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文揭示了大型语言模型（LLMs）在隐私保护任务中的系统性失败，提出了AMBENCH基准数据集，并发现LLMs对模糊人名的召回率较低且隐私保护摘要中容易忽略这些名字。

Motivation: 现有LLM-based隐私解决方案假设LLMs能可靠检测个人身份信息（PII），但实际中LLMs在模糊上下文中会忽略人名，威胁用户隐私。

Method: 通过AMBENCH基准数据集测试现代LLMs和专业工具对模糊人名的检测能力，并分析其隐私保护摘要中的表现。

Result: 模糊人名的召回率较易识别名字低20-40%，且在隐私保护摘要中被忽略的概率高四倍。

Conclusion: 依赖LLMs保护隐私存在风险，需系统性研究其失效模式。

Abstract: Large language models (LLMs) are increasingly being used to protect sensitive
user data. However, current LLM-based privacy solutions assume that these
models can reliably detect personally identifiable information (PII),
particularly named entities. In this paper, we challenge that assumption by
revealing systematic failures in LLM-based privacy tasks. Specifically, we show
that modern LLMs regularly overlook human names even in short text snippets due
to ambiguous contexts, which cause the names to be misinterpreted or
mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous
human names, leveraging the name regularity bias phenomenon, embedded within
concise text snippets along with benign prompt injections. Our experiments on
modern LLMs tasked to detect PII as well as specialized tools show that recall
of ambiguous names drops by 20--40% compared to more recognizable names.
Furthermore, ambiguous human names are four times more likely to be ignored in
supposedly privacy-preserving summaries generated by LLMs when benign prompt
injections are present. These findings highlight the underexplored risks of
relying solely on LLMs to safeguard user privacy and underscore the need for a
more systematic investigation into their privacy failure modes.

</details>


### [900] [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)
*Chongyang Shi,Sharon Lin,Shuang Song,Jamie Hayes,Ilia Shumailov,Itay Yona,Juliette Pluto,Aneesh Pappu,Christopher A. Choquette-Choo,Milad Nasr,Chawin Sitawarin,Gena Gibson,Andreas Terzis,John "Four" Flynn*

Key words: Gemini, 对抗鲁棒性, 恶意指令, 评估框架, 自适应攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: Google DeepMind评估了Gemini模型对恶意指令的对抗鲁棒性，通过持续攻击测试提升模型安全性。

Motivation: Gemini在执行任务时可能因访问不可信数据而受恶意指令影响，导致偏离用户期望或滥用权限，需增强其对抗能力。

Method: 采用对抗评估框架，持续使用自适应攻击技术测试Gemini模型的历史、当前及未来版本。

Result: 通过持续评估，发现并修复漏洞，提升了Gemini对抗恶意指令的鲁棒性。

Conclusion: 对抗评估框架能有效增强Gemini的防御能力，确保其安全性和用户信任。

Abstract: Gemini is increasingly used to perform tasks on behalf of users, where
function-calling and tool-use capabilities enable the model to access user
data. Some tools, however, require access to untrusted data introducing risk.
Adversaries can embed malicious instructions in untrusted data which cause the
model to deviate from the user's expectations and mishandle their data or
permissions. In this report, we set out Google DeepMind's approach to
evaluating the adversarial robustness of Gemini models and describe the main
lessons learned from the process. We test how Gemini performs against a
sophisticated adversary through an adversarial evaluation framework, which
deploys a suite of adaptive attack techniques to run continuously against past,
current, and future versions of Gemini. We describe how these ongoing
evaluations directly help make Gemini more resilient against manipulation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [901] [LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas](https://arxiv.org/abs/2505.12257)
*Evgeny Markhasin*

Key words: LLM, 错误检测, PWP, 科学文档, 多模态

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该研究探讨了通过结构化LLM上下文调节（基于PWP原则）来改进通用LLM在科学文档中的错误检测能力，初步结果显示该方法能提升文本和图像错误的识别效果。

Motivation: 解决LLM在复杂科学文档中因固有纠错倾向而掩盖错误的问题，提高其精确验证的可靠性。

Method: 采用PWP原则的结构化上下文调节策略，测试了Gemini 2.5 Pro和ChatGPT Plus o3在化学公式验证中的表现。

Result: PWP方法改善了文本错误识别，Gemini 2.5 Pro成功检测到图像公式错误，而ChatGPT Plus o3未能实现。

Conclusion: PWP上下文调节是一种有潜力提升LLM在科学文档中细节错误检测能力的可行方法，需进一步验证。

Abstract: Identifying subtle technical errors within complex scientific and technical
documents, especially those requiring multimodal interpretation (e.g., formulas
in images), presents a significant hurdle for Large Language Models (LLMs)
whose inherent error-correction tendencies can mask inaccuracies. This
exploratory proof-of-concept (PoC) study investigates structured LLM context
conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a
methodological strategy to modulate this LLM behavior at inference time. The
approach is designed to enhance the reliability of readily available,
general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for
precise validation tasks, crucially relying only on their standard chat
interfaces without API access or model modifications. To explore this
methodology, we focused on validating chemical formulas within a single,
complex test paper with known textual and image-based errors. Several prompting
strategies were evaluated: while basic prompts proved unreliable, an approach
adapting PWP structures to rigorously condition the LLM's analytical mindset
appeared to improve textual error identification with both models. Notably,
this method also guided Gemini 2.5 Pro to repeatedly identify a subtle
image-based formula error previously overlooked during manual review, a task
where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight
specific LLM operational modes that impede detail-oriented validation and
suggest that PWP-informed context conditioning offers a promising and highly
accessible technique for developing more robust LLM-driven analytical
workflows, particularly for tasks requiring meticulous error detection in
scientific and technical documents. Extensive validation beyond this limited
PoC is necessary to ascertain broader applicability.

</details>


### [902] [Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact](https://arxiv.org/abs/2505.13469)
*Aayam Bansal,Harsh Vardhan Narsaria*

Key words: 算法公平性,贷款决策,盈利性,公平约束

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 研究了贷款决策中公平约束与盈利之间的权衡，发现忽略保护属性的方法在公平性和盈利性上表现优于显式公平干预，并识别了公平贷款盈利的具体条件。

Motivation: 随着金融机构依赖机器学习自动化贷款决策，算法公平性问题凸显，需平衡公平约束与盈利。

Method: 通过模拟反映现实贷款模式的合成数据，量化不同公平干预对利润和违约率的影响。

Result: 忽略保护属性的方法在公平性和盈利性上均优于显式公平干预，并识别了公平贷款盈利的具体条件。

Conclusion: 提供了设计平衡伦理与业务目标的贷款算法的实用指导。

Abstract: As financial institutions increasingly rely on machine learning models to
automate lending decisions, concerns about algorithmic fairness have risen.
This paper explores the tradeoff between enforcing fairness constraints (such
as demographic parity or equal opportunity) and maximizing lender
profitability. Through simulations on synthetic data that reflects real-world
lending patterns, we quantify how different fairness interventions impact
profit margins and default rates. Our results demonstrate that equal
opportunity constraints typically impose lower profit costs than demographic
parity, but surprisingly, removing protected attributes from the model
(fairness through unawareness) outperforms explicit fairness interventions in
both fairness and profitability metrics. We further identify the specific
economic conditions under which fair lending becomes profitable and analyze the
feature-specific drivers of unfairness. These findings offer practical guidance
for designing lending algorithms that balance ethical considerations with
business objectives.

</details>


### [903] [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
*Shitong Duan,Xiaoyuan Yi,Peng Zhang,Dongkuan Xu,Jing Yao,Tun Lu,Ning Gu,Xing Xie*

Key words: 大型语言模型,价值差异,自扩展评估,信息论,文化适应性

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: AdAEM是一个自扩展的评估框架，用于揭示大型语言模型（LLM）的价值倾向，解决现有数据集信息不足的问题。

Motivation: 研究LLM的价值差异需要更动态和区分度高的评估方法，以解决当前数据集的通用性和过时性问题。

Method: 提出AdAEM框架，通过自适应生成测试问题，基于信息论目标优化，提取最新或有文化争议的主题。

Result: 生成12,310个基于Schwartz价值理论的问题，分析16个LLM的价值差异，验证方法的有效性。

Conclusion: AdAEM能够动态跟踪LLM的价值变化，为价值研究提供更全面的工具。

Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.

</details>


### [904] [Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks](https://arxiv.org/abs/2505.13565)
*Oier Mentxaka,Natalia Díaz-Rodríguez,Mark Coeckelbergh,Marcos López de Prado,Emilia Gómez,David Fernández Llorca,Enrique Herrera-Viedma,Francisco Herrera*

Key words: AI,民主治理,欧盟,伦理框架,透明度

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文提出了一个双重分类法来评估AI对民主的复杂关系，包括AI对民主的潜在风险（AIRD）和积极贡献（AIPD），并基于欧盟的伦理AI治理框架提出缓解策略。

Motivation: 探讨AI对民主治理的双重影响，建立一个规范性框架来指导研究、监管和制度设计，以实现可信赖的民主AI。

Method: 基于欧盟的可信赖AI要求，提出AIRD和AIPD双重分类法，并将风险与缓解策略对齐。

Result: 强调了透明度和社福祉在所有风险类别中的重要性，提供了一个结构化的视角来使AI系统与民主价值观保持一致。

Conclusion: 本文通过整合民主理论和实践治理工具，为学者、政策制定者和技术人员提供了一个行动框架，以弥合伦理目标与操作现实之间的差距。

Abstract: Artificial Intelligence (AI) poses both significant risks and valuable
opportunities for democratic governance. This paper introduces a dual taxonomy
to evaluate AI's complex relationship with democracy: the AI Risks to Democracy
(AIRD) taxonomy, which identifies how AI can undermine core democratic
principles such as autonomy, fairness, and trust; and the AI's Positive
Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to
enhance transparency, participation, efficiency, and evidence-based
policymaking.
  Grounded in the European Union's approach to ethical AI governance, and
particularly the seven Trustworthy AI requirements proposed by the European
Commission's High-Level Expert Group on AI, each identified risk is aligned
with mitigation strategies based on EU regulatory and normative frameworks. Our
analysis underscores the transversal importance of transparency and societal
well-being across all risk categories and offers a structured lens for aligning
AI systems with democratic values.
  By integrating democratic theory with practical governance tools, this paper
offers a normative and actionable framework to guide research, regulation, and
institutional design to support trustworthy, democratic AI. It provides
scholars with a conceptual foundation to evaluate the democratic implications
of AI, equips policymakers with structured criteria for ethical oversight, and
helps technologists align system design with democratic principles. In doing
so, it bridges the gap between ethical aspirations and operational realities,
laying the groundwork for more inclusive, accountable, and resilient democratic
systems in the algorithmic age.

</details>


### [905] [Fuck the Algorithm: Conceptual Issues in Algorithmic Bias](https://arxiv.org/abs/2505.13509)
*Catherine Stinson*

Key words: 算法偏见, 统计偏见, 责任归属, 歧视预防

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文探讨了算法偏见的概念，澄清了“算法本身无法偏见”这一争议性主张，并分析了偏见的多重含义。

Motivation: 旨在澄清算法偏见的争议，明确责任归属以避免算法中介的歧视。

Method: 通过概念分析和案例研究（如招聘、警务、医疗等领域的算法偏见）来揭示偏见的来源。

Result: 指出算法本身可以具有偏见，并与统计偏见、政治压迫等概念联系。

Conclusion: 认识到算法可以具有偏见是解决责任和预防歧视的关键。

Abstract: Algorithmic bias has been the subject of much recent controversy. To clarify
what is at stake and to make progress resolving the controversy, a better
understanding of the concepts involved would be helpful. The discussion here
focuses on the disputed claim that algorithms themselves cannot be biased. To
clarify this claim we need to know what kind of thing 'algorithms themselves'
are, and to disambiguate the several meanings of 'bias' at play. This further
involves showing how bias of moral import can result from statistical biases,
and drawing connections to previous conceptual work about political artifacts
and oppressive things. Data bias has been identified in domains like hiring,
policing and medicine. Examples where algorithms themselves have been
pinpointed as the locus of bias include recommender systems that influence
media consumption, academic search engines that influence citation patterns,
and the 2020 UK algorithmically-moderated A-level grades. Recognition that
algorithms are a kind of thing that can be biased is key to making decisions
about responsibility for harm, and preventing algorithmically mediated
discrimination.

</details>


### [906] [Upgrading Democracies with Fairer Voting Methods](https://arxiv.org/abs/2505.14349)
*Evangelos Pournaras,Srijoni Majumdar,Thomas Wellings,Joshua C. Yang,Fatemeh B. Heravan,Regula Hänggli Fricker,Dirk Helbing*

Key words: 投票方法, 民主升级, 参与式预算, 比例代表, 阿劳市

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文探讨了投票方法对民主决策的重要性，并提出现代化的比例投票方法（如累积投票和份额平等法）可以改进现有民主制度。通过瑞士阿劳市的参与式预算案例，研究表明新方法能更公平代表选民偏好，提升项目多样性和合法性。

Motivation: 尽管投票方法在多学科研究中取得进展，许多民主国家仍使用过时方法，未能适应现代多元社会。本文旨在展示如何通过改进投票方法提升民主决策的公平性和包容性。

Method: 研究采用新的参与式预算方法（累积投票和份额平等法），在瑞士阿劳市进行实证分析，评估其效果。

Result: 结果显示，新方法以相同预算选出更多项目，更广泛代表选民地理和偏好，尤其惠及以往被忽视的群体，同时增强项目多样性和合法性。

Conclusion: 比例投票方法更受选民欢迎，能体现民主价值观（如利他主义和妥协），为民主升级提供新蓝图。

Abstract: Voting methods are instrumental design element of democracies. Citizens use
them to express and aggregate their preferences to reach a collective decision.
However, voting outcomes can be as sensitive to voting rules as they are to
people's voting choices. Despite the significance and inter-disciplinary
scientific progress on voting methods, several democracies keep relying on
outdated voting methods that do not fit modern, pluralistic societies well,
while lacking social innovation. Here, we demonstrate how one can upgrade
real-world democracies, namely by using alternative preferential voting methods
such as cumulative voting and the method of equal shares designed for a
proportional representation of voters' preferences. By rigorously assessing a
new participatory budgeting approach applied in the city of Aarau, Switzerland,
we unravel the striking voting outcomes of fair voting methods: more winning
projects with the same budget and broader geographic and preference
representation of citizens by the elected projects, in particular for voters
who used to be under-represented, while promoting novel project ideas. We
provide profound causal evidence showing that citizens prefer proportional
voting methods, which possess strong legitimacy without the need of very
technical specialized explanations. We also reveal strong underlying democratic
values exhibited by citizens who support fair voting methods such as altruism
and compromise. These findings come with a global momentum to unleash a new and
long-awaited participation blueprint of how to upgrade democracies.

</details>


### [907] [Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI](https://arxiv.org/abs/2505.14435)
*Annika Bush,Meltem Aksoy,Markus Pauly,Greta Ontrup*

Key words: AI, Large Language Models, sustainability, bias, decision-making

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本研究探讨了五大先进LLMs（Claude、DeepSeek、GPT、LLaMA和Mistral）对可持续性及其与AI关系的理解差异，发现模型间存在显著分歧，强调在选择LLMs时需注意其特定偏见。

Motivation: 随着组织在可持续性决策中依赖AI系统，了解LLMs中的偏见和视角变得至关重要。

Method: 通过向每个模型100次施测已验证的可持续性心理测量问卷，分析响应模式和变异性。

Result: 发现模型间显著差异，如GPT对AI与可持续性兼容性持怀疑态度，而LLaMA表现出技术乐观主义。不同模型对责任归属也有分歧。

Conclusion: 模型选择可能显著影响组织可持续性策略，需注意LLMs的特定偏见。

Abstract: As organizations increasingly rely on AI systems for decision support in
sustainability contexts, it becomes critical to understand the inherent biases
and perspectives embedded in Large Language Models (LLMs). This study
systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,
GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship
with AI. We administered validated, psychometric sustainability-related
questionnaires - each 100 times per model -- to capture response patterns and
variability. Our findings revealed significant inter-model differences: For
example, GPT exhibited skepticism about the compatibility of AI and
sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect
scores for several Sustainable Development Goals (SDGs). Models also diverged
in attributing institutional responsibility for AI and sustainability
integration, a results that holds implications for technology governance
approaches. Our results demonstrate that model selection could substantially
influence organizational sustainability strategies, highlighting the need for
awareness of model-specific biases when deploying LLMs for
sustainability-related decision-making.

</details>


### [908] [LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas](https://arxiv.org/abs/2505.12257)
*Evgeny Markhasin*

Key words: Large Language Models, Persistent Workflow Prompting, error detection, scientific documents, multimodal interpretation

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 研究提出基于持续工作流程提示（PWP）的结构化LLM上下文调节方法，用于提升通用LLM在科学文档中精确验证任务的可靠性，初步结果显示在文本和图像公式错误识别中表现良好。

Motivation: 科学和技术文档中的细微错误对LLM构成挑战，其固有纠错倾向可能掩盖不准确性。研究旨在探索如何通过上下文调节提升LLM的精确验证能力。

Method: 基于PWP原则设计结构化LLM上下文调节策略，在无需API或模型修改的情况下，通过标准聊天界面评估其对化学公式错误识别的效果。

Result: PWP结构化方法显著提升了文本错误识别能力，且使Gemini 2.5 Pro能识别图像公式错误，而ChatGPT Plus o3未成功。

Conclusion: PWP-informed上下文调节为科学文档中的细节导向验证任务提供了一种高效可行的方法，但需进一步验证其广泛适用性。

Abstract: Identifying subtle technical errors within complex scientific and technical
documents, especially those requiring multimodal interpretation (e.g., formulas
in images), presents a significant hurdle for Large Language Models (LLMs)
whose inherent error-correction tendencies can mask inaccuracies. This
exploratory proof-of-concept (PoC) study investigates structured LLM context
conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a
methodological strategy to modulate this LLM behavior at inference time. The
approach is designed to enhance the reliability of readily available,
general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for
precise validation tasks, crucially relying only on their standard chat
interfaces without API access or model modifications. To explore this
methodology, we focused on validating chemical formulas within a single,
complex test paper with known textual and image-based errors. Several prompting
strategies were evaluated: while basic prompts proved unreliable, an approach
adapting PWP structures to rigorously condition the LLM's analytical mindset
appeared to improve textual error identification with both models. Notably,
this method also guided Gemini 2.5 Pro to repeatedly identify a subtle
image-based formula error previously overlooked during manual review, a task
where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight
specific LLM operational modes that impede detail-oriented validation and
suggest that PWP-informed context conditioning offers a promising and highly
accessible technique for developing more robust LLM-driven analytical
workflows, particularly for tasks requiring meticulous error detection in
scientific and technical documents. Extensive validation beyond this limited
PoC is necessary to ascertain broader applicability.

</details>


### [909] [Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact](https://arxiv.org/abs/2505.13469)
*Aayam Bansal,Harsh Vardhan Narsaria*

Key words: 公平性, 机器学习, 贷款决策, 盈利能力, 算法约束

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文研究了机器学习模型在贷款决策中公平性与盈利性的权衡，发现公平性的不同约束方式对利润和违约率有不同影响，且‘公平无知’方法表现最佳。

Motivation: 金融领域机器学习的广泛应用引发了算法公平性的担忧，研究旨在量化公平性约束对贷款决策中盈利性的影响。

Method: 通过模拟反映现实贷款模式的合成数据，比较了不同的公平性干预方法（如人口平等和机会均等）对利润和违约率的影响。

Result: 机会均等约束通常比人口平等对利润影响更小，但‘公平无知’方法在公平性和盈利性上表现更好。同时研究还明确了公平贷款盈利的具体经济条件。

Conclusion: 该研究为平衡贷款算法中的伦理与商业目标提供了实践指导。

Abstract: As financial institutions increasingly rely on machine learning models to
automate lending decisions, concerns about algorithmic fairness have risen.
This paper explores the tradeoff between enforcing fairness constraints (such
as demographic parity or equal opportunity) and maximizing lender
profitability. Through simulations on synthetic data that reflects real-world
lending patterns, we quantify how different fairness interventions impact
profit margins and default rates. Our results demonstrate that equal
opportunity constraints typically impose lower profit costs than demographic
parity, but surprisingly, removing protected attributes from the model
(fairness through unawareness) outperforms explicit fairness interventions in
both fairness and profitability metrics. We further identify the specific
economic conditions under which fair lending becomes profitable and analyze the
feature-specific drivers of unfairness. These findings offer practical guidance
for designing lending algorithms that balance ethical considerations with
business objectives.

</details>


### [910] [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
*Shitong Duan,Xiaoyuan Yi,Peng Zhang,Dongkuan Xu,Jing Yao,Tun Lu,Ning Gu,Xing Xie*

Key words: LLMs, 价值测量, AdAEM, 动态评估, 信息理论

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: AdAEM是一种新型自扩展评估框架，用于揭示大型语言模型（LLMs）的价值倾向，通过动态生成测试问题解决现有数据集信息不足的问题。

Motivation: 当前价值测量数据集因测试问题过时、污染或泛泛而无法有效捕捉LLMs的差异，导致结果饱和且缺乏信息量。

Method: AdAEM通过上下文化优化方式，自动生成和扩展测试问题，最大化信息理论目标以提取最新或有争议的话题。

Result: 生成12,310个基于Schwartz价值理论的问题，分析了16个LLMs的价值差异，验证了方法的有效性和区分能力。

Conclusion: AdAEM能够与LLMs共同演进，持续追踪其价值动态，为更深入的价值研究奠定基础。

Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.

</details>


### [911] [Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks](https://arxiv.org/abs/2505.13565)
*Oier Mentxaka,Natalia Díaz-Rodríguez,Mark Coeckelbergh,Marcos López de Prado,Emilia Gómez,David Fernández Llorca,Enrique Herrera-Viedma,Francisco Herrera*

Key words: AI, 民主, 风险, 机会, 欧盟治理, 透明度

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该论文提出了一种双重分类法（AIRD和AIPD）来评估AI对民主的影响，分析了其风险与积极贡献，并结合欧盟的伦理治理框架提出了缓解策略，旨在为研究、政策和技术设计提供指导。

Motivation: 探讨AI对民主治理的复杂影响，旨在平衡其风险与机遇，并通过实践治理工具实现民主价值观。

Method: 提出AIRD和AIPD双重分类法，结合欧盟伦理AI治理框架（如透明度和社会福利），分析风险与贡献。

Result: 提供了一个可操作的框架，帮助学者、政策制定者和技术人员在AI发展中融入民主原则。

Conclusion: 通过理论与实践的整合，论文为算法时代的包容性、责任感和韧性民主系统奠定了基础。

Abstract: Artificial Intelligence (AI) poses both significant risks and valuable
opportunities for democratic governance. This paper introduces a dual taxonomy
to evaluate AI's complex relationship with democracy: the AI Risks to Democracy
(AIRD) taxonomy, which identifies how AI can undermine core democratic
principles such as autonomy, fairness, and trust; and the AI's Positive
Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to
enhance transparency, participation, efficiency, and evidence-based
policymaking.
  Grounded in the European Union's approach to ethical AI governance, and
particularly the seven Trustworthy AI requirements proposed by the European
Commission's High-Level Expert Group on AI, each identified risk is aligned
with mitigation strategies based on EU regulatory and normative frameworks. Our
analysis underscores the transversal importance of transparency and societal
well-being across all risk categories and offers a structured lens for aligning
AI systems with democratic values.
  By integrating democratic theory with practical governance tools, this paper
offers a normative and actionable framework to guide research, regulation, and
institutional design to support trustworthy, democratic AI. It provides
scholars with a conceptual foundation to evaluate the democratic implications
of AI, equips policymakers with structured criteria for ethical oversight, and
helps technologists align system design with democratic principles. In doing
so, it bridges the gap between ethical aspirations and operational realities,
laying the groundwork for more inclusive, accountable, and resilient democratic
systems in the algorithmic age.

</details>


### [912] [Fuck the Algorithm: Conceptual Issues in Algorithmic Bias](https://arxiv.org/abs/2505.13509)
*Catherine Stinson*

Key words: 算法偏见, 统计偏见, 道德偏见, 责任划分, 算法歧视

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文探讨了算法偏见的争议，澄清了算法本身是否可以存在偏见的问题，分析了偏见的多重含义，并提出了如何从统计偏见中产生具有道德影响的偏见。

Motivation: 解决关于算法偏见的争议，澄清概念，为决策责任和预防算法歧视提供依据。

Method: 分析了‘算法本身’的定义与‘偏见’的多重含义，并通过实例说明如何从统计偏见中产生道德偏见。

Result: 明确了算法可以是偏见的来源，指出了在招聘、警务、医疗等领域的数据偏见实例。

Conclusion: 认识到算法本身可能存在偏见，对划分责任和预防算法歧视至关重要。

Abstract: Algorithmic bias has been the subject of much recent controversy. To clarify
what is at stake and to make progress resolving the controversy, a better
understanding of the concepts involved would be helpful. The discussion here
focuses on the disputed claim that algorithms themselves cannot be biased. To
clarify this claim we need to know what kind of thing 'algorithms themselves'
are, and to disambiguate the several meanings of 'bias' at play. This further
involves showing how bias of moral import can result from statistical biases,
and drawing connections to previous conceptual work about political artifacts
and oppressive things. Data bias has been identified in domains like hiring,
policing and medicine. Examples where algorithms themselves have been
pinpointed as the locus of bias include recommender systems that influence
media consumption, academic search engines that influence citation patterns,
and the 2020 UK algorithmically-moderated A-level grades. Recognition that
algorithms are a kind of thing that can be biased is key to making decisions
about responsibility for harm, and preventing algorithmically mediated
discrimination.

</details>


### [913] [Upgrading Democracies with Fairer Voting Methods](https://arxiv.org/abs/2505.14349)
*Evangelos Pournaras,Srijoni Majumdar,Thomas Wellings,Joshua C. Yang,Fatemeh B. Heravan,Regula Hänggli Fricker,Dirk Helbing*

Key words: 投票方法、民主升级、参与式预算、比例代表、公平性

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 本文探讨了如何通过替代性偏好投票方法（如累积投票和等额份额法）升级现实民主制度，以更公平地代表选民偏好，并通过瑞士阿劳市的参与式预算实践展示了这些方法的显著效果。

Motivation: 现有的投票方法在现代多元化社会中表现不佳，导致选民代表性不足和缺乏社会创新。

Method: 采用累积投票和等额份额法等替代性偏好投票方法，并在瑞士阿劳市的参与式预算中进行实证评估。

Result: 公平投票方法实现了更多项目中标、更广泛的地理和偏好代表性，尤其惠及过去被低估的选民，并推动了新项目理念。

Conclusion: 公民更倾向于比例投票方法，这些方法具有强合法性且无需复杂技术解释，同时体现了民主价值观如利他主义和妥协。

Abstract: Voting methods are instrumental design element of democracies. Citizens use
them to express and aggregate their preferences to reach a collective decision.
However, voting outcomes can be as sensitive to voting rules as they are to
people's voting choices. Despite the significance and inter-disciplinary
scientific progress on voting methods, several democracies keep relying on
outdated voting methods that do not fit modern, pluralistic societies well,
while lacking social innovation. Here, we demonstrate how one can upgrade
real-world democracies, namely by using alternative preferential voting methods
such as cumulative voting and the method of equal shares designed for a
proportional representation of voters' preferences. By rigorously assessing a
new participatory budgeting approach applied in the city of Aarau, Switzerland,
we unravel the striking voting outcomes of fair voting methods: more winning
projects with the same budget and broader geographic and preference
representation of citizens by the elected projects, in particular for voters
who used to be under-represented, while promoting novel project ideas. We
provide profound causal evidence showing that citizens prefer proportional
voting methods, which possess strong legitimacy without the need of very
technical specialized explanations. We also reveal strong underlying democratic
values exhibited by citizens who support fair voting methods such as altruism
and compromise. These findings come with a global momentum to unleash a new and
long-awaited participation blueprint of how to upgrade democracies.

</details>


### [914] [Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI](https://arxiv.org/abs/2505.14435)
*Annika Bush,Meltem Aksoy,Markus Pauly,Greta Ontrup*

Key words: 大型语言模型,可持续性,AI偏见,技术治理,SDGs

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 研究分析了五种先进大型语言模型（LLMs）对可持续性的理解及其与AI的关系，揭示了模型间显著差异及其对组织策略和技术治理的影响。

Motivation: 随着组织在可持续性决策中依赖AI系统，了解LLMs中的偏见和视角变得至关重要。

Method: 通过向五种LLMs（Claude、DeepSeek、GPT、LLaMA、Mistral）各100次施测验证过的可持续性相关问卷，捕捉响应模式和变异。

Result: 模型间存在显著差异，如GPT对AI与可持续性的兼容性持怀疑态度，而LLaMA表现出极端技术乐观主义。

Conclusion: 模型选择可能显著影响组织可持续性策略，提醒在部署LLMs时需要意识到模型特定偏见。

Abstract: As organizations increasingly rely on AI systems for decision support in
sustainability contexts, it becomes critical to understand the inherent biases
and perspectives embedded in Large Language Models (LLMs). This study
systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,
GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship
with AI. We administered validated, psychometric sustainability-related
questionnaires - each 100 times per model -- to capture response patterns and
variability. Our findings revealed significant inter-model differences: For
example, GPT exhibited skepticism about the compatibility of AI and
sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect
scores for several Sustainable Development Goals (SDGs). Models also diverged
in attributing institutional responsibility for AI and sustainability
integration, a results that holds implications for technology governance
approaches. Our results demonstrate that model selection could substantially
influence organizational sustainability strategies, highlighting the need for
awareness of model-specific biases when deploying LLMs for
sustainability-related decision-making.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [915] [CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction](https://arxiv.org/abs/2505.13558)
*Yingjie Kuang,Tianchen Zhang,Zhen-Wei Huang,Zhongjie Zeng,Zhe-Yuan Li,Ling Huang,Yuefang Gao*

Key words: 购买意向预测, 客户聚类, GRU, 注意力机制, 不平衡分布

<details>
  <summary>Details</summary>

Main category: econ.EM

TL;DR: 本文提出了一种结合聚类和注意力机制的GRU模型（CAGRU），用于预测客户购买意向，特别关注了客户群的分布不平衡问题。

Motivation: 预测客户是否重复购买是商业策略的关键，但目前研究多关注购买产品类型，而忽视了客户群的分布不平衡问题。

Method: 通过多模态数据，先聚类客户群，再用GRU提取时间序列特征，并引入注意力机制。针对不同客户群单独训练模型。

Result: 在四个数据集上的实验表明，CAGRU方法表现优越。

Conclusion: CAGRU能有效捕捉不同客户群的行为差异，解决了客户群不平衡问题。

Abstract: Accurately predicting customers' purchase intentions is critical to the
success of a business strategy. Current researches mainly focus on analyzing
the specific types of products that customers are likely to purchase in the
future, little attention has been paid to the critical factor of whether
customers will engage in repurchase behavior. Predicting whether a customer
will make the next purchase is a classic time series forecasting task. However,
in real-world purchasing behavior, customer groups typically exhibit imbalance
- i.e., there are a large number of occasional buyers and a small number of
loyal customers. This head-to-tail distribution makes traditional time series
forecasting methods face certain limitations when dealing with such problems.
To address the above challenges, this paper proposes a unified Clustering and
Attention mechanism GRU model (CAGRU) that leverages multi-modal data for
customer purchase intention prediction. The framework first performs customer
profiling with respect to the customer characteristics and clusters the
customers to delineate the different customer clusters that contain similar
features. Then, the time series features of different customer clusters are
extracted by GRU neural network and an attention mechanism is introduced to
capture the significance of sequence locations. Furthermore, to mitigate the
head-to-tail distribution of customer segments, we train the model separately
for each customer segment, to adapt and capture more accurately the differences
in behavioral characteristics between different customer segments, as well as
the similar characteristics of the customers within the same customer segment.
We constructed four datasets and conducted extensive experiments to demonstrate
the superiority of the proposed CAGRU approach.

</details>


### [916] [CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction](https://arxiv.org/abs/2505.13558)
*Yingjie Kuang,Tianchen Zhang,Zhen-Wei Huang,Zhongjie Zeng,Zhe-Yuan Li,Ling Huang,Yuefang Gao*

Key words: 客户购买意图预测、GRU模型、注意力机制、客户群体聚类、时间序列预测

<details>
  <summary>Details</summary>

Main category: econ.EM

TL;DR: 本文提出了一种结合聚类和注意力机制的GRU模型（CAGRU），用于预测客户购买意图，解决客户群体不平衡的问题。

Motivation: 现有研究主要关注客户未来可能购买的产品类型，而忽略了客户是否会重复购买这一关键因素。由于客户群体中通常存在不平衡分布（大量偶尔买家与少量忠诚客户），传统时间序列预测方法在此类问题上存在局限性。

Method: 提出CAGRU模型：首先通过客户特征进行聚类，划分不同客户群体；然后使用GRU神经网络提取时间序列特征，并引入注意力机制捕捉序列位置的重要性；针对每个客户群体分别训练模型。

Result: 在四个数据集上的实验证明了CAGRU方法的优越性。

Conclusion: CAGRU能够更准确地捕捉不同客户群体间的行为差异以及同一群体内的相似特征，为解决客户群体不平衡问题提供了有效方法。

Abstract: Accurately predicting customers' purchase intentions is critical to the
success of a business strategy. Current researches mainly focus on analyzing
the specific types of products that customers are likely to purchase in the
future, little attention has been paid to the critical factor of whether
customers will engage in repurchase behavior. Predicting whether a customer
will make the next purchase is a classic time series forecasting task. However,
in real-world purchasing behavior, customer groups typically exhibit imbalance
- i.e., there are a large number of occasional buyers and a small number of
loyal customers. This head-to-tail distribution makes traditional time series
forecasting methods face certain limitations when dealing with such problems.
To address the above challenges, this paper proposes a unified Clustering and
Attention mechanism GRU model (CAGRU) that leverages multi-modal data for
customer purchase intention prediction. The framework first performs customer
profiling with respect to the customer characteristics and clusters the
customers to delineate the different customer clusters that contain similar
features. Then, the time series features of different customer clusters are
extracted by GRU neural network and an attention mechanism is introduced to
capture the significance of sequence locations. Furthermore, to mitigate the
head-to-tail distribution of customer segments, we train the model separately
for each customer segment, to adapt and capture more accurately the differences
in behavioral characteristics between different customer segments, as well as
the similar characteristics of the customers within the same customer segment.
We constructed four datasets and conducted extensive experiments to demonstrate
the superiority of the proposed CAGRU approach.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [917] [Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning](https://arxiv.org/abs/2505.14581)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Key words: 强化学习,认知无线电网络,能量收集,深度Q网络

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 本文提出了一种基于强化学习的认知无线电网络性能优化方法，通过能量收集和深度Q网络（DQN）策略实现次用户的数据速率最大化。

Motivation: 在认知无线电网络中，次用户在满足主用户干扰限制的同时，需要高效利用有限的能量资源以实现数据传输优化。

Method: 采用时间切换方法从主用户信号中收集能量，并结合DQN算法动态选择能量收集或数据传输，以及传输功率。

Result: 该方法在数据速率优化上优于基准策略，且表现出良好的收敛性。

Conclusion: 强化学习和能量收集的结合为认知无线电网络的性能优化提供了有效解决方案。

Abstract: In this paper, a reinforcement learning technique is employed to maximize the
performance of a cognitive radio network (CRN). In the presence of primary
users (PUs), it is presumed that two secondary users (SUs) access the licensed
band within underlay mode. In addition, the SU transmitter is assumed to be an
energy-constrained device that requires harvesting energy in order to transmit
signals to their intended destination. Therefore, we propose that there are two
main sources of energy; the interference of PUs' transmissions and ambient
radio frequency (RF) sources. The SU will select whether to gather energy from
PUs or only from ambient sources based on a predetermined threshold. The
process of energy harvesting from the PUs' messages is accomplished via the
time switching approach. In addition, based on a deep Q-network (DQN) approach,
the SU transmitter determines whether to collect energy or transmit messages
during each time slot as well as selects the suitable transmission power in
order to maximize its average data rate. Our approach outperforms a baseline
strategy and converges, as shown by our findings.

</details>


### [918] [Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning](https://arxiv.org/abs/2505.14581)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Key words: 强化学习, 认知无线电网络, 能量收集, 深度Q网络, 数据传输

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 提出了一种基于强化学习的认知无线电网络性能优化方法，通过两种能量源为受限设备供能，并利用DQN算法实现能效和数据率的最大化。

Motivation: 在存在主用户的情况下，如何为能量受限的次级用户高效分配能量传输与信号传输，以最大化其平均数据率。

Method: 采用深度Q网络（DQN）算法，次级用户根据预定义阈值选择从主用户或环境RF源获取能量，并在每个时隙决定是收集能量还是传输信号。

Result: 所提方法优于基线策略并能够收敛。

Conclusion: 强化学习方法能有效优化认知无线电网络的性能，特别是在能量受限场景下。

Abstract: In this paper, a reinforcement learning technique is employed to maximize the
performance of a cognitive radio network (CRN). In the presence of primary
users (PUs), it is presumed that two secondary users (SUs) access the licensed
band within underlay mode. In addition, the SU transmitter is assumed to be an
energy-constrained device that requires harvesting energy in order to transmit
signals to their intended destination. Therefore, we propose that there are two
main sources of energy; the interference of PUs' transmissions and ambient
radio frequency (RF) sources. The SU will select whether to gather energy from
PUs or only from ambient sources based on a predetermined threshold. The
process of energy harvesting from the PUs' messages is accomplished via the
time switching approach. In addition, based on a deep Q-network (DQN) approach,
the SU transmitter determines whether to collect energy or transmit messages
during each time slot as well as selects the suitable transmission power in
order to maximize its average data rate. Our approach outperforms a baseline
strategy and converges, as shown by our findings.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [919] [An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents](https://arxiv.org/abs/2505.13504)
*Ayesha Amjad,Saurav Sthapit,Tahir Qasim Syed*

Key words: LLM, 强化学习, 多智能体, 信息提取, 自动化

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文提出了一种基于多智能体框架的自适应AI系统，利用大型语言模型（LLM）和强化学习（RL）驱动智能体，实现表单文档中字母数字数据的自动化提取和自我改进。

Motivation: 传统的基于OCR和单一流水线的数据提取方法存在局限性，无法系统性改进。本文旨在通过多智能体框架解决LLM提取的不确定性，实现自动化且无需人工干预的信息提取。

Method: 采用了模块化的多智能体框架，结合任务特定提示和RL策略（奖励与惩罚），通过元提示智能体学习过去错误并改进基于提示的提取智能体。

Result: 在SOIRE和CORD两个基准数据集上的实验结果显示了该框架的潜力。

Conclusion: 提出的自适应系统能够处理多样化的文档和布局，提高了信息提取的准确性和自动化程度。

Abstract: Extracting alphanumeric data from form-like documents such as invoices,
purchase orders, bills, and financial documents is often performed via vision
(OCR) and learning algorithms or monolithic pipelines with limited potential
for systemic improvements. We propose an agentic AI system that leverages Large
Language Model (LLM) agents and a reinforcement learning (RL) driver agent to
automate consistent, self-improving extraction under LLM inference uncertainty.
Our work highlights the limitations of monolithic LLM-based extraction and
introduces a modular, multi-agent framework with task-specific prompts and an
RL policy of rewards and penalties to guide a meta-prompting agent to learn
from past errors and improve prompt-based actor agents. This self-corrective
adaptive system handles diverse documents, file formats, layouts, and LLMs,
aiming to automate accurate information extraction without the need for human
intervention. Results as reported on two benchmark datasets of SOIRE, and CORD,
are promising for the agentic AI framework.

</details>


### [920] [Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering](https://arxiv.org/abs/2505.13520)
*Hessa Alawwad,Usman Naseem,Areej Alhothali,Ali Alkhathlan,Amani Jamal*

Key words: 教材问答, 多模态学习, 语义对齐, 文档检索, 多目标训练

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文提出了一种新的多模态教材问答方法JETRTQA，通过多目标联合训练增强语义表示，显著提升了复杂教育场景中的文档检索效果。

Motivation: 教材问答任务需要复杂的多模态上下文理解，现有方法在准确语义对齐和任务特定文档检索方面存在不足，JETRTQA旨在解决这些问题。

Method: JETRTQA是一个基于检索-生成架构的多模态学习框架，通过多目标联合训练（包括成对排序和答案隐含监督）优化语义表示。

Result: 在CK12-QA数据集上，JETRTQA显著优于现有方法，验证集准确率提升2.4%，测试集提升11.1%。

Conclusion: JETRTQA通过改进语义表示和文档检索相关性，为复杂教育场景下的多模态教材问答提供了有效解决方案。

Abstract: Textbook question answering (TQA) is a complex task, requiring the
interpretation of complex multimodal context. Although recent advances have
improved overall performance, they often encounter difficulties in educational
settings where accurate semantic alignment and task-specific document retrieval
are essential. In this paper, we propose a novel approach to multimodal
textbook question answering by introducing a mechanism for enhancing semantic
representations through multi-objective joint training. Our model, Joint
Embedding Training With Ranking Supervision for Textbook Question Answering
(JETRTQA), is a multimodal learning framework built on a retriever--generator
architecture that uses a retrieval-augmented generation setup, in which a
multimodal large language model generates answers. JETRTQA is designed to
improve the relevance of retrieved documents in complex educational contexts.
Unlike traditional direct scoring approaches, JETRTQA learns to refine the
semantic representations of questions and documents through a supervised signal
that combines pairwise ranking and implicit supervision derived from answers.
We evaluate our method on the CK12-QA dataset and demonstrate that it
significantly improves the discrimination between informative and irrelevant
documents, even when they are long, complex, and multimodal. JETRTQA
outperforms the previous state of the art, achieving a 2.4\% gain in accuracy
on the validation set and 11.1\% on the test set.

</details>


### [921] [Geography-Aware Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2505.13526)
*Zhao Liu,Wei Liu,Huajie Zhu,Jianxing Yu,Jian Yin,Wang-Chien Lee,Shun Wang*

Key words: POI推荐,LLM,地理信息,语义建模

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 提出GA-LLM框架，结合地理坐标和POI转移关系，提升LLM在POI推荐中的表现。

Motivation: 解决LLM在空间任务中难以建模精确地理信息与POI转移关系的挑战。

Method: 通过GCIM模块编码地理坐标，PAM模块整合POI转移关系。

Result: 在三个真实数据集上实现SOTA性能。

Conclusion: GA-LLM有效结合了地理信息和语义理解，提升推荐精确度。

Abstract: The next Point-of-Interest (POI) recommendation task aims to predict users'
next destinations based on their historical movement data and plays a key role
in location-based services and personalized applications. Accurate next POI
recommendation depends on effectively modeling geographic information and POI
transition relations, which are crucial for capturing spatial dependencies and
user movement patterns. While Large Language Models (LLMs) exhibit strong
capabilities in semantic understanding and contextual reasoning, applying them
to spatial tasks like next POI recommendation remains challenging. First, the
infrequent nature of specific GPS coordinates makes it difficult for LLMs to
model precise spatial contexts. Second, the lack of knowledge about POI
transitions limits their ability to capture potential POI-POI relationships. To
address these issues, we propose GA-LLM (Geography-Aware Large Language Model),
a novel framework that enhances LLMs with two specialized components. The
Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into
spatial representations using hierarchical and Fourier-based positional
encoding, enabling the model to understand geographic features from multiple
perspectives. The POI Alignment Module (PAM) incorporates POI transition
relations into the LLM's semantic space, allowing it to infer global POI
relationships and generalize to unseen POIs. Experiments on three real-world
datasets demonstrate the state-of-the-art performance of GA-LLM.

</details>


### [922] [LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems](https://arxiv.org/abs/2505.13528)
*Shengkang Gu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Ning Gu,Li Shang,Tun Lu*

Key words: 关键词：推荐系统, 虚假用户攻击, LLM代理, 评论攻击, 数据安全

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 总结：本文提出了Agent4SR框架，利用LLM代理进行高影响力的虚假用户攻击，通过生成评分和评论来操纵推荐系统，实验表明其有效性和隐蔽性优于现有方法。

Motivation: 动机：推荐系统容易受到虚假用户攻击，传统方法依赖简单启发式且忽略文本评论的潜在影响，因此需要更高效和隐蔽的攻击策略。

Method: 方法：使用基于LLM的代理Agent4SR，模拟真实用户行为，通过目标画像构建、混合记忆检索和评论攻击策略来增强操纵效果。

Result: 结果：在多数据集和推荐系统架构上的实验显示，Agent4SR在攻击效果和隐蔽性上优于现有低知识基线方法。

Conclusion: 结论：LLM驱动的代理带来了新的威胁，凸显了现代推荐系统急需加强防御措施。

Abstract: Recommender systems (RS) are increasingly vulnerable to shilling attacks,
where adversaries inject fake user profiles to manipulate system outputs.
Traditional attack strategies often rely on simplistic heuristics, require
access to internal RS data, and overlook the manipulation potential of textual
reviews. In this work, we introduce Agent4SR, a novel framework that leverages
Large Language Model (LLM)-based agents to perform low-knowledge, high-impact
shilling attacks through both rating and review generation. Agent4SR simulates
realistic user behavior by orchestrating adversarial interactions, selecting
items, assigning ratings, and crafting reviews, while maintaining behavioral
plausibility. Our design includes targeted profile construction, hybrid memory
retrieval, and a review attack strategy that propagates target item features
across unrelated reviews to amplify manipulation. Extensive experiments on
multiple datasets and RS architectures demonstrate that Agent4SR outperforms
existing low-knowledge baselines in both effectiveness and stealth. Our
findings reveal a new class of emergent threats posed by LLM-driven agents,
underscoring the urgent need for enhanced defenses in modern recommender
systems.

</details>


### [923] [Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments](https://arxiv.org/abs/2505.13535)
*Aniket Bhattacharyya,Anurag Tripathi,Ujjal Das,Archan Karmakar,Amit Pathak,Maneesh Gupta*

Key words: 信息提取, 视觉丰富文档, 语义块, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: BLOCKIE是一种基于LLM的新方法，通过将VRD组织为可重用的语义块进行独立处理，优于现有方法。

Motivation: 现有方法在VRD信息提取中缺乏推理能力，难以处理未明确提及的值或新格式文档。

Method: 将VRD分解为独立的语义块，通过局部化推理处理每个块。

Result: 在F1分数上比现有方法高1-3%，对未见过格式的文档也具有鲁棒性。

Conclusion: BLOCKIE提高了VRD信息提取的准确性和泛化能力。

Abstract: Information extraction (IE) from Visually Rich Documents (VRDs) containing
layout features along with text is a critical and well-studied task.
Specialized non-LLM NLP-based solutions typically involve training models using
both textual and geometric information to label sequences/tokens as named
entities or answers to specific questions. However, these approaches lack
reasoning, are not able to infer values not explicitly present in documents,
and do not generalize well to new formats. Generative LLM-based approaches
proposed recently are capable of reasoning, but struggle to comprehend clues
from document layout especially in previously unseen document formats, and do
not show competitive performance in heterogeneous VRD benchmark datasets. In
this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs
into localized, reusable semantic textual segments called $\textit{semantic
blocks}$, which are processed independently. Through focused and more
generalizable reasoning,our approach outperforms the state-of-the-art on public
VRD benchmarks by 1-3% in F1 scores, is resilient to document formats
previously not encountered and shows abilities to correctly extract information
not explicitly present in documents.

</details>


### [924] [RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines](https://arxiv.org/abs/2505.13538)
*Dvir Cohen,Lin Burg,Gilad Barkan*

Key words: RAG, 评估框架, 解释性, 可操作性, 性能优化

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: RAGXplain是一个新的评估框架，旨在通过将定量评分转化为可操作的改进建议，帮助用户理解和优化RAG系统的复杂流程。

Motivation: 传统RAG评估方法仅提供定量评分，但缺乏对系统改进的具体指导，因此需要一种能够提供透明解释和改进建议的框架。

Method: RAGXplain利用LLM推理将原始评分转化为清晰的叙述，识别性能差距并提出针对性改进措施。

Result: 实验表明，RAGXplain的评估与人类判断高度一致，且其改进建议能显著提升系统性能。

Conclusion: RAGXplain填补了定量评估与实际优化之间的鸿沟，帮助用户更好地理解和增强AI系统。

Abstract: Retrieval-Augmented Generation (RAG) systems show promise by coupling large
language models with external knowledge, yet traditional RAG evaluation methods
primarily report quantitative scores while offering limited actionable guidance
for refining these complex pipelines. In this paper, we introduce RAGXplain, an
evaluation framework that quantifies RAG performance and translates these
assessments into clear insights that clarify the workings of its complex,
multi-stage pipeline and offer actionable recommendations. Using LLM reasoning,
RAGXplain converts raw scores into coherent narratives identifying performance
gaps and suggesting targeted improvements. By providing transparent
explanations for AI decision-making, our framework fosters user trust-a key
challenge in AI adoption. Our LLM-based metric assessments show strong
alignment with human judgments, and experiments on public question-answering
datasets confirm that applying RAGXplain's actionable recommendations
measurably improves system performance. RAGXplain thus bridges quantitative
evaluation and practical optimization, empowering users to understand, trust,
and enhance their AI systems.

</details>


### [925] [Know Or Not: a library for evaluating out-of-knowledge base robustness](https://arxiv.org/abs/2505.13545)
*Jessica Foo,Pradyumna Shyama Prasad,Shaun Khoo*

Key words: 大语言模型，检索增强生成，OOKB鲁棒性，knowornot，PolicyBench

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文提出了一种新方法来评估大语言模型（LLMs）在检索增强生成（RAG）设置中对超出知识库（OOKB）问题的鲁棒性，并开发了一个开源工具knowornot。

Motivation: 在高风险应用中，LLMs必须避免对超出知识库的问题产生幻觉回答，现有方法缺乏自动评估能力。

Method: 开发了knowornot工具，提供统一API、模块化架构、数据建模和定制工具，用于评估OOKB鲁棒性。

Result: 通过PolicyBench基准测试展示了knowornot的实用性，分析了四个政府政策问答机器人的OOKB鲁棒性。

Conclusion: knowornot为评估LLMs的OOKB鲁棒性提供了高效、灵活的工具，支持定制化评估和扩展。

Abstract: While the capabilities of large language models (LLMs) have progressed
significantly, their use in high-stakes applications have been limited due to
risks of hallucination. One key approach in reducing hallucination is
retrieval-augmented generation (RAG), but even in such setups, LLMs may still
hallucinate when presented with questions outside of the knowledge base. Such
behavior is unacceptable in high-stake applications where LLMs are expected to
abstain from answering queries it does not have sufficient context on. In this
work, we present a novel methodology for systematically evaluating
out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not
know) in the RAG setting, without the need for manual annotation of gold
standard answers. We implement our methodology in knowornot, an open-source
library that enables users to develop their own customized evaluation data and
pipelines for OOKB robustness. knowornot comprises four main features. Firstly,
it provides a unified, high-level API that streamlines the process of setting
up and running robustness benchmarks. Secondly, its modular architecture
emphasizes extensibility and flexibility, allowing users to easily integrate
their own LLM clients and RAG settings. Thirdly, its rigorous data modeling
design ensures experiment reproducibility, reliability and traceability.
Lastly, it implements a comprehensive suite of tools for users to customize
their pipelines. We demonstrate the utility of knowornot by developing a
challenging benchmark, PolicyBench, which spans four Question-Answer (QA)
chatbots on government policies, and analyze its OOKB robustness. The source
code of knowornot is available
https://github.com/govtech-responsibleai/KnowOrNot.

</details>


### [926] [JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation](https://arxiv.org/abs/2505.13550)
*Ke Yang,Kevin Ros,Shankar Kumar Senthil Kumar,ChengXiang Zhai*

Key words: 即时信息推荐，JIR-Arena，基准数据集，评估框架，基础模型

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文提出了即时信息推荐（JIR）的数学定义和评估框架，并引入了多模态基准数据集JIR-Arena，用于评估JIR系统的性能。

Motivation: 当前缺乏对JIR任务的系统定义和评估方法，需要填补这一空白，以提升JIR系统的开发和评测能力。

Method: 通过JIR-Arena数据集，结合人类和AI模型的输入，评估用户信息需求推断、推荐相关性和内容检索效果。

Result: 实验表明，基于基础模型的JIR系统能合理模拟用户需求，但在召回率和内容检索方面仍有挑战。

Conclusion: JIR-Arena为未来研究提供了标准化的评估工具，促进JIR领域的进一步发展。

Abstract: Just-in-time Information Recommendation (JIR) is a service designed to
deliver the most relevant information precisely when users need it, ,
addressing their knowledge gaps with minimal effort and boosting
decision-making and efficiency in daily life. Advances in device-efficient
deployment of foundation models and the growing use of intelligent wearable
devices have made always-on JIR assistants feasible. However, there has been no
systematic effort to formally define JIR tasks or establish evaluation
frameworks. To bridge this gap, we present the first mathematical definition of
JIR tasks and associated evaluation metrics. Additionally, we introduce
JIR-Arena, a multimodal benchmark dataset featuring diverse,
information-request-intensive scenarios to evaluate JIR systems across critical
dimensions: i) accurately inferring user information needs, ii) delivering
timely and relevant recommendations, and iii) avoiding irrelevant content that
may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in
estimating user information needs and uncontrollable system variables affecting
reproducibility. To address these, JIR-Arena: i) combines input from multiple
humans and large AI models to approximate information need distributions; ii)
assesses JIR quality through information retrieval outcomes using static
knowledge base snapshots; and iii) employs a multi-turn, multi-entity
validation framework to improve objectivity and generality. Furthermore, we
implement a baseline JIR system capable of processing real-time information
streams aligned with user inputs. Our evaluation of this baseline system on
JIR-Arena indicates that while foundation model-based JIR systems simulate user
needs with reasonable precision, they face challenges in recall and effective
content retrieval. To support future research in this new area, we fully
release our code and data.

</details>


### [927] [AMAQA: A Metadata-based QA Dataset for RAG Systems](https://arxiv.org/abs/2505.13557)
*Davide Bruni,Marco Avvenuti,Nicola Tonellotto,Maurizio Tesconi*

Key words: 检索增强生成（RAG），QA基准测试，元数据集成，AMAQA数据集

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: AMAQA是一个新的开放访问QA数据集，专为结合文本和元数据的任务设计，填补了当前基准测试中元数据整合的不足。

Motivation: 当前QA任务的基准测试缺乏元数据整合，限制了在需要结合文本和外部信息场景下的评估。

Method: 通过收集110万条来自26个公共Telegram群的英文消息，并添加时间戳、主题、情绪和毒性指标等元数据，构建AMAQA数据集，包含450个高质量QA对。

Result: 利用元数据可将准确率从0.12提升到0.61。通过迭代上下文和添加噪声文档的策略，性能在最佳基线基础上再提升3点，比简单元数据过滤高出14点。

Conclusion: AMAQA为元数据驱动的QA和RAG系统研究提供了新标准，展示了元数据在提升任务性能中的重要性。

Abstract: Retrieval-augmented generation (RAG) systems are widely used in
question-answering (QA) tasks, but current benchmarks lack metadata
integration, hindering evaluation in scenarios requiring both textual data and
external information. To address this, we present AMAQA, a new open-access QA
dataset designed to evaluate tasks combining text and metadata. The integration
of metadata is especially important in fields that require rapid analysis of
large volumes of data, such as cybersecurity and intelligence, where timely
access to relevant information is critical. AMAQA includes about 1.1 million
English messages collected from 26 public Telegram groups, enriched with
metadata such as timestamps, topics, emotional tones, and toxicity indicators,
which enable precise and contextualized queries by filtering documents based on
specific criteria. It also includes 450 high-quality QA pairs, making it a
valuable resource for advancing research on metadata-driven QA and RAG systems.
To the best of our knowledge, AMAQA is the first single-hop QA benchmark to
incorporate metadata and labels such as topics covered in the messages. We
conduct extensive tests on the benchmark, establishing a new standard for
future research. We show that leveraging metadata boosts accuracy from 0.12 to
0.61, highlighting the value of structured context. Building on this, we
explore several strategies to refine the LLM input by iterating over provided
context and enriching it with noisy documents, achieving a further 3-point gain
over the best baseline and a 14-point improvement over simple metadata
filtering. The dataset is available at
https://anonymous.4open.science/r/AMAQA-5D0D/

</details>


### [928] [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
*Anand Selvadurai,Jasheen Shaik,Girish Chandrasekar,ShriRadhaKrishnan Balamurugan,Eswara Reddy*

Key words: MedEIR, 嵌入模型, 医学NLP, 长上下文处理, MTEB基准

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: MedEIR是一个针对医学和通用NLP任务联合优化的嵌入模型，支持长上下文处理，在多个基准测试中优于现有模型。

Motivation: 现有嵌入模型在医学文档语义捕捉、长文档处理和通用任务中表现不佳，需要更通用的解决方案。

Method: MedEIR结合ALiBi长上下文处理技术，预训练60亿token，微调300万句对。

Result: 在MTEB基准测试中，MedEIR在ArguAna、NFCorpus等多任务上表现优异。

Conclusion: MedEIR展示了跨领域任务的强大性能，是现有嵌入模型的有效替代方案。

Abstract: Embedding models have become essential for retrieval-augmented generation
(RAG) tasks, semantic clustering, and text re-ranking. But despite their
growing use, many of these come with notable limitations. For example, Jina
fails to capture the semantic content of medical documents, while models such
as MiniLM often perform poorly on long-form documents. Domain-adapted models,
while specialized, often underperform in general-purpose tasks, reducing their
overall applicability. General-domain tokenizers often misinterpret medical
vocabulary. The limitations of current embedding models, whether in
tokenization accuracy, domain comprehension, or handling long sequences,
highlight the need for more versatile solutions. In this work, we present
MedEIR, a novel embedding model and tokenizer jointly optimized for both
medical and general NLP tasks, incorporating ALiBi-based long-context
processing to support sequences of up to 8,192 tokens. MedEIR was pre-trained
on only 6 billion tokens, significantly fewer than Jina's, followed by
fine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina
V2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),
NFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID
(79.56). These results highlight the potential of MedEIR as a highly effective
embedding model, demonstrating strong performance across both general-purpose
and domain-specific tasks and outperforming existing models on multiple
benchmarks.

</details>


### [929] [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
*Tommaso Mario Buonocore,Enea Parimbelli*

Key words: 大型语言模型,内容审核,检索增强生成,动态拒绝,实时定制

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为RAR的新方法，利用检索增强生成（RAG）架构动态拒绝不安全的用户查询，无需重新训练模型。

Motivation: 解决大型语言模型（LLM）内容审核的挑战，提供灵活且适应性强的解决方案以快速应对新兴威胁。

Method: 通过在向量数据库中策略性地插入和标记恶意文档，系统可以在检索到这些文档时识别并拒绝有害请求。

Result: RAR的性能与Claude 3.5 Sonnet等LLM中的嵌入式审核相当，同时具备更强的灵活性和实时定制能力。

Conclusion: RAR无需改变现有RAG系统的架构，仅需添加专门设计的文档和基于检索结果的简单拒绝机制，即可实现高效审核。

Abstract: Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.

</details>


### [930] [Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning](https://arxiv.org/abs/2412.12504)
*Hong Liu,Saisai Gong,Yixin Ji,Kaixin Wu,Jia Xu,Jinjie Gu*

Key words: 大型语言模型,相关性建模,分布偏移,DaRL,Alipay搜索

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出一种名为DaRL的新框架，通过设计有效的损失函数和多阶段微调策略，增强基于大型语言模型（LLM）的相关性建模能力，以应对数据分布偏移问题。

Motivation: 现有基于LLM的相关性建模方法在细粒度相关度区分和实际数据分布偏移时性能下降。

Method: 设计了DaRL框架，包括分布感知样本增强模块（DASA）和多阶段微调策略。

Result: DaRL在Alipay搜索中提升了LLM模型的区分性和泛化能力。

Conclusion: DaRL有效地解决了LLM在相关性建模中的局限性，并在实际应用中验证了其效果。

Abstract: With the rapid advancement of pre-trained large language models (LLMs),
recent endeavors have leveraged the capabilities of LLMs in relevance modeling,
resulting in enhanced performance. This is usually done through the process of
fine-tuning LLMs on specifically annotated datasets to determine the relevance
between queries and items. However, there are two limitations when LLMs are
naively employed for relevance modeling through fine-tuning and inference.
First, it is not inherently efficient for performing nuanced tasks beyond
simple yes or no answers, such as assessing search relevance. It may therefore
tend to be overconfident and struggle to distinguish fine-grained degrees of
relevance (e.g., strong relevance, weak relevance, irrelevance) used in search
engines. Second, it exhibits significant performance degradation when
confronted with data distribution shift in real-world scenarios. In this paper,
we propose a novel Distribution-Aware Robust Learning framework (DaRL) for
relevance modeling in Alipay Search. Specifically, we design an effective loss
function to enhance the discriminability of LLM-based relevance modeling across
various fine-grained degrees of query-item relevance. To improve the
generalizability of LLM-based relevance modeling, we first propose the
Distribution-Aware Sample Augmentation (DASA) module. This module utilizes
out-of-distribution (OOD) detection techniques to actively select appropriate
samples that are not well covered by the original training set for model
fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to
simultaneously improve in-distribution (ID) and OOD performance, bridging the
performance gap between them. DaRL has been deployed online to serve the
Alipay's insurance product search...

</details>


### [931] [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
*Runchu Tian,Xueqiang Xu,Bowen Jin,SeongKu Kang,Jiawei Han*

Key words: 科学检索, 文档重排, 大语言模型, CoRank, 信息提取

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为CoRank的无训练、模型无关的科学检索重排框架，通过紧凑文档表示和混合设计提升性能。

Motivation: 科学检索中，传统基于大语言模型（LLM）的列表重排方法因首阶段检索效果不佳和上下文窗口限制，无法充分利用相关文档。

Method: CoRank框架分三个阶段：离线提取文档级特征、基于紧凑表示的粗粒度重排、以及对候选文档全文的细粒度重排。

Result: 在LitSearch和CSFCube上的实验表明，CoRank显著提升了重排性能，nDCG@10从32.0提高到39.7。

Conclusion: 信息提取对科学检索重排具有重要价值，紧凑表示与混合设计是有效解决方案。

Abstract: Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.

</details>


### [932] [Field Matters: A lightweight LLM-enhanced Method for CTR Prediction](https://arxiv.org/abs/2505.14057)
*Yu Cui,Feng Liu,Jiawei Chen,Xingyu Lou,Changwang Zhang,Jun Wang,Yuegang Sun,Xiaohu Yang,Can Wang*

Key words: 点击率预测,大语言模型,轻量化,字段级增强,特征交互

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种轻量化的LLM增强CTR方法LLaCTR，通过字段级增强范式解决现有方法计算开销大的问题。

Motivation: 现有LLM增强的CTR方法在处理大规模实例或用户/物品实体时计算开销大，需要一种更高效的解决方案。

Method: LLaCTR利用LLM从小规模特征字段中提炼轻量级语义知识，通过自监督的字段-特征微调，增强特征表示和特征交互。

Result: 在四个数据集上与六种代表性CTR模型集成，LLaCTR在效果和效率上均优于现有LLM增强方法。

Conclusion: LLaCTR是一种高效且有效的轻量化LLM增强CTR方法。

Abstract: Click-through rate (CTR) prediction is a fundamental task in modern
recommender systems. In recent years, the integration of large language models
(LLMs) has been shown to effectively enhance the performance of traditional CTR
methods. However, existing LLM-enhanced methods often require extensive
processing of detailed textual descriptions for large-scale instances or
user/item entities, leading to substantial computational overhead. To address
this challenge, this work introduces LLaCTR, a novel and lightweight
LLM-enhanced CTR method that employs a field-level enhancement paradigm.
Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight
semantic knowledge from small-scale feature fields through self-supervised
field-feature fine-tuning. Subsequently, it leverages this field-level semantic
knowledge to enhance both feature representation and feature interactions. In
our experiments, we integrate LLaCTR with six representative CTR models across
four datasets, demonstrating its superior performance in terms of both
effectiveness and efficiency compared to existing LLM-enhanced methods. Our
code is available at https://anonymous.4open.science/r/LLaCTR-EC46.

</details>


### [933] [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
*Eugene Yang,Andrew Yates,Kathryn Ricci,Orion Weller,Vivek Chari,Benjamin Van Durme,Dawn Lawrie*

Key words: retrieve-and-rerank, neural rerankers, Rank-K, multilingual retrieval, BM25, SPLADE-v3

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: Rank-K是一种基于大型语言模型的列表式段落重排序模型，显著提升了检索效率，尤其在多语言环境下表现优异。

Motivation: 现有的神经重排序模型虽然效果显著，但资源消耗巨大。为了提升效率和可扩展性，研究提出了Rank-K。

Method: Rank-K利用大型语言模型的推理能力，对初始排名列表进行列表式段落重排序，并优化查询时间的计算效率。

Result: Rank-K在BM25初始排名和SPLADE-v3强检索结果上分别提升了23%和19%的检索效果，且在多语言环境下同样有效。

Conclusion: Rank-K不仅显著提升了检索效果，还具有多语言适应性，为资源密集型任务提供了高效解决方案。

Abstract: Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.

</details>


### [934] [TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems](https://arxiv.org/abs/2505.13881)
*Jiahao Yu,Haozhuang Liu,Yeqiu Yang,Lu Chen,Wu Jian,Yuning Jiang,Bo Zheng*

Key words: 回归模型, 推荐系统, 再转换偏差, TranSUN, GTS

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种新颖的TranSUN方法，通过模型内部修正解决了推荐系统中回归模型的再转换偏差问题，并扩展为通用的GTS框架，在多个实际场景中验证了其有效性。

Motivation: 推荐系统中的回归模型存在再转换偏差问题，现有方法多为外部修正，实用性不足，需要一种内置的解决方案。

Method: 提出了TranSUN方法，采用联合偏差学习方式，并扩展为通用回归模型家族GTS，确保无偏差性。

Result: 在多个领域的数据实验中表现优异，并成功应用于淘宝App的实际推荐场景。

Conclusion: TranSUN和GTS通过内置修正有效解决了偏差问题，具有理论和实践的双重优势。

Abstract: Regression models are crucial in recommender systems. However,
retransformation bias problem has been conspicuously neglected within the
community. While many works in other fields have devised effective bias
correction methods, all of them are post-hoc cures externally to the model,
facing practical challenges when applied to real-world recommender systems.
Hence, we propose a preemptive paradigm to eradicate the bias intrinsically
from the models via minor model refinement. Specifically, a novel TranSUN
method is proposed with a joint bias learning manner to offer theoretically
guaranteed unbiasedness under empirical superior convergence. It is further
generalized into a novel generic regression model family, termed Generalized
TranSUN (GTS), which not only offers more theoretical insights but also serves
as a generic framework for flexibly developing various bias-free models.
Comprehensive experimental results demonstrate the superiority of our methods
across data from various domains, which have been successfully deployed in two
real-world industrial recommendation scenarios, i.e. product and short video
recommendation scenarios in Guess What You Like business domain in the homepage
of Taobao App (a leading e-commerce platform), to serve the major online
traffic. Codes will be released after this paper is published.

</details>


### [935] [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
*Sunhao Dai,Wenjie Wang,Liang Pang,Jun Xu,See-Kiong Ng,Ji-Rong Wen,Tat-Seng Chua*

Key words: 生成式AI搜索,反馈机制,NExT-Search,用户调试模式,影子用户模式

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: NExT-Search提出了一种新的生成式AI搜索范式，通过引入细粒度的反馈机制解决传统反馈环的断开问题。

Motivation: 生成式AI搜索虽然提供了便捷的端到端答案，但失去了传统搜索基于用户反馈的持续改进能力，导致系统各阶段难以优化。

Method: 提出了NExT-Search，结合用户调试模式和影子用户模式，通过在线适应和离线更新收集并利用细粒度反馈。

Result: NExT-Search能够为生成式AI搜索系统提供持续的反馈，从而优化查询分解、检索和答案生成等环节。

Conclusion: NExT-Search为构建反馈丰富的AI搜索系统提供了可进化的方向。

Abstract: Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.

</details>


### [936] [Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity](https://arxiv.org/abs/2505.14310)
*Shiyin Tan,Dongyuan Li,Renhe Jiang,Zhen Wang,Xingtong Yu,Manabu Okumura*

Key words: 流行度偏差, 推荐系统, 因果干预, 演化偏好, 解纠缠训练

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了一种名为CausalEPP的新方法，用于解决推荐系统中的流行度偏差问题，通过考虑用户对流行项目的偏好随时间演变的特点，显著提升了推荐准确性。

Motivation: 现有方法在缓解推荐系统的流行度偏差时，往往忽视了用户对流行项目偏好的差异性和时间演变特性，导致推荐效果不理想。

Method: 论文首先提出了一种名为“Evolving Personal Popularity”的度量方法，量化用户对流行项目的偏好；随后设计了一个因果图，并将演化个人流行度融入从众效应中，通过解纠缠训练来缓解偏差。

Result: 实验表明，CausalEPP在减少流行度偏差的同时，显著提高了推荐准确性，优于基线方法。

Conclusion: CausalEPP通过动态建模用户对流行项目的偏好变化，有效解决了推荐系统中的流行度偏差问题，为个性化推荐提供了新的思路。

Abstract: Popularity bias occurs when popular items are recommended far more frequently
than they should be, negatively impacting both user experience and
recommendation accuracy. Existing debiasing methods mitigate popularity bias
often uniformly across all users and only partially consider the time evolution
of users or items. However, users have different levels of preference for item
popularity, and this preference is evolving over time. To address these issues,
we propose a novel method called CausalEPP (Causal Intervention on Evolving
Personal Popularity) for taming recommendation bias, which accounts for the
evolving personal popularity of users. Specifically, we first introduce a
metric called {Evolving Personal Popularity} to quantify each user's preference
for popular items. Then, we design a causal graph that integrates evolving
personal popularity into the conformity effect, and apply deconfounded training
to mitigate the popularity bias of the causal graph. During inference, we
consider the evolution consistency between users and items to achieve a better
recommendation. Empirical studies demonstrate that CausalEPP outperforms
baseline methods in reducing popularity bias while improving recommendation
accuracy.

</details>


### [937] [An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents](https://arxiv.org/abs/2505.13504)
*Ayesha Amjad,Saurav Sthapit,Tahir Qasim Syed*

Key words: LLM, 强化学习, 多代理系统, 表单提取, 自动化

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 提出了一种基于LLM和强化学习的多代理系统，用于自动提取表单文档中的信息，具有自我改进能力。

Motivation: 现有表单数据提取方法依赖OCR和固定流程，改进空间有限，需提升系统适应性和准确性。

Method: 采用多代理框架，结合特定任务提示和强化学习策略，通过元提示代理优化其他代理的表现。

Result: 在SOIRE和CORD基准数据集上表现优异，验证了框架的有效性。

Conclusion: 提出的多代理系统能自动化、自适应地提取信息，无需人工干预。

Abstract: Extracting alphanumeric data from form-like documents such as invoices,
purchase orders, bills, and financial documents is often performed via vision
(OCR) and learning algorithms or monolithic pipelines with limited potential
for systemic improvements. We propose an agentic AI system that leverages Large
Language Model (LLM) agents and a reinforcement learning (RL) driver agent to
automate consistent, self-improving extraction under LLM inference uncertainty.
Our work highlights the limitations of monolithic LLM-based extraction and
introduces a modular, multi-agent framework with task-specific prompts and an
RL policy of rewards and penalties to guide a meta-prompting agent to learn
from past errors and improve prompt-based actor agents. This self-corrective
adaptive system handles diverse documents, file formats, layouts, and LLMs,
aiming to automate accurate information extraction without the need for human
intervention. Results as reported on two benchmark datasets of SOIRE, and CORD,
are promising for the agentic AI framework.

</details>


### [938] [Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering](https://arxiv.org/abs/2505.13520)
*Hessa Alawwad,Usman Naseem,Areej Alhothali,Ali Alkhathlan,Amani Jamal*

Key words: 教科书问答，多模态学习，语义表示，多目标联合训练，JETRTQA

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种新方法JETRTQA，通过多目标联合训练增强语义表示，提升教科书问答任务中的文档检索相关性。

Motivation: 解决教科书问答任务中现有的方法在语义对齐和任务特定文档检索上的不足。

Method: 提出JETRTQA模型，采用检索-生成架构，通过多模态大语言模型生成答案，并结合成对排序和监督信号优化语义表示。

Result: 在CK12-QA数据集上，JETRTQA显著提升了对信息相关文档的区分能力，验证集准确率提升2.4%，测试集提升11.1%。

Conclusion: JETRTQA通过改进语义表示和文档检索，为复杂教育场景中的问答任务提供了更优解。

Abstract: Textbook question answering (TQA) is a complex task, requiring the
interpretation of complex multimodal context. Although recent advances have
improved overall performance, they often encounter difficulties in educational
settings where accurate semantic alignment and task-specific document retrieval
are essential. In this paper, we propose a novel approach to multimodal
textbook question answering by introducing a mechanism for enhancing semantic
representations through multi-objective joint training. Our model, Joint
Embedding Training With Ranking Supervision for Textbook Question Answering
(JETRTQA), is a multimodal learning framework built on a retriever--generator
architecture that uses a retrieval-augmented generation setup, in which a
multimodal large language model generates answers. JETRTQA is designed to
improve the relevance of retrieved documents in complex educational contexts.
Unlike traditional direct scoring approaches, JETRTQA learns to refine the
semantic representations of questions and documents through a supervised signal
that combines pairwise ranking and implicit supervision derived from answers.
We evaluate our method on the CK12-QA dataset and demonstrate that it
significantly improves the discrimination between informative and irrelevant
documents, even when they are long, complex, and multimodal. JETRTQA
outperforms the previous state of the art, achieving a 2.4\% gain in accuracy
on the validation set and 11.1\% on the test set.

</details>


### [939] [Geography-Aware Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2505.13526)
*Zhao Liu,Wei Liu,Huajie Zhu,Jianxing Yu,Jian Yin,Wang-Chien Lee,Shun Wang*

Key words: POI推荐, 大语言模型, 地理感知, 位置编码, POI转换

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为GA-LLM的地理感知大语言模型框架，旨在解决大语言模型在POI推荐任务中难以建模精确空间上下文和POI转换关系的挑战，并通过实验验证了其性能。

Motivation: POI推荐任务需要有效建模地理信息和POI转换关系，而大语言模型在这一任务中存在空间上下文建模不足和缺乏POI转换知识的问题。

Method: GA-LLM框架包含两个组件：地理坐标注入模块（GCIM）通过分层和基于傅里叶的位置编码将GPS坐标转换为空间表示；POI对齐模块（PAM）将POI转换关系融入大语言模型的语义空间。

Result: 在三个真实数据集上的实验表明，GA-LLM达到了最先进的性能。

Conclusion: GA-LLM通过增强大语言模型对地理信息的理解和POI关系的捕捉能力，有效地提升了POI推荐的准确性。

Abstract: The next Point-of-Interest (POI) recommendation task aims to predict users'
next destinations based on their historical movement data and plays a key role
in location-based services and personalized applications. Accurate next POI
recommendation depends on effectively modeling geographic information and POI
transition relations, which are crucial for capturing spatial dependencies and
user movement patterns. While Large Language Models (LLMs) exhibit strong
capabilities in semantic understanding and contextual reasoning, applying them
to spatial tasks like next POI recommendation remains challenging. First, the
infrequent nature of specific GPS coordinates makes it difficult for LLMs to
model precise spatial contexts. Second, the lack of knowledge about POI
transitions limits their ability to capture potential POI-POI relationships. To
address these issues, we propose GA-LLM (Geography-Aware Large Language Model),
a novel framework that enhances LLMs with two specialized components. The
Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into
spatial representations using hierarchical and Fourier-based positional
encoding, enabling the model to understand geographic features from multiple
perspectives. The POI Alignment Module (PAM) incorporates POI transition
relations into the LLM's semantic space, allowing it to infer global POI
relationships and generalize to unseen POIs. Experiments on three real-world
datasets demonstrate the state-of-the-art performance of GA-LLM.

</details>


### [940] [LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems](https://arxiv.org/abs/2505.13528)
*Shengkang Gu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Ning Gu,Li Shang,Tun Lu*

Key words: recommender systems, shilling attacks, LLM, Agent4SR

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了Agent4SR框架，利用基于大语言模型（LLM）的代理进行低知识、高影响力的shilling攻击，通过生成评分和文本评论来操纵推荐系统。

Motivation: 推荐系统（RS）易受shilling攻击，传统攻击方法简单且忽视文本评论的潜在操纵力。

Method: 设计了Agent4SR框架，模拟真实用户行为，包括目标配置文件构建、混合记忆检索和跨评论传播目标特征。

Result: 在多个数据集和RS架构上实验，证明Agent4SR在效果和隐蔽性上优于现有低知识基线。

Conclusion: LLM驱动的代理带来新威胁，现代推荐系统需加强防御。

Abstract: Recommender systems (RS) are increasingly vulnerable to shilling attacks,
where adversaries inject fake user profiles to manipulate system outputs.
Traditional attack strategies often rely on simplistic heuristics, require
access to internal RS data, and overlook the manipulation potential of textual
reviews. In this work, we introduce Agent4SR, a novel framework that leverages
Large Language Model (LLM)-based agents to perform low-knowledge, high-impact
shilling attacks through both rating and review generation. Agent4SR simulates
realistic user behavior by orchestrating adversarial interactions, selecting
items, assigning ratings, and crafting reviews, while maintaining behavioral
plausibility. Our design includes targeted profile construction, hybrid memory
retrieval, and a review attack strategy that propagates target item features
across unrelated reviews to amplify manipulation. Extensive experiments on
multiple datasets and RS architectures demonstrate that Agent4SR outperforms
existing low-knowledge baselines in both effectiveness and stealth. Our
findings reveal a new class of emergent threats posed by LLM-driven agents,
underscoring the urgent need for enhanced defenses in modern recommender
systems.

</details>


### [941] [Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments](https://arxiv.org/abs/2505.13535)
*Aniket Bhattacharyya,Anurag Tripathi,Ujjal Das,Archan Karmakar,Amit Pathak,Maneesh Gupta*

Key words: 信息提取,视觉丰富文档,语义块,推理

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: BLOCKIE利用语义块改进视觉丰富文档的信息提取，优于现有方法。

Motivation: 解决传统NLP方法缺乏推理能力和LLMs对布局理解不足的问题。

Method: 提出BLOCKIE，将文档组织为可重用的语义块独立处理。

Result: 在公开VRD基准上F1分数提升1-3%，对新格式更具弹性。

Conclusion: BLOCKIE展示了在推理和泛化能力上的优势。

Abstract: Information extraction (IE) from Visually Rich Documents (VRDs) containing
layout features along with text is a critical and well-studied task.
Specialized non-LLM NLP-based solutions typically involve training models using
both textual and geometric information to label sequences/tokens as named
entities or answers to specific questions. However, these approaches lack
reasoning, are not able to infer values not explicitly present in documents,
and do not generalize well to new formats. Generative LLM-based approaches
proposed recently are capable of reasoning, but struggle to comprehend clues
from document layout especially in previously unseen document formats, and do
not show competitive performance in heterogeneous VRD benchmark datasets. In
this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs
into localized, reusable semantic textual segments called $\textit{semantic
blocks}$, which are processed independently. Through focused and more
generalizable reasoning,our approach outperforms the state-of-the-art on public
VRD benchmarks by 1-3% in F1 scores, is resilient to document formats
previously not encountered and shows abilities to correctly extract information
not explicitly present in documents.

</details>


### [942] [RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines](https://arxiv.org/abs/2505.13538)
*Dvir Cohen,Lin Burg,Gilad Barkan*

Key words: 检索增强生成, RAG, 评估框架, 可解释性, LLM

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: RAGXplain是一个新的评估框架，旨在通过提供清晰的见解和可操作的建议，改进传统的RAG系统评估方法。

Motivation: 传统的RAG评估方法主要提供定量分数，但缺乏对复杂管线的深入理解和改进建议。

Method: RAGXplain利用LLM推理将原始分数转化为解释性能差距和优化建议的连贯叙述。

Result: 实验表明，RAGXplain的建议显著提升了系统性能，并与人类判断高度一致。

Conclusion: RAGXplain在定量评估与实际优化之间架起桥梁，帮助用户理解、信任和增强AI系统。

Abstract: Retrieval-Augmented Generation (RAG) systems show promise by coupling large
language models with external knowledge, yet traditional RAG evaluation methods
primarily report quantitative scores while offering limited actionable guidance
for refining these complex pipelines. In this paper, we introduce RAGXplain, an
evaluation framework that quantifies RAG performance and translates these
assessments into clear insights that clarify the workings of its complex,
multi-stage pipeline and offer actionable recommendations. Using LLM reasoning,
RAGXplain converts raw scores into coherent narratives identifying performance
gaps and suggesting targeted improvements. By providing transparent
explanations for AI decision-making, our framework fosters user trust-a key
challenge in AI adoption. Our LLM-based metric assessments show strong
alignment with human judgments, and experiments on public question-answering
datasets confirm that applying RAGXplain's actionable recommendations
measurably improves system performance. RAGXplain thus bridges quantitative
evaluation and practical optimization, empowering users to understand, trust,
and enhance their AI systems.

</details>


### [943] [Know Or Not: a library for evaluating out-of-knowledge base robustness](https://arxiv.org/abs/2505.13545)
*Jessica Foo,Pradyumna Shyama Prasad,Shaun Khoo*

Key words: 大型语言模型, 检索增强生成, OOKB鲁棒性, 开源工具, 评估方法

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了一种评估大型语言模型在检索增强生成设置中处理超出知识库问题的方法，并开发了开源工具knowornot以支持自定义评估。

Motivation: 在高风险应用中，LLMs的幻觉问题限制了其应用。现有检索增强生成方法仍无法完全避免模型回答知识库外问题，因此需要系统评估模型的OOKB鲁棒性。

Method: 开发了knowornot工具，提供统一API、模块化架构、严格数据建模和自定义工具，支持用户创建评估数据与流程以测试模型的OOKB鲁棒性。

Result: 通过PolicyBench基准测试，展示了knowornot在实际政府政策问答场景中的有效性。工具开源，支持广泛使用与扩展。

Conclusion: knowornot为评估LLMs在RAG设置中的OOKB鲁棒性提供了实用且灵活的解决方案，填补了现有评估方法的空白。

Abstract: While the capabilities of large language models (LLMs) have progressed
significantly, their use in high-stakes applications have been limited due to
risks of hallucination. One key approach in reducing hallucination is
retrieval-augmented generation (RAG), but even in such setups, LLMs may still
hallucinate when presented with questions outside of the knowledge base. Such
behavior is unacceptable in high-stake applications where LLMs are expected to
abstain from answering queries it does not have sufficient context on. In this
work, we present a novel methodology for systematically evaluating
out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not
know) in the RAG setting, without the need for manual annotation of gold
standard answers. We implement our methodology in knowornot, an open-source
library that enables users to develop their own customized evaluation data and
pipelines for OOKB robustness. knowornot comprises four main features. Firstly,
it provides a unified, high-level API that streamlines the process of setting
up and running robustness benchmarks. Secondly, its modular architecture
emphasizes extensibility and flexibility, allowing users to easily integrate
their own LLM clients and RAG settings. Thirdly, its rigorous data modeling
design ensures experiment reproducibility, reliability and traceability.
Lastly, it implements a comprehensive suite of tools for users to customize
their pipelines. We demonstrate the utility of knowornot by developing a
challenging benchmark, PolicyBench, which spans four Question-Answer (QA)
chatbots on government policies, and analyze its OOKB robustness. The source
code of knowornot is available
https://github.com/govtech-responsibleai/KnowOrNot.

</details>


### [944] [JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation](https://arxiv.org/abs/2505.13550)
*Ke Yang,Kevin Ros,Shankar Kumar Senthil Kumar,ChengXiang Zhai*

Key words: 即时信息推荐,基准数据集,评估框架,多模态,基础模型

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文首次定义了即时信息推荐（JIR）任务及其评估指标，并提出了多模态基准数据集JIR-Arena，用于系统评测JIR系统。

Motivation: 当前缺乏对JIR任务的系统性定义和评估框架，研究旨在填补这一空白。

Method: 提出数学定义和评估指标，构建多模态基准数据集JIR-Arena，并通过多人类和AI模型输入近似信息需求分布。

Result: 基础模型能合理模拟用户需求，但在召回率和内容检索上仍有挑战。

Conclusion: JIR-Arena为JIR研究提供了标准化评测工具，推动了该领域的发展。

Abstract: Just-in-time Information Recommendation (JIR) is a service designed to
deliver the most relevant information precisely when users need it, ,
addressing their knowledge gaps with minimal effort and boosting
decision-making and efficiency in daily life. Advances in device-efficient
deployment of foundation models and the growing use of intelligent wearable
devices have made always-on JIR assistants feasible. However, there has been no
systematic effort to formally define JIR tasks or establish evaluation
frameworks. To bridge this gap, we present the first mathematical definition of
JIR tasks and associated evaluation metrics. Additionally, we introduce
JIR-Arena, a multimodal benchmark dataset featuring diverse,
information-request-intensive scenarios to evaluate JIR systems across critical
dimensions: i) accurately inferring user information needs, ii) delivering
timely and relevant recommendations, and iii) avoiding irrelevant content that
may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in
estimating user information needs and uncontrollable system variables affecting
reproducibility. To address these, JIR-Arena: i) combines input from multiple
humans and large AI models to approximate information need distributions; ii)
assesses JIR quality through information retrieval outcomes using static
knowledge base snapshots; and iii) employs a multi-turn, multi-entity
validation framework to improve objectivity and generality. Furthermore, we
implement a baseline JIR system capable of processing real-time information
streams aligned with user inputs. Our evaluation of this baseline system on
JIR-Arena indicates that while foundation model-based JIR systems simulate user
needs with reasonable precision, they face challenges in recall and effective
content retrieval. To support future research in this new area, we fully
release our code and data.

</details>


### [945] [AMAQA: A Metadata-based QA Dataset for RAG Systems](https://arxiv.org/abs/2505.13557)
*Davide Bruni,Marco Avvenuti,Nicola Tonellotto,Maurizio Tesconi*

Key words: 检索增强生成,问答任务,元数据,网络安全

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: AMAQA是一个新的开放问答数据集，专注于结合文本和元数据的任务评估，特别适用于需快速分析大量数据的领域。

Motivation: 当前基准测试缺乏元数据整合，限制了结合文本和外部信息场景的评估。AMAQA填补了这一空白。

Method: 构建包含110万条消息和450对问答的数据集，整合了时间戳、主题等多维元数据。

Result: 元数据的使用将准确率从0.12提升至0.61，进一步优化策略带来额外提升。

Conclusion: AMAQA为元数据驱动的问答系统研究设立了新标准，展示了结构化上下文的价值。

Abstract: Retrieval-augmented generation (RAG) systems are widely used in
question-answering (QA) tasks, but current benchmarks lack metadata
integration, hindering evaluation in scenarios requiring both textual data and
external information. To address this, we present AMAQA, a new open-access QA
dataset designed to evaluate tasks combining text and metadata. The integration
of metadata is especially important in fields that require rapid analysis of
large volumes of data, such as cybersecurity and intelligence, where timely
access to relevant information is critical. AMAQA includes about 1.1 million
English messages collected from 26 public Telegram groups, enriched with
metadata such as timestamps, topics, emotional tones, and toxicity indicators,
which enable precise and contextualized queries by filtering documents based on
specific criteria. It also includes 450 high-quality QA pairs, making it a
valuable resource for advancing research on metadata-driven QA and RAG systems.
To the best of our knowledge, AMAQA is the first single-hop QA benchmark to
incorporate metadata and labels such as topics covered in the messages. We
conduct extensive tests on the benchmark, establishing a new standard for
future research. We show that leveraging metadata boosts accuracy from 0.12 to
0.61, highlighting the value of structured context. Building on this, we
explore several strategies to refine the LLM input by iterating over provided
context and enriching it with noisy documents, achieving a further 3-point gain
over the best baseline and a 14-point improvement over simple metadata
filtering. The dataset is available at
https://anonymous.4open.science/r/AMAQA-5D0D/

</details>


### [946] [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
*Anand Selvadurai,Jasheen Shaik,Girish Chandrasekar,ShriRadhaKrishnan Balamurugan,Eswara Reddy*

Key words: 嵌入模型,医学NLP,ALiBi,长文本处理,MTEB基准

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: MedEIR提出了一种针对医学和通用NLP任务的嵌入模型和分词器，支持长文本处理，效果优于现有模型。

Motivation: 现有嵌入模型在医学文档、长文本处理和多领域任务中存在局限性，需要更通用的解决方案。

Method: 联合优化嵌入模型和分词器，基于ALiBi技术支持8192个令牌的长文本，仅需60亿令牌预训练和300万句对微调。

Result: 在多个基准测试中超越Jina V2和MiniLM，如ArguAna（55.24）、NFCorpus（38.44）、MedicalQARetrieval（74.25）等。

Conclusion: MedEIR表现优异，适用于通用和专业领域任务，是高效的嵌入模型解决方案。

Abstract: Embedding models have become essential for retrieval-augmented generation
(RAG) tasks, semantic clustering, and text re-ranking. But despite their
growing use, many of these come with notable limitations. For example, Jina
fails to capture the semantic content of medical documents, while models such
as MiniLM often perform poorly on long-form documents. Domain-adapted models,
while specialized, often underperform in general-purpose tasks, reducing their
overall applicability. General-domain tokenizers often misinterpret medical
vocabulary. The limitations of current embedding models, whether in
tokenization accuracy, domain comprehension, or handling long sequences,
highlight the need for more versatile solutions. In this work, we present
MedEIR, a novel embedding model and tokenizer jointly optimized for both
medical and general NLP tasks, incorporating ALiBi-based long-context
processing to support sequences of up to 8,192 tokens. MedEIR was pre-trained
on only 6 billion tokens, significantly fewer than Jina's, followed by
fine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina
V2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),
NFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID
(79.56). These results highlight the potential of MedEIR as a highly effective
embedding model, demonstrating strong performance across both general-purpose
and domain-specific tasks and outperforming existing models on multiple
benchmarks.

</details>


### [947] [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
*Tommaso Mario Buonocore,Enea Parimbelli*

Key words: 内容审核、检索增强生成（RAG）、大型语言模型（LLMs）、动态拒绝、实时定制

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 提出了一种基于检索增强生成（RAG）的新方法RAR，动态拒绝不安全的用户查询，无需重新训练模型。

Motivation: 大型语言模型（LLMs）的内容审核需要灵活且适应性强的解决方案以应对新兴威胁。

Method: 通过在向量数据库中插入并标记恶意文档，利用RAG架构动态识别和拒绝有害请求。

Result: RAR在性能上与Claude 3.5 Sonnet等模型的嵌入式审核相当，同时提供更高的灵活性和实时定制能力。

Conclusion: RAR无需改变现有RAG架构，仅需添加特定文档和基于检索结果的简单拒绝机制。

Abstract: Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.

</details>


### [948] [Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning](https://arxiv.org/abs/2412.12504)
*Hong Liu,Saisai Gong,Yixin Ji,Kaixin Wu,Jia Xu,Jinjie Gu*

Key words: 大型语言模型,相关性建模,数据分布偏移,Alipay搜索,DaRL

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文提出了一种名为DaRL的新型框架，通过增强预训练大型语言模型（LLMs）在相关性建模中的区分能力和泛化能力，解决了传统方法中细粒度判断不足和数据分布偏移的问题。

Motivation: 传统LLMs在相关性建模中存在两个主要问题：无法有效区分细粒度的相关程度，以及在数据分布偏移时性能下降。

Method: 提出DaRL框架，设计了改进的损失函数增强判别能力，并引入DASA模块主动选择未被覆盖的样本进行微调，采用多阶段微调策略同时优化ID和OOD性能。

Result: DaRL在Alipay搜索中成功部署，提升了保险产品搜索的相关性建模性能。

Conclusion: DaRL框架有效提升了LLMs在细粒度相关性建模和分布偏移场景中的表现，具备实际应用价值。

Abstract: With the rapid advancement of pre-trained large language models (LLMs),
recent endeavors have leveraged the capabilities of LLMs in relevance modeling,
resulting in enhanced performance. This is usually done through the process of
fine-tuning LLMs on specifically annotated datasets to determine the relevance
between queries and items. However, there are two limitations when LLMs are
naively employed for relevance modeling through fine-tuning and inference.
First, it is not inherently efficient for performing nuanced tasks beyond
simple yes or no answers, such as assessing search relevance. It may therefore
tend to be overconfident and struggle to distinguish fine-grained degrees of
relevance (e.g., strong relevance, weak relevance, irrelevance) used in search
engines. Second, it exhibits significant performance degradation when
confronted with data distribution shift in real-world scenarios. In this paper,
we propose a novel Distribution-Aware Robust Learning framework (DaRL) for
relevance modeling in Alipay Search. Specifically, we design an effective loss
function to enhance the discriminability of LLM-based relevance modeling across
various fine-grained degrees of query-item relevance. To improve the
generalizability of LLM-based relevance modeling, we first propose the
Distribution-Aware Sample Augmentation (DASA) module. This module utilizes
out-of-distribution (OOD) detection techniques to actively select appropriate
samples that are not well covered by the original training set for model
fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to
simultaneously improve in-distribution (ID) and OOD performance, bridging the
performance gap between them. DaRL has been deployed online to serve the
Alipay's insurance product search...

</details>


### [949] [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
*Runchu Tian,Xueqiang Xu,Bowen Jin,SeongKu Kang,Jiawei Han*

Key words: 科学检索, 文档重排, 大语言模型, CoRank

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为CoRank的无训练、模型无关的科学检索重排框架，通过紧凑的文档表示和混合设计提升重排性能。

Motivation: 解决科学领域中基于LLM的列表重排面临的挑战，包括初始检索结果不理想和相关文档被排除的问题。

Method: 提出CoRank框架，包括离线提取文档特征、基于紧凑表示的粗排和基于全文的细粒度重排。

Result: 实验显示CoRank显著提升了重排性能，nDCG@10从32.0提高到39.7。

Conclusion: 信息提取对科学检索中的重排具有重要价值。

Abstract: Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.

</details>


### [950] [Field Matters: A lightweight LLM-enhanced Method for CTR Prediction](https://arxiv.org/abs/2505.14057)
*Yu Cui,Feng Liu,Jiawei Chen,Xingyu Lou,Changwang Zhang,Jun Wang,Yuegang Sun,Xiaohu Yang,Can Wang*

Key words: CTR预测, 大语言模型, 字段级增强, 轻量级, 自监督学习

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种轻量级LLM增强的CTR预测方法LLaCTR，通过字段级增强范式减少计算开销，并在实验中验证了其优越性能。

Motivation: 现有LLM增强的CTR方法因处理大规模文本描述导致计算开销大，希望设计一种轻量级方法解决这一问题。

Method: LLaCTR利用LLM从小规模特征字段中提取轻量级语义知识，并通过自监督字段特征微调增强特征表示与交互。

Result: 在四个数据集上集成六种CTR模型，LLaCTR在效果和效率上均优于现有方法。

Conclusion: LLaCTR是一种高效的轻量级LLM增强CTR方法，显著提升了性能并减少了计算负担。

Abstract: Click-through rate (CTR) prediction is a fundamental task in modern
recommender systems. In recent years, the integration of large language models
(LLMs) has been shown to effectively enhance the performance of traditional CTR
methods. However, existing LLM-enhanced methods often require extensive
processing of detailed textual descriptions for large-scale instances or
user/item entities, leading to substantial computational overhead. To address
this challenge, this work introduces LLaCTR, a novel and lightweight
LLM-enhanced CTR method that employs a field-level enhancement paradigm.
Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight
semantic knowledge from small-scale feature fields through self-supervised
field-feature fine-tuning. Subsequently, it leverages this field-level semantic
knowledge to enhance both feature representation and feature interactions. In
our experiments, we integrate LLaCTR with six representative CTR models across
four datasets, demonstrating its superior performance in terms of both
effectiveness and efficiency compared to existing LLM-enhanced methods. Our
code is available at https://anonymous.4open.science/r/LLaCTR-EC46.

</details>


### [951] [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
*Eugene Yang,Andrew Yates,Kathryn Ricci,Orion Weller,Vivek Chari,Benjamin Van Durme,Dawn Lawrie*

Key words: Rank-K, 列表式重排序, 推理语言模型, 多语言检索

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: Rank-K是一种基于推理语言模型的列表式重排序模型，显著提升检索效果并在多语言任务中表现优异。

Motivation: 为了解决神经重排序模型资源消耗大的问题，同时利用语言模型的推理能力提升检索效率。

Method: 提出Rank-K模型，利用推理语言模型在查询时进行列表式重排序，支持硬查询的可扩展性。

Result: Rank-K在BM25初始排名列表上提升23%检索效果，在SPLADE-v3结果上提升19%，且在多语言任务中表现一致。

Conclusion: Rank-K通过高效利用语言模型的推理能力，显著提升了检索效果并具有多语言适应性。

Abstract: Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.

</details>


### [952] [TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems](https://arxiv.org/abs/2505.13881)
*Jiahao Yu,Haozhuang Liu,Yeqiu Yang,Lu Chen,Wu Jian,Yuning Jiang,Bo Zheng*

Key words: 推荐系统,回归模型,再转换偏差,TranSUN,GTS

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种名为TranSUN的新方法，通过模型内在的细微调整消除推荐系统中回归模型的再转换偏差问题，并扩展为通用回归模型家族GTS。

Motivation: 尽管再转换偏差问题在其他领域已有研究，但在推荐系统领域被忽视。现有方法多为外部后处理，难以实际应用。

Method: 提出TranSUN方法，通过联合偏差学习实现理论无偏性，并扩展为GTS模型家族，提供灵活的无偏模型开发框架。

Result: 实验证明方法在多领域数据中表现优越，并已成功应用于淘宝App的首页推荐场景。

Conclusion: TranSUN和GTS为推荐系统提供了有效的无偏回归解决方案，具有理论和实践价值。

Abstract: Regression models are crucial in recommender systems. However,
retransformation bias problem has been conspicuously neglected within the
community. While many works in other fields have devised effective bias
correction methods, all of them are post-hoc cures externally to the model,
facing practical challenges when applied to real-world recommender systems.
Hence, we propose a preemptive paradigm to eradicate the bias intrinsically
from the models via minor model refinement. Specifically, a novel TranSUN
method is proposed with a joint bias learning manner to offer theoretically
guaranteed unbiasedness under empirical superior convergence. It is further
generalized into a novel generic regression model family, termed Generalized
TranSUN (GTS), which not only offers more theoretical insights but also serves
as a generic framework for flexibly developing various bias-free models.
Comprehensive experimental results demonstrate the superiority of our methods
across data from various domains, which have been successfully deployed in two
real-world industrial recommendation scenarios, i.e. product and short video
recommendation scenarios in Guess What You Like business domain in the homepage
of Taobao App (a leading e-commerce platform), to serve the major online
traffic. Codes will be released after this paper is published.

</details>


### [953] [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
*Sunhao Dai,Wenjie Wang,Liang Pang,Jun Xu,See-Kiong Ng,Ji-Rong Wen,Tat-Seng Chua*

Key words: 生成式AI, 信息检索, 用户反馈, 搜索优化, 在线适应, 离线更新

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 生成式AI搜索通过提供端到端答案降低了用户手动浏览和总结的需求，但缺乏精细的用户反馈机制。NExT-Search提出通过两种模式（用户调试模式和影子用户模式）重新引入过程级反馈。

Motivation: 传统Web搜索通过用户反馈（如点击、停留时间）持续优化排名模型，而生成式AI搜索的反馈仅针对最终答案，难以改进中间环节。NExT-Search旨在恢复精细反馈机制。

Method: NExT-Search引入用户调试模式和影子用户模式，分别通过用户干预和AI模拟提供过程级反馈，并结合在线适应和离线更新优化搜索模型。

Result: NExT-Search为生成式AI搜索提供了精细反馈的新范式，能够实时优化搜索输出并定期改进模型。

Conclusion: NExT-Search通过恢复人类对关键环节的控制，为反馈丰富的AI搜索系统提供了可行的演进方向。

Abstract: Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.

</details>


### [954] [Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity](https://arxiv.org/abs/2505.14310)
*Shiyin Tan,Dongyuan Li,Renhe Jiang,Zhen Wang,Xingtong Yu,Manabu Okumura*

Key words: 推荐系统, 流行度偏差, 因果干预, 演化偏好, CausalEPP

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: TL;DR: CausalEPP方法通过量化用户对热门项目的偏好并设计因果图，有效减少推荐系统中的流行度偏差，提升推荐准确性。

Motivation: 流行度偏差（推荐系统过于偏爱热门项目）损害用户体验和推荐准确性。现有方法未能充分考虑用户对热门项目的个性化偏好及其随时间的变化。

Method: 提出CausalEPP方法，引入Evolving Personal Popularity指标量化用户偏好，设计因果图并结合去混淆训练，推理时考虑用户与项目的演化一致性。

Result: 实验表明，CausalEPP在减少流行度偏差的同时提高了推荐准确性。

Conclusion: CausalEPP有效解决了用户偏好随时间变化的流行度偏差问题，优于现有方法。

Abstract: Popularity bias occurs when popular items are recommended far more frequently
than they should be, negatively impacting both user experience and
recommendation accuracy. Existing debiasing methods mitigate popularity bias
often uniformly across all users and only partially consider the time evolution
of users or items. However, users have different levels of preference for item
popularity, and this preference is evolving over time. To address these issues,
we propose a novel method called CausalEPP (Causal Intervention on Evolving
Personal Popularity) for taming recommendation bias, which accounts for the
evolving personal popularity of users. Specifically, we first introduce a
metric called {Evolving Personal Popularity} to quantify each user's preference
for popular items. Then, we design a causal graph that integrates evolving
personal popularity into the conformity effect, and apply deconfounded training
to mitigate the popularity bias of the causal graph. During inference, we
consider the evolution consistency between users and items to achieve a better
recommendation. Empirical studies demonstrate that CausalEPP outperforms
baseline methods in reducing popularity bias while improving recommendation
accuracy.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [955] [HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems](https://arxiv.org/abs/2505.13516)
*Zhipeng Hou,Junyi Tang,Yipeng Wang*

Key words: 多智能体系统,大型语言模型,分层推理,蒙特卡洛树搜索,自适应提示优化

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 介绍了HALO，一种基于分层推理架构的多智能体协作框架，解决了现有智能体系统在复杂环境中适应性不足的问题，并在多项任务上表现优异。

Motivation: 现有智能体系统因预定义的代理角色和静态通信结构在复杂环境中适应性和灵活性不足，影响了专业和专家级任务的性能。

Method: 采用分层推理架构，包括高层规划代理、中层角色设计代理和底层推理代理，并利用蒙特卡洛树搜索和自适应提示优化模块。

Result: 在代码生成、通用推理和算术推理任务上平均提升14.4%，在特定子任务上表现尤为突出。

Conclusion: HALO框架显著提升了智能体系统在复杂和专业任务中的性能。

Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.
Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability
as well as flexibility in complex interaction environments and leading to
subpar performance on highly specialized and expert-level tasks. To address
these issues, we introduce HALO, a multi-agent collaboration framework based on
a hierarchical reasoning architecture. Specifically, we incorporate a
high-level planning agent for task decomposition, mid-level role-design agents
for subtask-specific agent instantiation, and low-level inference agents for
subtask execution. Particularly, subtask execution is reformulated as a
structured workflow search problem, where Monte Carlo Tree Search (MCTS)
systematically explores the agentic action space to construct optimal reasoning
trajectories. Additionally, as the majority of users lack expertise in prompt
engineering, we leverage an Adaptive Prompt Refinement module to transform raw
queries into task-specific prompts. Empirical evaluations on Code Generation
(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)
benchmark datasets highlight the effectiveness of HALO, yielding a 14.4%
average improvement over state-of-the-art baselines. Notably, HALO achieves up
to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark
and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,
indicating its advanced proficiency in tackling highly specialized and
expert-level tasks. The code repository is available at
https://github.com/23japhone/HALO.

</details>


### [956] [ACPs: Agent Collaboration Protocols for the Internet of Agents](https://arxiv.org/abs/2505.13523)
*Jun Liu,Ke Yu,Keliang Chen,Ke Li,Yuxinyue Qian,Xiaolian Guo,Haozhe Song,Yinming Li*

Key words: 智能体协作协议, IoA, 异构智能体, 标准化通信

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 论文提出Agent Collaboration Protocols (ACPs)，用于解决异构智能体间的互操作性、扩展性和协调性问题，通过标准化协议支持无缝协作。

Motivation: 现有智能体通信协议（如MCP、A2A、ANP）分散且场景特定，无法满足智能体互联需求。

Method: 设计ACPs协议套件，包括注册、发现、交互和工具协议，支持可信访问、能力编排和工作流构建。

Result: 在餐厅预订场景中验证了ACPs的有效性。

Conclusion: ACPs为构建安全、开放、可扩展的智能体互联网基础设施奠定了基础。

Abstract: With the rapid advancement of artificial intelligence, the proliferation of
autonomous agents has introduced new challenges in interoperability,
scalability, and coordination. The Internet of Agents (IoA) aims to
interconnect heterogeneous agents through standardized communication protocols,
enabling seamless collaboration and intelligent task execution. However,
existing agent communication protocols such as MCP, A2A, and ANP remain
fragmented and scenario-specific. To address this gap, we propose Agent
Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.
ACPs include registration, discovery, interaction, and tooling protocols to
support trustable access, capability orchestration, and workflow construction.
We present the architecture, key technologies, and application workflows of
ACPs, and demonstrate its effectiveness in a collaborative restaurant booking
scenario. ACPs lay the foundation for building a secure, open, and scalable
agent internet infrastructure.

</details>


### [957] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
*Haoyang Fang,Boran Han,Nick Erickson,Xiyuan Zhang,Su Zhou,Anirudh Dagar,Jiani Zhang,Ali Caner Turkmen,Cuixiong Hu,Huzefa Rangwala,Ying Nian Wu,Bernie Wang,George Karypis*

Key words: AutoML, 多模态数据, 大语言模型, 多代理框架, 端到端自动化

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: MLZero是一个基于大语言模型的多代理框架，通过端到端自动化处理多模态数据，显著减少人工干预，并在性能上优于现有方法。

Motivation: 现有的AutoML系统仍需大量人工配置和专家输入，尤其是在处理多模态数据时。MLZero旨在通过多代理框架和LLM实现更高效、自动化的机器学习。

Method: MLZero采用认知感知模块将多模态输入转换为感知上下文，并通过语义和情景记忆增强代码生成，解决了LLM的幻觉代码和过时API问题。

Result: MLZero在MLE-Bench Lite上表现最佳，获得六枚金牌；在Multimodal AutoML Agent Benchmark上成功率达92%，排名2.28，显著优于竞争对手。

Conclusion: MLZero展示了在多模态数据自动化处理中的强大能力，即使使用小型LLM也能优于现有方案。

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


### [958] [Personalized and Resilient Distributed Learning Through Opinion Dynamics](https://arxiv.org/abs/2505.14081)
*Luca Ballotta,Nicola Bastianello,Riccardo M. G. Ferrari,Karl H. Johansson*

Key words: 分布式学习, 个性化, 弹性, Friedkin-Johnsen模型, 分布式梯度下降

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 这篇论文提出了一种分布式学习算法，结合分布式梯度下降和Friedkin-Johnsen意见动态模型，以解决多代理网络系统中的个性化和弹性需求。

Motivation: 解决多代理网络中代理需要个性化学习本地模型以及学习过程需要抵御网络攻击或异常数据的双重挑战。

Method: 提出了一种结合分布式梯度下降和Friedkin-Johnsen模型的分布式学习算法。

Result: 通过数值实验验证了算法的有效性，能够在恶意代理存在的情况下实现高全局精度。

Conclusion: 该算法通过参数调整可实现更个性化或更弹性的行为，适应多种分布式学习任务。

Abstract: In this paper, we address two practical challenges of distributed learning in
multi-agent network systems, namely personalization and resilience.
Personalization is the need of heterogeneous agents to learn local models
tailored to their own data and tasks, while still generalizing well; on the
other hand, the learning process must be resilient to cyberattacks or anomalous
training data to avoid disruption. Motivated by a conceptual affinity between
these two requirements, we devise a distributed learning algorithm that
combines distributed gradient descent and the Friedkin-Johnsen model of opinion
dynamics to fulfill both of them. We quantify its convergence speed and the
neighborhood that contains the final learned models, which can be easily
controlled by tuning the algorithm parameters to enforce a more
personalized/resilient behavior. We numerically showcase the effectiveness of
our algorithm on synthetic and real-world distributed learning tasks, where it
achieves high global accuracy both for personalized models and with malicious
agents compared to standard strategies.

</details>


### [959] [HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems](https://arxiv.org/abs/2505.13516)
*Zhipeng Hou,Junyi Tang,Yipeng Wang*

Key words: 多智能体系统、大型语言模型、分层推理、MCTS、任务分解

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: HALO是一种基于分层推理架构的多智能体协作框架，通过任务分解、角色设计和子任务执行，提升了复杂交互环境中的适应性和灵活性。

Motivation: 现有智能体系统依赖预定义的代理角色和静态通信结构，限制了其在复杂交互环境中的适应性和灵活性，导致在专业任务中表现不佳。

Method: 引入HALO框架，包含高层规划代理进行任务分解，中层角色设计代理实例化子任务，以及低层推理代理执行子任务，并使用MCTS搜索最优推理轨迹。

Result: 在HumanEval、MMLU和MATH基准测试中平均提升14.4%，在专业任务中表现尤为突出。

Conclusion: HALO显著提升了多智能体系统在复杂和专业任务中的性能。

Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.
Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability
as well as flexibility in complex interaction environments and leading to
subpar performance on highly specialized and expert-level tasks. To address
these issues, we introduce HALO, a multi-agent collaboration framework based on
a hierarchical reasoning architecture. Specifically, we incorporate a
high-level planning agent for task decomposition, mid-level role-design agents
for subtask-specific agent instantiation, and low-level inference agents for
subtask execution. Particularly, subtask execution is reformulated as a
structured workflow search problem, where Monte Carlo Tree Search (MCTS)
systematically explores the agentic action space to construct optimal reasoning
trajectories. Additionally, as the majority of users lack expertise in prompt
engineering, we leverage an Adaptive Prompt Refinement module to transform raw
queries into task-specific prompts. Empirical evaluations on Code Generation
(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)
benchmark datasets highlight the effectiveness of HALO, yielding a 14.4%
average improvement over state-of-the-art baselines. Notably, HALO achieves up
to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark
and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,
indicating its advanced proficiency in tackling highly specialized and
expert-level tasks. The code repository is available at
https://github.com/23japhone/HALO.

</details>


### [960] [ACPs: Agent Collaboration Protocols for the Internet of Agents](https://arxiv.org/abs/2505.13523)
*Jun Liu,Ke Yu,Keliang Chen,Ke Li,Yuxinyue Qian,Xiaolian Guo,Haozhe Song,Yinming Li*

Key words: 人工智能, 代理协作协议, IoA, 互操作性, 可扩展性

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 本文提出了一种名为Agent Collaboration Protocols (ACPs)的全面协议套件，旨在解决异构代理在Internet of Agents (IoA)中的互操作性、可扩展性和协调性问题。

Motivation: 随着人工智能的快速发展，自主代理的激增带来了互操作性、可扩展性和协调性等新挑战。现有代理通信协议如MCP、A2A和ANP仍具有碎片化和场景特定的局限性。

Method: 通过设计ACPs协议套件，包括注册、发现、交互和工具协议，以支持可信访问、能力编排和工作流构建。文章还介绍了ACPs的架构、关键技术和应用工作流程。

Result: 在餐厅预订协作场景中展示了ACPs的有效性，证明了其在实际应用中的可行性。

Conclusion: ACPs为构建安全、开放和可扩展的代理互联网基础设施奠定了基础。

Abstract: With the rapid advancement of artificial intelligence, the proliferation of
autonomous agents has introduced new challenges in interoperability,
scalability, and coordination. The Internet of Agents (IoA) aims to
interconnect heterogeneous agents through standardized communication protocols,
enabling seamless collaboration and intelligent task execution. However,
existing agent communication protocols such as MCP, A2A, and ANP remain
fragmented and scenario-specific. To address this gap, we propose Agent
Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.
ACPs include registration, discovery, interaction, and tooling protocols to
support trustable access, capability orchestration, and workflow construction.
We present the architecture, key technologies, and application workflows of
ACPs, and demonstrate its effectiveness in a collaborative restaurant booking
scenario. ACPs lay the foundation for building a secure, open, and scalable
agent internet infrastructure.

</details>


### [961] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
*Haoyang Fang,Boran Han,Nick Erickson,Xiyuan Zhang,Su Zhou,Anirudh Dagar,Jiani Zhang,Ali Caner Turkmen,Cuixiong Hu,Huzefa Rangwala,Ying Nian Wu,Bernie Wang,George Karypis*

Key words: AutoML, 多模态数据, 大型语言模型, 自动化, 多代理框架

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: MLZero是一种基于大型语言模型的多代理框架，实现端到端的机器学习自动化，尤其在多模态数据上表现优异，显著减少人工干预。

Motivation: 现有AutoML系统在多模态数据处理上仍需大量手动配置和专家输入，MLZero旨在解决这一问题。

Method: 采用认知感知模块处理多模态输入，结合语义和情景记忆增强代码生成的迭代过程，以应对LLM的局限性。

Result: 在MLE-Bench Lite上表现最佳，获得六枚金牌；在Multimodal AutoML Agent Benchmark上以92%的成功率大幅领先。

Conclusion: MLZero在多模态数据自动化任务中表现卓越，即便是小型LLM也能超越现有的全尺寸系统。

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


### [962] [Personalized and Resilient Distributed Learning Through Opinion Dynamics](https://arxiv.org/abs/2505.14081)
*Luca Ballotta,Nicola Bastianello,Riccardo M. G. Ferrari,Karl H. Johansson*

Key words: 分布式学习、多智能体系统、个性化、鲁棒性、Friedkin-Johnsen模型

<details>
  <summary>Details</summary>

Main category: cs.MA

TL;DR: 该论文提出了一种分布式学习算法，结合了分布式梯度下降和意见动态模型，以解决多智能体网络中的个性化和鲁棒性问题。

Motivation: 为了解决多智能体系统中个性化（异构智能体学习本地模型）和鲁棒性（抵御网络攻击或异常数据）的挑战，作者设计了一种结合分布式梯度下降和意见动态模型的算法。

Method: 算法结合分布式梯度下降和Friedkin-Johnsen意见动态模型，通过调整参数控制个性化与鲁棒性的表现。

Result: 算法在合成和真实世界的分布式学习任务中表现出色，既能实现高精度的个性化模型，又能抵御恶意代理的干扰。

Conclusion: 该算法有效平衡了个性化和鲁棒性需求，展示了在多智能体系统中的实用价值。

Abstract: In this paper, we address two practical challenges of distributed learning in
multi-agent network systems, namely personalization and resilience.
Personalization is the need of heterogeneous agents to learn local models
tailored to their own data and tasks, while still generalizing well; on the
other hand, the learning process must be resilient to cyberattacks or anomalous
training data to avoid disruption. Motivated by a conceptual affinity between
these two requirements, we devise a distributed learning algorithm that
combines distributed gradient descent and the Friedkin-Johnsen model of opinion
dynamics to fulfill both of them. We quantify its convergence speed and the
neighborhood that contains the final learned models, which can be easily
controlled by tuning the algorithm parameters to enforce a more
personalized/resilient behavior. We numerically showcase the effectiveness of
our algorithm on synthetic and real-world distributed learning tasks, where it
achieves high global accuracy both for personalized models and with malicious
agents compared to standard strategies.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [963] [Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors](https://arxiv.org/abs/2505.11325)
*Thomas Nagler,David Rügamer*

Key words: 先验数据拟合网络, 不确定性量化, Martingale posteriors, 贝叶斯后验

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 本文提出了一种基于Martingale posteriors的采样方法，用于在先验数据拟合网络（PFNs）中量化预测不确定性，并在模拟和实际数据中验证了其有效性。

Motivation: 尽管PFNs在表格数据预测中表现出色，但缺乏对预测均值、分位数等的不确定性量化。本文旨在解决这一问题。

Method: 提出了一种基于Martingale posteriors的采样方法，构建贝叶斯后验分布，并证明其收敛性。

Result: 在模拟和真实数据实验中，该方法成功量化了预测的不确定性。

Conclusion: 该方法为PFNs提供了有效的预测不确定性量化工具，扩展了其应用范围。

Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models
for prediction from tabular data sets, achieving state-of-the-art performance
on small to moderate data sizes without tuning. While PFNs are motivated by
Bayesian ideas, they do not provide any uncertainty quantification for
predictive means, quantiles, or similar quantities. We propose a principled and
efficient sampling procedure to construct Bayesian posteriors for such
estimates based on Martingale posteriors, and prove its convergence. Several
simulated and real-world data examples showcase the uncertainty quantification
of our method in inference applications.

</details>


### [964] [Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors](https://arxiv.org/abs/2505.11325)
*Thomas Nagler,David Rügamer*

Key words: Prior-data拟合网络, 不确定性量化, Martingale后验, 贝叶斯方法

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 本文提出了一种基于Martingale后验的高效采样方法，为PFNs提供了不确定性量化，并通过实验验证了其有效性。

Motivation: 为了解决Prior-data拟合网络（PFNs）在预测中缺乏不确定性量化的问题，作者提出了一种基于贝叶斯思想的改进方法。

Method: 基于Martingale后验，设计了一种高效的采样程序，用于构建预测均值、分位数等估计的贝叶斯后验分布，并证明了其收敛性。

Result: 通过模拟和真实数据实验，验证了该方法在不确定性量化方面的有效性。

Conclusion: 所提出的方法成功地扩展了PFNs的功能，为预测提供了可靠的不确定性量化。

Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models
for prediction from tabular data sets, achieving state-of-the-art performance
on small to moderate data sizes without tuning. While PFNs are motivated by
Bayesian ideas, they do not provide any uncertainty quantification for
predictive means, quantiles, or similar quantities. We propose a principled and
efficient sampling procedure to construct Bayesian posteriors for such
estimates based on Martingale posteriors, and prove its convergence. Several
simulated and real-world data examples showcase the uncertainty quantification
of our method in inference applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [965] [Robust learning of halfspaces under log-concave marginals](https://arxiv.org/abs/2505.13708)
*Jane Lange,Arsen Vasilyan*

Key words: 对抗鲁棒性、边界体积、多项式回归、线性阈值函数、亚高斯分布

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: 论文提出了一种高效学习具有小边界体积的分类器的方法，适用于亚高斯各向同性对数凹分布的数据，并通过多项式回归等方法实现对抗鲁棒性。

Motivation: 研究如何高效学习对抗鲁棒性强的分类器，尤其是在扰动半径为r时，边界体积小的假设。

Method: 算法结合多项式回归、ℓ1误差回归、结构化分区与舍入步骤以及局部校正器，以确保分类器在扰动下的鲁棒性和低边界体积。

Result: 算法能够以时间与样本复杂度d^Õ(1/ε²)学习线性阈值函数，返回边界体积为O(r+ε)的分类器。

Conclusion: 该方法在保持多项式回归复杂度的同时，显著提升了分类器的对抗鲁棒性。

Abstract: We say that a classifier is \emph{adversarially robust} to perturbations of
norm $r$ if, with high probability over a point $x$ drawn from the input
distribution, there is no point within distance $\le r$ from $x$ that is
classified differently. The \emph{boundary volume} is the probability that a
point falls within distance $r$ of a point with a different label. This work
studies the task of computationally efficient learning of hypotheses with small
boundary volume, where the input is distributed as a subgaussian isotropic
log-concave distribution over $\mathbb{R}^d$.
  Linear threshold functions are adversarially robust; they have boundary
volume proportional to $r$. Such concept classes are efficiently learnable by
polynomial regression, which produces a polynomial threshold function (PTF),
but PTFs in general may have boundary volume $\Omega(1)$, even for $r \ll 1$.
  We give an algorithm that agnostically learns linear threshold functions and
returns a classifier with boundary volume $O(r+\varepsilon)$ at radius of
perturbation $r$. The time and sample complexity of
$d^{\tilde{O}(1/\varepsilon^2)}$ matches the complexity of polynomial
regression.
  Our algorithm augments the classic approach of polynomial regression with
three additional steps: a) performing the $\ell_1$-error regression under noise
sensitivity constraints, b) a structured partitioning and rounding step that
returns a Boolean classifier with error $\textsf{opt} + O(\varepsilon)$ and
noise sensitivity $O(r+\varepsilon)$ simultaneously, and c) a local corrector
that ``smooths'' a function with low noise sensitivity into a function that is
adversarially robust.

</details>


### [966] [Robust learning of halfspaces under log-concave marginals](https://arxiv.org/abs/2505.13708)
*Jane Lange,Arsen Vasilyan*

Key words: 对抗性鲁棒性、边界体积、多项式回归、线性阈值函数、噪声敏感性

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: 研究如何在计算高效的情况下学习具有小边界体积的假设，通过改进多项式回归方法，提出一种算法，能够在扰动半径为$r$时学习线性阈值函数并返回边界体积为$O(r+\varepsilon)$的分类器。

Motivation: 探讨对抗性鲁棒分类器的学习问题，特别是在输入分布为亚高斯各向同性对数凹分布时的边界体积优化。

Method: 算法结合多项式回归，加入$ℓ_1$误差回归、结构化分割与舍入以及局部校正器步骤，以优化分类器的边界体积和噪声敏感性。

Result: 算法能学习线性阈值函数并返回边界体积为$O(r+\varepsilon)$的分类器，时间和样本复杂度与多项式回归相当。

Conclusion: 通过改进多项式回归方法，可以有效学习具有小边界体积的分类器，同时保持对抗性鲁棒性。

Abstract: We say that a classifier is \emph{adversarially robust} to perturbations of
norm $r$ if, with high probability over a point $x$ drawn from the input
distribution, there is no point within distance $\le r$ from $x$ that is
classified differently. The \emph{boundary volume} is the probability that a
point falls within distance $r$ of a point with a different label. This work
studies the task of computationally efficient learning of hypotheses with small
boundary volume, where the input is distributed as a subgaussian isotropic
log-concave distribution over $\mathbb{R}^d$.
  Linear threshold functions are adversarially robust; they have boundary
volume proportional to $r$. Such concept classes are efficiently learnable by
polynomial regression, which produces a polynomial threshold function (PTF),
but PTFs in general may have boundary volume $\Omega(1)$, even for $r \ll 1$.
  We give an algorithm that agnostically learns linear threshold functions and
returns a classifier with boundary volume $O(r+\varepsilon)$ at radius of
perturbation $r$. The time and sample complexity of
$d^{\tilde{O}(1/\varepsilon^2)}$ matches the complexity of polynomial
regression.
  Our algorithm augments the classic approach of polynomial regression with
three additional steps: a) performing the $\ell_1$-error regression under noise
sensitivity constraints, b) a structured partitioning and rounding step that
returns a Boolean classifier with error $\textsf{opt} + O(\varepsilon)$ and
noise sensitivity $O(r+\varepsilon)$ simultaneously, and c) a local corrector
that ``smooths'' a function with low noise sensitivity into a function that is
adversarially robust.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [967] [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
*Avinash Patil*

Key words: 软件质量保证, 大语言模型, ISO标准, 自动化测试, 数据隐私

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 本文探讨了如何利用大语言模型（LLMs）增强软件质量保证（SQA）过程，并结合ISO/IEC等标准框架，分析了LLMs在需求验证、缺陷检测等任务中的应用及其挑战和未来方向。

Motivation: 随着LLMs技术的发展，其在SQA中的潜在应用逐渐显现，但如何与现有标准结合并解决实际挑战仍待研究。

Method: 综述了LLMs在SQA中的技术基础和应用场景，并通过案例研究和开源项目验证了其可行性。

Result: 研究表明LLMs可以提升SQA效率，但也面临数据隐私、模型偏见等挑战。

Conclusion: 未来需关注自适应学习、隐私保护部署等技术方向，并推动AI驱动SQA的标准制定。

Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.

</details>


### [968] [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
*Karina Zainullina,Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Daria Litvintseva,Simon Karasik,Filipp Fisin,Sergei Skvortsov,Maksim Nekrashevich,Anton Shevtsov,Boris Yangel*

Key words: 大型语言模型、RL环境、测试时搜索、动作价值函数、SWE-bench、Qwen-72B、GPT-4o

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 大型语言模型（LLMs）在复杂多步任务中表现优异，但多次尝试中性能不稳定。通过引导测试时搜索（如1步前瞻和轨迹选择）可提升性能，特别是在不可序列化RL环境中。实验表明，这些方法在SWE-bench基准上将Qwen-72B模型的成功率提升至40.8%，并适用于GPT-4o等闭源模型。

Motivation: 解决LLMs在多次尝试中性能不稳定的问题，特别是在不可序列化RL环境中的应用挑战。

Method: 提出1步前瞻和轨迹选择两种搜索策略，并利用学习的动作价值函数估计器引导。

Result: 在SWE-bench基准上，Qwen-72B模型的成功率翻倍至40.8%，且在GPT-4o上同样有效。

Conclusion: 引导测试时搜索策略可显著提升LLMs在不可序列化环境中的性能，并具有跨模型通用性。

Abstract: Large language models (LLMs) have recently achieved remarkable results in
complex multi-step tasks, such as mathematical reasoning and agentic software
engineering. However, they often struggle to maintain consistent performance
across multiple solution attempts. One effective approach to narrow the gap
between average-case and best-case performance is guided test-time search,
which explores multiple solution paths to identify the most promising one.
Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for
non-serializable RL environments, such as Docker containers, where intermediate
environment states cannot be easily saved and restored. We investigate two
complementary search strategies applicable to such environments: 1-step
lookahead and trajectory selection, both guided by a learned action-value
function estimator. On the SWE-bench Verified benchmark, a key testbed for
agentic software engineering, we find these methods to double the average
success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new
state-of-the-art for open-weights models. Additionally, we show that these
techniques are transferable to more advanced closed models, yielding similar
improvements with GPT-4o.

</details>


### [969] [Selective Code Generation for Functional Guarantees](https://arxiv.org/abs/2505.13553)
*Jaewoo Jeong,Taesoo Kim,Sangdon Park*

Key words: 大语言模型, 代码生成, 幻觉问题, 单元测试, FuzzEval

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该论文提出了一种通过动态代码分析工具自动生成单元测试的方法，以解决代码生成模型中的幻觉问题，并学习选择性代码生成器来控制幻觉率，同时提出FuzzEval评估范式。

Motivation: 代码生成模型的幻觉问题阻碍了其在需要高安全标准的系统中的应用，但目前该问题较少被研究。代码的非自然形式使得识别生成代码的预期功能变得复杂，而传统单元测试扩展成本高昂。

Method: 通过动态代码分析工具自动生成单元测试，提出选择性代码生成器，选择性地避免不确定的代码生成，以控制幻觉率。

Result: 实验表明该方法在开放和封闭的代码生成器上表现出色，能有效控制代码幻觉率并提供可信的代码生成。

Conclusion: 通过自动生成单元测试和学习选择性代码生成器，论文成功解决了代码幻觉问题，提高了代码生成的可控性和可靠性。

Abstract: Large language models (LLMs) show human-level performance and their
specialized descendants, code generation models, play core roles in solving
complex tasks, including mathematical reasoning and software development. On
the downside, the hallucination of LLMs mainly hinders their applicability to
systems requiring higher safety standards, thus drawing the attention of the AI
community. However, the hallucination of code generation models is rarely
considered. One critical bottleneck in considering code hallucination is the
intricate property of code to identify whether generated code has the intended
functionality due to its un-natural form, different to natural languages.
Handful of unit tests have been considered to address this issue, but
scaling-up its size is extremely expensive. We address this core bottleneck by
automatically generating unit tests using dynamic code analysis tools, which
leverages the \emph{executable nature} of code. Given generated unit tests from
true code for measuring functional correctness of generated code, we propose to
learn a \emph{selective code generator}, which abstains from answering for
unsure generation, to control the rate of code hallucination among
non-abstaining answers in terms of a false discovery rate. This learning
algorithm provides a controllability guarantee, providing trustworthiness of
code generation. Finally, we propose to use generated unit tests in evaluation
as well as in learning for precise code evaluation, calling this evaluation
paradigm \emph{FuzzEval}. We demonstrate the efficacy of our selective code
generator over open and closed code generators, showing clear benefit of
leveraging generated unit tests along with the controllability of code
hallucination and reasonable selection efficiency via our selective code
generator.

</details>


### [970] [HarmonE: A Self-Adaptive Approach to Architecting Sustainable MLOps](https://arxiv.org/abs/2505.13693)
*Hiya Bhatt,Shaunak Biswas,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Key words: 机器学习系统, 可持续性, MLOps, MAPE-K循环, 数字孪生

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文提出了一个名为HarmonE的架构方法，通过MAPE-K循环在MLOps中实现自适应，以解决机器学习系统在动态环境中的可持续性问题。

Motivation: 机器学习系统（MLS）在动态环境中运行时面临数据漂移和模型退化等不确定性，传统MLOps仅解决技术维度，而忽略了经济、环境和社会等其他维度的可持续性问题。

Method: 提出了HarmonE架构，允许系统设计师在设计时定义可持续性目标和适应阈值，运行时监控关键指标（如预测准确性、能耗等），并触发适应的策略。

Result: 通过智能交通系统的数字孪生验证，HarmonE能够有效适应变化的条件，同时保持准确性并满足可持续性目标。

Conclusion: HarmonE为机器学习系统的可持续性提供了一种全面的解决方案，尤其是在动态环境中。

Abstract: Machine Learning Enabled Systems (MLS) are becoming integral to real-world
applications, but ensuring their sustainable performance over time remains a
significant challenge. These systems operate in dynamic environments and face
runtime uncertainties like data drift and model degradation, which affect the
sustainability of MLS across multiple dimensions: technical, economical,
environmental, and social. While Machine Learning Operations (MLOps) addresses
the technical dimension by streamlining the ML model lifecycle, it overlooks
other dimensions. Furthermore, some traditional practices, such as frequent
retraining, incur substantial energy and computational overhead, thus
amplifying sustainability concerns. To address them, we introduce HarmonE, an
architectural approach that enables self-adaptive capabilities in MLOps
pipelines using the MAPE-K loop. HarmonE allows system architects to define
explicit sustainability goals and adaptation thresholds at design time, and
performs runtime monitoring of key metrics, such as prediction accuracy, energy
consumption, and data distribution shifts, to trigger appropriate adaptation
strategies. We validate our approach using a Digital Twin (DT) of an
Intelligent Transportation System (ITS), focusing on traffic flow prediction as
our primary use case. The DT employs time series ML models to simulate
real-time traffic and assess various flow scenarios. Our results show that
HarmonE adapts effectively to evolving conditions while maintaining accuracy
and meeting sustainability goals.

</details>


### [971] [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
*Avinash Patil*

Key words: 软件质量保证, 大型语言模型, 国际标准, 自动化测试, 缺陷检测

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLM）如何增强软件质量保证（SQA）流程，并与现有国际标准相结合，同时分析了其应用潜力和挑战。

Motivation: 软件质量保证对于交付可靠、安全和高效的软件产品至关重要。LLM的出现为自动化SQA任务提供了新机会，但如何使其与现有国际标准兼容是研究的动机。

Method: 本文首先回顾了软件质量标准和LLM技术基础，然后探讨了LLM在需求验证、缺陷检测、测试生成等SQA任务中的应用，并将其映射到国际标准中。

Result: 研究表明，LLM可以增强传统SQA方法，但需要解决数据隐私、模型偏差等挑战。

Conclusion: LLM为SQA带来了新机遇，但未来需关注自适应学习、隐私保护部署和多模态分析等方向。

Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.

</details>


### [972] [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
*Karina Zainullina,Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Daria Litvintseva,Simon Karasik,Filipp Fisin,Sergei Skvortsov,Maksim Nekrashevich,Anton Shevtsov,Boris Yangel*

Key words: 大型语言模型、搜索策略、非序列化环境、动作-价值函数、SWE-bench

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 通过两种搜索策略（一步前瞻和轨迹选择），提升了大型语言模型在非序列化环境中的性能，在SWE-bench测试中达到40.8%的成功率。

Motivation: 大型语言模型在复杂任务中表现不稳定，且传统搜索方法无法应用于非序列化环境，需要新的解决方案。

Method: 采用一步前瞻和轨迹选择策略，结合学习的动作-价值函数估计器，提升模型性能。

Result: 在SWE-bench测试中，成功率达到40.8%，为开源模型的新最佳成绩，同时在GPT-4o上也显示出类似改进。

Conclusion: 提出的搜索策略能显著提升模型在非序列化环境中的表现，且具有通用性。

Abstract: Large language models (LLMs) have recently achieved remarkable results in
complex multi-step tasks, such as mathematical reasoning and agentic software
engineering. However, they often struggle to maintain consistent performance
across multiple solution attempts. One effective approach to narrow the gap
between average-case and best-case performance is guided test-time search,
which explores multiple solution paths to identify the most promising one.
Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for
non-serializable RL environments, such as Docker containers, where intermediate
environment states cannot be easily saved and restored. We investigate two
complementary search strategies applicable to such environments: 1-step
lookahead and trajectory selection, both guided by a learned action-value
function estimator. On the SWE-bench Verified benchmark, a key testbed for
agentic software engineering, we find these methods to double the average
success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new
state-of-the-art for open-weights models. Additionally, we show that these
techniques are transferable to more advanced closed models, yielding similar
improvements with GPT-4o.

</details>


### [973] [Selective Code Generation for Functional Guarantees](https://arxiv.org/abs/2505.13553)
*Jaewoo Jeong,Taesoo Kim,Sangdon Park*

Key words: 大型语言模型,代码生成,幻觉控制,单元测试,FuzzEval

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该论文研究了大型语言模型（LLM）在代码生成中的幻觉问题，提出通过动态代码分析工具自动生成单元测试，并开发选择性代码生成器以控制幻觉率，同时提出FuzzEval评估范式，验证了其有效性。

Motivation: LLM在代码生成中存在幻觉问题，影响其在高安全性系统中的应用。论文旨在解决这一瓶颈，提升代码生成的可靠性和可控性。

Method: 通过动态代码分析工具自动生成单元测试，设计选择性代码生成器，通过FuzzEval范式进行学习和评估。

Result: 实验表明选择性代码生成器在控制幻觉率和选择效率上表现优于现有方法。

Conclusion: 提出的方法和评估范式有效解决了代码生成中的幻觉问题，增强了代码生成的可靠性和可控性。

Abstract: Large language models (LLMs) show human-level performance and their
specialized descendants, code generation models, play core roles in solving
complex tasks, including mathematical reasoning and software development. On
the downside, the hallucination of LLMs mainly hinders their applicability to
systems requiring higher safety standards, thus drawing the attention of the AI
community. However, the hallucination of code generation models is rarely
considered. One critical bottleneck in considering code hallucination is the
intricate property of code to identify whether generated code has the intended
functionality due to its un-natural form, different to natural languages.
Handful of unit tests have been considered to address this issue, but
scaling-up its size is extremely expensive. We address this core bottleneck by
automatically generating unit tests using dynamic code analysis tools, which
leverages the \emph{executable nature} of code. Given generated unit tests from
true code for measuring functional correctness of generated code, we propose to
learn a \emph{selective code generator}, which abstains from answering for
unsure generation, to control the rate of code hallucination among
non-abstaining answers in terms of a false discovery rate. This learning
algorithm provides a controllability guarantee, providing trustworthiness of
code generation. Finally, we propose to use generated unit tests in evaluation
as well as in learning for precise code evaluation, calling this evaluation
paradigm \emph{FuzzEval}. We demonstrate the efficacy of our selective code
generator over open and closed code generators, showing clear benefit of
leveraging generated unit tests along with the controllability of code
hallucination and reasonable selection efficiency via our selective code
generator.

</details>


### [974] [HarmonE: A Self-Adaptive Approach to Architecting Sustainable MLOps](https://arxiv.org/abs/2505.13693)
*Hiya Bhatt,Shaunak Biswas,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Key words: 机器学习系统,可持续性,MLOps,自适应,数字孪生,MAPE-K循环

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 论文提出了HarmonE架构方法，通过MAPE-K循环在MLOps管道中实现自适应能力，以解决机器学习系统在多维度可持续性方面的问题，并在智能交通系统的数字孪生中验证了其有效性。

Motivation: 机器学习系统在动态环境中运行时会面临数据漂移和模型退化等问题，影响其技术、经济、环境和社会多方面的可持续性。现有MLOps方法仅关注技术维度，且频繁重新训练等传统做法会加剧可持续性问题。

Method: 提出HarmonE架构，利用MAPE-K循环实现自适应能力，允许在设计时定义可持续性目标和适应阈值，并在运行时监测关键指标（如预测精度、能耗和数据分布变化）以触发适应策略。

Result: 在智能交通系统的数字孪生中验证，结果显示HarmonE能有效适应动态条件，同时保持预测精度并满足可持续性目标。

Conclusion: HarmonE为机器学习系统在多维度可持续性方面的挑战提供了有效解决方案，且在智能交通系统中表现出色。

Abstract: Machine Learning Enabled Systems (MLS) are becoming integral to real-world
applications, but ensuring their sustainable performance over time remains a
significant challenge. These systems operate in dynamic environments and face
runtime uncertainties like data drift and model degradation, which affect the
sustainability of MLS across multiple dimensions: technical, economical,
environmental, and social. While Machine Learning Operations (MLOps) addresses
the technical dimension by streamlining the ML model lifecycle, it overlooks
other dimensions. Furthermore, some traditional practices, such as frequent
retraining, incur substantial energy and computational overhead, thus
amplifying sustainability concerns. To address them, we introduce HarmonE, an
architectural approach that enables self-adaptive capabilities in MLOps
pipelines using the MAPE-K loop. HarmonE allows system architects to define
explicit sustainability goals and adaptation thresholds at design time, and
performs runtime monitoring of key metrics, such as prediction accuracy, energy
consumption, and data distribution shifts, to trigger appropriate adaptation
strategies. We validate our approach using a Digital Twin (DT) of an
Intelligent Transportation System (ITS), focusing on traffic flow prediction as
our primary use case. The DT employs time series ML models to simulate
real-time traffic and assess various flow scenarios. Our results show that
HarmonE adapts effectively to evolving conditions while maintaining accuracy
and meeting sustainability goals.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [975] [EuLearn: A 3D database for learning Euler characteristics](https://arxiv.org/abs/2505.13539)
*Rodrigo Fritz,Pablo Suárez-Serrato,Victor Mijangos,Anayanzi D. Martinez-Hernandez,Eduardo Ivan Velazquez Richards*

Key words: 拓扑数据集, 非欧几里得采样, 深度学习, 3D 神经网络

<details>
  <summary>Details</summary>

Main category: cs.CG

TL;DR: EuLearn 提出了首个多样性拓扑类型的数据集，通过随机纽结设计均匀变化的曲面，并开发了一种非欧几里得统计采样方法，显著提升深度学习模型在拓扑分类上的性能。

Motivation: 为训练能够识别拓扑特征的机器学习系统提供公平代表多样拓扑类型的表面数据集。

Method: 利用随机纽结设计曲面，开发非欧几里得统计采样方法，并改进 PointNet 和 Transformer 架构以结合拓扑信息。

Result: 通过结合拓扑信息，深度学习模型在 EuLearn 数据集上的性能显著提高。

Conclusion: 将拓扑信息整合到深度学习流程中可以解决复杂拓扑数据集的分类问题。

Abstract: We present EuLearn, the first surface datasets equitably representing a
diversity of topological types. We designed our embedded surfaces of uniformly
varying genera relying on random knots, thus allowing our surfaces to knot with
themselves. EuLearn contributes new topological datasets of meshes, point
clouds, and scalar fields in 3D. We aim to facilitate the training of machine
learning systems that can discern topological features. We experimented with
specific emblematic 3D neural network architectures, finding that their vanilla
implementations perform poorly on genus classification. To enhance performance,
we developed a novel, non-Euclidean, statistical sampling method adapted to
graph and manifold data. We also introduce adjacency-informed adaptations of
PointNet and Transformer architectures that rely on our non-Euclidean sampling
strategy. Our results demonstrate that incorporating topological information
into deep learning workflows significantly improves performance on these
otherwise challenging EuLearn datasets.

</details>


### [976] [Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks](https://arxiv.org/abs/2505.14417)
*Menglin Yang,Yifei Zhang,Jialin Chen,Melanie Weber,Rex Ying*

Key words: 非欧几里得几何, 基础模型, 机器学习, 网页技术, 数据表征

<details>
  <summary>Details</summary>

Main category: cs.CG

TL;DR: 探讨非欧几里得几何（如双曲、球形和混合曲率空间）在现代机器学习中应用的优势，特别是在网页相关技术中提升搜索、推荐和内容理解的潜力。

Motivation: 传统欧几里得空间的机器学习架构存在根本性限制，非欧几里得学习能更高效地表征复杂关系数据，如社交网络、查询-文档关系等。

Method: 利用非欧几里得几何（如双曲和球形空间）与基础模型结合，优化数据表征能力。

Result: 非欧几里得几何能够更有效地捕捉数据的固有几何特性，提升模型性能。

Conclusion: 非欧几里得基础模型与几何学习（NEGEL）的结合为网页技术发展提供了新的潜力与研究方向。

Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean
space is the de facto geometric setting of our machine learning architectures.
However, recent literature has demonstrated that this choice comes with
fundamental limitations. To that end, non-Euclidean learning is quickly gaining
traction, particularly in web-related applications where complex relationships
and structures are prevalent. Non-Euclidean spaces, such as hyperbolic,
spherical, and mixed-curvature spaces, have been shown to provide more
efficient and effective representations for data with intrinsic geometric
properties, including web-related data like social network topology,
query-document relationships, and user-item interactions. Integrating
foundation models with non-Euclidean geometries has great potential to enhance
their ability to capture and model the underlying structures, leading to better
performance in search, recommendations, and content understanding. This
workshop focuses on the intersection of Non-Euclidean Foundation Models and
Geometric Learning (NEGEL), exploring its potential benefits, including the
potential benefits for advancing web-related technologies, challenges, and
future directions. Workshop page:
[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)

</details>


### [977] [EuLearn: A 3D database for learning Euler characteristics](https://arxiv.org/abs/2505.13539)
*Rodrigo Fritz,Pablo Suárez-Serrato,Victor Mijangos,Anayanzi D. Martinez-Hernandez,Eduardo Ivan Velazquez Richards*

Key words: EuLearn, 拓扑数据集, 非欧几里得采样, 机器学习, 3D神经网络

<details>
  <summary>Details</summary>

Main category: cs.CG

TL;DR: EuLearn首次提出了均衡代表多种拓扑类型的表面数据集，通过随机结点设计均匀变化属的嵌入表面，并引入了针对3D数据的非欧几里得统计采样方法，显著提升了拓扑特征的识别性能。

Motivation: 旨在为机器学习系统提供能够识别拓扑特征的训练数据，解决现有3D神经网络架构在属分类任务上表现不佳的问题。

Method: 设计了基于随机结点的均匀变化属嵌入表面，开发了适用于图和流形数据的非欧几里得统计采样方法，并提出基于邻接信息的PointNet和Transformer架构改进方案。

Result: 实验表明，将拓扑信息融入深度学习流程能显著提升在EuLearn数据集上的性能。

Conclusion: EuLearn数据集及其配套方法为拓扑特征识别提供了有效工具，尤其在非欧几里得数据环境下表现突出。

Abstract: We present EuLearn, the first surface datasets equitably representing a
diversity of topological types. We designed our embedded surfaces of uniformly
varying genera relying on random knots, thus allowing our surfaces to knot with
themselves. EuLearn contributes new topological datasets of meshes, point
clouds, and scalar fields in 3D. We aim to facilitate the training of machine
learning systems that can discern topological features. We experimented with
specific emblematic 3D neural network architectures, finding that their vanilla
implementations perform poorly on genus classification. To enhance performance,
we developed a novel, non-Euclidean, statistical sampling method adapted to
graph and manifold data. We also introduce adjacency-informed adaptations of
PointNet and Transformer architectures that rely on our non-Euclidean sampling
strategy. Our results demonstrate that incorporating topological information
into deep learning workflows significantly improves performance on these
otherwise challenging EuLearn datasets.

</details>


### [978] [Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks](https://arxiv.org/abs/2505.14417)
*Menglin Yang,Yifei Zhang,Jialin Chen,Melanie Weber,Rex Ying*

Key words: 非欧几里得学习,基础模型,几何学习,Web技术,大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CG

TL;DR: 论文探讨了在基础模型和大型语言模型时代，非欧几里得几何学习在Web相关应用中的潜力与优势，尤其是对于复杂关系和结构的数据表示。

Motivation: 传统欧几里得空间在机器学习架构中存在局限，非欧几里得空间（如双曲空间、球面空间等）能更高效地表示具有内在几何属性的数据，如社交网络拓扑和用户-项目交互。

Method: 研究将基础模型与非欧几里得几何相结合，探索其在捕获底层结构方面的能力，提升搜索、推荐和内容理解的性能。

Result: 非欧几里得学习在Web相关技术中展现出显著潜力，能够更有效地建模复杂关系。

Conclusion: 非欧几里得基础模型与几何学习的结合为Web技术提供了新的研究方向，未来将面临挑战但也充满机遇。

Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean
space is the de facto geometric setting of our machine learning architectures.
However, recent literature has demonstrated that this choice comes with
fundamental limitations. To that end, non-Euclidean learning is quickly gaining
traction, particularly in web-related applications where complex relationships
and structures are prevalent. Non-Euclidean spaces, such as hyperbolic,
spherical, and mixed-curvature spaces, have been shown to provide more
efficient and effective representations for data with intrinsic geometric
properties, including web-related data like social network topology,
query-document relationships, and user-item interactions. Integrating
foundation models with non-Euclidean geometries has great potential to enhance
their ability to capture and model the underlying structures, leading to better
performance in search, recommendations, and content understanding. This
workshop focuses on the intersection of Non-Euclidean Foundation Models and
Geometric Learning (NEGEL), exploring its potential benefits, including the
potential benefits for advancing web-related technologies, challenges, and
future directions. Workshop page:
[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [979] [Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy](https://arxiv.org/abs/2505.14507)
*Jingyun Chen,David Horowitz,Yading Yuan*

Key words: 联邦学习,放射治疗,数据隐私,gRPC

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: 联邦学习平台 FedKBP+ 通过解决数据隐私问题，提升放射治疗计划的效率和一致性。

Motivation: 解决数据稀缺性和异构性问题，避免数据共享带来的隐私和技术障碍。

Method: 基于 gRPC 实现统一通信栈，支持集中式和完全分布式的联邦学习策略。

Result: FedKBP+ 在三个预测任务中表现出高效、稳健的潜力。

Conclusion: FedKBP+ 是一个有效的联邦学习平台，适用于放射治疗。

Abstract: Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.

</details>


### [980] [Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy](https://arxiv.org/abs/2505.14507)
*Jingyun Chen,David Horowitz,Yading Yuan*

Key words: 深度学习，联邦学习，放疗计划，数据隐私，SA-Net

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: FedKBP+ is a federated学习平台，旨在解决放疗中数据稀缺性和异质性导致的模型泛化问题，支持集中式和去中心化FL策略，并通过实验验证了其高效性和稳健性。

Motivation: 放疗计划中深度学习模型因数据稀缺性和机构间异质性泛化能力有限，而数据共享又面临隐私和技术障碍。

Method: 采用gRPC实现统一通信栈，支持集中式和去中心化的联邦学习，并通过SA-Net模型在三个预测任务上评估。

Result: FedKBP+表现出高效、有效和稳健的特性，适用于放疗领域。

Conclusion: FedKBP+作为联邦学习平台，在放疗治疗计划中具有巨大潜力。

Abstract: Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [981] [LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution](https://arxiv.org/abs/2505.13497)
*Claudius Kienle,Benjamin Alt,Oleg Arenz,Jan Peters*

Key words: 大型语言模型, 规划任务, 层次化学习, 仿真验证, 国际规划竞赛

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种分层学习方法，用于改进大型语言模型（LLMs）在规划任务中的表现，通过构建层次化问题域并结合仿真验证，显著提高了长期规划的成功率。

Motivation: 尽管LLMs能够根据自然语言指令进行规划，但其直接生成的计划常常存在缺陷。现有的领域学习方法依赖大量人工反馈，限制了实用性。

Method: 通过构建层次化问题域（将低级谓词和动作组合为高级版本），并利用仿真验证其条件和效果。同时引入中央错误推理器以确保规划层级的一致性。

Result: 在两个国际规划竞赛（IPC）领域和一个长期机器人操作任务中，该方法的表现优于当前最优的领域合成和LLM-modulo规划方法。

Conclusion: 提出的分层学习与仿真验证方法有效提升了LLMs在复杂规划任务中的表现，且能构建高质量的领域模型。

Abstract: Large Language Models (LLMs) enable planning from natural language
instructions using implicit world knowledge, but often produce flawed plans
that require refinement. Instead of directly predicting plans, recent methods
aim to learn a problem domain that can be solved for different goal states
using classical planners. However, these approaches require significant human
feedback to obtain useful models. We address this shortcoming by learning
hierarchical domains, where low-level predicates and actions are composed into
higher-level counterparts, and by leveraging simulation to validate their
preconditions and effects. This hierarchical approach is particularly powerful
for long-horizon planning, where LLM-based planning approaches typically
struggle. Furthermore, we introduce a central error reasoner to ensure
consistency among the different planning levels. Evaluation on two challenging
International Planning Competition (IPC) domains and a long-horizon robot
manipulation task demonstrates higher planning success rates than
state-of-the-art domain synthesis and LLM-modulo planning methods, while
constructing high-quality models of the domain. Resources, videos and detailed
experiment results are available at https://claudius-kienle.github.io/lodge/.

</details>


### [982] [Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios](https://arxiv.org/abs/2505.13532)
*Feihong Zhang,Guojian Zhan,Bin Shuai,Tianyi Zhang,Jingliang Duan,Shengbo Eben Li*

Key words: 强化学习, 自动驾驶, HPI, DSAC-H, 安全约束

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了一种新的安全导向训练技术HPI，结合DSAC算法开发了DSAC-H，在自动驾驶系统中实现了高效驾驶与安全约束的平衡。

Motivation: 现有强化学习算法在应对约束条件时存在挑战，尤其在现实应用中。本文旨在解决这一问题。

Method: 提出HPI技术，计算高效驾驶与安全约束的策略梯度，生成谐波梯度以减少冲突，结合DSAC算法形成DSAC-H。

Result: 多车道场景模拟显示DSAC-H实现了高效驾驶，且安全违规接近零。

Conclusion: DSAC-H在自动驾驶系统中成功平衡了效率与安全性。

Abstract: Reinforcement learning (RL), known for its self-evolution capability, offers
a promising approach to training high-level autonomous driving systems.
However, handling constraints remains a significant challenge for existing RL
algorithms, particularly in real-world applications. In this paper, we propose
a new safety-oriented training technique called harmonic policy iteration
(HPI). At each RL iteration, it first calculates two policy gradients
associated with efficient driving and safety constraints, respectively. Then, a
harmonic gradient is derived for policy updating, minimizing conflicts between
the two gradients and consequently enabling a more balanced and stable training
process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the
backbone and integrate it with our HPI to develop a new safe RL algorithm,
DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H
achieves efficient driving performance with near-zero safety constraint
violations.

</details>


### [983] [SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/abs/2505.13729)
*Abhinav Rajvanshi,Pritish Sahu,Tixiao Shan,Karan Sikka,Han-Pang Chiu*

Key words: 自适应协作，大型语言模型，多目标导航，异构机器人

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出SayCoNav方法，利用大型语言模型（LLMs）为机器人团队自动生成协作策略，提升多目标导航任务效率。

Motivation: 在未知环境中完成复杂导航任务需要机器人团队的自适应协作。

Method: SayCoNav通过LLM生成协作策略，并以分散式为每个机器人生成计划和行动，同时通过信息共享不断更新计划。

Result: 在MultiON任务中，SayCoNav比基线方法提升效率最高44.28%，并能动态适应变化条件。

Conclusion: SayCoNav通过LLM驱动的协作策略显著提升了机器人团队的导航效率和适应性。

Abstract: Adaptive collaboration is critical to a team of autonomous robots to perform
complicated navigation tasks in large-scale unknown environments. An effective
collaboration strategy should be determined and adapted according to each
robot's skills and current status to successfully achieve the shared goal. We
present SayCoNav, a new approach that leverages large language models (LLMs)
for automatically generating this collaboration strategy among a team of
robots. Building on the collaboration strategy, each robot uses the LLM to
generate its plans and actions in a decentralized way. By sharing information
to each other during navigation, each robot also continuously updates its
step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation
(MultiON) tasks, that require the team of the robots to utilize their
complementary strengths to efficiently search multiple different objects in
unknown environments. By validating SayCoNav with varied team compositions and
conditions against baseline methods, our experimental results show that
SayCoNav can improve search efficiency by at most 44.28% through effective
collaboration among heterogeneous robots. It can also dynamically adapt to the
changing conditions during task execution.

</details>


### [984] [Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams](https://arxiv.org/abs/2505.13834)
*Zhi Su,Yuman Gao,Emily Lukas,Yunfei Li,Jiaze Cai,Faris Tulbah,Fei Gao,Chao Yu,Zhongyu Li,Yi Wu,Koushil Sreenath*

Key words: 多智能体强化学习, 四足机器人, 机器人足球, 分层控制, 自主协作

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文提出了一种分层多智能体强化学习框架，用于实现四足机器人在足球比赛中的完全自主和分散式协作，结合了低层运动技能与高层策略规划，并展示了在真实机器人上的成功应用。

Motivation: 研究如何通过结合精细的运动控制和长期策略决策，实现四足机器人在动态、竞争和多智能体环境中的协作任务，以机器人足球为测试场景。

Method: 采用分层多智能体强化学习（MARL）框架，包括低层运动技能训练（如行走、带球、踢球）和高层策略规划（使用MAPPO和FSP）。

Result: 该方法在多智能体足球游戏中表现出显著优势，支持自主机器人和人机对抗比赛，适应多样化的对手策略和动态角色分配。

Conclusion: 分层MARL框架有效实现了四足机器人在复杂协作任务中的自主性，展示了在真实环境中的可行性和灵活性。

Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.

</details>


### [985] [Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements](https://arxiv.org/abs/2505.13837)
*Gokul Puthumanaillam,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Melkior Ornik*

Key words: 机器人导航, 不确定性管理, 任务特定地图, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: GUIDE框架通过任务特定不确定性地图（TSUM）将任务需求整合到导航策略中，显著提升了机器人在复杂环境中的性能。

Motivation: 解决机器人在复杂环境中因传感器噪声、环境变化和信息不完整带来的不确定性管理问题。

Method: 提出GUIDE框架，结合任务特定不确定性地图（TSUM）和强化学习，实现任务完成与不确定性管理的平衡。

Result: 实际测试表明，GUIDE显著优于缺乏任务特定不确定性感知的方法。

Conclusion: GUIDE框架为机器人在复杂环境中的导航提供了一种高效的任务特定不确定性管理解决方案。

Abstract: Robots navigating complex environments must manage uncertainty from sensor
noise, environmental changes, and incomplete information, with different tasks
requiring varying levels of precision in different areas. For example, precise
localization may be crucial near obstacles but less critical in open spaces. We
present GUIDE (Generalized Uncertainty Integration for Decision-Making and
Execution), a framework that integrates these task-specific requirements into
navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning
acceptable uncertainty levels to different locations, TSUMs enable robots to
adapt uncertainty management based on context. When combined with reinforcement
learning, GUIDE learns policies that balance task completion and uncertainty
management without extensive reward engineering. Real-world tests show
significant performance gains over methods lacking task-specific uncertainty
awareness.

</details>


### [986] [Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving](https://arxiv.org/abs/2505.13872)
*Jingzheng Li,Tiancheng Wang,Xingyu Peng,Jiacheng Chen,Zhijun Chen,Bing Li,Xianglong Liu*

Key words: 自动驾驶，安全评估，场景库，标准测试，安全威胁

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文介绍了Safety2Drive，一个专为评估自动驾驶系统安全性而设计的安全关键场景库。

Motivation: 当前自动驾驶数据集缺乏符合法规的闭环测试场景库，且真实事故数据不足，导致安全评估不充分。

Method: 提出Safety2Drive，覆盖70项标准测试、支持安全威胁注入和多维评估。

Result: Safety2Drive提供从场景构建到验证的标准化测试框架。

Conclusion: Safety2Drive为自动驾驶安全部署建立了标准化测试范式。

Abstract: Autonomous Driving (AD) systems demand the high levels of safety assurance.
Despite significant advancements in AD demonstrated on open-source benchmarks
like Longest6 and Bench2Drive, existing datasets still lack
regulatory-compliant scenario libraries for closed-loop testing to
comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD
accidents are underrepresented in current driving datasets. This scarcity leads
to inadequate evaluation of AD performance, posing risks to safety validation
and practical deployment. To address these challenges, we propose Safety2Drive,
a safety-critical scenario library designed to evaluate AD systems.
Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively
covers the test items required by standard regulations and contains 70 AD
function test items. (2) Safety2Drive supports the safety-critical scenario
generalization. It has the ability to inject safety threats such as natural
environment corruptions and adversarial attacks cross camera and LiDAR sensors.
(3) Safety2Drive supports multi-dimensional evaluation. In addition to the
evaluation of AD systems, it also supports the evaluation of various perception
tasks, such as object detection and lane detection. Safety2Drive provides a
paradigm from scenario construction to validation, establishing a standardized
test framework for the safe deployment of AD.

</details>


### [987] [APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight](https://arxiv.org/abs/2505.13921)
*Wanjing Huang,Weixiang Yan,Zhen Zhang,Ambuj Singh*

Key words: LLMs, physics reasoning, task planning, APEX, real-world applicability

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: APEX框架通过物理驱动的预见性增强LLMs的实时任务规划能力，显著优于现有方法。

Motivation: 大型语言模型（LLMs）在物理交互建模方面存在局限，现有方法无法捕捉动态对象交互或需要任务特定训练。

Method: APEX构建结构化图模型，提供物理状态更新和低延迟前向模拟，以支持LLMs的预测决策。

Result: 在物理推理、Tetris和动态避障三个基准测试中，APEX显著优于标准LLMs和基于VLM的模型。

Conclusion: APEX证明了显式物理推理对于连接语言智能与真实世界任务执行的必要性。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .

</details>


### [988] [Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World](https://arxiv.org/abs/2505.13969)
*Junya Nakanishi,Jun Baba,Yuichiro Yoshikawa,Hiroko Kamide,Hiroshi Ishiguro*

Key words: Global Workspace Theory, Selection-Broadcast Cycle, 人工智能, 机器人, 认知架构

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文探讨了Global Workspace Theory提出的Selection-Broadcast Cycle结构的功能优势，特别关注其在动态实时场景中的人工智能和机器人应用。

Motivation: 研究目的是强调Selection和Broadcast过程的循环结构及其对实时认知系统的综合效益。

Method: 通过分析Selection-Broadcast Cycle结构，提出其在动态、实时环境中的适应性。

Result: 研究发现该结构具有动态思维适应、经验基适应和即时实时适应三大优势。

Conclusion: GWT有望成为适用于复杂决策和自适应性能的认知架构，为通用AI和机器人系统的发展提供了新方向。

Abstract: This paper discusses the functional advantages of the Selection-Broadcast
Cycle structure proposed by Global Workspace Theory (GWT), inspired by human
consciousness, particularly focusing on its applicability to artificial
intelligence and robotics in dynamic, real-time scenarios. While previous
studies often examined the Selection and Broadcast processes independently,
this research emphasizes their combined cyclic structure and the resulting
benefits for real-time cognitive systems. Specifically, the paper identifies
three primary benefits: Dynamic Thinking Adaptation, Experience-Based
Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's
potential as a cognitive architecture suitable for sophisticated
decision-making and adaptive performance in unsupervised, dynamic environments.
It suggests new directions for the development and implementation of robust,
general-purpose AI and robotics systems capable of managing complex, real-world
tasks.

</details>


### [989] [Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures](https://arxiv.org/abs/2505.13556)
*Yiru Jiao,Simeon C. Calvert,Sander van Cranenburgh,Hans van Lint*

Key words: 广义替代安全度量（GSSM）、无监督学习、交通碰撞风险、上下文自适应、极值理论

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该研究提出了一种广义替代安全度量（GSSM），通过无监督学习自然驾驶数据来量化交通交互中的碰撞风险，优于现有方法。

Motivation: 现有方法依赖人工标注稀疏风险数据，且难以适应不同交互场景，因此需要一种无需标签、上下文自适应的安全评估方法。

Method: 利用神经网络学习正常驾驶行为的上下文条件分布，通过极值理论计算基于多向间距的风险评分。

Result: GSSM在真实碰撞测试中表现优异（AUPRC达0.9），可提前2.6秒预警，且在不同交互场景中优于基线方法。

Conclusion: GSSM为可扩展、上下文感知的碰撞风险评估提供了通用框架。

Abstract: Accurate and timely alerts for drivers or automated systems to unfolding
collisions remains a challenge in road safety, particularly in highly
interactive urban traffic. Existing approaches require labour-intensive
annotation of sparse risk, struggle to consider varying interaction context, or
are useful only in the scenarios they are designed for. To address these
limits, this study introduces the generalised surrogate safety measure (GSSM),
a new approach that learns exclusively from naturalistic driving without crash
or risk labels. GSSM captures the patterns of normal driving and estimates the
extent to which a traffic interaction deviates from the norm towards unsafe
extreme. Utilising neural networks, normal interactions are characterised by
context-conditioned distributions of multi-directional spacing between road
users. In the same interaction context, a spacing closer than normal entails
higher risk of potential collision. Then a context-adaptive risk score and its
associated probability can be calculated based on the theory of extreme values.
Any measurable factors, such as motion kinematics, weather, lighting, can serve
as part of the context, allowing for diverse coverage of safety-critical
interactions. Multiple public driving datasets are used to train GSSMs, which
are tested with 4,875 real-world crashes and near-crashes reconstructed from
the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of
0.9 and secures a median time advance of 2.6 seconds to prevent potential
collisions. Additional data and contextual factors provide further performance
gains. Across various interaction types such as rear-end, merging, and
crossing, the accuracy and timeliness of GSSM consistently outperforms existing
baselines. GSSM therefore establishes a scalable, context-aware, and
generalisable foundation to proactively quantify collision risk in traffic
interactions.

</details>


### [990] [Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction](https://arxiv.org/abs/2505.13889)
*Yiting Zhang,Shichen Li*

Key words: 可变形线性物体, 安全规划, 张力预测, 实时优化

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了一种安全运动规划和控制框架，用于可变形线性物体的操作，通过预测未来形状和张力，确保安全性。

Motivation: 现有模型多关注形状预测而忽略接触和张力约束，可能导致物体和机器人受损，因此需要更安全的操作方法。

Method: 使用预测模型联合估计形状和张力，结合基于多项式区间的实时轨迹优化器，强制执行安全约束。

Result: 在模拟线束装配任务中，比现有方法更高的成功率且无安全违规。

Conclusion: 该方法在接触丰富的环境中实现了稳健且安全的可变形线性物体操作。

Abstract: Manipulating deformable linear objects (DLOs) is challenging due to their
complex dynamics and the need for safe interaction in contact-rich
environments. Most existing models focus on shape prediction alone and fail to
account for contact and tension constraints, which can lead to damage to both
the DLO and the robot. In this work, we propose a certifiably safe motion
planning and control framework for DLO manipulation. At the core of our method
is a predictive model that jointly estimates the DLO's future shape and
tension. These predictions are integrated into a real-time trajectory optimizer
based on polynomial zonotopes, allowing us to enforce safety constraints
throughout the execution. We evaluate our framework on a simulated wire harness
assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,
our approach achieves a higher task success rate while avoiding all safety
violations. The results demonstrate that our method enables robust and safe DLO
manipulation in contact-rich environments.

</details>


### [991] [Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning](https://arxiv.org/abs/2505.13925)
*Yunpeng Jiang,Jianshu Hu,Paul Weng,Yutong Ban*

Key words: symmetry, deep reinforcement learning, time reversal, robotics, sample efficiency

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种利用时间反转对称性（TR-DRL）的方法，通过轨迹反转增强和奖励重塑，提高了深度强化学习在机器人任务中的样本效率和性能。

Motivation: 现有方法主要关注空间对称性，而忽略了时间对称性。论文通过探索时间反转对称性（如开关门的任务）来填补这一空白。

Method: 提出TR-DRL框架，结合轨迹反转增强和奖励重塑。通过动态一致性过滤器识别完全可逆的过渡，并生成反转轨迹；对于部分可逆的过渡，利用奖励重塑指导学习。

Result: 在Robosuite和MetaWorld基准测试中，TR-DRL在单任务和多任务设置中均表现出更高的样本效率和更强的最终性能。

Conclusion: TR-DRL有效利用了时间反转对称性，为深度强化学习在机器人任务中的应用提供了新的思路。

Abstract: Symmetry is pervasive in robotics and has been widely exploited to improve
sample efficiency in deep reinforcement learning (DRL). However, existing
approaches primarily focus on spatial symmetries, such as reflection, rotation,
and translation, while largely neglecting temporal symmetries. To address this
gap, we explore time reversal symmetry, a form of temporal symmetry commonly
found in robotics tasks such as door opening and closing. We propose Time
Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework
that combines trajectory reversal augmentation and time reversal guided reward
shaping to efficiently solve temporally symmetric tasks. Our method generates
reversed transitions from fully reversible transitions, identified by a
proposed dynamics-consistent filter, to augment the training data. For
partially reversible transitions, we apply reward shaping to guide learning,
according to successful trajectories from the reversed task. Extensive
experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL
is effective in both single-task and multi-task settings, achieving higher
sample efficiency and stronger final performance compared to baseline methods.

</details>


### [992] [NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/abs/2505.14526)
*Matteo El-Hariry,Antoine Richard,Ricard M. Castan,Luis F. W. Batista,Matthieu Geist,Cedric Pradalier,Miguel Olivares-Mendez*

Key words: 强化学习, 多域基准测试, 机器人导航, 仿真到现实, 跨平台

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: NavBench是一个多域基准测试框架，旨在通过标准化任务定义和评价标准，支持不同机器人在多样环境中的强化学习导航策略训练与评估。

Motivation: 现有强化学习基准测试通常局限于特定机器人平台，难以实现跨平台和跨领域的公平比较与泛化能力。

Method: NavBench基于IsaacLab构建，统一了任务定义和评价标准，支持多样化的机器人导航挑战，具有模块化设计和仿真到现实的验证能力。

Result: 实现了跨介质的统一基准测试，成功将策略迁移到卫星模拟器、无人水面艇和轮式地面车辆等多种真实机器人上。

Conclusion: NavBench通过标准化和模块化设计，简化了适应性导航策略的开发，并支持广泛的研究应用。

Abstract: Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.

</details>


### [993] [LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution](https://arxiv.org/abs/2505.13497)
*Claudius Kienle,Benjamin Alt,Oleg Arenz,Jan Peters*

Key words: Large Language Models, hierarchical learning, planning, simulation, long-horizon

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种分层学习方法来改进LLMs在规划任务中的表现，通过低级谓词和动作组合成高级别规划，并利用仿真验证条件，显著提升了长期规划的成功率。

Motivation: 为了解决LLMs在规划任务中直接预测计划易出错且需要大量人工反馈的问题，论文提出了一种分层学习方法。

Method: 采用分层学习策略，将低级谓词和动作组合成高级别规划，并结合仿真验证条件和效果，同时引入中央错误推理器确保规划一致性。

Result: 在IPC领域和机器人长期操作任务中，相比现有方法，该方法显著提高了规划成功率，并构建了高质量的领域模型。

Conclusion: 分层学习方法有效提升了LLMs在复杂规划任务中的表现，尤其是在长期规划中表现优异。

Abstract: Large Language Models (LLMs) enable planning from natural language
instructions using implicit world knowledge, but often produce flawed plans
that require refinement. Instead of directly predicting plans, recent methods
aim to learn a problem domain that can be solved for different goal states
using classical planners. However, these approaches require significant human
feedback to obtain useful models. We address this shortcoming by learning
hierarchical domains, where low-level predicates and actions are composed into
higher-level counterparts, and by leveraging simulation to validate their
preconditions and effects. This hierarchical approach is particularly powerful
for long-horizon planning, where LLM-based planning approaches typically
struggle. Furthermore, we introduce a central error reasoner to ensure
consistency among the different planning levels. Evaluation on two challenging
International Planning Competition (IPC) domains and a long-horizon robot
manipulation task demonstrates higher planning success rates than
state-of-the-art domain synthesis and LLM-modulo planning methods, while
constructing high-quality models of the domain. Resources, videos and detailed
experiment results are available at https://claudius-kienle.github.io/lodge/.

</details>


### [994] [Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios](https://arxiv.org/abs/2505.13532)
*Feihong Zhang,Guojian Zhan,Bin Shuai,Tianyi Zhang,Jingliang Duan,Shengbo Eben Li*

Key words: 强化学习, 自动驾驶, 安全约束, HPI, DSAC-H

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了一种名为HPI的安全导向强化学习技术，通过调和高效驾驶和安全约束的梯度，开发了新算法DSAC-H，在模拟中实现了高效驾驶且几乎零安全违规。

Motivation: 现有强化学习算法在真实应用中处理约束时面临挑战，特别是在自动驾驶领域。

Method: 提出HPI技术，计算并调和两个策略梯度（高效驾驶和安全约束），结合DSAC算法开发DSAC-H。

Result: DSAC-H在多车道场景模拟中实现了高效驾驶且安全约束违规接近于零。

Conclusion: HPI和DSAC-H为安全导向的强化学习提供了有效解决方案。

Abstract: Reinforcement learning (RL), known for its self-evolution capability, offers
a promising approach to training high-level autonomous driving systems.
However, handling constraints remains a significant challenge for existing RL
algorithms, particularly in real-world applications. In this paper, we propose
a new safety-oriented training technique called harmonic policy iteration
(HPI). At each RL iteration, it first calculates two policy gradients
associated with efficient driving and safety constraints, respectively. Then, a
harmonic gradient is derived for policy updating, minimizing conflicts between
the two gradients and consequently enabling a more balanced and stable training
process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the
backbone and integrate it with our HPI to develop a new safe RL algorithm,
DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H
achieves efficient driving performance with near-zero safety constraint
violations.

</details>


### [995] [SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/abs/2505.13729)
*Abhinav Rajvanshi,Pritish Sahu,Tixiao Shan,Karan Sikka,Han-Pang Chiu*

Key words: 自适应协作,机器人团队,大型语言模型,多目标导航

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: SayCoNav利用大型语言模型（LLMs）为机器人团队生成自适应协作策略，提高多目标导航任务效率。

Motivation: 在未知环境中，机器人团队的协作策略需根据各机器人技能和状态动态调整，以实现共同目标。

Method: 使用LLMs生成协作策略，机器人分散生成计划和行动，并在导航过程中共享信息以更新计划。

Result: 在MultiON任务中，SayCoNav可将搜索效率提高至多44.28%，并能动态适应任务执行中的变化条件。

Conclusion: SayCoNav通过LLMs支持的协作策略，有效提升了异构机器人团队的导航效率和适应性。

Abstract: Adaptive collaboration is critical to a team of autonomous robots to perform
complicated navigation tasks in large-scale unknown environments. An effective
collaboration strategy should be determined and adapted according to each
robot's skills and current status to successfully achieve the shared goal. We
present SayCoNav, a new approach that leverages large language models (LLMs)
for automatically generating this collaboration strategy among a team of
robots. Building on the collaboration strategy, each robot uses the LLM to
generate its plans and actions in a decentralized way. By sharing information
to each other during navigation, each robot also continuously updates its
step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation
(MultiON) tasks, that require the team of the robots to utilize their
complementary strengths to efficiently search multiple different objects in
unknown environments. By validating SayCoNav with varied team compositions and
conditions against baseline methods, our experimental results show that
SayCoNav can improve search efficiency by at most 44.28% through effective
collaboration among heterogeneous robots. It can also dynamically adapt to the
changing conditions during task execution.

</details>


### [996] [Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams](https://arxiv.org/abs/2505.13834)
*Zhi Su,Yuman Gao,Emily Lukas,Yunfei Li,Jiaze Cai,Faris Tulbah,Fei Gao,Chao Yu,Zhongyu Li,Yi Wu,Koushil Sreenath*

Key words: 多智能体强化学习, 机器人足球, 四足机器人, 分层学习, 自主协作

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一种分层多智能体强化学习框架，用于实现四足机器人在足球比赛中的完全自主和分散协作。

Motivation: 研究协调团队协作在动态、竞争和多智能体交互环境中的应用，以机器人足球为测试平台。

Method: 采用分层多智能体强化学习框架，包括低层技能（如行走、运球、射门）和高层战略规划（基于MAPPO和FSP）。

Result: 学习框架能适应多样化的对手策略，产生复杂的团队行为，如协作传球和动态角色分配，并在真实四足机器人中实现自主比赛。

Conclusion: 提出的方法在多智能体足球合作与竞争中表现出显著优势，支持自主机器人间及机器人与人类的比赛。

Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.

</details>


### [997] [Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements](https://arxiv.org/abs/2505.13837)
*Gokul Puthumanaillam,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Melkior Ornik*

Key words: 机器人导航,不确定性管理,强化学习,任务特定,TSUMs

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了GUIDE框架，通过任务特定不确定性地图（TSUMs）将任务需求融入导航策略，显著提升机器人在复杂环境中的性能。

Motivation: 机器人在复杂环境中导航时面临传感器噪声、环境变化和信息不全等问题，不同任务在不同区域对精度的需求不同。

Method: GUIDE框架通过TSUMs为不同位置分配可接受的不确定性水平，结合强化学习自动学习平衡任务完成和不确定性管理的策略。

Result: 实验表明，GUIDE在真实环境中的性能优于缺乏任务特定不确定性管理的方法。

Conclusion: GUIDE框架通过任务需求与不确定性管理的结合，有效提升了机器人在复杂环境中的导航能力。

Abstract: Robots navigating complex environments must manage uncertainty from sensor
noise, environmental changes, and incomplete information, with different tasks
requiring varying levels of precision in different areas. For example, precise
localization may be crucial near obstacles but less critical in open spaces. We
present GUIDE (Generalized Uncertainty Integration for Decision-Making and
Execution), a framework that integrates these task-specific requirements into
navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning
acceptable uncertainty levels to different locations, TSUMs enable robots to
adapt uncertainty management based on context. When combined with reinforcement
learning, GUIDE learns policies that balance task completion and uncertainty
management without extensive reward engineering. Real-world tests show
significant performance gains over methods lacking task-specific uncertainty
awareness.

</details>


### [998] [Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving](https://arxiv.org/abs/2505.13872)
*Jingzheng Li,Tiancheng Wang,Xingyu Peng,Jiacheng Chen,Zhijun Chen,Bing Li,Xianglong Liu*

Key words: 自动驾驶, 功能安全, 场景库, 测试框架, Safety2Drive

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: Safety2Drive是一个安全关键场景库，旨在评估自动驾驶系统的功能安全，填补现有数据集在法规合规性和实际事故场景覆盖上的不足。

Motivation: 现有自动驾驶数据集缺乏法规合规的场景库和实际事故场景，导致对自动驾驶系统功能安全的评估不足，存在安全验证和实际部署的风险。

Method: 提出Safety2Drive，涵盖70项标准法规测试项目，支持安全威胁注入（如自然环境干扰和对抗攻击）和多维度评估（如目标检测和车道检测）。

Result: Safety2Drive为自动驾驶安全部署提供了一套从场景构建到验证的标准化测试框架。

Conclusion: Safety2Drive弥补了现有数据集的不足，为自动驾驶系统的功能安全评估提供了全面的场景库和测试方法。

Abstract: Autonomous Driving (AD) systems demand the high levels of safety assurance.
Despite significant advancements in AD demonstrated on open-source benchmarks
like Longest6 and Bench2Drive, existing datasets still lack
regulatory-compliant scenario libraries for closed-loop testing to
comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD
accidents are underrepresented in current driving datasets. This scarcity leads
to inadequate evaluation of AD performance, posing risks to safety validation
and practical deployment. To address these challenges, we propose Safety2Drive,
a safety-critical scenario library designed to evaluate AD systems.
Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively
covers the test items required by standard regulations and contains 70 AD
function test items. (2) Safety2Drive supports the safety-critical scenario
generalization. It has the ability to inject safety threats such as natural
environment corruptions and adversarial attacks cross camera and LiDAR sensors.
(3) Safety2Drive supports multi-dimensional evaluation. In addition to the
evaluation of AD systems, it also supports the evaluation of various perception
tasks, such as object detection and lane detection. Safety2Drive provides a
paradigm from scenario construction to validation, establishing a standardized
test framework for the safe deployment of AD.

</details>


### [999] [APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight](https://arxiv.org/abs/2505.13921)
*Wanjing Huang,Weixiang Yan,Zhen Zhang,Ambuj Singh*

Key words: LLMs, 物理推理, 任务规划, APEX, 动态交互

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: APEX框架通过物理驱动的预见能力增强了LLMs的实时任务规划能力，显著优于传统LLMs和VLM模型。

Motivation: 研究旨在解决LLMs在物理交互建模方面的局限性，通过物理推理提升其在现实任务中的表现。

Method: APEX框架构建结构化图模型动态交互，提供物理状态更新和低延迟前向模拟，优化决策。

Result: 在物理学推理、Tetris和动态避障三个基准测试中，APEX显著优于现有模型。

Conclusion: 明确的物理推理对于连接语言智能和现实任务执行至关重要。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .

</details>


### [1000] [Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World](https://arxiv.org/abs/2505.13969)
*Junya Nakanishi,Jun Baba,Yuichiro Yoshikawa,Hiroko Kamide,Hiroshi Ishiguro*

Key words: 全局工作空间理论, 选择-广播循环, 人工智能, 机器人, 动态适应

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文探讨了全局工作空间理论（GWT）中提出的选择-广播循环结构的功能优势，尤其关注其在动态实时场景中对人工智能和机器人的适用性。

Motivation: 研究动机是弥补以往研究中独立分析选择和广播过程的不足，强调其循环结构的综合优势。

Method: 研究方法主要集中在分析选择和广播过程的循环结构，并探讨其三大主要优势。

Result: 研究结果表明，该结构能够支持动态思维适应、基于经验的适应和即时实时适应，适用于复杂、动态的决策场景。

Conclusion: 结论是GWT作为一种认知架构，在无监督动态环境中具有潜力，为开发通用AI和机器人系统提供了新方向。

Abstract: This paper discusses the functional advantages of the Selection-Broadcast
Cycle structure proposed by Global Workspace Theory (GWT), inspired by human
consciousness, particularly focusing on its applicability to artificial
intelligence and robotics in dynamic, real-time scenarios. While previous
studies often examined the Selection and Broadcast processes independently,
this research emphasizes their combined cyclic structure and the resulting
benefits for real-time cognitive systems. Specifically, the paper identifies
three primary benefits: Dynamic Thinking Adaptation, Experience-Based
Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's
potential as a cognitive architecture suitable for sophisticated
decision-making and adaptive performance in unsupervised, dynamic environments.
It suggests new directions for the development and implementation of robust,
general-purpose AI and robotics systems capable of managing complex, real-world
tasks.

</details>


### [1001] [Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures](https://arxiv.org/abs/2505.13556)
*Yiru Jiao,Simeon C. Calvert,Sander van Cranenburgh,Hans van Lint*

Key words: 交通安全, 通用替代安全度量, 神经网络, 极端值理论

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种新型的通用替代安全度量(GSSM)，通过从自然驾驶数据中学习，无需碰撞或风险标签，能够量化交通交互中的碰撞风险。该方法在多数据集上表现优异，优于现有基线。

Motivation: 现有方法在高度互动的城市交通中难以准确及时地预测碰撞风险，且通常依赖人工标注或特定场景设计。为解决这些问题，研究提出了GSSM。

Method: GSSM通过神经网络学习正常驾驶的模式，基于极端值理论计算上下文自适应的风险评分，可用于多种交互类型。

Result: 在多个公开数据集上测试，GSSM在4,875个真实碰撞和接近碰撞事件中表现优异，AUPRC为0.9，提前预警中位数达2.6秒。

Conclusion: GSSM为交通交互中的碰撞风险提供了可扩展、上下文感知和通用的量化基础，显著优于现有方法。

Abstract: Accurate and timely alerts for drivers or automated systems to unfolding
collisions remains a challenge in road safety, particularly in highly
interactive urban traffic. Existing approaches require labour-intensive
annotation of sparse risk, struggle to consider varying interaction context, or
are useful only in the scenarios they are designed for. To address these
limits, this study introduces the generalised surrogate safety measure (GSSM),
a new approach that learns exclusively from naturalistic driving without crash
or risk labels. GSSM captures the patterns of normal driving and estimates the
extent to which a traffic interaction deviates from the norm towards unsafe
extreme. Utilising neural networks, normal interactions are characterised by
context-conditioned distributions of multi-directional spacing between road
users. In the same interaction context, a spacing closer than normal entails
higher risk of potential collision. Then a context-adaptive risk score and its
associated probability can be calculated based on the theory of extreme values.
Any measurable factors, such as motion kinematics, weather, lighting, can serve
as part of the context, allowing for diverse coverage of safety-critical
interactions. Multiple public driving datasets are used to train GSSMs, which
are tested with 4,875 real-world crashes and near-crashes reconstructed from
the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of
0.9 and secures a median time advance of 2.6 seconds to prevent potential
collisions. Additional data and contextual factors provide further performance
gains. Across various interaction types such as rear-end, merging, and
crossing, the accuracy and timeliness of GSSM consistently outperforms existing
baselines. GSSM therefore establishes a scalable, context-aware, and
generalisable foundation to proactively quantify collision risk in traffic
interactions.

</details>


### [1002] [Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction](https://arxiv.org/abs/2505.13889)
*Yiting Zhang,Shichen Li*

Key words: 可变形线性物体, 安全运动规划, 实时轨迹优化, 多项式zonotopes

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了一种用于可变形线性物体（DLOs）的安全运动规划与控制框架，通过联合预测形状和张力，并在实时轨迹优化中确保安全性。

Motivation: 现有模型多关注形状预测而忽略接触和张力约束，可能导致物体和机器人损伤，亟需一种安全的DLO操作方法。

Method: 结合预测DLO未来形状和张力的模型，利用多项式zonotopes进行实时轨迹优化，确保全程安全约束。

Result: 在模拟线束装配任务中，相比现有方法，成功率高且无安全违规，实现了安全可靠的DLO操作。

Conclusion: 该方法在接触丰富环境中实现了DLO的强力且安全的操控。

Abstract: Manipulating deformable linear objects (DLOs) is challenging due to their
complex dynamics and the need for safe interaction in contact-rich
environments. Most existing models focus on shape prediction alone and fail to
account for contact and tension constraints, which can lead to damage to both
the DLO and the robot. In this work, we propose a certifiably safe motion
planning and control framework for DLO manipulation. At the core of our method
is a predictive model that jointly estimates the DLO's future shape and
tension. These predictions are integrated into a real-time trajectory optimizer
based on polynomial zonotopes, allowing us to enforce safety constraints
throughout the execution. We evaluate our framework on a simulated wire harness
assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,
our approach achieves a higher task success rate while avoiding all safety
violations. The results demonstrate that our method enables robust and safe DLO
manipulation in contact-rich environments.

</details>


### [1003] [Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning](https://arxiv.org/abs/2505.13925)
*Yunpeng Jiang,Jianshu Hu,Paul Weng,Yutong Ban*

Key words: 时间反转对称性; 深度强化学习; 轨迹反转; 奖励塑造; 样本效率

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种利用时间反转对称性增强深度强化学习（TR-DRL）的方法，通过轨迹反转增强和奖励塑造改进样本效率和性能。

Motivation: 现有方法主要关注空间对称性，忽视了时间对称性。论文旨在探索时间反转对称性在机器人任务中的应用。

Method: TR-DRL结合轨迹反转增强和奖励塑造，通过识别动力学一致的可逆过渡来扩充训练数据，并通过反向任务的轨迹指导学习。

Result: 实验表明，TR-DRL在单任务和多任务设置中比基线方法更高效且性能更强。

Conclusion: 时间反转对称性在强化学习中具有潜力，TR-DRL框架能显著提升样本效率和最终性能。

Abstract: Symmetry is pervasive in robotics and has been widely exploited to improve
sample efficiency in deep reinforcement learning (DRL). However, existing
approaches primarily focus on spatial symmetries, such as reflection, rotation,
and translation, while largely neglecting temporal symmetries. To address this
gap, we explore time reversal symmetry, a form of temporal symmetry commonly
found in robotics tasks such as door opening and closing. We propose Time
Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework
that combines trajectory reversal augmentation and time reversal guided reward
shaping to efficiently solve temporally symmetric tasks. Our method generates
reversed transitions from fully reversible transitions, identified by a
proposed dynamics-consistent filter, to augment the training data. For
partially reversible transitions, we apply reward shaping to guide learning,
according to successful trajectories from the reversed task. Extensive
experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL
is effective in both single-task and multi-task settings, achieving higher
sample efficiency and stronger final performance compared to baseline methods.

</details>


### [1004] [NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/abs/2505.14526)
*Matteo El-Hariry,Antoine Richard,Ricard M. Castan,Luis F. W. Batista,Matthieu Geist,Cedric Pradalier,Miguel Olivares-Mendez*

Key words: 导航基准测试、强化学习、跨介质、模块化设计、仿真到现实

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了NavBench，一个跨多种机器人和操作环境的多域基准测试，用于训练和评估基于强化学习的导航策略。

Motivation: 现有基准测试通常局限于特定平台，限制了不同移动系统间的泛化和公平比较。

Method: 基于IsaacLab构建的标准化任务定义框架，支持跨介质统一测试、模块化设计和仿真到现实的验证。

Result: 成功将策略迁移到多种真实机器人上，证明了框架的有效性和适应性。

Conclusion: NavBench简化了适应性导航策略的开发，模块化设计使其易于集成自定义任务和机器人。

Abstract: Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.

</details>
