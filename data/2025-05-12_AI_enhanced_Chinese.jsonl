{"id": "2505.05583", "pdf": "https://arxiv.org/pdf/2505.05583", "abs": "https://arxiv.org/abs/2505.05583", "authors": ["Qianbo Zang", "Christophe Zgrzendek", "Igor Tchappi", "Afshin Khadangi", "Johannes Sedlmeir"], "title": "KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "Hierarchical Text Classification (HTC) involves assigning documents to labels\norganized within a taxonomy. Most previous research on HTC has focused on\nsupervised methods. However, in real-world scenarios, employing supervised HTC\ncan be challenging due to a lack of annotated data. Moreover, HTC often faces\nissues with large label spaces and long-tail distributions. In this work, we\npresent Knowledge Graphs for zero-shot Hierarchical Text Classification\n(KG-HTC), which aims to address these challenges of HTC in applications by\nintegrating knowledge graphs with Large Language Models (LLMs) to provide\nstructured semantic context during classification. Our method retrieves\nrelevant subgraphs from knowledge graphs related to the input text using a\nRetrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to\nunderstand label semantics at various hierarchy levels. We evaluate KG-HTC on\nthree open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental\nresults show that KG-HTC significantly outperforms three baselines in the\nstrict zero-shot setting, particularly achieving substantial improvements at\ndeeper levels of the hierarchy. This evaluation demonstrates the effectiveness\nof incorporating structured knowledge into LLMs to address HTC's challenges in\nlarge label spaces and long-tailed label distributions. Our code is available\nat: https://github.com/QianboZang/KG-HTC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86KG-HTC\u65b9\u6cd5\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89e3\u51b3\u5c42\u6b21\u5316\u6587\u672c\u5206\u7c7b\uff08HTC\uff09\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u3001\u5927\u6807\u7b7e\u7a7a\u95f4\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\u3002\u901a\u8fc7RAG\u65b9\u6cd5\u68c0\u7d22\u76f8\u5173\u5b50\u56fe\uff0c\u589e\u5f3aLLMs\u5bf9\u6807\u7b7e\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14HTC\u9762\u4e34\u5927\u6807\u7b7e\u7a7a\u95f4\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e3aLLMs\u63d0\u4f9b\u7ed3\u6784\u5316\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u4ee5\u6539\u8fdb\u5206\u7c7b\u6548\u679c\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u4e0e\u8f93\u5165\u6587\u672c\u76f8\u5173\u7684\u5b50\u56fe\uff0c\u7ed3\u5408LLMs\u8fdb\u884c\u5c42\u6b21\u5316\u5206\u7c7b\u3002", "result": "\u5728WoS\u3001DBpedia\u548cAmazon\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cKG-HTC\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6df1\u5c42\u5c42\u6b21\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLMs\u80fd\u6709\u6548\u89e3\u51b3HTC\u4e2d\u7684\u5927\u6807\u7b7e\u7a7a\u95f4\u548c\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.05648", "pdf": "https://arxiv.org/pdf/2505.05648", "abs": "https://arxiv.org/abs/2505.05648", "authors": ["Abdelrahman Abouelenin", "Mohamed Abdelrehim", "Raffy Fahim", "Amr Hendy", "Mohamed Afify"], "title": "Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In this paper we train a transformer using differential privacy (DP) for\nlanguage modeling in SwiftKey. We run multiple experiments to balance the\ntrade-off between the model size, run-time speed and accuracy. We show that we\nget small and consistent gains in the next-word-prediction and accuracy with\ngraceful increase in memory and speed compared to the production GRU. This is\nobtained by scaling down a GPT2 architecture to fit the required size and a two\nstage training process that builds a seed model on general data and DP\nfinetunes it on typing data. The transformer is integrated using ONNX offering\nboth flexibility and efficiency.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5728SwiftKey\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u7684transformer\u8bed\u8a00\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u8fd0\u884c\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5185\u5b58\u548c\u901f\u5ea6\u65b9\u9762\u76f8\u6bd4\u751f\u4ea7\u7ea7GRU\u7684\u6539\u8fdb\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u4e2a\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u624b\u673a\u8f93\u5165\u6cd5\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u901a\u8fc7\u7f29\u5c0fGPT2\u67b6\u6784\u9002\u5e94\u624b\u673a\u8f93\u5165\u6cd5\u5927\u5c0f\uff0c\u5e76\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u901a\u7528\u6570\u636e\u9884\u8bad\u7ec3\u548c\u5dee\u5206\u9690\u79c1\u5fae\u8c03\uff0c\u6700\u540e\u4f7f\u7528ONNX\u96c6\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e0b\u4e00\u8bcd\u9884\u6d4b\u548c\u51c6\u786e\u6027\u4e0a\u53d6\u5f97\u5c0f\u5e45\u4f46\u4e00\u81f4\u7684\u63d0\u5347\uff0c\u540c\u65f6\u5185\u5b58\u548c\u901f\u5ea6\u6d88\u8017\u589e\u957f\u8f83\u4e3a\u5e73\u7f13\u3002", "conclusion": "\u8bc1\u660e\u4e86\u7f29\u5c0f\u7248transformer\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u5728\u624b\u673a\u8f93\u5165\u6cd5\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4f18\u4e8e\u73b0\u6709GRU\u6a21\u578b\u3002"}}
{"id": "2505.05687", "pdf": "https://arxiv.org/pdf/2505.05687", "abs": "https://arxiv.org/abs/2505.05687", "authors": ["Cindy Kim", "Daniela Puchall", "Jiangyi Liang", "Jiwon Kim"], "title": "Exploration of COVID-19 Discourse on Twitter: American Politician Edition", "categories": ["cs.CL"], "comment": null, "summary": "The advent of the COVID-19 pandemic has undoubtedly affected the political\nscene worldwide and the introduction of new terminology and public opinions\nregarding the virus has further polarized partisan stances. Using a collection\nof tweets gathered from leading American political figures online (Republican\nand Democratic), we explored the partisan differences in approach, response,\nand attitude towards handling the international crisis. Implementation of the\nbag-of-words, bigram, and TF-IDF models was used to identify and analyze\nkeywords, topics, and overall sentiments from each party. Results suggest that\nDemocrats are more concerned with the casualties of the pandemic, and give more\nmedical precautions and recommendations to the public whereas Republicans are\nmore invested in political responsibilities such as keeping the public updated\nthrough media and carefully watching the progress of the virus. We propose a\nsystematic approach to predict and distinguish a tweet's political stance (left\nor right leaning) based on its COVID-19 related terms using different\nclassification algorithms on different language models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86COVID-19\u5927\u6d41\u884c\u671f\u95f4\u7f8e\u56fd\u4e24\u5927\u653f\u515a\uff08\u5171\u548c\u515a\u4e0e\u6c11\u4e3b\u515a\uff09\u5728\u63a8\u7279\u4e0a\u7684\u8a00\u8bba\u5dee\u5f02\uff0c\u901a\u8fc7\u6587\u672c\u5206\u6790\u6280\u672f\u63ed\u793a\u4e86\u4ed6\u4eec\u5728\u6001\u5ea6\u548c\u5e94\u5bf9\u65b9\u5f0f\u4e0a\u7684\u4e0d\u540c\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u653f\u6cbb\u7acb\u573a\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3COVID-19\u5927\u6d41\u884c\u5982\u4f55\u52a0\u5267\u4e86\u7f8e\u56fd\u4e24\u515a\u7684\u653f\u6cbb\u6781\u5316\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u5206\u6790\u4ed6\u4eec\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u8a00\u8bba\u5dee\u5f02\u3002", "method": "\u4f7f\u7528bag-of-words\u3001bigram\u548cTF-IDF\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4e24\u515a\u653f\u5ba2\u7684\u63a8\u6587\u5173\u952e\u8bcd\u3001\u4e3b\u9898\u548c\u60c5\u611f\uff0c\u8fdb\u800c\u6784\u5efa\u5206\u7c7b\u7b97\u6cd5\u9884\u6d4b\u63a8\u6587\u7684\u653f\u6cbb\u7acb\u573a\u3002", "result": "\u6c11\u4e3b\u515a\u66f4\u5173\u6ce8\u75ab\u60c5\u4f24\u4ea1\u548c\u533b\u7597\u5efa\u8bae\uff0c\u800c\u5171\u548c\u515a\u66f4\u6ce8\u91cd\u653f\u6cbb\u8d23\u4efb\u5982\u901a\u8fc7\u5a92\u4f53\u66f4\u65b0\u4fe1\u606f\u548c\u76d1\u63a7\u75c5\u6bd2\u8fdb\u5c55\u3002\u5206\u7c7b\u7b97\u6cd5\u80fd\u6709\u6548\u533a\u5206\u63a8\u6587\u7684\u653f\u6cbb\u7acb\u573a\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u75ab\u60c5\u52a0\u5267\u4e86\u4e24\u515a\u7684\u8a00\u8bba\u5dee\u5f02\uff0c\u63d0\u51fa\u7684\u5206\u7c7b\u65b9\u6cd5\u6709\u52a9\u4e8e\u7406\u89e3\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u653f\u6cbb\u6781\u5316\u73b0\u8c61\u3002"}}
{"id": "2505.05704", "pdf": "https://arxiv.org/pdf/2505.05704", "abs": "https://arxiv.org/abs/2505.05704", "authors": ["Julia Shuieh", "Prasann Singhal", "Apaar Shanker", "John Heyer", "George Pu", "Samuel Denton"], "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR '25 Workshop on Spurious Correlation and Shortcut Learning", "summary": "Supervised and preference-based fine-tuning techniques have become popular\nfor aligning large language models (LLMs) with user intent and correctness\ncriteria. However, real-world training data often exhibits spurious\ncorrelations -- arising from biases, dataset artifacts, or other \"shortcut\"\nfeatures -- that can compromise a model's performance or generalization. In\nthis paper, we systematically evaluate three post-training algorithms --\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO\n(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and\nspuriousness conditions. Our tasks span mathematical reasoning, constrained\ninstruction-following, and document-grounded question answering. We vary the\ndegree of spurious correlation (10% vs. 90%) and investigate two forms of\nartifacts: \"Feature Ambiguity\" and \"Distributional Narrowness.\" Our results\nshow that the models often but not always degrade under higher spuriousness.\nThe preference-based methods (DPO/KTO) can demonstrate relative robustness in\nmathematical reasoning tasks. By contrast, SFT maintains stronger performance\nin complex, context-intensive tasks. These findings highlight that no single\npost-training strategy universally outperforms in all scenarios; the best\nchoice depends on the type of target task and the nature of spurious\ncorrelations.", "AI": {"tldr": "\u6587\u7ae0\u6bd4\u8f83\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u548c\u5361\u5c3c\u66fc-\u7279\u6c83\u65af\u57fa\u4f18\u5316\uff08KTO\uff09\u5728\u865a\u5047\u76f8\u5173\u6027\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u5404\u6709\u4f18\u52a3\u3002", "motivation": "\u7531\u4e8e\u5b9e\u9645\u8bad\u7ec3\u6570\u636e\u4e2d\u5b58\u5728\u865a\u5047\u76f8\u5173\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6216\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5fae\u8c03\u7b97\u6cd5\u5728\u865a\u5047\u76f8\u5173\u6027\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u6307\u4ee4\u9075\u5faa\u548c\u6587\u6863\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\uff0c\u6bd4\u8f83\u4e86SFT\u3001DPO\u548cKTO\u7b97\u6cd5\uff0c\u5e76\u8bbe\u7f6e\u4e86\u4e0d\u540c\u865a\u5047\u76f8\u5173\u6027\u7a0b\u5ea6\uff0810% vs. 90%\uff09\u548c\u4e24\u79cd\u865a\u5047\u7279\u5f81\u7c7b\u578b\uff08\u201c\u7279\u5f81\u6a21\u7cca\u6027\u201d\u548c\u201c\u5206\u5e03\u72ed\u7a84\u6027\u201d\uff09\u3002", "result": "\u504f\u597d\u65b9\u6cd5\uff08DPO/KTO\uff09\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u76f8\u5bf9\u7a33\u5065\uff0c\u800cSFT\u5728\u590d\u6742\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u6027\u80fd\u66f4\u5f3a\u3002", "conclusion": "\u6ca1\u6709\u4e00\u79cd\u540e\u8bad\u7ec3\u7b56\u7565\u5728\u6240\u6709\u573a\u666f\u4e0b\u90fd\u6700\u4f18\uff0c\u6700\u4f73\u9009\u62e9\u53d6\u51b3\u4e8e\u76ee\u6807\u4efb\u52a1\u7c7b\u578b\u548c\u865a\u5047\u76f8\u5173\u6027\u7684\u6027\u8d28\u3002"}}
{"id": "2505.05541", "pdf": "https://arxiv.org/pdf/2505.05541", "abs": "https://arxiv.org/abs/2505.05541", "authors": ["Markov Grey", "Charbel-Rapha\u00ebl Segerie"], "title": "Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods", "categories": ["cs.AI"], "comment": null, "summary": "As frontier AI systems advance toward transformative capabilities, we need a\nparallel transformation in how we measure and evaluate these systems to ensure\nsafety and inform governance. While benchmarks have been the primary method for\nestimating model capabilities, they often fail to establish true upper bounds\nor predict deployment behavior. This literature review consolidates the rapidly\nevolving field of AI safety evaluations, proposing a systematic taxonomy around\nthree dimensions: what properties we measure, how we measure them, and how\nthese measurements integrate into frameworks. We show how evaluations go beyond\nbenchmarks by measuring what models can do when pushed to the limit\n(capabilities), the behavioral tendencies exhibited by default (propensities),\nand whether our safety measures remain effective even when faced with\nsubversive adversarial AI (control). These properties are measured through\nbehavioral techniques like scaffolding, red teaming and supervised fine-tuning,\nalongside internal techniques such as representation analysis and mechanistic\ninterpretability. We provide deeper explanations of some safety-critical\ncapabilities like cybersecurity exploitation, deception, autonomous\nreplication, and situational awareness, alongside concerning propensities like\npower-seeking and scheming. The review explores how these evaluation methods\nintegrate into governance frameworks to translate results into concrete\ndevelopment decisions. We also highlight challenges to safety evaluations -\nproving absence of capabilities, potential model sandbagging, and incentives\nfor \"safetywashing\" - while identifying promising research directions. By\nsynthesizing scattered resources, this literature review aims to provide a\ncentral reference point for understanding AI safety evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u8baeAI\u5b89\u5168\u8bc4\u4f30\u9886\u57df\u7684\u7cfb\u7edf\u5206\u7c7b\uff0c\u5f3a\u8c03\u8d85\u8d8a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5173\u6ce8\u80fd\u529b\u4e0a\u9650\u3001\u884c\u4e3a\u503e\u5411\u548c\u63a7\u5236\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u65b9\u6cd5\u53ca\u6311\u6218\u4e0e\u7814\u7a76\u673a\u9047\u3002", "motivation": "\u968f\u7740AI\u524d\u6cbf\u7cfb\u7edf\u80fd\u529b\u63d0\u5347\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u786e\u4fdd\u5b89\u5168\u5e76\u6307\u5bfc\u6cbb\u7406\uff0c\u5f25\u8865\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u884c\u4e3a\u6280\u672f\uff08\u5982scaffolding\u3001\u7ea2\u961f\u6d4b\u8bd5\uff09\u548c\u5185\u90e8\u6280\u672f\uff08\u5982\u8868\u5f81\u5206\u6790\u3001\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff09\u8bc4\u4f30\u80fd\u529b\u3001\u503e\u5411\u548c\u63a7\u5236\u3002", "result": "\u63d0\u51fa\u4e09\u7ef4\u5ea6\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u5173\u952e\u5b89\u5168\u80fd\u529b\uff08\u5982\u7f51\u7edc\u653b\u51fb\u3001\u6b3a\u9a97\uff09\u548c\u884c\u4e3a\u503e\u5411\uff08\u5982\u6743\u529b\u5bfb\u6c42\uff09\uff0c\u5e76\u6574\u5408\u5230\u6cbb\u7406\u6846\u67b6\u4e2d\u3002", "conclusion": "\u4e3aAI\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u7cfb\u7edf\u53c2\u8003\uff0c\u5f3a\u8c03\u9700\u89e3\u51b3\u80fd\u529b\u8bc1\u660e\u7f3a\u5931\u3001\u6a21\u578b\u201c\u61c8\u6020\u201d\u7b49\u6311\u6218\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05522", "pdf": "https://arxiv.org/pdf/2505.05522", "abs": "https://arxiv.org/abs/2505.05522", "authors": ["Luke Darlow", "Ciaran Regan", "Sebastian Risi", "Jeffrey Seely", "Llion Jones"], "title": "Continuous Thought Machines", "categories": ["cs.LG", "cs.AI"], "comment": "Technical report accompanied by online project page", "summary": "Biological brains demonstrate complex neural activity, where the timing and\ninterplay between neurons is critical to how brains process information. Most\ndeep learning architectures simplify neural activity by abstracting away\ntemporal dynamics. In this paper we challenge that paradigm. By incorporating\nneuron-level processing and synchronization, we can effectively reintroduce\nneural timing as a foundational element. We present the Continuous Thought\nMachine (CTM), a model designed to leverage neural dynamics as its core\nrepresentation. The CTM has two core innovations: (1) neuron-level temporal\nprocessing, where each neuron uses unique weight parameters to process a\nhistory of incoming signals; and (2) neural synchronization employed as a\nlatent representation. The CTM aims to strike a balance between oversimplified\nneuron abstractions that improve computational efficiency, and biological\nrealism. It operates at a level of abstraction that effectively captures\nessential temporal dynamics while remaining computationally tractable for deep\nlearning. We demonstrate the CTM's strong performance and versatility across a\nrange of challenging tasks, including ImageNet-1K classification, solving 2D\nmazes, sorting, parity computation, question-answering, and RL tasks. Beyond\ndisplaying rich internal representations and offering a natural avenue for\ninterpretation owing to its internal process, the CTM is able to perform tasks\nthat require complex sequential reasoning. The CTM can also leverage adaptive\ncompute, where it can stop earlier for simpler tasks, or keep computing when\nfaced with more challenging instances. The goal of this work is to share the\nCTM and its associated innovations, rather than pushing for new\nstate-of-the-art results. To that end, we believe the CTM represents a\nsignificant step toward developing more biologically plausible and powerful\nartificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aCTM\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u795e\u7ecf\u5143\u7ea7\u65f6\u95f4\u5904\u7406\u548c\u795e\u7ecf\u540c\u6b65\u6765\u6a21\u62df\u751f\u7269\u5927\u8111\u7684\u65f6\u5e8f\u52a8\u6001\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u751f\u7269\u771f\u5b9e\u6027\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6311\u6218\u6df1\u5ea6\u5b66\u4e60\u5ffd\u7565\u795e\u7ecf\u65f6\u5e8f\u52a8\u6001\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u751f\u7269\u795e\u7ecf\u6d3b\u52a8\u63d0\u5347\u6a21\u578b\u7684\u751f\u7269\u771f\u5b9e\u6027\u548c\u8868\u73b0\u529b\u3002", "method": "\u63d0\u51faCTM\u6a21\u578b\uff0c\u6838\u5fc3\u521b\u65b0\u4e3a\u795e\u7ecf\u5143\u7ea7\u65f6\u95f4\u5904\u7406\u548c\u795e\u7ecf\u540c\u6b65\u4f5c\u4e3a\u6f5c\u5728\u8868\u5f81\u3002", "result": "CTM\u5728ImageNet\u5206\u7c7b\u3001\u8ff7\u5bab\u6c42\u89e3\u3001\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u81ea\u9002\u5e94\u8ba1\u7b97\u3002", "conclusion": "CTM\u662f\u5411\u751f\u7269\u5408\u7406\u4e14\u5f3a\u5927AI\u7cfb\u7edf\u8fc8\u8fdb\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4f46\u76ee\u6807\u662f\u5206\u4eab\u521b\u65b0\u800c\u975e\u8ffd\u6c42\u65b0SOTA\u3002"}}
{"id": "2505.05714", "pdf": "https://arxiv.org/pdf/2505.05714", "abs": "https://arxiv.org/abs/2505.05714", "authors": ["Jinze Lv", "Jian Chen", "Zi Long", "Xianghua Fu", "Yin Chen"], "title": "TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries", "categories": ["cs.CL"], "comment": "NLDB 2025", "summary": "Most existing multimodal machine translation (MMT) datasets are predominantly\ncomposed of static images or short video clips, lacking extensive video data\nacross diverse domains and topics. As a result, they fail to meet the demands\nof real-world MMT tasks, such as documentary translation. In this study, we\ndeveloped TopicVD, a topic-based dataset for video-supported multimodal machine\ntranslation of documentaries, aiming to advance research in this field. We\ncollected video-subtitle pairs from documentaries and categorized them into\neight topics, such as economy and nature, to facilitate research on domain\nadaptation in video-guided MMT. Additionally, we preserved their contextual\ninformation to support research on leveraging the global context of\ndocumentaries in video-guided MMT. To better capture the shared semantics\nbetween text and video, we propose an MMT model based on a cross-modal\nbidirectional attention module. Extensive experiments on the TopicVD dataset\ndemonstrate that visual information consistently improves the performance of\nthe NMT model in documentary translation. However, the MMT model's performance\nsignificantly declines in out-of-domain scenarios, highlighting the need for\neffective domain adaptation methods. Additionally, experiments demonstrate that\nglobal context can effectively improve translation performance. % Dataset and\nour implementations are available at https://github.com/JinzeLv/TopicVD", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86TopicVD\u6570\u636e\u96c6\uff0c\u652f\u6301\u57fa\u4e8e\u4e3b\u9898\u7684\u89c6\u9891\u591a\u6a21\u6001\u673a\u5668\u7ffb\u8bd1\uff08MMT\uff09\uff0c\u5e76\u5728\u6587\u6863\u7ffb\u8bd1\u4e2d\u9a8c\u8bc1\u4e86\u89c6\u89c9\u4fe1\u606f\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5bf9\u7ffb\u8bd1\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709MMT\u6570\u636e\u96c6\u591a\u4e3a\u9759\u6001\u56fe\u50cf\u6216\u77ed\u89c6\u9891\u7247\u6bb5\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u4efb\u52a1\uff08\u5982\u7eaa\u5f55\u7247\u7ffb\u8bd1\uff09\u7684\u9700\u6c42\u3002\u7814\u7a76\u5e0c\u671b\u901a\u8fc7TopicVD\u6570\u636e\u96c6\u63a8\u52a8\u89c6\u9891\u5f15\u5bfc\u7684MMT\u7814\u7a76\u3002", "method": "\u6536\u96c6\u7eaa\u5f55\u7247\u89c6\u9891-\u5b57\u5e55\u5bf9\u5e76\u6309\u4e3b\u9898\u5206\u7c7b\uff0c\u63d0\u51fa\u57fa\u4e8e\u8de8\u6a21\u6001\u53cc\u5411\u6ce8\u610f\u529b\u6a21\u5757\u7684MMT\u6a21\u578b\u3002", "result": "\u89c6\u89c9\u4fe1\u606f\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\uff0c\u4f46\u6a21\u578b\u5728\u9886\u57df\u5916\u573a\u666f\u8868\u73b0\u4e0b\u964d\uff0c\u5168\u5c40\u4e0a\u4e0b\u6587\u5219\u6709\u6548\u6539\u5584\u7ffb\u8bd1\u6548\u679c\u3002", "conclusion": "TopicVD\u4e3a\u89c6\u9891MMT\u7814\u7a76\u63d0\u4f9b\u65b0\u57fa\u51c6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u9886\u57df\u9002\u5e94\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5229\u7528\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.05602", "pdf": "https://arxiv.org/pdf/2505.05602", "abs": "https://arxiv.org/abs/2505.05602", "authors": ["Lennart Luettgau", "Harry Coppock", "Magda Dubois", "Christopher Summerfield", "Cozmin Ududec"], "title": "HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics", "categories": ["cs.AI", "stat.AP"], "comment": "23 pages, 9 figures", "summary": "As Large Language Models (LLMs) and other AI systems evolve, robustly\nestimating their capabilities from inherently stochastic outputs while\nsystematically quantifying uncertainty in these estimates becomes increasingly\nimportant. Further, advanced AI evaluations often have a nested hierarchical\nstructure, exhibit high levels of complexity, and come with high costs in\ntesting the most advanced AI systems. To address these challenges, we introduce\nHiBayES, a generalizable Hierarchical Bayesian modeling framework for AI\nEvaluation Statistics. HiBayES supports robust inferences in classical\nquestion-answer benchmarks and advanced agentic evaluations, particularly in\nlow-data scenarios (e.g., < 20 data points per evaluation). Built on\nGeneralized Linear Models (GLMs), Bayesian data analysis, and formal model\ncomparison, HiBayES provides principled uncertainty quantification and robust\nparameter estimation. This paper offers a comprehensive introduction to\nHiBayES, including illustrative examples, comparisons to conventional\nstatistical methods, and practical guidance for implementing multilevel\nBayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta\nversion) for out-of-the-box implementation.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86HiBayES\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5c42\u6b21\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3AI\u8bc4\u4f30\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u53d1\u5c55\uff0c\u8bc4\u4f30\u5176\u80fd\u529b\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u6027\u6210\u4e3a\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u7cfb\u7edf\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u8d1d\u53f6\u65af\u6a21\u578b\u548c\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b(GLM)\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u6570\u636e\u5206\u6790\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u63a8\u65ad\u548c\u53c2\u6570\u4f30\u8ba1\u3002", "result": "HiBayES\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86Beta\u7248\u8f6f\u4ef6\u5305\u3002", "conclusion": "HiBayES\u4e3aAI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u548c\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.05525", "pdf": "https://arxiv.org/pdf/2505.05525", "abs": "https://arxiv.org/abs/2505.05525", "authors": ["Selim Mecanna", "Aurore Loisy", "Christophe Eloy"], "title": "A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Navigating in a fluid flow while being carried by it, using only information\naccessible from on-board sensors, is a problem commonly faced by small\nplanktonic organisms. It is also directly relevant to autonomous robots\ndeployed in the oceans. In the last ten years, the fluid mechanics community\nhas widely adopted reinforcement learning, often in the form of its simplest\nimplementations, to address this challenge. But it is unclear how good are the\nstrategies learned by these algorithms. In this paper, we perform a\nquantitative assessment of reinforcement learning methods applied to navigation\nin partially observable flows. We first introduce a well-posed problem of\ndirectional navigation for which a quasi-optimal policy is known analytically.\nWe then report on the poor performance and robustness of commonly used\nalgorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered\nin the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and\ntwo-dimensional turbulence. We show that they are vastly surpassed by PPO\n(Proximal Policy Optimization), a more advanced algorithm that has established\ndominance across a wide range of benchmarks in the reinforcement learning\ncommunity. In particular, our custom implementation of PPO matches the\ntheoretical quasi-optimal performance in turbulent flow and does so in a robust\nmanner. Reaching this result required the use of several additional techniques,\nsuch as vectorized environments and generalized advantage estimation, as well\nas hyperparameter optimization. This study demonstrates the importance of\nalgorithm selection, implementation details, and fine-tuning for discovering\ntruly smart autonomous navigation strategies in complex flows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6d41\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u53d1\u73b0\u5e38\u7528\u7b97\u6cd5\uff08\u5982Q-Learning\u548cA2C\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u800cPPO\u7b97\u6cd5\u5219\u8868\u73b0\u51fa\u8272\uff0c\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5b9a\u91cf\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6d41\u4f53\u6d41\u52a8\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u89e3\u51b3\u5c0f\u578b\u6d6e\u6e38\u751f\u7269\u6216\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u6d77\u6d0b\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u7b97\u6cd5\u9009\u62e9\u4e0e\u4f18\u5316\u7684\u5173\u952e\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u65b9\u5411\u5bfc\u822a\u7684\u660e\u786e\u95ee\u9898\uff0c\u6bd4\u8f83Q-Learning\u3001A2C\u548cPPO\u5728\u591a\u79cd\u6d41\u52a8\uff08\u5982Taylor-Green\u6da1\u6d41\u548c\u4e8c\u7ef4\u6e4d\u6d41\uff09\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u91c7\u7528\u5411\u91cf\u5316\u73af\u5883\u548c\u5e7f\u4e49\u4f18\u52bf\u4f30\u8ba1\u7b49\u6280\u672f\u4f18\u5316PPO\u3002", "result": "PPO\u5728\u590d\u6742\u6d41\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8eQ-Learning\u548cA2C\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7b97\u6cd5\u9009\u62e9\u3001\u5b9e\u73b0\u7ec6\u8282\u548c\u8d85\u53c2\u6570\u4f18\u5316\u5bf9\u4e8e\u590d\u6742\u6d41\u4e2d\u7684\u667a\u80fd\u5bfc\u822a\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0cPPO\u662f\u5f53\u524d\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05755", "pdf": "https://arxiv.org/pdf/2505.05755", "abs": "https://arxiv.org/abs/2505.05755", "authors": ["Dhruvesh Patel", "Aishwarya Sahoo", "Avinash Amballa", "Tahira Naseem", "Tim G. J. Rudner", "Andrew McCallum"], "title": "Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Autoregressive models (ARMs), which predict subsequent tokens one-by-one\n``from left to right,'' have achieved significant success across a wide range\nof sequence generation tasks. However, they struggle to accurately represent\nsequences that require satisfying sophisticated constraints or whose sequential\ndependencies are better addressed by out-of-order generation. Masked Diffusion\nModels (MDMs) address some of these limitations, but the process of unmasking\nmultiple tokens simultaneously in MDMs can introduce incoherences, and MDMs\ncannot handle arbitrary infilling constraints when the number of tokens to be\nfilled in is not known in advance. In this work, we introduce Insertion\nLanguage Models (ILMs), which learn to insert tokens at arbitrary positions in\na sequence -- that is, they select jointly both the position and the vocabulary\nelement to be inserted. By inserting tokens one at a time, ILMs can represent\nstrong dependencies between tokens, and their ability to generate sequences in\narbitrary order allows them to accurately model sequences where token\ndependencies do not follow a left-to-right sequential structure. To train ILMs,\nwe propose a tailored network parameterization and use a simple denoising\nobjective. Our empirical evaluation demonstrates that ILMs outperform both ARMs\nand MDMs on common planning tasks. Furthermore, we show that ILMs outperform\nMDMs and perform on par with ARMs in an unconditional text generation task\nwhile offering greater flexibility than MDMs in arbitrary-length text\ninfilling.", "AI": {"tldr": "\u63d2\u5165\u8bed\u8a00\u6a21\u578b\uff08ILMs\uff09\u901a\u8fc7\u5728\u4efb\u610f\u4f4d\u7f6e\u63d2\u5165\u4ee4\u724c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u548c\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u548c\u63a9\u7801\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u6216\u975e\u987a\u5e8f\u4f9d\u8d56\u7684\u5e8f\u5217\u65f6\u5b58\u5728\u4e0d\u8db3\uff0cILMs\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u63d2\u5165\u8bed\u8a00\u6a21\u578b\uff08ILMs\uff09\uff0c\u901a\u8fc7\u8054\u5408\u9009\u62e9\u63d2\u5165\u4f4d\u7f6e\u548c\u8bcd\u6c47\u5143\u7d20\uff0c\u5e76\u91c7\u7528\u5b9a\u5236\u7684\u7f51\u7edc\u53c2\u6570\u5316\u548c\u53bb\u566a\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ILMs\u5728\u89c4\u5212\u4efb\u52a1\u4e0a\u4f18\u4e8eARMs\u548cMDMs\uff0c\u5728\u65e0\u6761\u4ef6\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u4e0eARMs\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u4efb\u610f\u957f\u5ea6\u6587\u672c\u586b\u5145\u4e2d\u6bd4MDMs\u66f4\u7075\u6d3b\u3002", "conclusion": "ILMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u987a\u5e8f\u548c\u7ea6\u675f\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u6a21\u578b\u7684\u80fd\u529b\u3002"}}
{"id": "2505.05612", "pdf": "https://arxiv.org/pdf/2505.05612", "abs": "https://arxiv.org/abs/2505.05612", "authors": ["Qing Wang", "Yining Pan", "Minghao Zhou", "Zijia Tang", "Yanfei Wang", "Guangyu Wang", "Qianqian Song"], "title": "scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "comment": "14 pages, 7 figures", "summary": "Drug resistance presents a major challenge in cancer therapy. Single cell\nprofiling offers insights into cellular heterogeneity, yet the application of\nlarge-scale foundation models for predicting drug response in single cell data\nremains underexplored. To address this, we developed scDrugMap, an integrated\nframework featuring both a Python command-line interface and a web server for\ndrug response prediction. scDrugMap evaluates a wide range of foundation\nmodels, including eight single-cell models and two large language models, using\na curated dataset of over 326,000 cells in the primary collection and 18,800\ncells in the validation set, spanning 36 datasets and diverse tissue and cancer\ntypes. We benchmarked model performance under pooled-data and cross-data\nevaluation settings, employing both layer freezing and Low-Rank Adaptation\n(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation\nachieved the best performance, with mean F1 scores of 0.971 (layer freezing)\nand 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.\nIn the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),\nwhile scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap\nprovides the first large-scale benchmark of foundation models for drug response\nprediction in single-cell data and serves as a user-friendly, flexible platform\nfor advancing drug discovery and translational research.", "AI": {"tldr": "scDrugMap\u662f\u4e00\u500b\u6574\u5408\u6846\u67b6\uff0c\u7528\u65bc\u55ae\u7d30\u80de\u6578\u64da\u4e2d\u7684\u85e5\u7269\u53cd\u61c9\u9810\u6e2c\uff0c\u8a55\u4f30\u4e86\u591a\u7a2e\u57fa\u790e\u6a21\u578b\u4e26\u63d0\u4f9b\u4e86\u53cb\u597d\u7684\u7528\u6236\u754c\u9762\u3002", "motivation": "\u85e5\u7269\u6297\u6027\u662f\u764c\u75c7\u6cbb\u7642\u7684\u4e3b\u8981\u6311\u6230\uff0c\u55ae\u7d30\u80de\u5206\u6790\u6709\u52a9\u65bc\u7406\u89e3\u7d30\u80de\u7570\u8cea\u6027\uff0c\u4f46\u5927\u898f\u6a21\u57fa\u790e\u6a21\u578b\u5728\u55ae\u7d30\u80de\u6578\u64da\u4e2d\u7684\u85e5\u7269\u53cd\u61c9\u9810\u6e2c\u61c9\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u958b\u767c\u4e86scDrugMap\u6846\u67b6\uff0c\u8a55\u4f30\u4e868\u7a2e\u55ae\u7d30\u80de\u6a21\u578b\u548c2\u7a2e\u5927\u578b\u8a9e\u8a00\u6a21\u578b\uff0c\u4f7f\u752832.6\u842c\u500b\u7d30\u80de\u7684\u4e3b\u8981\u6578\u64da\u96c6\u548c1.88\u842c\u500b\u7d30\u80de\u7684\u9a57\u8b49\u96c6\uff0c\u4e26\u63a1\u7528\u5c64\u51cd\u7d50\u548cLoRA\u5fae\u8abf\u7b56\u7565\u9032\u884c\u6a21\u578b\u6027\u80fd\u6bd4\u8f03\u3002", "result": "\u5728pooled-data\u60c5\u5883\u4e0b\uff0cscFoundation\u8868\u73fe\u6700\u4f73\uff08F1\u5206\u65780.971\uff09\uff1b\u5728cross-data\u60c5\u5883\u4e0b\uff0cUCE\u5fae\u8abf\u5f8c\u8868\u73fe\u6700\u597d\uff08F1\u5206\u65780.774\uff09\uff0c\u800cscGPT\u5728\u96f6\u6a23\u672c\u5b78\u7fd2\u4e2d\u9818\u5148\uff08F1\u5206\u65780.858\uff09\u3002", "conclusion": "scDrugMap\u9996\u6b21\u5927\u898f\u6a21\u6bd4\u8f03\u4e86\u57fa\u790e\u6a21\u578b\u5728\u55ae\u7d30\u80de\u85e5\u7269\u53cd\u61c9\u9810\u6e2c\u4e2d\u7684\u6027\u80fd\uff0c\u4e26\u63d0\u4f9b\u4e86\u4e00\u500b\u6613\u65bc\u4f7f\u7528\u7684\u5e73\u53f0\uff0c\u63a8\u52d5\u85e5\u7269\u7814\u767c\u548c\u8f49\u5316\u7814\u7a76\u3002"}}
{"id": "2505.05527", "pdf": "https://arxiv.org/pdf/2505.05527", "abs": "https://arxiv.org/abs/2505.05527", "authors": ["Giovanni Perin", "Cesare Bidini", "Riccardo Mazzieri", "Michele Rossi"], "title": "ADMM-Based Training for Spiking Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.NE", "eess.SP", "math.OC"], "comment": "6 pages, 4 figures. Preprint submitted to IEEE MLSP 2025", "summary": "In recent years, spiking neural networks (SNNs) have gained momentum due to\ntheir high potential in time-series processing combined with minimal energy\nconsumption. However, they still lack a dedicated and efficient training\nalgorithm. The popular backpropagation with surrogate gradients, adapted from\nstochastic gradient descent (SGD)-derived algorithms, has several drawbacks\nwhen used as an optimizer for SNNs. Specifically, it suffers from low\nscalability and numerical imprecision. In this paper, we propose a novel SNN\ntraining method based on the alternating direction method of multipliers\n(ADMM). Our ADMM-based training aims to solve the problem of the SNN step\nfunction's non-differentiability. We formulate the problem, derive closed-form\nupdates, and empirically show the optimizer's convergence properties, great\npotential, and possible new research directions to improve the method in a\nsimulated proof-of-concept.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eADMM\u7684\u65b0\u578bSNN\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u65b9\u6cd5\u5728SNN\u8bad\u7ec3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u6027\u5dee\u548c\u6570\u503c\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfSNN\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982\u5e26\u66ff\u4ee3\u68af\u5ea6\u7684\u53cd\u5411\u4f20\u64ad\uff09\u5b58\u5728\u53ef\u6269\u5c55\u6027\u4f4e\u548c\u6570\u503c\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u8bad\u7ec3SNN\uff0c\u901a\u8fc7\u95ee\u9898\u5f62\u5f0f\u5316\u548c\u95ed\u5f0f\u66f4\u65b0\u89e3\u51b3SNN\u9636\u8dc3\u51fd\u6570\u7684\u4e0d\u53ef\u5fae\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ADMM\u4f18\u5316\u5668\u7684\u6536\u655b\u6027\u548c\u6f5c\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u6539\u8fdb\u8be5\u65b9\u6cd5\u7684\u65b0\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "ADMM\u4e3aSNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u6027\u80fd\u3002"}}
{"id": "2505.05772", "pdf": "https://arxiv.org/pdf/2505.05772", "abs": "https://arxiv.org/abs/2505.05772", "authors": ["Zehao Fan", "Garrett Gagnon", "Zhenyu Liu", "Liu Liu"], "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.", "AI": {"tldr": "STARC\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u4f18\u5316\u6570\u636e\u6620\u5c04\u65b9\u6848\uff0c\u4e13\u4e3aPIM\u67b6\u6784\u4e0a\u7684\u9ad8\u6548LLM\u89e3\u7801\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u805a\u7c7bKV\u5bf9\uff0c\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u81ea\u56de\u5f52\u89e3\u7801\u65f6\u56e0\u9891\u7e41\u5185\u5b58\u8bbf\u95ee\u548cKV\u7f13\u5b58\u589e\u957f\u5bfc\u81f4\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u5f53\u524dPIM\u67b6\u6784\u5bf9\u7a00\u758f\u8bbf\u95ee\u6a21\u5f0f\u5904\u7406\u6548\u7387\u4f4e\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u63d0\u51faSTARC\u65b9\u6848\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u805a\u7c7bKV\u5bf9\u5e76\u6620\u5c04\u5230\u8fde\u7eed\u5185\u5b58\u533a\u57df\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u4e2d\u5fc3\u5b9e\u73b0\u9009\u62e9\u6027\u6ce8\u610f\u529b\uff0c\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u548c\u5e76\u884c\u5904\u7406\u5f00\u9500\u3002", "result": "\u5728HBM-PIM\u7cfb\u7edf\u4e0a\uff0cSTARC\u6bd4\u5e38\u89c1\u7a00\u758f\u65b9\u6cd5\u964d\u4f4e\u6ce8\u610f\u529b\u5c42\u5ef6\u8fdf19%-31%\uff0c\u80fd\u801719%-27%\uff1b\u5728KV\u7f13\u5b58\u9884\u7b971024\u4e0b\uff0c\u5ef6\u8fdf\u548c\u80fd\u8017\u5206\u522b\u51cf\u5c1154%-74%\u548c45%-67%\u3002", "conclusion": "STARC\u5728PIM\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u786c\u4ef6\u53cb\u597d\u7684\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\uff0c\u4e3a\u7a00\u758f\u6ce8\u610f\u529b\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05616", "pdf": "https://arxiv.org/pdf/2505.05616", "abs": "https://arxiv.org/abs/2505.05616", "authors": ["Lorenzo Di Fruscia", "Jana Marie Weber"], "title": "Leveraging Large Language Models for enzymatic reaction prediction and characterization", "categories": ["cs.AI", "cs.LG", "q-bio.BM"], "comment": null, "summary": "Predicting enzymatic reactions is crucial for applications in biocatalysis,\nmetabolic engineering, and drug discovery, yet it remains a complex and\nresource-intensive task. Large Language Models (LLMs) have recently\ndemonstrated remarkable success in various scientific domains, e.g., through\ntheir ability to generalize knowledge, reason over complex structures, and\nleverage in-context learning strategies. In this study, we systematically\nevaluate the capability of LLMs, particularly the Llama-3.1 family (8B and\n70B), across three core biochemical tasks: Enzyme Commission number prediction,\nforward synthesis, and retrosynthesis. We compare single-task and multitask\nlearning strategies, employing parameter-efficient fine-tuning via LoRA\nadapters. Additionally, we assess performance across different data regimes to\nexplore their adaptability in low-data settings. Our results demonstrate that\nfine-tuned LLMs capture biochemical knowledge, with multitask learning\nenhancing forward- and retrosynthesis predictions by leveraging shared\nenzymatic information. We also identify key limitations, for example challenges\nin hierarchical EC classification schemes, highlighting areas for further\nimprovement in LLM-driven biochemical modeling.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bc4\u4f30\u4e86Llama-3.1\u5bb6\u65cfLLMs\uff088B\u548c70B\uff09\u5728\u9176\u53cd\u5e94\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u53d1\u73b0\u591a\u4efb\u52a1\u5b66\u4e60\u5229\u7528\u5171\u4eab\u4fe1\u606f\u63d0\u5347\u4e86\u9884\u6d4b\u6548\u679c\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86EC\u5206\u7c7b\u7b49\u5c40\u9650\u6027\u3002", "motivation": "\u9176\u53cd\u5e94\u9884\u6d4b\u5728\u751f\u7269\u50ac\u5316\u3001\u4ee3\u8c22\u5de5\u7a0b\u548c\u836f\u7269\u53d1\u73b0\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002LLMs\u5728\u79d1\u5b66\u9886\u57df\u7684\u6210\u529f\u5e94\u7528\u4e3a\u8fd9\u4e00\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u4f7f\u7528Llama-3.1\u6a21\u578b\uff088B\u548c70B\uff09\uff0c\u901a\u8fc7LoRA\u9002\u914d\u5668\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u8bc4\u4f30\u5176\u5728\u9176\u59d4\u5458\u4f1a\u7f16\u53f7\u9884\u6d4b\u3001\u6b63\u5411\u5408\u6210\u548c\u9006\u5411\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6bd4\u8f83\u5355\u4efb\u52a1\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5fae\u8c03\u540e\u7684LLMs\u80fd\u591f\u6355\u83b7\u751f\u7269\u5316\u5b66\u77e5\u8bc6\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u901a\u8fc7\u5171\u4eab\u9176\u4fe1\u606f\u63d0\u5347\u4e86\u6b63\u9006\u5411\u5408\u6210\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46EC\u5206\u7c7b\u7b49\u4efb\u52a1\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "LLMs\u5728\u9176\u53cd\u5e94\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u591a\u4efb\u52a1\u5b66\u4e60\u6709\u6548\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5e94\u5bf9\u590d\u6742\u5206\u7c7b\u95ee\u9898\u3002"}}
{"id": "2505.05530", "pdf": "https://arxiv.org/pdf/2505.05530", "abs": "https://arxiv.org/abs/2505.05530", "authors": ["Kai Liu", "Qian Zheng", "Kaiwen Tao", "Zhiteng Li", "Haotong Qin", "Wenbo Li", "Yong Guo", "Xianglong Liu", "Linghe Kong", "Guihai Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "Low-bit Model Quantization for Deep Neural Networks: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "We have systematically collected and reviewed the state-of-the-art\n  quantization methods from the past five years, categorizing them into eight\n  distinct groups. A curated list of model quantization is provided at\n  https://github.com/Kai-Liu001/Awesome-Model-Quantization", "summary": "With unprecedented rapid development, deep neural networks (DNNs) have deeply\ninfluenced almost all fields. However, their heavy computation costs and model\nsizes are usually unacceptable in real-world deployment. Model quantization, an\neffective weight-lighting technique, has become an indispensable procedure in\nthe whole deployment pipeline. The essence of quantization acceleration is the\nconversion from continuous floating-point numbers to discrete integer ones,\nwhich significantly speeds up the memory I/O and calculation, i.e., addition\nand multiplication. However, performance degradation also comes with the\nconversion because of the loss of precision. Therefore, it has become\nincreasingly popular and critical to investigate how to perform the conversion\nand how to compensate for the information loss. This article surveys the recent\nfive-year progress towards low-bit quantization on DNNs. We discuss and compare\nthe state-of-the-art quantization methods and classify them into 8 main\ncategories and 24 sub-categories according to their core techniques.\nFurthermore, we shed light on the potential research opportunities in the field\nof model quantization. A curated list of model quantization is provided at\nhttps://github.com/Kai-Liu001/Awesome-Model-Quantization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8fc7\u53bb\u4e94\u5e74\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4f4e\u6bd4\u7279\u91cf\u5316\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5bf9\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u5e76\u63a2\u8ba8\u4e86\u6f5c\u5728\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1DNN\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u5927\u5c0f\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u91cf\u5316\u6280\u672f\u80fd\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u4f46\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u91cf\u5316\u548c\u5f25\u8865\u7cbe\u5ea6\u635f\u5931\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u5bf9\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u7c7b\uff0c\u5206\u4e3a8\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c24\u4e2a\u5b50\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5bf9\u91cf\u5316\u65b9\u6cd5\u7684\u5206\u7c7b\u548c\u6bd4\u8f83\uff0c\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u7684\u6838\u5fc3\u6280\u672f\u548c\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u9879\u76ee\u5217\u8868\u4f9b\u53c2\u8003\u3002", "conclusion": "\u91cf\u5316\u6280\u672f\u662fDNN\u90e8\u7f72\u7684\u5173\u952e\u73af\u8282\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316\u91cf\u5316\u6548\u7387\u548c\u7cbe\u5ea6\u5e73\u8861\uff0c\u540c\u65f6\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05815", "pdf": "https://arxiv.org/pdf/2505.05815", "abs": "https://arxiv.org/abs/2505.05815", "authors": ["Machi Shimmei", "Masaki Uto", "Yuichiroh Matsubayashi", "Kentaro Inui", "Aditi Mallavarapu", "Noboru Matsuda"], "title": "Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted", "categories": ["cs.CL"], "comment": "This is a pre-print version of a paper to appear in AIED2025", "summary": "The primary goal of this study is to develop and evaluate an innovative\nprompting technique, AnaQuest, for generating multiple-choice questions (MCQs)\nusing a pre-trained large language model. In AnaQuest, the choice items are\nsentence-level assertions about complex concepts. The technique integrates\nformative and summative assessments. In the formative phase, students answer\nopen-ended questions for target concepts in free text. For summative\nassessment, AnaQuest analyzes these responses to generate both correct and\nincorrect assertions. To evaluate the validity of the generated MCQs, Item\nResponse Theory (IRT) was applied to compare item characteristics between MCQs\ngenerated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An\nempirical study found that expert instructors rated MCQs generated by both AI\nmodels to be as valid as those created by human instructors. However, IRT-based\nanalysis revealed that AnaQuest-generated questions - particularly those with\nincorrect assertions (foils) - more closely resembled human-crafted items in\nterms of difficulty and discrimination than those produced by ChatGPT.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u63d0\u793a\u6280\u672fAnaQuest\uff0c\u7528\u4e8e\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9009\u62e9\u9898\uff08MCQ\uff09\u3002AnaQuest\u901a\u8fc7\u5206\u6790\u5b66\u751f\u56de\u7b54\u751f\u6210\u6b63\u786e\u548c\u9519\u8bef\u9009\u9879\uff0c\u7ed3\u5408\u5f62\u6210\u6027\u548c\u603b\u7ed3\u6027\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAnaQuest\u751f\u6210\u7684\u9898\u76ee\u5728\u96be\u5ea6\u548c\u533a\u5206\u5ea6\u4e0a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7f16\u5199\u7684\u9898\u76ee\u3002", "motivation": "\u89e3\u51b3\u4f7f\u7528AI\u751f\u6210\u9ad8\u8d28\u91cf\u9009\u62e9\u9898\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u751f\u6210\u5177\u6709\u6559\u80b2\u610f\u4e49\u7684\u5e72\u6270\u9009\u9879\uff08\u9519\u8bef\u9009\u9879\uff09\uff0c\u4ee5\u63d0\u5347\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u7ed3\u5408\u5f62\u6210\u6027\u548c\u603b\u7ed3\u6027\u8bc4\u4f30\uff0c\u5b66\u751f\u5148\u56de\u7b54\u5f00\u653e\u95ee\u9898\uff0cAnaQuest\u5206\u6790\u5176\u56de\u7b54\u751f\u6210MCQ\u9009\u9879\u3002\u4f7f\u7528IRT\u6bd4\u8f83AnaQuest\u3001ChatGPT\u548c\u4eba\u7c7b\u751f\u6210\u9898\u76ee\u7684\u7279\u6027\u3002", "result": "\u4e13\u5bb6\u8bc4\u4ef7\u663e\u793a\uff0cAI\u751f\u6210\u7684\u9898\u76ee\u4e0e\u4eba\u7c7b\u7f16\u5199\u7684\u9898\u76ee\u6709\u6548\u6027\u76f8\u5f53\uff1bIRT\u5206\u6790\u8868\u660e\uff0cAnaQuest\u751f\u6210\u7684\u9898\u76ee\uff08\u5c24\u5176\u662f\u5e72\u6270\u9879\uff09\u5728\u96be\u5ea6\u548c\u533a\u5206\u5ea6\u4e0a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u9898\u76ee\u3002", "conclusion": "AnaQuest\u5728\u751f\u6210\u6559\u80b2\u6709\u6548\u9009\u62e9\u9898\uff08\u5c24\u5176\u662f\u5e72\u6270\u9879\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4e3aAI\u5728\u6559\u80b2\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05684", "pdf": "https://arxiv.org/pdf/2505.05684", "abs": "https://arxiv.org/abs/2505.05684", "authors": ["Han Wu", "Jie Yin"], "title": "Prompted Meta-Learning for Few-shot Knowledge Graph Completion", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Few-shot knowledge graph completion (KGC) has obtained significant attention\ndue to its practical applications in real-world scenarios, where new knowledge\noften emerges with limited available data. While most existing methods for\nfew-shot KGC have predominantly focused on leveraging relational information,\nrich semantics inherent in KGs have been largely overlooked. To address this\ngap, we propose a novel prompted meta-learning (PromptMeta) framework that\nseamlessly integrates meta-semantics with relational information for few-shot\nKGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that\ncaptures and consolidates high-level meta-semantics, enabling effective\nknowledge transfer and adaptation to rare and newly emerging relations. (2) a\nlearnable fusion prompt that dynamically combines meta-semantic information\nwith task-specific relational information tailored to different few-shot tasks.\nBoth components are optimized together with model parameters within a\nmeta-learning framework. Extensive experiments on two benchmark datasets\ndemonstrate the effectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPromptMeta\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5143\u8bed\u4e49\u548c\u5173\u7cfb\u4fe1\u606f\u6765\u89e3\u51b3\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u5ffd\u89c6\u77e5\u8bc6\u56fe\u8c31\u4e30\u5bcc\u8bed\u4e49\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86PromptMeta\u6846\u67b6\uff0c\u5305\u542b\u5143\u8bed\u4e49\u63d0\u793a\u6c60\u548c\u53ef\u5b66\u4e60\u7684\u878d\u5408\u63d0\u793a\uff0c\u52a8\u6001\u7ed3\u5408\u5143\u8bed\u4e49\u4e0e\u4efb\u52a1\u7279\u5b9a\u5173\u7cfb\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PromptMeta\u6846\u67b6\u5728\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u5143\u8bed\u4e49\u4fe1\u606f\u3002"}}
{"id": "2505.05533", "pdf": "https://arxiv.org/pdf/2505.05533", "abs": "https://arxiv.org/abs/2505.05533", "authors": ["Zhiyuan Ning", "Pengfei Wang", "Ziyue Qiao", "Pengyang Wang", "Yuanchun Zhou"], "title": "Rethinking Graph Contrastive Learning through Relative Similarity Preservation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IJCAI2025; full version including appendix", "summary": "Graph contrastive learning (GCL) has achieved remarkable success by following\nthe computer vision paradigm of preserving absolute similarity between\naugmented views. However, this approach faces fundamental challenges in graphs\ndue to their discrete, non-Euclidean nature -- view generation often breaks\nsemantic validity and similarity verification becomes unreliable. Through\nanalyzing 11 real-world graphs, we discover a universal pattern transcending\nthe homophily-heterophily dichotomy: label consistency systematically\ndiminishes as structural distance increases, manifesting as smooth decay in\nhomophily graphs and oscillatory decay in heterophily graphs. We establish\ntheoretical guarantees for this pattern through random walk theory, proving\nlabel distribution convergence and characterizing the mechanisms behind\ndifferent decay behaviors. This discovery reveals that graphs naturally encode\nrelative similarity patterns, where structurally closer nodes exhibit\ncollectively stronger semantic relationships. Leveraging this insight, we\npropose RELGCL, a novel GCL framework with complementary pairwise and listwise\nimplementations that preserve these inherent patterns through collective\nsimilarity objectives. Extensive experiments demonstrate that our method\nconsistently outperforms 20 existing approaches across both homophily and\nheterophily graphs, validating the effectiveness of leveraging natural relative\nsimilarity over artificial absolute similarity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6RELGCL\uff0c\u5229\u7528\u56fe\u4e2d\u7ed3\u6784\u8ddd\u79bb\u4e0e\u6807\u7b7e\u4e00\u81f4\u6027\u7684\u81ea\u7136\u5173\u7cfb\uff0c\u901a\u8fc7\u76f8\u5bf9\u76f8\u4f3c\u6027\u76ee\u6807\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u7edd\u5bf9\u76f8\u4f3c\u6027\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6cbf\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u7edd\u5bf9\u76f8\u4f3c\u6027\u65b9\u6cd5\uff0c\u4f46\u5ffd\u89c6\u4e86\u56fe\u7684\u79bb\u6563\u3001\u975e\u6b27\u51e0\u91cc\u5f97\u7279\u6027\uff0c\u5bfc\u81f4\u89c6\u56fe\u751f\u6210\u7834\u574f\u8bed\u4e49\u6709\u6548\u6027\u4e14\u76f8\u4f3c\u6027\u9a8c\u8bc1\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u5206\u679011\u4e2a\u771f\u5b9e\u56fe\u6570\u636e\uff0c\u53d1\u73b0\u6807\u7b7e\u4e00\u81f4\u6027\u968f\u7ed3\u6784\u8ddd\u79bb\u589e\u52a0\u7684\u8870\u51cf\u6a21\u5f0f\uff0c\u63d0\u51faRELGCL\u6846\u67b6\uff0c\u5305\u542b\u6210\u5bf9\u548c\u5217\u8868\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u5229\u7528\u96c6\u4f53\u76f8\u4f3c\u6027\u76ee\u6807\u4fdd\u7559\u56fe\u7684\u81ea\u7136\u76f8\u5bf9\u76f8\u4f3c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRELGCL\u572820\u79cd\u73b0\u6709\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u65e0\u8bba\u662f\u540c\u8d28\u6027\u8fd8\u662f\u5f02\u8d28\u6027\u56fe\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u76f8\u6bd4\u4eba\u5de5\u5b9a\u4e49\u7684\u7edd\u5bf9\u76f8\u4f3c\u6027\uff0c\u5229\u7528\u56fe\u4e2d\u81ea\u7136\u7684\u76f8\u5bf9\u76f8\u4f3c\u6027\u6a21\u5f0f\u80fd\u66f4\u6709\u6548\u5730\u63d0\u5347\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.05864", "pdf": "https://arxiv.org/pdf/2505.05864", "abs": "https://arxiv.org/abs/2505.05864", "authors": ["Junhyeong Lee", "Jong Min Yuk", "Chan-Woo Lee"], "title": "Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI", "categories": ["cs.CL"], "comment": "29 pages", "summary": "The construction of experimental datasets is essential for expanding the\nscope of data-driven scientific discovery. Recent advances in natural language\nprocessing (NLP) have facilitated automatic extraction of structured data from\nunstructured scientific literature. While existing approaches-multi-step and\ndirect methods-offer valuable capabilities, they also come with limitations\nwhen applied independently. Here, we propose a novel hybrid text-mining\nframework that integrates the advantages of both methods to convert\nunstructured scientific text into structured data. Our approach first\ntransforms raw text into entity-recognized text, and subsequently into\nstructured form. Furthermore, beyond the overall data structuring framework, we\nalso enhance entity recognition performance by introducing an entity marker-a\nsimple yet effective technique that uses symbolic annotations to highlight\ntarget entities. Specifically, our entity marker-based hybrid approach not only\nconsistently outperforms previous entity recognition approaches across three\nbenchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the\nquality of final structured data-yielding up to a 58% improvement in\nentity-level F1 score and up to 83% improvement in relation-level F1 score\ncompared to direct approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6587\u672c\u6316\u6398\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u6b65\u548c\u76f4\u63a5\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5b9e\u4f53\u6807\u8bb0\u6280\u672f\u63d0\u5347\u4e86\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u6700\u7ec8\u7ed3\u6784\u5316\u6570\u636e\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u591a\u6b65\u548c\u76f4\u63a5\u65b9\u6cd5\uff09\u5728\u72ec\u7acb\u5e94\u7528\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\u6765\u66f4\u6709\u6548\u5730\u4ece\u65e0\u7ed3\u6784\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u9996\u5148\u5c06\u539f\u59cb\u6587\u672c\u8f6c\u6362\u4e3a\u5b9e\u4f53\u8bc6\u522b\u6587\u672c\uff0c\u518d\u8fdb\u4e00\u6b65\u7ed3\u6784\u5316\uff1b\u5e76\u5f15\u5165\u5b9e\u4f53\u6807\u8bb0\u6280\u672f\u4ee5\u63d0\u5347\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08MatScholar\u3001SOFC \u548c SOFC slot NER\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5\uff0c\u5b9e\u4f53\u7ea7 F1 \u5206\u6570\u63d0\u5347\u4e86 58%\uff0c\u5173\u7cfb\u7ea7 F1 \u5206\u6570\u63d0\u5347\u4e86 83%\u3002", "conclusion": "\u6df7\u5408\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u6b65\u548c\u76f4\u63a5\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u5229\u7528\u5b9e\u4f53\u6807\u8bb0\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u79d1\u5b66\u6587\u672c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u7684\u6027\u80fd\u3002"}}
{"id": "2505.05701", "pdf": "https://arxiv.org/pdf/2505.05701", "abs": "https://arxiv.org/abs/2505.05701", "authors": ["Jongchan Park", "Mingyu Park", "Donghwan Lee"], "title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) aims to learn a policy from a static\ndataset without further interactions with the environment. Collecting\nsufficiently large datasets for offline RL is exhausting since this data\ncollection requires colossus interactions with environments and becomes tricky\nwhen the interaction with the environment is restricted. Hence, how an agent\nlearns the best policy with a minimal static dataset is a crucial issue in\noffline RL, similar to the sample efficiency problem in online RL. In this\npaper, we propose a simple yet effective plug-and-play pretraining method to\ninitialize a feature of a $Q$-network to enhance data efficiency in offline RL.\nSpecifically, we introduce a shared $Q$-network structure that outputs\npredictions of the next state and $Q$-value. We pretrain the shared $Q$-network\nthrough a supervised regression task that predicts a next state and trains the\nshared $Q$-network using diverse offline RL methods. Through extensive\nexperiments, we empirically demonstrate that our method enhances the\nperformance of existing popular offline RL methods on the D4RL, Robomimic and\nV-D4RL benchmarks. Furthermore, we show that our method significantly boosts\ndata-efficient offline RL across various data qualities and data distributions\ntrough D4RL and ExoRL benchmarks. Notably, our method adapted with only 10% of\nthe dataset outperforms standard algorithms even with full datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eabQ\u7f51\u7edc\u7ed3\u6784\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u4ece\u9759\u6001\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u4f46\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u53d7\u9650\uff0c\u5982\u4f55\u7528\u6700\u5c0f\u6570\u636e\u96c6\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u662f\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5171\u4eabQ\u7f51\u7edc\u7ed3\u6784\uff0c\u901a\u8fc7\u76d1\u7763\u56de\u5f52\u4efb\u52a1\u9884\u8bad\u7ec3\u7f51\u7edc\u4ee5\u9884\u6d4b\u4e0b\u4e00\u72b6\u6001\u548cQ\u503c\uff0c\u5e76\u4e0e\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5728D4RL\u3001Robomimic\u548cV-D4RL\u7b49\u57fa\u51c6\u4e0a\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4ec5\u752810%\u6570\u636e\u96c6\u5373\u53ef\u8d85\u8d8a\u6807\u51c6\u7b97\u6cd5\u7684\u5168\u6570\u636e\u96c6\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u548c\u5206\u5e03\uff0c\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05538", "pdf": "https://arxiv.org/pdf/2505.05538", "abs": "https://arxiv.org/abs/2505.05538", "authors": ["Md Kamrujjaman Mobin", "Md Saiful Islam", "Sadik Al Barid", "Md Masum"], "title": "Cardioformer: Advancing AI in ECG Analysis with Multi-Granularity Patching and ResNet", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Electrocardiogram (ECG) classification is crucial for automated cardiac\ndisease diagnosis, yet existing methods often struggle to capture local\nmorphological details and long-range temporal dependencies simultaneously. To\naddress these challenges, we propose Cardioformer, a novel multi-granularity\nhybrid model that integrates cross-channel patching, hierarchical residual\nlearning, and a two-stage self-attention mechanism. Cardioformer first encodes\nmulti-scale token embeddings to capture fine-grained local features and global\ncontextual information and then selectively fuses these representations through\nintra- and inter-granularity self-attention. Extensive evaluations on three\nbenchmark ECG datasets under subject-independent settings demonstrate that\nmodel consistently outperforms four state-of-the-art baselines. Our\nCardioformer model achieves the AUROC of 96.34$\\pm$0.11, 89.99$\\pm$0.12, and\n95.59$\\pm$1.66 in MIMIC-IV, PTB-XL and PTB dataset respectively outperforming\nPatchTST, Reformer, Transformer, and Medformer models. It also demonstrates\nstrong cross-dataset generalization, achieving 49.18% AUROC on PTB and 68.41%\non PTB-XL when trained on MIMIC-IV. These findings underscore the potential of\nCardioformer to advance automated ECG analysis, paving the way for more\naccurate and robust cardiovascular disease diagnosis. We release the source\ncode at https://github.com/KMobin555/Cardioformer.", "AI": {"tldr": "Cardioformer \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u7c92\u5ea6\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u901a\u9053\u5206\u5757\u3001\u5206\u5c42\u6b8b\u5dee\u5b66\u4e60\u548c\u53cc\u9636\u6bb5\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5fc3\u7535\u56fe\u5206\u7c7b\u4e2d\u5c40\u90e8\u5f62\u6001\u7ec6\u8282\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u6311\u6218\u3002\u8be5\u6a21\u578b\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u5fc3\u7535\u56fe\u5206\u7c7b\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u5c40\u90e8\u5f62\u6001\u7ec6\u8282\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4e3a\u6b64\u5f00\u53d1\u4e86Cardioformer\u6765\u586b\u8865\u8fd9\u4e00\u6280\u672f\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8de8\u901a\u9053\u5206\u5757\u3001\u5206\u5c42\u6b8b\u5dee\u5b66\u4e60\u548c\u53cc\u9636\u6bb5\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u6807\u8bb0\u5d4c\u5165\u6765\u6355\u83b7\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728MIMIC-IV\u3001PTB-XL\u548cPTB\u6570\u636e\u96c6\u4e0a\uff0cCardioformer\u7684AUROC\u5206\u522b\u8fbe\u523096.34\u00b10.11\u300189.99\u00b10.12\u548c95.59\u00b11.66\uff0c\u4f18\u4e8ePatchTST\u3001Reformer\u3001Transformer\u548cMedformer\u3002\u540c\u65f6\u8fd8\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Cardioformer\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u7535\u56fe\u81ea\u52a8\u5206\u6790\u7684\u6027\u80fd\uff0c\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u51c6\u786e\u548c\u7a33\u5065\u8bca\u65ad\u94fa\u5e73\u4e86\u9053\u8def\u3002\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.05946", "pdf": "https://arxiv.org/pdf/2505.05946", "abs": "https://arxiv.org/abs/2505.05946", "authors": ["Vytenis \u0160liogeris", "Povilas Daniu\u0161is", "Art\u016bras Nakvosas"], "title": "Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 4 figures", "summary": "This technical report describes an experiment on autoregressive pre-training\nof Gemma2 2 billion parameter large language model (LLM) with 10\\% on the\nLithuanian language component of CulturaX from the point of view of continual\nlearning. We apply elastic weight consolidation (EWC) to the full set of the\nmodel's parameters and investigate language understanding benchmarks,\nconsisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande\nsets (both in English and Lithuanian versions), and perplexity benchmarks. We\nempirically demonstrate that EWC regularisation allows us not only to mitigate\ncatastrophic forgetting effects but also that it is potentially beneficial for\nlearning of the new task with LLMs.", "AI": {"tldr": "\u5b9e\u9a8c\u7814\u7a76\u4e86\u5728Gemma2\u6a21\u578b\u4e0a\u5e94\u7528\u5f39\u6027\u6743\u91cd\u5de9\u56fa\uff08EWC\uff09\u6765\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u53d1\u73b0EWC\u5bf9\u65b0\u4efb\u52a1\u5b66\u4e60\u4e5f\u6709\u6f5c\u5728\u76ca\u5904\u3002", "motivation": "\u63a2\u7d22\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u8bad\u7ec3\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7EWC\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u5347\u5bf9\u65b0\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5728Gemma2\u6a21\u578b\u4e0a\u5e94\u7528EWC\uff0c\u6d4b\u8bd5\u5176\u5728\u591a\u79cd\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u548c\u56f0\u60d1\u5ea6\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u82f1\u8bed\u548c\u7acb\u9676\u5b9b\u8bed\u7248\u672c\u3002", "result": "EWC\u4e0d\u4ec5\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u8fd8\u5bf9\u65b0\u4efb\u52a1\u5b66\u4e60\u6709\u6f5c\u5728\u76ca\u5904\u3002", "conclusion": "EWC\u662f\u4e00\u79cd\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eLLM\u7684\u9884\u8bad\u7ec3\u548c\u65b0\u4efb\u52a1\u5b66\u4e60\u3002"}}
{"id": "2505.05758", "pdf": "https://arxiv.org/pdf/2505.05758", "abs": "https://arxiv.org/abs/2505.05758", "authors": ["Azim Ospanov", "Roozbeh Yousefzadeh"], "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Formal reasoning and automated theorem proving constitute a challenging\nsubfield of machine learning, in which machines are tasked with proving\nmathematical theorems using formal languages like Lean. A formal verification\nsystem can check whether a formal proof is correct or not almost\ninstantaneously, but generating a completely correct formal proof with large\nlanguage models (LLMs) remains a formidable task. The usual approach in the\nliterature is to prompt the LLM many times (up to several thousands) until one\nof the generated proofs passes the verification system. In this work, we\npresent APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a\nmodular, model-agnostic pipeline that combines the strengths of the Lean\ncompiler with an LLM's reasoning abilities to achieve better proof-generation\nresults at a low sampling budget. Apollo directs a fully automated process in\nwhich the LLM generates proofs for theorems, a set of agents analyze the\nproofs, fix the syntax errors, identify the mistakes in the proofs using Lean,\nisolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on\neach remaining goal with a low top-K budget. The repaired sub-proofs are\nrecombined and reverified, iterating up to a user-controlled maximum number of\nattempts. On the miniF2F benchmark, we establish a new state-of-the-art\naccuracy of 75.0% among 7B-parameter models while keeping the sampling budget\nbelow one thousand. Moreover, Apollo raises the state-of-the-art accuracy for\nGoedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few\nhundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40%\naccuracy. Our results demonstrate that targeted, compiler-guided repair of LLM\noutputs yields dramatic gains in both efficiency and correctness, suggesting a\ngeneral paradigm for scalable automated theorem proving.", "AI": {"tldr": "APOLLO\u662f\u4e00\u4e2a\u7ed3\u5408Lean\u7f16\u8bd1\u5668\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4fee\u590d\u8bc1\u660e\u8bed\u6cd5\u9519\u8bef\u3001\u9694\u79bb\u5931\u8d25\u5b50\u5f15\u7406\u3001\u8c03\u7528LLM\u8fdb\u884c\u76ee\u6807\u4fee\u590d\u7b49\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u7406\u8bc1\u660e\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5b8c\u5168\u6b63\u786e\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u65f6\u7684\u56f0\u96be\uff0c\u901a\u8fc7\u7ed3\u5408Lean\u7f16\u8bd1\u5668\u7684\u9a8c\u8bc1\u80fd\u529b\u548cLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u91c7\u6837\u9884\u7b97\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "APOLLO\u6d41\u7a0b\u5305\u62ecLLM\u751f\u6210\u8bc1\u660e\u3001\u4ee3\u7406\u5206\u6790\u5e76\u4fee\u590d\u8bed\u6cd5\u9519\u8bef\u3001\u5229\u7528Lean\u8bc6\u522b\u9519\u8bef\u3001\u9694\u79bb\u5b50\u5f15\u7406\u3001\u8c03\u7528\u81ea\u52a8\u6c42\u89e3\u5668\uff0c\u5e76\u4ee5\u4f4e\u91c7\u6837\u9884\u7b97\u8fed\u4ee3\u4fee\u590d\u3002", "result": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPOLLO\u5c067B\u53c2\u6570\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u5347\u81f375.0%\uff0c\u5e76\u5c06Goedel-Prover-SFT\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u81f365.6%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6837\u672c\u590d\u6742\u5ea6\u3002", "conclusion": "\u5b9a\u5411\u7f16\u8bd1\u5668\u5f15\u5bfc\u7684LLM\u8f93\u51fa\u4fee\u590d\u5728\u6548\u7387\u548c\u6b63\u786e\u6027\u4e0a\u5e26\u6765\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u8303\u5f0f\u3002"}}
{"id": "2505.05568", "pdf": "https://arxiv.org/pdf/2505.05568", "abs": "https://arxiv.org/abs/2505.05568", "authors": ["Yanbo Wang", "Xiyuan Wang", "Quan Gan", "Minjie Wang", "Qibin Yang", "David Wipf", "Muhan Zhang"], "title": "Griffin: Towards a Graph-Centric Relational Database Foundation Model", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "We introduce Griffin, the first foundation model attemptation designed\nspecifically for Relational Databases (RDBs). Unlike previous smaller models\nfocused on single RDB tasks, Griffin unifies the data encoder and task decoder\nto handle diverse tasks. Additionally, we enhance the architecture by\nincorporating a cross-attention module and a novel aggregator. Griffin utilizes\npretraining on both single-table and RDB datasets, employing advanced encoders\nfor categorical, numerical, and metadata features, along with innovative\ncomponents such as cross-attention modules and enhanced message-passing neural\nnetworks (MPNNs) to capture the complexities of relational data. Evaluated on\nlarge-scale, heterogeneous, and temporal graphs extracted from RDBs across\nvarious domains (spanning over 150 million nodes), Griffin demonstrates\nsuperior or comparable performance to individually trained models, excels in\nlow-data scenarios, and shows strong transferability with similarity and\ndiversity in pretraining across new datasets and tasks, highlighting its\npotential as a universally applicable foundation model for RDBs. Code available\nat https://github.com/yanxwb/Griffin.", "AI": {"tldr": "Griffin\u662f\u4e00\u4e2a\u4e13\u4e3a\u5173\u7cfb\u6570\u636e\u5e93\u8bbe\u8ba1\u7684\u9996\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u6570\u636e\u7f16\u7801\u5668\u548c\u4efb\u52a1\u89e3\u7801\u5668\u5904\u7406\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5e76\u5728\u67b6\u6784\u4e2d\u52a0\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u65b0\u578b\u805a\u5408\u5668\u3002\u9884\u8bad\u7ec3\u91c7\u7528\u5355\u8868\u548c\u591a\u8868\u6570\u636e\u96c6\uff0c\u5229\u7528\u9ad8\u7ea7\u7f16\u7801\u5668\u548c\u521b\u65b0\u7ec4\u4ef6\u6355\u6349\u5173\u7cfb\u6570\u636e\u7684\u590d\u6742\u6027\u3002\u5728\u5927\u89c4\u6a21\u5f02\u6784\u65f6\u6001\u56fe\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cGriffin\u6027\u80fd\u4f18\u4e8e\u6216\u63a5\u8fd1\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u591a\u4e3a\u9488\u5bf9\u5355\u4e00\u5173\u7cfb\u6570\u636e\u5e93\u4efb\u52a1\u7684\u5c0f\u6a21\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u5904\u7406\u591a\u6837\u5316\u4efb\u52a1\u7684\u80fd\u529b\u3002Griffin\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u9ad8\u6548\u5904\u7406\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u3002", "method": "Griffin\u91c7\u7528\u7edf\u4e00\u7684\u6570\u636e\u7f16\u7801\u5668\u548c\u4efb\u52a1\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u65b0\u578b\u805a\u5408\u5668\u3002\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5355\u8868\u548c\u591a\u8868\u6570\u636e\u96c6\uff0c\u5229\u7528\u9ad8\u7ea7\u7f16\u7801\u5668\u5904\u7406\u5206\u7c7b\u3001\u6570\u503c\u548c\u5143\u6570\u636e\u7279\u5f81\uff0c\u540c\u65f6\u5f15\u5165\u521b\u65b0\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u6539\u8fdb\u7684\u6d88\u606f\u4f20\u9012\u795e\u7ecf\u7f51\u7edc\uff08MPNNs\uff09\u4ee5\u6355\u6349\u5173\u7cfb\u6570\u636e\u7684\u590d\u6742\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5f02\u6784\u65f6\u6001\u56fe\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGriffin\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u4e0e\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002\u6b64\u5916\uff0c\u6a21\u578b\u663e\u793a\u51fa\u5f3a\u5927\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u5728\u65b0\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u5feb\u901f\u9002\u5e94\u3002", "conclusion": "Griffin\u4f5c\u4e3a\u5173\u7cfb\u6570\u636e\u5e93\u7684\u9996\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u901a\u7528\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u591a\u6837\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u5176\u521b\u65b0\u67b6\u6784\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.05947", "pdf": "https://arxiv.org/pdf/2505.05947", "abs": "https://arxiv.org/abs/2505.05947", "authors": ["Bianca Steffes", "Nils Torben Wiedemann", "Alexander Gratz", "Pamela Hochreither", "Jana Elina Meyer", "Katharina Luise Schilke"], "title": "Summarisation of German Judgments in conjunction with a Class-based Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The automated summarisation of long legal documents can be a great aid for\nlegal experts in their daily work. We automatically create summaries (guiding\nprinciples) of German judgments by fine-tuning a decoder-based large language\nmodel. We enrich the judgments with information about legal entities before the\ntraining. For the evaluation of the created summaries, we define a set of\nevaluation classes which allows us to measure their language, pertinence,\ncompleteness and correctness. Our results show that employing legal entities\nhelps the generative model to find the relevant content, but the quality of the\ncreated summaries is not yet sufficient for a use in practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u52a8\u751f\u6210\u4e86\u5fb7\u56fd\u5224\u51b3\u4e66\u7684\u6458\u8981\uff08\u6307\u5bfc\u539f\u5219\uff09\u3002\u7814\u7a76\u5728\u8bad\u7ec3\u524d\u4e3a\u5224\u51b3\u4e66\u6dfb\u52a0\u4e86\u6cd5\u5f8b\u5b9e\u4f53\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5b9a\u4e49\u8bc4\u4f30\u6807\u51c6\u6765\u8861\u91cf\u6458\u8981\u7684\u8bed\u8a00\u3001\u76f8\u5173\u6027\u3001\u5b8c\u6574\u6027\u548c\u6b63\u786e\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f7f\u7528\u6cd5\u5f8b\u5b9e\u4f53\u6709\u52a9\u4e8e\u6a21\u578b\u627e\u5230\u76f8\u5173\u5185\u5bb9\uff0c\u4f46\u751f\u6210\u7684\u6458\u8981\u8d28\u91cf\u5c1a\u4e0d\u8db3\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u6cd5\u5f8b\u4e13\u5bb6\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u9700\u8981\u5904\u7406\u5927\u91cf\u5197\u957f\u7684\u6cd5\u5f8b\u6587\u4ef6\uff0c\u81ea\u52a8\u6458\u8981\u6280\u672f\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u4ed6\u4eec\u7684\u5de5\u4f5c\u6548\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u751f\u6210\u5fb7\u56fd\u5224\u51b3\u4e66\u7684\u6458\u8981\uff0c\u5e2e\u52a9\u6cd5\u5f8b\u4e13\u5bb6\u5feb\u901f\u83b7\u53d6\u5173\u952e\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u8bad\u7ec3\u524d\u5bf9\u5fb7\u56fd\u5224\u51b3\u4e66\u8fdb\u884c\u6cd5\u5f8b\u5b9e\u4f53\u4fe1\u606f\u7684\u589e\u5f3a\u3002\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u751f\u6210\u6458\u8981\uff0c\u5e76\u901a\u8fc7\u5b9a\u4e49\u7684\u8bc4\u4f30\u7c7b\u522b\uff08\u8bed\u8a00\u3001\u76f8\u5173\u6027\u3001\u5b8c\u6574\u6027\u548c\u6b63\u786e\u6027\uff09\u8fdb\u884c\u8bc4\u4ef7\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u52a0\u5165\u6cd5\u5f8b\u5b9e\u4f53\u4fe1\u606f\u786e\u5b9e\u6709\u52a9\u4e8e\u6a21\u578b\u8bc6\u522b\u76f8\u5173\u5185\u5bb9\uff0c\u4f46\u751f\u6210\u7684\u6458\u8981\u5728\u8d28\u91cf\u4e0a\u4ecd\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u7684\u9700\u6c42\u3002", "conclusion": "\u5c3d\u7ba1\u6cd5\u5f8b\u5b9e\u4f53\u4fe1\u606f\u7684\u52a0\u5165\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u751f\u6210\u7684\u6458\u8981\u8d28\u91cf\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u9ad8\u624d\u80fd\u7528\u4e8e\u5b9e\u8df5\u3002\u672a\u6765\u7684\u7814\u7a76\u53ef\u4ee5\u63a2\u7d22\u66f4\u591a\u4f18\u5316\u65b9\u6cd5\u6216\u7ed3\u5408\u5176\u4ed6\u6280\u672f\u4ee5\u63d0\u5347\u6458\u8981\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.05880", "pdf": "https://arxiv.org/pdf/2505.05880", "abs": "https://arxiv.org/abs/2505.05880", "authors": ["Bettina Fazzinga", "Sergio Flesca", "Filippo Furfaro", "Luigi Pontieri", "Francesco Scala"], "title": "Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams", "categories": ["cs.AI"], "comment": null, "summary": "Monitoring and analyzing process traces is a critical task for modern\ncompanies and organizations. In scenarios where there is a gap between trace\nevents and reference business activities, this entails an interpretation\nproblem, amounting to translating each event of any ongoing trace into the\ncorresponding step of the activity instance. Building on a recent approach that\nframes the interpretation problem as an acceptance problem within an Abstract\nArgumentation Framework (AAF), one can elegantly analyze plausible event\ninterpretations (possibly in an aggregated form), as well as offer explanations\nfor those that conflict with prior process knowledge. Since, in settings where\nevent-to-activity mapping is highly uncertain (or simply under-specified) this\nreasoning-based approach may yield lowly-informative results and heavy\ncomputation, one can think of discovering a sequencetagging model, trained to\nsuggest highly-probable candidate event interpretations in a context-aware way.\nHowever, training such a model optimally may require using a large amount of\nmanually-annotated example traces. Considering the urgent need of developing\nGreen AI solutions enabling environmental and societal sustainability (with\nreduced labor/computational costs and carbon footprint), we propose a\ndata/computation-efficient neuro-symbolic approach to the problem, where the\ncandidate interpretations returned by the example-driven sequence tagger is\nrefined by the AAF-based reasoner. This allows us to also leverage prior\nknowledge to compensate for the scarcity of example data, as confirmed by\nexperimental results; clearly, this property is particularly useful in settings\nwhere data annotation and model optimization costs are subject to stringent\nconstraints.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\u4e0e\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\uff08AAF\uff09\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u4e8b\u4ef6\u5230\u6d3b\u52a8\u6620\u5c04\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u6570\u636e\u6807\u6ce8\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u4ee3\u4f01\u4e1a\u548c\u7ec4\u7ec7\u9700\u8981\u76d1\u63a7\u548c\u5206\u6790\u8fc7\u7a0b\u8f68\u8ff9\uff0c\u4f46\u5728\u4e8b\u4ef6\u4e0e\u53c2\u8003\u4e1a\u52a1\u6d3b\u52a8\u4e4b\u95f4\u5b58\u5728\u6620\u5c04\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u4f4e\u4fe1\u606f\u91cf\u7ed3\u679c\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u7eff\u8272AI\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u73af\u5883\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5148\u901a\u8fc7\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\u751f\u6210\u5019\u9009\u4e8b\u4ef6\u89e3\u91ca\uff0c\u518d\u5229\u7528AAF\u6846\u67b6\u8fdb\u884c\u63a8\u7406\u4f18\u5316\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u5f25\u8865\u6570\u636e\u7a00\u7f3a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u548c\u4e25\u683c\u7ea6\u675f\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u80fd\u591f\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u6807\u6ce8\u548c\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.05577", "pdf": "https://arxiv.org/pdf/2505.05577", "abs": "https://arxiv.org/abs/2505.05577", "authors": ["Alejandro Velez-Arce", "Marinka Zitnik"], "title": "PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models", "categories": ["cs.LG", "cs.AI", "68-04, 92-04", "D.2.11; I.2.5; J.3"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada. PMLR 267, 2025", "summary": "Existing biomedical benchmarks do not provide end-to-end infrastructure for\ntraining, evaluation, and inference of models that integrate multimodal\nbiological data and a broad range of machine learning tasks in therapeutics. We\npresent PyTDC, an open-source machine-learning platform providing streamlined\ntraining, evaluation, and inference software for multimodal biological AI\nmodels. PyTDC unifies distributed, heterogeneous, continuously updated data\nsources and model weights and standardizes benchmarking and inference\nendpoints. This paper discusses the components of PyTDC's architecture and, to\nour knowledge, the first-of-its-kind case study on the introduced single-cell\ndrug-target nomination ML task. We find state-of-the-art methods in graph\nrepresentation learning and domain-specific methods from graph theory perform\npoorly on this task. Though we find a context-aware geometric deep learning\nmethod that outperforms the evaluated SoTA and domain-specific baseline\nmethods, the model is unable to generalize to unseen cell types or incorporate\nadditional modalities, highlighting PyTDC's capacity to facilitate an exciting\navenue of research developing multimodal, context-aware, foundation models for\nopen problems in biomedical AI.", "AI": {"tldr": "PyTDC \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u673a\u5668\u5b66\u4e60\u5e73\u53f0\uff0c\u65e8\u5728\u6574\u5408\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u5e76\u63d0\u4f9b\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u63a8\u7406\u7684\u7aef\u5230\u7aef\u57fa\u7840\u8bbe\u65bd\u3002\u5b83\u9996\u6b21\u5f15\u5165\u4e86\u5355\u7ec6\u80de\u836f\u7269\u9776\u70b9\u63d0\u540d\u4efb\u52a1\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u751f\u7269\u533b\u5b66 AI \u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u7269\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u6574\u5408\u591a\u6a21\u6001\u751f\u7269\u6570\u636e\u548c\u591a\u6837\u5316\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u57fa\u7840\u8bbe\u65bd\uff0cPyTDC \u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u751f\u7269\u533b\u5b66 AI \u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86 PyTDC \u5e73\u53f0\uff0c\u6574\u5408\u5206\u5e03\u5f0f\u3001\u5f02\u6784\u548c\u6301\u7eed\u66f4\u65b0\u7684\u6570\u636e\u6e90\u548c\u6a21\u578b\u6743\u91cd\uff0c\u5e76\u6807\u51c6\u5316\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u63a8\u7406\u7aef\u70b9\u3002\u901a\u8fc7\u4e00\u4e2a\u9996\u521b\u7684\u5355\u7ec6\u80de\u836f\u7269\u9776\u70b9\u63d0\u540d\u4efb\u52a1\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u548c\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8be5\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u77e5\u7ec6\u80de\u7c7b\u578b\u6216\u6574\u5408\u989d\u5916\u6a21\u6001\u3002", "conclusion": "PyTDC \u80fd\u591f\u4fc3\u8fdb\u591a\u6a21\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4e3a\u89e3\u51b3\u751f\u7269\u533b\u5b66 AI \u4e2d\u7684\u5f00\u653e\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05949", "pdf": "https://arxiv.org/pdf/2505.05949", "abs": "https://arxiv.org/abs/2505.05949", "authors": ["Max Glockner", "Xiang Jiang", "Leonardo F. R. Ribeiro", "Iryna Gurevych", "Markus Dreyer"], "title": "NeoQA: Evidence-based Question Answering with Generated News Events", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating Retrieval-Augmented Generation (RAG) in large language models\n(LLMs) is challenging because benchmarks can quickly become stale. Questions\ninitially requiring retrieval may become answerable from pretraining knowledge\nas newer models incorporate more recent information during pretraining, making\nit difficult to distinguish evidence-based reasoning from recall. We introduce\nNeoQA (News Events for Out-of-training Question Answering), a benchmark\ndesigned to address this issue. To construct NeoQA, we generated timelines and\nknowledge bases of fictional news events and entities along with news articles\nand Q\\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring\nthat no prior evidence exists in their training data. We propose our dataset as\na new platform for evaluating evidence-based question answering, as it requires\nLLMs to generate responses exclusively from retrieved evidence and only when\nsufficient evidence is available. NeoQA enables controlled evaluation across\nvarious evidence scenarios, including cases with missing or misleading details.\nOur findings indicate that LLMs struggle to distinguish subtle mismatches\nbetween questions and evidence, and suffer from short-cut reasoning when key\ninformation required to answer a question is missing from the evidence,\nunderscoring key limitations in evidence-based reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NeoQA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u589e\u5f3a\u68c0\u7d22\u751f\u6210\uff08RAG\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u865a\u6784\u65b0\u95fb\u4e8b\u4ef6\u548c\u5b9e\u4f53\u6784\u5efa\u6570\u636e\u96c6\uff0c\u4ee5\u907f\u514d\u6a21\u578b\u4f9d\u8d56\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u57fa\u51c6\u6613\u8fc7\u65f6\u4e14\u96be\u4ee5\u533a\u5206\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u4e0e\u8bb0\u5fc6\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u51c6\u786e\u8bc4\u4f30LLMs\u7684\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u865a\u6784\u65b0\u95fb\u4e8b\u4ef6\u7684\u65f6\u95f4\u7ebf\u548c\u77e5\u8bc6\u5e93\uff0c\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u786e\u4fdd\u6a21\u578b\u53ea\u80fd\u4f9d\u9760\u68c0\u7d22\u5230\u7684\u8bc1\u636e\u56de\u7b54\u95ee\u9898\u3002", "result": "LLMs\u5728\u5904\u7406\u95ee\u9898\u4e0e\u8bc1\u636e\u95f4\u7684\u7ec6\u5fae\u4e0d\u5339\u914d\u6216\u4fe1\u606f\u7f3a\u5931\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u663e\u793a\u51fa\u57fa\u4e8e\u8bc1\u636e\u63a8\u7406\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "NeoQA\u4e3a\u8bc4\u4f30\u57fa\u4e8e\u8bc1\u636e\u7684\u95ee\u9898\u56de\u7b54\u63d0\u4f9b\u4e86\u65b0\u5e73\u53f0\uff0c\u63ed\u793a\u4e86LLMs\u5728\u8bc1\u636e\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.05976", "pdf": "https://arxiv.org/pdf/2505.05976", "abs": "https://arxiv.org/abs/2505.05976", "authors": ["Chico Sundermann", "Stefan Vill", "Elias Kuiter", "Sebastian Krieter", "Thomas Th\u00fcm", "Matthias Tichy"], "title": "Pseudo-Boolean d-DNNF Compilation for Expressive Feature Modeling Constructs", "categories": ["cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Configurable systems typically consist of reusable assets that have\ndependencies between each other. To specify such dependencies, feature models\nare commonly used. As feature models in practice are often complex, automated\nreasoning is typically employed to analyze the dependencies. Here, the de facto\nstandard is translating the feature model to conjunctive normal form (CNF) to\nenable employing off-the-shelf tools, such as SAT or #SAT solvers. However,\nmodern feature-modeling dialects often contain constructs, such as cardinality\nconstraints, that are ill-suited for conversion to CNF. This mismatch between\nthe input of reasoning engines and the available feature-modeling dialects\nlimits the applicability of the more expressive constructs. In this work, we\nshorten this gap between expressive constructs and scalable automated\nreasoning. Our contribution is twofold: First, we provide a pseudo-Boolean\nencoding for feature models, which facilitates smaller representations of\ncommonly employed constructs compared to Boolean encoding. Second, we propose a\nnovel method to compile pseudo-Boolean formulas to Boolean d-DNNF. With the\ncompiled d-DNNFs, we can resort to a plethora of efficient analyses already\nused in feature modeling. Our empirical evaluation shows that our proposal\nsubstantially outperforms the state-of-the-art based on CNF inputs for\nexpressive constructs. For every considered dataset representing different\nfeature models and feature-modeling constructs, the feature models can be\nsignificantly faster translated to pseudo-Boolean than to CNF. Overall,\nderiving d-DNNFs from a feature model with the targeted expressive constraints\ncan be substantially accelerated using our pseudo-Boolean approach.\nFurthermore, our approach is competitive on feature models with only basic\nconstructs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f2a\u5e03\u5c14\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u8fbe\u590d\u6742\u7684\u7279\u5f81\u6a21\u578b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u7f16\u8bd1\u4e3a\u5e03\u5c14d-DNNF\u52a0\u901f\u81ea\u52a8\u63a8\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709CNF\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u6a21\u578b\u4e2d\u7684\u9ad8\u7ea7\u6784\u9020\uff08\u5982\u57fa\u6570\u7ea6\u675f\uff09\u96be\u4ee5\u6709\u6548\u8f6c\u6362\u4e3aCNF\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u63a8\u7406\u7684\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u7f29\u77ed\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u9ad8\u7ea7\u6784\u9020\u4e0e\u9ad8\u6548\u81ea\u52a8\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4f2a\u5e03\u5c14\u7f16\u7801\u65b9\u6cd5\uff0c\u5c06\u7279\u5f81\u6a21\u578b\u8f6c\u6362\u4e3a\u4f2a\u5e03\u5c14\u516c\u5f0f\uff0c\u5e76\u901a\u8fc7\u7f16\u8bd1\u4e3a\u5e03\u5c14d-DNNF\u683c\u5f0f\u8fdb\u884c\u9ad8\u6548\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f2a\u5e03\u5c14\u7f16\u7801\u5728\u8f6c\u6362\u65f6\u95f4\u548c\u63a8\u7406\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCNF\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u7ea7\u6784\u9020\u3002", "conclusion": "\u4f2a\u5e03\u5c14\u7f16\u7801\u7ed3\u5408d-DNNF\u7f16\u8bd1\u4e0d\u4ec5\u63d0\u5347\u4e86\u590d\u6742\u7279\u5f81\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e5f\u5728\u57fa\u7840\u6784\u9020\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002"}}
{"id": "2505.05594", "pdf": "https://arxiv.org/pdf/2505.05594", "abs": "https://arxiv.org/abs/2505.05594", "authors": ["Sura Alhanouti", "Parinaz Naghizadeh"], "title": "Anticipating Gaming to Incentivize Improvement: Guiding Agents in (Fair) Strategic Classification", "categories": ["cs.LG"], "comment": "31 pages, 12 figures", "summary": "As machine learning algorithms increasingly influence critical decision\nmaking in different application areas, understanding human strategic behavior\nin response to these systems becomes vital. We explore individuals' choice\nbetween genuinely improving their qualifications (``improvement'') vs.\nattempting to deceive the algorithm by manipulating their features\n(``manipulation'') in response to an algorithmic decision system. We further\ninvestigate an algorithm designer's ability to shape these strategic responses,\nand its fairness implications. Specifically, we formulate these interactions as\na Stackelberg game, where a firm deploys a (fair) classifier, and individuals\nstrategically respond. Our model incorporates both different costs and\nstochastic efficacy for manipulation and improvement. The analysis reveals\ndifferent potential classes of agent responses, and characterizes optimal\nclassifiers accordingly. Based on these, we highlight the impact of the firm's\nanticipation of strategic behavior, identifying when and why a (fair) strategic\npolicy can not only prevent manipulation, but also incentivize agents to opt\nfor improvement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u7c7b\u5728\u9762\u5bf9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u51b3\u7b56\u65f6\u7684\u7b56\u7565\u884c\u4e3a\uff0c\u5305\u62ec\u63d0\u5347\u81ea\u8eab\u80fd\u529b\uff08\u6539\u8fdb\uff09\u6216\u64cd\u7eb5\u7279\u5f81\uff08\u6b3a\u9a97\uff09\u7684\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u535a\u5f08\u8bba\u6a21\u578b\u5206\u6790\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u8005\u5982\u4f55\u5f15\u5bfc\u8fd9\u4e9b\u884c\u4e3a\u4ee5\u4fc3\u8fdb\u516c\u5e73\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u5e94\u5bf9\u65e5\u76ca\u5f71\u54cd\u5173\u952e\u51b3\u7b56\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u63a2\u8ba8\u7b97\u6cd5\u8bbe\u8ba1\u8005\u5982\u4f55\u901a\u8fc7\u7b56\u7565\u6027\u8bbe\u8ba1\u6fc0\u52b1\u6539\u8fdb\u800c\u975e\u64cd\u7eb5\u884c\u4e3a\uff0c\u4ece\u800c\u4fc3\u8fdb\u7cfb\u7edf\u516c\u5e73\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7Stackelberg\u535a\u5f08\u6a21\u578b\uff0c\u5c06\u516c\u53f8\u90e8\u7f72\uff08\u516c\u5e73\uff09\u5206\u7c7b\u5668\u4e0e\u4e2a\u4f53\u7b56\u7565\u6027\u53cd\u5e94\u76f8\u7ed3\u5408\uff0c\u5e76\u8003\u8651\u4e86\u64cd\u7eb5\u548c\u6539\u8fdb\u7684\u4e0d\u540c\u6210\u672c\u53ca\u968f\u673a\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7406\u4eba\u7684\u53cd\u5e94\u53ef\u5206\u4e3a\u4e0d\u540c\u7c7b\u522b\uff0c\u5e76\u636e\u6b64\u4f18\u5316\u5206\u7c7b\u5668\u8bbe\u8ba1\uff1b\u6218\u7565\u6027\u7b56\u7565\u4e0d\u4ec5\u80fd\u9632\u6b62\u64cd\u7eb5\uff0c\u8fd8\u80fd\u6fc0\u52b1\u4ee3\u7406\u4eba\u9009\u62e9\u6539\u8fdb\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u516c\u53f8\u9884\u89c1\u7b56\u7565\u884c\u4e3a\u7684\u91cd\u8981\u6027\uff0c\u6307\u51fa\u5408\u7406\u7684\u6218\u7565\u653f\u7b56\u53ef\u4ee5\u4fc3\u8fdb\u516c\u5e73\u5e76\u6fc0\u52b1\u79ef\u6781\u884c\u4e3a\u3002"}}
{"id": "2505.05970", "pdf": "https://arxiv.org/pdf/2505.05970", "abs": "https://arxiv.org/abs/2505.05970", "authors": ["Lennart St\u00f6pler", "Rufat Asadli", "Mitja Nikolaus", "Ryan Cotterell", "Alex Warstadt"], "title": "Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We propose a method for training language models in an interactive setting\ninspired by child language acquisition. In our setting, a speaker attempts to\ncommunicate some information to a listener in a single-turn dialogue and\nreceives a reward if communicative success is achieved. Unlike earlier related\nwork using image--caption data for interactive reference games, we\noperationalize communicative success in a more abstract language-only\nquestion--answering setting. First, we present a feasibility study\ndemonstrating that our reward provides an indirect signal about grammaticality.\nSecond, we conduct experiments using reinforcement learning to fine-tune\nlanguage models. We observe that cognitively plausible constraints on the\ncommunication channel lead to interpretable changes in speaker behavior.\nHowever, we do not yet see improvements on linguistic evaluations from our\ntraining regime. We outline potential modifications to the task design and\ntraining configuration that could better position future work to use our\nmethodology to observe the benefits of interaction on language learning in\ncomputational cognitive models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u542f\u53d1\u7684\u4ea4\u4e92\u5f0f\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u3002\u901a\u8fc7\u5355\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5956\u52b1\u673a\u5236\uff0c\u7814\u7a76\u53d1\u73b0\u867d\u7136\u901a\u4fe1\u9650\u5236\u80fd\u6539\u53d8\u8bf4\u8bdd\u8005\u884c\u4e3a\uff0c\u4f46\u5c1a\u672a\u63d0\u5347\u8bed\u8a00\u8bc4\u4f30\u8868\u73b0\uff0c\u672a\u6765\u9700\u6539\u8fdb\u4efb\u52a1\u8bbe\u8ba1\u548c\u8bad\u7ec3\u914d\u7f6e\u3002", "motivation": "\u53d7\u513f\u7ae5\u8bed\u8a00\u5b66\u4e60\u542f\u53d1\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5982\u4f55\u901a\u8fc7\u5355\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u901a\u4fe1\u6210\u529f\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u62bd\u8c61\u95ee\u7b54\u60c5\u5883\u5b9e\u73b0\u901a\u4fe1\u6210\u529f\u7684\u4fe1\u53f7\u4f20\u9012\uff0c\u5e76\u6d4b\u8bd5\u5176\u5bf9\u8bed\u6cd5\u6027\u7684\u95f4\u63a5\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u4fe1\u9650\u5236\u80fd\u6539\u53d8\u8bf4\u8bdd\u8005\u884c\u4e3a\uff0c\u4f46\u672a\u663e\u8457\u63d0\u5347\u8bed\u8a00\u8bc4\u4f30\u8868\u73b0\u3002", "conclusion": "\u867d\u7136\u4ea4\u4e92\u8bad\u7ec3\u65b9\u6cd5\u53ef\u884c\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4efb\u52a1\u8bbe\u8ba1\u548c\u8bad\u7ec3\u914d\u7f6e\u4ee5\u63d0\u5347\u8bed\u8a00\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2505.06020", "pdf": "https://arxiv.org/pdf/2505.06020", "abs": "https://arxiv.org/abs/2505.06020", "authors": ["Shuai Wang", "Ivona Najdenkoska", "Hongyi Zhu", "Stevan Rudinac", "Monika Kackovic", "Nachoem Wijnberg", "Marcel Worring"], "title": "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Understanding visual art requires reasoning across multiple perspectives --\ncultural, historical, and stylistic -- beyond mere object recognition. While\nrecent multimodal large language models (MLLMs) perform well on general image\ncaptioning, they often fail to capture the nuanced interpretations that fine\nart demands. We propose ArtRAG, a novel, training-free framework that combines\nstructured knowledge with retrieval-augmented generation (RAG) for\nmulti-perspective artwork explanation. ArtRAG automatically constructs an Art\nContext Knowledge Graph (ACKG) from domain-specific textual sources, organizing\nentities such as artists, movements, themes, and historical events into a rich,\ninterpretable graph. At inference time, a multi-granular structured retriever\nselects semantically and topologically relevant subgraphs to guide generation.\nThis enables MLLMs to produce contextually grounded, culturally informed art\ndescriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG\noutperforms several heavily trained baselines. Human evaluations further\nconfirm that ArtRAG generates coherent, insightful, and culturally enriched\ninterpretations.", "AI": {"tldr": "ArtRAG\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u4e3a\u591a\u89c6\u89d2\u827a\u672f\u54c1\u89e3\u91ca\u63d0\u4f9b\u652f\u6301\u3002\u5b83\u4ece\u9886\u57df\u6587\u672c\u6784\u5efa\u827a\u672f\u4e0a\u4e0b\u6587\u77e5\u8bc6\u56fe\uff08ACKG\uff09\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u76f8\u5173\u5b50\u56fe\u4ee5\u6307\u5bfc\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u827a\u672f\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u548c\u6587\u5316\u6df1\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u827a\u672f\u54c1\u89e3\u91ca\u4e2d\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u6587\u5316\u3001\u5386\u53f2\u548c\u98ce\u683c\u5185\u6db5\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u878d\u5408\u591a\u89c6\u89d2\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "method": "ArtRAG\u901a\u8fc7\u6784\u5efaACKG\uff08\u6db5\u76d6\u827a\u672f\u5bb6\u3001\u6d41\u6d3e\u3001\u4e3b\u9898\u7b49\u5b9e\u4f53\uff09\uff0c\u5e76\u8bbe\u8ba1\u591a\u7c92\u5ea6\u68c0\u7d22\u5668\u9009\u62e9\u76f8\u5173\u5b50\u56fe\uff0c\u589e\u5f3aMLLMs\u7684\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728SemArt\u548cArtpedia\u6570\u636e\u96c6\u4e0a\uff0cArtRAG\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u5176\u751f\u6210\u5185\u5bb9\u66f4\u5177\u6587\u5316\u6d1e\u5bdf\u529b\u548c\u8fde\u8d2f\u6027\u3002", "conclusion": "ArtRAG\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u827a\u672f\u89e3\u91ca\u8d28\u91cf\uff0c\u4e3a\u8de8\u6a21\u6001\u827a\u672f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.05597", "pdf": "https://arxiv.org/pdf/2505.05597", "abs": "https://arxiv.org/abs/2505.05597", "authors": ["Jacek Karolczak", "Jerzy Stefanowski"], "title": "This part looks alike this: identifying important parts of explained instances and prototypes", "categories": ["cs.LG"], "comment": null, "summary": "Although prototype-based explanations provide a human-understandable way of\nrepresenting model predictions they often fail to direct user attention to the\nmost relevant features. We propose a novel approach to identify the most\ninformative features within prototypes, termed alike parts. Using feature\nimportance scores derived from an agnostic explanation method, it emphasizes\nthe most relevant overlapping features between an instance and its nearest\nprototype. Furthermore, the feature importance score is incorporated into the\nobjective function of the prototype selection algorithms to promote global\nprototypes diversity. Through experiments on six benchmark datasets, we\ndemonstrate that the proposed approach improves user comprehension while\nmaintaining or even increasing predictive accuracy.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u65b9\u6cd5\u8bc6\u522b\u539f\u578b\u4e2d\u6700\u76f8\u5173\u7279\u5f81\uff0c\u63d0\u9ad8\u7528\u6237\u7406\u89e3\u4e14\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u539f\u578b\u7684\u89e3\u91ca\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u7a81\u51fa\u5173\u952e\u7279\u5f81\uff0c\u9700\u6539\u8fdb\u4ee5\u589e\u5f3a\u7528\u6237\u7406\u89e3", "method": "\u7ed3\u5408\u7279\u5f81\u91cd\u8981\u6027\u5206\u6570\u5f3a\u8c03\u5b9e\u4f8b\u4e0e\u539f\u578b\u91cd\u53e0\u7279\u5f81\uff0c\u5e76\u4f18\u5316\u539f\u578b\u9009\u62e9\u7b97\u6cd5\u4ee5\u63d0\u5347\u591a\u6837\u6027", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u63d0\u5347\u4e86\u7528\u6237\u7406\u89e3\u4e14\u672a\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u7387", "conclusion": "\u901a\u8fc7\u7a81\u51fa\u76f8\u5173\u7279\u5f81\u548c\u539f\u578b\u591a\u6837\u6027\uff0c\u65b0\u65b9\u6cd5\u5728\u89e3\u91ca\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861"}}
{"id": "2505.05973", "pdf": "https://arxiv.org/pdf/2505.05973", "abs": "https://arxiv.org/abs/2505.05973", "authors": ["M. Maziyah Mohamed", "R. H. Baayen"], "title": "An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition", "categories": ["cs.CL"], "comment": "24 pages, 5 figures, and 9 tables. Submitted to the Journal of\n  Morphology", "summary": "Studies of morphological processing have shown that semantic transparency is\ncrucial for word recognition. Its computational operationalization is still\nunder discussion. Our primary objectives are to explore embedding-based\nmeasures of semantic transparency, and assess their impact on reading. First,\nwe explored the geometry of complex words in semantic space. To do so, we\nconducted a t-distributed Stochastic Neighbor Embedding clustering analysis on\n4,226 Malay prefixed words. Several clusters were observed for complex words\nvaried by their prefix class. Then, we derived five simple measures, and\ninvestigated whether they were significant predictors of lexical decision\nlatencies. Two sets of Linear Discriminant Analyses were run in which the\nprefix of a word is predicted from either word embeddings or shift vectors\n(i.e., a vector subtraction of the base word from the derived word). The\naccuracy with which the model predicts the prefix of a word indicates the\ndegree of transparency of the prefix. Three further measures were obtained by\ncomparing embeddings between each word and all other words containing the same\nprefix (i.e., centroid), between each word and the shift from their base word,\nand between each word and the predicted word of the Functional Representations\nof Affixes in Compositional Semantic Space model. In a series of Generalized\nAdditive Mixed Models, all measures predicted decision latencies after\naccounting for word frequency, word length, and morphological family size. The\nmodel that included the correlation between each word and their centroid as a\npredictor provided the best fit to the data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u4e49\u900f\u660e\u5ea6\u5728\u8bcd\u6c47\u8bc6\u522b\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u5e76\u6d4b\u8bd5\u4e86\u4e94\u79cd\u57fa\u4e8e\u8bcd\u5d4c\u5165\u7684\u8bed\u4e49\u900f\u660e\u5ea6\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u9605\u8bfb\u53cd\u5e94\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u8bed\u4e49\u900f\u660e\u5ea6\u7684\u8ba1\u7b97\u64cd\u4f5c\u5316\u53ca\u5176\u5bf9\u8bcd\u6c47\u8bc6\u522b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u9a6c\u6765\u8bed\u524d\u7f00\u8bcd\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7t-SNE\u805a\u7c7b\u5206\u6790\u3001\u7ebf\u6027\u5224\u522b\u5206\u6790\u548c\u5e7f\u4e49\u52a0\u6027\u6df7\u5408\u6a21\u578b\uff0c\u57fa\u4e8e\u8bcd\u5d4c\u5165\u548c\u4f4d\u79fb\u5411\u91cf\u6d4b\u91cf\u8bed\u4e49\u900f\u660e\u5ea6\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u8bcd\u6c47\u51b3\u7b56\u53cd\u5e94\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "result": "\u4e94\u79cd\u6d4b\u91cf\u65b9\u6cd5\u5747\u80fd\u663e\u8457\u9884\u6d4b\u8bcd\u6c47\u51b3\u7b56\u53cd\u5e94\u65f6\u95f4\uff0c\u5176\u4e2d\u4e0e\u8bcd\u7c07\u8d28\u5fc3\u7684\u76f8\u5173\u6027\u6d4b\u91cf\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u57fa\u4e8e\u8bcd\u5d4c\u5165\u7684\u8bed\u4e49\u900f\u660e\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u6709\u6548\uff0c\u4e14\u8d28\u5fc3\u76f8\u5173\u6027\u662f\u6700\u5177\u9884\u6d4b\u529b\u7684\u6307\u6807\uff0c\u4e3a\u5f62\u6001\u52a0\u5de5\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.06030", "pdf": "https://arxiv.org/pdf/2505.06030", "abs": "https://arxiv.org/abs/2505.06030", "authors": ["Tobias Preintner", "Weixuan Yuan", "Qi Huang", "Adrian K\u00f6nig", "Thomas B\u00e4ck", "Elena Raponi", "Niki van Stein"], "title": "Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Combining natural language and geometric shapes is an emerging research area\nwith multiple applications in robotics and language-assisted design. A crucial\ntask in this domain is object referent identification, which involves selecting\na 3D object given a textual description of the target. Variability in language\ndescriptions and spatial relationships of 3D objects makes this a complex task,\nincreasing the need to better understand the behavior of neural network models\nin this domain. However, limited research has been conducted in this area.\nSpecifically, when a model makes an incorrect prediction despite being provided\nwith a seemingly correct object description, practitioners are left wondering:\n\"Why is the model wrong?\". In this work, we present a method answering this\nquestion by generating counterfactual examples. Our method takes a\nmisclassified sample, which includes two objects and a text description, and\ngenerates an alternative yet similar formulation that would have resulted in a\ncorrect prediction by the model. We have evaluated our approach with data from\nthe ShapeTalk dataset along with three distinct models. Our counterfactual\nexamples maintain the structure of the original description, are semantically\nsimilar and meaningful. They reveal weaknesses in the description, model bias\nand enhance the understanding of the models behavior. Theses insights help\npractitioners to better interact with systems as well as engineers to improve\nmodels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u6837\u672c\u6765\u89e3\u91ca\u6a21\u578b\u9519\u8bef\u9884\u6d4b\u539f\u56e0\u7684\u65b9\u6cd5\uff0c\u5e2e\u52a9\u7406\u89e3\u6a21\u578b\u57283D\u5bf9\u8c61\u6307\u4ee3\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b33D\u5bf9\u8c61\u6307\u4ee3\u4efb\u52a1\u4e2d\u6a21\u578b\u9519\u8bef\u9884\u6d4b\u7684\u56f0\u60d1\uff0c\u589e\u5f3a\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u7406\u89e3\u3002", "method": "\u751f\u6210\u53cd\u4e8b\u5b9e\u6837\u672c\uff0c\u4fee\u6b63\u63cf\u8ff0\u4ee5\u4f7f\u5176\u88ab\u6a21\u578b\u6b63\u786e\u9884\u6d4b\uff0c\u4fdd\u6301\u539f\u59cb\u63cf\u8ff0\u7ed3\u6784\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5728ShapeTalk\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u53cd\u4e8b\u5b9e\u6837\u672c\u80fd\u63ed\u793a\u63cf\u8ff0\u5f31\u70b9\u3001\u6a21\u578b\u504f\u89c1\uff0c\u63d0\u5347\u6a21\u578b\u7406\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u8df5\u8005\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u6539\u8fdb\u4ea4\u4e92\u4e0e\u6a21\u578b\u7684\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.05605", "pdf": "https://arxiv.org/pdf/2505.05605", "abs": "https://arxiv.org/abs/2505.05605", "authors": ["Andrew Qiu", "Shubham Barhate", "Hin Wai Lui", "Runze Su", "Rafael Rios M\u00fcller", "Kungang Li", "Ling Leng", "Han Sun", "Shayan Ehsani", "Zhifang Liu"], "title": "The Evolution of Embedding Table Optimization and Multi-Epoch Training in Pinterest Ads Conversion", "categories": ["cs.LG", "cs.CE", "cs.IR", "stat.AP", "F.2.2, I.2.7"], "comment": null, "summary": "Deep learning for conversion prediction has found widespread applications in\nonline advertising. These models have become more complex as they are trained\nto jointly predict multiple objectives such as click, add-to-cart, checkout and\nother conversion types. Additionally, the capacity and performance of these\nmodels can often be increased with the use of embedding tables that encode high\ncardinality categorical features such as advertiser, user, campaign, and\nproduct identifiers (IDs). These embedding tables can be pre-trained, but also\nlearned end-to-end jointly with the model to directly optimize the model\nobjectives. Training these large tables is challenging due to: gradient\nsparsity, the high cardinality of the categorical features, the non-uniform\ndistribution of IDs and the very high label sparsity. These issues make\ntraining prone to both slow convergence and overfitting after the first epoch.\nPrevious works addressed the multi-epoch overfitting issue by using: stronger\nfeature hashing to reduce cardinality, filtering of low frequency IDs,\nregularization of the embedding tables, re-initialization of the embedding\ntables after each epoch, etc. Some of these techniques reduce overfitting at\nthe expense of reduced model performance if used too aggressively. In this\npaper, we share key learnings from the development of embedding table\noptimization and multi-epoch training in Pinterest Ads Conversion models. We\nshowcase how our Sparse Optimizer speeds up convergence, and how multi-epoch\noverfitting varies in severity between different objectives in a multi-task\nmodel depending on label sparsity. We propose a new approach to deal with\nmulti-epoch overfitting: the use of a frequency-adaptive learning rate on the\nembedding tables and compare it to embedding re-initialization. We evaluate\nboth methods offline using an industrial large-scale production dataset.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86Pinterest\u5e7f\u544a\u8f6c\u5316\u6a21\u578b\u4e2d\u5d4c\u5165\u8868\u4f18\u5316\u548c\u591a\u8f6e\u8bad\u7ec3\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u63d0\u51fa\u4e86\u9891\u7387\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u8f6e\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5728\u7ebf\u5e7f\u544a\u8f6c\u5316\u9884\u6d4b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u9ad8\u57fa\u6570\u5206\u7c7b\u7279\u5f81\u7684\u5d4c\u5165\u8868\u8bad\u7ec3\u5b58\u5728\u68af\u5ea6\u7a00\u758f\u3001\u6807\u7b7e\u7a00\u758f\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6536\u655b\u6162\u548c\u8fc7\u62df\u5408\u3002", "method": "\u7814\u7a76\u4e86\u5d4c\u5165\u8868\u4f18\u5316\u548c\u591a\u8f6e\u8bad\u7ec3\u6280\u672f\uff0c\u63d0\u51fa\u9891\u7387\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u65b9\u6cd5\uff0c\u5e76\u4e0e\u5d4c\u5165\u91cd\u65b0\u521d\u59cb\u5316\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684Sparse Optimizer\u52a0\u901f\u4e86\u6536\u655b\uff0c\u9891\u7387\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u80fd\u6709\u6548\u7f13\u89e3\u591a\u8f6e\u8fc7\u62df\u5408\uff0c\u5c24\u5176\u662f\u5728\u6807\u7b7e\u7a00\u758f\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u9891\u7387\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u662f\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u5d4c\u5165\u8868\u591a\u8f6e\u8fc7\u62df\u5408\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u7ea7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002"}}
{"id": "2505.06004", "pdf": "https://arxiv.org/pdf/2505.06004", "abs": "https://arxiv.org/abs/2505.06004", "authors": ["Dawid Wisniewski", "Antoni Solarski", "Artur Nowakowski"], "title": "Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models", "categories": ["cs.CL"], "comment": "Accepted at MTSummit 2025 (The 20th Machine Translation Summit)", "summary": "Recent language models can successfully solve various language-related tasks,\nand many understand inputs stated in different languages. In this paper, we\nexplore the performance of 17 popular models used to correct grammatical issues\nin texts stated in English, German, Italian, and Swedish when using a single\nmodel to correct texts in all those languages. We analyze the outputs generated\nby these models, focusing on decreasing the number of grammatical errors while\nkeeping the changes small. The conclusions drawn help us understand what\nproblems occur among those models and which models can be recommended for\nmultilingual grammatical error correction tasks. We list six models that\nimprove grammatical correctness in all four languages and show that Gemma 9B is\ncurrently the best performing one for the languages considered.", "AI": {"tldr": "\u603b\u7ed317\u79cd\u6d41\u884c\u6a21\u578b\u5728\u82f1\u6587\u3001\u5fb7\u6587\u3001\u610f\u5927\u5229\u6587\u548c\u745e\u5178\u6587\u7684\u8bed\u6cd5\u7ea0\u6b63\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63a8\u83506\u79cd\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\u7684\u6a21\u578b\uff0c\u5e76\u6307\u51faGemma 9B\u662f\u76ee\u524d\u6700\u4f73\u9009\u62e9\u3002", "motivation": "\u63a2\u7d22\u5355\u6a21\u578b\u5904\u7406\u591a\u8bed\u8a00\u8bed\u6cd5\u7ea0\u6b63\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ee5\u89e3\u51b3\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8bed\u6cd5\u9519\u8bef\u95ee\u9898\u3002", "method": "\u5206\u679017\u79cd\u6a21\u578b\u5728\u56db\u79cd\u8bed\u8a00\u4e2d\u7684\u8f93\u51fa\uff0c\u91cd\u70b9\u5173\u6ce8\u51cf\u5c11\u8bed\u6cd5\u9519\u8bef\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6539\u52a8\u3002", "result": "\u5217\u51fa6\u79cd\u5728\u6240\u6709\u56db\u79cd\u8bed\u8a00\u4e2d\u63d0\u5347\u8bed\u6cd5\u6b63\u786e\u6027\u7684\u6a21\u578b\uff0cGemma 9B\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u591a\u8bed\u8a00\u8bed\u6cd5\u7ea0\u6b63\u4efb\u52a1\u7684\u6a21\u578b\u63a8\u8350\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u6700\u4f73\u9009\u62e9\u4e3aGemma 9B\u3002"}}
{"id": "2505.06049", "pdf": "https://arxiv.org/pdf/2505.06049", "abs": "https://arxiv.org/abs/2505.06049", "authors": ["Aleena Siji", "Joscha C\u00fcppers", "Osman Ali Mian", "Jilles Vreeken"], "title": "Seqret: Mining Rule Sets from Event Sequences", "categories": ["cs.AI"], "comment": null, "summary": "Summarizing event sequences is a key aspect of data mining. Most existing\nmethods neglect conditional dependencies and focus on discovering sequential\npatterns only. In this paper, we study the problem of discovering both\nconditional and unconditional dependencies from event sequence data. We do so\nby discovering rules of the form $X \\rightarrow Y$ where $X$ and $Y$ are\nsequential patterns. Rules like these are simple to understand and provide a\nclear description of the relation between the antecedent and the consequent. To\ndiscover succinct and non-redundant sets of rules we formalize the problem in\nterms of the Minimum Description Length principle. As the search space is\nenormous and does not exhibit helpful structure, we propose the Seqret method\nto discover high-quality rule sets in practice. Through extensive empirical\nevaluation we show that unlike the state of the art, Seqret ably recovers the\nground truth on synthetic datasets and finds useful rules from real datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSeqret\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e8b\u4ef6\u5e8f\u5217\u6570\u636e\u4e2d\u53d1\u73b0\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u4e3a\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u539f\u5219\u6765\u751f\u6210\u7b80\u6d01\u4e14\u975e\u5197\u4f59\u7684\u89c4\u5219\u96c6\uff0c\u5e76\u5728\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u6761\u4ef6\u4f9d\u8d56\uff0c\u4ec5\u5173\u6ce8\u987a\u5e8f\u6a21\u5f0f\u7684\u53d1\u73b0\uff0c\u672c\u6587\u65e8\u5728\u53d1\u73b0\u4e8b\u4ef6\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faSeqret\u65b9\u6cd5\uff0c\u5f62\u5f0f\u5316\u95ee\u9898\u4e3a\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u539f\u5219\uff0c\u641c\u7d22\u5e76\u751f\u6210\u7b80\u6d01\u4e14\u975e\u5197\u4f59\u7684\u89c4\u5219\u96c6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u80fd\u51c6\u786e\u8fd8\u539f\u771f\u5b9e\u89c4\u5219\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u4e5f\u80fd\u53d1\u73b0\u6709\u7528\u89c4\u5219\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Seqret\u65b9\u6cd5\u5728\u53d1\u73b0\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e8b\u4ef6\u5e8f\u5217\u6570\u636e\u6316\u6398\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.05609", "pdf": "https://arxiv.org/pdf/2505.05609", "abs": "https://arxiv.org/abs/2505.05609", "authors": ["Vasilis Pollatos", "Debmalya Mandal", "Goran Radanovic"], "title": "On Corruption-Robustness in Performative Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "In performative Reinforcement Learning (RL), an agent faces a\npolicy-dependent environment: the reward and transition functions depend on the\nagent's policy. Prior work on performative RL has studied the convergence of\nrepeated retraining approaches to a performatively stable policy. In the finite\nsample regime, these approaches repeatedly solve for a saddle point of a\nconvex-concave objective, which estimates the Lagrangian of a regularized\nversion of the reinforcement learning problem. In this paper, we aim to extend\nsuch repeated retraining approaches, enabling them to operate under corrupted\ndata. More specifically, we consider Huber's $\\epsilon$-contamination model,\nwhere an $\\epsilon$ fraction of data points is corrupted by arbitrary\nadversarial noise. We propose a repeated retraining approach based on\nconvex-concave optimization under corrupted gradients and a novel\nproblem-specific robust mean estimator for the gradients. We prove that our\napproach exhibits last-iterate convergence to an approximately stable policy,\nwith the approximation error linear in $\\sqrt{\\epsilon}$. We experimentally\ndemonstrate the importance of accounting for corruption in performative RL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6570\u636e\u88ab\u6c61\u67d3\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51f8\u51f9\u4f18\u5316\u548c\u9c81\u68d2\u68af\u5ea6\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4f3c\u7a33\u5b9a\u7b56\u7565\u7684\u6536\u655b\u3002", "motivation": "\u7814\u7a76\u5728\u6570\u636e\u88ab\u6c61\u67d3\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6269\u5c55\u91cd\u590d\u91cd\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u4fdd\u6301\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u51f8\u51f9\u4f18\u5316\u5904\u7406\u88ab\u6c61\u67d3\u7684\u68af\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u95ee\u9898\u7279\u5b9a\u7684\u9c81\u68d2\u5747\u503c\u4f30\u8ba1\u5668\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u65b9\u6cd5\u80fd\u6536\u655b\u5230\u8fd1\u4f3c\u7a33\u5b9a\u7b56\u7565\uff0c\u8bef\u5dee\u4e0e\u6c61\u67d3\u7a0b\u5ea6\u7684\u5e73\u65b9\u6839\u6210\u6b63\u6bd4\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8003\u8651\u6570\u636e\u6c61\u67d3\u5bf9\u5f3a\u5316\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u3002"}}
{"id": "2505.06010", "pdf": "https://arxiv.org/pdf/2505.06010", "abs": "https://arxiv.org/abs/2505.06010", "authors": ["Dawid Wisniewski", "Mikolaj Pokrywka", "Zofia Rostek"], "title": "Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective", "categories": ["cs.CL"], "comment": "Accepted at MTSummit 2025 (The 20th Machine Translation Summit)", "summary": "Current machine translation models provide us with high-quality outputs in\nmost scenarios. However, they still face some specific problems, such as\ndetecting which entities should not be changed during translation. In this\npaper, we explore the abilities of popular NMT models, including models from\nthe OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities\nsuch as URL addresses, IBAN numbers, or emails when producing translations\nbetween four languages: English, German, Polish, and Ukrainian. We investigate\nthe quality of popular NMT models in terms of accuracy, discuss errors made by\nthe models, and examine the reasons for errors. Our analysis highlights\nspecific categories, such as emojis, that pose significant challenges for many\nmodels considered. In addition to the analysis, we propose a new multilingual\nsynthetic dataset of 36,000 sentences that can help assess the quality of\nentity transfer across nine categories and four aforementioned languages.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f53\u524d\u6d41\u884cNMT\u6a21\u578b\u5728\u7ffb\u8bd1\u65f6\u4fdd\u7559\u5b9e\u4f53\uff08\u5982URL\u3001IBAN\u53f7\u7801\u7b49\uff09\u7684\u80fd\u529b\uff0c\u5206\u6790\u4e86\u5176\u9519\u8bef\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u65b0\u591a\u8bed\u8a00\u5408\u6210\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u5b9e\u4f53\u4f20\u8f93\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u5728\u591a\u6570\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u5904\u7406\u7279\u5b9a\u5b9e\u4f53\uff08\u5982URL\u3001\u7535\u5b50\u90ae\u4ef6\u7b49\uff09\u65f6\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5OPUS\u3001Google Translate\u3001MADLAD\u548cEuroLLM\u7b49\u6a21\u578b\u5728\u82f1\u3001\u5fb7\u3001\u6ce2\u3001\u4e4c\u56db\u79cd\u8bed\u8a00\u95f4\u7684\u5b9e\u4f53\u4fdd\u7559\u80fd\u529b\uff0c\u5206\u6790\u9519\u8bef\u7c7b\u578b\u548c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u67d0\u4e9b\u5b9e\u4f53\u7c7b\u522b\uff08\u5982\u8868\u60c5\u7b26\u53f7\uff09\u5bf9\u6a21\u578b\u7684\u6311\u6218\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u6570\u636e\u96c6\u5728\u8bc4\u4f30\u5b9e\u4f53\u4f20\u8f93\u8d28\u91cf\u4e0a\u7684\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5b9e\u4f53\u4fdd\u7559\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u65b0\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u5728\u6b64\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
{"id": "2505.06096", "pdf": "https://arxiv.org/pdf/2505.06096", "abs": "https://arxiv.org/abs/2505.06096", "authors": ["Sam Bush", "Matthew DeLorenzo", "Phat Tieu", "Jeyavijayan Rajendran"], "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog Generation using LLMs", "categories": ["cs.AI"], "comment": "Accepted at DAC 2025", "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30Verilog\u8bad\u7ec3\u7684LLMs\u751f\u6210\u53d7\u7248\u6743\u4fdd\u62a4\u4ee3\u7801\u98ce\u9669\u7684\u57fa\u51c6\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b22\u4e07\u6587\u4ef6\u7684\u5f00\u6e90Verilog\u6570\u636e\u96c6FreeSet\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u6570\u636e\u96c6\u6574\u7406\u6846\u67b6\u548c\u6301\u7eed\u7684\u9884\u8bad\u7ec3\uff0c\u5fae\u8c03\u7684FreeV\u6a21\u578b\u5728\u7248\u6743\u4fb5\u72af\u98ce\u9669\uff083%\uff09\u548c\u529f\u80fd\u751f\u6210\uff08VerilogEval pass@10\u63d0\u534710%\uff09\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\uff08\u5982\u751f\u6210\u529f\u80fdVerilog\u4ee3\u7801\uff09\u4e0a\u7684\u80fd\u529b\u6709\u9650\uff0c\u4e14\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u7248\u6743\u68c0\u67e5\u4e0d\u8db3\uff0c\u53ef\u80fd\u5bfc\u81f4\u5fae\u8c03\u540e\u7684LLM\u4fb5\u72af\u7248\u6743\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e00\u98ce\u9669\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4ee5\u91cf\u5316\u98ce\u9669\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90\u6570\u636e\u96c6FreeSet\uff0822\u4e07\u6587\u4ef6\uff09\u53ca\u5176\u81ea\u52a8\u5316\u6574\u7406\u6846\u67b6\u3002\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\u5fae\u8c03Llama\u6a21\u578b\uff0c\u5f97\u5230Verilog\u4e13\u7528\u6a21\u578bFreeV\u3002", "result": "FreeV\u7684\u7248\u6743\u4fb5\u72af\u98ce\u9669\u6700\u4f4e\uff08\u4ec53%\uff09\uff0c\u4e14\u5728\u529f\u80fd\u751f\u6210\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff08VerilogEval pass@10\u63d0\u534710%\uff09\u3002", "conclusion": "\u5f00\u6e90\u6570\u636e\u96c6\u548c\u81ea\u52a8\u5316\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86LLM\u7684\u7248\u6743\u98ce\u9669\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u5728Verilog\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u9886\u57df\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05625", "pdf": "https://arxiv.org/pdf/2505.05625", "abs": "https://arxiv.org/abs/2505.05625", "authors": ["Wenqing Peng", "Zhi-Song Liu", "Michael Boy"], "title": "SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimating rate constants from complex chemical reactions is essential for\nadvancing detailed chemistry. However, the stiffness inherent in real-world\natmospheric chemistry systems poses severe challenges, leading to training\ninstability and poor convergence that hinder effective rate constant estimation\nusing learning-based approaches. To address this, we propose a Stiff\nPhysics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction\nmodelling. Our method introduces a three-stage optimisation process: first, a\nlatent neural ODE learns the continuous and differentiable trajectory between\nchemical concentrations and their time derivatives; second, an explicit\nChemical Reaction Neural Network (CRNN) extracts the underlying rate\ncoefficients based on the learned dynamics; and third, fine-tune CRNN using a\nneural ODE solver to further improve rate coefficient estimation. Extensive\nexperiments on both synthetic and newly proposed real-world datasets validate\nthe effectiveness and robustness of our approach. As the first work on stiff\nNeural ODEs for chemical rate coefficient discovery, our study opens promising\ndirections for integrating neural networks with detailed chemistry.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u5316\u5b66\u53cd\u5e94\u4e2d\u901f\u7387\u5e38\u6570\u4f30\u8ba1\u7684\u96be\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SPIN-ODE\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u4f18\u5316\u8fc7\u7a0b\u5b66\u4e60\u5316\u5b66\u53cd\u5e94\u7684\u8fde\u7eed\u8f68\u8ff9\uff0c\u63d0\u53d6\u901f\u7387\u7cfb\u6570\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f18\u5316\u4f30\u8ba1\u7ed3\u679c\u3002", "motivation": "\u771f\u5b9e\u5927\u6c14\u5316\u5b66\u53cd\u5e94\u4e2d\u7684\u521a\u6027\u5bfc\u81f4\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u901f\u7387\u5e38\u6570\u4f30\u8ba1\u4e2d\u9762\u4e34\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528\u6f5c\u5728\u795e\u7ecfODE\u5b66\u4e60\u5316\u5b66\u6d53\u5ea6\u53ca\u5176\u65f6\u95f4\u5bfc\u6570\u7684\u8fde\u7eed\u8f68\u8ff9\uff1b2. \u901a\u8fc7\u5316\u5b66\u53cd\u5e94\u795e\u7ecf\u7f51\u7edc\uff08CRNN\uff09\u63d0\u53d6\u901f\u7387\u7cfb\u6570\uff1b3. \u901a\u8fc7\u795e\u7ecfODE\u6c42\u89e3\u5668\u5fae\u8c03CRNN\u4ee5\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SPIN-ODE\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4f5c\u4e3a\u9996\u4e2a\u9488\u5bf9\u521a\u6027\u795e\u7ecfODE\u7684\u5316\u5b66\u901f\u7387\u7cfb\u6570\u53d1\u73b0\u7814\u7a76\uff0c\u8be5\u5de5\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e0e\u8be6\u7ec6\u5316\u5b66\u7684\u6574\u5408\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2505.06027", "pdf": "https://arxiv.org/pdf/2505.06027", "abs": "https://arxiv.org/abs/2505.06027", "authors": ["Stefan Vasilev", "Christian Herold", "Baohao Liao", "Seyyed Hadi Hashemi", "Shahram Khadivi", "Christof Monz"], "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "16 pages, 6 figures, 5 tables, under review at ACL", "summary": "This paper introduces Unilogit, a novel self-distillation method for machine\nunlearning in Large Language Models. Unilogit addresses the challenge of\nselectively forgetting specific information while maintaining overall model\nutility, a critical task in compliance with data privacy regulations like GDPR.\nUnlike prior methods that rely on static hyperparameters or starting model\noutputs, Unilogit dynamically adjusts target logits to achieve a uniform\nprobability for the target token, leveraging the current model's outputs for\nmore accurate self-distillation targets. This approach not only eliminates the\nneed for additional hyperparameters but also enhances the model's ability to\napproximate the golden targets. Extensive experiments on public benchmarks and\nan in-house e-commerce dataset demonstrate Unilogit's superior performance in\nbalancing forget and retain objectives, outperforming state-of-the-art methods\nsuch as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness\nacross various scenarios, highlighting its practical applicability and\neffectiveness in achieving efficacious machine unlearning.", "AI": {"tldr": "Unilogit\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u3002\u5b83\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u76ee\u6807logits\u6765\u9009\u62e9\u6027\u9057\u5fd8\u7279\u5b9a\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5982NPO\u548cUnDIAL\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u9690\u79c1\u6cd5\u89c4\uff08\u5982GDPR\uff09\u8981\u6c42\u7684\u9009\u62e9\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002", "method": "\u52a8\u6001\u8c03\u6574\u76ee\u6807logits\uff0c\u5229\u7528\u5f53\u524d\u6a21\u578b\u8f93\u51fa\u751f\u6210\u66f4\u51c6\u786e\u7684\u81ea\u84b8\u998f\u76ee\u6807\uff0c\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u548c\u5185\u90e8\u7535\u5546\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u8861\u9057\u5fd8\u4e0e\u4fdd\u7559\u76ee\u6807\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Unilogit\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u9057\u5fd8\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2505.06191", "pdf": "https://arxiv.org/pdf/2505.06191", "abs": "https://arxiv.org/abs/2505.06191", "authors": ["Jiayuan Mao", "Joshua B. Tenenbaum", "Jiajun Wu"], "title": "Neuro-Symbolic Concepts", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "comment": "To appear in Communications of the ACM", "summary": "This article presents a concept-centric paradigm for building agents that can\nlearn continually and reason flexibly. The concept-centric agent utilizes a\nvocabulary of neuro-symbolic concepts. These concepts, such as object,\nrelation, and action concepts, are grounded on sensory inputs and actuation\noutputs. They are also compositional, allowing for the creation of novel\nconcepts through their structural combination. To facilitate learning and\nreasoning, the concepts are typed and represented using a combination of\nsymbolic programs and neural network representations. Leveraging such\nneuro-symbolic concepts, the agent can efficiently learn and recombine them to\nsolve various tasks across different domains, ranging from 2D images, videos,\n3D scenes, and robotic manipulation tasks. This concept-centric framework\noffers several advantages, including data efficiency, compositional\ngeneralization, continual learning, and zero-shot transfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6982\u5ff5\u6784\u5efa\u6301\u7eed\u5b66\u4e60\u548c\u7075\u6d3b\u63a8\u7406\u7684\u667a\u80fd\u4f53\uff0c\u5177\u5907\u9ad8\u6548\u6570\u636e\u5229\u7528\u3001\u7ec4\u5408\u6cdb\u5316\u7b49\u4f18\u52bf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u9886\u57df\u4efb\u52a1\u4e2d\u6301\u7eed\u5b66\u4e60\u548c\u7075\u6d3b\u63a8\u7406\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u548c\u7b26\u53f7\u8868\u793a\u7684\u6982\u5ff5\u4e2d\u5fc3\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6982\u5ff5\uff08\u5982\u5bf9\u8c61\u3001\u5173\u7cfb\u548c\u52a8\u4f5c\u6982\u5ff5\uff09\uff0c\u901a\u8fc7\u7b26\u53f7\u7a0b\u5e8f\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u8868\u793a\uff0c\u652f\u6301\u6982\u5ff5\u7684\u7ec4\u5408\u4e0e\u91cd\u7528\u3002", "result": "\u8be5\u6846\u67b6\u57282D\u56fe\u50cf\u3001\u89c6\u9891\u30013D\u573a\u666f\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6548\u5b66\u4e60\u3001\u7ec4\u5408\u6cdb\u5316\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u6982\u5ff5\u4e2d\u5fc3\u6846\u67b6\u4e3a\u591a\u9886\u57df\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6570\u636e\u5229\u7528\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.05650", "pdf": "https://arxiv.org/pdf/2505.05650", "abs": "https://arxiv.org/abs/2505.05650", "authors": ["Tien Dang", "Truong-Son Hy"], "title": "EquiHGNN: Scalable Rotationally Equivariant Hypergraph Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Molecular interactions often involve high-order relationships that cannot be\nfully captured by traditional graph-based models limited to pairwise\nconnections. Hypergraphs naturally extend graphs by enabling multi-way\ninteractions, making them well-suited for modeling complex molecular systems.\nIn this work, we introduce EquiHGNN, an Equivariant HyperGraph Neural Network\nframework that integrates symmetry-aware representations to improve molecular\nmodeling. By enforcing the equivariance under relevant transformation groups,\nour approach preserves geometric and topological properties, leading to more\nrobust and physically meaningful representations. We examine a range of\nequivariant architectures and demonstrate that integrating symmetry constraints\nleads to notable performance gains on large-scale molecular datasets.\nExperiments on both small and large molecules show that high-order interactions\noffer limited benefits for small molecules but consistently outperform 2D\ngraphs on larger ones. Adding geometric features to these high-order structures\nfurther improves the performance, emphasizing the value of spatial information\nin molecular learning. Our source code is available at\nhttps://github.com/HySonLab/EquiHGNN/", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EquiHGNN\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u591a\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\u6846\u67b6\uff0c\u5229\u7528\u5bf9\u79f0\u6027\u611f\u77e5\u8868\u793a\u63d0\u5347\u5206\u5b50\u5efa\u6a21\u6548\u679c\uff0c\u7279\u522b\u5728\u5927\u5206\u5b50\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u7684\u6a21\u578b\u53ea\u80fd\u5904\u7406\u4e8c\u5143\u5173\u7cfb\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u5206\u5b50\u95f4\u590d\u6742\u7684\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u3002\u8d85\u56fe\u901a\u8fc7\u591a\u8fb9\u8fde\u63a5\u6269\u5c55\u4e86\u56fe\u7684\u8868\u793a\u80fd\u529b\uff0c\u66f4\u9002\u5408\u5efa\u6a21\u590d\u6742\u5206\u5b50\u7cfb\u7edf", "method": "\u63d0\u51fa\u4e86EquiHGNN\u6846\u67b6\uff0c\u5f15\u5165\u7b49\u53d8\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5f3a\u5236\u76f8\u5173\u53d8\u6362\u7fa4\u4e0b\u7684\u7b49\u53d8\u6027\u6765\u4fdd\u6301\u51e0\u4f55\u548c\u62d3\u6251\u7279\u6027\uff0c\u6784\u5efa\u66f4\u5177\u9c81\u68d2\u6027\u7684\u7269\u7406\u610f\u4e49\u8868\u793a", "result": "\u5b9e\u9a8c\u8868\u660e\u5bf9\u79f0\u6027\u7ea6\u675f\u5e26\u6765\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u5bf9\u5c0f\u5206\u5b50\u5e2e\u52a9\u6709\u9650\uff0c\u4f46\u80fd\u6301\u7eed\u8d85\u8d8a\u4e8c\u7ef4\u56fe\u6a21\u578b\u5728\u5927\u5206\u5b50\u4e0a\u7684\u8868\u73b0\u3002\u52a0\u5165\u51e0\u4f55\u7279\u5f81\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd", "conclusion": "\u7a7a\u95f4\u4fe1\u606f\u5728\u5206\u5b50\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u7ed3\u5408\u9ad8\u9636\u7ed3\u6784\u548c\u51e0\u4f55\u7279\u5f81\u7684\u7b49\u53d8\u8d85\u56fe\u7f51\u7edc\u80fd\u66f4\u6709\u6548\u5730\u5efa\u6a21\u590d\u6742\u5206\u5b50\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5927\u5206\u5b50"}}
{"id": "2505.06046", "pdf": "https://arxiv.org/pdf/2505.06046", "abs": "https://arxiv.org/abs/2505.06046", "authors": ["Joshua Harris", "Fan Grayson", "Felix Feldman", "Timothy Laurence", "Toby Nonnenmacher", "Oliver Higgins", "Leo Loman", "Selina Patel", "Thomas Finnie", "Samuel Collins", "Michael Borowitz"], "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information", "categories": ["cs.CL", "cs.LG", "68T50"], "comment": "24 pages, 10 pages main text", "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries, created via an automated pipeline. We also\nrelease a new dataset of the extracted UK Government public health guidance\ndocuments used as source text for PubHealthBench. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% in the MCQA setup, and outperform\nhumans with cursory search engine use. However, in the free form setup we see\nlower performance with no model scoring >75%. Therefore, whilst there are\npromising signs that state of the art (SOTA) LLMs are an increasingly accurate\nsource of public health information, additional safeguards or tools may still\nbe needed when providing free form responses on public health topics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u57fa\u51c6PubHealthBench\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u516c\u5171\u5065\u5eb7\u9886\u57df\u7684\u77e5\u8bc6\u8868\u73b0\uff0c\u53d1\u73b0\u9876\u5c16\u6a21\u578b\u5728\u9009\u62e9\u9898\u56de\u7b54\u4e2d\u8868\u73b0\u4f18\u5f02\uff08>90%\uff09\uff0c\u4f46\u81ea\u7531\u56de\u7b54\u5f97\u5206\u8f83\u4f4e\uff08<75%\uff09\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e86\u89e3\u5176\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u516c\u5171\u5065\u5eb7\uff09\u7684\u77e5\u8bc6\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u8fd9\u4e9b\u6a21\u578b\u7684\u9519\u8bef\u53ef\u80fd\u5bf9\u516c\u4f17\u5065\u5eb7\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u6d41\u7a0b\u521b\u5efa\u5305\u542b8000\u591a\u4e2a\u95ee\u9898\u7684PubHealthBench\u57fa\u51c6\uff0c\u8bc4\u4f3024\u4e2aLLMs\u5728\u9009\u62e9\u9898\u548c\u81ea\u7531\u56de\u7b54\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u5e03\u76f8\u5173\u6570\u636e\u96c6\u3002", "result": "\u9876\u5c16LLMs\uff08\u5982GPT-4.5\uff09\u5728\u9009\u62e9\u9898\u4e2d\u5f97\u5206>90%\uff0c\u8d85\u8d8a\u4eba\u7c7b\u7b80\u5355\u641c\u7d22\u5f15\u64ce\u4f7f\u7528\uff1b\u4f46\u81ea\u7531\u56de\u7b54\u5f97\u5206\u5747\u4f4e\u4e8e75%\u3002", "conclusion": "\u5c3d\u7ba1SOTA LLMs\u5728\u516c\u5171\u5065\u5eb7\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u81ea\u7531\u56de\u7b54\u573a\u666f\u4ecd\u9700\u989d\u5916\u4fdd\u969c\u63aa\u65bd\u6216\u5de5\u5177\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\u3002"}}
{"id": "2505.04999", "pdf": "https://arxiv.org/pdf/2505.04999", "abs": "https://arxiv.org/abs/2505.04999", "authors": ["Anthony Liang", "Pavel Czempin", "Matthew Hong", "Yutai Zhou", "Erdem Biyik", "Stephen Tu"], "title": "CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Latent Action Models, Self-supervised Pretraining, Learning from\n  Videos", "summary": "Learning robot policies using imitation learning requires collecting large\namounts of costly action-labeled expert demonstrations, which fundamentally\nlimits the scale of training data. A promising approach to address this\nbottleneck is to harness the abundance of unlabeled observations-e.g., from\nvideo demonstrations-to learn latent action labels in an unsupervised way.\nHowever, we find that existing methods struggle when applied to complex robot\ntasks requiring fine-grained motions. We design continuous latent action models\n(CLAM) which incorporate two key ingredients we find necessary for learning to\nsolve complex continuous control tasks from unlabeled observation data: (a)\nusing continuous latent action labels instead of discrete representations, and\n(b) jointly training an action decoder to ensure that the latent action space\ncan be easily grounded to real actions with relatively few labeled examples.\nImportantly, the labeled examples can be collected from non-optimal play data,\nenabling CLAM to learn performant policies without access to any action-labeled\nexpert data. We demonstrate on continuous control benchmarks in DMControl\n(locomotion) and MetaWorld (manipulation), as well as on a real WidowX robot\narm that CLAM significantly outperforms prior state-of-the-art methods,\nremarkably with a 2-3x improvement in task success rate compared to the best\nbaseline. Videos and code can be found at clamrobot.github.io.", "AI": {"tldr": "\u6458\u8981\u4e3b\u8981\u4ecb\u7ecd\u4e86CLAM\uff08\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\uff09\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u65e0\u6807\u8bb0\u7684\u89c2\u6d4b\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u9700\u8981\u5927\u91cf\u6807\u8bb0\u4e13\u5bb6\u793a\u8303\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u5927\u91cf\u6807\u8bb0\u4e13\u5bb6\u793a\u8303\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u4e30\u5bcc\u7684\u65e0\u6807\u8bb0\u89c2\u6d4b\u6570\u636e\uff08\u5982\u89c6\u9891\u793a\u8303\uff09\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u52a8\u4f5c\u7684\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51faCLAM\u6a21\u578b\uff0c\u7ed3\u5408\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a(a) \u4f7f\u7528\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\u6807\u7b7e\u800c\u975e\u79bb\u6563\u8868\u793a\uff0c(b) \u8054\u5408\u8bad\u7ec3\u52a8\u4f5c\u89e3\u7801\u5668\uff0c\u786e\u4fdd\u6f5c\u5728\u52a8\u4f5c\u7a7a\u95f4\u80fd\u4ee5\u5c11\u91cf\u6807\u8bb0\u793a\u4f8b\u5feb\u901f\u6620\u5c04\u5230\u771f\u5b9e\u52a8\u4f5c\u3002\u6807\u8bb0\u6570\u636e\u53ef\u6765\u81ea\u975e\u6700\u4f18\u793a\u8303\uff0c\u65e0\u9700\u4e13\u5bb6\u6570\u636e\u3002", "result": "\u5728DMControl\uff08\u8fd0\u52a8\uff09\u548cMetaWorld\uff08\u64cd\u4f5c\uff09\u7684\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4ee5\u53ca\u771f\u5b9eWidowX\u673a\u68b0\u81c2\u4e0a\uff0cCLAM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53472-3\u500d\u3002", "conclusion": "CLAM\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u4ece\u65e0\u6807\u8bb0\u6570\u636e\u4e2d\u5b66\u4e60\u8fde\u7eed\u6f5c\u5728\u52a8\u4f5c\uff0c\u5927\u5e45\u51cf\u5c11\u5bf9\u6807\u8bb0\u4e13\u5bb6\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e14\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2505.05677", "pdf": "https://arxiv.org/pdf/2505.05677", "abs": "https://arxiv.org/abs/2505.05677", "authors": ["Winston Chen", "Trenton Chang", "Jenna Wiens"], "title": "Conditional Front-door Adjustment for Heterogeneous Treatment Assignment Effect Estimation Under Non-adherence", "categories": ["cs.LG"], "comment": "Accepted by Conference on Health, Inference, and Learning (CHIL) 2025", "summary": "Estimates of heterogeneous treatment assignment effects can inform treatment\ndecisions. Under the presence of non-adherence (e.g., patients do not adhere to\ntheir assigned treatment), both the standard backdoor adjustment (SBD) and the\nconditional front-door adjustment (CFD) can recover unbiased estimates of the\ntreatment assignment effects. However, the estimation variance of these\napproaches may vary widely across settings, which remains underexplored in the\nliterature. In this work, we demonstrate theoretically and empirically that CFD\nyields lower-variance estimates than SBD when the true effect of treatment\nassignment is small (i.e., assigning an intervention leads to small changes in\npatients' future outcome). Additionally, since CFD requires estimating multiple\nnuisance parameters, we introduce LobsterNet, a multi-task neural network that\nimplements CFD with joint modeling of the nuisance parameters. Empirically,\nLobsterNet reduces estimation error across several semi-synthetic and\nreal-world datasets compared to baselines. Our findings suggest CFD with shared\nnuisance parameter modeling can improve treatment assignment effect estimation\nunder non-adherence.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u975e\u4f9d\u4ece\u6027\u5b58\u5728\u4e0b\uff0cCFD\u65b9\u6cd5\u6bd4SBD\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u4f4e\u65b9\u5dee\u7684\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u6cbb\u7597\u6548\u679c\u8f83\u5c0f\u65f6\u3002\u901a\u8fc7\u5f15\u5165LobsterNet\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c0f\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u5728\u975e\u4f9d\u4ece\u6027\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982SBD\u548cCFD\uff09\u867d\u80fd\u65e0\u504f\u4f30\u8ba1\u6cbb\u7597\u6548\u679c\uff0c\u4f46\u5176\u4f30\u8ba1\u65b9\u5dee\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83CFD\u4e0eSBD\u7684\u65b9\u5dee\u6027\u80fd\uff0c\u5e76\u6539\u8fdbCFD\u7684\u4f30\u8ba1\u6548\u679c\u3002", "method": "\u4f5c\u8005\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CFD\u5728\u771f\u5b9e\u6548\u679c\u8f83\u5c0f\u65f6\u7684\u65b9\u5dee\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86LobsterNet\u8fd9\u4e00\u591a\u4efb\u52a1\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u591a\u4e2a\u5e72\u6270\u53c2\u6570\u6765\u5b9e\u73b0CFD\u4f30\u8ba1\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0cCFD\u5728\u771f\u5b9e\u6548\u679c\u8f83\u5c0f\u65f6\u65b9\u5dee\u66f4\u4f4e\uff0c\u4e14LobsterNet\u5728\u534a\u5408\u6210\u53ca\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u964d\u4f4e\u4e86\u4f30\u8ba1\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff0cCFD\u7ed3\u5408\u5171\u4eab\u5e72\u6270\u53c2\u6570\u5efa\u6a21\uff08\u5982LobsterNet\uff09\u80fd\u5728\u975e\u4f9d\u4ece\u6027\u60c5\u5883\u4e0b\u63d0\u5347\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.06062", "pdf": "https://arxiv.org/pdf/2505.06062", "abs": "https://arxiv.org/abs/2505.06062", "authors": ["Iuliia Zaitova", "Vitalii Hirak", "Badr M. Abdullah", "Dietrich Klakow", "Bernd M\u00f6bius", "Tania Avgustinova"], "title": "Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax", "categories": ["cs.CL"], "comment": "10 pages, 3 figures. Findings 2025", "summary": "This study analyzes the attention patterns of fine-tuned encoder-only models\nbased on the BERT architecture (BERT-based models) towards two distinct types\nof Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms\npresent challenges in semantic non-compositionality, whereas MSUs demonstrate\nunconventional syntactic behavior that does not conform to standard grammatical\ncategorizations. We aim to understand whether fine-tuning BERT-based models on\nspecific tasks influences their attention to MWEs, and how this attention\ndiffers between semantic and syntactic tasks. We examine attention scores to\nMWEs in both pre-trained and fine-tuned BERT-based models. We utilize\nmonolingual models and datasets in six Indo-European languages - English,\nGerman, Dutch, Polish, Russian, and Ukrainian. Our results show that\nfine-tuning significantly influences how models allocate attention to MWEs.\nSpecifically, models fine-tuned on semantic tasks tend to distribute attention\nto idiomatic expressions more evenly across layers. Models fine-tuned on\nsyntactic tasks show an increase in attention to MSUs in the lower layers,\ncorresponding with syntactic processing requirements.", "AI": {"tldr": "\u5206\u6790\u663e\u793a\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u5fae\u8c03\u7684BERT\u6a21\u578b\u5bf9\u591a\u8bcd\u8868\u8fbe\uff08MWEs\uff09\u7684\u6ce8\u610f\u529b\u5206\u914d\u53d7\u5230\u4efb\u52a1\u7c7b\u578b\uff08\u8bed\u4e49\u6216\u53e5\u6cd5\uff09\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5fae\u8c03\u662f\u5426\u5f71\u54cdBERT\u6a21\u578b\u5bf9\u591a\u8bcd\u8868\u8fbe\uff08MWEs\uff09\u7684\u6ce8\u610f\u529b\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6ce8\u610f\u529b\u5728\u8bed\u4e49\u548c\u53e5\u6cd5\u4efb\u52a1\u4e2d\u7684\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u516d\u79cd\u5370\u6b27\u8bed\u8a00\u7684\u5355\u8bed\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u540e\u7684BERT\u6a21\u578b\u5728MWEs\u4e0a\u7684\u6ce8\u610f\u529b\u5206\u6570\u3002", "result": "\u5fae\u8c03\u663e\u8457\u5f71\u54cd\u6a21\u578b\u5bf9MWEs\u7684\u6ce8\u610f\u529b\u5206\u914d\uff0c\u8bed\u4e49\u4efb\u52a1\u4f7f\u6a21\u578b\u5bf9\u60ef\u7528\u8bed\u7684\u6ce8\u610f\u529b\u5728\u5404\u5c42\u66f4\u5747\u5300\uff0c\u53e5\u6cd5\u4efb\u52a1\u5219\u589e\u5f3a\u4f4e\u5c42\u5bf9\u5fae\u53e5\u6cd5\u5355\u5143\u7684\u5173\u6ce8\u3002", "conclusion": "\u5fae\u8c03\u65b9\u5411\uff08\u8bed\u4e49\u6216\u53e5\u6cd5\uff09\u76f4\u63a5\u5f71\u54cdBERT\u6a21\u578b\u5bf9\u591a\u8bcd\u8868\u8fbe\u7684\u5173\u6ce8\u6a21\u5f0f\uff0c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u7c7b\u578b\u5bf9\u6a21\u578b\u6ce8\u610f\u529b\u673a\u5236\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2505.05481", "pdf": "https://arxiv.org/pdf/2505.05481", "abs": "https://arxiv.org/abs/2505.05481", "authors": ["Ryan Williams"], "title": "Structure & Quality: Conceptual and Formal Foundations for the Mind-Body Problem", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "This paper explores the hard problem of consciousness from a different\nperspective. Instead of drawing distinctions between the physical and the\nmental, an exploration of a more foundational relationship is examined: the\nrelationship between structure and quality.\n  Information-theoretic measures are developed to quantify the mutual\ndeterminability between structure and quality, including a novel Q-S space for\nanalyzing fidelity between the two domains. This novel space naturally points\ntoward a five-fold categorization of possible relationships between structural\nand qualitative properties, illustrating each through conceptual and formal\nmodels.\n  The ontological implications of each category are examined, shedding light on\ndebates around functionalism, emergentism, idealism, panpsychism, and neutral\nmonism.\n  This new line of inquiry has established a framework for deriving theoretical\nconstraints on qualitative systems undergoing evolution that is explored in my\ncompanion paper, Qualia & Natural Selection.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ece\u7ed3\u6784\uff08structure\uff09\u4e0e\u8d28\uff08quality\uff09\u7684\u5173\u7cfb\u5165\u624b\uff0c\u63a2\u7d22\u4e86\u610f\u8bc6\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6846\u67b6\uff08Q-S\u7a7a\u95f4\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5206\u7c7b\u4e86\u4e94\u79cd\u53ef\u80fd\u7684\u5173\u7cfb\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u5404\u7c7b\u7684\u54f2\u5b66\u610f\u4e49\u3002", "motivation": "\u4f20\u7edf\u610f\u8bc6\u7814\u7a76\u5e38\u533a\u5206\u7269\u7406\u4e0e\u5fc3\u667a\uff0c\u672c\u6587\u5219\u8bd5\u56fe\u4ece\u66f4\u57fa\u7840\u7684\u7ed3\u6784\u4e0e\u8d28\u7684\u5173\u7cfb\u5207\u5165\uff0c\u4e3a\u610f\u8bc6\u95ee\u9898\u63d0\u4f9b\u65b0\u7684\u5206\u6790\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u8bba\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u91cf\u5316\u7ed3\u6784-\u8d28\u4e92\u5b9a\u6027\u7684\u6307\u6807\uff0c\u5e76\u6784\u5efa\u4e86Q-S\u7a7a\u95f4\u6a21\u578b\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u4e0e\u6982\u5ff5\u6a21\u578b\u5206\u6790\u5176\u5173\u7cfb\u3002", "result": "\u63d0\u51fa\u4e86\u4e94\u79cd\u7ed3\u6784-\u8d28\u5173\u7cfb\u7684\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u529f\u80fd\u4e3b\u4e49\u3001\u6d8c\u73b0\u8bba\u7b49\u54f2\u5b66\u6d41\u6d3e\u4e2d\u7684\u610f\u4e49\u3002\u540c\u65f6\uff0c\u6846\u67b6\u4e3a\u5b9a\u6027\u7cfb\u7edf\u7684\u6f14\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u7ea6\u675f\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u4e0e\u8d28\u7684\u5173\u7cfb\u7814\u7a76\uff0c\u4e3a\u610f\u8bc6\u96be\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u548c\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u62d3\u5c55\u4e86\u5176\u5728\u6f14\u5316\u7406\u8bba\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.05683", "pdf": "https://arxiv.org/pdf/2505.05683", "abs": "https://arxiv.org/abs/2505.05683", "authors": ["Udaya Allani"], "title": "Interactive Diabetes Risk Prediction Using Explainable Machine Learning: A Dash-Based Approach with SHAP, LIME, and Comorbidity Insights", "categories": ["cs.LG", "cs.AI", "I.2.1; I.5.2; J.3"], "comment": "16 pages, 21 figures, submitted as a preprint for academic\n  dissemination", "summary": "This study presents a web-based interactive health risk prediction tool\ndesigned to assess diabetes risk using machine learning models. Built on the\n2015 CDC BRFSS dataset, the study evaluates models including Logistic\nRegression, Random Forest, XGBoost, LightGBM, KNN, and Neural Networks under\noriginal, SMOTE, and undersampling strategies. LightGBM with undersampling\nachieved the best recall, making it ideal for risk detection. The tool\nintegrates SHAP and LIME to explain predictions and highlights comorbidity\ncorrelations using Pearson analysis. A Dash-based UI enables user-friendly\ninteraction with model predictions, personalized suggestions, and feature\ninsights, supporting data-driven health awareness.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u4ea4\u4e92\u5f0f\u5065\u5eb7\u98ce\u9669\u9884\u6d4b\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u7cd6\u5c3f\u75c5\u98ce\u9669\uff0c\u91c7\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u91c7\u6837\u7b56\u7565\uff0c\u5176\u4e2dLightGBM\u7ed3\u5408\u6b20\u91c7\u6837\u8868\u73b0\u6700\u4f73\u3002\u5de5\u5177\u8fd8\u6574\u5408\u4e86SHAP\u548cLIME\u89e3\u91ca\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7Dash UI\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u4ea4\u4e92\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u63d0\u9ad8\u7cd6\u5c3f\u75c5\u98ce\u9669\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7528\u6237\u7406\u89e3\uff0c\u4fc3\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u5065\u5eb7\u610f\u8bc6\u3002", "method": "\u57fa\u4e8e2015\u5e74CDC BRFSS\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86Logistic\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001LightGBM\u3001KNN\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u591a\u79cd\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u6570\u636e\u3001SMOTE\u548c\u6b20\u91c7\u6837\u7b56\u7565\u3002", "result": "LightGBM\u7ed3\u5408\u6b20\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u4e86\u6700\u4f73\u53ec\u56de\u7387\uff0c\u9002\u5408\u98ce\u9669\u68c0\u6d4b\u3002\u5de5\u5177\u901a\u8fc7SHAP\u548cLIME\u63d0\u4f9b\u9884\u6d4b\u89e3\u91ca\uff0c\u5e76\u5229\u7528Pearson\u5206\u6790\u7a81\u51fa comorbidities \u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u5de5\u5177\u5728\u7cd6\u5c3f\u75c5\u98ce\u9669\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u901a\u8fc7\u4ea4\u4e92\u5f0f\u754c\u9762\u548c\u89e3\u91ca\u6027\u5206\u6790\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u5065\u5eb7\u610f\u8bc6\u3002"}}
{"id": "2505.06110", "pdf": "https://arxiv.org/pdf/2505.06110", "abs": "https://arxiv.org/abs/2505.06110", "authors": ["Jugal Gajjar", "Kaustik Ranaware"], "title": "Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 2 figures, 5 tables, and 19 references", "summary": "This project performs multimodal sentiment analysis using the CMU-MOSEI\ndataset, using transformer-based models with early fusion to integrate text,\naudio, and visual modalities. We employ BERT-based encoders for each modality,\nextracting embeddings that are concatenated before classification. The model\nachieves strong performance, with 97.87\\% 7-class accuracy and a 0.9682\nF1-score on the test set, demonstrating the effectiveness of early fusion in\ncapturing cross-modal interactions. The training utilized Adam optimization\n(lr=1e-4), dropout (0.3), and early stopping to ensure generalization and\nrobustness. Results highlight the superiority of transformer architectures in\nmodeling multimodal sentiment, with a low MAE (0.1060) indicating precise\nsentiment intensity prediction. Future work may compare fusion strategies or\nenhance interpretability. This approach utilizes multimodal learning by\neffectively combining linguistic, acoustic, and visual cues for sentiment\nanalysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65e9\u671f\u878d\u5408\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\uff0c\u4f7f\u7528CMU-MOSEI\u6570\u636e\u96c6\uff0c\u6574\u5408\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\uff0c\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u591a\u6a21\u6001\u6570\u636e\uff08\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\uff09\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u8054\u5408\u5efa\u6a21\u80fd\u529b\uff0c\u9a8c\u8bc1\u65e9\u671f\u878d\u5408\u7b56\u7565\u53caTransformer\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528BERT\u7f16\u7801\u5668\u5206\u522b\u5904\u7406\u5404\u6a21\u6001\u6570\u636e\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\uff08\u62fc\u63a5\u5d4c\u5165\uff09\u6574\u5408\u7279\u5f81\uff0c\u4f7f\u7528Adam\u4f18\u5316\u5668\uff08lr=1e-4\uff09\u3001dropout\uff080.3\uff09\u548c\u65e9\u505c\u6cd5\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u52307\u7c7b\u5206\u7c7b\u51c6\u786e\u738797.87%\u548cF1\u5206\u65700.9682\uff0cMAE\u4e3a0.1060\uff0c\u8868\u660e\u5bf9\u60c5\u611f\u5f3a\u5ea6\u7684\u9884\u6d4b\u7cbe\u786e\u3002", "conclusion": "Transformer\u67b6\u6784\u7ed3\u5408\u65e9\u671f\u878d\u5408\u80fd\u6709\u6548\u6355\u6349\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u672a\u6765\u53ef\u6bd4\u8f83\u4e0d\u540c\u878d\u5408\u7b56\u7565\u6216\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.05486", "pdf": "https://arxiv.org/pdf/2505.05486", "abs": "https://arxiv.org/abs/2505.05486", "authors": ["Anthony Kiggundu", "Dennis Krummacker", "Hans D. Schotten"], "title": "FedAvgen: Metadata for Model Aggregation In Communication Systems", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE NetSoft 2025", "summary": "To improve business efficiency and minimize costs, Artificial Intelligence\n(AI) practitioners have adopted a shift from formulating models from scratch\ntowards sharing pretrained models. The pretrained models are then aggregated\ninto a global model with higher generalization capabilities, which is\nafterwards distributed to the client devices. This approach is known as\nfederated learning and inherently utilizes different techniques to select the\ncandidate client models averaged to obtain the global model. This approach, in\nthe case of communication systems, faces challenges arising from the\nexistential diversity in device profiles. The multiplicity in profiles\nmotivates our conceptual assessment of a metaheuristic algorithm (FedAvgen),\nwhich relates each pretrained model with its weight space as metadata, to a\nphenotype and genotype, respectively. This parent-child genetic evolution\ncharacterizes the global averaging step in federated learning. We then compare\nthe results of our approach to two widely adopted baseline federated learning\nalgorithms like Federated Averaging (FedAvg) and Federated Stochastic Gradient\nDescent (FedSGD).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08FedAvgen\uff09\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u6a21\u578b\u6743\u91cd\u7a7a\u95f4\u5173\u8054\u7684\u5143\u6570\u636e\uff08\u8868\u578b\u4e0e\u57fa\u56e0\u578b\uff09\u4f18\u5316\u5168\u5c40\u6a21\u578b\u7684\u805a\u5408\u6548\u679c\uff0c\u5e76\u5bf9\u6bd4\u4e86FedAvg\u548cFedSGD\u4e24\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u63d0\u5347\u4e1a\u52a1\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u8054\u90a6\u5b66\u4e60\u4e2d\u9700\u89e3\u51b3\u8bbe\u5907\u591a\u6837\u6027\u5bfc\u81f4\u7684\u6a21\u578b\u805a\u5408\u6311\u6218\u3002\u7814\u7a76\u8005\u63d0\u51fa\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff08FedAvgen\uff09\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u7a7a\u95f4\u89c6\u4e3a\u57fa\u56e0\u578b\uff0c\u4ee5\u4f18\u5316\u5168\u5c40\u6a21\u578b\u805a\u5408\u3002", "method": "\u91c7\u7528FedAvgen\u7b97\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u7a7a\u95f4\u4f5c\u4e3a\u5143\u6570\u636e\uff08\u57fa\u56e0\u578b\uff09\uff0c\u4e0e\u6a21\u578b\u8868\u73b0\uff08\u8868\u578b\uff09\u5173\u8054\uff0c\u6a21\u62df\u9057\u4f20\u8fdb\u5316\u8fc7\u7a0b\u8fdb\u884c\u5168\u5c40\u6a21\u578b\u805a\u5408\u3002", "result": "\u4e0eFedAvg\u548cFedSGD\u76f8\u6bd4\uff0cFedAvgen\u5728\u5168\u5c40\u6a21\u578b\u805a\u5408\u4e2d\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FedAvgen\u901a\u8fc7\u5f15\u5165\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bbe\u5907\u591a\u6837\u6027\u95ee\u9898\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u7684\u5168\u5c40\u6a21\u578b\u805a\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05702", "pdf": "https://arxiv.org/pdf/2505.05702", "abs": "https://arxiv.org/abs/2505.05702", "authors": ["Seongjin Choi", "Gahee Kim", "Yong-Geun Oh"], "title": "Hypergraph Neural Sheaf Diffusion: A Symmetric Simplicial Set Framework for Higher-Order Learning", "categories": ["cs.LG", "05C65, 55U10, 68T07"], "comment": "This manuscript has been submitted to IEEE Access for publication", "summary": "The absence of intrinsic adjacency relations and orientation systems in\nhypergraphs creates fundamental challenges for constructing sheaf Laplacians of\narbitrary degrees. We resolve these limitations through symmetric simplicial\nsets derived directly from hypergraphs, which encode all possible oriented\nsubrelations within each hyperedge as ordered tuples. This construction\ncanonically defines adjacency via facet maps while inherently preserving\nhyperedge provenance. We establish that the normalized degree zero sheaf\nLaplacian on our induced symmetric simplicial set reduces exactly to the\ntraditional graph normalized sheaf Laplacian when restricted to graphs,\nvalidating its mathematical consistency with prior graph-based sheaf theory.\nFurthermore, the induced structure preserves all structural information from\nthe original hypergraph, ensuring that every multi-way relational detail is\nfaithfully retained. Leveraging this framework, we introduce Hypergraph Neural\nSheaf Diffusion (HNSD), the first principled extension of Neural Sheaf\nDiffusion (NSD) to hypergraphs. HNSD operates via normalized degree zero sheaf\nLaplacians over symmetric simplicial sets, resolving orientation ambiguity and\nadjacency sparsity inherent to hypergraph learning. Experimental evaluations\ndemonstrate HNSD's competitive performance across established benchmarks.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u8d85\u56fe\u4e2d\u7f3a\u4e4f\u56fa\u6709\u90bb\u63a5\u5173\u7cfb\u548c\u65b9\u5411\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4ece\u8d85\u56fe\u76f4\u63a5\u5bfc\u51fa\u7684\u5bf9\u79f0\u5355\u7eaf\u96c6\u6765\u5b9a\u4e49\u9ad8\u9636\u90bb\u63a5\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e86Hypergraph Neural Sheaf Diffusion (HNSD)\u65b9\u6cd5\u6765\u8fdb\u884c\u8d85\u56fe\u5b66\u4e60\u3002", "motivation": "\u8d85\u56fe\u7f3a\u4e4f\u56fa\u6709\u90bb\u63a5\u5173\u7cfb\u548c\u65b9\u5411\u7cfb\u7edf\uff0c\u5bfc\u81f4\u6784\u9020\u4efb\u610f\u5ea6\u6570\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5b58\u5728\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5bf9\u79f0\u5355\u7eaf\u96c6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u6269\u5c55\u795e\u7ecf\u5c42\u6269\u6563\u65b9\u6cd5\u5230\u8d85\u56fe\u3002", "method": "\u63d0\u51fa\u5bf9\u79f0\u5355\u7eaf\u96c6\u4ece\u8d85\u56fe\u4e2d\u7f16\u7801\u6240\u6709\u53ef\u80fd\u7684\u5b9a\u5411\u5b50\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u8be5\u65b9\u6cd5\u6784\u5efa\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u3002\u8fdb\u4e00\u6b65\u63d0\u51faHNSD\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u51c6\u5316\u96f6\u5ea6\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5728\u5bf9\u79f0\u5355\u7eaf\u96c6\u4e0a\u8fdb\u884c\u8d85\u56fe\u5b66\u4e60\u3002", "result": "\u6807\u51c6\u5316\u96f6\u5ea6\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5728\u5bf9\u79f0\u5355\u7eaf\u96c6\u4e0a\u4e0e\u4f20\u7edf\u56fe\u6807\u51c6\u5316\u5c42\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5728\u56fe\u4e0a\u4e00\u81f4\u3002HNSD\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5bf9\u79f0\u5355\u7eaf\u96c6\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u56fe\u7684\u65b9\u5411\u548c\u90bb\u63a5\u95ee\u9898\uff0cHNSD\u4e3a\u8d85\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2505.06120", "pdf": "https://arxiv.org/pdf/2505.06120", "abs": "https://arxiv.org/abs/2505.06120", "authors": ["Philippe Laban", "Hiroaki Hayashi", "Yingbo Zhou", "Jennifer Neville"], "title": "LLMs Get Lost In Multi-Turn Conversation", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are conversational interfaces. As such, LLMs\nhave the potential to assist their users not only when they can fully specify\nthe task at hand, but also to help them define, explore, and refine what they\nneed through multi-turn conversational exchange. Although analysis of LLM\nconversation logs has confirmed that underspecification occurs frequently in\nuser instructions, LLM evaluation has predominantly focused on the single-turn,\nfully-specified instruction setting. In this work, we perform large-scale\nsimulation experiments to compare LLM performance in single- and multi-turn\nsettings. Our experiments confirm that all the top open- and closed-weight LLMs\nwe test exhibit significantly lower performance in multi-turn conversations\nthan single-turn, with an average drop of 39% across six generation tasks.\nAnalysis of 200,000+ simulated conversations decomposes the performance\ndegradation into two components: a minor loss in aptitude and a significant\nincrease in unreliability. We find that LLMs often make assumptions in early\nturns and prematurely attempt to generate final solutions, on which they overly\nrely. In simpler terms, we discover that *when LLMs take a wrong turn in a\nconversation, they get lost and do not recover*.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6240\u6709\u6d4b\u8bd5\u7684LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u5e73\u5747\u4e0b\u964d39%\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65e9\u671f\u5047\u8bbe\u9519\u8bef\u5bfc\u81f4\u65e0\u6cd5\u6062\u590d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u73b0\u6709\u8bc4\u4f30\u591a\u96c6\u4e2d\u5728\u5355\u8f6e\u3001\u660e\u786e\u6307\u4ee4\u573a\u666f\uff0c\u800c\u7528\u6237\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u5e38\u51fa\u73b0\u6307\u4ee4\u4e0d\u5b8c\u6574\u6216\u9700\u8981\u591a\u6b21\u4ea4\u4e92\u7684\u60c5\u51b5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5927\u89c4\u6a21\u6a21\u62df\u5b9e\u9a8c\uff0c\u5bf9\u6bd4LLM\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e8620\u4e07+\u6a21\u62df\u5bf9\u8bdd\uff0c\u5c06\u6027\u80fd\u4e0b\u964d\u5206\u89e3\u4e3a\u80fd\u529b\u8f7b\u5fae\u4e0b\u964d\u548c\u4e0d\u53ef\u9760\u6027\u663e\u8457\u589e\u52a0\u4e24\u90e8\u5206\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709\u6d4b\u8bd5\u7684LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u5e73\u5747\u4e0b\u964d39%\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u6a21\u578b\u5728\u65e9\u671f\u9519\u8bef\u5047\u8bbe\u5e76\u8fc7\u5ea6\u4f9d\u8d56\u8fd9\u4e9b\u5047\u8bbe\uff0c\u5bfc\u81f4\u540e\u7eed\u5bf9\u8bdd\u65e0\u6cd5\u7ea0\u6b63\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0cLLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u663e\u8457\uff0c\u5c24\u5176\u662f\u5728\u65e9\u671f\u9519\u8bef\u51b3\u7b56\u540e\u96be\u4ee5\u6062\u590d\uff0c\u8fd9\u4e3a\u672a\u6765\u4f18\u5316LLM\u7684\u5bf9\u8bdd\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.05491", "pdf": "https://arxiv.org/pdf/2505.05491", "abs": "https://arxiv.org/abs/2505.05491", "authors": ["TianYi Yu"], "title": "MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Detection of small objects, especially traffic signs, is a critical\nsub-task in object detection and autonomous driving. Despite signficant\nprogress in previous research, two main challenges remain. First, the issue of\nfeature extraction being too singular. Second, the detection process struggles\nto efectively handle objects of varying sizes or scales. These problems are\nalso prevalent in general object detection tasks. To address these challenges,\nwe propose a novel object detection network, Mamba-based Dynamic Dual Fusion\nNetwork (MDDFNet), for traffic sign detection. The network integrates a dynamic\ndual fusion module and a Mamba-based backbone to simultaneously tackle the\naforementioned issues. Specifically, the dynamic dual fusion module utilizes\nmultiple branches to consolidate various spatial and semantic information, thus\nenhancing feature diversity. The Mamba-based backbone leverages global feature\nfusion and local feature interaction, combining features in an adaptive manner\nto generate unique classification characteristics. Extensive experiments\nconducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that\nMDDFNet outperforms other state-of-the-art detectors, maintaining real-time\nprocessing capabilities of single-stage models while achieving superior\nperformance. This confirms the efectiveness of MDDFNet in detecting small\ntraffic signs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u68c0\u6d4b\u7f51\u7edcMDDFNet\uff0c\u7528\u4e8e\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u63d0\u53d6\u5355\u4e00\u548c\u5c3a\u5bf8\u53d8\u5316\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728TT100K\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u63d0\u53d6\u5355\u4e00\u548c\u96be\u4ee5\u5904\u7406\u591a\u5c3a\u5ea6\u76ee\u6807\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4e5f\u666e\u904d\u5b58\u5728\u3002", "method": "\u63d0\u51faMDDFNet\u7f51\u7edc\uff0c\u7ed3\u5408\u52a8\u6001\u53cc\u878d\u5408\u6a21\u5757\u548cMamba\u9aa8\u5e72\u7f51\u7edc\uff0c\u52a8\u6001\u53cc\u878d\u5408\u6a21\u5757\u6574\u5408\u591a\u6837\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\uff0cMamba\u9aa8\u5e72\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u81ea\u9002\u5e94\u878d\u5408\u3002", "result": "\u5728TT100K\u6570\u636e\u96c6\u4e0a\uff0cMDDFNet\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u68c0\u6d4b\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "MDDFNet\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2505.05707", "pdf": "https://arxiv.org/pdf/2505.05707", "abs": "https://arxiv.org/abs/2505.05707", "authors": ["Rushabh Solanki", "Meghana Bhange", "Ulrich A\u00efvodji", "Elliot Creager"], "title": "Crowding Out The Noise: Algorithmic Collective Action Under Differential Privacy", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "The integration of AI into daily life has generated considerable attention\nand excitement, while also raising concerns about automating algorithmic harms\nand re-entrenching existing social inequities. While the responsible deployment\nof trustworthy AI systems is a worthy goal, there are many possible ways to\nrealize it, from policy and regulation to improved algorithm design and\nevaluation. In fact, since AI trains on social data, there is even a\npossibility for everyday users, citizens, or workers to directly steer its\nbehavior through Algorithmic Collective Action, by deliberately modifying the\ndata they share with a platform to drive its learning process in their favor.\nThis paper considers how these grassroots efforts to influence AI interact with\nmethods already used by AI firms and governments to improve model\ntrustworthiness. In particular, we focus on the setting where the AI firm\ndeploys a differentially private model, motivated by the growing regulatory\nfocus on privacy and data protection. We investigate how the use of\nDifferentially Private Stochastic Gradient Descent (DPSGD) affects the\ncollective's ability to influence the learning process. Our findings show that\nwhile differential privacy contributes to the protection of individual data, it\nintroduces challenges for effective algorithmic collective action. We\ncharacterize lower bounds on the success of algorithmic collective action under\ndifferential privacy as a function of the collective's size and the firm's\nprivacy parameters, and verify these trends experimentally by simulating\ncollective action during the training of deep neural network classifiers across\nseveral datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u4fe1\u4efb\u90e8\u7f72\u4e2d\uff0c\u96c6\u4f53\u884c\u4e3a\u5982\u4f55\u4e0e\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff08\u5982\u5dee\u5206\u9690\u79c1\uff09\u76f8\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0\u5dee\u5206\u9690\u79c1\u867d\u4fdd\u62a4\u6570\u636e\u4f46\u4e5f\u9650\u5236\u4e86\u96c6\u4f53\u884c\u4e3a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u4fe1\u4efb\u90e8\u7f72\u4e2d\uff0c\u7531\u666e\u901a\u7528\u6237\u901a\u8fc7\u6570\u636e\u5171\u4eab\u5f15\u5bfcAI\u884c\u4e3a\u7684\u96c6\u4f53\u884c\u52a8\u4e0e\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08DPSGD\uff09\u5bf9\u96c6\u4f53\u884c\u52a8\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u4e86\u6570\u636e\uff0c\u4f46\u540c\u65f6\u63d0\u9ad8\u4e86\u96c6\u4f53\u884c\u52a8\u7684\u96be\u5ea6\uff0c\u5176\u6210\u529f\u4e0e\u96c6\u4f53\u89c4\u6a21\u53ca\u9690\u79c1\u53c2\u6570\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u9690\u79c1\u4fdd\u62a4\u4e0e\u96c6\u4f53\u884c\u52a8\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9700\u5728\u63d0\u5347AI\u53ef\u4fe1\u5ea6\u7684\u540c\u65f6\u627e\u5230\u5e73\u8861\u70b9\u3002"}}
{"id": "2505.06145", "pdf": "https://arxiv.org/pdf/2505.06145", "abs": "https://arxiv.org/abs/2505.06145", "authors": ["Xu Han", "Yumeng Sun", "Weiqiang Huang", "Hongye Zheng", "Junliang Du"], "title": "Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Few-shot text classification has important application value in low-resource\nenvironments. This paper proposes a strategy that combines adaptive\nfine-tuning, contrastive learning, and regularization optimization to improve\nthe classification performance of Transformer-based models. Experiments on the\nFewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform\nwell in few-shot tasks, especially in the 5-shot setting, which can more\neffectively capture text features and improve classification accuracy. The\nexperiment also found that there are significant differences in the\nclassification difficulty of different relationship categories. Some categories\nhave fuzzy semantic boundaries or complex feature distributions, making it\ndifficult for the standard cross entropy loss to learn the discriminative\ninformation required to distinguish categories. By introducing contrastive loss\nand regularization loss, the generalization ability of the model is enhanced,\neffectively alleviating the overfitting problem in few-shot environments. In\naddition, the research results show that the use of Transformer models or\ngenerative architectures with stronger self-attention mechanisms can help\nimprove the stability and accuracy of few-shot classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u9002\u5e94\u5fae\u8c03\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u6b63\u5219\u5316\u4f18\u5316\u7684\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u5c0f\u6837\u672c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u5e76\u7f13\u89e3\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u5728\u5c0f\u6837\u672c\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6587\u672c\u7279\u5f81\u5e76\u533a\u5206\u8bed\u4e49\u6a21\u7cca\u7684\u7c7b\u522b\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u591a\u79cd\u6280\u672f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u5fae\u8c03\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u6b63\u5219\u5316\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u4e0d\u540cTransformer\u6a21\u578b\uff08\u5982T5-small\u3001DeBERTa-v3\u7b49\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728FewRel 2.0\u6570\u636e\u96c6\u4e0a\uff0c\u5c24\u5176\u57285-shot\u8bbe\u7f6e\u4e2d\uff0c\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u5b66\u4e60\u6587\u672c\u7279\u5f81\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u635f\u5931\u548c\u6b63\u5219\u5316\u635f\u5931\uff0c\u6a21\u578b\u5728\u5c0f\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a\u4e14\u51c6\u786e\uff0c\u9a8c\u8bc1\u4e86Transformer\u548c\u751f\u6210\u67b6\u6784\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.05492", "pdf": "https://arxiv.org/pdf/2505.05492", "abs": "https://arxiv.org/abs/2505.05492", "authors": ["Ignacy St\u0119pka", "Lukasz Sztukiewicz", "Micha\u0142 Wili\u0144ski", "Jerzy Stefanowski"], "title": "DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While machine learning fairness has made significant progress in recent\nyears, most existing solutions focus on tabular data and are poorly suited for\nvision-based classification tasks, which rely heavily on deep learning. To\nbridge this gap, we introduce DetoxAI, an open-source Python library for\nimproving fairness in deep learning vision classifiers through post-hoc\ndebiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness\nmetrics, and visualization tools. It supports debiasing via interventions in\ninternal representations and includes attribution-based visualization tools and\nquantitative algorithmic fairness metrics to show how bias is mitigated. This\npaper presents the motivation, design, and use cases of DetoxAI, demonstrating\nits tangible value to engineers and researchers.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DetoxAI\uff0c\u4e00\u4e2a\u7528\u4e8e\u6539\u5584\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9\u5206\u7c7b\u5668\u516c\u5e73\u6027\u7684\u5f00\u6e90Python\u5e93\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u516c\u5e73\u6027\u89e3\u51b3\u65b9\u6848\u9488\u5bf9\u8868\u683c\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u3002", "method": "DetoxAI\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53bb\u504f\u7b97\u6cd5\u3001\u516c\u5e73\u6027\u5ea6\u91cf\u548c\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u652f\u6301\u901a\u8fc7\u5185\u90e8\u8868\u5f81\u5e72\u9884\u53bb\u504f\u3002", "result": "\u8be5\u5e93\u63d0\u4f9b\u4e86\u5f52\u56e0\u53ef\u89c6\u5316\u5de5\u5177\u548c\u5b9a\u91cf\u516c\u5e73\u6027\u5ea6\u91cf\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u7f13\u89e3\u504f\u89c1\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86DetoxAI\u7684\u52a8\u673a\u3001\u8bbe\u8ba1\u548c\u4f7f\u7528\u6848\u4f8b\uff0c\u4e3a\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2505.05732", "pdf": "https://arxiv.org/pdf/2505.05732", "abs": "https://arxiv.org/abs/2505.05732", "authors": ["Limai Jiang", "Yunpeng Cai"], "title": "Automated Learning of Semantic Embedding Representations for Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "Extended version of the paper published in SDM25", "summary": "Generative models capture the true distribution of data, yielding\nsemantically rich representations. Denoising diffusion models (DDMs) exhibit\nsuperior generative capabilities, though efficient representation learning for\nthem are lacking. In this work, we employ a multi-level denoising autoencoder\nframework to expand the representation capacity of DDMs, which introduces\nsequentially consistent Diffusion Transformers and an additional\ntimestep-dependent encoder to acquire embedding representations on the\ndenoising Markov chain through self-conditional diffusion learning.\nIntuitively, the encoder, conditioned on the entire diffusion process,\ncompresses high-dimensional data into directional vectors in latent under\ndifferent noise levels, facilitating the learning of image embeddings across\nall timesteps. To verify the semantic adequacy of embeddings generated through\nthis approach, extensive experiments are conducted on various datasets,\ndemonstrating that optimally learned embeddings by DDMs surpass\nstate-of-the-art self-supervised representation learning methods in most cases,\nachieving remarkable discriminative semantic representation quality. Our work\njustifies that DDMs are not only suitable for generative tasks, but also\npotentially advantageous for general-purpose deep learning applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u7ea7\u53bb\u566a\u81ea\u7f16\u7801\u5668\u6846\u67b6\u6269\u5c55\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08DDMs\uff09\u8868\u5f81\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5f15\u5165\u65f6\u5e8f\u4e00\u81f4\u7684\u6269\u6563\u53d8\u6362\u5668\u548c\u989d\u5916\u7684\u65f6\u95f4\u6b65\u7f16\u7801\u5668\uff0c\u9a8c\u8bc1\u4e86DDMs\u5728\u8868\u5f81\u5b66\u4e60\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u5c3d\u7ba1\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08DDMs\uff09\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u672c\u6587\u65e8\u5728\u6269\u5c55DDMs\u7684\u8868\u5f81\u80fd\u529b\uff0c\u4f7f\u5176\u4e0d\u4ec5\u9002\u7528\u4e8e\u751f\u6210\u4efb\u52a1\uff0c\u8fd8\u80fd\u7528\u4e8e\u901a\u7528\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u591a\u7ea7\u53bb\u566a\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5f15\u5165\u65f6\u5e8f\u4e00\u81f4\u7684\u6269\u6563\u53d8\u6362\u5668\u548c\u65f6\u95f4\u6b65\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u81ea\u6761\u4ef6\u6269\u6563\u5b66\u4e60\u5728\u53bb\u566a\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0a\u83b7\u53d6\u5d4c\u5165\u8868\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u65b9\u6cd5\u5b66\u4e60\u7684\u6700\u4f18\u5d4c\u5165\u8868\u5f81\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u5224\u522b\u6027\u8bed\u4e49\u8868\u5f81\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86DDMs\u4e0d\u4ec5\u9002\u5408\u751f\u6210\u4efb\u52a1\uff0c\u8fd8\u53ef\u80fd\u4e3a\u901a\u7528\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u5e26\u6765\u4f18\u52bf\uff0c\u6269\u5c55\u4e86\u5176\u5728\u8868\u5f81\u5b66\u4e60\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.06149", "pdf": "https://arxiv.org/pdf/2505.06149", "abs": "https://arxiv.org/abs/2505.06149", "authors": ["Faeze Ghorbanpour", "Daryna Dementieva", "Alexander Fraser"], "title": "Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study", "categories": ["cs.CL", "cs.CY", "cs.MM"], "comment": null, "summary": "Despite growing interest in automated hate speech detection, most existing\napproaches overlook the linguistic diversity of online content. Multilingual\ninstruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ\noffer promising capabilities across languages, but their effectiveness in\nidentifying hate speech through zero-shot and few-shot prompting remains\nunderexplored. This work evaluates LLM prompting-based detection across eight\nnon-English languages, utilizing several prompting techniques and comparing\nthem to fine-tuned encoder models. We show that while zero-shot and few-shot\nprompting lag behind fine-tuned encoder models on most of the real-world\nevaluation sets, they achieve better generalization on functional tests for\nhate speech detection. Our study also reveals that prompt design plays a\ncritical role, with each language often requiring customized prompting\ntechniques to maximize performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8eLLM\u63d0\u793a\u7684\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u5982\u5fae\u8c03\u6a21\u578b\uff0c\u4f46\u5728\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u63d0\u793a\u8bbe\u8ba1\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\u591a\u5ffd\u89c6\u8bed\u8a00\u591a\u6837\u6027\uff0c\u800c\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff08\u5982LLaMA\u3001Aya\u7b49\uff09\u5728\u6b64\u9886\u57df\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u6280\u672f\u5728\u516b\u79cd\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8bc4\u4f30LLM\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u4e0e\u5fae\u8c03\u7f16\u7801\u5668\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u5728\u591a\u6570\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4e0d\u53ca\u5fae\u8c03\u6a21\u578b\uff0c\u4f46\u5728\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u66f4\u4f18\uff0c\u4e14\u63d0\u793a\u8bbe\u8ba1\u9700\u9488\u5bf9\u4e0d\u540c\u8bed\u8a00\u5b9a\u5236\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u5bf9LLM\u5728\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u793a\u6280\u672f\u3002"}}
{"id": "2505.05494", "pdf": "https://arxiv.org/pdf/2505.05494", "abs": "https://arxiv.org/abs/2505.05494", "authors": ["Avanija Menon", "Ovidiu Serban"], "title": "An Automated LLM-based Pipeline for Asset-Level Database Creation to Assess Deforestation Impact", "categories": ["cs.DB", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted to ACL ClimateNLP 2025", "summary": "The European Union Deforestation Regulation (EUDR) requires companies to\nprove their products do not contribute to deforestation, creating a critical\ndemand for precise, asset-level environmental impact data. Current databases\nlack the necessary detail, relying heavily on broad financial metrics and\nmanual data collection, which limits regulatory compliance and accurate\nenvironmental modeling. This study presents an automated, end-to-end data\nextraction pipeline that uses LLMs to create, clean, and validate structured\ndatabases, specifically targeting sectors with a high risk of deforestation.\nThe pipeline introduces Instructional, Role-Based, Zero-Shot Chain-of-Thought\n(IRZ-CoT) prompting to enhance data extraction accuracy and a\nRetrieval-Augmented Validation (RAV) process that integrates real-time web\nsearches for improved data reliability. Applied to SEC EDGAR filings in the\nMining, Oil & Gas, and Utilities sectors, the pipeline demonstrates significant\nimprovements over traditional zero-shot prompting approaches, particularly in\nextraction accuracy and validation coverage. This work advances NLP-driven\nautomation for regulatory compliance, CSR (Corporate Social Responsibility),\nand ESG, with broad sectoral applicability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\uff0c\u5229\u7528LLMs\u6280\u672f\u521b\u5efa\u3001\u6e05\u6d01\u548c\u9a8c\u8bc1\u7ed3\u6784\u5316\u6570\u636e\u5e93\uff0c\u9488\u5bf9\u9ad8\u6bc1\u6797\u98ce\u9669\u884c\u4e1a\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u63d0\u53d6\u51c6\u786e\u6027\u3002", "motivation": "\u56e0\u6b27\u76df\u300a\u53cd\u6bc1\u6797\u6761\u4f8b\u300b\uff08EUDR\uff09\u8981\u6c42\u4f01\u4e1a\u8bc1\u660e\u5176\u4ea7\u54c1\u672a\u5bfc\u81f4\u6bc1\u6797\uff0c\u800c\u73b0\u6709\u6570\u636e\u5e93\u7f3a\u4e4f\u7ec6\u8282\uff0c\u4f9d\u8d56\u5e7f\u6cdb\u8d22\u52a1\u6307\u6807\u548c\u624b\u52a8\u6536\u96c6\uff0c\u9650\u5236\u4e86\u5408\u89c4\u6027\u548c\u73af\u5883\u5efa\u6a21\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528IRZ-CoT\u63d0\u793a\u63d0\u5347\u63d0\u53d6\u7cbe\u5ea6\uff0c\u7ed3\u5408RAV\u6d41\u7a0b\u5b9e\u65f6\u68c0\u7d22\u7f51\u7edc\u6570\u636e\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u5e94\u7528\u4e8eSEC EDGAR\u6587\u4ef6\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u8be5\u6d41\u7a0b\u5728\u63d0\u53d6\u7cbe\u5ea6\u548c\u9a8c\u8bc1\u8986\u76d6\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86NLP\u9a71\u52a8\u7684\u5408\u89c4\u3001CSR\u53caESG\u81ea\u52a8\u5316\uff0c\u5177\u6709\u5e7f\u6cdb\u884c\u4e1a\u9002\u7528\u6027\u3002"}}
{"id": "2505.05738", "pdf": "https://arxiv.org/pdf/2505.05738", "abs": "https://arxiv.org/abs/2505.05738", "authors": ["Yiming Niu", "Jinliang Deng", "Lulu Zhang", "Zimu Zhou", "Yongxin Tong"], "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate and efficient multivariate time series (MTS) forecasting is\nessential for applications such as traffic management and weather prediction,\nwhich depend on capturing long-range temporal dependencies and interactions\nbetween entities. Existing methods, particularly those based on Transformer\narchitectures, compute pairwise dependencies across all time steps, leading to\na computational complexity that scales quadratically with the length of the\ninput. To overcome these challenges, we introduce the Forecaster with Offline\nClustering Using Segments (FOCUS), a novel approach to MTS forecasting that\nsimplifies long-range dependency modeling through the use of prototypes\nextracted via offline clustering. These prototypes encapsulate high-level\nevents in the real-world system underlying the data, summarizing the key\ncharacteristics of similar time segments. In the online phase, FOCUS\ndynamically adapts these patterns to the current input and captures\ndependencies between the input segment and high-level events, enabling both\naccurate and efficient forecasting. By identifying prototypes during the\noffline clustering phase, FOCUS reduces the computational complexity of\nmodeling long-range dependencies in the online phase to linear scaling.\nExtensive experiments across diverse benchmarks demonstrate that FOCUS achieves\nstate-of-the-art accuracy while significantly reducing computational costs.", "AI": {"tldr": "FOCUS\u662f\u4e00\u79cd\u65b0\u578b\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u805a\u7c7b\u63d0\u53d6\u539f\u578b\u6765\u7b80\u5316\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u5b9e\u4f53\u95f4\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165FOCUS\u65b9\u6cd5\uff0c\u5229\u7528\u79bb\u7ebf\u805a\u7c7b\u83b7\u53d6\u539f\u578b\uff0c\u5728\u7ebf\u9636\u6bb5\u52a8\u6001\u9002\u914d\u8f93\u5165\u7247\u6bb5\u4e0e\u9ad8\u7ea7\u4e8b\u4ef6\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFOCUS\u5728\u51c6\u786e\u6027\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "FOCUS\u901a\u8fc7\u539f\u578b\u805a\u7c7b\u6709\u6548\u7b80\u5316\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\uff0c\u517c\u987e\u9ad8\u6548\u4e0e\u51c6\u786e\u3002"}}
{"id": "2505.06150", "pdf": "https://arxiv.org/pdf/2505.06150", "abs": "https://arxiv.org/abs/2505.06150", "authors": ["Ryan Lagasse", "Aidan Kiernans", "Avijit Ghosh", "Shiri Dori-Hacohen"], "title": "A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a scaling law for fine-tuning large language models (LLMs) under\nfixed compute budgets that explicitly accounts for data composition.\nConventional approaches measure training data solely by total tokens, yet the\nnumber of examples and their average token length -- what we term \\emph{dataset\nvolume} -- play a decisive role in model performance. Our formulation is tuned\nfollowing established procedures. Experiments on the BRICC dataset\n\\cite{salavati2024reducing} and subsets of the MMLU dataset\n\\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple\nsubsampling strategies, reveal that data composition significantly affects\ntoken efficiency. These results motivate refined scaling laws for practical LLM\nfine-tuning in resource-constrained settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u6570\u636e\u7ec4\u6210\u7684\u6269\u5c55\u5b9a\u5f8b\uff0c\u7528\u4e8e\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u53d1\u73b0\u6570\u636e\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u901a\u8fc7\u603b\u6807\u8bb0\u6570\u8861\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u8bba\u6587\u8868\u660e\u6570\u636e\u91cf\u548c\u5e73\u5747\u6807\u8bb0\u957f\u5ea6\uff08\u6570\u636e\u96c6\u4f53\u79ef\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u6269\u5c55\u5b9a\u5f8b\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5728BRICC\u6570\u636e\u96c6\u548cMMLU\u6570\u636e\u96c6\u7684\u5b50\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u91c7\u7528\u591a\u79cd\u5b50\u91c7\u6837\u7b56\u7565\u8bc4\u4f30\u6570\u636e\u7ec4\u6210\u5bf9\u6807\u8bb0\u6548\u7387\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6570\u636e\u7ec4\u6210\u663e\u8457\u5f71\u54cd\u6807\u8bb0\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u6269\u5c55\u5b9a\u5f8b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u6269\u5c55\u5b9a\u5f8b\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u4f53\u79ef\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.05498", "pdf": "https://arxiv.org/pdf/2505.05498", "abs": "https://arxiv.org/abs/2505.05498", "authors": ["Noor ul Misbah Khanum", "Hayssam Dahrouj", "Ramesh C. Bansal", "Hissam Mouayad Tawfik"], "title": "An Overview of the Prospects and Challenges of Using Artificial Intelligence for Energy Management Systems in Microgrids", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "70 pages, 7 figures", "summary": "Microgrids have emerged as a pivotal solution in the quest for a sustainable\nand energy-efficient future. While microgrids offer numerous advantages, they\nare also prone to issues related to reliably forecasting renewable energy\ndemand and production, protecting against cyberattacks, controlling operational\ncosts, optimizing power flow, and regulating the performance of energy\nmanagement systems (EMS). Tackling these energy management challenges is\nessential to facilitate microgrid applications and seamlessly incorporate\nrenewable energy resources. Artificial intelligence (AI) has recently\ndemonstrated immense potential for optimizing energy management in microgrids,\nproviding efficient and reliable solutions. This paper highlights the combined\nbenefits of enabling AI-based methodologies in the energy management systems of\nmicrogrids by examining the applicability and efficiency of AI-based EMS in\nachieving specific technical and economic objectives. The paper also points out\nseveral future research directions that promise to spearhead AI-driven EMS,\nnamely the development of self-healing microgrids, integration with blockchain\ntechnology, use of Internet of things (IoT), and addressing interpretability,\ndata privacy, scalability, and the prospects to generative AI in the context of\nfuture AI-based EMS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u5fae\u7535\u7f51\u80fd\u91cf\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86AI\u5728\u89e3\u51b3\u53ef\u518d\u751f\u80fd\u6e90\u9884\u6d4b\u3001\u7f51\u7edc\u5b89\u5168\u3001\u6210\u672c\u63a7\u5236\u7b49\u6311\u6218\u65f6\u7684\u6548\u7387\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5fae\u7535\u7f51\u5728\u53ef\u6301\u7eed\u53d1\u5c55\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9762\u4e34\u591a\u79cd\u80fd\u91cf\u7ba1\u7406\u6311\u6218\uff08\u5982\u9884\u6d4b\u53ef\u9760\u6027\u3001\u7f51\u7edc\u5b89\u5168\u7b49\uff09\uff0cAI\u88ab\u89c6\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u6709\u6548\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u7814\u7a76AI\u5728\u5fae\u7535\u7f51\u80fd\u91cf\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u548c\u6548\u7387\uff0c\u5206\u6790AI\u5982\u4f55\u5b9e\u73b0\u6280\u672f\u4e0e\u7ecf\u6d4e\u76ee\u6807\u3002", "result": "AI\u5c55\u793a\u4e86\u4f18\u5316\u5fae\u7535\u7f51\u80fd\u91cf\u7ba1\u7406\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u63d0\u5347\u6548\u7387\u548c\u53ef\u9760\u6027\u65b9\u9762\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u81ea\u6108\u5fae\u7535\u7f51\u3001\u533a\u5757\u94fe\u6280\u672f\u96c6\u6210\u3001IoT\u5e94\u7528\u7b49\uff0c\u9700\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u3001\u6570\u636e\u9690\u79c1\u548c\u751f\u6210\u5f0fAI\u7b49\u95ee\u9898\u3002"}}
{"id": "2505.05740", "pdf": "https://arxiv.org/pdf/2505.05740", "abs": "https://arxiv.org/abs/2505.05740", "authors": ["Xi He", "Yi Miao", "Max A. Little"], "title": "Deep-ICE: The first globally optimal algorithm for empirical risk minimization of two-layer maxout and ReLU networks", "categories": ["cs.LG"], "comment": null, "summary": "This paper introduces the first globally optimal algorithm for the empirical\nrisk minimization problem of two-layer maxout and ReLU networks, i.e.,\nminimizing the number of misclassifications. The algorithm has a worst-case\ntime complexity of $O\\left(N^{DK+1}\\right)$, where $K$ denotes the number of\nhidden neurons and $D$ represents the number of features. It can be can be\ngeneralized to accommodate arbitrary computable loss functions without\naffecting its computational complexity. Our experiments demonstrate that the\nproposed algorithm provides provably exact solutions for small-scale datasets.\nTo handle larger datasets, we introduce a novel coreset selection method that\nreduces the data size to a manageable scale, making it feasible for our\nalgorithm. This extension enables efficient processing of large-scale datasets\nand achieves significantly improved performance, with a 20-30\\% reduction in\nmisclassifications for both training and prediction, compared to\nstate-of-the-art approaches (neural networks trained using gradient descent and\nsupport vector machines), when applied to the same models (two-layer networks\nwith fixed hidden nodes and linear models).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e24\u5c42maxout\u548cReLU\u7f51\u7edc\u7684\u5168\u5c40\u6700\u4f18\u7b97\u6cd5\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u8bef\u5206\u7c7b\u6570\uff0c\u5e76\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u7cbe\u786e\u89e3\uff0c\u540c\u65f6\u901a\u8fc7\u6838\u5fc3\u96c6\u9009\u62e9\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6027\u80fd\u63d0\u534720-30%\u3002", "motivation": "\u73b0\u6709\u7684\u68af\u5ea6\u4e0b\u964d\u548c\u652f\u6301\u5411\u91cf\u673a\u65b9\u6cd5\u5728\u4e24\u5c42maxout\u548cReLU\u7f51\u7edc\u7684\u8bad\u7ec3\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u65e0\u6cd5\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u89e3\u3002\u672c\u6587\u9996\u6b21\u63d0\u51fa\u5168\u5c40\u6700\u4f18\u7b97\u6cd5\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7b97\u6cd5\u57fa\u4e8e\u5168\u5c40\u6700\u4f18\u7684ERM\u6846\u67b6\u8bbe\u8ba1\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$O(N^{DK+1})$\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u786e\u6027\uff0c\u6838\u5fc3\u96c6\u6269\u5c55\u540e\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u8bef\u5206\u7c7b\u51cf\u5c1120-30%\uff09\uff0c\u8d85\u8d8a\u68af\u5ea6\u4e0b\u964d\u548cSVM\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u662f\u9996\u4e2a\u9488\u5bf9\u4e24\u5c42maxout/ReLU\u7f51\u7edc\u7684\u5168\u5c40\u6700\u4f18\u89e3\u65b9\u6848\uff0c\u6838\u5fc3\u96c6\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.06151", "pdf": "https://arxiv.org/pdf/2505.06151", "abs": "https://arxiv.org/abs/2505.06151", "authors": ["Alice Rueda", "Argyrios Perivolaris", "Niloy Roy", "Dylan Weston", "Sarmed Shaya", "Zachary Cote", "Martin Ivanov", "Bazen G. Teferra", "Yuqi Wu", "Sirisha Rambhatla", "Divya Sharma", "Andrew Greenshaw", "Rakesh Jetly", "Yanbo Zhang", "Bo Cao", "Reza Samavi", "Sridhar Krishnan", "Venkat Bhat"], "title": "Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework", "categories": ["cs.CL"], "comment": "12 pages, 4 figures, 7 tables", "summary": "Engagement between client and therapist is a critical determinant of\ntherapeutic success. We propose a multi-dimensional natural language processing\n(NLP) framework that objectively classifies engagement quality in counseling\nsessions based on textual transcripts. Using 253 motivational interviewing\ntranscripts (150 high-quality, 103 low-quality), we extracted 42 features\nacross four domains: conversational dynamics, semantic similarity as topic\nalignment, sentiment classification, and question detection. Classifiers,\nincluding Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM),\nwere hyperparameter tuned and trained using a stratified 5-fold\ncross-validation and evaluated on a holdout test set. On balanced\n(non-augmented) data, RF achieved the highest classification accuracy (76.7%),\nand SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation,\nperformance improved significantly: RF achieved up to 88.9% accuracy, 90.0%\nF1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and\n93.6% AUC. The augmented data results reflect the potential of the framework in\nfuture larger-scale applications. Feature contribution revealed conversational\ndynamics and semantic similarity between clients and therapists were among the\ntop contributors, led by words uttered by the client (mean and standard\ndeviation). The framework was robust across the original and augmented datasets\nand demonstrated consistent improvements in F1 scores and recall. While\ncurrently text-based, the framework supports future multimodal extensions\n(e.g., vocal tone, facial affect) for more holistic assessments. This work\nintroduces a scalable, data-driven method for evaluating engagement quality of\nthe therapy session, offering clinicians real-time feedback to enhance the\nquality of both virtual and in-person therapeutic interactions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u591a\u7ef4\u6846\u67b6\uff0c\u7528\u4e8e\u5ba2\u89c2\u5206\u7c7b\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u4e2d\u7684\u53c2\u4e0e\u8d28\u91cf\uff0c\u5e76\u5728\u589e\u5f3a\u6570\u636e\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u548c\u6539\u5584\u5fc3\u7406\u6cbb\u7597\u4e2d\u5ba2\u6237\u4e0e\u6cbb\u7597\u5e08\u7684\u4e92\u52a8\u8d28\u91cf\uff0c\u8fd9\u5bf9\u6cbb\u7597\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4ece253\u4e2a\u6cbb\u7597\u5bf9\u8bdd\u6587\u672c\u4e2d\u63d0\u53d642\u4e2a\u7279\u5f81\uff0c\u4f7f\u7528Random Forest\u3001Cat-Boost\u548cSVM\u7b49\u5206\u7c7b\u5668\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cRandom Forest\u5728\u589e\u5f3a\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u8fbe88.9%\uff0cAUC\u8fbe94.6%\uff0c\u4f1a\u8bdd\u52a8\u6001\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u662f\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9a71\u52a8\uff0c\u80fd\u5b9e\u65f6\u53cd\u9988\u6cbb\u7597\u8d28\u91cf\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u591a\u6a21\u6001\u8bc4\u4f30\u3002"}}
{"id": "2505.05501", "pdf": "https://arxiv.org/pdf/2505.05501", "abs": "https://arxiv.org/abs/2505.05501", "authors": ["Pu Cao", "Feng Zhou", "Junyi Ji", "Qingye Kong", "Zhixiang Lv", "Mingjian Zhang", "Xuekun Zhao", "Siqi Wu", "Yinghui Lin", "Qing Song", "Lu Yang"], "title": "Preliminary Explorations with GPT-4o(mni) Native Image Generation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Recently, the visual generation ability by GPT-4o(mni) has been unlocked by\nOpenAI. It demonstrates a very remarkable generation capability with excellent\nmultimodal condition understanding and varied task instructions. In this paper,\nwe aim to explore the capabilities of GPT-4o across various tasks. Inspired by\nprevious study, we constructed a task taxonomy along with a carefully curated\nset of test samples to conduct a comprehensive qualitative test. Benefiting\nfrom GPT-4o's powerful multimodal comprehension, its image-generation process\ndemonstrates abilities surpassing those of traditional image-generation tasks.\nThus, regarding the dimensions of model capabilities, we evaluate its\nperformance across six task categories: traditional image generation tasks,\ndiscriminative tasks, knowledge-based generation, commonsense-based generation,\nspatially-aware image generation, and temporally-aware image generation. These\ntasks not only assess the quality and conditional alignment of the model's\noutputs but also probe deeper into GPT-4o's understanding of real-world\nconcepts. Our results reveal that GPT-4o performs impressively well in\ngeneral-purpose synthesis tasks, showing strong capabilities in text-to-image\ngeneration, visual stylization, and low-level image processing. However,\nsignificant limitations remain in its ability to perform precise spatial\nreasoning, instruction-grounded generation, and consistent temporal prediction.\nFurthermore, when faced with knowledge-intensive or domain-specific scenarios,\nsuch as scientific illustrations or mathematical plots, the model often\nexhibits hallucinations, factual errors, or structural inconsistencies. These\nfindings suggest that while GPT-4o marks a substantial advancement in unified\nmultimodal generation, there is still a long way to go before it can be\nreliably applied to professional or safety-critical domains.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86GPT-4o\u5728\u591a\u79cd\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u901a\u7528\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u7b49\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u63a2\u7d22GPT-4o\u5728\u89c6\u89c9\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4efb\u52a1\u5206\u7c7b\u6cd5\u5e76\u8bbe\u8ba1\u6d4b\u8bd5\u6837\u672c\uff0c\u5bf9GPT-4o\u5728\u516d\u7c7b\u4efb\u52a1\uff08\u4f20\u7edf\u56fe\u50cf\u751f\u6210\u3001\u5224\u522b\u4efb\u52a1\u3001\u77e5\u8bc6/\u5e38\u8bc6\u751f\u6210\u3001\u7a7a\u95f4/\u65f6\u95f4\u611f\u77e5\u751f\u6210\uff09\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "GPT-4o\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u89c6\u89c9\u98ce\u683c\u5316\u7b49\u901a\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u79d1\u5b66\u63d2\u56fe\uff09\u4e2d\u5b58\u5728\u5e7b\u89c9\u6216\u9519\u8bef\u3002", "conclusion": "GPT-4o\u5728\u591a\u6a21\u6001\u751f\u6210\u9886\u57df\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u4e13\u4e1a\u6216\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u5e94\u7528\u6c34\u5e73\u3002"}}
{"id": "2505.05744", "pdf": "https://arxiv.org/pdf/2505.05744", "abs": "https://arxiv.org/abs/2505.05744", "authors": ["Ruxue Shi", "Hengrui Gu", "Xu Shen", "Xin Wang"], "title": "Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable ability in solving complex\ntasks, making them a promising tool for enhancing tabular learning. However,\nexisting LLM-based methods suffer from high resource requirements, suboptimal\ndemonstration selection, and limited interpretability, which largely hinder\ntheir prediction performance and application in the real world. To overcome\nthese problems, we propose a novel in-context learning framework for tabular\nprediction. The core idea is to leverage the explanations generated by LLMs to\nguide a smaller, locally deployable Surrogate Language Model (SLM) to make\ninterpretable tabular predictions. Specifically, our framework mainly involves\nthree stages: (i) Post Hoc Explanation Generation, where LLMs are utilized to\ngenerate explanations for question-answer pairs in candidate demonstrations,\nproviding insights into the reasoning behind the answer. (ii) Post Hoc\nExplanation-Guided Demonstrations Selection, which utilizes explanations\ngenerated by LLMs to guide the process of demonstration selection from\ncandidate demonstrations. (iii) Post Hoc Explanation-Guided Interpretable SLM\nPrediction, which utilizes the demonstrations obtained in step (ii) as\nin-context and merges corresponding explanations as rationales to improve the\nperformance of SLM and guide the model to generate interpretable outputs.\nExperimental results highlight the framework's effectiveness, with an average\naccuracy improvement of 5.31% across various tabular datasets in diverse\ndomains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u8868\u683c\u9884\u6d4b\u4e2d\u8d44\u6e90\u9700\u6c42\u9ad8\u3001\u6f14\u793a\u9009\u62e9\u4e0d\u8db3\u53ca\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u89e3\u91ca\u6307\u5bfc\u66f4\u5c0f\u7684SLM\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLMs\u7684\u65b9\u6cd5\u5728\u8868\u683c\u5b66\u4e60\u4e2d\u5b58\u5728\u8d44\u6e90\u6d88\u8017\u5927\u3001\u6f14\u793a\u9009\u62e9\u4e0d\u4f73\u53ca\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7LLMs\u751f\u6210\u89e3\u91ca\u6765\u6307\u5bfc\u66f4\u5c0f\u7684SLM\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u6846\u67b6\u5206\u4e3a\u4e09\u6b65\uff1a(i) LLMs\u751f\u6210\u5019\u9009\u6f14\u793a\u7684\u89e3\u91ca\uff1b(ii) \u5229\u7528\u89e3\u91ca\u6307\u5bfc\u6f14\u793a\u9009\u62e9\uff1b(iii) \u7ed3\u5408\u89e3\u91ca\u548c\u6f14\u793a\u6307\u5bfcSLM\u8fdb\u884c\u53ef\u89e3\u91ca\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8868\u683c\u6570\u636e\u96c6\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e865.31%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7LLMs\u7684\u89e3\u91ca\u6307\u5bfcSLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u9884\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06186", "pdf": "https://arxiv.org/pdf/2505.06186", "abs": "https://arxiv.org/abs/2505.06186", "authors": ["Massimiliano Pronesti", "Joao Bettencourt-Silva", "Paul Flanagan", "Alessandra Pascale", "Oisin Redmond", "Anya Belz", "Yufang Hou"], "title": "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extracting scientific evidence from biomedical studies for clinical research\nquestions (e.g., Does stem cell transplantation improve quality of life in\npatients with medically refractory Crohn's disease compared to placebo?) is a\ncrucial step in synthesising biomedical evidence. In this paper, we focus on\nthe task of document-level scientific evidence extraction for clinical\nquestions with conflicting evidence. To support this task, we create a dataset\ncalled CochraneForest, leveraging forest plots from Cochrane systematic\nreviews. It comprises 202 annotated forest plots, associated clinical research\nquestions, full texts of studies, and study-specific conclusions. Building on\nCochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a\nretrieval-augmented generation framework designed to tackle the unique\nchallenges of evidence extraction. Our experiments show that URCA outperforms\nthe best existing methods by up to 10.3% in F1 score on this task. However, the\nresults also underscore the complexity of CochraneForest, establishing it as a\nchallenging testbed for advancing automated evidence synthesis systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aURCA\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u63d0\u53d6\u79d1\u5b66\u8bc1\u636e\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCochraneForest\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u8fd9\u4e00\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eURCA\u5728F1\u5f97\u5206\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa10.3%\uff0c\u4f46\u4e5f\u51f8\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u4ece\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u63d0\u53d6\u79d1\u5b66\u8bc1\u636e\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u8bc1\u636e\u51b2\u7a81\u7684\u4e34\u5e8a\u95ee\u9898\u4e2d\uff0c\u8fd9\u5bf9\u7efc\u5408\u751f\u7269\u533b\u5b66\u8bc1\u636e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u521b\u5efa\u4e86CochraneForest\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86URCA\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u8bc1\u636e\u63d0\u53d6\u4efb\u52a1\u7684\u72ec\u7279\u6311\u6218\u3002", "result": "URCA\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cF1\u5f97\u5206\u63d0\u9ad8\u4e8610.3%\uff0c\u4f46\u4efb\u52a1\u590d\u6742\u5ea6\u8f83\u9ad8\uff0c\u8868\u660e\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "URCA\u4e3a\u81ea\u52a8\u5316\u8bc1\u636e\u5408\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46CochraneForest\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u4e3a\u672a\u6765\u7814\u7a76\u8bbe\u7acb\u4e86\u9ad8\u6807\u51c6\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2505.05516", "pdf": "https://arxiv.org/pdf/2505.05516", "abs": "https://arxiv.org/abs/2505.05516", "authors": ["Yue Wu", "Yibo Guo", "Yulong Yan", "Jiancheng Yang", "Xin Zhou", "Ching-Yu Cheng", "Danli Shi", "Mingguang He"], "title": "AI-powered virtual eye: perspective, challenges and opportunities", "categories": ["q-bio.TO", "cs.AI", "cs.HC"], "comment": "30 Pages, 3 figures, 1 table", "summary": "We envision the \"virtual eye\" as a next-generation, AI-powered platform that\nuses interconnected foundation models to simulate the eye's intricate structure\nand biological function across all scales. Advances in AI, imaging, and\nmultiomics provide a fertile ground for constructing a universal, high-fidelity\ndigital replica of the human eye. This perspective traces the evolution from\nearly mechanistic and rule-based models to contemporary AI-driven approaches,\nintegrating in a unified model with multimodal, multiscale, dynamic predictive\ncapabilities and embedded feedback mechanisms. We propose a development roadmap\nemphasizing the roles of large-scale multimodal datasets, generative AI,\nfoundation models, agent-based architectures, and interactive interfaces.\nDespite challenges in interpretability, ethics, data processing and evaluation,\nthe virtual eye holds the potential to revolutionize personalized ophthalmic\ncare and accelerate research into ocular health and disease.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u2018\u865a\u62df\u773c\u2019\u7684\u4e0b\u4e00\u4ee3AI\u9a71\u52a8\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u4e92\u8054\u7684\u57fa\u7840\u6a21\u578b\u6a21\u62df\u773c\u775b\u7684\u590d\u6742\u7ed3\u6784\u548c\u751f\u7269\u529f\u80fd\uff0c\u5229\u7528\u591a\u7ec4\u5b66\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u63a8\u52a8\u4e2a\u6027\u5316\u773c\u79d1\u62a4\u7406\u548c\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7684AI\u3001\u6210\u50cf\u548c\u591a\u7ec4\u5b66\u6280\u672f\u8fdb\u6b65\u4e3a\u6784\u5efa\u9ad8\u4fdd\u771f\u7684\u4eba\u7c7b\u773c\u775b\u6570\u5b57\u590d\u5236\u54c1\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u76ee\u6807\u662f\u9769\u547d\u5316\u4e2a\u6027\u5316\u773c\u79d1\u62a4\u7406\u5e76\u52a0\u901f\u773c\u90e8\u5065\u5eb7\u4e0e\u75be\u75c5\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u591a\u6a21\u6001\u3001\u591a\u5c3a\u5ea6\u3001\u52a8\u6001\u9884\u6d4b\u80fd\u529b\u548c\u53cd\u9988\u673a\u5236\u7684AI\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f9d\u8d56\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u3001\u751f\u6210\u5f0fAI\u3001\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\u3002", "result": "\u865a\u62df\u773c\u5e73\u53f0\u867d\u9762\u4e34\u89e3\u91ca\u6027\u3001\u4f26\u7406\u3001\u6570\u636e\u5904\u7406\u548c\u8bc4\u4f30\u7b49\u6311\u6218\uff0c\u4f46\u6709\u671b\u6210\u4e3a\u773c\u79d1\u7814\u7a76\u548c\u4e2a\u6027\u5316\u62a4\u7406\u7684\u9769\u547d\u6027\u5de5\u5177\u3002", "conclusion": "\u865a\u62df\u773c\u5c55\u73b0\u4e86\u901a\u8fc7AI\u6280\u672f\u6a21\u62df\u548c\u4f18\u5316\u773c\u79d1\u62a4\u7406\u7684\u5de8\u5927\u6f5c\u529b\u548c\u524d\u666f\uff0c\u5c3d\u7ba1\u4ecd\u6709\u4e00\u4e9b\u6280\u672f\u548c\u4f26\u7406\u6311\u6218\u9700\u514b\u670d\u3002"}}
{"id": "2505.05763", "pdf": "https://arxiv.org/pdf/2505.05763", "abs": "https://arxiv.org/abs/2505.05763", "authors": ["Yize Zhou", "Jie Zhang", "Meijie Wang", "Lun Yu"], "title": "BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Academic misconduct detection in biomedical research remains challenging due\nto algorithmic narrowness in existing methods and fragmented analytical\npipelines. We present BMMDetect, a multimodal deep learning framework that\nintegrates journal metadata (SJR, institutional data), semantic embeddings\n(PubMedBERT), and GPT-4o-mined textual attributes (methodological statistics,\ndata anomalies) for holistic manuscript evaluation. Key innovations include:\n(1) multimodal fusion of domain-specific features to reduce detection bias; (2)\nquantitative evaluation of feature importance, identifying journal authority\nmetrics (e.g., SJR-index) and textual anomalies (e.g., statistical outliers) as\ndominant predictors; and (3) the BioMCD dataset, a large-scale benchmark with\n13,160 retracted articles and 53,411 controls. BMMDetect achieves 74.33% AUC,\noutperforming single-modality baselines by 8.6%, and demonstrates\ntransferability across biomedical subfields. This work advances scalable,\ninterpretable tools for safeguarding research integrity.", "AI": {"tldr": "BMMDetect\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u671f\u520a\u5143\u6570\u636e\u3001\u8bed\u4e49\u5d4c\u5165\u548cGPT-4o\u6316\u6398\u7684\u6587\u672c\u5c5e\u6027\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u5b66\u672f\u4e0d\u7aef\u884c\u4e3a\uff0c\u6027\u80fd\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf8.6%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b66\u672f\u4e0d\u7aef\u68c0\u6d4b\u4e2d\u5b58\u5728\u7b97\u6cd5\u5c40\u9650\u6027\u548c\u5206\u6790\u6d41\u7a0b\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u671f\u520a\u5143\u6570\u636e\uff08SJR\u3001\u673a\u6784\u6570\u636e\uff09\u3001\u8bed\u4e49\u5d4c\u5165\uff08PubMedBERT\uff09\u548cGPT-4o\u6316\u6398\u7684\u6587\u672c\u5c5e\u6027\uff08\u65b9\u6cd5\u7edf\u8ba1\u3001\u6570\u636e\u5f02\u5e38\uff09\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6BMMDetect\u3002", "result": "AUC\u8fbe\u523074.33%\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf8.6%\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u751f\u7269\u533b\u5b66\u5b50\u9886\u57df\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "BMMDetect\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u7814\u7a76\u8bda\u4fe1\u4fdd\u62a4\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.05528", "pdf": "https://arxiv.org/pdf/2505.05528", "abs": "https://arxiv.org/abs/2505.05528", "authors": ["Hanxun Huang", "Sarah Erfani", "Yige Li", "Xingjun Ma", "James Bailey"], "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly\nadopted for diverse downstream tasks and integrated into large vision-language\nmodels (VLMs), their susceptibility to adversarial perturbations has emerged as\na critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel\nattack method that exposes a universal adversarial vulnerability in CLIP.\nX-Transfer generates a Universal Adversarial Perturbation (UAP) capable of\ndeceiving various CLIP encoders and downstream VLMs across different samples,\ntasks, and domains. We refer to this property as \\textbf{super\ntransferability}--a single perturbation achieving cross-data, cross-domain,\ncross-model, and cross-task adversarial transferability simultaneously. This is\nachieved through \\textbf{surrogate scaling}, a key innovation of our approach.\nUnlike existing methods that rely on fixed surrogate models, which are\ncomputationally intensive to scale, X-Transfer employs an efficient surrogate\nscaling strategy that dynamically selects a small subset of suitable surrogates\nfrom a large search space. Extensive evaluations demonstrate that X-Transfer\nsignificantly outperforms previous state-of-the-art UAP methods, establishing a\nnew benchmark for adversarial transferability across CLIP models. The code is\npublicly available in our\n\\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.", "AI": {"tldr": "X-Transfer\u662f\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff08UAP\uff09\uff0c\u63ed\u793a\u4e86CLIP\u6a21\u578b\u5728\u4e0d\u540c\u6837\u672c\u3001\u4efb\u52a1\u548c\u9886\u57df\u7684\u901a\u7528\u5bf9\u6297\u8106\u5f31\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709UAP\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9CLIP\u6a21\u578b\u53ca\u5176\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5bf9\u6297\u6270\u52a8\u7684\u654f\u611f\u6027\u65e5\u76ca\u7a81\u51fa\uff0c\u7814\u7a76\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u5bf9\u6297\u8106\u5f31\u6027\u4ee5\u63d0\u5347\u6a21\u578b\u5b89\u5168\u3002", "method": "\u91c7\u7528\u52a8\u6001\u9009\u62e9\u5408\u9002\u4ee3\u7406\u6a21\u578b\u7684\u4ee3\u7406\u7f29\u653e\u7b56\u7565\uff08surrogate scaling\uff09\uff0c\u751f\u6210\u5177\u6709\u8d85\u7ea7\u53ef\u8fc1\u79fb\u6027\u7684UAP\uff0c\u5b9e\u73b0\u8de8\u6570\u636e\u3001\u9886\u57df\u3001\u6a21\u578b\u548c\u4efb\u52a1\u7684\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cX-Transfer\u663e\u8457\u4f18\u4e8e\u73b0\u6709UAP\u65b9\u6cd5\uff0c\u786e\u7acb\u4e86CLIP\u6a21\u578b\u5bf9\u6297\u53ef\u8fc1\u79fb\u6027\u7684\u65b0\u57fa\u51c6\u3002", "conclusion": "X-Transfer\u5c55\u793a\u4e86CLIP\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u901a\u7528\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5b89\u5168\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.05520", "pdf": "https://arxiv.org/pdf/2505.05520", "abs": "https://arxiv.org/abs/2505.05520", "authors": ["Chengwei Ye", "Huanzhen Zhang", "Yufei Lin", "Kangsheng Wang", "Linuo Xu", "Shuyan Liu"], "title": "GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gliomas are aggressive brain tumors that pose serious health risks. Deep\nlearning aids in lesion segmentation, but CNN and Transformer-based models\noften lack context modeling or demand heavy computation, limiting real-time use\non mobile medical devices. We propose GaMNet, integrating the NMamba module for\nglobal modeling and a multi-scale CNN for efficient local feature extraction.\nTo improve interpretability and mimic the human visual system, we apply Gabor\nfilters at multiple scales. Our method achieves high segmentation accuracy with\nfewer parameters and faster computation. Extensive experiments show GaMNet\noutperforms existing methods, notably reducing false positives and negatives,\nwhich enhances the reliability of clinical diagnosis.", "AI": {"tldr": "GaMNet\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NMamba\u6a21\u5757\u548c\u591a\u5c3a\u5ea6CNN\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5206\u5272\u8111\u80f6\u8d28\u7624\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eCNN\u548cTransformer\u7684\u6a21\u578b\u5728\u8111\u7624\u5206\u5272\u4e2d\u5e38\u56e0\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u4e0d\u8db3\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5728\u79fb\u52a8\u533b\u7597\u8bbe\u5907\u4e0a\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u96c6\u6210\u4e86NMamba\u6a21\u5757\u8fdb\u884c\u5168\u5c40\u5efa\u6a21\uff0c\u591a\u5c3a\u5ea6CNN\u7528\u4e8e\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u4f7f\u7528\u591a\u5c3a\u5ea6Gabor\u6ee4\u6ce2\u5668\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGaMNet\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5047\u9633\u6027\u548c\u5047\u9634\u6027\uff0c\u63d0\u5347\u4e34\u5e8a\u8bca\u65ad\u53ef\u9760\u6027\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u66f4\u5c11\u3001\u8ba1\u7b97\u66f4\u5feb\u3002", "conclusion": "GaMNet\u4e3a\u8111\u80f6\u8d28\u7624\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2505.05785", "pdf": "https://arxiv.org/pdf/2505.05785", "abs": "https://arxiv.org/abs/2505.05785", "authors": ["Henan Sun", "Xunkai Li", "Lei Zhu", "Junyi Han", "Guang Zeng", "Ronghua Li", "Guoren Wang"], "title": "Rethinking Graph Out-Of-Distribution Generalization: A Learnable Random Walk Perspective", "categories": ["cs.LG"], "comment": "Under Review", "summary": "Out-Of-Distribution (OOD) generalization has gained increasing attentions for\nmachine learning on graphs, as graph neural networks (GNNs) often exhibit\nperformance degradation under distribution shifts. Existing graph OOD methods\ntend to follow the basic ideas of invariant risk minimization and structural\ncausal models, interpreting the invariant knowledge across datasets under\nvarious distribution shifts as graph topology or graph spectrum. However, these\ninterpretations may be inconsistent with real-world scenarios, as neither\ninvariant topology nor spectrum is assured. In this paper, we advocate the\nlearnable random walk (LRW) perspective as the instantiation of invariant\nknowledge, and propose LRW-OOD to realize graph OOD generalization learning.\nInstead of employing fixed probability transition matrix (i.e.,\ndegree-normalized adjacency matrix), we parameterize the transition matrix with\nan LRW-sampler and a path encoder. Furthermore, we propose the kernel density\nestimation (KDE)-based mutual information (MI) loss to generate random walk\nsequences that adhere to OOD principles. Extensive experiment demonstrates that\nour model can effectively enhance graph OOD generalization under various types\nof distribution shifts and yield a significant accuracy improvement of 3.87%\nover state-of-the-art graph OOD generalization baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u968f\u673a\u6e38\u8d70\uff08LRW\uff09\u89c6\u89d2\u7684\u65b9\u6cd5LRW-OOD\uff0c\u7528\u4e8e\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6cdb\u5316\u4e2d\u7684\u6027\u80fd\u3002\u901a\u8fc7\u53c2\u6570\u5316\u8f6c\u79fb\u77e9\u9635\u548c\u4f7f\u7528KDE-based MI\u635f\u5931\u51fd\u6570\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u56feOOD\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u62d3\u6251\u6216\u56fe\u8c31\u4f5c\u4e3a\u4e0d\u53d8\u91cf\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u53ef\u80fd\u4e0e\u771f\u5b9e\u573a\u666f\u4e0d\u7b26\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51faLRW\u4f5c\u4e3a\u4e0d\u53d8\u91cf\u77e5\u8bc6\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4ee5\u66f4\u7075\u6d3b\u5730\u9002\u5e94\u5206\u5e03\u53d8\u5316\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u53c2\u6570\u5316\u8f6c\u79fb\u77e9\u9635\u7684LRW\u91c7\u6837\u5668\u548c\u8def\u5f84\u7f16\u7801\u5668\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eKDE\u7684\u4e92\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u751f\u6210\u7b26\u5408OOD\u539f\u5219\u7684\u968f\u673a\u6e38\u8d70\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLRW-OOD\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\uff0c\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e863.87%\u3002", "conclusion": "LRW\u89c6\u89d2\u4e3a\u56feOOD\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0cLRW-OOD\u901a\u8fc7\u7075\u6d3b\u5efa\u6a21\u8f6c\u79fb\u77e9\u9635\u548c\u4f18\u5316\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2505.05665", "pdf": "https://arxiv.org/pdf/2505.05665", "abs": "https://arxiv.org/abs/2505.05665", "authors": ["Neeloy Chakraborty", "John Pohovey", "Melkior Ornik", "Katherine Driggs-Campbell"], "title": "Adaptive Stress Testing Black-Box LLM Planners", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "26 pages, 16 figures, 4 tables", "summary": "Large language models (LLMs) have recently demonstrated success in\ngeneralizing across decision-making tasks including planning, control and\nprediction, but their tendency to hallucinate unsafe and undesired outputs\nposes risks. We argue that detecting such failures is necessary, especially in\nsafety-critical scenarios. Existing black-box methods often detect\nhallucinations by identifying inconsistencies across multiple samples. Many of\nthese approaches typically introduce prompt perturbations like randomizing\ndetail order or generating adversarial inputs, with the intuition that a\nconfident model should produce stable outputs. We first perform a manual case\nstudy showing that other forms of perturbations (e.g., adding noise, removing\nsensor details) cause LLMs to hallucinate in a driving environment. We then\npropose a novel method for efficiently searching the space of prompt\nperturbations using Adaptive Stress Testing (AST) with Monte-Carlo Tree Search\n(MCTS). Our AST formulation enables discovery of scenarios and prompts that\ncause language models to act with high uncertainty. By generating MCTS prompt\nperturbation trees across diverse scenarios, we show that offline analyses can\nbe used at runtime to automatically generate prompts that influence model\nuncertainty, and to inform real-time trust assessments of an LLM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u81ea\u9002\u5e94\u538b\u529b\u6d4b\u8bd5\uff08AST\uff09\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u63d0\u793a\u6270\u52a8\u6765\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e0d\u5b89\u5168\u6216\u5e7b\u89c9\u8f93\u51fa\uff0c\u4ece\u800c\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u6216\u5e7b\u89c9\u8f93\u51fa\u7684\u98ce\u9669\u9700\u8981\u88ab\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AST\u548cMCTS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u641c\u7d22\u63d0\u793a\u6270\u52a8\u7a7a\u95f4\u6765\u53d1\u73b0\u5bfc\u81f4\u6a21\u578b\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u573a\u666f\u548c\u63d0\u793a\u3002", "result": "\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684MCTS\u63d0\u793a\u6270\u52a8\u6811\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210\u5f71\u54cd\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u63d0\u793a\uff0c\u5e76\u4e3a\u5b9e\u65f6\u4fe1\u4efb\u8bc4\u4f30\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u63d0\u5347\u4e86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5bf9LLM\u8f93\u51fa\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2505.05798", "pdf": "https://arxiv.org/pdf/2505.05798", "abs": "https://arxiv.org/abs/2505.05798", "authors": ["Youngjoon Lee", "Jinu Gong", "Joonhyuk Kang"], "title": "Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes", "categories": ["cs.LG", "cs.CV", "eess.IV", "eess.SP"], "comment": "4 pages", "summary": "Kolmogorov-Arnold Networks (KAN) offer universal function approximation using\nunivariate spline compositions without nonlinear activations. In this work, we\nintegrate Error-Correcting Output Codes (ECOC) into the KAN framework to\ntransform multi-class classification into multiple binary tasks, improving\nrobustness via Hamming-distance decoding. Our proposed KAN with ECOC method\noutperforms vanilla KAN on a challenging blood cell classification dataset,\nachieving higher accuracy under diverse hyperparameter settings. Ablation\nstudies further confirm that ECOC consistently enhances performance across\nFastKAN and FasterKAN variants. These results demonstrate that ECOC integration\nsignificantly boosts KAN generalizability in critical healthcare AI\napplications. To the best of our knowledge, this is the first integration of\nECOC with KAN for enhancing multi-class medical image classification\nperformance.", "AI": {"tldr": "KAN\u7ed3\u5408ECOC\u63d0\u5347\u591a\u7c7b\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd", "motivation": "\u63d0\u9ad8KAN\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u533b\u7597AI\u5e94\u7528\u4e2d", "method": "\u5c06ECOC\u96c6\u6210\u5230KAN\u6846\u67b6\u4e2d\uff0c\u5c06\u591a\u7c7b\u5206\u7c7b\u8f6c\u5316\u4e3a\u591a\u4e2a\u4e8c\u5143\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u6c49\u660e\u8ddd\u79bb\u89e3\u7801\u63d0\u5347\u9c81\u68d2\u6027", "result": "\u5728\u8840\u7ec6\u80de\u5206\u7c7b\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u4e8e\u539f\u59cbKAN\uff0c\u4e14\u5728FastKAN\u548cFasterKAN\u53d8\u4f53\u4e2dECOC\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd", "conclusion": "ECOC\u663e\u8457\u589e\u5f3a\u4e86KAN\u5728\u5173\u952e\u533b\u7597AI\u5e94\u7528\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u662f\u9996\u6b21\u5c06ECOC\u4e0eKAN\u7ed3\u5408\u7528\u4e8e\u591a\u7c7b\u533b\u5b66\u56fe\u50cf\u5206\u7c7b"}}
{"id": "2505.05523", "pdf": "https://arxiv.org/pdf/2505.05523", "abs": "https://arxiv.org/abs/2505.05523", "authors": ["Anna Kusetogullari", "Huseyin Kusetogullari", "Martin Andersson", "Tony Gorschek"], "title": "GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)\nare recognized to have significant effects on industry and business dynamics,\nnot least because of their impact on the preconditions for entrepreneurship.\nThere is still a lack of knowledge of GenAI as a theme in entrepreneurship\nresearch. This paper presents a systematic literature review aimed at\nidentifying and analyzing the evolving landscape of research on the effects of\nGenAI on entrepreneurship. We analyze 83 peer-reviewed articles obtained from\nleading academic databases: Web of Science and Scopus. Using natural language\nprocessing and unsupervised machine learning techniques with TF-IDF\nvectorization, Principal Component Analysis (PCA), and hierarchical clustering,\nfive major thematic clusters are identified: (1) Digital Transformation and\nBehavioral Models, (2) GenAI-Enhanced Education and Learning Systems, (3)\nSustainable Innovation and Strategic AI Impact, (4) Business Models and Market\nTrends, and (5) Data-Driven Technological Trends in Entrepreneurship. Based on\nthe review, we discuss future research directions, gaps in the current\nliterature, as well as ethical concerns raised in the literature. We highlight\nthe need for more macro-level research on GenAI and LLMs as external enablers\nfor entrepreneurship and for research on effective regulatory frameworks that\nfacilitate business experimentation, innovation, and further technology\ndevelopment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5bf9\u521b\u4e1a\u7814\u7a76\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u4e86\u4e94\u4e2a\u4e3b\u8981\u4e3b\u9898\u96c6\u7fa4\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3001\u6587\u732e\u7f3a\u53e3\u53ca\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8eGenAI\u5728\u521b\u4e1a\u7814\u7a76\u4e2d\u4f5c\u7528\u7684\u77e5\u8bc6\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6280\u672f\uff08\u5982TF-IDF\u5411\u91cf\u5316\u3001PCA\u548c\u5c42\u6b21\u805a\u7c7b\uff09\uff0c\u5206\u6790\u4e8683\u7bc7\u6765\u81eaWeb of Science\u548cScopus\u7684\u540c\u884c\u8bc4\u5ba1\u6587\u7ae0\u3002", "result": "\u8bc6\u522b\u51fa\u4e94\u4e2a\u4e3b\u8981\u4e3b\u9898\u96c6\u7fa4\uff1a\u6570\u5b57\u8f6c\u578b\u4e0e\u884c\u4e3a\u6a21\u578b\u3001GenAI\u589e\u5f3a\u7684\u6559\u80b2\u4e0e\u5b66\u4e60\u7cfb\u7edf\u3001\u53ef\u6301\u7eed\u521b\u65b0\u4e0e\u6218\u7565AI\u5f71\u54cd\u3001\u5546\u4e1a\u6a21\u5f0f\u4e0e\u5e02\u573a\u8d8b\u52bf\u3001\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u7684\u521b\u4e1a\u6280\u672f\u8d8b\u52bf\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u672a\u6765\u9700\u8981\u5bf9GenAI\u4f5c\u4e3a\u521b\u4e1a\u5916\u90e8\u63a8\u52a8\u8005\u8fdb\u884c\u5b8f\u89c2\u7814\u7a76\uff0c\u5e76\u63a2\u8ba8\u6709\u6548\u7684\u76d1\u7ba1\u6846\u67b6\u4ee5\u4fc3\u8fdb\u5546\u4e1a\u5b9e\u9a8c\u548c\u521b\u65b0\u3002"}}
{"id": "2505.05799", "pdf": "https://arxiv.org/pdf/2505.05799", "abs": "https://arxiv.org/abs/2505.05799", "authors": ["Haojie Duanmu", "Xiuhong Li", "Zhihang Yuan", "Size Zheng", "Jiangfei Duan", "Xingcheng Zhang", "Dahua Lin"], "title": "MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models face deployment challenges due to their large\nparameter counts and computational demands. We explore quantization for MoE\nmodels and highlight two key insights: 1) linear blocks exhibit varying\nquantization sensitivity, and 2) divergent expert activation frequencies create\nheterogeneous computational characteristics. Based on these observations, we\nintroduce MxMoE, a mixed-precision optimization framework for MoE models that\nconsiders both algorithmic and system perspectives. MxMoE navigates the design\nspace defined by parameter sensitivity, expert activation dynamics, and\nhardware resources to derive efficient mixed-precision configurations.\nAdditionally, MxMoE automatically generates optimized mixed-precision GroupGEMM\nkernels, enabling parallel execution of GEMMs with different precisions.\nEvaluations show that MxMoE outperforms existing methods, achieving 2.4 lower\nWikitext-2 perplexity than GPTQ at 2.25-bit and delivering up to 3.4x speedup\nover full precision, as well as up to 29.4% speedup over uniform quantization\nat equivalent accuracy with 5-bit weight-activation quantization. Our code is\navailable at https://github.com/cat538/MxMoE.", "AI": {"tldr": "MoE\u6a21\u578b\u7684\u91cf\u5316\u90e8\u7f72\u5b58\u5728\u6311\u6218\uff0cMxMoE\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u4f18\u5316\u89e3\u51b3\u53c2\u6570\u654f\u611f\u6027\u548c\u4e13\u5bb6\u6fc0\u6d3b\u9891\u7387\u5dee\u5f02\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "MoE\u6a21\u578b\u56e0\u53c2\u6570\u591a\u548c\u8ba1\u7b97\u9700\u6c42\u5927\u800c\u96be\u4ee5\u90e8\u7f72\uff0c\u9700\u63a2\u7d22\u91cf\u5316\u65b9\u6cd5\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51faMxMoE\u6846\u67b6\uff0c\u7ed3\u5408\u53c2\u6570\u654f\u611f\u6027\u548c\u4e13\u5bb6\u6fc0\u6d3b\u9891\u7387\uff0c\u8bbe\u8ba1\u6df7\u5408\u7cbe\u5ea6\u914d\u7f6e\u5e76\u81ea\u52a8\u751f\u6210\u4f18\u5316\u7684GroupGEMM\u5185\u6838\u3002", "result": "MxMoE\u57282.25-bit\u4e0b\u6bd4GPTQ\u964d\u4f4e2.4\u500d\u56f0\u60d1\u5ea6\uff0c\u901f\u5ea6\u6bd4\u5168\u7cbe\u5ea6\u5feb3.4\u500d\uff0c\u6bd4\u5747\u5300\u91cf\u5316\u5feb29.4%\u3002", "conclusion": "MxMoE\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86MoE\u6a21\u578b\u7684\u90e8\u7f72\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2505.05736", "pdf": "https://arxiv.org/pdf/2505.05736", "abs": "https://arxiv.org/abs/2505.05736", "authors": ["Da Wu", "Zhanliang Wang", "Quan Nguyen", "Zhuoran Xu", "Kai Wang"], "title": "Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications", "categories": ["q-bio.QM", "cs.CL", "cs.CV", "cs.LG"], "comment": "First Draft", "summary": "The scarcity of high-quality multimodal biomedical data limits the ability to\neffectively fine-tune pretrained Large Language Models (LLMs) for specialized\nbiomedical tasks. To address this challenge, we introduce MINT (Multimodal\nIntegrated kNowledge Transfer), a framework that aligns unimodal large decoder\nmodels with domain-specific decision patterns from multimodal biomedical data\nthrough preference optimization. While MINT supports different optimization\ntechniques, we primarily implement it with the Odds Ratio Preference\nOptimization (ORPO) framework as its backbone. This strategy enables the\naligned LLMs to perform predictive tasks using text-only or image-only inputs\nwhile retaining knowledge learnt from multimodal data. MINT leverages an\nupstream multimodal machine learning (MML) model trained on high-quality\nmultimodal data to transfer domain-specific insights to downstream text-only or\nimage-only LLMs. We demonstrate its effectiveness through two key applications:\n(1) Rare genetic disease prediction from texts, where MINT uses a multimodal\nencoder model, trained on facial photos and clinical notes, to generate a\npreference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite\nrelying on text input only, the MINT-derived model outperforms models trained\nwith SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue\ntype classification using cell nucleus images, where MINT uses a\nvision-language foundation model as the preference generator, containing\nknowledge learnt from both text and histopathological images to align\ndownstream image-only models. The resulting MINT-derived model significantly\nimproves the performance of Llama 3.2-Vision-11B-Instruct on tissue type\nclassification. In summary, MINT provides an effective strategy to align\nunimodal LLMs with high-quality multimodal expertise through preference\noptimization.", "AI": {"tldr": "MINT\u6846\u67b6\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5c06\u5355\u6a21\u6001\u5927\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u6570\u636e\u7684\u9886\u57df\u7279\u5b9a\u51b3\u7b56\u6a21\u5f0f\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u6216\u56fe\u50cf\u5355\u72ec\u8f93\u5165\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u5fae\u8c03\u6548\u679c\uff0cMINT\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "MINT\u91c7\u7528Odds Ratio Preference Optimization (ORPO)\u6846\u67b6\uff0c\u5229\u7528\u4e0a\u6e38\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6a21\u578b\u751f\u6210\u504f\u597d\u6570\u636e\u96c6\uff0c\u5c06\u9886\u57df\u77e5\u8bc6\u4f20\u9012\u81f3\u4e0b\u6e38\u5355\u6a21\u6001\u6a21\u578b\u3002", "result": "\u5728\u7f55\u89c1\u9057\u4f20\u75be\u75c5\u9884\u6d4b\u548c\u7ec4\u7ec7\u7c7b\u578b\u5206\u7c7b\u4e2d\uff0cMINT\u5bf9\u9f50\u7684\u6a21\u578b\u6027\u80fd\u8d85\u8d8aSFT\u3001RAG\u3001DPO\u7b49\u65b9\u6cd5\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "conclusion": "MINT\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5b9e\u73b0\u4e86\u5355\u6a21\u6001LLMs\u4e0e\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u4e13\u4e1a\u77e5\u8bc6\u7684\u5bf9\u9f50\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05803", "pdf": "https://arxiv.org/pdf/2505.05803", "abs": "https://arxiv.org/abs/2505.05803", "authors": ["Yiming Li", "Man He", "Jiapeng Liu"], "title": "A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve", "categories": ["cs.LG"], "comment": "28 pages, 6 figures", "summary": "The state of health (SOH) of lithium-ion batteries (LIBs) is crucial for\nensuring the safe and reliable operation of electric vehicles. Nevertheless,\nthe prevailing SOH estimation methods often have limited generalizability. This\npaper introduces a data-driven approach for estimating the SOH of LIBs, which\nis designed to improve generalization. We construct a hybrid model named ACLA,\nwhich integrates the attention mechanism, convolutional neural network (CNN),\nand long short-term memory network (LSTM) into the augmented neural ordinary\ndifferential equation (ANODE) framework. This model employs normalized charging\ntime corresponding to specific voltages in the constant current charging phase\nas input and outputs the SOH as well as remaining useful of life. The model is\ntrained on NASA and Oxford datasets and validated on the TJU and HUST datasets.\nCompared to the benchmark models NODE and ANODE, ACLA exhibits higher accuracy\nwith root mean square errors (RMSE) for SOH estimation as low as 1.01% and\n2.24% on the TJU and HUST datasets, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACLA\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u9502\u79bb\u5b50\u7535\u6c60\u5065\u5eb7\u72b6\u6001\uff08SOH\uff09\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3001CNN\u548cLSTM\uff0c\u57fa\u4e8eANODE\u6846\u67b6\uff0c\u901a\u8fc7\u5145\u7535\u9636\u6bb5\u7684\u7279\u5b9a\u7535\u538b\u65f6\u95f4\u6570\u636e\u9884\u6d4bSOH\u548c\u5269\u4f59\u5bff\u547d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cACLA\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684RMSE\u4f4e\u81f31.01%\u548c2.24%\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u9502\u79bb\u5b50\u7535\u6c60\u7684SOH\u5bf9\u7535\u52a8\u8f66\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u3001CNN\u548cLSTM\u7684\u6df7\u5408\u6a21\u578bACLA\uff0c\u57fa\u4e8eANODE\u6846\u67b6\uff0c\u4f7f\u7528\u5145\u7535\u9636\u6bb5\u7684\u7279\u5b9a\u7535\u538b\u65f6\u95f4\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u8f93\u51faSOH\u548c\u5269\u4f59\u5bff\u547d\u3002\u6a21\u578b\u5728NASA\u548cOxford\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728TJU\u548cHUST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "ACLA\u5728TJU\u548cHUST\u6570\u636e\u96c6\u4e0a\u7684SOH\u4f30\u8ba1RMSE\u5206\u522b\u4e3a1.01%\u548c2.24%\uff0c\u8868\u73b0\u4f18\u4e8eNODE\u548cANODE\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "ACLA\u901a\u8fc7\u591a\u6a21\u6001\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86SOH\u4f30\u8ba1\u7684\u6cdb\u5316\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2505.05813", "pdf": "https://arxiv.org/pdf/2505.05813", "abs": "https://arxiv.org/abs/2505.05813", "authors": ["Qiufu Li", "Huibin Xiao", "Linlin Shen"], "title": "BCE vs. CE in Deep Feature Learning", "categories": ["cs.LG"], "comment": "Accepted by ICML2025", "summary": "When training classification models, it expects that the learned features are\ncompact within classes, and can well separate different classes. As the\ndominant loss function for training classification models, minimizing\ncross-entropy (CE) loss maximizes the compactness and distinctiveness, i.e.,\nreaching neural collapse (NC). The recent works show that binary CE (BCE)\nperforms also well in multi-class tasks. In this paper, we compare BCE and CE\nin deep feature learning. For the first time, we prove that BCE can also\nmaximize the intra-class compactness and inter-class distinctiveness when\nreaching its minimum, i.e., leading to NC. We point out that CE measures the\nrelative values of decision scores in the model training, implicitly enhancing\nthe feature properties by classifying samples one-by-one. In contrast, BCE\nmeasures the absolute values of decision scores and adjust the\npositive/negative decision scores across all samples to uniformly high/low\nlevels. Meanwhile, the classifier biases in BCE present a substantial\nconstraint on the decision scores to explicitly enhance the feature properties\nin the training. The experimental results are aligned with above analysis, and\nshow that BCE could improve the classification and leads to better compactness\nand distinctiveness among sample features. The codes will be released.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u548c\u591a\u7c7b\u4ea4\u53c9\u71b5\uff08CE\uff09\u5728\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u9996\u6b21\u8bc1\u660eBCE\u4e5f\u80fd\u5728\u8fbe\u5230\u6700\u5c0f\u503c\u65f6\u6700\u5927\u5316\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5dee\u5f02\u6027\uff0c\u5373\u5bfc\u81f4\u795e\u7ecf\u5d29\u584c\uff08NC\uff09\u3002\u7814\u7a76\u53d1\u73b0BCE\u901a\u8fc7\u8c03\u6574\u6240\u6709\u6837\u672c\u7684\u51b3\u7b56\u5206\u6570\u6765\u589e\u5f3a\u7279\u5f81\u5c5e\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aBCE\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u5e76\u5e26\u6765\u66f4\u597d\u7684\u7279\u5f81\u7d27\u51d1\u6027\u548c\u5dee\u5f02\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u4e8c\u5143\u4ea4\u53c9\u71b5\uff08BCE\uff09\u5728\u591a\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u4e2d\u5bf9\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5dee\u5f02\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6bd4\u8f83BCE\u548cCE\u5728\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660eBCE\u4e5f\u80fd\u5bfc\u81f4\u795e\u7ecf\u5d29\u584c\uff08NC\uff09\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0eCE\u5728\u8bad\u7ec3\u673a\u5236\u4e0a\u7684\u4e0d\u540c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aBCE\u80fd\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5e26\u6765\u66f4\u597d\u7684\u7c7b\u5185\u7d27\u51d1\u6027\u548c\u7c7b\u95f4\u5dee\u5f02\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660eBCE\u5728\u591a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u80fd\u6709\u6548\u589e\u5f3a\u7279\u5f81\u7684\u7d27\u51d1\u6027\u548c\u5dee\u5f02\u6027\u3002"}}
{"id": "2505.05819", "pdf": "https://arxiv.org/pdf/2505.05819", "abs": "https://arxiv.org/abs/2505.05819", "authors": ["Lorenzo Beretta"], "title": "New Statistical and Computational Results for Learning Junta Distributions", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "We study the problem of learning junta distributions on $\\{0, 1\\}^n$, where a\ndistribution is a $k$-junta if its probability mass function depends on a\nsubset of at most $k$ variables. We make two main contributions:\n  - We show that learning $k$-junta distributions is \\emph{computationally}\nequivalent to learning $k$-parity functions with noise (LPN), a landmark\nproblem in computational learning theory.\n  - We design an algorithm for learning junta distributions whose statistical\ncomplexity is optimal, up to polylogarithmic factors. Computationally, our\nalgorithm matches the complexity of previous (non-sample-optimal) algorithms.\n  Combined, our two contributions imply that our algorithm cannot be\nsignificantly improved, statistically or computationally, barring a\nbreakthrough for LPN.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728$\"${0, 1}^n$\u4e0a\u5b66\u4e60k-junta\u5206\u5e03\u7684\u95ee\u9898\uff0c\u8868\u660e\u5b66\u4e60\u5b83\u4e0e\u5b66\u4e60\u5e26\u566a\u58f0\u7684k-\u5947\u5076\u51fd\u6570\uff08LPN\uff09\u8ba1\u7b97\u7b49\u4ef7\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u590d\u6742\u5ea6\u63a5\u8fd1\u6700\u4f18\u7684\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76k-junta\u5206\u5e03\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u65e8\u5728\u63ed\u793a\u5176\u4e0eLPN\u95ee\u9898\u7684\u8ba1\u7b97\u7b49\u4ef7\u6027\uff0c\u5e76\u63a2\u7d22\u6700\u4f18\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60junta\u5206\u5e03\u7684\u7b97\u6cd5\uff0c\u7edf\u8ba1\u590d\u6742\u5ea6\u63a5\u8fd1\u6700\u4f18\uff08\u591a\u5bf9\u6570\u56e0\u5b50\u5185\uff09\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u4e4b\u524d\u975e\u6700\u4f18\u7b97\u6cd5\u76f8\u5f53\u3002", "result": "\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u7edf\u8ba1\u548c\u8ba1\u7b97\u4e0a\u7684\u6700\u4f18\u6027\uff0c\u9664\u975eLPN\u95ee\u9898\u6709\u7a81\u7834\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u7edf\u8ba1\u548c\u8ba1\u7b97\u4e0a\u5747\u96be\u4ee5\u663e\u8457\u6539\u8fdb\uff0c\u4e3ak-junta\u5206\u5e03\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2505.05828", "pdf": "https://arxiv.org/pdf/2505.05828", "abs": "https://arxiv.org/abs/2505.05828", "authors": ["Alba Mar\u00eda M\u00e1rmol-Romero", "Manuel Garc\u00eda-Vega", "Miguel \u00c1ngel Garc\u00eda-Cumbreras", "Arturo Montejo-R\u00e1ez"], "title": "An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers", "categories": ["cs.HC", "cs.CL"], "comment": "This is an Accepted Manuscript version of the following article,\n  accepted for publication in International Journal of Human-Computer\n  Interaction. It is deposited under the terms of the Creative Commons\n  Attribution-NonCommercial-NoDerivatives License", "summary": "This paper presents a chatbot-based system to engage young Spanish people in\nthe awareness of certain mental disorders through a self-disclosure technique.\nThe study was carried out in a population of teenagers aged between 12 and 18\nyears. The dialogue engine mixes closed and open conversations, so certain\ncontrolled messages are sent to focus the chat on a specific disorder, which\nwill change over time. Once a set of trial questions is answered, the system\ncan initiate the conversation on the disorder under the focus according to the\nuser's sensibility to that disorder, in an attempt to establish a more\nempathetic communication. Then, an open conversation based on the GPT-3\nlanguage model is initiated, allowing the user to express themselves with more\nfreedom. The results show that these systems are of interest to young people\nand could help them become aware of certain mental disorders.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\uff1a\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u804a\u5929\u673a\u5668\u4eba\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u6211\u62ab\u9732\u6280\u672f\u63d0\u9ad8\u897f\u73ed\u7259\u5e74\u8f7b\u4eba\u5bf9\u7279\u5b9a\u5fc3\u7406\u969c\u788d\u7684\u8ba4\u77e5\u3002\u7cfb\u7edf\u7ed3\u5408\u5c01\u95ed\u548c\u5f00\u653e\u5bf9\u8bdd\uff0c\u6839\u636e\u7528\u6237\u654f\u611f\u5ea6\u8c03\u6574\u8bdd\u9898\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6b64\u7c7b\u7cfb\u7edf\u5bf9\u9752\u5c11\u5e74\u5177\u6709\u5438\u5f15\u529b\u5e76\u80fd\u63d0\u5347\u5fc3\u7406\u969c\u788d\u610f\u8bc6\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u63d0\u5347\u9752\u5c11\u5e74\u5bf9\u5fc3\u7406\u969c\u788d\u7684\u8ba4\u77e5\uff0c\u91c7\u7528\u804a\u5929\u673a\u5668\u4eba\u5f62\u5f0f\u589e\u5f3a\u4e92\u52a8\u6027\u4e0e\u540c\u7406\u5fc3\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6df7\u5408\u5c01\u95ed\u4e0e\u5f00\u653e\u5bf9\u8bdd\u7684\u804a\u5929\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408GPT-3\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u5bf9\u8bdd\uff0c\u9488\u5bf912-18\u5c81\u897f\u73ed\u7259\u9752\u5c11\u5e74\u7fa4\u4f53\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u7cfb\u7edf\u53d7\u5230\u9752\u5c11\u5e74\u6b22\u8fce\uff0c\u5e76\u6709\u6548\u63d0\u9ad8\u4e86\u4ed6\u4eec\u5bf9\u7279\u5b9a\u5fc3\u7406\u969c\u788d\u7684\u8ba4\u77e5\u3002", "conclusion": "\u57fa\u4e8e\u804a\u5929\u673a\u5668\u4eba\u7684\u81ea\u6211\u62ab\u9732\u6280\u672f\u662f\u63d0\u5347\u9752\u5c11\u5e74\u5fc3\u7406\u969c\u788d\u610f\u8bc6\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.05857", "pdf": "https://arxiv.org/pdf/2505.05857", "abs": "https://arxiv.org/abs/2505.05857", "authors": ["Nathan Justin", "Qingshi Sun", "Andr\u00e9s G\u00f3mez", "Phebe Vayanos"], "title": "Mixed-Integer Optimization for Responsible Machine Learning", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "56 pages, 10 figures", "summary": "In the last few decades, Machine Learning (ML) has achieved significant\nsuccess across domains ranging from healthcare, sustainability, and the social\nsciences, to criminal justice and finance. But its deployment in increasingly\nsophisticated, critical, and sensitive areas affecting individuals, the groups\nthey belong to, and society as a whole raises critical concerns around\nfairness, transparency, robustness, and privacy, among others. As the\ncomplexity and scale of ML systems and of the settings in which they are\ndeployed grow, so does the need for responsible ML methods that address these\nchallenges while providing guaranteed performance in deployment.\n  Mixed-integer optimization (MIO) offers a powerful framework for embedding\nresponsible ML considerations directly into the learning process while\nmaintaining performance. For example, it enables learning of inherently\ntransparent models that can conveniently incorporate fairness or other domain\nspecific constraints. This tutorial paper provides an accessible and\ncomprehensive introduction to this topic discussing both theoretical and\npractical aspects. It outlines some of the core principles of responsible ML,\ntheir importance in applications, and the practical utility of MIO for building\nML models that align with these principles. Through examples and mathematical\nformulations, it illustrates practical strategies and available tools for\nefficiently solving MIO problems for responsible ML. It concludes with a\ndiscussion on current limitations and open research questions, providing\nsuggestions for future work.", "AI": {"tldr": "\u8be5\u6559\u7a0b\u8bba\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528\u6df7\u5408\u6574\u6570\u4f18\u5316\uff08MIO\uff09\u6846\u67b6\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5d4c\u5165\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\u7b49\u8d23\u4efb\u6027\u8003\u91cf\uff0c\u5e76\u4fdd\u6301\u6027\u80fd\u3002\u8ba8\u8bba\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\uff0c\u63d0\u4f9b\u4e86\u89e3\u51b3MIO\u95ee\u9898\u7684\u7b56\u7565\u548c\u5de5\u5177\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u5173\u952e\u654f\u611f\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\u7b49\u95ee\u9898\u7684\u5173\u6ce8\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5d4c\u5165\u8d23\u4efb\u6027\u8003\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u4f18\u5316\uff08MIO\uff09\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7eb3\u5165\u8d23\u4efb\u6027\u7ea6\u675f\uff08\u5982\u516c\u5e73\u6027\u3001\u900f\u660e\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u516c\u5f0f\u548c\u793a\u4f8b\u5c55\u793a\u89e3\u51b3MIO\u95ee\u9898\u7684\u7b56\u7565\u548c\u5de5\u5177\u3002", "result": "MIO\u4e3a\u8d23\u4efb\u6027\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u4fdd\u8bc1\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b66\u4e60\u900f\u660e\u6a21\u578b\u5e76\u6ee1\u8db3\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86MIO\u5728\u8d23\u4efb\u6027\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u5f53\u524d\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05863", "pdf": "https://arxiv.org/pdf/2505.05863", "abs": "https://arxiv.org/abs/2505.05863", "authors": ["Reiji Suzuki", "Takaya Arita"], "title": "Evolutionary ecology of words", "categories": ["q-bio.PE", "cs.AI", "cs.CL", "92B20"], "comment": "8 pages, 5 figures. Preprint of the paper published in Proceedings of\n  2025 IEEE Symposium on Computational Intelligence in Artificial Life and\n  Cooperative Intelligent Systems (ALIFE-CIS)", "summary": "We propose a model for the evolutionary ecology of words as one attempt to\nextend evolutionary game theory and agent-based models by utilizing the rich\nlinguistic expressions of Large Language Models (LLMs). Our model enables the\nemergence and evolution of diverse and infinite options for interactions among\nagents. Within the population, each agent possesses a short word (or phrase)\ngenerated by an LLM and moves within a spatial environment. When agents become\nadjacent, the outcome of their interaction is determined by the LLM based on\nthe relationship between their words, with the loser's word being replaced by\nthe winner's. Word mutations, also based on LLM outputs, may occur. We\nconducted preliminary experiments assuming that ``strong animal species\" would\nsurvive. The results showed that from an initial population consisting of\nwell-known species, many species emerged both gradually and in a punctuated\nequilibrium manner. Each trial demonstrated the unique evolution of diverse\npopulations, with one type of large species becoming dominant, such as\nterrestrial animals, marine life, or extinct species, which were ecologically\nspecialized and adapted ones across diverse extreme habitats. We also conducted\na long-term experiment with a large population, demonstrating the emergence and\ncoexistence of diverse species.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u5316\u751f\u6001\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u4ee3\u7406\u4e4b\u95f4\u7684\u4ea4\u4e92\u548c\u8bcd\u8bed\u7a81\u53d8\u4e3a\u8bcd\u6c47\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u79cd\u7fa4\u591a\u6837\u6027\u7684\u72ec\u7279\u6f14\u5316\u548c\u9002\u5e94\u6781\u7aef\u73af\u5883\u7684\u7269\u79cd\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6269\u5c55\u8fdb\u5316\u535a\u5f08\u7406\u8bba\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e30\u5bcc\u8868\u8fbe\u80fd\u529b\uff0c\u63a2\u7d22\u8bcd\u6c47\u7684\u8fdb\u5316\u751f\u6001\u5b66\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7a7a\u95f4\u73af\u5883\u6a21\u578b\uff0c\u4ee3\u7406\u901a\u8fc7LLM\u751f\u6210\u7684\u77ed\u8bcd\u6216\u77ed\u8bed\u4ea4\u4e92\uff0c\u80dc\u8005\u66ff\u4ee3\u8d25\u8005\u8bcd\u6c47\uff0c\u5e76\u53ef\u80fd\u53d1\u751f\u57fa\u4e8eLLM\u8f93\u51fa\u7684\u8bcd\u6c47\u7a81\u53d8\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\u79cd\u7fa4\u4ece\u5df2\u77e5\u7269\u79cd\u51fa\u53d1\uff0c\u4ee5\u6e10\u8fdb\u548c\u95f4\u65ad\u5e73\u8861\u65b9\u5f0f\u6f14\u5316\u51fa\u591a\u6837\u7269\u79cd\uff0c\u6700\u7ec8\u7279\u5b9a\u7c7b\u578b\uff08\u5982\u9646\u5730\u52a8\u7269\u3001\u6d77\u6d0b\u751f\u7269\uff09\u5360\u636e\u4e3b\u5bfc\u3002\u957f\u671f\u5b9e\u9a8c\u4e2d\u89c2\u5bdf\u5230\u591a\u6837\u7269\u79cd\u7684\u5171\u5b58\u3002", "conclusion": "\u6a21\u578b\u5c55\u793a\u4e86\u8bcd\u6c47\u8fdb\u5316\u7684\u591a\u6837\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u8fdb\u5316\u751f\u6001\u5b66\u548c\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05868", "pdf": "https://arxiv.org/pdf/2505.05868", "abs": "https://arxiv.org/abs/2505.05868", "authors": ["Changkun Ye", "Russell Tsuchida", "Lars Petersson", "Nick Barnes"], "title": "Open Set Label Shift with Test Time Out-of-Distribution Reference", "categories": ["cs.LG"], "comment": "Accepted at CVPR 2025", "summary": "Open set label shift (OSLS) occurs when label distributions change from a\nsource to a target distribution, and the target distribution has an additional\nout-of-distribution (OOD) class. In this work, we build estimators for both\nsource and target open set label distributions using a source domain\nin-distribution (ID) classifier and an ID/OOD classifier. With reasonable\nassumptions on the ID/OOD classifier, the estimators are assembled into a\nsequence of three stages: 1) an estimate of the source label distribution of\nthe OOD class, 2) an EM algorithm for Maximum Likelihood estimates (MLE) of the\ntarget label distribution, and 3) an estimate of the target label distribution\nof OOD class under relaxed assumptions on the OOD classifier. The sampling\nerrors of estimates in 1) and 3) are quantified with a concentration\ninequality. The estimation result allows us to correct the ID classifier\ntrained on the source distribution to the target distribution without\nretraining. Experiments on a variety of open set label shift settings\ndemonstrate the effectiveness of our model. Our code is available at\nhttps://github.com/ChangkunYe/OpenSetLabelShift.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f30\u8ba1\u6e90\u548c\u76ee\u6807\u5f00\u653e\u96c6\u6807\u7b7e\u5206\u5e03\u7684\u4e09\u4e2a\u9636\u6bb5\u65b9\u6cd5\uff0c\u4f7f\u7528ID/OOD\u5206\u7c7b\u5668\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u96c6\u6807\u7b7e\u504f\u79fb\uff08OSLS\uff09\u95ee\u9898\uff0c\u5373\u76ee\u6807\u5206\u5e03\u4e2d\u5b58\u5728\u6e90\u5206\u5e03\u672a\u89c1\u7684\u7c7b\u522b\u65f6\u6807\u7b7e\u5206\u5e03\u7684\u53d8\u5316\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u4f30\u8ba1\u6e90\u6807\u7b7e\u5206\u5e03\u4e2d\u7684OOD\u7c7b\u522b\uff1b2) \u4f7f\u7528EM\u7b97\u6cd5\u5bf9\u76ee\u6807\u6807\u7b7e\u5206\u5e03\u8fdb\u884c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\uff1b3) \u5728\u653e\u5bbdOOD\u5206\u7c7b\u5668\u5047\u8bbe\u4e0b\u4f30\u8ba1\u76ee\u6807OOD\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5f00\u653e\u96c6\u6807\u7b7e\u504f\u79fb\u8bbe\u7f6e\u4e0b\u6709\u6548\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8c03\u6574\u6e90\u5206\u5e03\u5206\u7c7b\u5668\u81f3\u76ee\u6807\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e09\u9636\u6bb5\u4f30\u8ba1\u6709\u6548\u89e3\u51b3OSLS\u95ee\u9898\uff0c\u4e3a\u6807\u7b7e\u5206\u5e03\u504f\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06032", "pdf": "https://arxiv.org/pdf/2505.06032", "abs": "https://arxiv.org/abs/2505.06032", "authors": ["Leon Eshuijs", "Shihan Wang", "Antske Fokkens"], "title": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reliance on spurious correlations (shortcuts) has been shown to underlie many\nof the successes of language models. Previous work focused on identifying the\ninput elements that impact prediction. We investigate how shortcuts are\nactually processed within the model's decision-making mechanism. We use actor\nnames in movie reviews as controllable shortcuts with known impact on the\noutcome. We use mechanistic interpretability methods and identify specific\nattention heads that focus on shortcuts. These heads gear the model towards a\nlabel before processing the complete input, effectively making premature\ndecisions that bypass contextual analysis. Based on these findings, we\nintroduce Head-based Token Attribution (HTA), which traces intermediate\ndecisions back to input tokens. We show that HTA is effective in detecting\nshortcuts in LLMs and enables targeted mitigation by selectively deactivating\nshortcut-related attention heads.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\uff08\u6377\u5f84\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\u51b3\u7b56\u673a\u5236\u4e2d\u6377\u5f84\u7684\u5904\u7406\u65b9\u5f0f\u3002\u901a\u8fc7\u7535\u5f71\u8bc4\u8bba\u4e2d\u6f14\u5458\u540d\u5b57\u4f5c\u4e3a\u53ef\u63a7\u6377\u5f84\uff0c\u5229\u7528\u673a\u7406\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u5173\u6ce8\u6377\u5f84\u7684\u6ce8\u610f\u529b\u5934\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6ce8\u610f\u529b\u5934\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u65e9\u505a\u51fa\u51b3\u7b56\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u5934\u7684\u6807\u8bb0\u5f52\u56e0\u65b9\u6cd5\uff08HTA\uff09\uff0c\u6709\u6548\u68c0\u6d4b\u5e76\u9488\u5bf9\u6027\u7f13\u89e3\u6377\u5f84\u95ee\u9898\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u51b3\u7b56\u673a\u5236\u4e2d\u5982\u4f55\u5904\u7406\u4f9d\u8d56\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u7535\u5f71\u8bc4\u8bba\u4e2d\u7684\u6f14\u5458\u540d\u5b57\u4f5c\u4e3a\u53ef\u63a7\u6377\u5f84\uff0c\u5e94\u7528\u673a\u7406\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u76f8\u5173\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u51faHTA\u65b9\u6cd5\u8ffd\u8e2a\u4e2d\u95f4\u51b3\u7b56\u5230\u8f93\u5165\u6807\u8bb0\u3002", "result": "\u53d1\u73b0\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u4f1a\u4fc3\u4f7f\u6a21\u578b\u8fc7\u65e9\u4f9d\u8d56\u6377\u5f84\u505a\u51fa\u51b3\u7b56\uff0cHTA\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u5e76\u7f13\u89e3\u6b64\u7c7b\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7HTA\u65b9\u6cd5\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6709\u9488\u5bf9\u6027\u5730\u51cf\u5c11\u5bf9\u865a\u5047\u76f8\u5173\u6027\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.05543", "pdf": "https://arxiv.org/pdf/2505.05543", "abs": "https://arxiv.org/abs/2505.05543", "authors": ["Ahdiyeh Alipour", "Tilo Hartmann", "Maryam Alimardani"], "title": "Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "75 pages, Figure 11, Table 5", "summary": "Trust is a fundamental component of human-agent interaction. With the\nincreasing presence of artificial agents in daily life, it is essential to\nunderstand how people perceive and trust these agents. One of the key\nchallenges affecting this perception is the Uncanny Valley Effect (UVE), where\nincreasingly human-like artificial beings can be perceived as eerie or\nrepelling. Despite growing interest in trust and the UVE, existing research\nvaries widely in terms of how these concepts are defined and operationalized.\nThis inconsistency raises important questions about how and under what\nconditions the UVE influences trust in agents. A systematic understanding of\ntheir relationship is currently lacking. This review aims to examine the impact\nof the UVE on human trust in agents and to identify methodological patterns,\nlimitations, and gaps in the existing empirical literature. Following PRISMA\nguidelines, a systematic search identified 53 empirical studies that\ninvestigated both UVE-related constructs and trust or trust-related outcomes.\nStudies were analyzed based on a structured set of categories, including types\nof agents and interactions, methodological and measurement approaches, and key\nfindings. The results of our systematic review reveal that most studies rely on\nstatic images or hypothetical scenarios with limited real-time interaction, and\nthe majority use subjective trust measures. This review offers a novel\nframework for classifying trust measurement approaches with regard to the\nbest-practice criteria for empirically investigating the UVE. As the first\nsystematic attempt to map the intersection of UVE and trust, this review\ncontributes to a deeper understanding of their interplay and offers a\nfoundation for future research. Keywords: the uncanny valley effect, trust,\nhuman-likeness, affinity response, human-agent interaction", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u2018\u6050\u6016\u8c37\u6548\u5e94\u2019\uff08UVE\uff09\u5bf9\u4eba\u7c7b\u4fe1\u4efb\u4eba\u5de5\u4ee3\u7406\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u7684\u65b9\u6cd5\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u4fe1\u4efb\u6d4b\u91cf\u65b9\u6cd5\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u666e\u53ca\uff0c\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u611f\u77e5\u548c\u4fe1\u4efb\u8fd9\u4e9b\u4ee3\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u63a2\u8ba8\u4e86UVE\u4e0e\u4fe1\u4efb\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7406\u89e3\u7684\u7a7a\u767d\u3002", "method": "\u9075\u5faaPRISMA\u6307\u5357\uff0c\u7cfb\u7edf\u6027\u641c\u7d22\u5e76\u5206\u6790\u4e8653\u9879\u540c\u65f6\u7814\u7a76UVE\u548c\u4fe1\u4efb\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5185\u5bb9\u6db5\u76d6\u4ee3\u7406\u7c7b\u578b\u3001\u4ea4\u4e92\u65b9\u5f0f\u3001\u65b9\u6cd5\u548c\u6d4b\u91cf\u5de5\u5177\u3002", "result": "\u53d1\u73b0\u591a\u6570\u7814\u7a76\u4f9d\u8d56\u9759\u6001\u56fe\u50cf\u6216\u5047\u8bbe\u573a\u666f\uff0c\u7f3a\u4e4f\u5b9e\u65f6\u4ea4\u4e92\uff0c\u4e14\u4e3b\u8981\u4f7f\u7528\u4e3b\u89c2\u4fe1\u4efb\u6d4b\u91cf\u65b9\u6cd5\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6700\u4f73\u5b9e\u8df5\u7684\u5206\u7c7b\u6846\u67b6\u3002", "conclusion": "\u8fd9\u9879\u7efc\u8ff0\u4e3a\u7406\u89e3UVE\u548c\u4fe1\u4efb\u7684\u76f8\u4e92\u5173\u7cfb\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.05869", "pdf": "https://arxiv.org/pdf/2505.05869", "abs": "https://arxiv.org/abs/2505.05869", "authors": ["Hao Xu", "Yuntian Chen", "Rui Cao", "Tianning Tang", "Mengge Du", "Jian Li", "Adrian H. Callaghan", "Dongxiao Zhang"], "title": "Generative Discovery of Partial Differential Equations by Learning from Math Handbooks", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": null, "summary": "Data driven discovery of partial differential equations (PDEs) is a promising\napproach for uncovering the underlying laws governing complex systems. However,\npurely data driven techniques face the dilemma of balancing search space with\noptimization efficiency. This study introduces a knowledge guided approach that\nincorporates existing PDEs documented in a mathematical handbook to facilitate\nthe discovery process. These PDEs are encoded as sentence like structures\ncomposed of operators and basic terms, and used to train a generative model,\ncalled EqGPT, which enables the generation of free form PDEs. A loop of\ngeneration evaluation optimization is constructed to autonomously identify the\nmost suitable PDE. Experimental results demonstrate that this framework can\nrecover a variety of PDE forms with high accuracy and computational efficiency,\nparticularly in cases involving complex temporal derivatives or intricate\nspatial terms, which are often beyond the reach of conventional methods. The\napproach also exhibits generalizability to irregular spatial domains and higher\ndimensional settings. Notably, it succeeds in discovering a previously\nunreported PDE governing strongly nonlinear surface gravity waves propagating\ntoward breaking, based on real world experimental data, highlighting its\napplicability to practical scenarios and its potential to support scientific\ndiscovery.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u5f15\u5bfc\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u5b66\u624b\u518c\u4e2d\u7684\u73b0\u6709\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\uff0c\u8bad\u7ec3\u751f\u6210\u6a21\u578bEqGPT\u6765\u81ea\u52a8\u53d1\u73b0PDEs\u3002\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u65f6\u7a7a\u9879\u548c\u9ad8\u7ef4\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u6210\u529f\u53d1\u73b0\u4e86\u4e00\u4e2a\u65b0\u578bPDE\u3002", "motivation": "\u7eaf\u7cb9\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728PDE\u53d1\u73b0\u4e2d\u5b58\u5728\u641c\u7d22\u7a7a\u95f4\u4e0e\u4f18\u5316\u6548\u7387\u7684\u77db\u76fe\uff0c\u800c\u7ed3\u5408\u5df2\u6709\u77e5\u8bc6\u53ef\u4ee5\u63d0\u9ad8\u53d1\u73b0\u6548\u7387\u5e76\u6269\u5c55\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5c06\u73b0\u6709PDE\u7f16\u7801\u4e3a\u53e5\u6cd5\u7ed3\u6784\uff0c\u8bad\u7ec3\u751f\u6210\u6a21\u578bEqGPT\uff0c\u5e76\u6784\u5efa\u751f\u6210-\u8bc4\u4f30-\u4f18\u5316\u7684\u5faa\u73af\u6846\u67b6\u6765\u81ea\u4e3b\u8bc6\u522b\u6700\u4f18PDE\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u5730\u6062\u590d\u591a\u79cdPDE\u5f62\u5f0f\uff0c\u5e76\u63a8\u5e7f\u5230\u4e0d\u89c4\u5219\u7a7a\u95f4\u57df\u548c\u9ad8\u7ef4\u573a\u666f\uff0c\u8fd8\u6210\u529f\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u578bPDE\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u7cfb\u7edf\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u652f\u6301\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2505.06107", "pdf": "https://arxiv.org/pdf/2505.06107", "abs": "https://arxiv.org/abs/2505.06107", "authors": ["Faeze Ghorbanpour", "Thiago Zordan Malaguth", "Aliakbar Akbaritabar"], "title": "Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models", "categories": ["cs.DL", "cs.CL", "cs.MM"], "comment": "Accepted to appear @ ICWSM 2025. The link to the camera-ready paper\n  will be added soon", "summary": "Most web and digital trace data do not include information about an\nindividual's nationality due to privacy concerns. The lack of data on\nnationality can create challenges for migration research. It can lead to a\nleft-censoring issue since we are uncertain about the migrant's country of\norigin. Once we observe an emigration event, if we know the nationality, we can\ndifferentiate it from return migration. We propose methods to detect the\nnationality with the least available data, i.e., full names. We use the\ndetected nationality in comparison with the country of academic origin, which\nis a common approach in studying the migration of researchers. We gathered 2.6\nmillion unique name-nationality pairs from Wikipedia and categorized them into\nfamilies of nationalities with three granularity levels to use as our training\ndata. Using a character-based machine learning model, we achieved a weighted F1\nscore of 84% for the broadest and 67% for the most granular, country-level\ncategorization. In our empirical study, we used the trained and tested model to\nassign nationality to 8+ million scholars' full names in Scopus data. Our\nresults show that using the country of first publication as a proxy for\nnationality underestimates the size of return flows, especially for countries\nwith a more diverse academic workforce, such as the USA, Australia, and Canada.\nWe found that around 48% of emigration from the USA was return migration once\nwe used the country of name origin, in contrast to 33% based on academic\norigin. In the most recent period, 79% of scholars whose affiliation has\nconsistently changed from the USA to China, and are considered emigrants, have\nChinese names in contrast to 41% with a Chinese academic origin. Our proposed\nmethods for addressing left-censoring issues are beneficial for other research\nthat uses digital trace data to study migration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5168\u540d\u68c0\u6d4b\u56fd\u7c4d\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u79fb\u6c11\u7814\u7a76\u4e2d\u56e0\u9690\u79c1\u5bfc\u81f4\u56fd\u7c4d\u6570\u636e\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7814\u7a76\u5b66\u8005\u8fc1\u79fb\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u56e0\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u7684\u56fd\u7c4d\u6570\u636e\u7f3a\u5931\uff0c\u5c24\u5176\u662f\u5728\u79fb\u6c11\u7814\u7a76\u4e2d\u533a\u5206\u79fb\u6c11\u548c\u56de\u5f52\u79fb\u6c11\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4eceWikipedia\u6536\u96c6\u7684260\u4e07\u59d3\u540d-\u56fd\u7c4d\u5bf9\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u57fa\u4e8e\u5b57\u7b26\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u56fd\u7c4d\uff0c\u5e76\u5728Scopus\u6570\u636e\u4e2d\u5e94\u7528\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u7c92\u5ea6\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\uff08\u52a0\u6743F1\u6700\u9ad884%\uff09\uff0c\u53d1\u73b0\u5b66\u672f\u8d77\u6e90\u4f4e\u4f30\u4e86\u56de\u5f52\u79fb\u6c11\u89c4\u6a21\uff0c\u5c24\u5176\u5728\u591a\u6837\u6027\u56fd\u5bb6\u5982\u7f8e\u56fd\u3001\u6fb3\u5927\u5229\u4e9a\u548c\u52a0\u62ff\u5927\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u6c11\u7814\u7a76\u4e2d\u7684\u5de6\u622a\u5c3e\u95ee\u9898\uff0c\u4e3a\u4f7f\u7528\u6570\u5b57\u75d5\u8ff9\u6570\u636e\u7814\u7a76\u8fc1\u79fb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05874", "pdf": "https://arxiv.org/pdf/2505.05874", "abs": "https://arxiv.org/abs/2505.05874", "authors": ["Anjie Qiao", "Hao Zhang", "Qianmu Yuan", "Qirui Deng", "Jingtian Su", "Weifeng Huang", "Huihao Zhou", "Guo-Bo Li", "Zhen Wang", "Jinping Lei"], "title": "A 3D pocket-aware and evolutionary conserved interaction guided diffusion model for molecular optimization", "categories": ["cs.LG", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Generating molecules that bind to specific protein targets via diffusion\nmodels has shown good promise for structure-based drug design and molecule\noptimization. Especially, the diffusion models with binding interaction\nguidance enables molecule generation with high affinity through forming\nfavorable interaction within protein pocket. However, the generated molecules\nmay not form interactions with the highly conserved residues, which are\nimportant for protein functions and bioactivities of the ligands. Herein, we\ndeveloped a new 3D target-aware diffusion model DiffDecip, which explicitly\nincorporates the protein-ligand binding interactions and evolutionary\nconservation information of protein residues into both diffusion and sampling\nprocess, for molecule optimization through scaffold decoration. The model\nperformance revealed that DiffDecip outperforms baseline model DiffDec on\nmolecule optimization towards higher affinity through forming more non-covalent\ninteractions with highly conserved residues in the protein pocket.", "AI": {"tldr": "DiffDecip\u662f\u4e00\u79cd\u65b0\u578b3D\u76ee\u6807\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u76f8\u4e92\u4f5c\u7528\u548c\u86cb\u767d\u8d28\u6b8b\u57fa\u7684\u8fdb\u5316\u4fdd\u5b88\u4fe1\u606f\uff0c\u4f18\u5316\u5206\u5b50\u751f\u6210\uff0c\u7279\u522b\u662f\u63d0\u9ad8\u4e0e\u9ad8\u5ea6\u4fdd\u5b88\u6b8b\u57fa\u7684\u975e\u5171\u4ef7\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4e0e\u7279\u5b9a\u86cb\u767d\u8d28\u7ed3\u5408\u7684\u5206\u5b50\u65f6\uff0c\u53ef\u80fd\u5ffd\u7565\u4e0e\u9ad8\u5ea6\u4fdd\u5b88\u6b8b\u57fa\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u800c\u8fd9\u4e9b\u6b8b\u57fa\u5bf9\u86cb\u767d\u8d28\u529f\u80fd\u548c\u914d\u4f53\u751f\u7269\u6d3b\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86DiffDecip\u6a21\u578b\uff0c\u5c06\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u76f8\u4e92\u4f5c\u7528\u548c\u6b8b\u57fa\u4fdd\u5b88\u4fe1\u606f\u6574\u5408\u5230\u6269\u6563\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u7528\u4e8e\u652f\u67b6\u4fee\u9970\u7684\u5206\u5b50\u4f18\u5316\u3002", "result": "DiffDecip\u5728\u5206\u5b50\u4f18\u5316\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bDiffDec\uff0c\u80fd\u751f\u6210\u4e0e\u86cb\u767d\u8d28\u53e3\u888b\u4e2d\u9ad8\u5ea6\u4fdd\u5b88\u6b8b\u57fa\u5f62\u6210\u66f4\u591a\u975e\u5171\u4ef7\u76f8\u4e92\u4f5c\u7528\u7684\u9ad8\u4eb2\u548c\u529b\u5206\u5b50\u3002", "conclusion": "DiffDecip\u901a\u8fc7\u7ed3\u5408\u4fdd\u5b88\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u751f\u6210\u7684\u4eb2\u548c\u529b\u548c\u529f\u80fd\u6027\uff0c\u4e3a\u57fa\u4e8e\u7ed3\u6784\u7684\u836f\u7269\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.06184", "pdf": "https://arxiv.org/pdf/2505.06184", "abs": "https://arxiv.org/abs/2505.06184", "authors": ["Vahid Rahimzadeh", "Ali Hamzehpour", "Azadeh Shakery", "Masoud Asadpour"], "title": "From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling", "categories": ["cs.SI", "cs.CL", "cs.IR", "I.2.7"], "comment": "Accepted at MisD @ AAAI ICWSM 2025", "summary": "Social media user profiling through content analysis is crucial for tasks\nlike misinformation detection, engagement prediction, hate speech monitoring,\nand user behavior modeling. However, existing profiling techniques, including\ntweet summarization, attribute-based profiling, and latent representation\nlearning, face significant limitations: they often lack transferability,\nproduce non-interpretable features, require large labeled datasets, or rely on\nrigid predefined categories that limit adaptability. We introduce a novel large\nlanguage model (LLM)-based approach that leverages domain-defining statements,\nwhich serve as key characteristics outlining the important pillars of a domain\nas foundations for profiling. Our two-stage method first employs\nsemi-supervised filtering with a domain-specific knowledge base, then generates\nboth abstractive (synthesized descriptions) and extractive (representative\ntweet selections) user profiles. By harnessing LLMs' inherent knowledge with\nminimal human validation, our approach is adaptable across domains while\nreducing the need for large labeled datasets. Our method generates\ninterpretable natural language user profiles, condensing extensive user data\ninto a scale that unlocks LLMs' reasoning and knowledge capabilities for\ndownstream social network tasks. We contribute a Persian political Twitter (X)\ndataset and an LLM-based evaluation framework with human validation.\nExperimental results show our method significantly outperforms state-of-the-art\nLLM-based and traditional methods by 9.8%, demonstrating its effectiveness in\ncreating flexible, adaptable, and interpretable user profiles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7528\u6237\u753b\u50cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u5b9a\u4e49\u8bed\u53e5\u751f\u6210\u53ef\u89e3\u91ca\u7684\u81ea\u7136\u8bed\u8a00\u7528\u6237\u753b\u50cf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd59.8%\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7528\u6237\u753b\u50cf\u6280\u672f\u7f3a\u4e4f\u53ef\u8fc1\u79fb\u6027\u3001\u4e0d\u53ef\u89e3\u91ca\u3001\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u9884\u5b9a\u4e49\u5206\u7c7b\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u534a\u76d1\u7763\u8fc7\u6ee4\u4e0e\u9886\u57df\u77e5\u8bc6\u5e93\u7ed3\u5408\uff1b2\uff09\u751f\u6210\u62bd\u8c61\uff08\u7efc\u5408\u63cf\u8ff0\uff09\u548c\u62bd\u53d6\uff08\u4ee3\u8868\u6027\u63a8\u6587\uff09\u7528\u6237\u753b\u50cf\u3002", "result": "\u5728\u6ce2\u65af\u653f\u6cbbTwitter\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6700\u5148\u8fdb\u7684LLM\u548c\u4f20\u7edf\u65b9\u6cd5\u9ad8\u51fa9.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u7075\u6d3b\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u7528\u6237\u753b\u50cf\uff0c\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.05573", "pdf": "https://arxiv.org/pdf/2505.05573", "abs": "https://arxiv.org/abs/2505.05573", "authors": ["Mikhail Chaichuk", "Sushant Gautam", "Steven Hicks", "Elena Tutubalina"], "title": "Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.4.8; J.3"], "comment": "code available at\n  https://github.com/THunderCondOR/ImageCLEFmed-MEDVQA-GI-2024-MMCP-Team", "summary": "The generation of realistic medical images from text descriptions has\nsignificant potential to address data scarcity challenges in healthcare AI\nwhile preserving patient privacy. This paper presents a comprehensive study of\ntext-to-image synthesis in the medical domain, comparing two distinct\napproaches: (1) fine-tuning large pre-trained latent diffusion models and (2)\ntraining small, domain-specific models. We introduce a novel model named MSDM,\nan optimized architecture based on Stable Diffusion that integrates a clinical\ntext encoder, variational autoencoder, and cross-attention mechanisms to better\nalign medical text prompts with generated images. Our study compares two\napproaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus\ntraining compact domain-specific models (MSDM). Evaluation across colonoscopy\n(MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models\nachieve higher fidelity, our optimized MSDM delivers comparable quality with\nlower computational costs. Quantitative metrics and qualitative evaluations by\nmedical experts reveal strengths and limitations of each approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u751f\u6210\u533b\u5b66\u56fe\u50cf\u7684\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff1a\u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6a21\u578b\u8bad\u7ec3\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684MSDM\u6a21\u578b\uff0c\u7ed3\u679c\u8868\u660e\u5927\u578b\u6a21\u578b\u4fdd\u771f\u5ea6\u9ad8\uff0c\u4f46MSDM\u6a21\u578b\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u751f\u6210\u903c\u771f\u7684\u533b\u5b66\u56fe\u50cf\u3002", "method": "1. \u5fae\u8c03\u5927\u578b\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08\u5982FLUX\u3001Kandinsky\uff09\u30022. \u8bad\u7ec3\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08MSDM\uff09\uff0c\u6574\u5408\u4e34\u5e8a\u6587\u672c\u7f16\u7801\u5668\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002\u5728\u7ed3\u80a0\u955c\uff08MedVQA-GI\uff09\u548c\u653e\u5c04\u5b66\uff08ROCOv2\uff09\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u5927\u578b\u6a21\u578b\u751f\u6210\u56fe\u50cf\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff0c\u4f46MSDM\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6bd4\u7684\u8d28\u91cf\uff0c\u4e13\u5bb6\u8bc4\u4f30\u63ed\u793a\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "MSDM\u662f\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u5e73\u8861\u4e86\u8d28\u91cf\u4e0e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2505.05877", "pdf": "https://arxiv.org/pdf/2505.05877", "abs": "https://arxiv.org/abs/2505.05877", "authors": ["Rong Yin", "Ruyue Liu", "Xiaoshuai Hao", "Xingrui Zhou", "Yong Liu", "Can Ma", "Weiping Wang"], "title": "Multi-Modal Molecular Representation Learning via Structure Awareness", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by IEEE Transactions on Image Processing (TIP) 2025", "summary": "Accurate extraction of molecular representations is a critical step in the\ndrug discovery process. In recent years, significant progress has been made in\nmolecular representation learning methods, among which multi-modal molecular\nrepresentation methods based on images, and 2D/3D topologies have become\nincreasingly mainstream. However, existing these multi-modal approaches often\ndirectly fuse information from different modalities, overlooking the potential\nof intermodal interactions and failing to adequately capture the complex\nhigher-order relationships and invariant features between molecules. To\novercome these challenges, we propose a structure-awareness-based multi-modal\nself-supervised molecular representation pre-training framework (MMSA) designed\nto enhance molecular graph representations by leveraging invariant knowledge\nbetween molecules. The framework consists of two main modules: the multi-modal\nmolecular representation learning module and the structure-awareness module.\nThe multi-modal molecular representation learning module collaboratively\nprocesses information from different modalities of the same molecule to\novercome intermodal differences and generate a unified molecular embedding.\nSubsequently, the structure-awareness module enhances the molecular\nrepresentation by constructing a hypergraph structure to model higher-order\ncorrelations between molecules. This module also introduces a memory mechanism\nfor storing typical molecular representations, aligning them with memory\nanchors in the memory bank to integrate invariant knowledge, thereby improving\nthe model generalization ability. Extensive experiments have demonstrated the\neffectiveness of MMSA, which achieves state-of-the-art performance on the\nMoleculeNet benchmark, with average ROC-AUC improvements ranging from 1.8% to\n9.6% over baseline methods.", "AI": {"tldr": "MMSA\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u7ed3\u6784\u611f\u77e5\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u5206\u5b50\u8868\u793a\u5b66\u4e60\uff0c\u5728MoleculeNet\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u65b9\u6cd5\u76f4\u63a5\u878d\u5408\u6a21\u6001\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u6a21\u6001\u95f4\u4ea4\u4e92\u548c\u9ad8\u9636\u5173\u7cfb\u53ca\u4e0d\u53d8\u7279\u5f81\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faMMSA\u6846\u67b6\uff0c\u5305\u542b\u591a\u6a21\u6001\u5206\u5b50\u8868\u793a\u5b66\u4e60\u6a21\u5757\u548c\u7ed3\u6784\u611f\u77e5\u6a21\u5757\uff0c\u5229\u7528\u8d85\u56fe\u7ed3\u6784\u5efa\u6a21\u5206\u5b50\u95f4\u9ad8\u9636\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u8bb0\u5fc6\u673a\u5236\u5b58\u50a8\u4e0d\u53d8\u77e5\u8bc6\u3002", "result": "\u5728MoleculeNet\u57fa\u51c6\u4e0a\uff0cMMSA\u5e73\u5747ROC-AUC\u63d0\u53471.8%\u81f39.6%\u3002", "conclusion": "MMSA\u901a\u8fc7\u6a21\u6001\u534f\u540c\u548c\u7ed3\u6784\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u8868\u793a\u8d28\u91cf\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2505.05916", "pdf": "https://arxiv.org/pdf/2505.05916", "abs": "https://arxiv.org/abs/2505.05916", "authors": ["Yifan Zhou", "Yibo Wang", "Chao Shang"], "title": "IRNN: Innovation-driven Recurrent Neural Network for Time-Series Data Modeling and Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many real-world datasets are time series that are sequentially collected and\ncontain rich temporal information. Thus, a common interest in practice is to\ncapture dynamics of time series and predict their future evolutions. To this\nend, the recurrent neural network (RNN) has been a prevalent and effective\nmachine learning option, which admits a nonlinear state-space model\nrepresentation. Motivated by the resemblance between RNN and Kalman filter (KF)\nfor linear state-space models, we propose in this paper Innovation-driven RNN\n(IRNN), a novel RNN architecture tailored to time-series data modeling and\nprediction tasks. By adapting the concept of \"innovation\" from KF to RNN, past\nprediction errors are adopted as additional input signals to update hidden\nstates of RNN and boost prediction performance. Since innovation data depend on\nnetwork parameters, existing training algorithms for RNN do not apply to IRNN\nstraightforwardly. Thus, a tailored training algorithm dubbed input\nupdating-based back-propagation through time (IU-BPTT) is further proposed,\nwhich alternates between updating innovations and optimizing network parameters\nvia gradient descent. Experiments on real-world benchmark datasets show that\nthe integration of innovations into various forms of RNN leads to remarkably\nimproved prediction accuracy of IRNN without increasing the training cost\nsubstantially.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u521b\u65b0\u9a71\u52a8RNN\uff08IRNN\uff09\u7684\u65b0\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\u7684\u201c\u521b\u65b0\u201d\u6982\u5ff5\uff0c\u5c06\u9884\u6d4b\u8bef\u5dee\u4f5c\u4e3a\u9644\u52a0\u8f93\u5165\u4fe1\u53f7\u6765\u63d0\u5347RNN\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5bcc\u542b\u65f6\u5e8f\u4fe1\u606f\uff0c\u4f46\u73b0\u6709RNN\u5728\u6355\u6349\u52a8\u6001\u548c\u9884\u6d4b\u672a\u6765\u6f14\u53d8\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u8bba\u6587\u53d7RNN\u4e0e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\u76f8\u4f3c\u6027\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5229\u7528\u201c\u521b\u65b0\u201d\u6982\u5ff5\u4f18\u5316RNN\u3002", "method": "\u63d0\u51faIRNN\u67b6\u6784\uff0c\u5c06\u5386\u53f2\u9884\u6d4b\u8bef\u5dee\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\u66f4\u65b0\u9690\u85cf\u72b6\u6001\uff1b\u8bbe\u8ba1\u4e13\u95e8\u8bad\u7ec3\u7b97\u6cd5IU-BPTT\uff0c\u4ea4\u66ff\u66f4\u65b0\u201c\u521b\u65b0\u201d\u6570\u636e\u548c\u7f51\u7edc\u53c2\u6570\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cIRNN\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u672a\u5927\u5e45\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "IRNN\u901a\u8fc7\u6574\u5408\u201c\u521b\u65b0\u201d\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86RNN\u7684\u65f6\u5e8f\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05588", "pdf": "https://arxiv.org/pdf/2505.05588", "abs": "https://arxiv.org/abs/2505.05588", "authors": ["Somrita Banerjee", "Abhishek Cauligi", "Marco Pavone"], "title": "Flight Validation of Learning-Based Trajectory Optimization for the Astrobee Free-Flyer", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Submitted to RSS 2025 Workshop on Space Robotics", "summary": "Although widely used in commercial and industrial robotics, trajectory\noptimization has seen limited use in space applications due to its high\ncomputational demands. In this work, we present flight results from experiments\nwith the Astrobee free-flying robot on board the International Space Station\n(ISS), that demonstrate how machine learning can accelerate on-board trajectory\noptimization while preserving theoretical solver guarantees. To the best of the\nauthors' knowledge, this is the first-ever demonstration of learning-based\ncontrol on the ISS. Our approach leverages the GuSTO sequential convex\nprogramming framework and uses a neural network, trained offline, to map\nproblem parameters to effective initial ``warm-start'' trajectories, paving the\nway for faster real-time optimization on resource-constrained space platforms.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u52a0\u901f\u592a\u7a7a\u673a\u5668\u4eba\u8f68\u8ff9\u4f18\u5316\uff0c\u9996\u6b21\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u9a8c\u8bc1\u5b66\u4e60\u578b\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u592a\u7a7a\u5e94\u7528\u4e2d\u8f68\u8ff9\u4f18\u5316\u8ba1\u7b97\u91cf\u5927\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u5b9e\u65f6\u4f18\u5316\u6548\u7387\u3002", "method": "\u7ed3\u5408GuSTO\u5e8f\u5217\u51f8\u7f16\u7a0b\u6846\u67b6\u548c\u79bb\u7ebf\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u751f\u6210\u521d\u59cb\u8f68\u8ff9\u4ee5\u52a0\u901f\u4f18\u5316\u3002", "result": "\u6210\u529f\u5728\u56fd\u9645\u7a7a\u95f4\u7ad9\u4f7f\u7528Astrobee\u98de\u884c\u673a\u5668\u4eba\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u592a\u7a7a\u5e73\u53f0\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u5b9e\u65f6\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05926", "pdf": "https://arxiv.org/pdf/2505.05926", "abs": "https://arxiv.org/abs/2505.05926", "authors": ["Milad Khademi Nori", "Il-Min Kim", "Guanghui Wang"], "title": "Autoencoder-Based Hybrid Replay for Class-Incremental Learning", "categories": ["cs.LG"], "comment": "Accepted ICML 2025", "summary": "In class-incremental learning (CIL), effective incremental learning\nstrategies are essential to mitigate task confusion and catastrophic\nforgetting, especially as the number of tasks $t$ increases. Current exemplar\nreplay strategies impose $\\mathcal{O}(t)$ memory/compute complexities. We\npropose an autoencoder-based hybrid replay (AHR) strategy that leverages our\nnew hybrid autoencoder (HAE) to function as a compressor to alleviate the\nrequirement for large memory, achieving $\\mathcal{O}(0.1 t)$ at the worst case\nwith the computing complexity of $\\mathcal{O}(t)$ while accomplishing\nstate-of-the-art performance. The decoder later recovers the exemplar data\nstored in the latent space, rather than in raw format. Additionally, HAE is\ndesigned for both discriminative and generative modeling, enabling\nclassification and replay capabilities, respectively. HAE adopts the charged\nparticle system energy minimization equations and repulsive force algorithm for\nthe incremental embedding and distribution of new class centroids in its latent\nspace. Our results demonstrate that AHR consistently outperforms recent\nbaselines across multiple benchmarks while operating with the same\nmemory/compute budgets. The source code is included in the supplementary\nmaterial and will be open-sourced upon publication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6df7\u5408\u56de\u653e\u7b56\u7565\uff08AHR\uff09\uff0c\u901a\u8fc7\u65b0\u578b\u6df7\u5408\u81ea\u52a8\u7f16\u7801\u5668\uff08HAE\uff09\u4f5c\u4e3a\u538b\u7f29\u5668\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u5728\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\uff08CIL\uff09\u4e2d\u51cf\u5c11\u4efb\u52a1\u6df7\u6dc6\u548c\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5c24\u5176\u662f\u5728\u4efb\u52a1\u6570\u91cf\u589e\u52a0\u65f6\uff0c\u9700\u8981\u6709\u6548\u7b56\u7565\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u5f15\u5165HAE\u4f5c\u4e3a\u538b\u7f29\u5668\uff0c\u7ed3\u5408\u7c92\u5b50\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u65b9\u7a0b\u548c\u6392\u65a5\u529b\u7b97\u6cd5\uff0c\u5b9e\u73b0\u589e\u91cf\u5d4c\u5165\u548c\u65b0\u7c7b\u4e2d\u5fc3\u7684\u5206\u5e03\uff0c\u540c\u65f6\u7528\u4e8e\u5206\u7c7b\u548c\u56de\u653e\u751f\u6210\u3002", "result": "AHR\u5728\u76f8\u540c\u5185\u5b58/\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5185\u5b58\u9700\u6c42\u7684\u663e\u8457\u4f18\u5316\uff08\u6700\u574f\u60c5\u51b5\u4e0b\u964d\u81f3$\\mathcal{O}(0.1 t)$\uff09\u3002", "conclusion": "AHR\u901a\u8fc7\u7ed3\u5408HAE\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u4f4e\u5185\u5b58\u9700\u6c42\u7684\u5e73\u8861\uff0c\u4e3aCIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05589", "pdf": "https://arxiv.org/pdf/2505.05589", "abs": "https://arxiv.org/abs/2505.05589", "authors": ["Jingzhong Lin", "Yuanyuan Qi", "Xinru Li", "Wenxuan Huang", "Xiangfeng Xu", "Bangyan Li", "Xuejiao Wang", "Gaoqi He"], "title": "ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Reactive dance generation (RDG) produces follower movements conditioned on\nguiding dancer and music while ensuring spatial coordination and temporal\ncoherence. However, existing methods overemphasize global constraints and\noptimization, overlooking local information, such as fine-grained spatial\ninteractions and localized temporal context. Therefore, we present ReactDance,\na novel diffusion-based framework for high-fidelity RDG with long-term\ncoherence and multi-scale controllability. Unlike existing methods that\nstruggle with interaction fidelity, synchronization, and temporal consistency\nin duet synthesis, our approach introduces two key innovations: 1)Group\nResidual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion\nrepresentation that captures interaction semantics from coarse body rhythms to\nfine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling\nstrategy eliminating error accumulation in long sequence generation via local\nblock causal masking and periodic positional encoding. Built on the decoupled\nmulti-scale GRFSQ representation, we implement a diffusion model\nwithLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control\nover motion semantics across scales. Extensive experiments on standard\nbenchmarks demonstrate that ReactDance surpasses existing methods, achieving\nstate-of-the-art performance.", "AI": {"tldr": "ReactDance\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u7684\u53cd\u5e94\u5f0f\u821e\u8e48\u751f\u6210\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89e3\u8026\u8fd0\u52a8\u8868\u793a\u548c\u5c40\u90e8\u5757\u4e0a\u4e0b\u6587\u7684\u91c7\u6837\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4ea4\u4e92\u4fdd\u771f\u5ea6\u3001\u540c\u6b65\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u5f3a\u8c03\u5168\u5c40\u7ea6\u675f\u548c\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u4fe1\u606f\uff08\u5982\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u4ea4\u4e92\u548c\u5c40\u90e8\u65f6\u95f4\u4e0a\u4e0b\u6587\uff09\uff0c\u5bfc\u81f4\u4ea4\u4e92\u4fdd\u771f\u5ea6\u3001\u540c\u6b65\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86Group Residual Finite Scalar Quantization\uff08GRFSQ\uff09\u591a\u5c3a\u5ea6\u89e3\u8026\u8fd0\u52a8\u8868\u793a\u548cBlockwise Local Context\uff08BLC\uff09\u91c7\u6837\u7b56\u7565\uff0c\u7ed3\u5408Layer-Decoupled Classifier-free Guidance\uff08LDCFG\uff09\u5b9e\u73b0\u591a\u5c3a\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReactDance\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ReactDance\u901a\u8fc7\u591a\u5c3a\u5ea6\u8868\u793a\u548c\u5c40\u90e8\u4e0a\u4e0b\u6587\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u5e94\u5f0f\u821e\u8e48\u751f\u6210\u7684\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u821e\u8e48\u5408\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05950", "pdf": "https://arxiv.org/pdf/2505.05950", "abs": "https://arxiv.org/abs/2505.05950", "authors": ["Yuxin Zhou", "Zheng Li", "Jun Zhang", "Jue Wang", "Yiping Wang", "Zhongle Xie", "Ke Chen", "Lidan Shou"], "title": "FloE: On-the-Fly MoE Inference", "categories": ["cs.LG"], "comment": "Accepted at ICML 2025", "summary": "With the widespread adoption of Mixture-of-Experts (MoE) models, there is a\ngrowing demand for efficient inference on memory-constrained devices. While\noffloading expert parameters to CPU memory and loading activated experts on\ndemand has emerged as a potential solution, the large size of activated experts\noverburdens the limited PCIe bandwidth, hindering the effectiveness in\nlatency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly\nMoE inference system on memory-constrained GPUs. FloE is built on the insight\nthat there exists substantial untapped redundancy within sparsely activated\nexperts. It employs various compression techniques on the expert's internal\nparameter matrices to reduce the data movement load, combined with low-cost\nsparse prediction, achieving perceptible inference acceleration in wall-clock\ntime on resource-constrained devices. Empirically, FloE achieves a 9.3x\ncompression of parameters per expert in Mixtral-8x7B; enables deployment on a\nGPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and\ndelivers a 48.7x inference speedup compared to DeepSpeed-MII on a single\nGeForce RTX 3090.", "AI": {"tldr": "FloE\u662f\u4e00\u4e2a\u9488\u5bf9\u5185\u5b58\u53d7\u9650GPU\u7684MoE\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u538b\u7f29\u6fc0\u6d3b\u4e13\u5bb6\u53c2\u6570\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u8d1f\u62c5\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u968f\u7740MoE\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u63a8\u7406\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0PCIe\u5e26\u5bbd\u9650\u5236\u65e0\u6cd5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u573a\u666f\u3002", "method": "\u5229\u7528\u7a00\u758f\u6fc0\u6d3b\u4e13\u5bb6\u5185\u90e8\u7684\u5197\u4f59\u6027\uff0c\u91c7\u7528\u53c2\u6570\u77e9\u9635\u538b\u7f29\u6280\u672f\u548c\u4f4e\u6210\u672c\u7a00\u758f\u9884\u6d4b\uff0c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u8d1f\u8f7d\u3002", "result": "\u5728Mixtral-8x7B\u4e0a\u5b9e\u73b0\u6bcf\u4e13\u5bb6\u53c2\u65709.3\u500d\u538b\u7f29\uff0c\u5185\u5b58\u5360\u7528\u964d\u4f4e8.5\u500d\uff0811GB VRAM\uff09\uff0c\u5355\u5361\u63a8\u7406\u901f\u5ea6\u63d0\u534748.7\u500d\u3002", "conclusion": "FloE\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u96be\u9898\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05595", "pdf": "https://arxiv.org/pdf/2505.05595", "abs": "https://arxiv.org/abs/2505.05595", "authors": ["Wenhao Guo", "Yuda Wang", "Zeqiao Huang", "Changjiang Zhang", "Shumin ma"], "title": "Trading Under Uncertainty: A Distribution-Based Strategy for Futures Markets Using FutureQuant Transformer", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "In the complex landscape of traditional futures trading, where vast data and\nvariables like real-time Limit Order Books (LOB) complicate price predictions,\nwe introduce the FutureQuant Transformer model, leveraging attention mechanisms\nto navigate these challenges. Unlike conventional models focused on point\npredictions, the FutureQuant model excels in forecasting the range and\nvolatility of future prices, thus offering richer insights for trading\nstrategies. Its ability to parse and learn from intricate market patterns\nallows for enhanced decision-making, significantly improving risk management\nand achieving a notable average gain of 0.1193% per 30-minute trade over\nstate-of-the-art models with a simple algorithm using factors such as RSI, ATR,\nand Bollinger Bands. This innovation marks a substantial leap forward in\npredictive analytics within the volatile domain of futures trading.", "AI": {"tldr": "FutureQuant Transformer\u6a21\u578b\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u671f\u8d27\u4ef7\u683c\u8303\u56f4\u548c\u6ce2\u52a8\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u70b9\u9884\u6d4b\u6a21\u578b\uff0c\u5e73\u5747\u6bcf30\u5206\u949f\u4ea4\u6613\u6536\u76ca0.1193%\u3002", "motivation": "\u4f20\u7edf\u671f\u8d27\u4ea4\u6613\u4e2d\u590d\u6742\u7684\u5b9e\u65f6\u6570\u636e\u548c\u53d8\u91cf\uff08\u5982\u9650\u4ef7\u8ba2\u5355\u7c3f\uff09\u4f7f\u4ef7\u683c\u9884\u6d4b\u56f0\u96be\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u63d0\u4f9b\u4e30\u5bcc\u4ea4\u6613\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u7684FutureQuant Transformer\u6a21\u578b\uff0c\u7ed3\u5408RSI\u3001ATR\u548c\u5e03\u6797\u5e26\u7b49\u56e0\u5b50\uff0c\u9884\u6d4b\u4ef7\u683c\u8303\u56f4\u53ca\u6ce2\u52a8\u6027\u3002", "result": "\u6a21\u578b\u663e\u8457\u63d0\u5347\u98ce\u9669\u7ba1\u7406\u80fd\u529b\uff0c\u5e73\u5747\u6bcf30\u5206\u949f\u4ea4\u6613\u6536\u76ca0.1193%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "FutureQuant Transformer\u4e3a\u671f\u8d27\u4ea4\u6613\u7684\u9884\u6d4b\u5206\u6790\u5e26\u6765\u91cd\u8981\u7a81\u7834\uff0c\u5c24\u5176\u5728\u6ce2\u52a8\u6027\u9ad8\u7684\u5e02\u573a\u4e2d\u8868\u73b0\u5353\u8d8a\u3002"}}
{"id": "2505.05967", "pdf": "https://arxiv.org/pdf/2505.05967", "abs": "https://arxiv.org/abs/2505.05967", "authors": ["Uyoata E. Uyoata", "Gilberto Berardinelli", "Ramoni Adeogun"], "title": "Learning Power Control Protocol for In-Factory 6G Subnetworks", "categories": ["cs.LG", "cs.NI", "eess.SP"], "comment": "Accepted for presented at IEEE EuCNC & 6G Summit 2025", "summary": "In-X Subnetworks are envisioned to meet the stringent demands of short-range\ncommunication in diverse 6G use cases. In the context of In-Factory scenarios,\neffective power control is critical to mitigating the impact of interference\nresulting from potentially high subnetwork density. Existing approaches to\npower control in this domain have predominantly emphasized the data plane,\noften overlooking the impact of signaling overhead. Furthermore, prior work has\ntypically adopted a network-centric perspective, relying on the assumption of\ncomplete and up-to-date channel state information (CSI) being readily available\nat the central controller. This paper introduces a novel multi-agent\nreinforcement learning (MARL) framework designed to enable access points to\nautonomously learn both signaling and power control protocols in an In-Factory\nSubnetwork environment. By formulating the problem as a partially observable\nMarkov decision process (POMDP) and leveraging multi-agent proximal policy\noptimization (MAPPO), the proposed approach achieves significant advantages.\nThe simulation results demonstrate that the learning-based method reduces\nsignaling overhead by a factor of 8 while maintaining a buffer flush rate that\nlags the ideal \"Genie\" approach by only 5%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u57286G\u5de5\u5382\u5b50\u7f51\u7edc\u4e2d\u81ea\u4e3b\u4f18\u5316\u4fe1\u53f7\u548c\u529f\u7387\u63a7\u5236\uff0c\u51cf\u5c11\u4fe1\u53f7\u5f00\u9500\u5e76\u4fdd\u6301\u63a5\u8fd1\u7406\u60f3\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u529f\u7387\u63a7\u5236\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u5e73\u9762\uff0c\u5ffd\u89c6\u4e86\u4fe1\u53f7\u5f00\u9500\u7684\u5f71\u54cd\uff0c\u4e14\u4f9d\u8d56\u7f51\u7edc\u4e2d\u5fc3\u7684\u5047\u8bbe\uff08\u5982\u5b8c\u6574\u4e14\u6700\u65b0\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u5e76\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08MAPPO\uff09\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u4fe1\u53f7\u5f00\u9500\u964d\u4f4e\u4e868\u500d\uff0c\u540c\u65f6\u7f13\u51b2\u5237\u65b0\u7387\u4ec5\u6bd4\u7406\u60f3\u6027\u80fd\u4f4e5%\u3002", "conclusion": "\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u5de5\u5382\u5b50\u7f51\u7edc\u4e2d\u80fd\u6709\u6548\u5e73\u8861\u4fe1\u53f7\u5f00\u9500\u4e0e\u6027\u80fd\uff0c\u4e3a6G\u77ed\u8ddd\u79bb\u901a\u4fe1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05599", "pdf": "https://arxiv.org/pdf/2505.05599", "abs": "https://arxiv.org/abs/2505.05599", "authors": ["Seraj Al Mahmud Mostafa", "Chenxi Wang", "Jia Yue", "Yuta Hozumi", "Jianwu Wang"], "title": "Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to International conference on Advanced\n  Machine Learning and Data Science (AMLDS) 2025", "summary": "Object localization in satellite imagery is particularly challenging due to\nthe high variability of objects, low spatial resolution, and interference from\nnoise and dominant features such as clouds and city lights. In this research,\nwe focus on three satellite datasets: upper atmospheric Gravity Waves (GW),\nmesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique\nchallenges. These challenges include the variability in the scale and\nappearance of the main object patterns, where the size, shape, and feature\nextent of objects of interest can differ significantly. To address these\nchallenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed\nto improve object localization in these complex scenarios. YOLO-DCAP\nincorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture\nmulti-scale features at scale with varying dilation rates, and an\nAttention-aided Spatial Pooling (AaSP) module to focus on the global relevant\nspatial regions, enhancing feature selection. These structural improvements\nhelp to better localize objects in satellite imagery. Experimental results\ndemonstrate that YOLO-DCAP significantly outperforms both the YOLO base model\nand state-of-the-art approaches, achieving an average improvement of 20.95% in\nmAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively\nover state-of-the-art alternatives, consistently across all three satellite\ndatasets. These consistent gains across all three satellite datasets highlight\nthe robustness and generalizability of the proposed approach. Our code is open\nsourced at\nhttps://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faYOLO-DCAP\u6a21\u578b\uff0c\u9488\u5bf9\u536b\u661f\u56fe\u50cf\u4e2d\u7269\u4f53\u5b9a\u4f4d\u7684\u9ad8\u53d8\u5f02\u6027\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u5e72\u6270\u7b49\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6269\u5f20\u6b8b\u5dee\u5377\u79ef\u548c\u6ce8\u610f\u529b\u8f85\u52a9\u7a7a\u95f4\u6c60\u5316\u6a21\u5757\u6539\u8fdbYOLOv5\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u4e2d\u7269\u4f53\u5b9a\u4f4d\u56e0\u9ad8\u53d8\u5f02\u6027\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u5e72\u6270\uff08\u5982\u4e91\u5c42\u3001\u57ce\u5e02\u706f\u5149\uff09\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u9488\u5bf9\u4e09\u4e2a\u7279\u5b9a\u6570\u636e\u96c6\uff08\u91cd\u529b\u6ce2\u3001\u4e2d\u5c42\u9876\u69fd\u548c\u6d77\u6d0b\u6da1\u65cb\uff09\u3002", "method": "YOLO-DCAP\u7ed3\u5408\u591a\u5c3a\u5ea6\u6269\u5f20\u6b8b\u5dee\u5377\u79ef\uff08MDRC\uff09\u6355\u6349\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u8f85\u52a9\u7a7a\u95f4\u6c60\u5316\uff08AaSP\uff09\u6a21\u5757\u805a\u7126\u5168\u5c40\u76f8\u5173\u533a\u57df\uff0c\u63d0\u5347\u7279\u5f81\u9009\u62e9\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cYOLO-DCAP\u5728mAP50\u548cIoU\u4e0a\u5206\u522b\u5e73\u5747\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534720.95%\u548c32.23%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd57.35%\u548c9.84%\uff0c\u4e14\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "YOLO-DCAP\u901a\u8fc7\u7ed3\u6784\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u56fe\u50cf\u4e2d\u7269\u4f53\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.05968", "pdf": "https://arxiv.org/pdf/2505.05968", "abs": "https://arxiv.org/abs/2505.05968", "authors": ["Dan Qiao", "Wenhao Li", "Shanchao Yang", "Hongyuan Zha", "Baoxiang Wang"], "title": "Offline Multi-agent Reinforcement Learning via Score Decomposition", "categories": ["cs.LG", "cs.MA"], "comment": "Working papers", "summary": "Offline multi-agent reinforcement learning (MARL) faces critical challenges\ndue to distributional shifts, further exacerbated by the high dimensionality of\njoint action spaces and the diversity in coordination strategies and quality\namong agents. Conventional approaches, including independent learning\nframeworks and value decomposition methods based on pessimistic principles,\nremain susceptible to out-of-distribution (OOD) joint actions and often yield\nsuboptimal performance. Through systematic analysis of prevalent offline MARL\nbenchmarks, we identify that this limitation primarily stems from the\ninherently multimodal nature of joint collaborative policies induced by offline\ndata collection. To address these challenges, we propose a novel two-stage\nframework: First, we employ a diffusion-based generative model to explicitly\ncapture the complex behavior policy, enabling accurate modeling of diverse\nmulti-agent coordination patterns. Second, we introduce a sequential score\nfunction decomposition mechanism to regularize individual policies and enable\ndecentralized execution. Extensive experiments on continuous control tasks\ndemonstrate state-of-the-art performance across multiple standard offline MARL\nbenchmarks, outperforming existing methods by 26.3\\% in normalized returns. Our\napproach provides new insights into offline coordination and equilibrium\nselection in cooperative multi-agent systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u56e0\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u7684\u9ad8\u7ef4\u6027\u548c\u534f\u8c03\u7b56\u7565\u7684\u591a\u6837\u6027\u9762\u4e34\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9OOD\u52a8\u4f5c\u4e14\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u6269\u6563\u751f\u6210\u6a21\u578b\u663e\u5f0f\u5efa\u6a21\u590d\u6742\u884c\u4e3a\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u987a\u5e8f\u8bc4\u5206\u51fd\u6570\u5206\u89e3\u673a\u5236\u89c4\u8303\u5316\u4e2a\u4f53\u7b56\u7565\uff0c\u5b9e\u73b0\u5206\u6563\u6267\u884c\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5f52\u4e00\u5316\u56de\u62a5\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad826.3%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79bb\u7ebf\u534f\u8c03\u548c\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5747\u8861\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05622", "pdf": "https://arxiv.org/pdf/2505.05622", "abs": "https://arxiv.org/abs/2505.05622", "authors": ["Weichen Zhang", "Chen Gao", "Shiquan Yu", "Ruiying Peng", "Baining Zhao", "Qian Zhang", "Jinqiang Cui", "Xinlei Chen", "Yong Li"], "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Aerial vision-and-language navigation (VLN), requiring drones to interpret\nnatural language instructions and navigate complex urban environments, emerges\nas a critical embodied AI challenge that bridges human-robot interaction, 3D\nspatial reasoning, and real-world deployment. Although existing ground VLN\nagents achieved notable results in indoor and outdoor settings, they struggle\nin aerial VLN due to the absence of predefined navigation graphs and the\nexponentially expanding action space in long-horizon exploration. In this work,\nwe propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent\nthat significantly reduces the navigation complexity for urban aerial VLN.\nSpecifically, we design a hierarchical semantic planning module (HSPM) that\ndecomposes the long-horizon task into sub-goals with different semantic levels.\nThe agent reaches the target progressively by achieving sub-goals with\ndifferent capacities of the LLM. Additionally, a global memory module storing\nhistorical trajectories into a topological graph is developed to simplify\nnavigation for visited targets. Extensive benchmark experiments show that our\nmethod achieves state-of-the-art performance with significant improvement.\nFurther experiments demonstrate the effectiveness of different modules of\nCityNavAgent for aerial VLN in continuous city environments. The code is\navailable at \\href{https://github.com/VinceOuti/CityNavAgent}{link}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCityNavAgent\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u89c4\u5212\u6a21\u5757\u548c\u5168\u5c40\u8bb0\u5fc6\u6a21\u5757\u663e\u8457\u964d\u4f4e\u57ce\u5e02\u7a7a\u4e2d\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5730\u9762\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u667a\u80fd\u4f53\u5728\u7a7a\u4e2d\u5bfc\u822a\u4e2d\u56e0\u7f3a\u4e4f\u9884\u5b9a\u4e49\u5bfc\u822a\u56fe\u548c\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\u800c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u8bed\u4e49\u89c4\u5212\u6a21\u5757\uff08HSPM\uff09\u5c06\u957f\u7a0b\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\uff0c\u5e76\u5229\u7528\u5168\u5c40\u8bb0\u5fc6\u6a21\u5757\u5b58\u50a8\u5386\u53f2\u8f68\u8ff9\u4ee5\u7b80\u5316\u5bfc\u822a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8fde\u7eed\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff0c\u5404\u6a21\u5757\u5747\u6709\u6548\u3002", "conclusion": "CityNavAgent\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u548c\u5168\u5c40\u8bb0\u5fc6\u6210\u529f\u89e3\u51b3\u4e86\u7a7a\u4e2d\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5c55\u73b0\u4e86\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2505.05983", "pdf": "https://arxiv.org/pdf/2505.05983", "abs": "https://arxiv.org/abs/2505.05983", "authors": ["Vivek Mohan", "Biyan Zhou", "Zhou Wang", "Anil Bharath", "Emmanuel Drakakis", "Arindam Basu"], "title": "Architectural Exploration of Hybrid Neural Decoders for Neuromorphic Implantable BMI", "categories": ["cs.LG"], "comment": "The paper has been accepted for lecture presentation at the 2025 IEEE\n  International Symposium on Circuits and Systems in London", "summary": "This work presents an efficient decoding pipeline for neuromorphic\nimplantable brain-machine interfaces (Neu-iBMI), leveraging sparse neural event\ndata from an event-based neural sensing scheme. We introduce a tunable event\nfilter (EvFilter), which also functions as a spike detector (EvFilter-SPD),\nsignificantly reducing the number of events processed for decoding by 192X and\n554X, respectively. The proposed pipeline achieves high decoding performance,\nup to R^2=0.73, with ANN- and SNN-based decoders, eliminating the need for\nsignal recovery, spike detection, or sorting, commonly performed in\nconventional iBMI systems. The SNN-Decoder reduces computations and memory\nrequired by 5-23X compared to NN-, and LSTM-Decoders, while the ST-NN-Decoder\ndelivers similar performance to an LSTM-Decoder requiring 2.5X fewer resources.\nThis streamlined approach significantly reduces computational and memory\ndemands, making it ideal for low-power, on-implant, or wearable iBMIs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u62df\u6001\u690d\u5165\u5f0f\u8111\u673a\u63a5\u53e3\u89e3\u7801\u6d41\u7a0b\uff0c\u5229\u7528\u4e8b\u4ef6\u57fa\u795e\u7ecf\u4f20\u611f\u65b9\u6848\u7684\u7a00\u758f\u6570\u636e\uff0c\u901a\u8fc7\u53ef\u8c03\u4e8b\u4ef6\u8fc7\u6ee4\u5668\uff08EvFilter\uff09\u5927\u5e45\u51cf\u5c11\u5904\u7406\u4e8b\u4ef6\u6570\u91cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u89e3\u7801\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u8111\u673a\u63a5\u53e3\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u8fdb\u884c\u4fe1\u53f7\u6062\u590d\u3001\u5c16\u5cf0\u68c0\u6d4b\u6216\u6392\u5e8f\uff0c\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u7801\u6d41\u7a0b\uff0c\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u5408\u4f4e\u529f\u8017\u690d\u5165\u6216\u53ef\u7a7f\u6234\u8bbe\u5907\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8c03\u4e8b\u4ef6\u8fc7\u6ee4\u5668\uff08EvFilter\uff09\uff0c\u540c\u65f6\u4f5c\u4e3a\u5c16\u5cf0\u68c0\u6d4b\u5668\uff08EvFilter-SPD\uff09\uff0c\u5927\u5e45\u51cf\u5c11\u5904\u7406\u4e8b\u4ef6\u6570\u91cf\uff1b\u91c7\u7528ANN\u548cSNN\u89e3\u7801\u5668\uff0c\u907f\u514d\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u6b65\u9aa4\u3002", "result": "EvFilter\u5c06\u5904\u7406\u4e8b\u4ef6\u6570\u91cf\u51cf\u5c11192\u500d\u81f3554\u500d\uff1bSNN\u89e3\u7801\u5668\u6bd4NN\u548cLSTM\u89e3\u7801\u5668\u51cf\u5c115-23\u500d\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0cST-NN\u89e3\u7801\u5668\u6027\u80fd\u63a5\u8fd1LSTM\u4f46\u8d44\u6e90\u9700\u6c42\u51cf\u5c112.5\u500d\u3002\u89e3\u7801\u6027\u80fd\u9ad8\u8fbeR^2=0.73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u690d\u5165\u6216\u53ef\u7a7f\u6234\u8111\u673a\u63a5\u53e3\uff0c\u65e0\u9700\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89e3\u7801\u6027\u80fd\u3002"}}
{"id": "2505.06000", "pdf": "https://arxiv.org/pdf/2505.06000", "abs": "https://arxiv.org/abs/2505.06000", "authors": ["Stephan Bartl", "Kevin Innerebner", "Elisabeth Lex"], "title": "Differentiable Fuzzy Neural Networks for Recommender Systems", "categories": ["cs.LG"], "comment": "Accepted for publication at the HyPer workshop, co-located with ACM\n  UMAP 2025", "summary": "As recommender systems become increasingly complex, transparency is essential\nto increase user trust, accountability, and regulatory compliance.\nNeuro-symbolic approaches that integrate symbolic reasoning with sub-symbolic\nlearning offer a promising approach toward transparent and user-centric\nsystems. In this work-in-progress, we investigate using fuzzy neural networks\n(FNNs) as a neuro-symbolic approach for recommendations that learn logic-based\nrules over predefined, human-readable atoms. Each rule corresponds to a fuzzy\nlogic expression, making the recommender's decision process inherently\ntransparent. In contrast to black-box machine learning methods, our approach\nreveals the reasoning behind a recommendation while maintaining competitive\nperformance. We evaluate our method on a synthetic and MovieLens 1M datasets\nand compare it to state-of-the-art recommendation algorithms. Our results\ndemonstrate that our approach accurately captures user behavior while providing\na transparent decision-making process. Finally, the differentiable nature of\nthis approach facilitates an integration with other neural models, enabling the\ndevelopment of hybrid, transparent recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff08\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\uff09\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u7ade\u4e89\u529b\u3002", "motivation": "\u968f\u7740\u63a8\u8350\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u900f\u660e\u6027\u5bf9\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\u3001\u95ee\u8d23\u548c\u6cd5\u89c4\u9075\u4ece\u81f3\u5173\u91cd\u8981\u3002\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7ed3\u5408\u4e86\u7b26\u53f7\u63a8\u7406\u548c\u5b50\u7b26\u53f7\u5b66\u4e60\uff0c\u4e3a\u5b9e\u73b0\u900f\u660e\u4e14\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\uff08FNNs\uff09\u4f5c\u4e3a\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5b66\u4e60\u57fa\u4e8e\u903b\u8f91\u7684\u89c4\u5219\uff0c\u6bcf\u6761\u89c4\u5219\u5bf9\u5e94\u4e00\u4e2a\u6a21\u7cca\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u900f\u660e\u7684\u63a8\u8350\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548cMovieLens 1M\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u51c6\u786e\u6355\u6349\u7528\u6237\u884c\u4e3a\uff0c\u8fd8\u63d0\u4f9b\u4e86\u900f\u660e\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e14\u6027\u80fd\u4e0e\u73b0\u6709\u63a8\u8350\u7b97\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u79cd\u53ef\u5fae\u5206\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\uff0c\u8fd8\u80fd\u4e0e\u5176\u4ed6\u795e\u7ecf\u6a21\u578b\u96c6\u6210\uff0c\u4e3a\u5f00\u53d1\u6df7\u5408\u578b\u900f\u660e\u63a8\u8350\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.05626", "pdf": "https://arxiv.org/pdf/2505.05626", "abs": "https://arxiv.org/abs/2505.05626", "authors": ["Aarti Ghatkesar", "Uddeshya Upadhyay", "Ganesh Venkatesh"], "title": "Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Achieving deep alignment between vision and language remains a central\nchallenge for Multimodal Large Language Models (MLLMs). These models often fail\nto fully leverage visual input, defaulting to strong language priors. Our\napproach first provides insights into how MLLMs internally build visual\nunderstanding of image regions and then introduces techniques to amplify this\ncapability. Specifically, we explore techniques designed both to deepen the\nmodel's understanding of visual content and to ensure that these visual\ninsights actively guide language generation. We demonstrate the superior\nmultimodal understanding of our resultant model through a detailed upstream\nanalysis quantifying its ability to predict visually-dependent tokens as well\nas 10 pt boost on visually challenging tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u89c6\u89c9\u4e0e\u8bed\u8a00\u6df1\u5ea6\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u5176\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u5e76\u786e\u4fdd\u89c6\u89c9\u4fe1\u606f\u4e3b\u5bfc\u8bed\u8a00\u751f\u6210\uff0c\u6700\u7ec8\u5728\u89c6\u89c9\u4f9d\u8d56\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8610\u5206\u7684\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5b58\u5728\u89c6\u89c9\u8f93\u5165\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f80\u5f80\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u5e76\u786e\u4fdd\u89c6\u89c9\u4fe1\u606f\u4e3b\u5bfc\u8bed\u8a00\u751f\u6210\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5206\u6790\u4e86MLLMs\u5185\u90e8\u5982\u4f55\u6784\u5efa\u5bf9\u56fe\u50cf\u533a\u57df\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u5e76\u5f15\u5165\u6280\u672f\u624b\u6bb5\u6765\u589e\u5f3a\u8fd9\u79cd\u80fd\u529b\u3002\u8fd9\u4e9b\u6280\u672f\u65e8\u5728\u52a0\u6df1\u6a21\u578b\u5bf9\u89c6\u89c9\u5185\u5bb9\u7684\u7406\u89e3\uff0c\u5e76\u786e\u4fdd\u89c6\u89c9\u4fe1\u606f\u80fd\u6709\u6548\u6307\u5bfc\u8bed\u8a00\u751f\u6210\u3002\u901a\u8fc7\u4e0a\u6e38\u5206\u6790\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u89c6\u89c9\u4f9d\u8d56\u6807\u8bb0\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u4f9d\u8d56\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8610\u5206\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u6a21\u6001\u7406\u89e3\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86MLLMs\u5bf9\u89c6\u89c9\u4fe1\u606f\u7684\u7406\u89e3\u548c\u5229\u7528\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2505.06017", "pdf": "https://arxiv.org/pdf/2505.06017", "abs": "https://arxiv.org/abs/2505.06017", "authors": ["Hiroki Shiraishi", "Yohei Hayamizu", "Tomonori Hashiyama"], "title": "Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in Michigan-Style Learning Fuzzy-Classifier Systems", "categories": ["cs.LG"], "comment": "Accepted by the ACM Genetic and Evolutionary Computation Conference\n  (GECCO) 2023", "summary": "This paper focuses on the impact of rule representation in Michigan-style\nLearning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A\nwell-representation of the rules in an LFCS is crucial for improving its\nperformance. However, conventional rule representations frequently need help\naddressing problems with unknown data characteristics. To address this issue,\nthis paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive\nrule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates\na fuzzy indicator as a new rule parameter that sets the membership function of\na rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes.\nThe fuzzy indicator is optimized with evolutionary operators, allowing the\nsystem to search for an optimal rule representation. Results from extensive\nexperiments conducted on continuous space problems demonstrate that\nAdaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular\nand fuzzy-hypertrapezoidal rule representations in classification accuracy.\nAdditionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and\nreal-world problems with inherent uncertainty, such as missing values, leading\nto stable classification performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdaptive-UCS\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u7cca\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89c4\u5219\u8868\u793a\u673a\u5236\u4f18\u5316\u5206\u7c7b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u89c4\u5219\u8868\u793a\u5728\u672a\u77e5\u6570\u636e\u7279\u6027\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u673a\u5236\u6765\u63d0\u5347\u5206\u7c7b\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faAdaptive-UCS\uff0c\u5f15\u5165\u6a21\u7cca\u6307\u793a\u5668\u4f5c\u4e3a\u65b0\u89c4\u5219\u53c2\u6570\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u5b50\u4f18\u5316\u89c4\u5219\u8868\u793a\uff0c\u52a8\u6001\u9009\u62e9\u77e9\u5f62\u6216\u4e09\u89d2\u5f62\u96b6\u5c5e\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAdaptive-UCS\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u5728\u566a\u58f0\u8f93\u5165\u548c\u73b0\u5b9e\u95ee\u9898\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "Adaptive-UCS\u901a\u8fc7\u81ea\u9002\u5e94\u89c4\u5219\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4e14\u4e0d\u786e\u5b9a\u7684\u6570\u636e\u73af\u5883\u3002"}}
{"id": "2505.05638", "pdf": "https://arxiv.org/pdf/2505.05638", "abs": "https://arxiv.org/abs/2505.05638", "authors": ["Mohamed-Khalil Bouzidi", "Christian Schlauch", "Nicole Scheuerer", "Yue Yao", "Nadja Klein", "Daniel G\u00f6hring", "J\u00f6rg Reichardt"], "title": "Closing the Loop: Motion Prediction Models beyond Open-Loop Benchmarks", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Fueled by motion prediction competitions and benchmarks, recent years have\nseen the emergence of increasingly large learning based prediction models, many\nwith millions of parameters, focused on improving open-loop prediction accuracy\nby mere centimeters. However, these benchmarks fail to assess whether such\nimprovements translate to better performance when integrated into an autonomous\ndriving stack. In this work, we systematically evaluate the interplay between\nstate-of-the-art motion predictors and motion planners. Our results show that\nhigher open-loop accuracy does not always correlate with better closed-loop\ndriving behavior and that other factors, such as temporal consistency of\npredictions and planner compatibility, also play a critical role. Furthermore,\nwe investigate downsized variants of these models, and, surprisingly, find that\nin some cases models with up to 86% fewer parameters yield comparable or even\nsuperior closed-loop driving performance. Our code is available at\nhttps://github.com/continental/pred2plan.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\uff0c\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u5f00\u653e\u5f0f\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u5e76\u4e0d\u603b\u662f\u8f6c\u5316\u4e3a\u95ed\u73af\u9a7e\u9a76\u884c\u4e3a\u7684\u6539\u8fdb\uff0c\u4e14\u5c0f\u578b\u5316\u6a21\u578b\u6709\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u9884\u6d4b\u7ade\u8d5b\u548c\u57fa\u51c6\u8fc7\u4e8e\u5173\u6ce8\u5f00\u653e\u5f0f\u9884\u6d4b\u7684\u5fae\u5c0f\u7cbe\u5ea6\u63d0\u5347\uff0c\u800c\u5ffd\u89c6\u4e86\u5176\u5bf9\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u95ed\u73af\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\u4e0e\u8fd0\u52a8\u89c4\u5212\u5668\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u5e76\u6d4b\u8bd5\u4e86\u53c2\u6570\u51cf\u5c11\u7684\u6a21\u578b\u53d8\u4f53\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f00\u653e\u5f0f\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u95ed\u73af\u9a7e\u9a76\u884c\u4e3a\u65e0\u76f4\u63a5\u5173\u8054\uff0c\u4e14\u90e8\u5206\u5c0f\u578b\u5316\u6a21\u578b\uff08\u53c2\u6570\u51cf\u5c1186%\uff09\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u8bbe\u8ba1\u9884\u6d4b\u6a21\u578b\u65f6\u9700\u8003\u8651\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c4\u5212\u5668\u517c\u5bb9\u6027\uff0c\u800c\u975e\u4ec5\u8ffd\u6c42\u5f00\u653e\u5f0f\u7cbe\u5ea6\uff1b\u5c0f\u578b\u5316\u6a21\u578b\u53ef\u80fd\u66f4\u5177\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.06023", "pdf": "https://arxiv.org/pdf/2505.06023", "abs": "https://arxiv.org/abs/2505.06023", "authors": ["Qian Qi"], "title": "Universal Approximation Theorem for Deep Q-Learning via FBSDE System", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "The approximation capabilities of Deep Q-Networks (DQNs) are commonly\njustified by general Universal Approximation Theorems (UATs) that do not\nleverage the intrinsic structural properties of the optimal Q-function, the\nsolution to a Bellman equation. This paper establishes a UAT for a class of\nDQNs whose architecture is designed to emulate the iterative refinement process\ninherent in Bellman updates. A central element of our analysis is the\npropagation of regularity: while the transformation induced by a single Bellman\noperator application exhibits regularity, for which Backward Stochastic\nDifferential Equations (BSDEs) theory provides analytical tools, the uniform\nregularity of the entire sequence of value iteration iterates--specifically,\ntheir uniform Lipschitz continuity on compact domains under standard Lipschitz\nassumptions on the problem data--is derived from finite-horizon dynamic\nprogramming principles. We demonstrate that layers of a deep residual network,\nconceived as neural operators acting on function spaces, can approximate the\naction of the Bellman operator. The resulting approximation theorem is thus\nintrinsically linked to the control problem's structure, offering a proof\ntechnique wherein network depth directly corresponds to iterations of value\nfunction refinement, accompanied by controlled error propagation. This\nperspective reveals a dynamic systems view of the network's operation on a\nspace of value functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u7279\u5b9a\u8bbe\u8ba1\u7684\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u67b6\u6784\u5efa\u7acb\u4e86\u901a\u7528\u903c\u8fd1\u5b9a\u7406\uff08UAT\uff09\uff0c\u8be5\u67b6\u6784\u65e8\u5728\u6a21\u62df\u8d1d\u5c14\u66fc\u66f4\u65b0\u4e2d\u7684\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\uff0c\u8868\u660e\u7f51\u7edc\u6df1\u5ea6\u4e0e\u503c\u51fd\u6570\u8fed\u4ee3\u76f4\u63a5\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u672a\u80fd\u5229\u7528\u6700\u4f18Q\u51fd\u6570\u7684\u7ed3\u6784\u7279\u6027\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bbe\u8ba1\u4e00\u79cd\u4e0e\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7ed3\u6784\u66f4\u7d27\u5bc6\u76f8\u5173\u7684\u903c\u8fd1\u7406\u8bba\u3002", "method": "\u7814\u7a76\u8005\u5229\u7528\u4e86\u5411\u540e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08BSDEs\uff09\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u67b6\u6784\u6765\u903c\u8fd1\u8d1d\u5c14\u66fc\u7b97\u5b50\u7684\u4f5c\u7528\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6807\u51c6\u5229\u666e\u5e0c\u8328\u5047\u8bbe\u4e0b\uff0c\u7f51\u7edc\u5c42\u53ef\u4ee5\u6709\u6548\u5730\u6a21\u62df\u8d1d\u5c14\u66fc\u7b97\u5b50\uff0c\u4e14\u8bef\u5dee\u4f20\u64ad\u53ef\u63a7\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u7cfb\u7edf\u89c6\u89d2\u5206\u6790\u7f51\u7edc\u64cd\u4f5c\uff0c\u8868\u660e\u4e86\u7f51\u7edc\u6df1\u5ea6\u4e0e\u503c\u51fd\u6570\u8fed\u4ee3\u7684\u5173\u8054\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u5177\u7ed3\u6784\u6027\u7684\u903c\u8fd1\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2505.05666", "pdf": "https://arxiv.org/pdf/2505.05666", "abs": "https://arxiv.org/abs/2505.05666", "authors": ["Alexander Most", "Joseph Winjum", "Ayan Biswas", "Shawn Jones", "Nishath Rajiv Ranasinghe", "Dan O'Malley", "Manish Bhattarai"], "title": "Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a popular technique for\nenhancing the reliability and utility of Large Language Models (LLMs) by\ngrounding responses in external documents. Traditional RAG systems rely on\nOptical Character Recognition (OCR) to first process scanned documents into\ntext. However, even state-of-the-art OCRs can introduce errors, especially in\ndegraded or complex documents. Recent vision-language approaches, such as\nColPali, propose direct visual embedding of documents, eliminating the need for\nOCR. This study presents a systematic comparison between a vision-based RAG\nsystem (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2\n(90B) and Nougat OCR across varying document qualities. Beyond conventional\nretrieval accuracy metrics, we introduce a semantic answer evaluation benchmark\nto assess end-to-end question-answering performance. Our findings indicate that\nwhile vision-based RAG performs well on documents it has been fine-tuned on,\nOCR-based RAG is better able to generalize to unseen documents of varying\nquality. We highlight the key trade-offs between computational efficiency and\nsemantic accuracy, offering practical guidance for RAG practitioners in\nselecting between OCR-dependent and vision-based document retrieval systems in\nproduction environments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684RAG\u7cfb\u7edf\uff08ColPali\uff09\u4e0e\u4f20\u7edfOCR-based RAG\u7cfb\u7edf\u7684\u5bf9\u6bd4\uff0c\u53d1\u73b0\u89c6\u89c9\u65b9\u6cd5\u5728\u5df2\u5fae\u8c03\u6587\u6863\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u800cOCR\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u591a\u6837\u8d28\u91cf\u6587\u6863\u4e0a\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u5e76\u8ba8\u8bba\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u8bed\u4e49\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "motivation": "\u4f20\u7edf\u7684RAG\u7cfb\u7edf\u4f9d\u8d56OCR\u5904\u7406\u626b\u63cf\u6587\u6863\uff0c\u4f46OCR\u5728\u590d\u6742\u6216\u8d28\u91cf\u5dee\u6587\u6863\u4e2d\u5bb9\u6613\u51fa\u9519\u3002\u7814\u7a76\u6bd4\u8f83\u89c6\u89c9\u5d4c\u5165\uff08\u8df3\u8fc7OCR\uff09\u4e0e\u4f20\u7edfOCR-based\u65b9\u6cd5\u7684\u6548\u679c\u5dee\u5f02\u3002", "method": "\u4f7f\u7528Llama 3.2 (90B)\u548cNougat OCR\uff0c\u5bf9\u6bd4\u89c6\u89c9RAG\uff08ColPali\uff09\u548cOCR-based RAG\uff0c\u5f15\u5165\u8bed\u4e49\u7b54\u6848\u8bc4\u4f30\u57fa\u51c6\u8861\u91cf\u7aef\u5230\u7aef\u95ee\u7b54\u6027\u80fd\u3002", "result": "\u89c6\u89c9RAG\u5728\u5fae\u8c03\u6587\u6863\u4e0a\u8868\u73b0\u4f18\uff0c\u4f46OCR-based RAG\u5bf9\u591a\u6837\u8d28\u91cf\u672a\u89c1\u8fc7\u6587\u6863\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "OCR-based\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u66f4\u4f18\uff0c\u89c6\u89c9RAG\u5728\u7279\u5b9a\u573a\u666f\u9ad8\u6548\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u6743\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u8bed\u4e49\u51c6\u786e\u6027\u3002"}}
{"id": "2505.06047", "pdf": "https://arxiv.org/pdf/2505.06047", "abs": "https://arxiv.org/abs/2505.06047", "authors": ["Francesco Spinnato", "Cristiano Landi"], "title": "PYRREGULAR: A Unified Framework for Irregular Time Series, with Classification Benchmarks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Irregular temporal data, characterized by varying recording frequencies,\ndiffering observation durations, and missing values, presents significant\nchallenges across fields like mobility, healthcare, and environmental science.\nExisting research communities often overlook or address these challenges in\nisolation, leading to fragmented tools and methods. To bridge this gap, we\nintroduce a unified framework, and the first standardized dataset repository\nfor irregular time series classification, built on a common array format to\nenhance interoperability. This repository comprises 34 datasets on which we\nbenchmark 12 classifier models from diverse domains and communities. This work\naims to centralize research efforts and enable a more robust evaluation of\nirregular temporal data analysis methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u548c\u9996\u4e2a\u6807\u51c6\u5316\u6570\u636e\u96c6\u5e93\uff0c\u7528\u4e8e\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff0c\u65e8\u5728\u96c6\u4e2d\u7814\u7a76\u52aa\u529b\u5e76\u63d0\u5347\u65b9\u6cd5\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u56e0\u8bb0\u5f55\u9891\u7387\u3001\u89c2\u6d4b\u65f6\u957f\u548c\u7f3a\u5931\u503c\u7b49\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u7814\u7a76\u4e2d\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u788e\u7247\u5316\u3002", "method": "\u57fa\u4e8e\u901a\u7528\u6570\u7ec4\u683c\u5f0f\u6784\u5efa\u6807\u51c6\u5316\u6570\u636e\u96c6\u5e93\uff0c\u5e76\u572834\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf912\u79cd\u5206\u7c7b\u5668\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u5e93\uff0c\u652f\u6301\u8de8\u9886\u57df\u65b9\u6cd5\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u96c6\u4e2d\u5316\u7684\u8d44\u6e90\u548c\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u7840\u3002"}}
{"id": "2505.06053", "pdf": "https://arxiv.org/pdf/2505.06053", "abs": "https://arxiv.org/abs/2505.06053", "authors": ["Rustem Islamov", "Yarden As", "Ilyas Fatkhullin"], "title": "Safe-EF: Error Feedback for Nonsmooth Constrained Optimization", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Federated learning faces severe communication bottlenecks due to the high\ndimensionality of model updates. Communication compression with contractive\ncompressors (e.g., Top-K) is often preferable in practice but can degrade\nperformance without proper handling. Error feedback (EF) mitigates such issues\nbut has been largely restricted for smooth, unconstrained problems, limiting\nits real-world applicability where non-smooth objectives and safety constraints\nare critical. We advance our understanding of EF in the canonical non-smooth\nconvex setting by establishing new lower complexity bounds for first-order\nalgorithms with contractive compression. Next, we propose Safe-EF, a novel\nalgorithm that matches our lower bound (up to a constant) while enforcing\nsafety constraints essential for practical applications. Extending our approach\nto the stochastic setting, we bridge the gap between theory and practical\nimplementation. Extensive experiments in a reinforcement learning setup,\nsimulating distributed humanoid robot training, validate the effectiveness of\nSafe-EF in ensuring safety and reducing communication complexity.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5Safe-EF\uff0c\u901a\u8fc7\u538b\u7f29\u6280\u672f\u548c\u8bef\u5dee\u53cd\u9988\u4f18\u5316\u901a\u4fe1\u6548\u7387\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u578b\u66f4\u65b0\u7ef4\u5ea6\u9ad8\u5bfc\u81f4\u901a\u4fe1\u74f6\u9888\uff0c\u73b0\u6709\u538b\u7f29\u6280\u672f\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\uff0c\u4e14\u8bef\u5dee\u53cd\u9988\u65b9\u6cd5\u591a\u9650\u4e8e\u5e73\u6ed1\u65e0\u7ea6\u675f\u95ee\u9898\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u5e38\u9700\u5904\u7406\u975e\u5e73\u6ed1\u76ee\u6807\u548c\u5b89\u5168\u7ea6\u675f\u3002", "method": "\u8bba\u6587\u63d0\u51faSafe-EF\u7b97\u6cd5\uff0c\u7ed3\u5408\u538b\u7f29\u6280\u672f\u548c\u8bef\u5dee\u53cd\u9988\uff0c\u652f\u6301\u975e\u5e73\u6ed1\u51f8\u95ee\u9898\u4e2d\u7684\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u5728\u968f\u673a\u73af\u5883\u4e0b\u6269\u5c55\uff0c\u5339\u914d\u7406\u8bba\u4e0b\u754c\u3002", "result": "\u5b9e\u9a8c\u5728\u5206\u5e03\u5f0f\u4eba\u5f62\u673a\u5668\u4eba\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86Safe-EF\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4e86\u5b89\u5168\u6027\u5e76\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u8bef\u5dee\u53cd\u9988\u5728\u975e\u5e73\u6ed1\u5e26\u7ea6\u675f\u95ee\u9898\u4e2d\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u63d0\u51fa\u7684Safe-EF\u7b97\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5b89\u5168\u7ea6\u675f\u7684\u573a\u666f\u3002"}}
{"id": "2505.06080", "pdf": "https://arxiv.org/pdf/2505.06080", "abs": "https://arxiv.org/abs/2505.06080", "authors": ["Luis Miguel Esquivel-Sancho", "Maryam Ghandchi Tehrani", "Mauricio Mu\u00f1oz-Arias", "Mahmoud Askari"], "title": "Fault Diagnosis of 3D-Printed Scaled Wind Turbine Blades", "categories": ["cs.LG"], "comment": null, "summary": "This study presents an integrated methodology for fault detection in wind\nturbine blades using 3D-printed scaled models, finite element simulations,\nexperimental modal analysis, and machine learning techniques. A scaled model of\nthe NREL 5MW blade was fabricated using 3D printing, and crack-type damages\nwere introduced at critical locations. Finite Element Analysis was employed to\npredict the impact of these damages on the natural frequencies, with the\nresults validated through controlled hammer impact tests. Vibration data was\nprocessed to extract both time-domain and frequency-domain features, and key\ndiscriminative variables were identified using statistical analyses (ANOVA).\nMachine learning classifiers, including Support Vector Machine and K-Nearest\nNeighbors, achieved classification accuracies exceeding 94%. The results\nrevealed that vibration modes 3, 4, and 6 are particularly sensitive to\nstructural anomalies for this blade. This integrated approach confirms the\nfeasibility of combining numerical simulations with experimental validations\nand paves the way for structural health monitoring systems in wind energy\napplications.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u6253\u5370\u3001\u6709\u9650\u5143\u6a21\u62df\u3001\u5b9e\u9a8c\u6a21\u6001\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u7684\u98ce\u529b\u6da1\u8f6e\u673a\u53f6\u7247\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u632f\u52a8\u6a21\u5f0f\u5bf9\u7ed3\u6784\u5f02\u5e38\u7684\u654f\u611f\u6027\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc794%\u3002", "motivation": "\u98ce\u529b\u6da1\u8f6e\u673a\u53f6\u7247\u7684\u6545\u969c\u68c0\u6d4b\u5bf9\u4fdd\u969c\u98ce\u80fd\u8bbe\u5907\u7684\u5b89\u5168\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u96c6\u6210\u65b9\u6cd5\u4ee5\u9ad8\u6548\u8bc6\u522b\u7ed3\u6784\u5f02\u5e38\u3002", "method": "\u4f7f\u75283D\u6253\u5370\u7684\u7f29\u6bd4\u6a21\u578b\uff0c\u5f15\u5165\u88c2\u7eb9\u635f\u4f24\uff0c\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\u548c\u5b9e\u9a8c\u6a21\u6001\u5206\u6790\u9a8c\u8bc1\uff0c\u5e76\u7528\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5904\u7406\u632f\u52a8\u6570\u636e\u3002", "result": "\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8d85\u8fc794%\uff0c\u632f\u52a8\u6a21\u5f0f3\u30014\u30016\u5bf9\u7ed3\u6784\u5f02\u5e38\u654f\u611f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u6570\u503c\u6a21\u62df\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u7ed3\u5408\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u98ce\u80fd\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.05710", "pdf": "https://arxiv.org/pdf/2505.05710", "abs": "https://arxiv.org/abs/2505.05710", "authors": ["Wooyoung Jeong", "Hyun Jae Park", "Seonghun Jeong", "Jong Wook Jang", "Tae Hoon Lim", "Dae Seoung Kim"], "title": "HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Hyperspectral imagery provides rich spectral detail but poses unique\nchallenges because of its high dimensionality in both spatial and spectral\ndomains. We propose \\textit{HyperspectralMAE}, a Transformer-based foundation\nmodel for hyperspectral data that employs a \\textit{dual masking} strategy:\nduring pre-training we randomly occlude 50\\% of spatial patches and 50\\% of\nspectral bands. This forces the model to learn representations capable of\nreconstructing missing information across both dimensions. To encode spectral\norder, we introduce learnable harmonic Fourier positional embeddings based on\nwavelength. The reconstruction objective combines mean-squared error (MSE) with\nthe spectral angle mapper (SAM) to balance pixel-level accuracy and\nspectral-shape fidelity.\n  The resulting model contains about $1.8\\times10^{8}$ parameters and produces\n768-dimensional embeddings, giving it sufficient capacity for transfer\nlearning. We pre-trained HyperspectralMAE on two large hyperspectral corpora --\nNASA EO-1 Hyperion ($\\sim$1\\,600 scenes, $\\sim$$3\\times10^{11}$ pixel spectra)\nand DLR EnMAP Level-0 ($\\sim$1\\,300 scenes, $\\sim$$3\\times10^{11}$ pixel\nspectra) -- and fine-tuned it for land-cover classification on the Indian Pines\nbenchmark. HyperspectralMAE achieves state-of-the-art transfer-learning\naccuracy on Indian Pines, confirming that masked dual-dimensional pre-training\nyields robust spectral-spatial representations. These results demonstrate that\ndual masking and wavelength-aware embeddings advance hyperspectral image\nreconstruction and downstream analysis.", "AI": {"tldr": "HyperspectralMAE\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u53cc\u91cd\u63a9\u7801\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u540c\u65f6\u63a9\u7801\u7a7a\u95f4\u548c\u5149\u8c31\u7ef4\u5ea6\uff0c\u7ed3\u5408\u8c10\u6ce2\u5085\u91cc\u53f6\u4f4d\u7f6e\u7f16\u7801\uff0c\u5b9e\u73b0\u9ad8\u7ef4\u5149\u8c31\u56fe\u50cf\u7684\u9ad8\u6548\u8868\u793a\u548c\u4e0b\u6e38\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u9ad8\u7ef4\u7279\u6027\u5728\u7a7a\u95f4\u548c\u5149\u8c31\u57df\u5747\u5e26\u6765\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u80fd\u540c\u65f6\u5b66\u4e60\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\u7684\u6a21\u578b\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u63a9\u7801\u7b56\u7565\uff0850%\u7a7a\u95f4\u5757\u548c50%\u5149\u8c31\u5e26\uff09\uff0c\u7ed3\u5408\u8c10\u6ce2\u5085\u91cc\u53f6\u4f4d\u7f6e\u7f16\u7801\u548cMSE+SAM\u91cd\u5efa\u76ee\u6807\uff0c\u9884\u8bad\u7ec3\u5927\u89c4\u6a21\u9ad8\u5149\u8c31\u6570\u636e\u540e\u5fae\u8c03\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "\u5728Indian Pines\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\uff0c\u9a8c\u8bc1\u4e86\u53cc\u91cd\u63a9\u7801\u9884\u8bad\u7ec3\u548c\u6ce2\u957f\u611f\u77e5\u5d4c\u5165\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53cc\u91cd\u63a9\u7801\u548c\u8c10\u6ce2\u7f16\u7801\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u91cd\u5efa\u53ca\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u8bbe\u8ba1\u7684\u5408\u7406\u6027\u3002"}}
{"id": "2505.06087", "pdf": "https://arxiv.org/pdf/2505.06087", "abs": "https://arxiv.org/abs/2505.06087", "authors": ["Sergio Garc\u00eda-Heredia", "\u00c1ngela Fern\u00e1ndez", "Carlos M. Ala\u00edz"], "title": "Deep Diffusion Maps", "categories": ["cs.LG"], "comment": null, "summary": "One of the fundamental problems within the field of machine learning is\ndimensionality reduction. Dimensionality reduction methods make it possible to\ncombat the so-called curse of dimensionality, visualize high-dimensional data\nand, in general, improve the efficiency of storing and processing large data\nsets. One of the best-known nonlinear dimensionality reduction methods is\nDiffusion Maps. However, despite their virtues, both Diffusion Maps and many\nother manifold learning methods based on the spectral decomposition of kernel\nmatrices have drawbacks such as the inability to apply them to data outside the\ninitial set, their computational complexity, and high memory costs for large\ndata sets. In this work, we propose to alleviate these problems by resorting to\ndeep learning. Specifically, a new formulation of Diffusion Maps embedding is\noffered as a solution to a certain unconstrained minimization problem and,\nbased on it, a cost function to train a neural network which computes Diffusion\nMaps embedding -- both inside and outside the training sample -- without the\nneed to perform any spectral decomposition. The capabilities of this approach\nare compared on different data sets, both real and synthetic, with those of\nDiffusion Maps and the Nystrom method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\u6765\u6539\u8fdb\u6269\u6563\u6620\u5c04\uff08Diffusion Maps\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u65e0\u6cd5\u5e94\u7528\u4e8e\u65b0\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6620\u5c04\u7b49\u975e\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u5185\u5b58\u6d88\u8017\u5927\u4e14\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u65b0\u6570\u636e\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u6269\u6563\u6620\u5c04\u91cd\u6784\u4e3a\u65e0\u7ea6\u675f\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u76ee\u6807\u51fd\u6570\uff0c\u4f7f\u5176\u65e0\u9700\u8c31\u5206\u89e3\u5373\u53ef\u8ba1\u7b97\u5d4c\u5165\u7ed3\u679c\uff0c\u4e14\u9002\u7528\u4e8e\u8bad\u7ec3\u96c6\u5185\u5916\u6570\u636e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4e0e\u4f20\u7edf\u6269\u6563\u6620\u5c04\u548cNystrom\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6620\u5c04\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9002\u7528\u6027\uff0c\u4e3a\u975e\u7ebf\u6027\u964d\u7ef4\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06091", "pdf": "https://arxiv.org/pdf/2505.06091", "abs": "https://arxiv.org/abs/2505.06091", "authors": ["Xinxin Li", "Juan Zhang", "Da Li", "Xingyu Liu", "Jin Xu", "Junping Yin"], "title": "UniSymNet: A Unified Symbolic Network Guided by Transformer", "categories": ["cs.LG", "cs.AI", "cs.SC"], "comment": null, "summary": "Symbolic Regression (SR) is a powerful technique for automatically\ndiscovering mathematical expressions from input data. Mainstream SR algorithms\nsearch for the optimal symbolic tree in a vast function space, but the\nincreasing complexity of the tree structure limits their performance. Inspired\nby neural networks, symbolic networks have emerged as a promising new paradigm.\nHowever, most existing symbolic networks still face certain challenges: binary\nnonlinear operators $\\{\\times, \\div\\}$ cannot be naturally extended to\nmultivariate operators, and training with fixed architecture often leads to\nhigher complexity and overfitting. In this work, we propose a Unified Symbolic\nNetwork that unifies nonlinear binary operators into nested unary operators and\ndefine the conditions under which UniSymNet can reduce complexity. Moreover, we\npre-train a Transformer model with a novel label encoding method to guide\nstructural selection, and adopt objective-specific optimization strategies to\nlearn the parameters of the symbolic network. UniSymNet shows high fitting\naccuracy, excellent symbolic solution rate, and relatively low expression\ncomplexity, achieving competitive performance on low-dimensional Standard\nBenchmarks and high-dimensional SRBench.", "AI": {"tldr": "\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u7b26\u53f7\u7f51\u7edc\u5728\u7b26\u53f7\u56de\u5f52\u4e2d\u5b58\u5728\u4e8c\u5143\u975e\u7ebf\u6027\u7b97\u5b50\u96be\u4ee5\u6269\u5c55\u548c\u56fa\u5b9a\u67b6\u6784\u5bfc\u81f4\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u7edf\u4e00\u7684\u7b26\u53f7\u7f51\u7edc(UniSymNet),\u5c06\u4e8c\u5143\u7b97\u5b50\u7edf\u4e00\u4e3a\u5d4c\u5957\u4e00\u5143\u7b97\u5b50,\u5e76\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u6307\u5bfc\u7ed3\u6784\u9009\u62e9,\u4f18\u5316\u53c2\u6570\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660eUniSymNet\u5728\u9ad8\u7ef4\u548c\u4f4e\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7b26\u53f7\u7f51\u7edc\u4e2d\u4e8c\u5143\u975e\u7ebf\u6027\u7b97\u5b50\u96be\u4ee5\u81ea\u7136\u6269\u5c55\u5230\u591a\u5143\u7b97\u5b50,\u4ee5\u53ca\u56fa\u5b9a\u67b6\u6784\u8bad\u7ec3\u6613\u5bfc\u81f4\u9ad8\u590d\u6742\u5ea6\u548c\u8fc7\u62df\u5408\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faUnified Symbolic Network(UniSymNet),\u7edf\u4e00\u4e8c\u5143\u975e\u7ebf\u6027\u7b97\u5b50\u4e3a\u5d4c\u5957\u4e00\u5143\u7b97\u5b50;\u9884\u8bad\u7ec3Transformer\u6a21\u578b,\u91c7\u7528\u65b0\u7684\u6807\u7b7e\u7f16\u7801\u65b9\u6cd5\u6307\u5bfc\u7ed3\u6784\u9009\u62e9;\u76ee\u6807\u7279\u5b9a\u7684\u4f18\u5316\u7b56\u7565\u5b66\u4e60\u7f51\u7edc\u53c2\u6570\u3002", "result": "UniSymNet\u5728\u62df\u5408\u7cbe\u5ea6\u3001\u7b26\u53f7\u89e3\u7387\u548c\u8868\u8fbe\u5f0f\u590d\u6742\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02,\u5728\u4f4e\u7ef4Standard Benchmarks\u548c\u9ad8\u7ef4SRBench\u4e0a\u5747\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "UniSymNet\u901a\u8fc7\u7edf\u4e00\u7b97\u5b50\u548c\u4f18\u5316\u5b66\u4e60\u7b56\u7565,\u663e\u8457\u63d0\u5347\u4e86\u7b26\u53f7\u7f51\u7edc\u7684\u6027\u80fd,\u4e3a\u7b26\u53f7\u56de\u5f52\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05753", "pdf": "https://arxiv.org/pdf/2505.05753", "abs": "https://arxiv.org/abs/2505.05753", "authors": ["Bo Ai", "Liu Dai", "Nico Bohlinger", "Dichen Li", "Tongzhou Mu", "Zhanxin Wu", "K. Fay", "Henrik I. Christensen", "Jan Peters", "Hao Su"], "title": "Towards Embodiment Scaling Laws in Robot Locomotion", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "32 pages. Project website: https://embodiment-scaling-laws.github.io/", "summary": "Developing generalist agents that can operate across diverse tasks,\nenvironments, and physical embodiments is a grand challenge in robotics and\nartificial intelligence. In this work, we focus on the axis of embodiment and\ninvestigate embodiment scaling laws$\\unicode{x2013}$the hypothesis that\nincreasing the number of training embodiments improves generalization to unseen\nones. Using robot locomotion as a test bed, we procedurally generate a dataset\nof $\\sim$1,000 varied embodiments, spanning humanoids, quadrupeds, and\nhexapods, and train generalist policies capable of handling diverse observation\nand action spaces on random subsets. We find that increasing the number of\ntraining embodiments improves generalization to unseen ones, and scaling\nembodiments is more effective in enabling embodiment-level generalization than\nscaling data on small, fixed sets of embodiments. Notably, our best policy,\ntrained on the full dataset, zero-shot transfers to novel embodiments in the\nreal world, such as Unitree Go2 and H1. These results represent a step toward\ngeneral embodied intelligence, with potential relevance to adaptive control for\nconfigurable robots, co-design of morphology and control, and beyond.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u4e2d\u7684\u673a\u5668\u4eba\u5f62\u6001\u6570\u91cf\uff0c\u53ef\u4ee5\u63d0\u5347\u672a\u89c1\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u96f6\u6b21\u8fc1\u79fb\u7684\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u80fd\u9002\u5e94\u591a\u79cd\u4efb\u52a1\u3001\u73af\u5883\u548c\u5f62\u6001\u7684\u901a\u7528\u667a\u80fd\u4f53\u662f\u673a\u5668\u4eba\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u91cd\u8981\u6311\u6218\u3002\u672c\u6587\u805a\u7126\u5f62\u6001\u591a\u6837\u6027\uff0c\u9a8c\u8bc1\u5f62\u6001\u6570\u91cf\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8fc7\u7a0b\u5316\u751f\u6210\u7ea61000\u79cd\u5f62\u6001\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u901a\u7528\u7b56\u7565\u5904\u7406\u591a\u6837\u7684\u89c2\u6d4b\u4e0e\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u6d4b\u8bd5\u5f62\u6001\u6570\u91cf\u5bf9\u6cdb\u5316\u7684\u4f5c\u7528\u3002", "result": "\u589e\u52a0\u8bad\u7ec3\u5f62\u6001\u80fd\u663e\u8457\u63d0\u5347\u672a\u89c1\u5f62\u6001\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u6548\u679c\u4f18\u4e8e\u56fa\u5b9a\u5c0f\u89c4\u6a21\u5f62\u6001\u7684\u6570\u636e\u589e\u5f3a\u3002\u6700\u4f73\u7b56\u7565\u80fd\u96f6\u6b21\u8fc1\u79fb\u81f3\u771f\u5b9e\u673a\u5668\u4eba\uff08\u5982Unitree Go2\u548cH1\uff09\u3002", "conclusion": "\u5f62\u6001\u6269\u5c55\u662f\u5b9e\u73b0\u901a\u7528\u5177\u8eab\u667a\u80fd\u7684\u6709\u6548\u8def\u5f84\uff0c\u5bf9\u53ef\u914d\u7f6e\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u63a7\u5236\u3001\u5f62\u6001\u4e0e\u63a7\u5236\u7684\u534f\u540c\u8bbe\u8ba1\u7b49\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.06108", "pdf": "https://arxiv.org/pdf/2505.06108", "abs": "https://arxiv.org/abs/2505.06108", "authors": ["Lennart Justen"], "title": "LLMs Outperform Experts on Challenging Biology Benchmarks", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "This study systematically evaluates 27 frontier Large Language Models on\neight diverse biology benchmarks spanning molecular biology, genetics, cloning,\nvirology, and biosecurity. Models from major AI developers released between\nNovember 2022 and April 2025 were assessed through ten independent runs per\nbenchmark. The findings reveal dramatic improvements in biological\ncapabilities. Top model performance increased more than 4-fold on the\nchallenging text-only subset of the Virology Capabilities Test over the study\nperiod, with the top model now performing twice as well as expert virologists.\nSeveral models now match or exceed expert-level performance on other\nchallenging benchmarks, including LAB-Bench CloningScenarios and the biology\nsubsets of GPQA and WMDP. Contrary to expectations, chain-of-thought did not\nsubstantially improve performance over zero-shot evaluation, while extended\nreasoning features in o3-mini and Claude 3.7 Sonnet typically improved\nperformance as predicted by inference scaling. Benchmarks such as PubMedQA and\nthe MMLU and WMDP biology subsets exhibited performance plateaus well below\n100%, suggesting benchmark saturation and errors in the underlying benchmark\ndata. The analysis highlights the need for more sophisticated evaluation\nmethodologies as AI systems continue to advance.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8627\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57288\u4e2a\u751f\u7269\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u90e8\u5206\u6a21\u578b\u751a\u81f3\u8d85\u8d8a\u4e13\u5bb6\u6c34\u5e73\u3002\u63a8\u7406\u6269\u5c55\u529f\u80fd\u5982Claude 3.7 Sonnet\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u67d0\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u51fa\u73b0\u9971\u548c\u73b0\u8c61\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u9886\u57df\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5176\u8fdb\u6b65\u548c\u5c40\u9650\u6027\u3002", "method": "\u57288\u4e2a\u751f\u7269\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf927\u79cd\u6a21\u578b\u8fdb\u884c10\u6b21\u72ec\u7acb\u8fd0\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u90e8\u5206\u8d85\u8d8a\u4e13\u5bb6\u6c34\u5e73\uff1b\u63a8\u7406\u6269\u5c55\u529f\u80fd\u6709\u6548\uff0c\u4f46\u67d0\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u51fa\u73b0\u9971\u548c\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u5e94\u5bf9AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u3002"}}
{"id": "2505.05756", "pdf": "https://arxiv.org/pdf/2505.05756", "abs": "https://arxiv.org/abs/2505.05756", "authors": ["Antonio Jimeno Yepes", "Pieter Barnard"], "title": "Evolutionary thoughts: integration of large language models and evolutionary algorithms", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have unveiled remarkable capabilities in\nunderstanding and generating both natural language and code, but LLM reasoning\nis prone to hallucination and struggle with complex, novel scenarios, often\ngetting stuck on partial or incorrect solutions. However, the inherent ability\nof Evolutionary Algorithms (EAs) to explore extensive and complex search spaces\nmakes them particularly effective in scenarios where traditional optimization\nmethodologies may falter. However, EAs explore a vast search space when applied\nto complex problems.\n  To address the computational bottleneck of evaluating large populations,\nparticularly crucial for complex evolutionary tasks, we introduce a highly\nefficient evaluation framework. This implementation maintains compatibility\nwith existing primitive definitions, ensuring the generation of valid\nindividuals.\n  Using LLMs, we propose an enhanced evolutionary search strategy that enables\na more focused exploration of expansive solution spaces. LLMs facilitate the\ngeneration of superior candidate solutions, as evidenced by empirical results\ndemonstrating their efficacy in producing improved outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u8fdb\u5316\u7b97\u6cd5\uff08EAs\uff09\u7684\u9ad8\u6548\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3LLMs\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u63a8\u7406\u5c40\u9650\u6027\u548cEAs\u641c\u7d22\u7a7a\u95f4\u8fc7\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u6216\u65b0\u9896\u573a\u666f\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u6216\u9677\u5165\u5c40\u90e8\u89e3\uff0c\u800c\u8fdb\u5316\u7b97\u6cd5\uff08EAs\uff09\u867d\u64c5\u957f\u63a2\u7d22\u590d\u6742\u641c\u7d22\u7a7a\u95f4\uff0c\u4f46\u4ecd\u9762\u4e34\u8ba1\u7b97\u74f6\u9888\u3002\u4e3a\u4e86\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u5e76\u514b\u670d\u5404\u81ea\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u517c\u5bb9\u73b0\u6709\u539f\u59cb\u5b9a\u4e49\uff0c\u5e76\u5229\u7528LLMs\u4f18\u5316\u4e86\u8fdb\u5316\u641c\u7d22\u7b56\u7565\uff0c\u4ee5\u66f4\u805a\u7126\u7684\u65b9\u5f0f\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cLLMs\u80fd\u591f\u751f\u6210\u66f4\u4f18\u8d28\u7684\u5019\u9009\u89e3\uff0c\u63d0\u5347\u4e86\u8fdb\u5316\u7b97\u6cd5\u7684\u8868\u73b0\u3002", "conclusion": "\u7ed3\u5408LLMs\u548cEAs\u7684\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u641c\u7d22\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\uff0c\u4e3aLLMs\u548c\u8fdb\u5316\u8ba1\u7b97\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2505.06114", "pdf": "https://arxiv.org/pdf/2505.06114", "abs": "https://arxiv.org/abs/2505.06114", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Hao Wang", "Huayu Li", "Zihan Li", "Yalin Wang", "Aristeidis Sotiras", "Abolfazl Razi"], "title": "FIC-TSC: Learning Time Series Classification with Fisher Information Constraint", "categories": ["cs.LG"], "comment": "Accepted by ICML2025. Pre camera-ready version", "summary": "Analyzing time series data is crucial to a wide spectrum of applications,\nincluding economics, online marketplaces, and human healthcare. In particular,\ntime series classification plays an indispensable role in segmenting different\nphases in stock markets, predicting customer behavior, and classifying worker\nactions and engagement levels. These aspects contribute significantly to the\nadvancement of automated decision-making and system optimization in real-world\napplications. However, there is a large consensus that time series data often\nsuffers from domain shifts between training and test sets, which dramatically\ndegrades the classification performance. Despite the success of (reversible)\ninstance normalization in handling the domain shifts for time series regression\ntasks, its performance in classification is unsatisfactory. In this paper, we\npropose \\textit{FIC-TSC}, a training framework for time series classification\nthat leverages Fisher information as the constraint. We theoretically and\nempirically show this is an efficient and effective solution to guide the model\nconverge toward flatter minima, which enhances its generalizability to\ndistribution shifts. We rigorously evaluate our method on 30 UEA multivariate\nand 85 UCR univariate datasets. Our empirical results demonstrate the\nsuperiority of the proposed method over 14 recent state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFisher\u4fe1\u606f\u7684\u8bad\u7ec3\u6846\u67b6FIC-TSC\uff0c\u7528\u4e8e\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u7684\u6cdb\u5316\u6027\uff0c\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5b9e\u4f8b\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faFIC-TSC\u6846\u67b6\uff0c\u5229\u7528Fisher\u4fe1\u606f\u4f5c\u4e3a\u7ea6\u675f\u6761\u4ef6\uff0c\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u80fd\u591f\u5f15\u5bfc\u6a21\u578b\u6536\u655b\u5230\u66f4\u5e73\u5766\u7684\u6781\u5c0f\u503c\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9002\u5e94\u6027\u3002", "result": "\u572830\u4e2aUEA\u591a\u53d8\u91cf\u548c85\u4e2aUCR\u5355\u53d8\u91cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFIC-TSC\u4f18\u4e8e14\u79cd\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "FIC-TSC\u901a\u8fc7Fisher\u4fe1\u606f\u7ea6\u675f\u6709\u6548\u5730\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u57df\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.05762", "pdf": "https://arxiv.org/pdf/2505.05762", "abs": "https://arxiv.org/abs/2505.05762", "authors": ["Junhong Chen", "Ziqi Yang", "Haoyuan G Xu", "Dandan Zhang", "George Mylonas"], "title": "Multi-Agent Systems for Robotic Autonomy with LLMs", "categories": ["cs.RO", "cs.AI"], "comment": "11 pages, 2 figures, 5 tables, submitted for publication", "summary": "Since the advent of Large Language Models (LLMs), various research based on\nsuch models have maintained significant academic attention and impact,\nespecially in AI and robotics. In this paper, we propose a multi-agent\nframework with LLMs to construct an integrated system for robotic task\nanalysis, mechanical design, and path generation. The framework includes three\ncore agents: Task Analyst, Robot Designer, and Reinforcement Learning Designer.\nOutputs are formatted as multimodal results, such as code files or technical\nreports, for stronger understandability and usability. To evaluate\ngeneralizability comparatively, we conducted experiments with models from both\nGPT and DeepSeek. Results demonstrate that the proposed system can design\nfeasible robots with control strategies when appropriate task inputs are\nprovided, exhibiting substantial potential for enhancing the efficiency and\naccessibility of robotic system development in research and industrial\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\u5206\u6790\u3001\u673a\u68b0\u8bbe\u8ba1\u548c\u8def\u5f84\u751f\u6210\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u8bbe\u8ba1\u53ef\u884c\u673a\u5668\u4eba\u53ca\u5176\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728AI\u548c\u673a\u5668\u4eba\u9886\u57df\u5907\u53d7\u5173\u6ce8\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u667a\u80fd\u4f53\uff1a\u4efb\u52a1\u5206\u6790\u5458\u3001\u673a\u5668\u4eba\u8bbe\u8ba1\u5458\u548c\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u5458\uff0c\u8f93\u51fa\u4e3a\u591a\u6a21\u6001\u7ed3\u679c\uff08\u5982\u4ee3\u7801\u6587\u4ef6\u6216\u6280\u672f\u62a5\u544a\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u4e3a\u7ed9\u5b9a\u4efb\u52a1\u8bbe\u8ba1\u53ef\u884c\u7684\u673a\u5668\u4eba\u53ca\u63a7\u5236\u7b56\u7565\uff0c\u5c55\u73b0\u51fa\u5728\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7LLMs\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5f00\u53d1\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u573a\u666f\u3002"}}
{"id": "2505.06123", "pdf": "https://arxiv.org/pdf/2505.06123", "abs": "https://arxiv.org/abs/2505.06123", "authors": ["Philip Naumann", "Jacob Kauffmann", "Gr\u00e9goire Montavon"], "title": "Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Wasserstein distances provide a powerful framework for comparing data\ndistributions. They can be used to analyze processes over time or to detect\ninhomogeneities within data. However, simply calculating the Wasserstein\ndistance or analyzing the corresponding transport map (or coupling) may not be\nsufficient for understanding what factors contribute to a high or low\nWasserstein distance. In this work, we propose a novel solution based on\nExplainable AI that allows us to efficiently and accurately attribute\nWasserstein distances to various data components, including data subgroups,\ninput features, or interpretable subspaces. Our method achieves high accuracy\nacross diverse datasets and Wasserstein distance specifications, and its\npractical utility is demonstrated in two use cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u5f52\u56e0Wasserstein\u8ddd\u79bb\u5230\u4e0d\u540c\u6570\u636e\u7ec4\u4ef6\uff0c\u5982\u5b50\u7fa4\u3001\u7279\u5f81\u6216\u53ef\u89e3\u91ca\u5b50\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u7684Wasserstein\u8ddd\u79bb\u8ba1\u7b97\u6216\u4f20\u8f93\u6620\u5c04\u5206\u6790\u4e0d\u8db3\u4ee5\u7406\u89e3\u5176\u9ad8\u4f4e\u7684\u5177\u4f53\u539f\u56e0\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6df1\u5165\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u53ef\u89e3\u91caAI\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6570\u636e\u5b50\u7fa4\u3001\u8f93\u5165\u7279\u5f81\u6216\u53ef\u89e3\u91ca\u5b50\u7a7a\u95f4\u6765\u5f52\u56e0Wasserstein\u8ddd\u79bb\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u548cWasserstein\u8ddd\u79bb\u89c4\u683c\u4e0b\u5b9e\u73b0\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9645\u7528\u4f8b\u9a8c\u8bc1\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aWasserstein\u8ddd\u79bb\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u7406\u89e3\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u5176\u5b9e\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.05768", "pdf": "https://arxiv.org/pdf/2505.05768", "abs": "https://arxiv.org/abs/2505.05768", "authors": ["Weiyi Zhang", "Peranut Chotcomwongse", "Yinwen Li", "Pusheng Xu", "Ruijie Yao", "Lianhao Zhou", "Yuxuan Zhou", "Hui Feng", "Qiping Zhou", "Xinyue Wang", "Shoujin Huang", "Zihao Jin", "Florence H. T. Chung", "Shujun Wang", "Yalin Zheng", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "42 pages,5 tables, 12 figures, challenge report", "summary": "Diabetic macular edema (DME) significantly contributes to visual impairment\nin diabetic patients. Treatment responses to intravitreal therapies vary,\nhighlighting the need for patient stratification to predict therapeutic\nbenefits and enable personalized strategies. To our knowledge, this study is\nthe first to explore pre-treatment stratification for predicting DME treatment\nresponses. To advance this research, we organized the 2nd Asia-Pacific\nTele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The\ncompetition focused on improving predictive accuracy for anti-VEGF therapy\nresponses using ophthalmic OCT images. We provided a dataset containing tens of\nthousands of OCT images from 2,000 patients with labels across four sub-tasks.\nThis paper details the competition's structure, dataset, leading methods, and\nevaluation metrics. The competition attracted strong scientific community\nparticipation, with 170 teams initially registering and 41 reaching the final\nround. The top-performing team achieved an AUC of 80.06%, highlighting the\npotential of AI in personalized DME treatment and clinical decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e9a\u592a\u8fdc\u7a0b\u773c\u79d1\u5b66\u4f1a\u5927\u6570\u636e\u7ade\u8d5b\uff0c\u5229\u7528OCT\u56fe\u50cf\u9884\u6d4b\u7cd6\u5c3f\u75c5\u9ec4\u6591\u6c34\u80bf\u60a3\u8005\u7684\u6297VEGF\u6cbb\u7597\u53cd\u5e94\uff0c\u5c55\u793a\u4e86AI\u5728\u4e2a\u6027\u5316\u6cbb\u7597\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u9ec4\u6591\u6c34\u80bf\uff08DME\uff09\u60a3\u8005\u5bf9\u6297VEGF\u6cbb\u7597\u7684\u53cd\u5e94\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8981\u5206\u5c42\u9884\u6d4b\u4ee5\u4f18\u5316\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u3002\u76ee\u524d\u7f3a\u4e4f\u76f8\u5173\u7814\u7a76\uff0c\u56e0\u6b64\u901a\u8fc7\u7ade\u8d5b\u63a2\u7d22\u9884\u6cbb\u7597\u5206\u5c42\u65b9\u6cd5\u3002", "method": "\u7ec4\u7ec7\u4e9a\u592a\u8fdc\u7a0b\u773c\u79d1\u5b66\u4f1a\u5927\u6570\u636e\u7ade\u8d5b\uff0c\u63d0\u4f9b2,000\u540d\u60a3\u8005\u7684OCT\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5206\u4e3a\u56db\u9879\u5b50\u4efb\u52a1\uff0c\u5438\u5f15170\u652f\u56e2\u961f\u53c2\u4e0e\u5e76\u63d0\u4ea4\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u6700\u4f73\u56e2\u961f\u7684AUC\u8fbe\u523080.06%\uff0c\u8868\u660eAI\u53ef\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002", "conclusion": "\u7ade\u8d5b\u8bc1\u660e\u4e86AI\u5728DME\u4e2a\u6027\u5316\u6cbb\u7597\u9884\u6d4b\u4e2d\u7684\u4ef7\u503c\uff0c\u4e3a\u672a\u6765\u4e34\u5e8a\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.06134", "pdf": "https://arxiv.org/pdf/2505.06134", "abs": "https://arxiv.org/abs/2505.06134", "authors": ["Julian F. Schumann", "Jeroen Hagenus", "Frederik Baymler Mathiesen", "Arkady Zgonnikov"], "title": "Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation", "categories": ["cs.LG", "cs.HC"], "comment": "20 pages, 3 figures", "summary": "Trajectory prediction is a key element of autonomous vehicle systems,\nenabling them to anticipate and react to the movements of other road users.\nEvaluating the robustness of prediction models against adversarial attacks is\nessential to ensure their reliability in real-world traffic. However, current\napproaches tend to focus on perturbing the past positions of surrounding\nagents, which can generate unrealistic scenarios and overlook critical\nvulnerabilities. This limitation may result in overly optimistic assessments of\nmodel performance in real-world conditions.\n  In this work, we demonstrate that perturbing not just past but also future\nstates of adversarial agents can uncover previously undetected weaknesses and\nthereby provide a more rigorous evaluation of model robustness. Our novel\napproach incorporates dynamic constraints and preserves tactical behaviors,\nenabling more effective and realistic adversarial attacks. We introduce new\nperformance measures to assess the realism and impact of these adversarial\ntrajectories. Testing our method on a state-of-the-art prediction model\nrevealed significant increases in prediction errors and collision rates under\nadversarial conditions. Qualitative analysis further showed that our attacks\ncan expose critical weaknesses, such as the inability of the model to detect\npotential collisions in what appear to be safe predictions. These results\nunderscore the need for more comprehensive adversarial testing to better\nevaluate and improve the reliability of trajectory prediction models for\nautonomous vehicles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6270\u52a8\u5bf9\u6297\u667a\u80fd\u4f53\u7684\u8fc7\u53bb\u548c\u672a\u6765\u72b6\u6001\u6765\u66f4\u5168\u9762\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5173\u952e\u5f31\u70b9\u3002", "motivation": "\u5f53\u524d\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6270\u52a8\u8fc7\u53bb\u4f4d\u7f6e\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u8fc7\u4e8e\u4e50\u89c2\uff0c\u5ffd\u89c6\u5b9e\u9645\u4ea4\u901a\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u5bf9\u6297\u667a\u80fd\u4f53\u7684\u8fc7\u53bb\u548c\u672a\u6765\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u7ea6\u675f\u548c\u6218\u672f\u884c\u4e3a\u4fdd\u7559\uff0c\u751f\u6210\u66f4\u73b0\u5b9e\u7684\u5bf9\u6297\u8f68\u8ff9\u3002", "result": "\u5728\u5148\u8fdb\u9884\u6d4b\u6a21\u578b\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u52a0\u4e86\u9884\u6d4b\u9519\u8bef\u7387\u548c\u78b0\u649e\u7387\uff0c\u5e76\u66b4\u9732\u4e86\u6a21\u578b\u65e0\u6cd5\u68c0\u6d4b\u6f5c\u5728\u78b0\u649e\u7b49\u5173\u952e\u5f31\u70b9\u3002", "conclusion": "\u9700\u8981\u66f4\u5168\u9762\u7684\u5bf9\u6297\u6d4b\u8bd5\u6765\u66f4\u597d\u5730\u8bc4\u4f30\u548c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2505.05777", "pdf": "https://arxiv.org/pdf/2505.05777", "abs": "https://arxiv.org/abs/2505.05777", "authors": ["Domenico Cotroneo", "Giuseppe De Rosa", "Pietro Liguori"], "title": "PyResBugs: A Dataset of Residual Python Bugs for Natural Language-Driven Fault Injection", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper presents PyResBugs, a curated dataset of residual bugs, i.e.,\ndefects that persist undetected during traditional testing but later surface in\nproduction, collected from major Python frameworks. Each bug in the dataset is\npaired with its corresponding fault-free (fixed) version and annotated with\nmulti-level natural language (NL) descriptions. These NL descriptions enable\nnatural language-driven fault injection, offering a novel approach to\nsimulating real-world faults in software systems. By bridging the gap between\nsoftware fault injection techniques and real-world representativeness,\nPyResBugs provides researchers with a high-quality resource for advancing\nAI-driven automated testing in Python systems.", "AI": {"tldr": "PyResBugs\u662f\u4e00\u4e2a\u7cbe\u5fc3\u6574\u7406\u7684Python\u6846\u67b6\u4e2d\u7684\u9057\u7559\u7f3a\u9677\u6570\u636e\u96c6\uff0c\u5305\u542b\u7f3a\u9677\u53ca\u5176\u4fee\u590d\u7248\u672c\uff0c\u5e76\u6807\u6ce8\u591a\u7ea7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u96be\u4ee5\u68c0\u6d4b\u5230\u7684\u7f3a\u9677\u5728\u540e\u671f\u751f\u4ea7\u4e2d\u66b4\u9732\uff0cPyResBugs\u65e8\u5728\u586b\u8865\u8f6f\u4ef6\u6545\u969c\u6ce8\u5165\u6280\u672f\u4e0e\u771f\u5b9e\u4e16\u754c\u4ee3\u8868\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u6536\u96c6Python\u6846\u67b6\u4e2d\u7684\u9057\u7559\u7f3a\u9677\uff0c\u914d\u5bf9\u7f3a\u9677\u4e0e\u4fee\u590d\u7248\u672c\uff0c\u6807\u6ce8\u591a\u7ea7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\u3002", "result": "PyResBugs\u4e3aPython\u7cfb\u7edf\u7684AI\u9a71\u52a8\u81ea\u52a8\u5316\u6d4b\u8bd5\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8d44\u6e90\u3002", "conclusion": "PyResBugs\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\uff0c\u63d0\u5347\u4e86\u8f6f\u4ef6\u7cfb\u7edf\u7684\u771f\u5b9e\u4e16\u754c\u6545\u969c\u6a21\u62df\u80fd\u529b\u3002"}}
{"id": "2505.06169", "pdf": "https://arxiv.org/pdf/2505.06169", "abs": "https://arxiv.org/abs/2505.06169", "authors": ["Egor Bakaev", "Florestan Brunck", "Christoph Hertrich", "Daniel Reichman", "Amir Yehudayoff"], "title": "On the Depth of Monotone ReLU Neural Networks and ICNNs", "categories": ["cs.LG", "cs.DM", "cs.NE", "math.CO"], "comment": "27 pages, 17 figures", "summary": "We study two models of ReLU neural networks: monotone networks (ReLU$^+$) and\ninput convex neural networks (ICNN). Our focus is on expressivity, mostly in\nterms of depth, and we prove the following lower bounds. For the maximum\nfunction MAX$_n$ computing the maximum of $n$ real numbers, we show that\nReLU$^+$ networks cannot compute MAX$_n$, or even approximate it. We prove a\nsharp $n$ lower bound on the ICNN depth complexity of MAX$_n$. We also prove\ndepth separations between ReLU networks and ICNNs; for every $k$, there is a\ndepth-2 ReLU network of size $O(k^2)$ that cannot be simulated by a depth-$k$\nICNN. The proofs are based on deep connections between neural networks and\npolyhedral geometry, and also use isoperimetric properties of triangulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e24\u79cdReLU\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u5355\u8c03\u7f51\u7edc\u548c\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5bf9\u8ba1\u7b97\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u8bc1\u660e\u4e86\u76f8\u5173\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3ReLU\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u7279\u522b\u662f\u5355\u8c03\u7f51\u7edc\u548c\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u8ba1\u7b97\u6700\u5927\u51fd\u6570\u65f6\u7684\u8868\u8fbe\u80fd\u529b\u9650\u5236\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u7684\u6df1\u5ea6\u5206\u79bb\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u6df1\u5ea6\u4e0e\u591a\u9762\u4f53\u51e0\u4f55\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u4ee5\u53ca\u4e09\u89d2\u5256\u5206\u7684\u7b49\u5468\u6027\u8d28\uff0c\u8bc1\u660e\u4e86\u76f8\u5173\u4e0b\u754c\u548c\u5206\u79bb\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5355\u8c03\u7f51\u7edc\u65e0\u6cd5\u8ba1\u7b97\u6216\u903c\u8fd1\u6700\u5927\u51fd\u6570\uff0c\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u5bf9\u6700\u5927\u51fd\u6570\u7684\u6df1\u5ea6\u590d\u6742\u5ea6\u6709\u4e00\u4e2a\u5c16\u9510\u7684n\u4e0b\u754c\uff1b\u540c\u65f6\u8fd8\u8bc1\u660e\u4e86ReLU\u7f51\u7edc\u4e0e\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u6df1\u5ea6\u5206\u79bb\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86ReLU\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u4e0d\u540c\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u9650\u5236\uff0c\u4e3a\u7406\u89e3\u7f51\u7edc\u6df1\u5ea6\u4e0e\u8ba1\u7b97\u80fd\u529b\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2505.05784", "pdf": "https://arxiv.org/pdf/2505.05784", "abs": "https://arxiv.org/abs/2505.05784", "authors": ["Yang Li", "Zhi Chen", "Steve Yang"], "title": "FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions", "categories": ["q-fin.TR", "cs.AI", "cs.CE", "q-fin.CP"], "comment": "14 pages, 1 figure, 6 tables, 2 algorithms", "summary": "High-frequency trading (HFT) is an investing strategy that continuously\nmonitors market states and places bid and ask orders at millisecond speeds.\nTraditional HFT approaches fit models with historical data and assume that\nfuture market states follow similar patterns. This limits the effectiveness of\nany single model to the specific conditions it was trained for. Additionally,\nthese models achieve optimal solutions only under specific market conditions,\nsuch as assumptions about stock price's stochastic process, stable order flow,\nand the absence of sudden volatility. Real-world markets, however, are dynamic,\ndiverse, and frequently volatile. To address these challenges, we propose the\nFlowHFT, a novel imitation learning framework based on flow matching policy.\nFlowHFT simultaneously learns strategies from numerous expert models, each\nproficient in particular market scenarios. As a result, our framework can\nadaptively adjust investment decisions according to the prevailing market\nstate. Furthermore, FlowHFT incorporates a grid-search fine-tuning mechanism.\nThis allows it to refine strategies and achieve superior performance even in\ncomplex or extreme market scenarios where expert strategies may be suboptimal.\nWe test FlowHFT in multiple market environments. We first show that flow\nmatching policy is applicable in stochastic market environments, thus enabling\nFlowHFT to learn trading strategies under different market conditions. Notably,\nour single framework consistently achieves performance superior to the best\nexpert for each market condition.", "AI": {"tldr": "FlowHFT\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7b56\u7565\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u4ece\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u4e2d\u5b66\u4e60\u4ea4\u6613\u7b56\u7565\uff0c\u5e76\u6839\u636e\u5e02\u573a\u72b6\u6001\u81ea\u9002\u5e94\u8c03\u6574\u6295\u8d44\u51b3\u7b56\u3002", "motivation": "\u4f20\u7edf\u9ad8\u9891\u4ea4\u6613\u6a21\u578b\u4f9d\u8d56\u5386\u53f2\u6570\u636e\uff0c\u5047\u8bbe\u672a\u6765\u5e02\u573a\u72b6\u6001\u7c7b\u4f3c\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002\u73b0\u5b9e\u5e02\u573a\u662f\u52a8\u6001\u591a\u6837\u7684\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FlowHFT\u901a\u8fc7\u6d41\u5339\u914d\u7b56\u7565\u4ece\u591a\u4e13\u5bb6\u6a21\u578b\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u7f51\u683c\u641c\u7d22\u5fae\u8c03\u673a\u5236\uff0c\u63d0\u5347\u4e86\u590d\u6742\u5e02\u573a\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0cFlowHFT\u5728\u4e0d\u540c\u5e02\u573a\u73af\u5883\u4e0b\u8868\u73b0\u5747\u4f18\u4e8e\u5355\u4e00\u4e13\u5bb6\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5176\u9002\u5e94\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "FlowHFT\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4e3a\u9ad8\u9891\u4ea4\u6613\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.06178", "pdf": "https://arxiv.org/pdf/2505.06178", "abs": "https://arxiv.org/abs/2505.06178", "authors": ["Linjiang Cao", "Maonan Wang", "Xi Xiong"], "title": "A Large Language Model-Enhanced Q-learning for Capacitated Vehicle Routing Problem with Time Windows", "categories": ["cs.LG"], "comment": null, "summary": "The Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) is a\nclassic NP-hard combinatorial optimization problem widely applied in logistics\ndistribution and transportation management. Its complexity stems from the\nconstraints of vehicle capacity and time windows, which pose significant\nchallenges to traditional approaches. Advances in Large Language Models (LLMs)\nprovide new possibilities for finding approximate solutions to CVRPTW. This\npaper proposes a novel LLM-enhanced Q-learning framework to address the CVRPTW\nwith real-time emergency constraints. Our solution introduces an adaptive\ntwo-phase training mechanism that transitions from the LLM-guided exploration\nphase to the autonomous optimization phase of Q-network. To ensure reliability,\nwe design a three-tier self-correction mechanism based on the Chain-of-Thought\n(CoT) for LLMs: syntactic validation, semantic verification, and physical\nconstraint enforcement. In addition, we also prioritized replay of the\nexperience generated by LLMs to amplify the regulatory role of LLMs in the\narchitecture. Experimental results demonstrate that our framework achieves a\n7.3\\% average reduction in cost compared to traditional Q-learning, with fewer\ntraining steps required for convergence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u7684Q\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5e26\u6709\u5b9e\u65f6\u7d27\u6025\u7ea6\u675f\u7684CVRPTW\u95ee\u9898\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u4e24\u9636\u6bb5\u8bad\u7ec3\u673a\u5236\u548c\u4e09\u5c42\u81ea\u6821\u6b63\u673a\u5236\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u76f8\u8f83\u4e8e\u4f20\u7edfQ\u5b66\u4e60\u6210\u672c\u964d\u4f4e7.3%\uff0c\u4e14\u6536\u655b\u6240\u9700\u7684\u8bad\u7ec3\u6b65\u9aa4\u66f4\u5c11\u3002", "motivation": "CVRPTW\u4f5c\u4e3a\u4e00\u79cd\u5178\u578b\u7684NP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5176\u590d\u6742\u6027\u53d7\u5230\u8f66\u8f86\u5bb9\u91cf\u548c\u65f6\u95f4\u7a97\u53e3\u7ea6\u675f\u7684\u5f71\u54cd\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u589e\u5f3a\u7684Q\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u4eceLLM\u5f15\u5bfc\u7684\u63a2\u7d22\u9636\u6bb5\u8fc7\u6e21\u5230\u81ea\u4e3b\u4f18\u5316\u7684Q\u7f51\u7edc\u9636\u6bb5\uff09\u548c\u4e09\u5c42\u81ea\u6821\u6b63\u673a\u5236\uff08\u8bed\u6cd5\u9a8c\u8bc1\u3001\u8bed\u4e49\u9a8c\u8bc1\u548c\u7269\u7406\u7ea6\u675f\u6267\u884c\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u4f20\u7edfQ\u5b66\u4e60\u5e73\u5747\u964d\u4f4e\u6210\u672c7.3%\uff0c\u5e76\u4e14\u6536\u655b\u6240\u9700\u7684\u8bad\u7ec3\u6b65\u9aa4\u66f4\u5c11\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0cLLM\u589e\u5f3a\u7684Q\u5b66\u4e60\u6846\u67b6\u5728\u89e3\u51b3CVRPTW\u95ee\u9898\u65f6\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2505.05794", "pdf": "https://arxiv.org/pdf/2505.05794", "abs": "https://arxiv.org/abs/2505.05794", "authors": ["Renjie Li", "Wenjie Wei", "Qi Xin", "Xiaoli Liu", "Sixuan Mao", "Erik Ma", "Zijian Chen", "Malu Zhang", "Haizhou Li", "Zhaoyu Zhang"], "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips", "categories": ["cs.AR", "cs.AI", "cs.NE"], "comment": "36 pages, 22 figures", "summary": "Large language models (LLMs) are rapidly pushing the limits of contemporary\ncomputing hardware. For example, training GPT-3 has been estimated to consume\naround 1300 MWh of electricity, and projections suggest future models may\nrequire city-scale (gigawatt) power budgets. These demands motivate exploration\nof computing paradigms beyond conventional von Neumann architectures. This\nreview surveys emerging photonic hardware optimized for next-generation\ngenerative AI computing. We discuss integrated photonic neural network\narchitectures (e.g., Mach-Zehnder interferometer meshes, lasers,\nwavelength-multiplexed microring resonators) that perform ultrafast matrix\noperations. We also examine promising alternative neuromorphic devices,\nincluding spiking neural network circuits and hybrid spintronic-photonic\nsynapses, which combine memory and processing. The integration of\ntwo-dimensional materials (graphene, TMDCs) into silicon photonic platforms is\nreviewed for tunable modulators and on-chip synaptic elements.\nTransformer-based LLM architectures (self-attention and feed-forward layers)\nare analyzed in this context, identifying strategies and challenges for mapping\ndynamic matrix multiplications onto these novel hardware substrates. We then\ndissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and\nLLaMA, highlighting their architectural similarities and differences. We\nsynthesize state-of-the-art components, algorithms, and integration methods,\nhighlighting key advances and open issues in scaling such systems to mega-sized\nLLM models. We find that photonic computing systems could potentially surpass\nelectronic processors by orders of magnitude in throughput and energy\nefficiency, but require breakthroughs in memory, especially for long-context\nwindows and long token sequences, and in storage of ultra-large datasets.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9762\u5411\u4e0b\u4e00\u4ee3\u751f\u6210\u5f0fAI\u8ba1\u7b97\u7684\u5149\u5b50\u786c\u4ef6\uff0c\u63a2\u8ba8\u4e86\u8d85\u5feb\u77e9\u9635\u8fd0\u7b97\u7684\u5149\u5b50\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u65b0\u578b\u795e\u7ecf\u5f62\u6001\u5668\u4ef6\uff0c\u5206\u6790\u4e86\u5149\u5b50\u8ba1\u7b97\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba1\u7b97\u9700\u6c42\u6fc0\u589e\uff0c\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u9762\u4e34\u80fd\u6548\u74f6\u9888\uff0c\u4e9f\u9700\u63a2\u7d22\u5149\u5b50\u8ba1\u7b97\u7b49\u65b0\u5174\u786c\u4ef6\u4ee5\u63d0\u5347\u80fd\u6548\u548c\u541e\u5410\u91cf\u3002", "method": "\u7efc\u8ff0\u4e86\u96c6\u6210\u5149\u5b50\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5982\u9a6c\u8d6b-\u66fe\u5fb7\u5c14\u5e72\u6d89\u4eea\u7f51\u683c\u3001\u6fc0\u5149\u5668\u3001\u6ce2\u957f\u590d\u7528\u5fae\u73af\u8c10\u632f\u5668\uff09\u548c\u795e\u7ecf\u5f62\u6001\u5668\u4ef6\uff08\u5982\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7535\u8def\u3001\u6df7\u5408\u81ea\u65cb\u5149\u5b50\u7a81\u89e6\uff09\uff0c\u7ed3\u5408\u4e8c\u7ef4\u6750\u6599\uff08\u5982\u77f3\u58a8\u70ef\uff09\u4ee5\u5b9e\u73b0\u53ef\u8c03\u8c10\u8c03\u5236\u5668\u3002\u5206\u6790\u4e86Transformer\u67b6\u6784\u5728\u5149\u5b50\u786c\u4ef6\u4e0a\u7684\u6620\u5c04\u7b56\u7565\u3002", "result": "\u5149\u5b50\u8ba1\u7b97\u7cfb\u7edf\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u4e0a\u53ef\u80fd\u8fdc\u8d85\u7535\u5b50\u5904\u7406\u5668\uff0c\u4f46\u9700\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u957f\u4ee4\u724c\u5e8f\u5217\u7684\u5185\u5b58\u95ee\u9898\u53ca\u8d85\u5927\u6570\u636e\u96c6\u7684\u5b58\u50a8\u6280\u672f\u74f6\u9888\u3002", "conclusion": "\u5149\u5b50\u786c\u4ef6\u4e3aLLM\u7684\u9ad8\u6548\u80fd\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u4f9d\u8d56\u5185\u5b58\u548c\u5b58\u50a8\u6280\u672f\u7684\u7a81\u7834\u3002"}}
{"id": "2505.06185", "pdf": "https://arxiv.org/pdf/2505.06185", "abs": "https://arxiv.org/abs/2505.06185", "authors": ["Kodai Hirata", "Tsuyoshi Okita"], "title": "Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet", "categories": ["cs.LG", "cs.CV"], "comment": "8 pages,4 figures", "summary": "This paper proposes a method MTL-Swin-Unet which is multi-task learning using\ntransformers for classification and semantic segmentation. For\nspurious-correlation problems, this method allows us to enhance the image\nrepresentation with two other image representations: representation obtained by\nsemantic segmentation and representation obtained by image reconstruction. In\nour experiments, the proposed method outperformed in F-value measure than other\nclassifiers when the test data included slices from the same patient (no\ncovariate shift). Similarly, when the test data did not include slices from the\nsame patient (covariate shift setting), the proposed method outperformed in AUC\nmeasure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684MTL-Swin-Unet\u65b9\u6cd5\uff0c\u5229\u7528Transformer\u540c\u65f6\u8fdb\u884c\u5206\u7c7b\u4e0e\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5206\u5272\u4e0e\u56fe\u50cf\u91cd\u6784\u7684\u8868\u793a\u63d0\u5347\u56fe\u50cf\u8868\u5f81\u80fd\u529b\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u6709\u65e0\u534f\u53d8\u91cf\u504f\u79fb\u7684\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u5176\u4ed6\u5206\u7c7b\u5668\u3002", "motivation": "\u89e3\u51b3\u7531\u865a\u5047\u5173\u8054\uff08spurious-correlation\uff09\u5bfc\u81f4\u7684\u56fe\u50cf\u8868\u5f81\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eTransformer\u7684Swin-Unet\u67b6\u6784\uff0c\u540c\u65f6\u6267\u884c\u56fe\u50cf\u5206\u7c7b\u4e0e\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u5206\u5272\u548c\u56fe\u50cf\u91cd\u6784\u7684\u8868\u793a\u4ee5\u589e\u5f3a\u56fe\u50cf\u8868\u5f81\u3002", "result": "\u5728\u65e0\u534f\u53d8\u91cf\u504f\u79fb\u7684\u6d4b\u8bd5\u6570\u636e\uff08\u540c\u60a3\u8005\u5207\u7247\uff09\u4e2dF\u503c\u8868\u73b0\u6700\u4f18\uff1b\u5728\u6709\u534f\u53d8\u91cf\u504f\u79fb\u7684\u6d4b\u8bd5\u6570\u636e\uff08\u4e0d\u540c\u60a3\u8005\u5207\u7247\uff09\u4e2dAUC\u6307\u6807\u9886\u5148\u3002", "conclusion": "MTL-Swin-Unet\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5e94\u5bf9\u6570\u636e\u5206\u5e03\u53d8\u5316\u65f6\u8868\u73b0\u7a33\u5b9a\u3002"}}
{"id": "2505.05796", "pdf": "https://arxiv.org/pdf/2505.05796", "abs": "https://arxiv.org/abs/2505.05796", "authors": ["Xinyu Liang", "Frits de Nijs", "Buser Say", "Hao Wang"], "title": "Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency", "categories": ["eess.SY", "cs.AI", "cs.SY", "math.OC"], "comment": "ACM e-Energy 2025", "summary": "Heating, Ventilation, and Air Conditioning (HVAC) systems account for\napproximately 38% of building energy consumption globally, making them one of\nthe most energy-intensive services. The increasing emphasis on energy\nefficiency and sustainability, combined with the need for enhanced occupant\ncomfort, presents a significant challenge for traditional HVAC systems. These\nsystems often fail to dynamically adjust to real-time changes in electricity\nmarket rates or individual comfort preferences, leading to increased energy\ncosts and reduced comfort. In response, we propose a Human-in-the-Loop (HITL)\nArtificial Intelligence framework that optimizes HVAC performance by\nincorporating real-time user feedback and responding to fluctuating electricity\nprices. Unlike conventional systems that require predefined information about\noccupancy or comfort levels, our approach learns and adapts based on ongoing\nuser input. By integrating the occupancy prediction model with reinforcement\nlearning, the system improves operational efficiency and reduces energy costs\nin line with electricity market dynamics, thereby contributing to demand\nresponse initiatives. Through simulations, we demonstrate that our method\nachieves significant cost reductions compared to baseline approaches while\nmaintaining or enhancing occupant comfort. This feedback-driven approach\nensures personalized comfort control without the need for predefined settings,\noffering a scalable solution that balances individual preferences with economic\nand environmental goals.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u673a\u4ea4\u4e92\uff08HITL\uff09\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316HVAC\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u548c\u7535\u4ef7\u6ce2\u52a8\uff0c\u5b9e\u73b0\u8282\u80fd\u4e0e\u8212\u9002\u5ea6\u7684\u52a8\u6001\u5e73\u8861\u3002", "motivation": "HVAC\u7cfb\u7edf\u5360\u5168\u7403\u5efa\u7b51\u80fd\u8017\u768438%\uff0c\u4f20\u7edf\u7cfb\u7edf\u65e0\u6cd5\u52a8\u6001\u54cd\u5e94\u7535\u4ef7\u53d8\u5316\u6216\u4e2a\u4eba\u8212\u9002\u504f\u597d\uff0c\u5bfc\u81f4\u80fd\u8017\u589e\u52a0\u548c\u8212\u9002\u5ea6\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u8c03\u6574\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b9e\u65f6\u7528\u6237\u53cd\u9988\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6a21\u578b\u4f18\u5316HVAC\u8fd0\u884c\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u4fe1\u606f\u5373\u53ef\u52a8\u6001\u8c03\u6574\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u80fd\u8017\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u8212\u9002\u5ea6\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u79cd\u53cd\u9988\u9a71\u52a8\u7684\u6846\u67b6\u4e3aHVAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u4e2a\u4eba\u504f\u597d\u4e0e\u7ecf\u6d4e\u73af\u4fdd\u76ee\u6807\u3002"}}
{"id": "2505.06203", "pdf": "https://arxiv.org/pdf/2505.06203", "abs": "https://arxiv.org/abs/2505.06203", "authors": ["Hiroki Hasegawa", "Yukihiko Okada"], "title": "Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free Framework for Tensor Denoising", "categories": ["cs.LG"], "comment": "16 pages, 4 figures", "summary": "In modern data-driven tasks such as classification, optimization, and\nforecasting, mitigating the effects of intrinsic noise is crucial for improving\npredictive accuracy. While numerous denoising techniques have been developed,\nthe rising dimensionality of real-world datasets limits conventional\nmatrix-based methods in preserving data structure and accuracy. This challenge\nhas led to increasing interest in tensor-based approaches, which naturally\ncapture multi-way data relationships. However, classical tensor decomposition\nmethods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative\noptimization, making them computationally expensive and less practical. In this\nwork, we propose a novel low-rank approximation method for tensor data that\navoids these limitations. Our approach applies statistically grounded singular\nvalue thresholding to mode-wise matricizations, enabling automatic extraction\nof significant components without requiring prior rank specification or\niterative refinement. Experiments on synthetic and real-world tensors show that\nour method consistently outperforms existing techniques in terms of estimation\naccuracy and computational efficiency, especially in noisy high-dimensional\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u79e9\u5f20\u91cf\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u57fa\u7840\u7684\u5947\u5f02\u503c\u9608\u503c\u5904\u7406\u81ea\u52a8\u63d0\u53d6\u91cd\u8981\u6210\u5206\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u79e9\u6216\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u566a\u58f0\u9ad8\u7ef4\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u968f\u7740\u6570\u636e\u7ef4\u5ea6\u7684\u589e\u52a0\uff0c\u4f20\u7edf\u77e9\u9635\u964d\u566a\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u6301\u6570\u636e\u7ed3\u6784\u548c\u51c6\u786e\u6027\uff0c\u800c\u7ecf\u5178\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u9700\u8981\u9884\u5b9a\u4e49\u79e9\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u5b9e\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7edf\u8ba1\u57fa\u7840\u7684\u5947\u5f02\u503c\u9608\u503c\u5904\u7406\u5bf9\u5f20\u91cf\u6a21\u6001\u77e9\u9635\u5316\uff0c\u81ea\u52a8\u63d0\u53d6\u91cd\u8981\u6210\u5206\uff0c\u907f\u514d\u9884\u5b9a\u4e49\u79e9\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5f20\u91cf\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u9ad8\u7ef4\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u5f20\u91cf\u964d\u566a\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u566a\u58f0\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2505.06224", "pdf": "https://arxiv.org/pdf/2505.06224", "abs": "https://arxiv.org/abs/2505.06224", "authors": ["Christos Plachouras", "Julien Guinot", "George Fazekas", "Elio Quinton", "Emmanouil Benetos", "Johan Pauwels"], "title": "Towards a Unified Representation Evaluation Framework Beyond Downstream Tasks", "categories": ["cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "Downstream probing has been the dominant method for evaluating model\nrepresentations, an important process given the increasing prominence of\nself-supervised learning and foundation models. However, downstream probing\nprimarily assesses the availability of task-relevant information in the model's\nlatent space, overlooking attributes such as equivariance, invariance, and\ndisentanglement, which contribute to the interpretability, adaptability, and\nutility of representations in real-world applications. While some attempts have\nbeen made to measure these qualities in representations, no unified evaluation\nframework with modular, generalizable, and interpretable metrics exists.\n  In this paper, we argue for the importance of representation evaluation\nbeyond downstream probing. We introduce a standardized protocol to quantify\ninformativeness, equivariance, invariance, and disentanglement of factors of\nvariation in model representations. We use it to evaluate representations from\na variety of models in the image and speech domains using different\narchitectures and pretraining approaches on identified controllable factors of\nvariation. We find that representations from models with similar downstream\nperformance can behave substantially differently with regard to these\nattributes. This hints that the respective mechanisms underlying their\ndownstream performance are functionally different, prompting new research\ndirections to understand and improve representations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u4e0b\u6e38\u63a2\u6d4b\u5728\u8bc4\u4f30\u6a21\u578b\u8868\u793a\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u6807\u51c6\u5316\u534f\u8bae\u6765\u91cf\u5316\u4fe1\u606f\u6027\u3001\u7b49\u53d8\u6027\u3001\u4e0d\u53d8\u6027\u548c\u89e3\u8026\u6027\uff0c\u53d1\u73b0\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u8868\u793a\u5c5e\u6027\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u4e0b\u6e38\u63a2\u6d4b\u65b9\u6cd5\u4e3b\u8981\u8bc4\u4f30\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u4f46\u5ffd\u7565\u4e86\u8868\u793a\u7684\u5176\u4ed6\u91cd\u8981\u5c5e\u6027\uff08\u5982\u7b49\u53d8\u6027\u3001\u4e0d\u53d8\u6027\u548c\u89e3\u8026\u6027\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u6807\u51c6\u5316\u534f\u8bae\uff0c\u91cf\u5316\u8868\u793a\u7684\u4fe1\u606f\u6027\u3001\u7b49\u53d8\u6027\u3001\u4e0d\u53d8\u6027\u548c\u89e3\u8026\u6027\uff0c\u5e76\u5728\u56fe\u50cf\u548c\u8bed\u97f3\u9886\u57df\u7684\u591a\u79cd\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u4e0b\u6e38\u6027\u80fd\u76f8\u4f3c\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u5c5e\u6027\u4e0a\u8868\u73b0\u663e\u8457\u4e0d\u540c\uff0c\u63d0\u793a\u5176\u6027\u80fd\u673a\u5236\u5728\u529f\u80fd\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u8868\u793a\u8bc4\u4f30\u9700\u8d85\u8d8a\u4e0b\u6e38\u63a2\u6d4b\uff0c\u65b0\u7684\u7814\u7a76\u65b9\u5411\u5e94\u5173\u6ce8\u8868\u793a\u5c5e\u6027\u7684\u7406\u89e3\u4e0e\u6539\u8fdb\u3002"}}
{"id": "2505.05849", "pdf": "https://arxiv.org/pdf/2505.05849", "abs": "https://arxiv.org/abs/2505.05849", "authors": ["Zhun Wang", "Vincent Siu", "Zhe Ye", "Tianneng Shi", "Yuzhou Nie", "Xuandong Zhao", "Chenguang Wang", "Wenbo Guo", "Dawn Song"], "title": "AgentXploit: End-to-End Redteaming of Black-Box AI Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The strong planning and reasoning capabilities of Large Language Models\n(LLMs) have fostered the development of agent-based systems capable of\nleveraging external tools and interacting with increasingly complex\nenvironments. However, these powerful features also introduce a critical\nsecurity risk: indirect prompt injection, a sophisticated attack vector that\ncompromises the core of these agents, the LLM, by manipulating contextual\ninformation rather than direct user prompts. In this work, we propose a generic\nblack-box fuzzing framework, AgentXploit, designed to automatically discover\nand exploit indirect prompt injection vulnerabilities across diverse LLM\nagents. Our approach starts by constructing a high-quality initial seed corpus,\nthen employs a seed selection algorithm based on Monte Carlo Tree Search (MCTS)\nto iteratively refine inputs, thereby maximizing the likelihood of uncovering\nagent weaknesses. We evaluate AgentXploit on two public benchmarks, AgentDojo\nand VWA-adv, where it achieves 71% and 70% success rates against agents based\non o3-mini and GPT-4o, respectively, nearly doubling the performance of\nbaseline attacks. Moreover, AgentXploit exhibits strong transferability across\nunseen tasks and internal LLMs, as well as promising results against defenses.\nBeyond benchmark evaluations, we apply our attacks in real-world environments,\nsuccessfully misleading agents to navigate to arbitrary URLs, including\nmalicious sites.", "AI": {"tldr": "AgentXploit\u662f\u4e00\u4e2a\u9ed1\u76d2\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u5e76\u5229\u7528LLM\u4ee3\u7406\u4e2d\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u5f3a\u5927\u529f\u80fd\u5e26\u6765\u4e86\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\u6765\u53d1\u73b0\u548c\u5229\u7528\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u521d\u59cb\u79cd\u5b50\u5e93\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u79cd\u5b50\u9009\u62e9\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u8f93\u5165\uff0c\u4ee5\u6700\u5927\u5316\u53d1\u73b0\u6f0f\u6d1e\u7684\u6982\u7387\u3002", "result": "\u5728AgentDojo\u548cVWA-adv\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u8fbe\u523071%\u548c70%\uff0c\u4e14\u5177\u6709\u5f3a\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u578b\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "AgentXploit\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u64cd\u63a7\u4ee3\u7406\u884c\u4e3a\uff0c\u7a81\u663e\u4e86\u5176\u5b89\u5168\u5a01\u80c1\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2503.03137", "pdf": "https://arxiv.org/pdf/2503.03137", "abs": "https://arxiv.org/abs/2503.03137", "authors": ["Changliang Zhou", "Xi Lin", "Zhenkun Wang", "Qingfu Zhang"], "title": "L2R: Learning to Reduce Search Space for Generalizable Neural Routing Solver", "categories": ["cs.AI", "cs.LG", "cs.NE"], "comment": "23 pages, 10 figures", "summary": "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u641c\u7d22\u7a7a\u95f4\u7f29\u51cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\uff08NCO\uff09\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709NCO\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u96be\u4ee5\u9ad8\u6548\u6355\u6349\u7ed3\u6784\u6a21\u5f0f\uff0c\u56e0\u6b64\u9700\u6539\u8fdb\u5176\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5019\u9009\u8282\u70b9\u7684\u5b66\u4e60\u6a21\u578b\uff0c\u7f29\u51cf\u641c\u7d22\u7a7a\u95f4\uff0c\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u57fa\u4e8e100\u8282\u70b9\u8bad\u7ec3\uff0c\u5373\u53ef\u6cdb\u5316\u81f3\u767e\u4e07\u7ea7TSP\u548cCVRP\u95ee\u9898\uff0c\u4e14\u4fdd\u6301\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NCO\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u5c55\u73b0\u4e86\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.05478", "pdf": "https://arxiv.org/pdf/2505.05478", "abs": "https://arxiv.org/abs/2505.05478", "authors": ["Yufei Zhang", "Andrew Sonta"], "title": "OccuEMBED: Occupancy Extraction Merged with Building Energy Disaggregation for Occupant-Responsive Operation at Scale", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY"], "comment": "33 pages, 16 figures", "summary": "Buildings account for a significant share of global energy consumption and\nemissions, making it critical to operate them efficiently. As electricity grids\nbecome more volatile with renewable penetration, buildings must provide\nflexibility to support grid stability. Building automation plays a key role in\nenhancing efficiency and flexibility via centralized operations, but it must\nprioritize occupant-centric strategies to balance energy and comfort targets.\nHowever, incorporating occupant information into large-scale, centralized\nbuilding operations remains challenging due to data limitations. We investigate\nthe potential of using whole-building smart meter data to infer both occupancy\nand system operations. Integrating these insights into data-driven building\nenergy analysis allows more occupant-centric energy-saving and flexibility at\nscale. Specifically, we propose OccuEMBED, a unified framework for occupancy\ninference and system-level load analysis. It combines two key components: a\nprobabilistic occupancy profile generator, and a controllable and interpretable\nload disaggregator supported by Kolmogorov-Arnold Networks (KAN). This design\nembeds knowledge of occupancy patterns and load-occupancy-weather relationships\ninto deep learning models. We conducted comprehensive evaluations to\ndemonstrate its effectiveness across synthetic and real-world datasets compared\nto various occupancy inference baselines. OccuEMBED always achieved average F1\nscores above 0.8 in discrete occupancy inference and RMSE within 0.1-0.2 for\ncontinuous occupancy ratios. We further demonstrate how OccuEMBED integrates\nwith building load monitoring platforms to display occupancy profiles, analyze\nsystem-level operations, and inform occupant-responsive strategies. Our model\nlays a robust foundation in scaling occupant-centric building management\nsystems to meet the challenges of an evolving energy system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OccuEMBED\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u7535\u8868\u6570\u636e\u63a8\u65ad\u5efa\u7b51\u5360\u7528\u60c5\u51b5\u548c\u7cfb\u7edf\u64cd\u4f5c\uff0c\u4ee5\u63d0\u5347\u80fd\u6548\u548c\u5c45\u4f4f\u8005\u8212\u9002\u5ea6\u3002\u7ed3\u5408\u6982\u7387\u5360\u7528\u751f\u6210\u5668\u548cKAN\u652f\u6301\u7684\u8d1f\u8f7d\u5206\u89e3\u5668\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5efa\u7b51\u5360\u5168\u7403\u80fd\u8017\u548c\u6392\u653e\u7684\u5f88\u5927\u6bd4\u4f8b\uff0c\u9700\u4f18\u5316\u8fd0\u8425\u4ee5\u652f\u6301\u7535\u7f51\u7a33\u5b9a\u3002\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u5c45\u4f4f\u8005\u4fe1\u606f\u7684\u6574\u5408\uff0c\u96be\u4ee5\u5e73\u8861\u80fd\u6548\u4e0e\u8212\u9002\u5ea6\u3002", "method": "\u63d0\u51faOccuEMBED\u6846\u67b6\uff0c\u5305\u542b\u6982\u7387\u5360\u7528\u751f\u6210\u5668\u548c\u57fa\u4e8eKAN\u7684\u8d1f\u8f7d\u5206\u89e3\u5668\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5d4c\u5165\u5360\u7528\u6a21\u5f0f\u548c\u8d1f\u8f7d-\u5360\u7528-\u5929\u6c14\u5173\u7cfb\u3002", "result": "\u5728\u79bb\u6563\u5360\u7528\u63a8\u65ad\u4e2d\u5e73\u5747F1\u5206\u6570>0.8\uff0c\u8fde\u7eed\u5360\u7528\u6bd4RMSE\u4e3a0.1-0.2\u3002\u53ef\u96c6\u6210\u81f3\u8d1f\u8f7d\u76d1\u63a7\u5e73\u53f0\uff0c\u652f\u6301\u5c45\u4f4f\u8005\u54cd\u5e94\u7b56\u7565\u3002", "conclusion": "OccuEMBED\u4e3a\u89c4\u6a21\u5316\u5c45\u4f4f\u8005-centric\u5efa\u7b51\u7ba1\u7406\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u9002\u5e94\u80fd\u6e90\u7cfb\u7edf\u53d8\u9769\u6311\u6218\u3002"}}
{"id": "2505.05479", "pdf": "https://arxiv.org/pdf/2505.05479", "abs": "https://arxiv.org/abs/2505.05479", "authors": ["Finn Gueterbock", "Raul Santos-Rodriguez", "Jeffrey N. Clark"], "title": "Improving Local Air Quality Predictions Using Transfer Learning on Satellite Data and Graph Neural Networks", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Air pollution is a significant global health risk, contributing to millions\nof premature deaths annually. Nitrogen dioxide (NO2), a harmful pollutant,\ndisproportionately affects urban areas where monitoring networks are often\nsparse. We propose a novel method for predicting NO2 concentrations at\nunmonitored locations using transfer learning with satellite and meteorological\ndata. Leveraging the GraphSAGE framework, our approach integrates\nautoregression and transfer learning to enhance predictive accuracy in\ndata-scarce regions like Bristol. Pre-trained on data from London, UK, our\nmodel achieves a 8.6% reduction in Normalised Root Mean Squared Error (NRMSE)\nand a 32.6% reduction in Gradient RMSE compared to a baseline model. This work\ndemonstrates the potential of virtual sensors for cost-effective air quality\nmonitoring, contributing to actionable insights for climate and health\ninterventions.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u536b\u661f\u548c\u6c14\u8c61\u6570\u636e\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548cGraphSAGE\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u76d1\u6d4b\u7a00\u758f\u7684\u57ce\u533a\u9884\u6d4bNO2\u6d53\u5ea6\u3002\u6a21\u578b\u5728\u4f26\u6566\u6570\u636e\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u5e03\u91cc\u65af\u6258\u5c14\u5b9e\u73b0\u4e86\u6bd4\u57fa\u7ebf\u6a21\u578b\u66f4\u4f4e\u7684\u8bef\u5dee\u3002", "motivation": "\u7a7a\u6c14\u6c61\u67d3\u662f\u5168\u7403\u5065\u5eb7\u7684\u91cd\u8981\u5a01\u80c1\uff0c\u5c24\u5176NO2\u5728\u76d1\u6d4b\u7a00\u758f\u7684\u57ce\u533a\u5f71\u54cd\u663e\u8457\u3002\u73b0\u6709\u76d1\u6d4b\u7f51\u7edc\u96be\u4ee5\u8986\u76d6\u6240\u6709\u533a\u57df\uff0c\u4e9f\u9700\u4f4e\u6210\u672c\u9ad8\u7cbe\u5ea6\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548cGraphSAGE\u6846\u67b6\uff0c\u5229\u7528\u536b\u661f\u548c\u6c14\u8c61\u6570\u636e\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u548c\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u5730\u533a\u5982\u5e03\u91cc\u65af\u6258\u5c14\u3002", "result": "\u6a21\u578b\u5728\u4f26\u6566\u6570\u636e\u9884\u8bad\u7ec3\u540e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0cNRMSE\u964d\u4f4e8.6%\uff0c\u68af\u5ea6RMSE\u964d\u4f4e32.6%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u865a\u62df\u4f20\u611f\u5668\u5728\u4f4e\u6210\u672c\u7a7a\u6c14\u8d28\u91cf\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6c14\u5019\u548c\u5065\u5eb7\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.05870", "pdf": "https://arxiv.org/pdf/2505.05870", "abs": "https://arxiv.org/abs/2505.05870", "authors": ["Yimin Zhou", "Yichong Xia", "Bin Chen", "Baoyi An", "Haoqian Wang", "Zhi Wang", "Yaowei Wang", "Zikun Zhou"], "title": "Towards Facial Image Compression with Consistency Preserving Diffusion Prior", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "With the widespread application of facial image data across various domains,\nthe efficient storage and transmission of facial images has garnered\nsignificant attention. However, the existing learned face image compression\nmethods often produce unsatisfactory reconstructed image quality at low bit\nrates. Simply adapting diffusion-based compression methods to facial\ncompression tasks results in reconstructed images that perform poorly in\ndownstream applications due to insufficient preservation of high-frequency\ninformation. To further explore the diffusion prior in facial image\ncompression, we propose Facial Image Compression with a Stable Diffusion Prior\n(FaSDiff), a method that preserves consistency through frequency enhancement.\nFaSDiff employs a high-frequency-sensitive compressor in an end-to-end\nframework to capture fine image details and produce robust visual prompts.\nAdditionally, we introduce a hybrid low-frequency enhancement module that\ndisentangles low-frequency facial semantics and stably modulates the diffusion\nprior alongside visual prompts. The proposed modules allow FaSDiff to leverage\ndiffusion priors for superior human visual perception while minimizing\nperformance loss in machine vision due to semantic inconsistency. Extensive\nexperiments show that FaSDiff outperforms state-of-the-art methods in balancing\nhuman visual quality and machine vision accuracy. The code will be released\nafter the paper is accepted.", "AI": {"tldr": "\u63d0\u51faFaSDiff\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u7a33\u5b9a\u6269\u6563\u5148\u9a8c\uff0c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u63d0\u5347\u4eba\u8138\u56fe\u50cf\u538b\u7f29\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u91cd\u5efa\u8d28\u91cf\u4e0d\u4f73\uff0c\u4e14\u76f4\u63a5\u5e94\u7528\u6269\u6563\u65b9\u6cd5\u4f1a\u56e0\u9ad8\u9891\u4fe1\u606f\u4e22\u5931\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "FaSDiff\u7ed3\u5408\u9ad8\u9891\u654f\u611f\u538b\u7f29\u5668\u548c\u6df7\u5408\u4f4e\u9891\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u6846\u67b6\u6355\u83b7\u7ec6\u8282\u5e76\u7a33\u5b9a\u8c03\u5236\u6269\u6563\u5148\u9a8c\u3002", "result": "FaSDiff\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u673a\u5668\u89c6\u89c9\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FaSDiff\u6210\u529f\u5e73\u8861\u4e86\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u4e0e\u673a\u5668\u89c6\u89c9\u9700\u6c42\uff0c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2505.05485", "pdf": "https://arxiv.org/pdf/2505.05485", "abs": "https://arxiv.org/abs/2505.05485", "authors": ["Antonio Arauzo-Azofra", "Jose Molina-Baena", "Maria Luque-Rodriguez"], "title": "Evolutionary Optimization for the Classification of Small Molecules Regulating the Circadian Rhythm Period: A Reliable Assessment", "categories": ["cs.NE", "cs.LG"], "comment": "13 pages, 5 figures, 8 tables. To be published", "summary": "The circadian rhythm plays a crucial role in regulating biological processes,\nand its disruption is linked to various health issues. Identifying small\nmolecules that influence the circadian period is essential for developing\ntargeted therapies. This study explores the use of evolutionary optimization\ntechniques to enhance the classification of these molecules. We applied an\nevolutionary algorithm to optimize feature selection and classification\nperformance. Several machine learning classifiers were employed, and\nperformance was evaluated using accuracy and generalization ability. The\nfindings demonstrate that the proposed evolutionary optimization method\nimproves classification accuracy and reduces overfitting compared to baseline\nmodels. Additionally, the use of variance in accuracy as a penalty factor may\nenhance the model's reliability for real-world applications. Our study confirms\nthat evolutionary optimization is an effective strategy for classifying small\nmolecules regulating the circadian rhythm. The proposed approach not only\nimproves predictive performance but also ensures a more robust model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u8fdb\u5316\u4f18\u5316\u6280\u672f\u63d0\u5347\u5c0f\u5206\u5b50\u5206\u7c7b\u6027\u80fd\uff0c\u6539\u8fdb\u751f\u7269\u949f\u8c03\u8282\u5206\u5b50\u7684\u8bc6\u522b\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u548c\u51cf\u5c11\u8fc7\u62df\u5408\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8bc6\u522b\u5f71\u54cd\u751f\u7269\u949f\u5468\u671f\u7684\u5c0f\u5206\u5b50\u5bf9\u9776\u5411\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u63a2\u7d22\u8fdb\u5316\u4f18\u5316\u6280\u672f\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u7279\u5f81\u9009\u62e9\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u5e94\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5e76\u901a\u8fc7\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u8fdb\u5316\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u65b9\u5dee\u60e9\u7f5a\u673a\u5236\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u53ef\u9760\u6027\u3002", "conclusion": "\u8fdb\u5316\u4f18\u5316\u662f\u5206\u7c7b\u751f\u7269\u949f\u8c03\u8282\u5c0f\u5206\u5b50\u7684\u6709\u6548\u7b56\u7565\uff0c\u517c\u5177\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u548c\u6a21\u578b\u7a33\u5065\u6027\u3002"}}
{"id": "2505.05893", "pdf": "https://arxiv.org/pdf/2505.05893", "abs": "https://arxiv.org/abs/2505.05893", "authors": ["Seunghee Han", "Soongyu Choi", "Joo-Young Kim"], "title": "LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.LG", "B.7; I.2; J.3"], "comment": "To appear in the Proceedings of the 52nd IEEE/ACM International\n  Symposium on Computer Architecture (ISCA 2025)", "summary": "Recent advances in Protein Structure Prediction Models (PPMs), such as\nAlphaFold2 and ESMFold, have revolutionized computational biology by achieving\nunprecedented accuracy in predicting three-dimensional protein folding\nstructures. However, these models face significant scalability challenges,\nparticularly when processing proteins with long amino acid sequences (e.g.,\nsequence length > 1,000). The primary bottleneck that arises from the\nexponential growth in activation sizes is driven by the unique data structure\nin PPM, which introduces an additional dimension that leads to substantial\nmemory and computational demands. These limitations have hindered the effective\nscaling of PPM for real-world applications, such as analyzing large proteins or\ncomplex multimers with critical biological and pharmaceutical relevance.\n  In this paper, we present LightNobel, the first hardware-software co-designed\naccelerator developed to overcome scalability limitations on the sequence\nlength in PPM. At the software level, we propose Token-wise Adaptive Activation\nQuantization (AAQ), which leverages unique token-wise characteristics, such as\ndistogram patterns in PPM activations, to enable fine-grained quantization\ntechniques without compromising accuracy. At the hardware level, LightNobel\nintegrates the multi-precision reconfigurable matrix processing unit (RMPU) and\nversatile vector processing unit (VVPU) to enable the efficient execution of\nAAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup\nand 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100\nGPUs, respectively, while maintaining negligible accuracy loss. It also reduces\nthe peak memory requirement up to 120.05x in PPM, enabling scalable processing\nfor proteins with long sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e86LightNobel\uff0c\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u6a21\u578b\uff08PPM\uff09\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u91cf\u5316\u6280\u672f\u548c\u786c\u4ef6\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "\u76ee\u524d\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u6a21\u578b\uff08\u5982AlphaFold2\u548cESMFold\uff09\u5728\u5904\u7406\u957f\u6c28\u57fa\u9178\u5e8f\u5217\u65f6\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u5de8\u5927\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86Token-wise Adaptive Activation Quantization (AAQ)\u8f6f\u4ef6\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u591a\u7cbe\u5ea6\u53ef\u91cd\u6784\u77e9\u9635\u5904\u7406\u5355\u5143\uff08RMPU\uff09\u548c\u591a\u529f\u80fd\u5411\u91cf\u5904\u7406\u5355\u5143\uff08VVPU\uff09\u7684\u786c\u4ef6\u8bbe\u8ba1\u3002", "result": "LightNobel\u76f8\u6bd4NVIDIA A100\u548cH100 GPU\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad88.44\u500d\u548c8.41\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4ee5\u53ca37.29\u500d\u548c43.35\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u5cf0\u503c\u5185\u5b58\u9700\u6c42\u964d\u4f4e\u4e86120.05\u500d\u3002", "conclusion": "LightNobel\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86PPM\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u957f\u5e8f\u5217\u86cb\u767d\u8d28\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05489", "pdf": "https://arxiv.org/pdf/2505.05489", "abs": "https://arxiv.org/abs/2505.05489", "authors": ["Alberto Morando"], "title": "Akkumula: Evidence accumulation driver models with Spiking Neural Networks", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Processes of evidence accumulation for motor control contribute to the\necological validity of driver models. According to established theories of\ncognition, drivers make control adjustments when a process of accumulation of\nperceptual inputs reaches a decision boundary. Unfortunately, there is not a\nstandard way for building such models, limiting their use. Current\nimplementations are hand-crafted, lack adaptability, and rely on inefficient\noptimization techniques that do not scale well with large datasets. This paper\nintroduces Akkumula, an evidence accumulation modelling framework built using\ndeep learning techniques to leverage established coding libraries, gradient\noptimization, and large batch training. The core of the library is based on\nSpiking Neural Networks, whose operation mimic the evidence accumulation\nprocess in the biological brain. The model was tested on data collected during\na test-track experiment. Results are promising. The model fits well the time\ncourse of vehicle control (brake, accelerate, steering) based on vehicle sensor\ndata. The perceptual inputs are extracted by a dedicated neural network,\nincreasing the context-awareness of the model in dynamic scenarios. Akkumula\nintegrates with existing machine learning architectures, benefits from\ncontinuous advancements in deep learning, efficiently processes large datasets,\nadapts to diverse driving scenarios, and maintains a degree of transparency in\nits core mechanisms.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Akkumula\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6a21\u62df\u9a7e\u9a76\u5458\u611f\u77e5\u4fe1\u606f\u7d2f\u79ef\u8fc7\u7a0b\uff0c\u4ee5\u6539\u8fdb\u8f66\u8f86\u63a7\u5236\u7684\u751f\u6001\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u9a7e\u9a76\u5458\u6a21\u578b\u7f3a\u4e4f\u6807\u51c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5bfc\u81f4\u9002\u5e94\u6027\u4e0d\u8db3\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8eSpiking Neural Networks\u6784\u5efaAkkumula\u6846\u67b6\uff0c\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u548c\u5927\u6279\u91cf\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u611f\u77e5\u8f93\u5165\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u573a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u80fd\u51c6\u786e\u62df\u5408\u8f66\u8f86\u63a7\u5236\uff08\u5239\u8f66\u3001\u52a0\u901f\u3001\u8f6c\u5411\uff09\u7684\u65f6\u95f4\u8fc7\u7a0b\u3002", "conclusion": "Akkumula\u4e0d\u4ec5\u517c\u5bb9\u73b0\u6709\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u8fd8\u80fd\u9ad8\u6548\u5904\u7406\u5927\u6570\u636e\u96c6\u5e76\u9002\u5e94\u591a\u6837\u5316\u9a7e\u9a76\u573a\u666f\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u673a\u5236\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2505.05895", "pdf": "https://arxiv.org/pdf/2505.05895", "abs": "https://arxiv.org/abs/2505.05895", "authors": ["Benjamin Raphael Ernhofer", "Daniil Prokhorov", "Jannica Langner", "Dominik Bollmann"], "title": "Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern automotive infotainment systems require intelligent and adaptive\nsolutions to handle frequent User Interface (UI) updates and diverse design\nvariations. We introduce a vision-language framework for understanding and\ninteracting with automotive infotainment systems, enabling seamless adaptation\nacross different UI designs. To further support research in this field, we\nrelease AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208\nannotations. Additionally, we present a synthetic data pipeline to generate\ntraining data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation\n(LoRa) and incorporating reasoning generated by our pipeline, along with visual\ngrounding and evaluation capabilities. The fine-tuned Evaluative Large Action\nModel (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and\ndataset are available on Hugging Face) and demonstrating strong cross-domain\ngeneralization, including a +5.2% improvement on ScreenSpot over the baseline\nmodel. Notably, our approach achieves 80.4% average accuracy on ScreenSpot,\nclosely matching or even surpassing specialized models for desktop, mobile, and\nweb, such as ShowUI, despite being trained for the infotainment domain. This\nresearch investigates how data collection and subsequent fine-tuning can lead\nto AI-driven progress within automotive UI understanding and interaction. The\napplied method is cost-efficient and fine-tuned models can be deployed on\nconsumer-grade GPUs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\u7684\u6c7d\u8f66\u4fe1\u606f\u5a31\u4e50\u7cfb\u7edf\u7406\u89e3\u4e0e\u4ea4\u4e92\u65b9\u6cd5\uff0c\u53d1\u5e03\u4e86\u5f00\u6e90\u6570\u636e\u96c6AutomotiveUI-Bench-4K\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u5fae\u8c03\u6a21\u578bELAM\u5728\u8de8\u9886\u57df\u6cdb\u5316\u4e0a\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u6570\u636e\u6536\u96c6\u548c\u5fae\u8c03\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u6c7d\u8f66\u4fe1\u606f\u5a31\u4e50\u7cfb\u7edf\u9891\u7e41\u7684\u7528\u6237\u754c\u9762\u66f4\u65b0\u548c\u591a\u6837\u5316\u8bbe\u8ba1\u9700\u6c42\uff0c\u63a8\u52a8AI\u5728\u8be5\u9886\u57df\u7684\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u91c7\u7528\u89c6\u89c9-\u8bed\u8a00\u6846\u67b6\uff0c\u7ed3\u5408\u5f00\u6e90\u6570\u636e\u96c6\u548c\u5408\u6210\u6570\u636e\u7ba1\u9053\uff0c\u5229\u7528LoRa\u5fae\u8c03Molmo-7B\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u5b9a\u4f4d\u548c\u8bc4\u4f30\u80fd\u529b\uff0c\u6700\u7ec8\u5f97\u5230ELAM\u6a21\u578b\u3002", "result": "ELAM\u5728AutomotiveUI-Bench-4K\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cScreenSpot\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u7387\u8fbe80.4%\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u63d0\u53475.2%\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u6c7d\u8f66UI\u7406\u89e3\u4e0e\u4ea4\u4e92\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u4e14\u65b9\u6cd5\u6210\u672c\u4f4e\u5ec9\uff0c\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7GPU\u90e8\u7f72\u3002"}}
{"id": "2505.05901", "pdf": "https://arxiv.org/pdf/2505.05901", "abs": "https://arxiv.org/abs/2505.05901", "authors": ["Hanzhe Liang", "Aoran Wang", "Jie Zhou", "Xin Jin", "Can Gao", "Jinbao Wang"], "title": "Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages", "summary": "In this paper, we go beyond identifying anomalies only in structural terms\nand think about better anomaly detection motivated by anomaly causes. Most\nanomalies are regarded as the result of unpredictable defective forces from\ninternal and external sources, and their opposite forces are sought to correct\nthe anomalies. We introduced a Mechanics Complementary framework for 3D anomaly\ndetection (MC4AD) to generate internal and external Corrective forces for each\npoint. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to\nsimulate various anomalies. Then, we present a Corrective Force Prediction\nNetwork (CFP-Net) with complementary representations for point-level\nrepresentation to simulate the different contributions of internal and external\ncorrective forces. A combined loss was proposed, including a new symmetric loss\nand an overall loss, to constrain the corrective forces properly. As a\nhighlight, we consider 3D anomaly detection in industry more comprehensively,\ncreating a hierarchical quality control strategy based on a three-way decision\nand contributing a dataset named Anomaly-IntraVariance with intraclass variance\nto evaluate the model. On the proposed and existing five datasets, we obtained\nnine state-of-the-art performers with the minimum parameters and the fastest\ninference speed. The source is available at\nhttps://github.com/hzzzzzhappy/MC4AD", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u5e38\u539f\u56e0\u76843D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff08MC4AD\uff09\uff0c\u901a\u8fc7\u529b\u5b66\u4e92\u8865\u6846\u67b6\u751f\u6210\u5185\u5916\u90e8\u7ea0\u6b63\u529b\uff0c\u7ed3\u5408\u591a\u6837\u5316\u5f02\u5e38\u751f\u6210\u6a21\u5757\u548c\u7ea0\u6b63\u529b\u9884\u6d4b\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8f7b\u91cf\u5316\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b\u591a\u5173\u6ce8\u7ed3\u6784\u6027\u5f02\u5e38\uff0c\u800c\u672c\u6587\u4ece\u5f02\u5e38\u6210\u56e0\u51fa\u53d1\uff0c\u901a\u8fc7\u6a21\u62df\u5185\u5916\u90e8\u7ea0\u6b63\u529b\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86MC4AD\u6846\u67b6\uff0c\u5305\u62ecDA-Gen\u6a21\u5757\u751f\u6210\u591a\u6837\u5316\u5f02\u5e38\uff0cCFP-Net\u9884\u6d4b\u7ea0\u6b63\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5bf9\u79f0\u635f\u5931\u548c\u6574\u4f53\u635f\u5931\u7684\u7ec4\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u81ea\u5efa\u548c\u73b0\u6709\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e5d\u9879\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6a21\u578b\u53c2\u6570\u6700\u5c11\u4e14\u63a8\u7406\u901f\u5ea6\u6700\u5feb\u3002", "conclusion": "MC4AD\u4e0d\u4ec5\u9ad8\u6548\u8f7b\u91cf\u5316\uff0c\u8fd8\u901a\u8fc7\u5206\u5c42\u8d28\u91cf\u63a7\u5236\u7b56\u7565\u548c\u4e09\u652f\u51b3\u7b56\u62d3\u5c55\u4e86\u5de5\u4e1a\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.05510", "pdf": "https://arxiv.org/pdf/2505.05510", "abs": "https://arxiv.org/abs/2505.05510", "authors": ["Thomas Sommariva", "Simone Calderara", "Angelo Porrello"], "title": "How to Train Your Metamorphic Deep Neural Network", "categories": ["cs.NE", "cs.CV", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural\nnetworks of varying width and depth. Based on Implicit Neural Representation\n(INR), NeuMeta learns a continuous weight manifold, enabling the direct\ngeneration of compressed models, including those with configurations not seen\nduring training. While promising, the original formulation of NeuMeta proves\neffective only for the final layers of the undelying model, limiting its\nbroader applicability. In this work, we propose a training algorithm that\nextends the capabilities of NeuMeta to enable full-network metamorphosis with\nminimal accuracy degradation. Our approach follows a structured recipe\ncomprising block-wise incremental training, INR initialization, and strategies\nfor replacing batch normalization. The resulting metamorphic networks maintain\ncompetitive accuracy across a wide range of compression ratios, offering a\nscalable solution for adaptable and efficient deployment of deep models. The\ncode is available at: https://github.com/TSommariva/HTTY_NeuMeta.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Neural Metamorphosis (NeuMeta)\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u589e\u91cf\u8bad\u7ec3\u3001INR\u521d\u59cb\u5316\u548c\u6279\u91cf\u5f52\u4e00\u5316\u66ff\u6362\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5168\u7f51\u7edc\u53d8\u5f62\uff0c\u51cf\u5c11\u4e86\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u539f\u59cb\u7684NeuMeta\u65b9\u6cd5\u4ec5\u5728\u6a21\u578b\u7684\u6700\u540e\u4e00\u5c42\u6709\u6548\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u6269\u5c55NeuMeta\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u6574\u4e2a\u7f51\u7edc\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u7cbe\u5ea6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u5757\u589e\u91cf\u8bad\u7ec3\u3001INR\u521d\u59cb\u5316\u548c\u6279\u91cf\u5f52\u4e00\u5316\u66ff\u6362\u7b56\u7565\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u5171\u540c\u5b9e\u73b0\u4e86\u5168\u7f51\u7edc\u53d8\u5f62\u3002", "result": "\u6539\u8fdb\u540e\u7684NeuMeta\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u538b\u7f29\u6bd4\u8303\u56f4\u5185\u4fdd\u6301\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u7075\u6d3b\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0cNeuMeta\u7684\u80fd\u529b\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u5b9e\u73b0\u5168\u7f51\u7edc\u53d8\u5f62\uff0c\u4e14\u5728\u7cbe\u5ea6\u4e0a\u4ec5\u6709\u6700\u5c0f\u7a0b\u5ea6\u7684\u4e0b\u964d\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9002\u5e94\u6027\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2505.05943", "pdf": "https://arxiv.org/pdf/2505.05943", "abs": "https://arxiv.org/abs/2505.05943", "authors": ["Maan Alhazmi", "Abdulrahman Altahhan"], "title": "Achieving 3D Attention via Triplet Squeeze and Excitation Block", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of ConvNeXt and its variants has reaffirmed the conceptual and\nstructural suitability of CNN-based models for vision tasks, re-establishing\nthem as key players in image classification in general, and in facial\nexpression recognition (FER) in particular. In this paper, we propose a new set\nof models that build on these advancements by incorporating a new set of\nattention mechanisms that combines Triplet attention with\nSqueeze-and-Excitation (TripSE) in four different variants. We demonstrate the\neffectiveness of these variants by applying them to the ResNet18, DenseNet and\nConvNext architectures to validate their versatility and impact. Our study\nshows that incorporating a TripSE block in these CNN models boosts their\nperformances, particularly for the ConvNeXt architecture, indicating its\nutility. We evaluate the proposed mechanisms and associated models across four\ndatasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where\nConvNext with TripSE achieves state-of-the-art results with an accuracy of\n\\textbf{78.27\\%} on the popular FER2013 dataset, a new feat for this dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Triplet\u6ce8\u610f\u529b\u548cSqueeze-and-Excitation\uff08TripSE\uff09\u7684\u65b0\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5728ResNet18\u3001DenseNet\u548cConvNeXt\u67b6\u6784\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728ConvNeXt\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5728FER2013\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8678.27%\u7684\u51c6\u786e\u7387\u3002", "motivation": "ConvNeXt\u53ca\u5176\u53d8\u4f53\u7684\u51fa\u73b0\u8bc1\u660e\u4e86CNN\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236TripSE\uff0c\u4ee5\u589e\u5f3aCNN\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u7ed3\u5408Triplet\u6ce8\u610f\u529b\u548cSqueeze-and-Excitation\uff08TripSE\uff09\u7684\u53d8\u4f53\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eResNet18\u3001DenseNet\u548cConvNeXt\u67b6\u6784\u4e2d\uff0c\u9a8c\u8bc1\u5176\u666e\u9002\u6027\u548c\u6548\u679c\u3002", "result": "TripSE\u663e\u8457\u63d0\u5347\u4e86CNN\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728ConvNeXt\u67b6\u6784\u4e0a\u8868\u73b0\u6700\u4f73\u3002\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08CIFAR100\u3001ImageNet\u3001FER2013\u548cAffectNet\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cConvNeXt+TripSE\u5728FER2013\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8678.27%\u7684\u51c6\u786e\u7387\uff0c\u5237\u65b0\u4e86\u8be5\u6570\u636e\u96c6\u7684\u8bb0\u5f55\u3002", "conclusion": "TripSE\u673a\u5236\u7684\u5f15\u5165\u6709\u6548\u63d0\u5347\u4e86CNN\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0cConvNeXt+TripSE\u5c55\u73b0\u4e86\u76ee\u524d\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002\u8fd9\u4e00\u65b9\u6cd5\u4e3aCNN\u6a21\u578b\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.05511", "pdf": "https://arxiv.org/pdf/2505.05511", "abs": "https://arxiv.org/abs/2505.05511", "authors": ["Yanghui Song", "Aoqi Li", "Lilei Huo"], "title": "Economic Analysis and Optimization of Energy Storage Configuration for Park Power Systems Based on Random Forest and Genetic Algorithm", "categories": ["cs.NE", "cs.LG", "cs.SI"], "comment": "8 pages, 8 figures,International Journal of New Developments in\n  Engineering and Society ISSN 2522-3488 Vol. 8, Issue 4: 22-29", "summary": "This study aims to analyze the economic performance of various parks under\ndifferent conditions, particularly focusing on the operational costs and power\nload balancing before and after the deployment of energy storage systems.\nFirstly, the economic performance of the parks without energy storage was\nanalyzed using a random forest model. Taking Park A as an example, it was found\nthat the cost had the greatest correlation with electricity purchase, followed\nby photovoltaic output, indicating that solar and wind power output are key\nfactors affecting economic performance. Subsequently, the operation of the\nparks after the configuration of a 50kW/100kWh energy storage system was\nsimulated, and the total cost and operation strategy of the energy storage\nsystem were calculated. The results showed that after the deployment of energy\nstorage, the amount of wind and solar power curtailment in each park decreased,\nand the operational costs were reduced. Finally, a genetic algorithm was used\nto optimize the energy storage configuration of each park. The energy storage\noperation strategy was optimized through fitness functions, crossover\noperations, and mutation operations. After optimization, the economic\nindicators of Parks A, B, and C all improved. The research results indicate\nthat by optimizing energy storage configuration, each park can reduce costs,\nenhance economic benefits, and achieve sustainable development of the power\nsystem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u516c\u56ed\u7684\u7ecf\u6d4e\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u80fd\u6e90\u5b58\u50a8\u7cfb\u7edf\u90e8\u7f72\u524d\u540e\u7684\u8fd0\u8425\u6210\u672c\u548c\u7535\u529b\u8d1f\u8f7d\u5e73\u8861\uff0c\u53d1\u73b0\u4f18\u5316\u50a8\u80fd\u914d\u7f6e\u53ef\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u7ecf\u6d4e\u6548\u76ca\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u80fd\u6e90\u5b58\u50a8\u7cfb\u7edf\u5728\u516c\u56ed\u7ecf\u6d4e\u8868\u73b0\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5149\u4f0f\u548c\u98ce\u529b\u8f93\u51fa\u5bf9\u6210\u672c\u7684\u5f71\u54cd\uff0c\u4ee5\u5b9e\u73b0\u7535\u529b\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "method": "\u9996\u5148\u4f7f\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u5206\u6790\u672a\u914d\u7f6e\u50a8\u80fd\u7684\u56ed\u533a\u7ecf\u6d4e\u8868\u73b0\uff0c\u968f\u540e\u6a21\u62df\u90e8\u7f7250kW/100kWh\u50a8\u80fd\u7cfb\u7edf\u540e\u7684\u8fd0\u8425\uff0c\u6700\u540e\u5229\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u50a8\u80fd\u914d\u7f6e\u3002", "result": "\u90e8\u7f72\u50a8\u80fd\u540e\uff0c\u5404\u56ed\u533a\u7684\u98ce\u5149\u5f03\u7535\u91cf\u51cf\u5c11\uff0c\u8fd0\u8425\u6210\u672c\u964d\u4f4e\uff1b\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u540e\uff0cA\u3001B\u3001C\u56ed\u533a\u7684\u7ecf\u6d4e\u6307\u6807\u5747\u5f97\u5230\u6539\u5584\u3002", "conclusion": "\u4f18\u5316\u50a8\u80fd\u914d\u7f6e\u53ef\u5e2e\u52a9\u56ed\u533a\u964d\u4f4e\u6210\u672c\u3001\u63d0\u5347\u7ecf\u6d4e\u6548\u76ca\uff0c\u5e76\u4fc3\u8fdb\u7535\u529b\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2505.05515", "pdf": "https://arxiv.org/pdf/2505.05515", "abs": "https://arxiv.org/abs/2505.05515", "authors": ["Zinan Liu", "Haoran Li", "Jingyi Lu", "Gaoyuan Ma", "Xu Hong", "Giovanni Iacca", "Arvind Kumar", "Shaojun Tang", "Lin Wang"], "title": "Nature's Insight: A Novel Framework and Comprehensive Analysis of Agentic Reasoning Through the Lens of Neuroscience", "categories": ["q-bio.NC", "cs.LG"], "comment": "39 pages, 17 figures", "summary": "Autonomous AI is no longer a hard-to-reach concept, it enables the agents to\nmove beyond executing tasks to independently addressing complex problems,\nadapting to change while handling the uncertainty of the environment. However,\nwhat makes the agents truly autonomous? It is agentic reasoning, that is\ncrucial for foundation models to develop symbolic logic, statistical\ncorrelations, or large-scale pattern recognition to process information, draw\ninferences, and make decisions. However, it remains unclear why and how\nexisting agentic reasoning approaches work, in comparison to biological\nreasoning, which instead is deeply rooted in neural mechanisms involving\nhierarchical cognition, multimodal integration, and dynamic interactions. In\nthis work, we propose a novel neuroscience-inspired framework for agentic\nreasoning. Grounded in three neuroscience-based definitions and supported by\nmathematical and biological foundations, we propose a unified framework\nmodeling reasoning from perception to action, encompassing four core types,\nperceptual, dimensional, logical, and interactive, inspired by distinct\nfunctional roles observed in the human brain. We apply this framework to\nsystematically classify and analyze existing AI reasoning methods, evaluating\ntheir theoretical foundations, computational designs, and practical\nlimitations. We also explore its implications for building more generalizable,\ncognitively aligned agents in physical and virtual environments. Finally,\nbuilding on our framework, we outline future directions and propose new\nneural-inspired reasoning methods, analogous to chain-of-thought prompting. By\nbridging cognitive neuroscience and AI, this work offers a theoretical\nfoundation and practical roadmap for advancing agentic reasoning in intelligent\nsystems. The associated project can be found at:\nhttps://github.com/BioRAILab/Awesome-Neuroscience-Agent-Reasoning .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u548c\u589e\u5f3aAI\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u8111\u7684\u611f\u77e5\u3001\u7ef4\u5ea6\u3001\u903b\u8f91\u548c\u4ea4\u4e92\u63a8\u7406\u7c7b\u578b\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3bAI\u7684\u63a8\u7406\u673a\u5236\u4ecd\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u4e0e\u751f\u7269\u795e\u7ecf\u673a\u5236\u7684\u5bf9\u6bd4\u3002\u672c\u6587\u65e8\u5728\u4ece\u795e\u7ecf\u79d1\u5b66\u89d2\u5ea6\u51fa\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u5e76\u63a8\u52a8\u66f4\u901a\u7528\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u795e\u7ecf\u79d1\u5b66\u5b9a\u4e49\u548c\u6570\u5b66\u3001\u751f\u7269\u5b66\u57fa\u7840\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u63a8\u7406\u6846\u67b6\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u7ef4\u5ea6\u3001\u903b\u8f91\u548c\u4ea4\u4e92\u56db\u79cd\u6838\u5fc3\u63a8\u7406\u7c7b\u578b\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709AI\u65b9\u6cd5\u7684\u7406\u8bba\u3001\u8ba1\u7b97\u8bbe\u8ba1\u548c\u5c40\u9650\u6027\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u5206\u7c7b\u548c\u8bc4\u4f30\u4e86\u73b0\u6709AI\u63a8\u7406\u65b9\u6cd5\uff0c\u8fd8\u4e3a\u6784\u5efa\u66f4\u5177\u901a\u7528\u6027\u548c\u8ba4\u77e5\u5bf9\u9f50\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u63d0\u51fa\u4e86\u7c7b\u4f3c\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u795e\u7ecf\u542f\u53d1\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4e0eAI\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u8def\u5f84\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.05965", "pdf": "https://arxiv.org/pdf/2505.05965", "abs": "https://arxiv.org/abs/2505.05965", "authors": ["Abdelfateh Bekkair", "Slimane Bellaouar", "Slimane Oulad-Naoui"], "title": "A Noise-Resilient Semi-Supervised Graph Autoencoder for Overlapping Semantic Community Detection", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Community detection in networks with overlapping structures remains a\nsignificant challenge, particularly in noisy real-world environments where\nintegrating topology, node attributes, and prior information is critical. To\naddress this, we propose a semi-supervised graph autoencoder that combines\ngraph multi-head attention and modularity maximization to robustly detect\noverlapping communities. The model learns semantic representations by fusing\nstructural, attribute, and prior knowledge while explicitly addressing noise in\nnode features. Key innovations include a noise-resistant architecture and a\nsemantic semi-supervised design optimized for community quality through\nmodularity constraints. Experiments demonstrate superior performance the model\noutperforms state-of-the-art methods in overlapping community detection\n(improvements in NMI and F1-score) and exhibits exceptional robustness to\nattribute noise, maintaining stable performance under 60\\% feature corruption.\nThese results highlight the importance of integrating attribute semantics and\nstructural patterns for accurate community discovery in complex networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u4e0e\u6a21\u5757\u5316\u6700\u5927\u5316\uff0c\u7528\u4e8e\u68c0\u6d4b\u91cd\u53e0\u793e\u533a\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u7f51\u7edc\u4e2d\u91cd\u53e0\u793e\u533a\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u9700\u6574\u5408\u62d3\u6251\u3001\u8282\u70b9\u5c5e\u6027\u548c\u5148\u9a8c\u4fe1\u606f\uff0c\u5e76\u5904\u7406\u566a\u58f0\u95ee\u9898\u3002", "method": "\u91c7\u7528\u566a\u58f0\u62b5\u6297\u67b6\u6784\u548c\u8bed\u4e49\u534a\u76d1\u7763\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7ea6\u675f\u4f18\u5316\u793e\u533a\u8d28\u91cf\uff0c\u878d\u5408\u7ed3\u6784\u3001\u5c5e\u6027\u548c\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5728NMI\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u572860%\u7279\u5f81\u566a\u58f0\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u6574\u5408\u5c5e\u6027\u8bed\u4e49\u548c\u7ed3\u6784\u6a21\u5f0f\u5bf9\u590d\u6742\u7f51\u7edc\u4e2d\u793e\u533a\u53d1\u73b0\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.05517", "pdf": "https://arxiv.org/pdf/2505.05517", "abs": "https://arxiv.org/abs/2505.05517", "authors": ["Hongyi Chen", "Yunchao Yao", "Yufei Ye", "Zhixuan Xu", "Homanga Bharadhwaj", "Jiashun Wang", "Shubham Tulsiani", "Zackory Erickson", "Jeffrey Ichnowski"], "title": "Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Functional grasp is essential for enabling dexterous multi-finger robot hands\nto manipulate objects effectively. However, most prior work either focuses on\npower grasping, which simply involves holding an object still, or relies on\ncostly teleoperated robot demonstrations to teach robots how to grasp each\nobject functionally. Instead, we propose extracting human grasp information\nfrom web images since they depict natural and functional object interactions,\nthereby bypassing the need for curated demonstrations. We reconstruct human\nhand-object interaction (HOI) 3D meshes from RGB images, retarget the human\nhand to multi-finger robot hands, and align the noisy object mesh with its\naccurate 3D shape. We show that these relatively low-quality HOI data from\ninexpensive web sources can effectively train a functional grasping model. To\nfurther expand the grasp dataset for seen and unseen objects, we use the\ninitially-trained grasping policy with web data in the IsaacGym simulator to\ngenerate physically feasible grasps while preserving functionality. We train\nthe grasping model on 10 object categories and evaluate it on 9 unseen objects,\nincluding challenging items such as syringes, pens, spray bottles, and tongs,\nwhich are underrepresented in existing datasets. The model trained on the web\nHOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across\nall objects in simulation, with a 6.7% improvement in success rate and a 1.8x\nincrease in functionality ratings over baselines. Simulator-augmented data\nfurther boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the\nLEAP Hand achieves a 85% success rate. Project website is at:\nhttps://webgrasp.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7f51\u7edc\u56fe\u50cf\u63d0\u53d6\u4eba\u7c7b\u6293\u53d6\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u4eba\u673a\u4ea4\u4e923D\u7f51\u683c\u5e76\u8bad\u7ec3\u6293\u53d6\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u624b\u7684\u7075\u6d3b\u6293\u53d6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4fa7\u91cd\u4e8e\u7b80\u5355\u7684\u529b\u91cf\u6293\u53d6\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u9065\u63a7\u6f14\u793a\uff0c\u800c\u7f51\u7edc\u56fe\u50cf\u80fd\u63d0\u4f9b\u81ea\u7136\u7684\u4ea4\u4e92\u4fe1\u606f\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u5347\u529f\u80fd\u6027\u3002", "method": "\u4eceRGB\u56fe\u50cf\u91cd\u5efa\u4eba\u673a\u4ea4\u4e923D\u7f51\u683c\uff0c\u91cd\u65b0\u6620\u5c04\u5230\u673a\u5668\u4eba\u624b\uff0c\u5e76\u4e0e\u7cbe\u786e3D\u6a21\u578b\u5bf9\u9f50\uff1b\u5229\u7528\u6a21\u62df\u5668\u6269\u5c55\u6293\u53d6\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u4e2d\u8fbe\u523075.8%\uff08\u5df2\u77e5\u7269\u4f53\uff09\u548c61.8%\uff08\u5168\u90e8\u7269\u4f53\uff09\u6210\u529f\u7387\uff0c\u6a21\u62df\u589e\u5f3a\u540e\u63d0\u5347\u81f383.4%\uff1b\u5b9e\u7269\u6d4b\u8bd5\u6210\u529f\u7387\u4e3a85%\u3002", "conclusion": "\u7f51\u7edc\u6570\u636e\u9a71\u52a8\u7684\u6293\u53d6\u6a21\u578b\u9ad8\u6548\u4e14\u6210\u672c\u4f4e\uff0c\u6a21\u62df\u589e\u5f3a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2505.05988", "pdf": "https://arxiv.org/pdf/2505.05988", "abs": "https://arxiv.org/abs/2505.05988", "authors": ["J\u00f8rgen Villadsen"], "title": "Minimal Sequent Calculus for Teaching First-Order Logic: Lessons Learned", "categories": ["cs.LO", "cs.AI", "F.4; I.2.3; K.3.1"], "comment": "In Proceedings ThEdu24, arXiv:2505.04677", "summary": "MiniCalc is a web app for teaching first-order logic based on a minimal\nsequent calculus. As an option the proofs can be verified in the Isabelle proof\nassistant. We present the lessons learned using the tool in recent years at our\nuniversity.", "AI": {"tldr": "MiniCalc\u662f\u4e00\u4e2a\u7528\u4e8e\u6559\u6388\u4e00\u9636\u903b\u8f91\u7684\u7f51\u9875\u5e94\u7528\uff0c\u57fa\u4e8e\u6700\u5c0f\u5e8f\u8d2f\u6f14\u7b97\uff0c\u5e76\u53ef\u9009\u5728Isabelle\u8bc1\u660e\u52a9\u624b\u4e2d\u9a8c\u8bc1\u8bc1\u660e\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7MiniCalc\u5de5\u5177\u6559\u6388\u4e00\u9636\u903b\u8f91\uff0c\u5e76\u5206\u4eab\u8fd1\u5e74\u6765\u5728\u5927\u5b66\u4f7f\u7528\u8be5\u5de5\u5177\u7684\u7ecf\u9a8c\u3002", "method": "\u57fa\u4e8e\u6700\u5c0f\u5e8f\u8d2f\u6f14\u7b97\u5f00\u53d1\u7f51\u9875\u5e94\u7528\uff0c\u652f\u6301Isabelle\u9a8c\u8bc1\u3002", "result": "\u63d0\u4f9b\u4e86\u4f7f\u7528MiniCalc\u5de5\u5177\u7684\u6559\u5b66\u7ecf\u9a8c\u548c\u53cd\u9988\u3002", "conclusion": "MiniCalc\u4f5c\u4e3a\u4e00\u79cd\u6559\u5b66\u5de5\u5177\uff0c\u6709\u6548\u8f85\u52a9\u4e86\u4e00\u9636\u903b\u8f91\u7684\u6559\u5b66\uff0c\u5e76\u7ed3\u5408Isabelle\u589e\u5f3a\u4e86\u9a8c\u8bc1\u80fd\u529b\u3002"}}
{"id": "2505.05540", "pdf": "https://arxiv.org/pdf/2505.05540", "abs": "https://arxiv.org/abs/2505.05540", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Sudipta Chowdhury", "Harshvardhan Sikka"], "title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 26 figures", "summary": "Vision-language-action (VLA) models represent an important step toward\ngeneral-purpose robotic systems by integrating visual perception, language\nunderstanding, and action execution. However, systematic evaluation of these\nmodels, particularly their zero-shot generalization capabilities in\nout-of-distribution (OOD) environments, remains limited. In this paper, we\nintroduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and\nanalyze the generalization performance of state-of-the-art VLM and VLA\nmodels-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse\nprocedural tasks from the Procgen benchmark. Our analysis reveals several\ncritical insights: (1) all evaluated models exhibit significant limitations in\nzero-shot generalization to OOD tasks, with performance heavily influenced by\nfactors such as action representation and task complexit; (2) VLAs generally\noutperform other models due to their robust architectural design; and (3) VLM\nvariants demonstrate substantial improvements when constrained appropriately,\nhighlighting the sensitivity of model performance to precise prompt\nengineering.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMultiNet v0.2\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4f46VLA\u6a21\u578b\u8868\u73b0\u8f83\u597d\uff0c\u4e14\u6a21\u578b\u6027\u80fd\u53d7\u63d0\u793a\u5de5\u7a0b\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u901a\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u4f46\u5bf9\u5176\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u8bbe\u8ba1\u6d4b\u8bd5\u57fa\u51c6\u5206\u6790\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faMultiNet v0.2\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u5305\u62ecGPT-4o\u3001OpenVLA\u7b49\u591a\u79cdVLM\u548cVLA\u6a21\u578b\u5728Procgen\u57fa\u51c6\u4e2d\u7684\u591a\u6837\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) \u6240\u6709\u6a21\u578b\u5728OOD\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b(2) VLA\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1b(3) VLM\u53d8\u79cd\u5728\u63d0\u793a\u5de5\u7a0b\u7ea6\u675f\u4e0b\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VLA\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u63d0\u793a\u5de5\u7a0b\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002"}}
{"id": "2505.05542", "pdf": "https://arxiv.org/pdf/2505.05542", "abs": "https://arxiv.org/abs/2505.05542", "authors": ["Guillaume Dalle", "Adrian Hill"], "title": "A Common Interface for Automatic Differentiation", "categories": ["cs.MS", "cs.LG", "cs.NA", "math.NA", "G.1.4"], "comment": "11 pages, 2 figures, 3 listings, 1 table", "summary": "For scientific machine learning tasks with a lot of custom code, picking the\nright Automatic Differentiation (AD) system matters. Our Julia package\nDifferentiationInterface.jl provides a common frontend to a dozen AD backends,\nunlocking easy comparison and modular development. In particular, its built-in\npreparation mechanism leverages the strengths of each backend by amortizing\none-time computations. This is key to enabling sophisticated features like\nsparsity handling without putting additional burdens on the user.", "AI": {"tldr": "DifferentiationInterface.jl\u662f\u4e00\u4e2aJulia\u5305\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u524d\u7aef\u63a5\u53e3\uff0c\u7b80\u5316\u591a\u79cd\u81ea\u52a8\u5fae\u5206\u540e\u7aef\u7684\u6bd4\u8f83\u548c\u6a21\u5757\u5316\u5f00\u53d1\u3002", "motivation": "\u9488\u5bf9\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u81ea\u5b9a\u4e49\u4ee3\u7801\u8f83\u591a\u7684\u60c5\u51b5\uff0c\u9009\u62e9\u9002\u5408\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5185\u7f6e\u51c6\u5907\u673a\u5236\uff0c\u5229\u7528\u5404\u540e\u7aef\u7684\u4f18\u52bf\u5e76\u5206\u644a\u4e00\u6b21\u6027\u8ba1\u7b97\u5f00\u9500\uff0c\u652f\u6301\u9ad8\u7ea7\u529f\u80fd\uff08\u5982\u7a00\u758f\u5904\u7406\uff09\u800c\u65e0\u9700\u7528\u6237\u989d\u5916\u8d1f\u62c5\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u79cd\u81ea\u52a8\u5fae\u5206\u540e\u7aef\u7684\u65e0\u7f1d\u96c6\u6210\u4e0e\u9ad8\u6548\u5229\u7528\u3002", "conclusion": "DifferentiationInterface.jl\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u81ea\u52a8\u5fae\u5206\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2505.06085", "pdf": "https://arxiv.org/pdf/2505.06085", "abs": "https://arxiv.org/abs/2505.06085", "authors": ["Hiari Pizzini Cavagna", "Daniele Cesarini", "Andrea Bartolini"], "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities", "categories": ["cs.PF", "cs.AI", "cs.AR"], "comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings", "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86Tenstorrent Grayskull e75 RISC-V\u52a0\u901f\u5668\u5728\u4f4e\u7cbe\u5ea6\u7ebf\u6027\u4ee3\u6570\u8fd0\u7b97\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u82f1\u7279\u5c14Sapphire Rapids\u53caNVIDIA GPU\u5bf9\u6bd4\uff0c\u663e\u793a\u5176\u5728\u529f\u8017\u4e0e\u8ba1\u7b97\u541e\u5410\u91cf\u4e0a\u7684\u7ade\u4e89\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u670d\u52a1\u7684\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u8017\u7684\u4e13\u7528\u786c\u4ef6\u67b6\u6784\uff0cGrayskull\u7684\u6f5c\u529b\u9700\u8981\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u5206\u6790Grayskull\u7684\u6267\u884c\u6a21\u578b\u3001\u7f51\u683c\u5927\u5c0f\u3001\u77e9\u9635\u7ef4\u5ea6\u3001\u6570\u636e\u683c\u5f0f\u53ca\u6570\u503c\u7cbe\u5ea6\u5bf9\u6548\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u4e3b\u6d41\u786c\u4ef6\uff08\u5982\u82f1\u7279\u5c14Sapphire Rapids\u548cNVIDIA V100/A100\uff09\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "NVIDIA GPU\u5728\u539f\u59cb\u6027\u80fd\u4e0a\u5360\u4f18\uff0c\u4f46Grayskull\u5728\u529f\u8017\u4e0e\u8ba1\u7b97\u541e\u5410\u91cf\uff081.55 TFLOPs/Watt @ BF16\uff09\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "conclusion": "Grayskull\u5728\u80fd\u6548\u6bd4\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5c24\u5176\u5728\u6ce8\u91cd\u80fd\u8017\u7684\u573a\u666f\u4e2d\u53ef\u66ff\u4ee3\u4f20\u7edfGPU\u3002"}}
{"id": "2505.05549", "pdf": "https://arxiv.org/pdf/2505.05549", "abs": "https://arxiv.org/abs/2505.05549", "authors": ["Vishnu Jejjala", "Suresh Nampuri", "Dumisani Nxumalo", "Pratik Roy", "Abinash Swain"], "title": "Machine learning automorphic forms for black holes", "categories": ["hep-th", "cs.LG", "math.NT"], "comment": null, "summary": "Modular, Jacobi, and mock-modular forms serve as generating functions for BPS\nblack hole degeneracies. By training feed-forward neural networks on Fourier\ncoefficients of automorphic forms derived from the Dedekind eta function,\nEisenstein series, and Jacobi theta functions, we demonstrate that machine\nlearning techniques can accurately predict modular weights from truncated\nexpansions. Our results reveal strong performance for negative weight modular\nand quasi-modular forms, particularly those arising in exact black hole\ncounting formulae, with lower accuracy for positive weights and more\ncomplicated combinations of Jacobi theta functions. This study establishes a\nproof of concept for using machine learning to identify how data is organized\nin terms of modular symmetries in gravitational systems and suggests a pathway\ntoward automated detection and verification of symmetries in quantum gravity.", "AI": {"tldr": "\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6a21\u5f62\u5f0f\u548c\u96c5\u53ef\u6bd4\u5f62\u5f0f\u7684\u6a21\u91cd\u91cf\uff0c\u7ed3\u679c\u8868\u660e\u5728\u8d1f\u6743\u5f62\u5f0f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u9a8c\u8bc1\u4e86\u673a\u5668\u5b66\ufffd\ufffd\ufffd\u5728\u8bc6\u522b\u5f15\u529b\u7cfb\u7edf\u4e2d\u6a21\u5bf9\u79f0\u6027\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u8bc6\u522b\u548c\u9884\u6d4b\u6a21\u5f62\u5f0f\u3001\u96c5\u53ef\u6bd4\u5f62\u5f0f\u53ca\u6a21\u62df\u6a21\u5f62\u5f0f\u5728BPS\u9ed1\u6d1e\u7b80\u5e76\u4e2d\u7684\u5bf9\u79f0\u6027\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u4e3a\u91cf\u5b50\u5f15\u529b\u4e2d\u7684\u5bf9\u79f0\u6027\u81ea\u52a8\u68c0\u6d4b\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "method": "\u4f7f\u7528\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u81eaDedekind eta\u51fd\u6570\u3001Eisenstein\u7ea7\u6570\u548c\u96c5\u53ef\u6bd4theta\u51fd\u6570\u5bfc\u51fa\u7684\u81ea\u5b88\u5f62\u5f0f\u7684\u5085\u91cc\u53f6\u7cfb\u6570\uff0c\u9884\u6d4b\u6a21\u91cd\u91cf\u3002", "result": "\u5728\u8d1f\u6743\u6a21\u5f62\u5f0f\u548c\u62df\u6a21\u5f62\u5f0f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7cbe\u786e\u9ed1\u6d1e\u8ba1\u6570\u516c\u5f0f\u4e2d\uff1b\u4f46\u5728\u6b63\u6743\u5f62\u5f0f\u548c\u590d\u6742\u96c5\u53ef\u6bd4theta\u51fd\u6570\u7ec4\u5408\u4e2d\u51c6\u786e\u7387\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u673a\u5668\u5b66\u4e60\u5728\u8bc6\u522b\u5f15\u529b\u7cfb\u7edf\u6a21\u5bf9\u79f0\u6027\u4e2d\u7684\u6982\u5ff5\u53ef\u884c\u6027\uff0c\u4e3a\u91cf\u5b50\u5f15\u529b\u4e2d\u7684\u5bf9\u79f0\u6027\u81ea\u52a8\u68c0\u6d4b\u4e0e\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6f5c\u5728\u8def\u5f84\u3002"}}
{"id": "2505.05584", "pdf": "https://arxiv.org/pdf/2505.05584", "abs": "https://arxiv.org/abs/2505.05584", "authors": ["Mohamed Salah Bouafif", "Mohammad Hamdaqa", "Edward Zulkoski"], "title": "PRIMG : Efficient LLM-driven Test Generation Using Mutant Prioritization", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Mutation testing is a widely recognized technique for assessing and enhancing\nthe effectiveness of software test suites by introducing deliberate code\nmutations. However, its application often results in overly large test suites,\nas developers generate numerous tests to kill specific mutants, increasing\ncomputational overhead. This paper introduces PRIMG (Prioritization and\nRefinement Integrated Mutation-driven Generation), a novel framework for\nincremental and adaptive test case generation for Solidity smart contracts.\nPRIMG integrates two core components: a mutation prioritization module, which\nemploys a machine learning model trained on mutant subsumption graphs to\npredict the usefulness of surviving mutants, and a test case generation module,\nwhich utilizes Large Language Models (LLMs) to generate and iteratively refine\ntest cases to achieve syntactic and behavioral correctness.\n  We evaluated PRIMG on real-world Solidity projects from Code4Arena to assess\nits effectiveness in improving mutation scores and generating high-quality test\ncases. The experimental results demonstrate that PRIMG significantly reduces\ntest suite size while maintaining high mutation coverage. The prioritization\nmodule consistently outperformed random mutant selection, enabling the\ngeneration of high-impact tests with reduced computational effort. Furthermore,\nthe refining process enhanced the correctness and utility of LLM-generated\ntests, addressing their inherent limitations in handling edge cases and complex\nprogram logic.", "AI": {"tldr": "PRIMG\u6846\u67b6\u7ed3\u5408\u7a81\u53d8\u4f18\u5148\u7ea7\u548c\u6d4b\u8bd5\u751f\u6210\u6a21\u5757\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6d4b\u8bd5\u5957\u4ef6\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7a81\u53d8\u8986\u76d6\u7387\uff0c\u5e76\u901a\u8fc7LLMs\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u3002", "motivation": "\u7a81\u53d8\u6d4b\u8bd5\u5e38\u5bfc\u81f4\u6d4b\u8bd5\u5957\u4ef6\u8fc7\u5927\uff0c\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002PRIMG\u65e8\u5728\u901a\u8fc7\u667a\u80fd\u5316\u7684\u7a81\u53d8\u4f18\u5148\u7ea7\u548c\u6d4b\u8bd5\u751f\u6210\u6765\u4f18\u5316\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PRIMG\u5305\u542b\u7a81\u53d8\u4f18\u5148\u7ea7\u6a21\u5757\uff08\u57fa\u4e8e\u5b50\u56fe\u673a\u5668\u5b66\u4e60\u7684\u7a81\u53d8\u9884\u6d4b\uff09\u548c\u6d4b\u8bd5\u751f\u6210\u6a21\u5757\uff08\u5229\u7528LLMs\u751f\u6210\u5e76\u8fed\u4ee3\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\uff09\u3002", "result": "\u5728Solidity\u9879\u76ee\u4e2d\u9a8c\u8bc1\u65f6\uff0cPRIMG\u663e\u8457\u964d\u4f4e\u6d4b\u8bd5\u5957\u4ef6\u89c4\u6a21\u4e14\u4fdd\u6301\u9ad8\u7a81\u53d8\u8986\u76d6\u7387\uff0c\u4f18\u5148\u7ea7\u6a21\u5757\u8868\u73b0\u4f18\u4e8e\u968f\u673a\u9009\u62e9\u3002", "conclusion": "PRIMG\u6709\u6548\u63d0\u5347\u7a81\u53d8\u5206\u6570\u548c\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u8fb9\u7f18\u6848\u4f8b\u548c\u590d\u6742\u903b\u8f91\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.06111", "pdf": "https://arxiv.org/pdf/2505.06111", "abs": "https://arxiv.org/abs/2505.06111", "authors": ["Qingwen Bu", "Yanting Yang", "Jisong Cai", "Shenyuan Gao", "Guanghui Ren", "Maoqing Yao", "Ping Luo", "Hongyang Li"], "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA", "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.", "AI": {"tldr": "UniVLA is a new framework for learning cross-embodiment vision-language-action policies, using latent action models to transfer knowledge across diverse robots and environments, achieving superior efficiency and performance compared to existing methods.", "motivation": "Existing robot learning approaches rely heavily on action-annotated data, limiting their adaptability to different embodiments and environments. UniVLA aims to overcome this by leveraging cross-embodiment learning from diverse videos.", "method": "UniVLA derives task-centric action representations from videos using a latent action model in the DINO feature space, integrating language instructions to filter irrelevant dynamics.", "result": "UniVLA achieves state-of-the-art performance in manipulation and navigation benchmarks, with significant efficiency gains (1/20 pretraining compute and 1/10 downstream data vs. OpenVLA). Performance improves further with heterogeneous data.", "conclusion": "UniVLA demonstrates scalable and efficient robot policy learning, showcasing potential for generalist robot applications through cross-embodiment knowledge transfer."}}
{"id": "2505.05592", "pdf": "https://arxiv.org/pdf/2505.05592", "abs": "https://arxiv.org/abs/2505.05592", "authors": ["Noriaki Hirose", "Lydia Ignatova", "Kyle Stachowicz", "Catherine Glossop", "Sergey Levine", "Dhruv Shah"], "title": "Learning to Drive Anywhere with Model-Based Reannotation11", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "19 pages, 11 figures, 8 tables", "summary": "Developing broadly generalizable visual navigation policies for robots is a\nsignificant challenge, primarily constrained by the availability of\nlarge-scale, diverse training data. While curated datasets collected by\nresearchers offer high quality, their limited size restricts policy\ngeneralization. To overcome this, we explore leveraging abundant, passively\ncollected data sources, including large volumes of crowd-sourced teleoperation\ndata and unlabeled YouTube videos, despite their potential for lower quality or\nmissing action labels. We propose Model-Based ReAnnotation (MBRA), a framework\nthat utilizes a learned short-horizon, model-based expert model to relabel or\ngenerate high-quality actions for these passive datasets. This relabeled data\nis then distilled into LogoNav, a long-horizon navigation policy conditioned on\nvisual goals or GPS waypoints. We demonstrate that LogoNav, trained using\nMBRA-processed data, achieves state-of-the-art performance, enabling robust\nnavigation over distances exceeding 300 meters in previously unseen indoor and\noutdoor environments. Our extensive real-world evaluations, conducted across a\nfleet of robots (including quadrupeds) in six cities on three continents,\nvalidate the policy's ability to generalize and navigate effectively even\namidst pedestrians in crowded settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMBRA\u7684\u6846\u67b6\uff0c\u5229\u7528\u6a21\u578b\u91cd\u65b0\u6807\u6ce8\u88ab\u52a8\u6536\u96c6\u7684\u6570\u636e\uff0c\u4ee5\u8bad\u7ec3\u901a\u7528\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565LogoNav\u3002\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u5bfc\u822a\u7b56\u7565\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u5b9e\u73b0\u5e7f\u6cdb\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u88ab\u52a8\u6536\u96c6\u7684\u4f17\u5305\u6570\u636e\u548cYouTube\u89c6\u9891\uff0c\u5c3d\u7ba1\u5176\u8d28\u91cf\u8f83\u4f4e\u6216\u7f3a\u5c11\u52a8\u4f5c\u6807\u7b7e\uff0c\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MBRA\u6846\u67b6\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u77ed\u65f6\u6a21\u578b\u4e13\u5bb6\u6a21\u578b\u4e3a\u88ab\u52a8\u6570\u636e\u96c6\u91cd\u65b0\u6807\u6ce8\u6216\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u52a8\u4f5c\u6807\u7b7e\u3002\u968f\u540e\uff0c\u5c06\u8fd9\u4e9b\u6570\u636e\u63d0\u70bc\u4e3a\u4e00\u4e2a\u957f\u65f6\u5bfc\u822a\u7b56\u7565LogoNav\uff0c\u8be5\u7b56\u7565\u57fa\u4e8e\u89c6\u89c9\u76ee\u6807\u6216GPS\u822a\u70b9\u3002", "result": "LogoNav\u5728\u672a\u7ecf\u89c1\u7684\u73af\u5883\uff08\u5ba4\u5185\u5916\uff09\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc7300\u7c73\u7684\u7a33\u5065\u5bfc\u822a\uff0c\u5e76\u5728\u5168\u7403\u516d\u4e2a\u57ce\u5e02\u7684\u771f\u5b9e\u73af\u5883\u4e2d\uff08\u5305\u62ec\u4eba\u7fa4\u5bc6\u96c6\u573a\u666f\uff09\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7MBRA\u5904\u7406\u88ab\u52a8\u6570\u636e\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5bfc\u822a\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u5229\u7528\u601d\u8def\u3002"}}
{"id": "2505.06118", "pdf": "https://arxiv.org/pdf/2505.06118", "abs": "https://arxiv.org/abs/2505.06118", "authors": ["Jingguo Qu", "Xinyang Han", "Man-Lik Chui", "Yao Pu", "Simon Takadiyi Gunda", "Ziman Chen", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Ying"], "title": "The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automatic lymph node segmentation is the cornerstone for advances in computer\nvision tasks for early detection and staging of cancer. Traditional\nsegmentation methods are constrained by manual delineation and variability in\noperator proficiency, limiting their ability to achieve high accuracy. The\nintroduction of deep learning technologies offers new possibilities for\nimproving the accuracy of lymph node image analysis. This study evaluates the\napplication of deep learning in lymph node segmentation and discusses the\nmethodologies of various deep learning architectures such as convolutional\nneural networks, encoder-decoder networks, and transformers in analyzing\nmedical imaging data across different modalities. Despite the advancements, it\nstill confronts challenges like the shape diversity of lymph nodes, the\nscarcity of accurately labeled datasets, and the inadequate development of\nmethods that are robust and generalizable across different imaging modalities.\nTo the best of our knowledge, this is the first study that provides a\ncomprehensive overview of the application of deep learning techniques in lymph\nnode segmentation task. Furthermore, this study also explores potential future\nresearch directions, including multimodal fusion techniques, transfer learning,\nand the use of large-scale pre-trained models to overcome current limitations\nwhile enhancing cancer diagnosis and treatment planning strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u6dcb\u5df4\u7ed3\u81ea\u52a8\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u67b6\u6784\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u6dcb\u5df4\u7ed3\u5206\u5272\u65b9\u6cd5\u53d7\u9650\u4e8e\u624b\u52a8\u7ed8\u5236\u548c\u64cd\u4f5c\u8005\u6c34\u5e73\u7684\u5dee\u5f02\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e3a\u63d0\u9ad8\u51c6\u786e\u6027\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u548cTransformer\u7b49\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u591a\u6a21\u6001\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u6dcb\u5df4\u7ed3\u5f62\u72b6\u591a\u6837\u3001\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u53ca\u8de8\u6a21\u6001\u65b9\u6cd5\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "conclusion": "\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u6df1\u5ea6\u5b66\u4e60\u5728\u6dcb\u5df4\u7ed3\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5173\u6ce8\u591a\u6a21\u6001\u878d\u5408\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2505.05600", "pdf": "https://arxiv.org/pdf/2505.05600", "abs": "https://arxiv.org/abs/2505.05600", "authors": ["Jos\u00e9 Gon\u00e7alves", "Miguel Silva", "Eva Maia", "Isabel Pra\u00e7a"], "title": "Enhancing Large Language Models with Faster Code Preprocessing for Vulnerability Detection", "categories": ["cs.SE", "cs.LG"], "comment": "10 pages, 3 tables, DCAI'25: Distributed Computing and Artificial\n  Intelligence 2025", "summary": "The application of Artificial Intelligence has become a powerful approach to\ndetecting software vulnerabilities. However, effective vulnerability detection\nrelies on accurately capturing the semantic structure of code and its\ncontextual relationships. Given that the same functionality can be implemented\nin various forms, a preprocessing tool that standardizes code representation is\nimportant. This tool must be efficient, adaptable across programming languages,\nand capable of supporting new transformations. To address this challenge, we\nbuild on the existing SCoPE framework and introduce SCoPE2, an enhanced version\nwith improved performance. We compare both versions in terms of processing time\nand memory usage and evaluate their impact on a Large Language Model (LLM) for\nvulnerability detection. Our results show a 97.3\\% reduction in processing time\nwith SCoPE2, along with an improved F1-score for the LLM, solely due to the\nrefined preprocessing approach.", "AI": {"tldr": "SCoPE2\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u9884\u5904\u7406\u5de5\u5177\uff0c\u663e\u8457\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u5e76\u63d0\u5347LLM\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6f0f\u6d1e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6807\u51c6\u5316\u4ee3\u7801\u8868\u793a\u4e14\u9ad8\u6548\u7684\u9884\u5904\u7406\u5de5\u5177\u3002", "method": "\u57fa\u4e8eSCoPE\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u6539\u8fdb\u7248\u672cSCoPE2\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u8005\u5728\u5904\u7406\u65f6\u95f4\u548c\u5185\u5b58\u4f7f\u7528\u4e0a\u7684\u6027\u80fd\u3002", "result": "SCoPE2\u5c06\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u4e8697.3%\uff0c\u5e76\u63d0\u5347\u4e86LLM\u7684F1\u5206\u6570\u3002", "conclusion": "SCoPE2\u901a\u8fc7\u6539\u8fdb\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2505.06136", "pdf": "https://arxiv.org/pdf/2505.06136", "abs": "https://arxiv.org/abs/2505.06136", "authors": ["Yifeng Zhu"], "title": "Efficient Sensorimotor Learning for Open-world Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "Ph.D. Dissertation", "summary": "This dissertation considers Open-world Robot Manipulation, a manipulation\nproblem where a robot must generalize or quickly adapt to new objects, scenes,\nor tasks for which it has not been pre-programmed or pre-trained. This\ndissertation tackles the problem using a methodology of efficient sensorimotor\nlearning. The key to enabling efficient sensorimotor learning lies in\nleveraging regular patterns that exist in limited amounts of demonstration\ndata. These patterns, referred to as ``regularity,'' enable the data-efficient\nlearning of generalizable manipulation skills. This dissertation offers a new\nperspective on formulating manipulation problems through the lens of\nregularity. Building upon this notion, we introduce three major contributions.\nFirst, we introduce methods that endow robots with object-centric priors,\nallowing them to learn generalizable, closed-loop sensorimotor policies from a\nsmall number of teleoperation demonstrations. Second, we introduce methods that\nconstitute robots' spatial understanding, unlocking their ability to imitate\nmanipulation skills from in-the-wild video observations. Last but not least, we\nintroduce methods that enable robots to identify reusable skills from their\npast experiences, resulting in systems that can continually imitate multiple\ntasks in a sequential manner. Altogether, the contributions of this\ndissertation help lay the groundwork for building general-purpose personal\nrobots that can quickly adapt to new situations or tasks with low-cost data\ncollection and interact easily with humans. By enabling robots to learn and\ngeneralize from limited data, this dissertation takes a step toward realizing\nthe vision of intelligent robotic assistants that can be seamlessly integrated\ninto everyday scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5f00\u73af\u4e16\u754c\u673a\u5668\u4eba\u64cd\u63a7\u7684\u65b0\u65b9\u6cd5\uff0c\u91cd\u70b9\u662f\u5229\u7528\u6709\u9650\u7684\u6f14\u793a\u6570\u636e\u4e2d\u7684\u89c4\u5f8b\u8fdb\u884c\u9ad8\u6548\u5b66\u4e60\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u65b0\u4efb\u52a1\u548c\u5bf9\u8c61\u7684\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u672a\u9884\u7f16\u7a0b\u6216\u9884\u8bad\u7ec3\u7684\u4efb\u52a1\u3001\u5bf9\u8c61\u6216\u573a\u666f\u5feb\u901f\u9002\u5e94\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u5229\u7528\u6570\u636e\u4e2d\u7684\u89c4\u5f8b\u6765\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u548c\u6cdb\u5316\u3002", "method": "\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u5148\u9a8c\u3001\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u548c\u6280\u80fd\u590d\u7528\u4e09\u79cd\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5c0f\u6837\u672c\u5b66\u4e60\u548c\u89c6\u9891\u89c2\u5bdf\u6a21\u4eff\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ece\u5c11\u91cf\u6570\u636e\u4e2d\u5b66\u4e60\u901a\u7528\u64cd\u63a7\u7b56\u7565\uff0c\u5e76\u80fd\u6301\u7eed\u6a21\u4eff\u591a\u4e2a\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3a\u6784\u5efa\u4f4e\u6210\u672c\u6570\u636e\u9002\u5e94\u3001\u6613\u4e0e\u4eba\u4ea4\u4e92\u7684\u901a\u7528\u4e2a\u4eba\u673a\u5668\u4eba\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.05613", "pdf": "https://arxiv.org/pdf/2505.05613", "abs": "https://arxiv.org/abs/2505.05613", "authors": ["Achraf Azize", "Yulian Wu", "Junya Honda", "Francesco Orabona", "Shinji Ito", "Debabrota Basu"], "title": "Optimal Regret of Bernoulli Bandits under Global Differential Privacy", "categories": ["stat.ML", "cs.CR", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "comment": null, "summary": "As sequential learning algorithms are increasingly applied to real life,\nensuring data privacy while maintaining their utilities emerges as a timely\nquestion. In this context, regret minimisation in stochastic bandits under\n$\\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike\nbandits without DP, there is a significant gap between the best-known regret\nlower and upper bound in this setting, though they \"match\" in order. Thus, we\nrevisit the regret lower and upper bounds of $\\epsilon$-global DP algorithms\nfor Bernoulli bandits and improve both. First, we prove a tighter regret lower\nbound involving a novel information-theoretic quantity characterising the\nhardness of $\\epsilon$-global DP in stochastic bandits. Our lower bound\nstrictly improves on the existing ones across all $\\epsilon$ values. Then, we\nchoose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED,\nand propose their DP versions using a unified blueprint, i.e., (a) running in\narm-dependent phases, and (b) adding Laplace noise to achieve privacy. For\nBernoulli bandits, we analyse the regrets of these algorithms and show that\ntheir regrets asymptotically match our lower bound up to a constant arbitrary\nclose to 1. This refutes the conjecture that forgetting past rewards is\nnecessary to design optimal bandit algorithms under global DP. At the core of\nour algorithms lies a new concentration inequality for sums of Bernoulli\nvariables under Laplace mechanism, which is a new DP version of the Chernoff\nbound. This result is universally useful as the DP literature commonly treats\nthe concentrations of Laplace noise and random variables separately, while we\ncouple them to yield a tighter bound.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u03f5-\u5168\u5c40\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u7ea6\u675f\u4e0b\u7684\u968f\u673a\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\u7684\u9057\u61be\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u66f4\u7d27\u7684\u9057\u61be\u4e0b\u754c\u548c\u4e0a\u754c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u7c7b\u6e10\u8fd1\u6700\u4f18\u7684DP\u7b97\u6cd5\uff08DP-KLUCB\u548cDP-IMED\uff09\uff0c\u5176\u9057\u61be\u5339\u914d\u65b0\u7684\u4e0b\u754c\u3002", "motivation": "\u5728\u5e8f\u5217\u5b66\u4e60\u7b97\u6cd5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5b9e\u9645\u65f6\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u5176\u6548\u7528\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u5728\u03f5-\u5168\u5c40DP\u4e0b\u7684\u9057\u61be\u754c\u9650\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5e76\u6539\u8fdb\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u8bba\u91cf\u523b\u753b\u03f5-\u5168\u5c40DP\u5728\u968f\u673a\u8d4c\u535a\u673a\u4e2d\u7684\u96be\u5ea6\uff0c\u5e76\u8bc1\u660e\u66f4\u7d27\u7684\u9057\u61be\u4e0b\u754c\uff1b2. \u8bbe\u8ba1\u4e24\u7c7bDP\u7b97\u6cd5\uff08DP-KLUCB\u548cDP-IMED\uff09\uff0c\u901a\u8fc7\u81c2\u4f9d\u8d56\u9636\u6bb5\u548c\u62c9\u666e\u62c9\u65af\u566a\u58f0\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u65b0\u7b97\u6cd5\uff08DP-KLUCB\u548cDP-IMED\uff09\u7684\u9057\u61be\u6e10\u8fd1\u5339\u914d\u65b0\u4e0b\u754c\uff0c\u4e14\u65e0\u9700\u9057\u5fd8\u5386\u53f2\u5956\u52b1\u5373\u53ef\u5b9e\u73b0\u6700\u4f18\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62c9\u666e\u62c9\u65af\u673a\u5236\u4e0b\u4f2f\u52aa\u5229\u53d8\u91cf\u548c\u7684\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u3002", "conclusion": "\u8bba\u6587\u63a8\u7ffb\u4e86\u2018\u9057\u5fd8\u5386\u53f2\u5956\u52b1\u662f\u8bbe\u8ba1\u5168\u5c40DP\u4e0b\u6700\u4f18\u8d4c\u535a\u673a\u7b97\u6cd5\u7684\u5fc5\u8981\u6761\u4ef6\u2019\u7684\u731c\u60f3\uff0c\u4e3aDP\u4e0b\u7684\u5e8f\u5217\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7406\u8bba\u548c\u7b97\u6cd5\u652f\u6301\u3002"}}
{"id": "2505.06152", "pdf": "https://arxiv.org/pdf/2505.06152", "abs": "https://arxiv.org/abs/2505.06152", "authors": ["Wenqi Zeng", "Yuqi Sun", "Chenxi Ma", "Weimin Tan", "Bo Yan"], "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical vision-language models (VLMs) have shown promise as clinical\nassistants across various medical fields. However, specialized dermatology VLM\ncapable of delivering professional and detailed diagnostic analysis remains\nunderdeveloped, primarily due to less specialized text descriptions in current\ndermatology multimodal datasets. To address this issue, we propose MM-Skin, the\nfirst large-scale multimodal dermatology dataset that encompasses 3 imaging\nmodalities, including clinical, dermoscopic, and pathological and nearly 10k\nhigh-quality image-text pairs collected from professional textbooks. In\naddition, we generate over 27k diverse, instruction-following vision question\nanswering (VQA) samples (9 times the size of current largest dermatology VQA\ndataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a\ndermatology-specific VLM designed for precise and nuanced skin disease\ninterpretation. Comprehensive benchmark evaluations of SkinVL on VQA,\nsupervised fine-tuning (SFT) and zero-shot classification tasks across 8\ndatasets, reveal its exceptional performance for skin diseases in comparison to\nboth general and medical VLM models. The introduction of MM-Skin and SkinVL\noffers a meaningful contribution to advancing the development of clinical\ndermatology VLM assistants. MM-Skin is available at\nhttps://github.com/ZwQ803/MM-Skin", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u76ae\u80a4\u75c5\u591a\u6a21\u6001\u6570\u636e\u96c6MM-Skin\uff0c\u5305\u542b\u8fd110k\u9ad8\u8d28\u91cf\u56fe\u6587\u5bf9\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u76ae\u80a4\u75c5\u4e13\u7528VLM\u6a21\u578bSkinVL\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u53ca\u533b\u7597VLM\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u76ae\u80a4\u75c5\u591a\u6a21\u6001\u6570\u636e\u96c6\u7f3a\u4e4f\u4e13\u4e1a\u6027\u6587\u672c\u63cf\u8ff0\uff0c\u9650\u5236\u4e86\u76ae\u80a4\u75c5\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u53d1\u5c55\uff0c\u9700\u8981\u6784\u5efa\u66f4\u4e13\u4e1a\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ee5\u63d0\u5347\u8bca\u65ad\u80fd\u529b\u3002", "method": "\u6784\u5efaMM-Skin\u6570\u636e\u96c6\uff08\u542b3\u79cd\u5f71\u50cf\u6a21\u6001\u548c\u8fd110k\u56fe\u6587\u5bf9\uff09\uff0c\u5e76\u751f\u621027k\u591a\u6837\u5316VQA\u6837\u672c\uff1b\u57fa\u4e8e\u6b64\u5f00\u53d1\u76ae\u80a4\u75c5\u4e13\u7528VLM\u6a21\u578bSkinVL\u3002", "result": "SkinVL\u5728VQA\u3001\u76d1\u7763\u5fae\u8c03\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c8\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u901a\u7528\u53ca\u533b\u7597VLM\u6a21\u578b\u3002", "conclusion": "MM-Skin\u548cSkinVL\u4e3a\u76ae\u80a4\u75c5\u4e34\u5e8aVLM\u52a9\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\uff0c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.06175", "pdf": "https://arxiv.org/pdf/2505.06175", "abs": "https://arxiv.org/abs/2505.06175", "authors": ["Zihang Song", "Matteo Zecchin", "Bipin Rajendran", "Osvaldo Simeone"], "title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u7f16\u7801\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u7cfb\u7edf\u4e2d\u7684\u8f6f\u8f93\u5165\u8f6f\u8f93\u51fa\u4fe1\u9053\u5747\u8861\u3002\u901a\u8fc7\u7ed3\u5408\u89e3\u7801\u5668\u53cd\u9988\u548c\u63d0\u793a\u589e\u5f3a\u6280\u672f\uff0cICL\u6a21\u578b\u80fd\u591f\u8fed\u4ee3\u4f18\u5316\u7b26\u53f7\u4f30\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u7ebf\u6027\u5047\u8bbe\u5728\u4f4e\u5206\u8fa8\u7387\u91cf\u5316\u7b49\u590d\u6742\u573a\u666f\u4e0b\u5931\u6548\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7075\u611f\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u5b66\u4e60\u7b26\u53f7\u540e\u9a8c\u5206\u5e03\u7684ICL\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u548c\u72b6\u6001\u7a7a\u95f4\u67b6\u6784\u7684ICL\u6a21\u578b\uff0c\u5229\u7528\u63d0\u793a\u589e\u5f3a\u6280\u672f\u5c06\u89e3\u7801\u5668\u7684\u5916\u90e8\u4fe1\u606f\u4f5c\u4e3a\u989d\u5916\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cICL\u5747\u8861\u5668\u5728\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u6216\u8d44\u6e90\u53d7\u9650\u65f6\uff0cTransformer\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5206\u522b\u663e\u793a\u51fa\u4e0d\u540c\u7684\u4f18\u52bf\u3002", "conclusion": "ICL\u6846\u67b6\u4e3a\u89e3\u51b3\u590d\u6742\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u5747\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u975e\u7ebf\u6027\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.05619", "pdf": "https://arxiv.org/pdf/2505.05619", "abs": "https://arxiv.org/abs/2505.05619", "authors": ["Kalyan Nakka", "Jimmy Dani", "Ausmit Mondal", "Nitesh Saxena"], "title": "LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities", "categories": ["cs.CR", "cs.LG"], "comment": "14 pages, 18 figures, and 4 tables", "summary": "The growing adoption of Large Language Models (LLMs) has influenced the\ndevelopment of their lighter counterparts-Small Language Models (SLMs)-to\nenable on-device deployment across smartphones and edge devices. These SLMs\noffer enhanced privacy, reduced latency, server-free functionality, and\nimproved user experience. However, due to resource constraints of on-device\nenvironment, SLMs undergo size optimization through compression techniques like\nquantization, which can inadvertently introduce fairness, ethical and privacy\nrisks. Critically, quantized SLMs may respond to harmful queries directly,\nwithout requiring adversarial manipulation, raising significant safety and\ntrust concerns.\n  To address this, we propose LiteLMGuard (LLMG), an on-device prompt guard\nthat provides real-time, prompt-level defense for quantized SLMs. Additionally,\nour prompt guard is designed to be model-agnostic such that it can be\nseamlessly integrated with any SLM, operating independently of underlying\narchitectures. Our LLMG formalizes prompt filtering as a deep learning\n(DL)-based prompt answerability classification task, leveraging semantic\nunderstanding to determine whether a query should be answered by any SLM. Using\nour curated dataset, Answerable-or-Not, we trained and fine-tuned several DL\nmodels and selected ELECTRA as the candidate, with 97.75% answerability\nclassification accuracy.\n  Our safety effectiveness evaluations demonstrate that LLMG defends against\nover 87% of harmful prompts, including both direct instruction and jailbreak\nattack strategies. We further showcase its ability to mitigate the Open\nKnowledge Attacks, where compromised SLMs provide unsafe responses without\nadversarial prompting. In terms of prompt filtering effectiveness, LLMG\nachieves near state-of-the-art filtering accuracy of 94%, with an average\nlatency of 135 ms, incurring negligible overhead for users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LiteLMGuard (LLMG)\uff0c\u4e00\u79cd\u9488\u5bf9\u91cf\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u5b9e\u65f6\u63d0\u793a\u9632\u62a4\u673a\u5236\uff0c\u65e8\u5728\u89e3\u51b3\u56e0\u91cf\u5316\u5bfc\u81f4\u7684\u516c\u5e73\u6027\u3001\u4f26\u7406\u548c\u9690\u79c1\u98ce\u9669\u3002LLMG\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u4efb\u52a1\u8fc7\u6ee4\u6709\u5bb3\u63d0\u793a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe97.75%\uff0c\u80fd\u9632\u5fa187%\u4ee5\u4e0a\u7684\u6709\u5bb3\u63d0\u793a\uff0c\u4e14\u5ef6\u8fdf\u4f4e\u81f3135\u6beb\u79d2\u3002", "motivation": "\u7531\u4e8e\u91cf\u5316SLM\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u53ef\u80fd\u5f15\u53d1\u516c\u5e73\u6027\u3001\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u751a\u81f3\u65e0\u9700\u5bf9\u6297\u6027\u64cd\u4f5c\u5373\u53ef\u54cd\u5e94\u6709\u5bb3\u67e5\u8be2\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5b9e\u65f6\u9632\u62a4\u673a\u5236\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u63d0\u51faLLMG\uff0c\u5c06\u5176\u8bbe\u8ba1\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u63d0\u793a\u9632\u62a4\u5de5\u5177\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\uff08ELECTRA\u6a21\u578b\uff09\u7684\u63d0\u793a\u53ef\u7b54\u6027\u5206\u7c7b\u4efb\u52a1\uff0c\u5229\u7528\u8bed\u4e49\u7406\u89e3\u8fc7\u6ee4\u4e0d\u5e94\u7531SLM\u56de\u7b54\u7684\u67e5\u8be2\u3002\u4f7f\u7528\u81ea\u5efa\u6570\u636e\u96c6Answerable-or-Not\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "LLMG\u5728\u5b89\u5168\u6709\u6548\u6027\u6d4b\u8bd5\u4e2d\u9632\u5fa1\u4e8687%\u4ee5\u4e0a\u7684\u6709\u5bb3\u63d0\u793a\uff08\u5305\u62ec\u76f4\u63a5\u6307\u4ee4\u548c\u8d8a\u72f1\u653b\u51fb\uff09\uff0c\u5e76\u5728\u5f00\u653e\u77e5\u8bc6\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u63d0\u793a\u8fc7\u6ee4\u51c6\u786e\u7387\u8fbe94%\uff0c\u5e73\u5747\u5ef6\u8fdf135\u6beb\u79d2\uff0c\u7528\u6237\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "conclusion": "LLMG\u4e3a\u91cf\u5316SLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u9632\u62a4\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u5907\u7aef\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e14\u5177\u5907\u6a21\u578b\u65e0\u5173\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2505.06218", "pdf": "https://arxiv.org/pdf/2505.06218", "abs": "https://arxiv.org/abs/2505.06218", "authors": ["Kwan-Yee Lin", "Stella X. Yu"], "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "CVPR 2025. Project page:\n  https://lego-h-humanoidrobothiking.github.io/", "summary": "Hiking on complex trails demands balance, agility, and adaptive\ndecision-making over unpredictable terrain. Current humanoid research remains\nfragmented and inadequate for hiking: locomotion focuses on motor skills\nwithout long-term goals or situational awareness, while semantic navigation\noverlooks real-world embodiment and local terrain variability. We propose\ntraining humanoids to hike on complex trails, driving integrative skill\ndevelopment across visual perception, decision making, and motor execution. We\ndevelop a learning framework, LEGO-H, that enables a vision-equipped humanoid\nrobot to hike complex trails autonomously. We introduce two technical\ninnovations: 1) A temporal vision transformer variant - tailored into\nHierarchical Reinforcement Learning framework - anticipates future local goals\nto guide movement, seamlessly integrating locomotion with goal-directed\nnavigation. 2) Latent representations of joint movement patterns, combined with\nhierarchical metric learning - enhance Privileged Learning scheme - enable\nsmooth policy transfer from privileged training to onboard execution. These\ncomponents allow LEGO-H to handle diverse physical and environmental challenges\nwithout relying on predefined motion patterns. Experiments across varied\nsimulated trails and robot morphologies highlight LEGO-H's versatility and\nrobustness, positioning hiking as a compelling testbed for embodied autonomy\nand LEGO-H as a baseline for future humanoid development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEGO-H\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u81ea\u4e3b\u5f92\u6b65\u590d\u6742\u5c0f\u5f84\uff0c\u6574\u5408\u89c6\u89c9\u611f\u77e5\u3001\u51b3\u7b56\u5236\u5b9a\u548c\u8fd0\u52a8\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u7814\u7a76\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5f92\u6b65\u590d\u6742\u5730\u5f62\u7684\u6311\u6218\uff0c\u7f3a\u4e4f\u957f\u671f\u76ee\u6807\u548c\u60c5\u5883\u611f\u77e5\u3002LEGO-H\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u6280\u672f\u521b\u65b0\u5b9e\u73b0\uff1a\u4e00\u662f\u57fa\u4e8e\u65f6\u95f4\u89c6\u89c9\u53d8\u6362\u5668\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9884\u6d4b\u672a\u6765\u76ee\u6807\u6307\u5bfc\u8fd0\u52a8\uff1b\u4e8c\u662f\u6f5c\u5728\u5173\u8282\u8fd0\u52a8\u6a21\u5f0f\u8868\u793a\u7ed3\u5408\u5206\u5c42\u5ea6\u91cf\u5b66\u4e60\uff0c\u5b9e\u73b0\u7b56\u7565\u4ece\u8bad\u7ec3\u5230\u5b9e\u9645\u6267\u884c\u7684\u5e73\u6ed1\u8f6c\u79fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLEGO-H\u80fd\u5e94\u5bf9\u591a\u6837\u7684\u7269\u7406\u548c\u73af\u5883\u6311\u6218\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "LEGO-H\u5c55\u793a\u4e86\u5f92\u6b65\u4f5c\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u6d4b\u8bd5\u5e73\u53f0\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u4eba\u5f62\u673a\u5668\u4eba\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2505.05657", "pdf": "https://arxiv.org/pdf/2505.05657", "abs": "https://arxiv.org/abs/2505.05657", "authors": ["Zhongweiyang Xu", "Xulin Fan", "Zhong-Qiu Wang", "Xilin Jiang", "Romit Roy Choudhury"], "title": "Unsupervised Blind Speech Separation with a Diffusion Prior", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "eess.SP"], "comment": "Paper Accepted at ICML2025 Demo:\n  https://arraydps.github.io/ArrayDPSDemo/ Code:\n  https://github.com/ArrayDPS/ArrayDPS", "summary": "Blind Speech Separation (BSS) aims to separate multiple speech sources from\naudio mixtures recorded by a microphone array. The problem is challenging\nbecause it is a blind inverse problem, i.e., the microphone array geometry, the\nroom impulse response (RIR), and the speech sources, are all unknown. We\npropose ArrayDPS to solve the BSS problem in an unsupervised, array-agnostic,\nand generative manner. The core idea builds on diffusion posterior sampling\n(DPS), but unlike DPS where the likelihood is tractable, ArrayDPS must\napproximate the likelihood by formulating a separate optimization problem. The\nsolution to the optimization approximates room acoustics and the relative\ntransfer functions between microphones. These approximations, along with the\ndiffusion priors, iterate through the ArrayDPS sampling process and ultimately\nyield separated voice sources. We only need a simple single-speaker speech\ndiffusion model as a prior along with the mixtures recorded at the microphones;\nno microphone array information is necessary. Evaluation results show that\nArrayDPS outperforms all baseline unsupervised methods while being comparable\nto supervised methods in terms of SDR. Audio demos are provided at:\nhttps://arraydps.github.io/ArrayDPSDemo/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ArrayDPS\u65b9\u6cd5\uff0c\u4ee5\u65e0\u76d1\u7763\u3001\u9635\u5217\u65e0\u5173\u7684\u65b9\u5f0f\u89e3\u51b3\u76f2\u8bed\u97f3\u5206\u79bb\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u540e\u9a8c\u91c7\u6837\u548c\u4f18\u5316\u8fd1\u4f3c\u5b9e\u73b0\uff0c\u6027\u80fd\u4f18\u4e8e\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u76f2\u8bed\u97f3\u5206\u79bb\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u9006\u95ee\u9898\uff0c\u6d89\u53ca\u672a\u77e5\u7684\u9ea6\u514b\u98ce\u9635\u5217\u51e0\u4f55\u5f62\u72b6\u3001\u623f\u95f4\u8109\u51b2\u54cd\u5e94\u548c\u8bed\u97f3\u6e90\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u6216\u5df2\u77e5\u9635\u5217\u4fe1\u606f\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u76d1\u7763\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u9635\u5217\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "ArrayDPS\u57fa\u4e8e\u6269\u6563\u540e\u9a8c\u91c7\u6837\uff08DPS\uff09\uff0c\u901a\u8fc7\u5355\u72ec\u4f18\u5316\u95ee\u9898\u8fd1\u4f3c\u623f\u95f4\u58f0\u5b66\u548c\u9ea6\u514b\u98ce\u95f4\u76f8\u5bf9\u4f20\u9012\u51fd\u6570\uff0c\u7ed3\u5408\u6269\u6563\u5148\u9a8c\uff0c\u8fed\u4ee3\u5b9e\u73b0\u8bed\u97f3\u5206\u79bb\u3002\u4ec5\u9700\u5355\u8bf4\u8bdd\u4eba\u8bed\u97f3\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u65e0\u9700\u9635\u5217\u4fe1\u606f\u3002", "result": "ArrayDPS\u5728\u4fe1\u53f7\u5931\u771f\u6bd4\uff08SDR\uff09\u4e0a\u4f18\u4e8e\u6240\u6709\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e0e\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "ArrayDPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u76f2\u8bed\u97f3\u5206\u79bb\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u9635\u5217\u4fe1\u606f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.05659", "pdf": "https://arxiv.org/pdf/2505.05659", "abs": "https://arxiv.org/abs/2505.05659", "authors": ["Guilherme Vieira Neto", "Marcos Eduardo Valle"], "title": "V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "EfficientNet models are convolutional neural networks optimized for parameter\nallocation by jointly balancing network width, depth, and resolution. Renowned\nfor their exceptional accuracy, these models have become a standard for image\nclassification tasks across diverse computer vision benchmarks. While\ntraditional neural networks learn correlations between feature channels during\ntraining, vector-valued neural networks inherently treat multidimensional data\nas coherent entities, taking for granted the inter-channel relationships. This\npaper introduces vector-valued EfficientNets (V-EfficientNets), a novel\nextension of EfficientNet designed to process arbitrary vector-valued data. The\nproposed models are evaluated on a medical image classification task, achieving\nan average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute\nlymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency,\nsignificantly reducing parameters while outperforming state-of-the-art models,\nincluding the original EfficientNet. The source code is available at\nhttps://github.com/mevalle/v-nets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86V-EfficientNets\uff0c\u5b83\u662f\u4e00\u79cd\u9488\u5bf9\u5411\u91cf\u503c\u6570\u636e\u4f18\u5316\u7684EfficientNet\u6269\u5c55\u6a21\u578b\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u53d6\u5f97\u4e8699.46%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u65f6\u5b66\u4e60\u7279\u5f81\u901a\u9053\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u800c\u5411\u91cf\u503c\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u5c06\u591a\u7ef4\u6570\u636e\u89c6\u4e3a\u8fde\u8d2f\u5b9e\u4f53\u3002\u4e3a\u4e86\u66f4\u9ad8\u6548\u5730\u5904\u7406\u5411\u91cf\u503c\u6570\u636e\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86V-EfficientNets\u3002", "method": "\u901a\u8fc7\u6269\u5c55EfficientNet\uff0c\u8bbe\u8ba1\u4e86V-EfficientNets\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u4efb\u610f\u5411\u91cf\u503c\u6570\u636e\u3002\u6a21\u578b\u4f18\u5316\u4e86\u53c2\u6570\u5206\u914d\uff0c\u5e73\u8861\u4e86\u7f51\u7edc\u7684\u5bbd\u5ea6\u3001\u6df1\u5ea6\u548c\u5206\u8fa8\u7387\u3002", "result": "\u5728ALL-IDB2\u6570\u636e\u96c6\u4e0a\u7684\u6025\u6027\u6dcb\u5df4\u7ec6\u80de\u767d\u8840\u75c5\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cV-EfficientNets\u8fbe\u5230\u4e8699.46%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u4e14\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "V-EfficientNets\u5728\u5904\u7406\u5411\u91cf\u503c\u6570\u636e\u65f6\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05689", "pdf": "https://arxiv.org/pdf/2505.05689", "abs": "https://arxiv.org/abs/2505.05689", "authors": ["Fuyao Chen", "Yuexi Du", "Tal Zeevi", "Nicha C. Dvornek", "John A. Onofrey"], "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Accepted by MIDL 2025", "summary": "Histopathology evaluation of tissue specimens through microscopic examination\nis essential for accurate disease diagnosis and prognosis. However, traditional\nmanual analysis by specially trained pathologists is time-consuming,\nlabor-intensive, cost-inefficient, and prone to inter-rater variability,\npotentially affecting diagnostic consistency and accuracy. As digital pathology\nimages continue to proliferate, there is a pressing need for automated analysis\nto address these challenges. Recent advancements in artificial\nintelligence-based tools such as machine learning (ML) models, have\nsignificantly enhanced the precision and efficiency of analyzing\nhistopathological slides. However, despite their impressive performance, ML\nmodels are invariant only to translation, lacking invariance to rotation and\nreflection. This limitation restricts their ability to generalize effectively,\nparticularly in histopathology, where images intrinsically lack meaningful\norientation. In this study, we develop robust, equivariant histopathological\nbiomarkers through a novel symmetric convolutional kernel via unsupervised\nsegmentation. The approach is validated using prostate tissue micro-array (TMA)\nimages from 50 patients in the Gleason 2019 Challenge public dataset. The\nbiomarkers extracted through this approach demonstrate enhanced robustness and\ngeneralizability against rotation compared to models using standard convolution\nkernels, holding promise for enhancing the accuracy, consistency, and\nrobustness of ML models in digital pathology. Ultimately, this work aims to\nimprove diagnostic and prognostic capabilities of histopathology beyond\nprostate cancer through equivariant imaging.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u5377\u79ef\u6838\u7684\u7b49\u53d8\u75c5\u7406\u751f\u7269\u6807\u8bb0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u75c5\u7406\u56fe\u50cf\u5206\u6790\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u9488\u5bf9\u65cb\u8f6c\u4e0d\u53d8\u6027\u95ee\u9898\uff0c\u572850\u4f8b\u524d\u5217\u817a\u764c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u4e00\u81f4\u6027\u5dee\uff1b\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u75c5\u7406\u56fe\u50cf\u7684\u65e0\u53d6\u5411\u7279\u6027\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u589e\u5f3a\u75c5\u7406\u5206\u6790\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5bf9\u79f0\u5377\u79ef\u6838\u7684\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u751f\u6210\u7b49\u53d8\u7684\u75c5\u7406\u751f\u7269\u6807\u8bb0\uff0c\u786e\u4fdd\u6a21\u578b\u5bf9\u65cb\u8f6c\u548c\u53cd\u5c04\u5177\u6709\u4e0d\u53d8\u6027\u3002\u65b9\u6cd5\u5728\u524d\u5217\u817a\u7ec4\u7ec7\u5fae\u9635\u5217\u56fe\u50cf\uff08Gleason 2019\u6570\u636e\u96c6\uff09\u4e0a\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u6807\u51c6\u5377\u79ef\u6838\u6a21\u578b\uff0c\u65b0\u65b9\u6cd5\u63d0\u53d6\u7684\u751f\u7269\u6807\u8bb0\u5728\u65cb\u8f6c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u63d0\u5347\u4e86\u75c5\u7406\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7b49\u53d8\u6210\u50cf\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u8d85\u8d8a\u524d\u5217\u817a\u764c\u9886\u57df\uff0c\u63d0\u5347\u75c5\u7406\u8bca\u65ad\u548c\u9884\u540e\u7684\u6574\u4f53\u80fd\u529b\u3002"}}
{"id": "2505.05691", "pdf": "https://arxiv.org/pdf/2505.05691", "abs": "https://arxiv.org/abs/2505.05691", "authors": ["Ruiqi Ni", "Zherong Pan", "Ahmed H Qureshi"], "title": "Physics-informed Temporal Difference Metric Learning for Robot Motion Planning", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted to ICLR 2025", "summary": "The motion planning problem involves finding a collision-free path from a\nrobot's starting to its target configuration. Recently, self-supervised\nlearning methods have emerged to tackle motion planning problems without\nrequiring expensive expert demonstrations. They solve the Eikonal equation for\ntraining neural networks and lead to efficient solutions. However, these\nmethods struggle in complex environments because they fail to maintain key\nproperties of the Eikonal equation, such as optimal value functions and\ngeodesic distances. To overcome these limitations, we propose a novel\nself-supervised temporal difference metric learning approach that solves the\nEikonal equation more accurately and enhances performance in solving complex\nand unseen planning tasks. Our method enforces Bellman's principle of\noptimality over finite regions, using temporal difference learning to avoid\nspurious local minima while incorporating metric learning to preserve the\nEikonal equation's essential geodesic properties. We demonstrate that our\napproach significantly outperforms existing self-supervised learning methods in\nhandling complex environments and generalizing to unseen environments, with\nrobot configurations ranging from 2 to 12 degrees of freedom (DOF).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6765\u66f4\u51c6\u786e\u5730\u89e3\u51b3\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684Eikonal\u65b9\u7a0b\uff0c\u589e\u5f3a\u4e86\u5728\u590d\u6742\u548c\u672a\u77e5\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u7ef4\u6301Eikonal\u65b9\u7a0b\u7684\u5173\u952e\u7279\u6027\uff08\u5982\u6700\u4f18\u503c\u51fd\u6570\u548c\u6d4b\u5730\u8ddd\u79bb\uff09\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u65f6\u95f4\u5dee\u5206\u5ea6\u91cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f3a\u5236\u6ee1\u8db3\u8d1d\u5c14\u66fc\u6700\u4f18\u6027\u539f\u5219\uff0c\u907f\u514d\u5c40\u90e8\u6781\u5c0f\u503c\uff0c\u540c\u65f6\u901a\u8fc7\u5ea6\u91cf\u5b66\u4e60\u4fdd\u7559\u6d4b\u5730\u7279\u6027\u3002", "result": "\u57282\u81f312\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u914d\u7f6e\u7684\u590d\u6742\u548c\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u66f4\u7cbe\u786e\u5730\u89e3\u51b3Eikonal\u65b9\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u6027\u80fd\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.05694", "pdf": "https://arxiv.org/pdf/2505.05694", "abs": "https://arxiv.org/abs/2505.05694", "authors": ["Ohida Binte Amin", "Varun Mishra", "Tinashe M. Tapera", "Robert Volpe", "Aarti Sathyanarayana"], "title": "Extending Stress Detection Reproducibility to Consumer Wearable Sensors", "categories": ["cs.HC", "cs.LG"], "comment": "Accepted at IEEE EMBC 2025", "summary": "Wearable sensors are widely used to collect physiological data and develop\nstress detection models. However, most studies focus on a single dataset,\nrarely evaluating model reproducibility across devices, populations, or study\nconditions. We previously assessed the reproducibility of stress detection\nmodels across multiple studies, testing models trained on one dataset against\nothers using heart rate (with R-R interval) and electrodermal activity (EDA).\nIn this study, we extended our stress detection reproducibility to consumer\nwearable sensors. We compared validated research-grade devices, to consumer\nwearables - Biopac MP160, Polar H10, Empatica E4, to the Garmin Forerunner 55s,\nassessing device-specific stress detection performance by conducting a new\nstress study on undergraduate students. Thirty-five students completed three\nstandardized stress-induction tasks in a lab setting. Biopac MP160 performed\nthe best, being consistent with our expectations of it as the gold standard,\nthough performance varied across devices and models. Combining heart rate\nvariability (HRV) and EDA enhanced stress prediction across most scenarios.\nHowever, Empatica E4 showed variability; while HRV and EDA improved stress\ndetection in leave-one-subject-out (LOSO) evaluations (AUROC up to 0.953),\ndevice-specific limitations led to underperformance when tested with our\npre-trained stress detection tool (AUROC 0.723), highlighting generalizability\nchallenges related to hardware-model compatibility. Garmin Forerunner 55s\ndemonstrated strong potential for real-world stress monitoring, achieving the\nbest mental arithmetic stress detection performance in LOSO (AUROC up to 0.961)\ncomparable to research-grade devices like Polar H10 (AUROC 0.954), and Empatica\nE4 (AUROC 0.905 with HRV-only model and AUROC 0.953 with HRV+EDA model), with\nthe added advantage of consumer-friendly wearability for free-living contexts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u5728\u538b\u529b\u68c0\u6d4b\u6a21\u578b\u4e2d\u7684\u53ef\u91cd\u590d\u6027\uff0c\u6bd4\u8f83\u4e86\u7814\u7a76\u7ea7\u8bbe\u5907\u4e0e\u6d88\u8d39\u7ea7\u8bbe\u5907\u7684\u6027\u80fd\u3002Biopac MP160\u8868\u73b0\u6700\u4f73\uff0c\u800cGarmin Forerunner 55s\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u8bbe\u5907\u3001\u4eba\u7fa4\u6216\u7814\u7a76\u6761\u4ef6\u4e0b\u6a21\u578b\u53ef\u91cd\u590d\u6027\u7684\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u591a\u79cd\u8bbe\u5907\uff08Biopac MP160, Polar H10, Empatica E4, Garmin Forerunner 55s\uff09\uff0c\u572835\u540d\u672c\u79d1\u751f\u4e2d\u8fdb\u884c\u6807\u51c6\u5316\u538b\u529b\u8bf1\u5bfc\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bbe\u5907\u7279\u5f02\u6027\u538b\u529b\u68c0\u6d4b\u6027\u80fd\u3002", "result": "Biopac MP160\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u800cGarmin Forerunner 55s\u5728\u5b9e\u65f6\u538b\u529b\u76d1\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5fc3\u7b97\u4efb\u52a1\u4e2dAUROC\u9ad8\u8fbe0.961\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u786c\u4ef6-\u6a21\u578b\u517c\u5bb9\u6027\u5bf9\u538b\u529b\u68c0\u6d4b\u901a\u7528\u6027\u7684\u6311\u6218\uff0c\u4f46\u6d88\u8d39\u7ea7\u8bbe\u5907\u5982Garmin Forerunner 55s\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.05713", "pdf": "https://arxiv.org/pdf/2505.05713", "abs": "https://arxiv.org/abs/2505.05713", "authors": ["Jinkun Lin", "Ziheng Jiang", "Zuquan Song", "Sida Zhao", "Menghan Yu", "Zhanghan Wang", "Chenyuan Wang", "Zuocheng Shi", "Xiang Shi", "Wei Jia", "Zherui Liu", "Shuguang Wang", "Haibin Lin", "Xiu Liu", "Aurojit Panda", "Jinyang Li"], "title": "Understanding Stragglers in Large Model Training Using What-if Analysis", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Large language model (LLM) training is one of the most demanding distributed\ncomputations today, often requiring thousands of GPUs with frequent\nsynchronization across machines. Such a workload pattern makes it susceptible\nto stragglers, where the training can be stalled by few slow workers. At\nByteDance we find stragglers are not trivially always caused by hardware\nfailures, but can arise from multiple complex factors. This work aims to\npresent a comprehensive study on the straggler issues in LLM training, using a\nfive-month trace collected from our ByteDance LLM training cluster. The core\nmethodology is what-if analysis that simulates the scenario without any\nstragglers and contrasts with the actual case. We use this method to study the\nfollowing questions: (1) how often do stragglers affect training jobs, and what\neffect do they have on job performance; (2) do stragglers exhibit temporal or\nspatial patterns; and (3) what are the potential root causes for stragglers?", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86LLM\u8bad\u7ec3\u4e2d\u7684\u62d6\u540e\u817f\u73b0\u8c61\uff08stragglers\uff09\uff0c\u901a\u8fc7\u4e94\u4e2a\u6708\u7684\u8ddf\u8e2a\u6570\u636e\uff0c\u4f7f\u7528\u5047\u8bbe\u5206\u6790\u65b9\u6cd5\u63a2\u8ba8\u4e86\u5176\u53d1\u751f\u9891\u7387\u3001\u5f71\u54cd\u3001\u65f6\u7a7a\u6a21\u5f0f\u53ca\u6839\u672c\u539f\u56e0\u3002", "motivation": "LLM\u8bad\u7ec3\u5bf9\u5206\u5e03\u5f0f\u8ba1\u7b97\u8981\u6c42\u6781\u9ad8\uff0c\u5e38\u56e0\u62d6\u540e\u817f\u95ee\u9898\uff08\u5c11\u6570\u6162\u901f\u5de5\u4f5c\u8282\u70b9\u5bfc\u81f4\u6574\u4f53\u505c\u6ede\uff09\u800c\u53d7\u9650\u3002ByteDance\u53d1\u73b0\u62d6\u540e\u817f\u95ee\u9898\u4e0d\u4ec5\u6e90\u4e8e\u786c\u4ef6\u6545\u969c\uff0c\u8fd8\u6d89\u53ca\u590d\u6742\u56e0\u7d20\uff0c\u56e0\u6b64\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u57fa\u4e8eByteDance LLM\u8bad\u7ec3\u96c6\u7fa4\u7684\u4e94\u4e2a\u6708\u8ddf\u8e2a\u6570\u636e\uff0c\u91c7\u7528\u5047\u8bbe\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u5b9e\u9645\u62d6\u540e\u817f\u573a\u666f\u4e0e\u65e0\u62d6\u540e\u817f\u7684\u7406\u60f3\u573a\u666f\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u62d6\u540e\u817f\u95ee\u9898\u7684\u53d1\u751f\u9891\u7387\u3001\u5bf9\u8bad\u7ec3\u6027\u80fd\u7684\u5f71\u54cd\u3001\u65f6\u7a7a\u6a21\u5f0f\u53ca\u6f5c\u5728\u6839\u672c\u539f\u56e0\uff0c\u4e3a\u4f18\u5316LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u62d6\u540e\u817f\u95ee\u9898\u662fLLM\u8bad\u7ec3\u7684\u91cd\u8981\u74f6\u9888\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002\u5047\u8bbe\u5206\u6790\u65b9\u6cd5\u662f\u8bc6\u522b\u548c\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.05752", "pdf": "https://arxiv.org/pdf/2505.05752", "abs": "https://arxiv.org/abs/2505.05752", "authors": ["Amin Ghafourian", "Andrew Lee", "Dechen Gao", "Tyler Beer", "Kin Yen", "Iman Soltani"], "title": "Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data", "categories": ["cs.CV", "cs.CY", "cs.LG", "cs.RO", "eess.IV"], "comment": "19 pages, 15 figures, 4 tables", "summary": "Automation can play a prominent role in improving efficiency, accuracy, and\nscalability in infrastructure surveying and assessing construction and\ncompliance standards. This paper presents a framework for automation of\ngeometric measurements and compliance assessment using point cloud data. The\nproposed approach integrates deep learning-based detection and segmentation, in\nconjunction with geometric and signal processing techniques, to automate\nsurveying tasks. As a proof of concept, we apply this framework to\nautomatically evaluate the compliance of curb ramps with the Americans with\nDisabilities Act (ADA), demonstrating the utility of point cloud data in survey\nautomation. The method leverages a newly collected, large annotated dataset of\ncurb ramps, made publicly available as part of this work, to facilitate robust\nmodel training and evaluation. Experimental results, including comparison with\nmanual field measurements of several ramps, validate the accuracy and\nreliability of the proposed method, highlighting its potential to significantly\nreduce manual effort and improve consistency in infrastructure assessment.\nBeyond ADA compliance, the proposed framework lays the groundwork for broader\napplications in infrastructure surveying and automated construction evaluation,\npromoting wider adoption of point cloud data in these domains. The annotated\ndatabase, manual ramp survey data, and developed algorithms are publicly\navailable on the project's GitHub page:\nhttps://github.com/Soltanilara/SurveyAutomation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u70b9\u4e91\u6570\u636e\u81ea\u52a8\u5316\u51e0\u4f55\u6d4b\u91cf\u548c\u5408\u89c4\u8bc4\u4f30\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u51e0\u4f55/\u4fe1\u53f7\u5904\u7406\u6280\u672f\uff0c\u4ee5\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u901a\u8fc7\u5e94\u7528\u4e8eADA\u6807\u51c6\u7684\u4eba\u884c\u9053\u5761\u9053\u5408\u89c4\u8bc4\u4f30\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u80fd\u63d0\u9ad8\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u800c\u70b9\u4e91\u6570\u636e\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u5c1a\u6709\u63a2\u7d22\u7a7a\u95f4\u3002", "method": "\u6574\u5408\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u4e0e\u5206\u5272\u6280\u672f\uff0c\u7ed3\u5408\u51e0\u4f55\u548c\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u6d4b\u91cf\u4e0e\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30ADA\u5408\u89c4\u6027\u65f6\u51c6\u786e\u53ef\u9760\uff0c\u4e0e\u4eba\u5de5\u6d4b\u91cf\u7ed3\u679c\u76f8\u7b26\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u4eba\u529b\u6210\u672c\u5e76\u63d0\u9ad8\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8eADA\u5408\u89c4\u8bc4\u4f30\uff0c\u8fd8\u4e3a\u66f4\u5e7f\u6cdb\u7684\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u548c\u81ea\u52a8\u5316\u65bd\u5de5\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u70b9\u4e91\u6570\u636e\u5728\u8fd9\u4e9b\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2505.05816", "pdf": "https://arxiv.org/pdf/2505.05816", "abs": "https://arxiv.org/abs/2505.05816", "authors": ["Antti Koskela", "Mohamed Seif", "Andrea J. Goldsmith"], "title": "On the Price of Differential Privacy for Spectral Clustering over Stochastic Block Models", "categories": ["cs.SI", "cs.CR", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "We investigate privacy-preserving spectral clustering for community detection\nwithin stochastic block models (SBMs). Specifically, we focus on edge\ndifferential privacy (DP) and propose private algorithms for community\nrecovery. Our work explores the fundamental trade-offs between the privacy\nbudget and the accurate recovery of community labels. Furthermore, we establish\ninformation-theoretic conditions that guarantee the accuracy of our methods,\nproviding theoretical assurances for successful community recovery under edge\nDP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u968f\u673a\u5757\u6a21\u578b\uff08SBMs\uff09\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u8c31\u805a\u7c7b\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fb9\u7f18\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u7684\u7b97\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u9690\u79c1\u9884\u7b97\u4e0e\u793e\u533a\u6807\u7b7e\u51c6\u786e\u6062\u590d\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5982\u4f55\u5728\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\uff08\u901a\u8fc7\u8fb9\u7f18\u5dee\u5206\u9690\u79c1\uff09\u7684\u540c\u65f6\uff0c\u6709\u6548\u5730\u8fdb\u884c\u793e\u533a\u68c0\u6d4b\uff0c\u89e3\u51b3\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8c31\u805a\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fb9\u7f18\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u8bbe\u8ba1\u4e86\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u7528\u4e8e\u793e\u533a\u6807\u7b7e\u7684\u6062\u590d\u3002", "result": "\u7814\u7a76\u5efa\u7acb\u4e86\u4fe1\u606f\u8bba\u6761\u4ef6\uff0c\u786e\u4fdd\u4e86\u5728\u8fb9\u7f18\u5dee\u5206\u9690\u79c1\u4e0b\u793e\u533a\u6807\u7b7e\u7684\u51c6\u786e\u6062\u590d\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5728\u9690\u79c1\u4fdd\u62a4\u6761\u4ef6\u4e0b\u8fdb\u884c\u793e\u533a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u548c\u7406\u8bba\u652f\u6301\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u9700\u6c42\u3002"}}
{"id": "2505.05829", "pdf": "https://arxiv.org/pdf/2505.05829", "abs": "https://arxiv.org/abs/2505.05829", "authors": ["Zhiyuan Chen", "Keyi Li", "Yifan Jia", "Le Ye", "Yufei Ma"], "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "accepted by CVPR2025", "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u91cf\u6821\u51c6\u7f13\u5b58\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u6269\u6563\u53d8\u538b\u5668\uff08DiT\uff09\u6a21\u578b\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u901a\u9053\u611f\u77e5SVD\u6280\u672f\uff0c\u6709\u6548\u6821\u51c6\u7f13\u5b58\u5e76\u5904\u7406\u5f02\u5e38\u6fc0\u6d3b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u53d8\u538b\u5668\uff08DiT\uff09\u6a21\u578b\u867d\u7136\u751f\u6210\u80fd\u529b\u5f3a\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u5176\u8fed\u4ee3\u6027\u8d28\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u7f13\u5b58\u52a0\u901f\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6821\u51c6\u53ef\u80fd\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u589e\u91cf\u6821\u51c6\u7f13\u5b58\u6280\u672f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u6821\u51c6\u53c2\u6570\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u4f18\u5316\u3002\u9488\u5bf9\u5f02\u5e38\u6fc0\u6d3b\u5f15\u5165\u901a\u9053\u611f\u77e5SVD\uff0c\u8fdb\u4e00\u6b65\u5f3a\u5316\u6821\u51c6\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6027\u80fd\u4f18\u4e8e\u6734\u7d20\u7f13\u5b58\u65b9\u6cd5\uff0c\u76f8\u6bd435\u6b65DDIM\u51cf\u5c1145%\u4ee5\u4e0a\u8ba1\u7b97\uff0cIS\u63d0\u534712\uff0cFID\u4ec5\u589e\u52a0\u4e0d\u52300.06\u3002", "conclusion": "\u589e\u91cf\u6821\u51c6\u7f13\u5b58\u662f\u4e00\u79cd\u9ad8\u6548\u7684DiT\u52a0\u901f\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u4e14\u51e0\u4e4e\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2505.05842", "pdf": "https://arxiv.org/pdf/2505.05842", "abs": "https://arxiv.org/abs/2505.05842", "authors": ["Yun Xin", "Jianfeng Lu", "Shuqin Cao", "Gang Li", "Haozhao Wang", "Guanghui Wen"], "title": "DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning under Two-sided Incomplete Information", "categories": ["cs.GT", "cs.LG", "stat.ML"], "comment": null, "summary": "Online Federated Learning (OFL) is a real-time learning paradigm that\nsequentially executes parameter aggregation immediately for each random\narriving client. To motivate clients to participate in OFL, it is crucial to\noffer appropriate incentives to offset the training resource consumption.\nHowever, the design of incentive mechanisms in OFL is constrained by the\ndynamic variability of Two-sided Incomplete Information (TII) concerning\nresources, where the server is unaware of the clients' dynamically changing\ncomputational resources, while clients lack knowledge of the real-time\ncommunication resources allocated by the server. To incentivize clients to\nparticipate in training by offering dynamic rewards to each arriving client, we\ndesign a novel Dynamic Bayesian persuasion pricing for online Federated\nlearning (DaringFed) under TII. Specifically, we begin by formulating the\ninteraction between the server and clients as a dynamic signaling and pricing\nallocation problem within a Bayesian persuasion game, and then demonstrate the\nexistence of a unique Bayesian persuasion Nash equilibrium. By deriving the\noptimal design of DaringFed under one-sided incomplete information, we further\nanalyze the approximate optimal design of DaringFed with a specific bound under\nTII. Finally, extensive evaluation conducted on real datasets demonstrate that\nDaringFed optimizes accuracy and converges speed by 16.99%, while experiments\nwith synthetic datasets validate the convergence of estimate unknown values and\nthe effectiveness of DaringFed in improving the server's utility by up to\n12.6%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDaringFed\u7684\u52a8\u6001\u8d1d\u53f6\u65af\u8bf4\u670d\u5b9a\u4ef7\u673a\u5236\uff0c\u7528\u4e8e\u5728\u7ebf\u8054\u90a6\u5b66\u4e60\uff08OFL\uff09\u4e2d\u4ee5\u6fc0\u52b1\u5ba2\u6237\u7aef\u53c2\u4e0e\u3002\u8be5\u673a\u5236\u5728\u53cc\u5411\u4e0d\u5b8c\u5168\u4fe1\u606f\uff08TII\uff09\u6761\u4ef6\u4e0b\u52a8\u6001\u5206\u914d\u5956\u52b1\uff0c\u4f18\u5316\u4e86\u6a21\u578b\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u5728\u7ebf\u8054\u90a6\u5b66\u4e60\uff08OFL\uff09\u9700\u8981\u5ba2\u6237\u7aef\u52a8\u6001\u53c2\u4e0e\uff0c\u4f46\u4f20\u7edf\u7684\u6fc0\u52b1\u673a\u5236\u65e0\u6cd5\u5e94\u5bf9\u53cc\u5411\u4e0d\u5b8c\u5168\u4fe1\u606f\uff08TII\uff09\u6761\u4ef6\u4e0b\u7684\u52a8\u6001\u8d44\u6e90\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u8bbe\u8ba1\u4e00\u79cd\u6709\u6548\u7684\u52a8\u6001\u6fc0\u52b1\u673a\u5236\u6210\u4e3a\u5173\u952e\u3002", "method": "\u8bba\u6587\u5c06\u670d\u52a1\u5668\u4e0e\u5ba2\u6237\u7aef\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u4e2a\u52a8\u6001\u4fe1\u53f7\u4e0e\u5b9a\u4ef7\u5206\u914d\u7684\u8d1d\u53f6\u65af\u8bf4\u670d\u535a\u5f08\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u5b58\u5728\u552f\u4e00\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u5728\u4e00\u4fa7\u4e0d\u5b8c\u5168\u4fe1\u606f\u6761\u4ef6\u4e0b\u63a8\u5bfc\u51fa\u6700\u4f18\u8bbe\u8ba1\uff0c\u8fdb\u4e00\u6b65\u5206\u6790\u4e86TII\u6761\u4ef6\u4e0b\u7684\u8fd1\u4f3c\u6700\u4f18\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDaringFed\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u5316\u4e8616.99%\u7684\u51c6\u786e\u6027\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u80fd\u6709\u6548\u63d0\u5347\u670d\u52a1\u5668\u6548\u7528\u8fbe12.6%\u3002", "conclusion": "DaringFed\u5728\u53cc\u5411\u4e0d\u5b8c\u5168\u4fe1\u606f\u6761\u4ef6\u4e0b\u901a\u8fc7\u52a8\u6001\u6fc0\u52b1\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u8054\u90a6\u5b66\u4e60\u7684\u53c2\u4e0e\u5ea6\u548c\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05845", "pdf": "https://arxiv.org/pdf/2505.05845", "abs": "https://arxiv.org/abs/2505.05845", "authors": ["Guohao Lin", "Shidong Pan", "Rasul Khanbayov", "Changxi Yang", "Ani Khaloian-Sarnaghi", "Andriy Kovryga"], "title": "Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Knots in wood are critical to both aesthetics and structural integrity,\nmaking their detection and pairing essential in timber processing. However,\ntraditional manual annotation was labor-intensive and inefficient,\nnecessitating automation. This paper proposes a lightweight and fully automated\npipeline for knot detection and pairing based on machine learning techniques.\nIn the detection stage, high-resolution surface images of wooden boards were\ncollected using industrial-grade cameras, and a large-scale dataset was\nmanually annotated and preprocessed. After the transfer learning, the YOLOv8l\nachieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were\nanalyzed and paired based on multidimensional feature extraction. A triplet\nneural network was used to map the features into a latent space, enabling\nclustering algorithms to identify and pair corresponding knots. The triplet\nnetwork with learnable weights achieved a pairing accuracy of 0.85. Further\nanalysis revealed that he distances from the knot's start and end points to the\nbottom of the wooden board, and the longitudinal coordinates play crucial roles\nin achieving high pairing accuracy. Our experiments validate the effectiveness\nof the proposed solution, demonstrating the potential of AI in advancing wood\nscience and industry.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u5168\u81ea\u52a8\u7ba1\u9053\uff0c\u7528\u4e8e\u6728\u6750\u4e2d\u7ed3\u7684\u68c0\u6d4b\u548c\u914d\u5bf9\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u9a8c\u8bc1\u4e86AI\u5728\u6728\u6750\u79d1\u5b66\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u6728\u6750\u4e2d\u7684\u7ed3\u5bf9\u5176\u7f8e\u89c2\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4f20\u7edf\u7684\u624b\u52a8\u6807\u6ce8\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u5206\u4e24\u4e2a\u9636\u6bb5\uff1a\u68c0\u6d4b\u9636\u6bb5\u4f7f\u7528YOLOv8l\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u7ed3\u68c0\u6d4b\uff0c\u914d\u5bf9\u9636\u6bb5\u901a\u8fc7\u591a\u7ef4\u7279\u5f81\u63d0\u53d6\u548c\u4e09\u5143\u7ec4\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u7ed3\u7684\u914d\u5bf9\u3002", "result": "\u68c0\u6d4b\u9636\u6bb5\u7684mAP@0.5\u8fbe\u52300.887\uff0c\u914d\u5bf9\u9636\u6bb5\u51c6\u786e\u7387\u4e3a0.85\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u5c55\u793a\u4e86AI\u5728\u6728\u6750\u79d1\u5b66\u548c\u5de5\u4e1a\u4e2d\u7684\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ed3\u68c0\u6d4b\u548c\u914d\u5bf9\u7684\u6548\u7387\u3002"}}
{"id": "2505.05851", "pdf": "https://arxiv.org/pdf/2505.05851", "abs": "https://arxiv.org/abs/2505.05851", "authors": ["Janik Kaden", "Maximilian Hilger", "Tim Schreiter", "Marius Schaab", "Thomas Graichen", "Andrey Rudenko", "Ulrich Heinkel", "Achim J. Lilienthal"], "title": "Collecting Human Motion Data in Large and Occlusion-Prone Environments using Ultra-Wideband Localization", "categories": ["cs.RO", "cs.HC", "cs.LG"], "comment": "accepted for presentation at the 7th Workshop on Long-term Human\n  Motion Prediction (LHMP) at International Conference on Robotics and\n  Automation (ICRA) 2025", "summary": "With robots increasingly integrating into human environments, understanding\nand predicting human motion is essential for safe and efficient interactions.\nModern human motion and activity prediction approaches require high quality and\nquantity of data for training and evaluation, usually collected from motion\ncapture systems, onboard or stationary sensors. Setting up these systems is\nchallenging due to the intricate setup of hardware components, extensive\ncalibration procedures, occlusions, and substantial costs. These constraints\nmake deploying such systems in new and large environments difficult and limit\ntheir usability for in-the-wild measurements. In this paper we investigate the\npossibility to apply the novel Ultra-Wideband (UWB) localization technology as\na scalable alternative for human motion capture in crowded and occlusion-prone\nenvironments. We include additional sensing modalities such as eye-tracking,\nonboard robot LiDAR and radar sensors, and record motion capture data as ground\ntruth for evaluation and comparison. The environment imitates a museum setup,\nwith up to four active participants navigating toward random goals in a natural\nway, and offers more than 130 minutes of multi-modal data. Our investigation\nprovides a step toward scalable and accurate motion data collection beyond\nvision-based systems, laying a foundation for evaluating sensing modalities\nlike UWB in larger and complex environments like warehouses, airports, or\nconvention centers.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u8d85\u5bbd\u5e26\uff08UWB\uff09\u5b9a\u4f4d\u6280\u672f\u4f5c\u4e3a\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u62e5\u6324\u548c\u6613\u906e\u6321\u73af\u5883\u4e2d\u6355\u6349\u4eba\u4f53\u8fd0\u52a8\uff0c\u5e76\u7ed3\u5408\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u4eba\u7c7b\u73af\u5883\uff0c\u7406\u89e3\u548c\u9884\u6d4b\u4eba\u4f53\u8fd0\u52a8\u5bf9\u4e8e\u5b89\u5168\u9ad8\u6548\u7684\u4e92\u52a8\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u786c\u4ef6\u8bbe\u7f6e\u590d\u6742\u3001\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u90e8\u7f72\u5728\u65b0\u73af\u5883\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528UWB\u5b9a\u4f4d\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\u3001\u673a\u5668\u4ebaLiDAR\u548c\u96f7\u8fbe\u4f20\u611f\u5668\u7b49\u591a\u79cd\u611f\u77e5\u6a21\u6001\uff0c\u540c\u65f6\u5728\u6a21\u62df\u535a\u7269\u9986\u73af\u5883\u4e2d\u8bb0\u5f55\u8fd0\u52a8\u6355\u6349\u6570\u636e\u4f5c\u4e3a\u5730\u9762\u771f\u503c\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u8d85\u8fc7130\u5206\u949f\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5c55\u793a\u4e86UWB\u6280\u672f\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5728\u66f4\u5927\u89c4\u6a21\u573a\u666f\uff08\u5982\u4ed3\u5e93\u3001\u673a\u573a\u7b49\uff09\u4e2d\u8bc4\u4f30\u6b64\u7c7b\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "UWB\u6280\u672f\u53ef\u4f5c\u4e3a\u89c6\u89c9\u7cfb\u7edf\u4e4b\u5916\u7684\u53e6\u4e00\u79cd\u9009\u62e9\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u91c7\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.05872", "pdf": "https://arxiv.org/pdf/2505.05872", "abs": "https://arxiv.org/abs/2505.05872", "authors": ["Aqsa Shabbir", "Halil \u0130brahim Kanpak", "Alptekin K\u00fcp\u00e7\u00fc", "Sinem Sav"], "title": "A Taxonomy of Attacks and Defenses in Split Learning", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Split Learning (SL) has emerged as a promising paradigm for distributed deep\nlearning, allowing resource-constrained clients to offload portions of their\nmodel computation to servers while maintaining collaborative learning. However,\nrecent research has demonstrated that SL remains vulnerable to a range of\nprivacy and security threats, including information leakage, model inversion,\nand adversarial attacks. While various defense mechanisms have been proposed, a\nsystematic understanding of the attack landscape and corresponding\ncountermeasures is still lacking. In this study, we present a comprehensive\ntaxonomy of attacks and defenses in SL, categorizing them along three key\ndimensions: employed strategies, constraints, and effectiveness. Furthermore,\nwe identify key open challenges and research gaps in SL based on our\nsystematization, highlighting potential future directions.", "AI": {"tldr": "Split Learning (SL) facilitates distributed deep learning but faces privacy and security risks like information leakage and adversarial attacks. This study categorizes attacks and defenses in SL, identifying gaps and future directions.", "motivation": "To address the lack of systematic understanding of privacy and security threats in Split Learning (SL) and propose a structured approach to categorize attacks and defenses.", "method": "A comprehensive taxonomy is developed, classifying attacks and defenses in SL based on strategies, constraints, and effectiveness.", "result": "The study identifies key vulnerabilities in SL and organizes existing defense mechanisms, while highlighting unresolved challenges.", "conclusion": "The research underscores the need for further investigation into SL's security and privacy issues, offering a foundation for future work in this evolving field."}}
{"id": "2505.05892", "pdf": "https://arxiv.org/pdf/2505.05892", "abs": "https://arxiv.org/abs/2505.05892", "authors": ["Alexander Lappe", "Martin A. Giese"], "title": "Register and CLS tokens yield a decoupling of local and global features in large ViTs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent work has shown that the attention maps of the widely popular DINOv2\nmodel exhibit artifacts, which hurt both model interpretability and performance\non dense image tasks. These artifacts emerge due to the model repurposing patch\ntokens with redundant local information for the storage of global image\ninformation. To address this problem, additional register tokens have been\nincorporated in which the model can store such information instead. We\ncarefully examine the influence of these register tokens on the relationship\nbetween global and local image features, showing that while register tokens\nyield cleaner attention maps, these maps do not accurately reflect the\nintegration of local image information in large models. Instead, global\ninformation is dominated by information extracted from register tokens, leading\nto a disconnect between local and global features. Inspired by these findings,\nwe show that the CLS token itself, which can be interpreted as a register,\nleads to a very similar phenomenon in models without explicit register tokens.\nOur work shows that care must be taken when interpreting attention maps of\nlarge ViTs. Further, by clearly attributing the faulty behaviour to register\nand CLS tokens, we show a path towards more interpretable vision models.", "AI": {"tldr": "DINOv2\u6a21\u578b\u7684\u6ce8\u610f\u529b\u56fe\u5b58\u5728\u7455\u75b5\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002\u901a\u8fc7\u5f15\u5165register token\u6539\u5584\u4e86\u6ce8\u610f\u529b\u56fe\u8d28\u91cf\uff0c\u4f46\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u7684\u6574\u5408\u4ecd\u4e0d\u51c6\u786e\u3002CLS token\u4f1a\u5f15\u53d1\u7c7b\u4f3c\u95ee\u9898\uff0c\u7814\u7a76\u6307\u51fa\u4e86\u63d0\u5347\u89c6\u89c9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u5411\u3002", "motivation": "DINOv2\u6a21\u578b\u7684\u6ce8\u610f\u529b\u56fe\u56e0\u5b58\u50a8\u5168\u5c40\u4fe1\u606f\u7684\u5197\u4f59\u5c40\u90e8patch token\u800c\u51fa\u73b0\u7455\u75b5\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5bc6\u96c6\u56fe\u50cf\u4efb\u52a1\u7684\u6027\u80fd\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u989d\u5916\u7684register token\u5b58\u50a8\u5168\u5c40\u4fe1\u606f\uff0c\u5206\u6790\u5176\u5bf9\u5168\u5c40\u4e0e\u5c40\u90e8\u7279\u5f81\u5173\u7cfb\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83CLS token\u7684\u7c7b\u4f3c\u884c\u4e3a\u3002", "result": "register token\u6539\u5584\u4e86\u6ce8\u610f\u529b\u56fe\u7684\u6e05\u6670\u5ea6\uff0c\u4f46\u672a\u80fd\u51c6\u786e\u53cd\u6620\u5c40\u90e8\u4fe1\u606f\u7684\u6574\u5408\uff1bCLS token\u5728\u65e0register token\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u7c7b\u4f3c\u3002", "conclusion": "\u5927\u578bViT\u6ce8\u610f\u529b\u56fe\u7684\u89e3\u91ca\u9700\u8c28\u614e\uff1b\u901a\u8fc7\u660e\u786eregister\u548cCLS token\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2505.05922", "pdf": "https://arxiv.org/pdf/2505.05922", "abs": "https://arxiv.org/abs/2505.05922", "authors": ["Haoqi Wu", "Wei Dai", "Li Wang", "Qiang Yan"], "title": "CAPE: Context-Aware Prompt Perturbation Mechanism with Differential Privacy", "categories": ["cs.CR", "cs.LG"], "comment": "to be published in ICML 2025", "summary": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCape\uff0c\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u6270\u52a8\u673a\u5236\uff0c\u65e8\u5728\u63d0\u5347LLMs\u63a8\u7406\u6548\u7387\u53ca\u9690\u79c1-\u6548\u7528\u5e73\u8861\u3002", "motivation": "\u7531\u4e8eLLMs\u5728\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7528\u6237\u654f\u611f\u6570\u636e\u6cc4\u9732\u98ce\u9669\u5f15\u8d77\u5173\u6ce8\uff0c\u73b0\u6709\u65b9\u6848\u5728\u6548\u7387\u3001\u9690\u79c1\u548c\u6548\u7528\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u63d0\u51faCape\uff1a1. \u6df7\u5408\u6548\u7528\u51fd\u6570\u66f4\u597d\u6355\u83b7 token \u76f8\u4f3c\u6027\uff1b2. \u6876\u5316\u91c7\u6837\u673a\u5236\u5904\u7406\u5927\u91c7\u6837\u7a7a\u95f4\u7684\u957f\u5c3e\u73b0\u8c61\u3002", "result": "\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cCape\u5728\u9690\u79c1-\u6548\u7528\u5e73\u8861\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "Cape\u901a\u8fc7\u5dee\u5206\u9690\u79c1\u548c\u4f18\u5316\u91c7\u6837\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2505.05940", "pdf": "https://arxiv.org/pdf/2505.05940", "abs": "https://arxiv.org/abs/2505.05940", "authors": ["Rodrigo Diaz", "Mark Sandler"], "title": "Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates", "categories": ["cs.SD", "cs.LG", "eess.AS", "physics.comp-ph"], "comment": "accepted to DAFx 2025", "summary": "Modal methods for simulating vibrations of strings, membranes, and plates are\nwidely used in acoustics and physically informed audio synthesis. However,\ntraditional implementations, particularly for non-linear models like the von\nK\\'arm\\'an plate, are computationally demanding and lack differentiability,\nlimiting inverse modelling and real-time applications. We introduce a fast,\ndifferentiable, GPU-accelerated modal framework built with the JAX library,\nproviding efficient simulations and enabling gradient-based inverse modelling.\nBenchmarks show that our approach significantly outperforms CPU and GPU-based\nimplementations, particularly for simulations with many modes. Inverse\nmodelling experiments demonstrate that our approach can recover physical\nparameters, including tension, stiffness, and geometry, from both synthetic and\nexperimental data. Although fitting physical parameters is more sensitive to\ninitialisation compared to other methods, it provides greater interpretability\nand more compact parameterisation. The code is released as open source to\nsupport future research and applications in differentiable physical modelling\nand sound synthesis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eJAX\u5e93\u7684\u5feb\u901f\u3001\u53ef\u5fae\u5206\u3001GPU\u52a0\u901f\u7684\u6a21\u6001\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ebf\u6027\u632f\u52a8\u6a21\u578b\uff08\u5982von K\u00e1rm\u00e1n\u677f\uff09\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u9006\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u975e\u7ebf\u6027\u632f\u52a8\u6a21\u578b\uff08\u5982von K\u00e1rm\u00e1n\u677f\uff09\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u53ef\u5fae\u5206\u6027\uff0c\u9650\u5236\u4e86\u9006\u5efa\u6a21\u548c\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u5229\u7528JAX\u5e93\u6784\u5efa\u4e86GPU\u52a0\u901f\u7684\u6a21\u6001\u6846\u67b6\uff0c\u652f\u6301\u9ad8\u6548\u6a21\u62df\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u9006\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eCPU\u548cGPU\u4f20\u7edf\u5b9e\u73b0\uff0c\u5e76\u6210\u529f\u4ece\u5408\u6210\u548c\u5b9e\u9a8c\u6570\u636e\u4e2d\u6062\u590d\u7269\u7406\u53c2\u6570\uff08\u5982\u5f20\u529b\u3001\u521a\u5ea6\u548c\u51e0\u4f55\u5f62\u72b6\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u867d\u7136\u5bf9\u53c2\u6570\u521d\u59cb\u5316\u66f4\u654f\u611f\uff0c\u4f46\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u548c\u66f4\u7d27\u51d1\u7684\u53c2\u6570\u5316\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.05956", "pdf": "https://arxiv.org/pdf/2505.05956", "abs": "https://arxiv.org/abs/2505.05956", "authors": ["Xiyu Wang", "Gilberto Berardinelli", "Hei Victor Cheng", "Petar Popovski", "Ramoni Adeogun"], "title": "Multi-User Beamforming with Deep Reinforcement Learning in Sensing-Aided Communication", "categories": ["eess.SP", "cs.LG", "cs.NI"], "comment": "Accepted for Presentation at IEEE EuCNC & 6G Summit 2025", "summary": "Mobile users are prone to experience beam failure due to beam drifting in\nmillimeter wave (mmWave) communications. Sensing can help alleviate beam\ndrifting with timely beam changes and low overhead since it does not need user\nfeedback. This work studies the problem of optimizing sensing-aided\ncommunication by dynamically managing beams allocated to mobile users. A\nmulti-beam scheme is introduced, which allocates multiple beams to the users\nthat need an update on the angle of departure (AoD) estimates and a single beam\nto the users that have satisfied AoD estimation precision. A deep reinforcement\nlearning (DRL) assisted method is developed to optimize the beam allocation\npolicy, relying only upon the sensing echoes. For comparison, a heuristic\nAoD-based method using approximated Cram\\'er-Rao lower bound (CRLB) for\nallocation is also presented. Both methods require neither user feedback nor\nprior state evolution information. Results show that the DRL-assisted method\nachieves a considerable gain in throughput than the conventional beam sweeping\nmethod and the AoD-based method, and it is robust to different user speeds.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u4f20\u611f\u8f85\u52a9\u901a\u4fe1\u4f18\u5316\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\u7684\u6ce2\u675f\u7ba1\u7406\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u6ce2\u675f\u65b9\u6848\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u4e2d\uff0c\u79fb\u52a8\u7528\u6237\u5bb9\u6613\u56e0\u6ce2\u675f\u6f02\u79fb\u800c\u5931\u8d25\u3002\u4f20\u611f\u6280\u672f\u53ef\u4ee5\u65e0\u9700\u7528\u6237\u53cd\u9988\u4e14\u4f4e\u5f00\u9500\u5730\u53ca\u65f6\u8c03\u6574\u6ce2\u675f\uff0c\u4ece\u800c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6ce2\u675f\u65b9\u6848\uff0c\u6839\u636e\u7528\u6237\u9700\u6c42\u52a8\u6001\u5206\u914d\u6ce2\u675f\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6ce2\u675f\u5206\u914d\u4f18\u5316\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u4f20\u611f\u56de\u6ce2\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAoD\u548cCRLB\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f5c\u4e3a\u5bf9\u6bd4\u3002", "result": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u541e\u5410\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6ce2\u675f\u626b\u63cf\u548cAoD\u65b9\u6cd5\uff0c\u4e14\u5bf9\u4e0d\u540c\u7528\u6237\u901f\u5ea6\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u6ce2\u675f\u7ba1\u7406\uff0c\u63d0\u5347\u6beb\u7c73\u6ce2\u901a\u4fe1\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u7528\u6237\u53cd\u9988\u6216\u5148\u9a8c\u72b6\u6001\u4fe1\u606f\u3002"}}
{"id": "2505.05957", "pdf": "https://arxiv.org/pdf/2505.05957", "abs": "https://arxiv.org/abs/2505.05957", "authors": ["Peter R\u00f6seler", "Oliver Schaudt", "Helmut Berg", "Christian Bauckhage", "Matthias Koch"], "title": "Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints", "categories": ["quant-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "While classical convolutional neural networks (CNNs) have revolutionized\nimage classification, the emergence of quantum computing presents new\nopportunities for enhancing neural network architectures. Quantum CNNs (QCNNs)\nleverage quantum mechanical properties and hold potential to outperform\nclassical approaches. However, their implementation on current noisy\nintermediate-scale quantum (NISQ) devices remains challenging due to hardware\nlimitations. In our research, we address this challenge by introducing an\nencoding scheme that significantly reduces the input dimensionality. We\ndemonstrate that a primitive QCNN architecture with 49 qubits is sufficient to\ndirectly process $28\\times 28$ pixel MNIST images, eliminating the need for\nclassical dimensionality reduction pre-processing. Additionally, we propose an\nautomated framework based on expressibility, entanglement, and complexity\ncharacteristics to identify the building blocks of QCNNs, parameterized quantum\ncircuits (PQCs). Our approach demonstrates advantages in accuracy and\nconvergence speed with a similar parameter count compared to both hybrid QCNNs\nand classical CNNs. We validated our experiments on IBM's Heron r2 quantum\nprocessor, achieving $96.08\\%$ classification accuracy, surpassing the\n$71.74\\%$ benchmark of traditional approaches under identical training\nconditions. These results represent one of the first implementations of image\nclassifications on real quantum hardware and validate the potential of quantum\ncomputing in this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u7684\u7f16\u7801\u65b9\u6848\uff0c\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\uff0c\u5e76\u572849\u91cf\u5b50\u4f4d\u7684\u67b6\u6784\u4e0a\u76f4\u63a5\u5904\u7406MNIST\u56fe\u50cf\uff0c\u65e0\u9700\u4f20\u7edf\u964d\u7ef4\u9884\u5904\u7406\u3002\u5b9e\u9a8c\u5728IBM\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5b9e\u73b096.08%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4f20\u7edf\u65b9\u6cd5\u768471.74%\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4e3a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u73b0\u6709NISQ\u8bbe\u5907\u7684\u786c\u4ef6\u9650\u5236\u4f7f\u5f97\u91cf\u5b50CNN\u7684\u5b9e\u73b0\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u8f93\u5165\u7ef4\u5ea6\u7684\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8868\u8fbe\u6027\u3001\u7ea0\u7f20\u548c\u590d\u6742\u6027\u7279\u5f81\u7684\u81ea\u52a8\u5316\u6846\u67b6\u6765\u9009\u62e9QCNN\u7684\u57fa\u672c\u7ec4\u4ef6PQCs\u3002", "result": "\u5728IBM Heron r2\u91cf\u5b50\u5904\u7406\u5668\u4e0a\uff0c\u5b9e\u73b0\u4e8696.08%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u65b9\u6cd5\u768471.74%\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u662f\u9996\u6279\u5728\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u7684\u9ad8\u6027\u80fd\u56fe\u50cf\u5206\u7c7b\u65b9\u6848\u4e4b\u4e00\u3002"}}
{"id": "2505.05989", "pdf": "https://arxiv.org/pdf/2505.05989", "abs": "https://arxiv.org/abs/2505.05989", "authors": ["Hongye Zheng", "Yue Xing", "Lipeng Zhu", "Xu Han", "Junliang Du", "Wanyu Cui"], "title": "Modeling Multi-Hop Semantic Paths for Recommendation in Heterogeneous Information Networks", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "This study focuses on the problem of path modeling in heterogeneous\ninformation networks and proposes a multi-hop path-aware recommendation\nframework. The method centers on multi-hop paths composed of various types of\nentities and relations. It models user preferences through three stages: path\nselection, semantic representation, and attention-based fusion. In the path\nselection stage, a path filtering mechanism is introduced to remove redundant\nand noisy information. In the representation learning stage, a sequential\nmodeling structure is used to jointly encode entities and relations, preserving\nthe semantic dependencies within paths. In the fusion stage, an attention\nmechanism assigns different weights to each path to generate a global user\ninterest representation. Experiments conducted on real-world datasets such as\nAmazon-Book show that the proposed method significantly outperforms existing\nrecommendation models across multiple evaluation metrics, including HR@10,\nRecall@10, and Precision@10. The results confirm the effectiveness of multi-hop\npaths in capturing high-order interaction semantics and demonstrate the\nexpressive modeling capabilities of the framework in heterogeneous\nrecommendation scenarios. This method provides both theoretical and practical\nvalue by integrating structural information modeling in heterogeneous networks\nwith recommendation algorithm design. It offers a more expressive and flexible\nparadigm for learning user preferences in complex data environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8df3\u8def\u5f84\u611f\u77e5\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u9009\u62e9\u3001\u8bed\u4e49\u8868\u793a\u548c\u6ce8\u610f\u529b\u878d\u5408\u4e09\u9636\u6bb5\u5efa\u6a21\u7528\u6237\u504f\u597d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u5f02\u6784\u4fe1\u606f\u7f51\u7edc\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5f02\u6784\u4fe1\u606f\u7f51\u7edc\u4e2d\u7684\u8def\u5f84\u5efa\u6a21\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u8df3\u8def\u5f84\u6355\u6349\u9ad8\u9636\u4ea4\u4e92\u8bed\u4e49\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u8def\u5f84\u9009\u62e9\uff08\u8fc7\u6ee4\u5197\u4f59\u4fe1\u606f\uff09\u3001\u8bed\u4e49\u8868\u793a\uff08\u5e8f\u5217\u5efa\u6a21\u7f16\u7801\u5b9e\u4f53\u548c\u5173\u7cfb\uff09\u548c\u6ce8\u610f\u529b\u878d\u5408\uff08\u52a0\u6743\u751f\u6210\u7528\u6237\u5174\u8da3\u8868\u793a\uff09\u3002", "result": "\u5728Amazon-Book\u7b49\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728HR@10\u3001Recall@10\u548cPrecision@10\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5f02\u6784\u7f51\u7edc\u7ed3\u6784\u4fe1\u606f\u548c\u63a8\u8350\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u4e3a\u590d\u6742\u6570\u636e\u73af\u5883\u4e2d\u7684\u7528\u6237\u504f\u597d\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u6709\u6548\u7684\u8303\u5f0f\u3002"}}
{"id": "2505.06003", "pdf": "https://arxiv.org/pdf/2505.06003", "abs": "https://arxiv.org/abs/2505.06003", "authors": ["Moritz Vandenhirtz", "Julia E. Vogt"], "title": "From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection", "categories": ["cs.CV", "cs.LG"], "comment": "International Conference on Machine Learning", "summary": "Understanding the decision-making process of machine learning models provides\nvaluable insights into the task, the data, and the reasons behind a model's\nfailures. In this work, we propose a method that performs inherently\ninterpretable predictions through the instance-wise sparsification of input\nimages. To align the sparsification with human perception, we learn the masking\nin the space of semantically meaningful pixel regions rather than on\npixel-level. Additionally, we introduce an explicit way to dynamically\ndetermine the required level of sparsity for each instance. We show empirically\non semi-synthetic and natural image datasets that our inherently interpretable\nclassifier produces more meaningful, human-understandable predictions than\nstate-of-the-art benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u522b\u7a00\u758f\u5316\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u5b66\u4e60\u8bed\u4e49\u533a\u57df\u7684\u63a9\u7801\u5e76\u52a8\u6001\u786e\u5b9a\u7a00\u758f\u5ea6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6613\u7406\u89e3\u3002", "motivation": "\u7406\u89e3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u5bf9\u4efb\u52a1\u3001\u6570\u636e\u53ca\u6a21\u578b\u5931\u8d25\u539f\u56e0\u7684\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u65e8\u5728\u63d0\u5347\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u5b9e\u4f8b\u7ea7\u522b\u7684\u8f93\u5165\u56fe\u50cf\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u5728\u8bed\u4e49\u50cf\u7d20\u533a\u57df\u5b66\u4e60\u63a9\u7801\uff0c\u5e76\u52a8\u6001\u786e\u5b9a\u6bcf\u4e2a\u5b9e\u4f8b\u6240\u9700\u7684\u7a00\u758f\u5ea6\u3002", "result": "\u5728\u534a\u5408\u6210\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u9884\u6d4b\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u6613\u4e8e\u4eba\u7c7b\u7406\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u7684\u9884\u6d4b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.06042", "pdf": "https://arxiv.org/pdf/2505.06042", "abs": "https://arxiv.org/abs/2505.06042", "authors": ["Christos Plachouras", "Emmanouil Benetos", "Johan Pauwels"], "title": "Learning Music Audio Representations With Limited Data", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "Presented at ICASSP 2025", "summary": "Large deep-learning models for music, including those focused on learning\ngeneral-purpose music audio representations, are often assumed to require\nsubstantial training data to achieve high performance. If true, this would pose\nchallenges in scenarios where audio data or annotations are scarce, such as for\nunderrepresented music traditions, non-popular genres, and personalized music\ncreation and listening. Understanding how these models behave in limited-data\nscenarios could be crucial for developing techniques to tackle them.\n  In this work, we investigate the behavior of several music audio\nrepresentation models under limited-data learning regimes. We consider music\nmodels with various architectures, training paradigms, and input durations, and\ntrain them on data collections ranging from 5 to 8,000 minutes long. We\nevaluate the learned representations on various music information retrieval\ntasks and analyze their robustness to noise. We show that, under certain\nconditions, representations from limited-data and even random models perform\ncomparably to ones from large-dataset models, though handcrafted features\noutperform all learned representations in some tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\uff0c\u4e0d\u540c\u97f3\u4e50\u97f3\u9891\u8868\u793a\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5c0f\u6570\u636e\u96c6\u6a21\u578b\u751a\u81f3\u968f\u673a\u6a21\u578b\u7684\u8868\u73b0\u53ef\u4e0e\u5927\u6570\u636e\u96c6\u6a21\u578b\u5ab2\u7f8e\uff0c\u4f46\u624b\u5de5\u7279\u5f81\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u4ecd\u4f18\u4e8e\u5b66\u4e60\u5230\u7684\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u5728\u97f3\u9891\u6570\u636e\u6216\u6807\u6ce8\u7a00\u7f3a\u7684\u60c5\u5883\u4e0b\uff08\u5982\u5c0f\u4f17\u97f3\u4e50\u4f20\u7edf\u3001\u975e\u6d41\u884c\u6d41\u6d3e\u6216\u4e2a\u6027\u5316\u97f3\u4e50\u521b\u4f5c\u4e0e\u8046\u542c\uff09\uff0c\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u6a21\u578b\u7684\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u591a\u79cd\u67b6\u6784\u3001\u8bad\u7ec3\u8303\u5f0f\u53ca\u8f93\u5165\u65f6\u957f\u7684\u97f3\u4e50\u97f3\u9891\u8868\u793a\u6a21\u578b\uff08\u6570\u636e\u91cf\u4ece5\u52308,000\u5206\u949f\u4e0d\u7b49\uff09\uff0c\u5728\u591a\u79cd\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u8bc4\u4f30\u5176\u8868\u73b0\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u6709\u9650\u6570\u636e\u6216\u968f\u673a\u6a21\u578b\u7684\u8868\u793a\u6027\u80fd\u4e0e\u5927\u6570\u636e\u96c6\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u624b\u5de5\u7279\u5f81\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u53ef\u80fd\u88ab\u4f4e\u4f30\uff0c\u672a\u6765\u7814\u7a76\u53ef\u63a2\u7d22\u5982\u4f55\u8fdb\u4e00\u6b65\u63d0\u5347\u5c0f\u6570\u636e\u96c6\u7684\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u624b\u5de5\u7279\u5f81\u5728\u67d0\u4e9b\u573a\u666f\u4e2d\u4ecd\u5177\u7ade\u4e89\u529b\u3002"}}
{"id": "2505.06146", "pdf": "https://arxiv.org/pdf/2505.06146", "abs": "https://arxiv.org/abs/2505.06146", "authors": ["Idan Attias", "Xing Gao", "Lev Reyzin"], "title": "Learning-Augmented Algorithms for Boolean Satisfiability", "categories": ["cs.DS", "cs.CC", "cs.LG"], "comment": null, "summary": "Learning-augmented algorithms are a prominent recent development in beyond\nworst-case analysis. In this framework, a problem instance is provided with a\nprediction (``advice'') from a machine-learning oracle, which provides partial\ninformation about an optimal solution, and the goal is to design algorithms\nthat leverage this advice to improve worst-case performance. We study the\nclassic Boolean satisfiability (SAT) decision and optimization problems within\nthis framework using two forms of advice. ``Subset advice\" provides a random\n$\\epsilon$ fraction of the variables from an optimal assignment, whereas\n``label advice\" provides noisy predictions for all variables in an optimal\nassignment.\n  For the decision problem $k$-SAT, by using the subset advice we accelerate\nthe exponential running time of the PPSZ family of algorithms due to Paturi,\nPudlak, Saks and Zane, which currently represent the state of the art in the\nworst case. We accelerate the running time by a multiplicative factor of\n$2^{-c}$ in the base of the exponent, where $c$ is a function of $\\epsilon$ and\n$k$. For the optimization problem, we show how to incorporate subset advice in\na black-box fashion with any $\\alpha$-approximation algorithm, improving the\napproximation ratio to $\\alpha + (1 - \\alpha)\\epsilon$. Specifically, we\nachieve approximations of $0.94 + \\Omega(\\epsilon)$ for MAX-$2$-SAT, $7/8 +\n\\Omega(\\epsilon)$ for MAX-$3$-SAT, and $0.79 + \\Omega(\\epsilon)$ for MAX-SAT.\nMoreover, for label advice, we obtain near-optimal approximation for instances\nwith large average degree, thereby generalizing recent results on MAX-CUT and\nMAX-$2$-LIN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u4fe1\u606f\u8f85\u52a9\u4e0b\uff0c\u5982\u4f55\u6539\u8fdb\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff08SAT\uff09\u7684\u51b3\u7b56\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e24\u79cd\u7c7b\u578b\u7684\u201c\u5efa\u8bae\u201d\uff08\u5b50\u96c6\u5efa\u8bae\u548c\u6807\u7b7e\u5efa\u8bae\uff09\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u7684\u4fe1\u606f\uff08\u201c\u5efa\u8bae\u201d\uff09\u6765\u6539\u8fdb\u7ecf\u5178\u95ee\u9898\u7684\u7b97\u6cd5\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5e03\u5c14\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u4e2d\uff0c\u901a\u8fc7\u52a0\u901f\u51b3\u7b56\u8fc7\u7a0b\u6216\u63d0\u9ad8\u8fd1\u4f3c\u6bd4\u6765\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u79cd\u5f62\u5f0f\u7684\u5efa\u8bae\uff1a\u5b50\u96c6\u5efa\u8bae\uff08\u968f\u673a\u63d0\u4f9b\u6700\u4f18\u89e3\u7684\u90e8\u5206\u53d8\u91cf\uff09\u548c\u6807\u7b7e\u5efa\u8bae\uff08\u63d0\u4f9b\u6240\u6709\u53d8\u91cf\u7684\u566a\u58f0\u9884\u6d4b\uff09\uff0c\u5206\u522b\u7528\u4e8e\u52a0\u901fPPSZ\u7b97\u6cd5\u7684\u51b3\u7b56\u8fc7\u7a0b\u548c\u6539\u8fdb\u4f18\u5316\u95ee\u9898\u7684\u8fd1\u4f3c\u6bd4\u3002", "result": "\u5728\u51b3\u7b56\u95ee\u9898\u4e2d\uff0c\u5b50\u96c6\u5efa\u8bae\u5c06PPSZ\u7b97\u6cd5\u7684\u8fd0\u884c\u65f6\u95f4\u6307\u6570\u7ea7\u52a0\u901f\uff1b\u5728\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u5b50\u96c6\u5efa\u8bae\u5c06\u8fd1\u4f3c\u6bd4\u6539\u8fdb\u4e3a$\\alpha + (1 - \\alpha)\\epsilon$\uff0c\u6807\u7b7e\u5efa\u8bae\u5219\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u8fd1\u4f3c\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5efa\u8bae\u53ef\u4ee5\u6709\u6548\u63d0\u5347SAT\u95ee\u9898\u7684\u7b97\u6cd5\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u51b3\u7b56\u901f\u5ea6\u548c\u4f18\u5316\u8d28\u91cf\u65b9\u9762\u5c55\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2505.06176", "pdf": "https://arxiv.org/pdf/2505.06176", "abs": "https://arxiv.org/abs/2505.06176", "authors": ["Niladri Shekhar Dutt", "Duygu Ceylan", "Niloy J. Mitra"], "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted at SIGGRAPH 2025 [ACM Transactions on Graphics]; Project\n  website: https://monetgpt.github.io", "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u7167\u7247\u4fee\u590d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7406\u89e3\u56fe\u50cf\u5904\u7406\u64cd\u4f5c\uff0c\u5e76\u57fa\u4e8e\u4e13\u5bb6\u7f16\u8f91\u7684\u7167\u7247\u751f\u6210\u63a8\u7406\u6570\u636e\u96c6\uff0c\u6700\u7ec8\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u80fd\u4fdd\u7559\u56fe\u50cf\u7ec6\u8282\u7684\u4fee\u590d\u5efa\u8bae\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u5f0f\u7f16\u8f91\uff08\u5982\u57fa\u4e8e\u6587\u672c\u6216\u7b14\u89e6\u7684\u7f16\u8f91\uff09\u53ef\u80fd\u4e0d\u53ef\u9884\u6d4b\u5730\u6539\u53d8\u539f\u59cb\u5bf9\u8c61\u8eab\u4efd\uff0c\u800c\u4e13\u4e1a\u6444\u5f71\u5e08\u66f4\u503e\u5411\u4e8e\u4fdd\u5b88\u7684\u4f20\u7edf\u7a0b\u5e8f\u5316\u7f16\u8f91\u3002\u4f46\u7531\u4e8e\u7a0b\u5e8f\u5316\u7f16\u8f91\u64cd\u4f5c\u590d\u6742\uff0c\u5bf9\u65b0\u624b\u6765\u8bf4\u96be\u4ee5\u89c4\u5212\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22MLLM\u80fd\u5426\u901a\u8fc7\u5b66\u4e60\u548c\u89c4\u5212\u8fd9\u4e9b\u64cd\u4f5c\uff0c\u63d0\u4f9b\u4e13\u4e1a\u8d28\u91cf\u7684\u4fee\u590d\u5efa\u8bae\u3002", "method": "\u9996\u5148\u8bad\u7ec3MLLM\u901a\u8fc7\u89e3\u51b3\u89c6\u89c9\u8c1c\u9898\u6765\u7406\u89e3\u56fe\u50cf\u5904\u7406\u64cd\u4f5c\u3002\u7136\u540e\uff0c\u5229\u7528\u4e13\u5bb6\u7f16\u8f91\u7684\u7167\u7247\u5408\u6210\u63a8\u7406\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u64cd\u7eb5\u4e13\u5bb6\u7f16\u8f91\u5e76\u57fa\u4e8e\u89c6\u89c9\u8c03\u6574\u9884\u8bad\u7ec3LLM\uff0c\u751f\u6210\u5fae\u8c03\u6240\u9700\u7684\u63a8\u7406\u6570\u636e\u3002\u6700\u7ec8\uff0c\u6a21\u578b\u53ef\u4ee5\u89c4\u5212\u5e76\u63d0\u51fa\u7f16\u8f91\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u751f\u6210\u5f0f\u548c\u7a0b\u5e8f\u5316\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u89e3\u91ca\u6027\u548c\u8eab\u4efd\u4fdd\u7559\u65b9\u9762\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7MLLM\u5b66\u4e60\u548c\u89c4\u5212\u7a0b\u5e8f\u5316\u7f16\u8f91\u64cd\u4f5c\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u4fdd\u7559\u7ec6\u8282\u7684\u9ad8\u8d28\u91cf\u7167\u7247\u4fee\u590d\uff0c\u4e3a\u65b0\u624b\u63d0\u4f9b\u63a5\u8fd1\u4e13\u4e1a\u7684\u7f16\u8f91\u5de5\u5177\u3002"}}
{"id": "2505.06182", "pdf": "https://arxiv.org/pdf/2505.06182", "abs": "https://arxiv.org/abs/2505.06182", "authors": ["Tim Schneider", "Cristiana de Farias", "Roberto Calandra", "Liming Chen", "Jan Peters"], "title": "Active Perception for Tactile Sensing: A Task-Agnostic Attention-Based Approach", "categories": ["cs.RO", "cs.LG"], "comment": "16 pages; 13 figures", "summary": "Humans make extensive use of haptic exploration to map and identify the\nproperties of the objects that we touch. In robotics, active tactile perception\nhas emerged as an important research domain that complements vision for tasks\nsuch as object classification, shape reconstruction, and manipulation. This\nwork introduces TAP (Task-agnostic Active Perception) -- a novel framework that\nleverages reinforcement learning (RL) and transformer-based architectures to\naddress the challenges posed by partially observable environments. TAP\nintegrates Soft Actor-Critic (SAC) and CrossQ algorithms within a unified\noptimization objective, jointly training a perception module and\ndecision-making policy. By design, TAP is completely task-agnostic and can, in\nprinciple, generalize to any active perception problem. We evaluate TAP across\ndiverse tasks, including toy examples and realistic applications involving\nhaptic exploration of 3D models from the Tactile MNIST benchmark. Experiments\ndemonstrate the efficacy of TAP, achieving high accuracies on the Tactile MNIST\nhaptic digit recognition task and a tactile pose estimation task. These\nfindings underscore the potential of TAP as a versatile and generalizable\nframework for advancing active tactile perception in robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAP\u7684\u4efb\u52a1\u65e0\u5173\u4e3b\u52a8\u611f\u77e5\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548cTransformer\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u89e6\u89c9\u6570\u5b57\u8bc6\u522b\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u89e6\u89c9\u63a2\u7d22\u6765\u8bc6\u522b\u7269\u4f53\u5c5e\u6027\uff0c\u800c\u673a\u5668\u4eba\u9700\u8981\u7c7b\u4f3c\u7684\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "TAP\u6846\u67b6\u6574\u5408\u4e86Soft Actor-Critic (SAC)\u548cCrossQ\u7b97\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u4f18\u5316\u76ee\u6807\u8054\u5408\u8bad\u7ec3\u611f\u77e5\u6a21\u5757\u548c\u51b3\u7b56\u7b56\u7565\uff0c\u4e14\u8bbe\u8ba1\u4e3a\u4efb\u52a1\u65e0\u5173\u3002", "result": "\u5728Tactile MNIST\u89e6\u89c9\u6570\u5b57\u8bc6\u522b\u548c\u89e6\u89c9\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "TAP\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u6709\u6f5c\u529b\u63a8\u52a8\u673a\u5668\u4eba\u4e3b\u52a8\u89e6\u89c9\u611f\u77e5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.06207", "pdf": "https://arxiv.org/pdf/2505.06207", "abs": "https://arxiv.org/abs/2505.06207", "authors": ["Muhy Eddin Za'ter", "Amir Sajad", "Bri-Mathias Hodge"], "title": "Leveraging Multi-Task Learning for Multi-Label Power System Security Assessment", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "This paper introduces a novel approach to the power system security\nassessment using Multi-Task Learning (MTL), and reformulating the problem as a\nmulti-label classification task. The proposed MTL framework simultaneously\nassesses static, voltage, transient, and small-signal stability, improving both\naccuracy and interpretability with respect to the most state of the art machine\nlearning methods. It consists of a shared encoder and multiple decoders,\nenabling knowledge transfer between stability tasks. Experiments on the IEEE\n68-bus system demonstrate a measurable superior performance of the proposed\nmethod compared to the extant state-of-the-art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u5c06\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u5668\u548c\u591a\u4e2a\u89e3\u7801\u5668\u63d0\u5347\u8bc4\u4f30\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u8bc4\u4f30\u901a\u5e38\u9700\u8981\u72ec\u7acb\u5206\u6790\u591a\u4e2a\u7a33\u5b9a\u6027\u95ee\u9898\uff08\u5982\u9759\u6001\u3001\u7535\u538b\u3001\u6682\u6001\u548c\u5c0f\u4fe1\u53f7\u7a33\u5b9a\u6027\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u5f15\u5165\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u6765\u540c\u65f6\u4f18\u5316\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u5171\u4eab\u7f16\u7801\u5668\u548c\u591a\u4e2a\u89e3\u7801\u5668\uff0c\u5206\u522b\u5bf9\u5e94\u4e0d\u540c\u7a33\u5b9a\u6027\u4efb\u52a1\uff0c\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u548c\u9ad8\u6548\u8bc4\u4f30\u3002\u5e76\u5728IEEE 68\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u8bc4\u4f30\u7684\u591a\u9879\u7a33\u5b9a\u6027\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u4e3a\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u77e5\u8bc6\u5171\u4eab\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.06228", "pdf": "https://arxiv.org/pdf/2505.06228", "abs": "https://arxiv.org/abs/2505.06228", "authors": ["Mariona Badenas-Agusti", "Siyi Xu", "Andrew Vanderburg", "Kishalay De", "Patrick Dufour", "Laura K. Rogers", "Susana Hoyos", "Simon Blouin", "Javier Via\u00f1a", "Amy Bonsor", "Ben Zuckerman"], "title": "A Machine-Learning Compositional Study of Exoplanetary Material Accreted Onto Five Helium-Atmosphere White Dwarfs with $\\texttt{cecilia}$", "categories": ["astro-ph.EP", "astro-ph.IM", "astro-ph.SR", "cs.LG"], "comment": "28 pages, 14 figures, 5 tables. Accepted for publication in MNRAS", "summary": "We present the first application of the Machine Learning (ML) pipeline\n$\\texttt{cecilia}$ to determine the physical parameters and photospheric\ncomposition of five metal-polluted He-atmosphere white dwarfs without\nwell-characterised elemental abundances. To achieve this, we perform a joint\nand iterative Bayesian fit to their $\\textit{SDSS}$ (R=2,000) and\n$\\textit{Keck/ESI}$ (R=4,500) optical spectra, covering the wavelength range\nfrom about 3,800\\r{A} to 9,000\\r{A}. Our analysis measures the abundances of at\nleast two $-$and up to six$-$ chemical elements in their atmospheres with a\npredictive accuracy similar to that of conventional WD analysis techniques\n($\\approx$0.20 dex). The white dwarfs with the largest number of detected heavy\nelements are SDSS J0859$+$5732 and SDSS J2311$-$0041, which simultaneously\nexhibit O, Mg, Si, Ca, and Fe in their $\\textit{Keck/ESI}$ spectra. For all\nsystems, we find that the bulk composition of their pollutants is largely\nconsistent with those of primitive CI chondrites to within 1-2$\\sigma$. We also\nfind evidence of statistically significant ($>2\\sigma$) oxygen excesses for\nSDSS J0859$+$5732 and SDSS J2311$-$0041, which could point to the accretion of\noxygen-rich exoplanetary material. In the future, as wide-field astronomical\nsurveys deliver millions of public WD spectra to the scientific community,\n$\\texttt{cecilia}$ aspires to unlock population-wide studies of polluted WDs,\ntherefore helping to improve our statistical knowledge of extrasolar\ncompositions.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u5de5\u5177cecilia\u5206\u6790\u4e86\u4e94\u4e2a\u91d1\u5c5e\u6c61\u67d3\u7684\u767d\u77ee\u661f\u5927\u6c14\u53c2\u6570\u548c\u5316\u5b66\u6210\u5206\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7cbe\u5ea6\u76f8\u5f53\uff0c\u53d1\u73b0\u5176\u6c61\u67d3\u7269\u6210\u5206\u4e0e\u539f\u59cbCI\u7403\u7c92\u9668\u77f3\u76f8\u4f3c\uff0c\u5e76\u53d1\u73b0\u67d0\u4e9b\u6052\u661f\u53ef\u80fd\u5b58\u5728\u5bcc\u6c27\u7269\u8d28\u7684\u5438\u79ef\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5bf9\u767d\u77ee\u661f\u5927\u6c14\u6210\u5206\u5206\u6790\u8017\u65f6\u4e14\u4f9d\u8d56\u4eba\u5de5\uff0c\u673a\u5668\u5b66\u4e60\u5de5\u5177cecilia\u6709\u671b\u9ad8\u6548\u5904\u7406\u5927\u91cf\u5149\u8c31\u6570\u636e\uff0c\u63a8\u52a8\u7edf\u8ba1\u7814\u7a76\u3002", "method": "\u7ed3\u5408SDSS\u548cKeck/ESI\u5149\u8c31\u6570\u636e\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u8054\u5408\u62df\u5408\u8fed\u4ee3\u5206\u6790\u6ce2\u957f\u8303\u56f43800-9000\u00c5\u7684\u5149\u8c31\uff0c\u6d4b\u91cf\u591a\u79cd\u5143\u7d20\u7684\u4e30\u5ea6\u3002", "result": "\u6d4b\u5f972-6\u79cd\u5143\u7d20\u4e30\u5ea6\uff08\u7cbe\u5ea6\u7ea60.20 dex\uff09\uff0c\u5176\u4e2dSDSS J0859+5732\u548cSDSS J2311-0041\u68c0\u6d4b\u5230O\u3001Mg\u3001Si\u3001Ca\u548cFe\uff0c\u4e14\u6c61\u67d3\u7269\u6210\u5206\u4e0eCI\u7403\u7c92\u9668\u77f3\u4e00\u81f4\uff081-2\u03c3\uff09\u3002\u90e8\u5206\u767d\u77ee\u661f\u5b58\u5728\u663e\u8457\u5bcc\u6c27\u8ff9\u8c61\uff08>2\u03c3\uff09\u3002", "conclusion": "cecilia\u53ef\u9ad8\u6548\u5206\u6790\u6c61\u67d3\u767d\u77ee\u661f\uff0c\u672a\u6765\u5927\u89c4\u6a21\u5149\u8c31\u6570\u636e\u5c06\u52a9\u529b\u7edf\u8ba1\u7814\u7a76\u5916\u592a\u9633\u7cfb\u7269\u8d28\u7ec4\u6210\u3002"}}
