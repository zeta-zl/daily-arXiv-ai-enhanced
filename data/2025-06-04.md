<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 107]
- [cs.LG](#cs.LG) [Total: 126]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.GT](#cs.GT) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IT](#cs.IT) [Total: 2]
- [stat.ML](#stat.ML) [Total: 14]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CR](#cs.CR) [Total: 9]
- [cs.HC](#cs.HC) [Total: 3]
- [math.AT](#math.AT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 11]
- [eess.AS](#eess.AS) [Total: 7]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 38]
- [math.NA](#math.NA) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)
*Jinzhu Yang*

Key words: 命名实体识别, 提示学习, 医疗领域, BioBERT, Prompt-bioMRC

TL;DR: 本文研究了提示学习方法在医学领域命名实体识别（NER）中的应用，提出了结合硬模板和软提示设计的Prompt-bioMRC模型，实验证明其性能优于传统方法，为智能诊断系统等应用提供了技术支撑。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 近年来，大规模语言模型（如BioBERT）显著提升了医学文本的NER性能，但仍有改进空间。本文旨在通过提示学习方法进一步提高医学实体识别的精度和效率。

Method: 提出了Prompt-bioMRC模型，结合了硬模板和软提示设计，优化医学领域的命名实体识别。

Result: 在多个医学数据集上的实验表明，该方法优于传统模型，验证了其有效性。

Conclusion: Prompt-bioMRC模型为医学领域的NER提供了可靠技术支持，有助于自动化医疗数据处理和智能诊断系统的发展。

Abstract: This study is dedicated to exploring the application of prompt learning
methods to advance Named Entity Recognition (NER) within the medical domain. In
recent years, the emergence of large-scale models has driven significant
progress in NER tasks, particularly with the introduction of the BioBERT
language model, which has greatly enhanced NER capabilities in medical texts.
Our research introduces the Prompt-bioMRC model, which integrates both hard
template and soft prompt designs aimed at refining the precision and efficiency
of medical entity recognition. Through extensive experimentation across diverse
medical datasets, our findings consistently demonstrate that our approach
surpasses traditional models. This enhancement not only validates the efficacy
of our methodology but also highlights its potential to provide reliable
technological support for applications like intelligent diagnosis systems. By
leveraging advanced NER techniques, this study contributes to advancing
automated medical data processing, facilitating more accurate medical
information extraction, and supporting efficient healthcare decision-making
processes.

</details>


### [2] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
*Lukas Rauch,Moritz Wirth,Denis Huseljic,Marek Herde,Bernhard Sick,Matthias Aßenmacher*

Key words: 大语言模型、深度主动学习、嵌入质量、查询策略、文本分类

TL;DR: 研究了利用高质量LLM嵌入对深度主动学习的影响，发现嵌入质量直接影响查询策略选择，且多样性采样与高质量嵌入结合在早期迭代中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨高质量LLM嵌入如何降低深度主动学习的计算成本并提升效果，填补了LLM嵌入在AL任务中的系统性研究空白。

Method: 采用MTEB排行榜上的五种高性能LLM模型和两种基线，在十种文本分类任务中测试不同查询策略的表现。

Result: 多样性采样与高质量嵌入协同提升早期AL性能；嵌入质量影响最佳策略选择，Badge策略在多数任务中表现稳健。

Conclusion: AL策略需结合嵌入质量和任务特点进行针对性评估，嵌入质量对策略效果有显著影响。

Abstract: The advent of large language models (LLMs) capable of producing
general-purpose representations lets us revisit the practicality of deep active
learning (AL): By leveraging frozen LLM embeddings, we can mitigate the
computational costs of iteratively fine-tuning large backbones. This study
establishes a benchmark and systematically investigates the influence of LLM
embedding quality on query strategies in deep AL. We employ five top-performing
models from the massive text embedding benchmark (MTEB) leaderboard and two
baselines for ten diverse text classification tasks. Our findings reveal key
insights: First, initializing the labeled pool using diversity-based sampling
synergizes with high-quality embeddings, boosting performance in early AL
iterations. Second, the choice of the optimal query strategy is sensitive to
embedding quality. While the computationally inexpensive Margin sampling can
achieve performance spikes on specific datasets, we find that strategies like
Badge exhibit greater robustness across tasks. Importantly, their effectiveness
is often enhanced when paired with higher-quality embeddings. Our results
emphasize the need for context-specific evaluation of AL strategies, as
performance heavily depends on embedding quality and the target task.

</details>


### [3] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
*Abhay Gupta,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Key words: 大语言模型, 多跳推理, 长上下文, 问答基准, 自然叙事

TL;DR: NovelHopQA是首个评估在64k-128k令牌长文小说片段上进行1-4跳问答的基准，发现模型在多跳和长上下文下表现下降。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大语言模型（LLMs）在涉及多跳推理的长上下文问答中表现不佳，缺乏结合长上下文和多跳推理的自然叙事评估基准。

Method: 通过关键词引导的管道构建基于连贯故事情节的多跳问答链，评估6种前沿模型，并应用理想上下文过滤确保问题可解。

Result: 模型在多跳和长上下文下的准确性显著下降，揭示仅靠规模无法保证稳健推理。

Conclusion: NovelHopQA为大规模多跳推理的测试提供了诊断工具，揭示了模型的常见失败模式。

Abstract: Current large language models (LLMs) struggle to answer questions that span
tens of thousands of tokens, especially when multi-hop reasoning is involved.
While prior benchmarks explore long-context comprehension or multi-hop
reasoning in isolation, none jointly vary context length and reasoning depth in
natural narrative settings. We introduce NovelHopQA, the first benchmark to
evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length
public-domain novels. A keyword-guided pipeline builds hop-separated chains
grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models
and apply oracle-context filtering to ensure all questions are genuinely
answerable. Human annotators validate both alignment and hop depth. We noticed
consistent accuracy drops with increased hops and context length, even in
frontier models-revealing that sheer scale does not guarantee robust reasoning.
Our failure mode analysis highlights common breakdowns, such as missed
final-hop integration and long-range drift. NovelHopQA offers a controlled
diagnostic setting to stress-test multi-hop reasoning at scale.

</details>


### [4] [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
*Timothy Do,Pranav Saran,Harshita Poojary,Pranav Prabhu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Key words: 比喻语言, 低资源语言, mBERT, 注意力头剪枝, Konkani

TL;DR: 该论文提出了一种混合模型，结合mBERT、双向LSTM和线性分类器，用于低资源语言（如Konkani）中的比喻语言分类，并通过梯度注意力头剪枝提升效率，分别达到78%和83%的准确率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决低资源语言中比喻语言表达对NLP系统带来的挑战。

Method: 提出混合模型（mBERT+双向LSTM+线性分类器），并采用梯度注意力头剪枝策略。

Result: 在比喻分类和习语分类任务中分别达到78%和83%的准确率。

Conclusion: 注意力头剪枝对低资源语言中高效NLP工具构建有效。

Abstract: In this paper, we address the persistent challenges that figurative language
expressions pose for natural language processing (NLP) systems, particularly in
low-resource languages such as Konkani. We present a hybrid model that
integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM
and a linear classifier. This architecture is fine-tuned on a newly introduced
annotated dataset for metaphor classification, developed as part of this work.
To improve the model's efficiency, we implement a gradient-based attention head
pruning strategy. For metaphor classification, the pruned model achieves an
accuracy of 78%. We also applied our pruning approach to expand on an existing
idiom classification task, achieving 83% accuracy. These results demonstrate
the effectiveness of attention head pruning for building efficient NLP tools in
underrepresented languages.

</details>


### [5] [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)
*Christopher Lee Lübbers*

Key words: 转述生成, 直接偏好优化, 语义分析, 语言模型, 人工标注

TL;DR: 该研究通过人类排名数据集和直接偏好优化（DPO）技术，提升了语义保持和语言变换的准确性，改进了生成和检测模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法依赖自动化指标和有限的人类标注数据，导致生成结果与人类偏好不一致。

Method: 使用人类排名的数据集和DPO技术，直接对齐模型输出与人类判断。

Result: 生成准确率提高3%，人类偏好评分提升7%。检测模型在特定任务中F1分数达0.91。

Conclusion: 偏好数据和DPO训练能生成更可靠的语义准确转述，推动下游应用。

Abstract: Paraphrasing re-expresses meaning to enhance applications like text
simplification, machine translation, and question-answering. Specific
paraphrase types facilitate accurate semantic analysis and robust language
models. However, existing paraphrase-type generation methods often misalign
with human preferences due to reliance on automated metrics and limited
human-annotated training data, obscuring crucial aspects of semantic fidelity
and linguistic transformations.
  This study addresses this gap by leveraging a human-ranked paraphrase-type
dataset and integrating Direct Preference Optimization (DPO) to align model
outputs directly with human judgments. DPO-based training increases
paraphrase-type generation accuracy by 3 percentage points over a supervised
baseline and raises human preference ratings by 7 percentage points. A newly
created human-annotated dataset supports more rigorous future evaluations.
Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for
addition/deletion, 0.78 for same polarity substitution, and 0.70 for
punctuation changes.
  These findings demonstrate that preference data and DPO training produce more
reliable, semantically accurate paraphrases, enabling downstream applications
such as improved summarization and more robust question-answering. The PTD
model surpasses automated metrics and provides a more reliable framework for
evaluating paraphrase quality, advancing paraphrase-type research toward
richer, user-aligned language generation and establishing a stronger foundation
for future evaluations grounded in human-centric criteria.

</details>


### [6] [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)
*E Fan,Weizong Wang,Tianhan Zhang*

Key words: ChatCFD, CFD, OpenFOAM, 自然语言处理, 自动化

TL;DR: ChatCFD是一个基于大语言模型的自动化CFD工作流程工具，通过自然语言或文献快速配置和执行复杂模拟，无需专业背景。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有CFD工具操作复杂且需要专业知识，限制了科学和工程应用，ChatCFD旨在解决这一问题。

Method: 通过结构化数据库构建、配置验证和错误反思，将CFD与OpenFOAM知识融入普通语言模型，提升准确性和适应性。

Result: 验证表明，ChatCFD能自主复现已发表结果，并处理复杂未知配置，超越普通语言模型。

Conclusion: ChatCFD为CFD提供了一种高效、易用的自动化解决方案，拓展了非专家的使用场景。

Abstract: Computational Fluid Dynamics (CFD) is essential for scientific and
engineering advancements but is limited by operational complexity and the need
for extensive expertise. This paper presents ChatCFD, a large language
model-driven pipeline that automates CFD workflows within the OpenFOAM
framework. It enables users to configure and execute complex simulations from
natural language prompts or published literature with minimal expertise. The
innovation is its structured approach to database construction, configuration
validation, and error reflection, integrating CFD and OpenFOAM knowledge with
general language models to improve accuracy and adaptability. Validation shows
ChatCFD can autonomously reproduce published CFD results, handling complex,
unseen configurations beyond basic examples, a task challenging for general
language models.

</details>


### [7] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
*Feng Wang,Yiding Sun,Jiaxin Mao,Wei Xue,Danqing Xu*

Key words: 大型语言模型,金融RAG,FinS-Pilot,动态数据整合

TL;DR: FinS-Pilot是一个用于评估在线金融应用中RAG系统的新基准，解决了数据保密性和动态数据整合的问题，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于数据保密性和缺乏动态数据整合，金融RAG基准的发展受限，FinS-Pilot旨在填补这一空白。

Method: 基于真实金融助手交互构建，结合实时API数据和结构化文本源，采用意图分类框架覆盖关键金融领域。

Result: 通过系统性实验验证了FinS-Pilot在评估金融助手能力方面的有效性。

Conclusion: FinS-Pilot为金融NLP系统研究提供了实用的评估框架和精选数据集。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various professional domains, with their performance typically evaluated
through standardized benchmarks. However, the development of financial RAG
benchmarks has been constrained by data confidentiality issues and the lack of
dynamic data integration. To address this issue, we introduces FinS-Pilot, a
novel benchmark for evaluating RAG systems in online financial applications.
Constructed from real-world financial assistant interactions, our benchmark
incorporates both real-time API data and structured text sources, organized
through an intent classification framework covering critical financial domains
such as equity analysis and macroeconomic forecasting. The benchmark enables
comprehensive evaluation of financial assistants' capabilities in handling both
static knowledge and time-sensitive market information. Through systematic
experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's
effectiveness in identifying models suitable for financial applications while
addressing the current gap in specialized evaluation tools for the financial
domain. Our work contributes both a practical evaluation framework and a
curated dataset to advance research in financial NLP systems. The code and
dataset are accessible on
GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [8] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
*Duzhen Zhang,Yong Ren,Zhong-Zhi Li,Yahan Yu,Jiahua Dong,Chenxing Li,Zhilong Ji,Jinfeng Bai*

Key words: 多模态持续指令调优, 灾难性遗忘, BranchLoRA, 任务路由, MLLM

TL;DR: 论文提出BranchLoRA框架，通过非对称结构和任务路由机制，显著提升多模态持续指令调优的效率和性能，避免灾难性遗忘。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于Mixture-of-Experts (MoE) LoRA的方法在持续任务中易受灾难性遗忘影响，且参数效率低，亟需改进。

Method: 引入BranchLoRA框架，采用灵活调优-冻结机制和任务路由策略，优化分支协作与任务分配。

Result: 实验表明BranchLoRA在最新基准上显著优于MoELoRA，且适用于不同规模的MLLM。

Conclusion: BranchLoRA通过任务适应和协作机制，有效提升持续指令调优的性能和稳定性。

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal
Large Language Models (MLLMs) to continually align with human intent across
sequential tasks. Existing approaches often rely on the Mixture-of-Experts
(MoE) LoRA framework to preserve previous instruction alignments. However,
these methods are prone to Catastrophic Forgetting (CF), as they aggregate all
LoRA blocks via simple summation, which compromises performance over time. In
this paper, we identify a critical parameter inefficiency in the MoELoRA
framework within the MCIT context. Based on this insight, we propose
BranchLoRA, an asymmetric framework to enhance both efficiency and performance.
To mitigate CF, we introduce a flexible tuning-freezing mechanism within
BranchLoRA, enabling branches to specialize in intra-task knowledge while
fostering inter-task collaboration. Moreover, we incrementally incorporate
task-specific routers to ensure an optimal branch distribution over time,
rather than favoring the most recent task. To streamline inference, we
introduce a task selector that automatically routes test inputs to the
appropriate router without requiring task identity. Extensive experiments on
the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms
MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [9] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
*Xiang Li,Jiayi Xin,Qi Long,Weijie J. Su*

Key words: 大语言模型, 评估框架, 未见知识, KnowSum

TL;DR: 论文提出了KnowSum框架，通过量化未见知识来更全面地评估大语言模型，解决了当前评估中忽视未观察知识的问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前对大语言模型的评估往往不一致且不能全面反映其真实能力，原因是忽视了编码在模型中但未被观察到的知识，即“未见知识”。

Method: 引入KnowSum统计框架，通过从已观察知识实例的出现频率外推，量化未见知识的量，从而提供更全面的评估。

Result: 实验表明，KnowSum在估计总知识量、评估信息检索效率和衡量输出多样性方面有效，且发现仅依赖观察性能会遗漏大量知识。

Conclusion: KnowSum显著改变了基于内部知识的常见大语言模型比较排名，为更准确的评估提供了工具。

Abstract: Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.

</details>


### [10] [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)
*Juncheng Wu,Sheng Liu,Haoqin Tu,Hang Yu,Xiaoke Huang,James Zou,Cihang Xie,Yuyin Zhou*

Key words: 大语言模型,推理质量,监督微调,强化学习,医疗领域,数学领域

TL;DR: 论文研究了大语言模型的推理过程质量与透明度，通过分解知识推理路径并引入评估框架，揭示了监督微调和强化学习在不同领域的效果差异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大语言模型在复杂任务上表现优异，但其内部推理过程的质量与透明度尚未充分研究。

Method: 提出细粒度评估框架，将推理分解为知识与推理两部分，分别用知识指数（KI）和信息增益（InfoGain）衡量。

Result: 研究发现：R1模型在医疗领域的通用推理能力未有效迁移；监督微调提升准确率但降低推理质量；强化学习能优化医疗推理的准确性。

Conclusion: 不同训练方法对推理能力的影响因领域而异，强化学习在医疗领域中表现更优。

Abstract: Recent advances in reasoning-enhanced Large Language Models such as
OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex
tasks. However, the quality and transparency of their internal reasoning
processes remain underexplored. This work moves beyond the final-answer
accuracy and investigates step-by-step reasoning in the medical and
mathematical domains by explicitly decomposing the thinking trajectories into
two parts: knowledge and reasoning. Specifically, we introduce a fine-grained
evaluation framework that judges: (1) the correctness of knowledge used
(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured
by Information Gain (InfoGain)). Using this framework, we study R1-distilled
and base Qwen models trained with supervised fine-tuning (SFT) and/or
reinforcement learning (RL) in the medical and math domains. Three intriguing
findings emerge: (1) The general reasoning abilities in R1-distilled models do
not transfer effectively to the medical domain through either SFT or RL. (2)
SFT raises final-answer accuracy in both domains, but often at the cost of
reasoning quality: InfoGain drops by 38.9% on average compared with untrained
models; In the medical domain, however, SFT remains crucial because domain
knowledge is indispensable. (3) RL enhances medical reasoning by pruning
inaccurate or irrelevant knowledge from reasoning paths, thereby improving both
reasoning accuracy and knowledge correctness.

</details>


### [11] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
*Michael Li,Nishant Subramani*

Key words: Transformer, 语言模型, 词汇表示, 形态学, 线性可分

TL;DR: 研究了不同规模的Transformer模型（包括经典和现代LLMs）如何编码词汇和形态信息，发现词汇信息早期线性，后期非线性，而形态信息始终线性可分，且依赖记忆词汇但抽象形态。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 理解现代大型语言模型如何编码语言信息，填补对早期模型（如BERT、GPT-2）研究的不足。

Method: 在模型各层激活上训练线性和非线性分类器，预测词的原型和形态特征。

Result: 词汇信息在早期线性、后期非线性；形态信息始终线性可分，依赖抽象规则。16种模型均表现出相同模式。

Conclusion: 尽管模型差异大，但语言信息的组织方式一致，可能是预训练中习得的基本特性。

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing.

</details>


### [12] [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)
*Joshua Rozner,Leonie Weissweiler,Cory Shain*

Key words: Construction grammar, pretrained language models, developmental plausibility, BabyLM

TL;DR: 研究表明，即使在符合儿童语言发展数据量的训练下，模型也能学习多样的构造，且构造学习能力与模型性能正相关。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 质疑现有预训练语言模型（PLM）在数据量上对人类语言学习的相关性，验证在更符合儿童语言发展数据量的模型是否仍能学习构造。

Method: 使用Rozner等的方法评估2024 BabyLM挑战中模型对构造的学习能力。

Result: 即使在符合发展的数据量下，模型仍能学习多样的构造，且构造学习能力与模型性能正相关。

Conclusion: 构造学习能力在数据量受限时仍存在，且可能对模型性能有功能上的重要性。

Abstract: Construction grammar posits that children acquire constructions (form-meaning
pairings) from the statistics of their environment. Recent work supports this
hypothesis by showing sensitivity to constructions in pretrained language
models (PLMs), including one recent study (Rozner et al., 2025) demonstrating
that constructions shape the PLM's output distribution. However, models under
study have generally been trained on developmentally implausible amounts of
data, casting doubt on their relevance to human language learning. Here we use
Rozner et al.'s methods to evaluate constructional learning in models from the
2024 BabyLM challenge. Our results show that even when trained on
developmentally plausible quantities of data, models represent diverse
constructions, even hard cases that are superficially indistinguishable. We
further find correlational evidence that constructional performance may be
functionally relevant: models that better represent constructions perform
better on the BabyLM benchmarks.

</details>


### [13] [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)
*Amir Hussein,Cihan Xiao,Matthew Wiesner,Dan Povey,Leibny Paola Garcia,Sanjeev Khudanpur*

Key words: 神经转导器,HENT-SRT,语音识别,语音翻译,自蒸馏

TL;DR: HENT-SRT是一种针对语音识别和翻译任务的新型神经转导器框架，显著提升了性能并降低了计算成本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有神经转导器在语音翻译任务中处理词序和性能退化的挑战，同时降低高计算训练成本。

Method: 使用分层分解ASR和翻译任务，结合自蒸馏与CTC一致性正则化，优化编码器和预测器设计，并引入解码空白惩罚。

Result: 在多个对话数据集上达到新的SOTA性能，显著缩小了与AED模型的差距。

Conclusion: HENT-SRT有效解决了神经转导器在语音翻译中的问题，同时提升了效率和性能。

Abstract: Neural transducers (NT) provide an effective framework for speech streaming,
demonstrating strong performance in automatic speech recognition (ASR).
However, the application of NT to speech translation (ST) remains challenging,
as existing approaches struggle with word reordering and performance
degradation when jointly modeling ASR and ST, resulting in a gap with
attention-based encoder-decoder (AED) models. Existing NT-based ST approaches
also suffer from high computational training costs. To address these issues, we
propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech
Recognition and Translation), a novel framework that factorizes ASR and
translation tasks to better handle reordering. To ensure robust ST while
preserving ASR performance, we use self-distillation with CTC consistency
regularization. Moreover, we improve computational efficiency by incorporating
best practices from ASR transducers, including a down-sampled hierarchical
encoder, a stateless predictor, and a pruned transducer loss to reduce training
complexity. Finally, we introduce a blank penalty during decoding, reducing
deletions and improving translation quality. Our approach is evaluated on three
conversational datasets Arabic, Spanish, and Mandarin achieving new
state-of-the-art performance among NT models and substantially narrowing the
gap with AED-based systems.

</details>


### [14] [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)
*Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli,Andre Martins,Giuseppe Attanasio*

Key words: 语音翻译, 性别编码, 模型架构, 翻译偏差

TL;DR: 研究了语音翻译模型中隐藏状态对说话者性别特征的捕获能力，发现传统模型能捕捉性别信息，而新型模型（通过适配器集成语音编码器和机器翻译系统）则不能，且后者更倾向于男性默认翻译。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨语音翻译模型是否捕获说话者性别特征及其对翻译中性别分配的影响。

Method: 使用探测方法评估不同语音翻译模型中的性别编码能力。

Result: 传统编码器-解码器模型能捕捉性别信息，新架构则不能，且新架构更倾向于男性默认翻译。

Conclusion: 语音翻译模型的架构差异影响性别编码能力，新架构在性别分配上存在偏差。

Abstract: Recent studies on interpreting the hidden states of speech models have shown
their ability to capture speaker-specific features, including gender. Does this
finding also hold for speech translation (ST) models? If so, what are the
implications for the speaker's gender assignment in translation? We address
these questions from an interpretability perspective, using probing methods to
assess gender encoding across diverse ST models. Results on three language
directions (English-French/Italian/Spanish) indicate that while traditional
encoder-decoder models capture gender information, newer architectures --
integrating a speech encoder with a machine translation system via adapters --
do not. We also demonstrate that low gender encoding capabilities result in
systems' tendency toward a masculine default, a translation bias that is more
pronounced in newer architectures.

</details>


### [15] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
*Salman Rahman,Sheriff Issaka,Ashima Suvarna,Genglin Liu,James Shiffer,Jaeyoung Lee,Md Rizwan Parvez,Hamid Palangi,Shi Feng,Nanyun Peng,Yejin Choi,Julian Michael,Liwei Jiang,Saadia Gabriel*

Key words: AI辩论, 事实判断, 人类偏见, COVID-19, 监督AI

TL;DR: 该论文研究了通过AI辩论来解决人类偏见对事实判断的影响，特别是在COVID-19相关争议性话题中，发现辩论显著提高了判断的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着AI影响力的增加，其可能放大错误信息或加深社会分裂。研究旨在探讨如何通过AI辩论帮助人类在存在偏见的情况下更准确地判断事实。

Method: 研究包括两个部分：一是让具有主流或怀疑论信念的人类法官通过AI辩论或咨询协议评估事实性声明；二是用模拟人类信念的AI法官重复实验。

Result: 研究发现，辩论模式比单一顾问模式整体提高10%的判断准确性，特别是对主流信念持有者（+15.2%）。AI法官的准确性（78.5%）高于人类（70.1%）和默认AI法官（69.8%）。

Conclusion: AI辩论是一种有效的、可扩展的监督方法，能够结合人类和AI的多样性判断，帮助在争议性领域中更接近真相。

Abstract: As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics like public health
where factual accuracy directly impacts well-being. Scalable Oversight aims to
ensure AI truthfulness by enabling humans to supervise systems that may exceed
human capabilities--yet humans themselves hold different beliefs and biases
that impair their judgment. We study whether AI debate can guide biased judges
toward the truth by having two AI systems debate opposing sides of
controversial COVID-19 factuality claims where people hold strong prior
beliefs. We conduct two studies: one with human judges holding either
mainstream or skeptical beliefs evaluating factuality claims through
AI-assisted debate or consultancy protocols, and a second examining the same
problem with personalized AI judges designed to mimic these different human
belief systems. In our human study, we find that debate-where two AI advisor
systems present opposing evidence-based arguments-consistently improves
judgment accuracy and confidence calibration, outperforming consultancy with a
single-advisor system by 10% overall. The improvement is most significant for
judges with mainstream beliefs (+15.2% accuracy), though debate also helps
skeptical judges who initially misjudge claims move toward accurate views
(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like
personas achieve even higher accuracy (78.5%) than human judges (70.1%) and
default AI judges without personas (69.8%), suggesting their potential for
supervising frontier AI models. These findings highlight AI debate as a
promising path toward scalable, bias-resilient oversight--leveraging both
diverse human and AI judgments to move closer to truth in contested domains.

</details>


### [16] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
*Dennis Fucci,Marco Gaido,Matteo Negri,Mauro Cettolo,Luisa Bentivogli*

Key words: ASR, 声学线索, 特征归因, Conformer模型, 可解释性

TL;DR: 本文通过特征归因技术分析了现代Conformer-based ASR系统依赖的声学线索，发现模型对元音、摩擦音和爆破音的感知方式与人类听觉特性一致，但也揭示了潜在的鲁棒性不足问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管ASR技术取得了显著进展，但模型依赖的具体声学线索仍不明确。之前的研究仅针对有限音素和过时模型，因此有必要对现代ASR系统进行更全面的分析。

Method: 使用特征归因技术，分析现代Conformer-based ASR模型对爆破音、摩擦音和元音的依赖，并与时频域中的声学特性及人类听觉特性对比。

Result: 模型更依赖元音的整个时间跨度（特别是前两个共振峰），且对男性语音更敏感；对摩擦音中的擦音谱特征捕捉更准确；爆破音中更重视释放阶段的爆破特性。

Conclusion: 研究增强了ASR模型的可解释性，并指出了未来研究的方向，以揭示模型鲁棒性可能存在的不足。

Abstract: Despite significant advances in ASR, the specific acoustic cues models rely
on remain unclear. Prior studies have examined such cues on a limited set of
phonemes and outdated models. In this work, we apply a feature attribution
technique to identify the relevant acoustic cues for a modern Conformer-based
ASR system. By analyzing plosives, fricatives, and vowels, we assess how
feature attributions align with their acoustic properties in the time and
frequency domains, also essential for human speech perception. Our findings
show that the ASR model relies on vowels' full time spans, particularly their
first two formants, with greater saliency in male speech. It also better
captures the spectral characteristics of sibilant fricatives than non-sibilants
and prioritizes the release phase in plosives, especially burst
characteristics. These insights enhance the interpretability of ASR models and
highlight areas for future research to uncover potential gaps in model
robustness.

</details>


### [17] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
*Lindia Tjuatja,Graham Neubig*

Key words: 语言模型评估, 性能比较, BehaviorBox, 上下文嵌入, 细粒度特征

TL;DR: 论文提出了一种名为BehaviorBox的自动化方法，用于比较语言模型的细粒度性能差异，通过上下文嵌入找到模型间的具体优劣场景。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语言模型评估复杂且主观，传统的困惑度指标模糊，难以捕捉模型间的具体差异。

Method: 使用性能感知的上下文嵌入（BehaviorBox），提取细粒度特征，展示模型在特定上下文中的性能差异。

Result: BehaviorBox能够识别模型在具体上下文中的优劣，如特定短语或标点使用场景，优于传统困惑度指标。

Conclusion: 自动化方法BehaviorBox为语言模型评估提供了更细粒度和可解释的差异分析工具。

Abstract: Language model evaluation is a daunting task: prompts are brittle,
corpus-level perplexities are vague, and the choice of benchmarks are endless.
Finding examples that show meaningful, generalizable differences between two
LMs is crucial to understanding where one model succeeds and another fails. Can
this process be done automatically? In this work, we propose methodology for
automated comparison of language models that uses performance-aware contextual
embeddings to find fine-grained features of text where one LM outperforms
another. Our method, which we name BehaviorBox, extracts coherent features that
demonstrate differences with respect to the ease of generation between two LMs.
Specifically, BehaviorBox finds features that describe groups of words in
fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'"
and "exclamation marks after emotional statements", where one model outperforms
another within a particular datatset. We apply BehaviorBox to compare models
that vary in size, model family, and post-training, and enumerate insights into
specific contexts that illustrate meaningful differences in performance which
cannot be found by measures such as corpus-level perplexity alone.

</details>


### [18] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
*Ella Rannon,David Burstein*

Key words: NLP,生物序列,基因组学,transformer,生物信息学

TL;DR: 本文综述了NLP方法在生物序列数据（如DNA、RNA和蛋白质）中的应用，探讨了从经典模型到先进技术的适应性、优势和局限性，并强调了其在生物信息学中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索NLP技术在生物序列分析中的适应性，以解决基因组、转录组和蛋白质组数据分析中的挑战。

Method: 综述了包括word2vec、transformer和hyena算子等NLP方法在生物序列中的适应性，并分析其建模和标记化策略。

Result: 展示了NLP方法在结构预测、基因表达和进化分析等生物任务中的应用潜力。

Conclusion: 随着语言模型的进步，NLP在生物信息学中的应用有望为理解生命过程提供新的见解。

Abstract: Natural Language Processing (NLP) has transformed various fields beyond
linguistics by applying techniques originally developed for human language to
the analysis of biological sequences. This review explores the application of
NLP methods to biological sequence data, focusing on genomics, transcriptomics,
and proteomics. We examine how various NLP methods, from classic approaches
like word2vec to advanced models employing transformers and hyena operators,
are being adapted to analyze DNA, RNA, protein sequences, and entire genomes.
The review also examines tokenization strategies and model architectures,
evaluating their strengths, limitations, and suitability for different
biological tasks. We further cover recent advances in NLP applications for
biological data, such as structure prediction, gene expression, and
evolutionary analysis, highlighting the potential of these methods for
extracting meaningful insights from large-scale genomic data. As language
models continue to advance, their integration into bioinformatics holds immense
promise for advancing our understanding of biological processes in all domains
of life.

</details>


### [19] [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)
*Sofoklis Kakouros*

Key words: 情感识别, 语音信号, 词信息量, 自监督学习

TL;DR: 研究提出用预训练语言模型的词信息量识别语义重要片段，计算这些片段的声学特征以提高语音情感识别的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统方法在整句或较长语音段上计算声学特征的功能统计量，可能忽略细粒度变化。

Method: 利用预训练语言模型的词信息量筛选重要语音段，计算其声学特征和自监督表示。

Result: 基于词信息选择的片段显著提升了情感识别的性能。

Conclusion: 词信息量筛选片段的方法能有效提高情感识别的准确性。

Abstract: In emotion recognition from speech, a key challenge lies in identifying
speech signal segments that carry the most relevant acoustic variations for
discerning specific emotions. Traditional approaches compute functionals for
features such as energy and F0 over entire sentences or longer speech portions,
potentially missing essential fine-grained variation in the long-form
statistics. This research investigates the use of word informativeness, derived
from a pre-trained language model, to identify semantically important segments.
Acoustic features are then computed exclusively for these identified segments,
enhancing emotion recognition accuracy. The methodology utilizes standard
acoustic prosodic features, their functionals, and self-supervised
representations. Results indicate a notable improvement in recognition
performance when features are computed on segments selected based on word
informativeness, underscoring the effectiveness of this approach.

</details>


### [20] [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)
*Radin Shayanfar,Chu Fei Luo,Rohan Bhambhoria,Samuel Dahan,Xiaodan Zhu*

Key words: 对话系统, 专家知识, 结构化异构图, 零样本, 迭代改进

TL;DR: 论文提出了一个名为CoDial的新框架，将专家知识转化为可执行的对话逻辑，支持领域特定对话系统的快速开发与迭代。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于专家知识、训练数据和技术难度的限制，为对话系统教授专业任务具有挑战性。作者旨在建立一个框架，使非技术专家能够轻松定义和改进系统行为。

Method: CoDial框架通过结构化异构图表示专家知识，并将其转换为可执行的对话逻辑，支持现有护栏语言的实现。

Result: CoDial在STAR数据集上达到了最先进的性能，并在MultiWOZ数据集上与基线模型表现相当。同时展示了通过人工和LLM反馈进行迭代改进的能力。

Conclusion: CoDial是一个实用的工具，支持高风险领域中专家引导的LLM对齐，且易于实施和修改。

Abstract: It is often challenging to teach specialized, unseen tasks to dialogue
systems due to the high cost of expert knowledge, training data, and high
technical difficulty. To support domain-specific applications - such as law,
medicine, or finance - it is essential to build frameworks that enable
non-technical experts to define, test, and refine system behaviour with minimal
effort. Achieving this requires cross-disciplinary collaboration between
developers and domain specialists. In this work, we introduce a novel
framework, CoDial (Code for Dialogue), that converts expert knowledge,
represented as a novel structured heterogeneous graph, into executable
conversation logic. CoDial can be easily implemented in existing guardrailing
languages, such as Colang, to enable interpretable, modifiable, and true
zero-shot specification of task-oriented dialogue systems. Empirically, CoDial
achieves state-of-the-art performance on the STAR dataset for inference-based
models and is competitive with similar baselines on the well-known MultiWOZ
dataset. We also demonstrate CoDial's iterative improvement via manual and
LLM-aided feedback, making it a practical tool for expert-guided alignment of
LLMs in high-stakes domains.

</details>


### [21] [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
*Wenzheng Zhang,Xi Victoria Lin,Karl Stratos,Wen-tau Yih,Mingda Chen*

Key words: Retrieval-Augmented Generation, 查询自由, 统一模型, 知识密集型任务

TL;DR: ImpRAG提出了一个无需显式查询的RAG系统，通过统一检索和生成任务，显著提升了模型在多样化任务上的泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统RAG系统将检索和生成作为独立过程，限制了任务的泛化能力。ImpRAG旨在通过统一模型解决这一问题。

Method: ImpRAG将预训练解码器语言模型分为专用层组，通过两阶段推理同时优化检索和生成任务。

Result: 在8个知识密集型任务中，ImpRAG的精确匹配分数提升了3.6-11.5，证明了其有效性。

Conclusion: ImpRAG通过平衡检索和生成参数及利用生成困惑度作为检索目标，显著提升了性能。

Abstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval
and generation as separate processes, requiring explicit textual queries to
connect them. This separation can limit the ability of models to generalize
across diverse tasks. In this work, we propose a query-free RAG system, named
ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG
allows models to implicitly express their information needs, eliminating the
need for human-specified queries. By dividing pretrained decoder-only language
models into specialized layer groups, ImpRAG optimizes retrieval and generation
tasks simultaneously. Our approach employs a two-stage inference process, using
the same model parameters and forward pass for both retrieval and generation,
thereby minimizing the disparity between retrievers and language models.
Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves
3.6-11.5 improvements in exact match scores on unseen tasks with diverse
formats, highlighting its effectiveness in enabling models to articulate their
own information needs and generalize across tasks. Our analysis underscores the
importance of balancing retrieval and generation parameters and leveraging
generation perplexities as retrieval training objectives for enhanced
performance.

</details>


### [22] [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)
*Sofoklis Kakouros,Haoyu Chen*

Key words: 韵律特征、自我监督学习、Wav2Vec 2.0、HuBERT、机器学习

TL;DR: 研究通过韵律特征和自我监督学习（SSL）表征，探讨了网球赛后采访中胜负选手的韵律特征及其分类效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在通过分析赛后采访中的韵律特征和SSL表征，了解胜负选手的声学差异，并探索其分类潜力。

Method: 利用韵律特征（如音高和强度）和SSL模型（Wav2Vec 2.0和HuBERT），提取传统声学特征与深度语音表征，并使用机器学习分类器区分胜负。

Result: SSL表征能有效区分胜负，捕捉与情绪相关的细微语音模式；韵律特征（如音高变异性）仍是胜负的强指标。

Conclusion: 韵律特征和SSL表征在区分网球赛胜负方面具有潜力，尤其是结合情绪相关的语音模式。

Abstract: This study examines the prosodic characteristics associated with winning and
losing in post-match tennis interviews. Additionally, this research explores
the potential to classify match outcomes solely based on post-match interview
recordings using prosodic features and self-supervised learning (SSL)
representations. By analyzing prosodic elements such as pitch and intensity,
alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine
whether an athlete has won or lost their match. Traditional acoustic features
and deep speech representations are extracted from the data, and machine
learning classifiers are employed to distinguish between winning and losing
players. Results indicate that SSL representations effectively differentiate
between winning and losing outcomes, capturing subtle speech patterns linked to
emotional states. At the same time, prosodic cues -- such as pitch variability
-- remain strong indicators of victory.

</details>


### [23] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
*Thai Hoang,Kung-Hsiang Huang,Shirley Kokane,Jianguo Zhang,Zuxin Liu,Ming Zhu,Jake Grigsby,Tian Lan,Michael S Ryoo,Chien-Sheng Wu,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Key words: Large Action Models, AI Agents, LAM SIMULATOR, training data, LLM Agents

TL;DR: LAM SIMULATOR是一个用于在线探索代理任务的框架，通过动态生成任务查询、提供工具集合和实时反馈，帮助LLM代理自主完成任务并生成高质量训练数据。实验显示，使用该框架的数据训练LAM模型性能显著提升（最高49.3%）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决LAMs在需要高质量训练数据，尤其是多步任务中的挑战，提出了LAM SIMULATOR框架。

Method: 框架包括动态任务查询生成器、工具集合和交互环境，使LLM代理能自主探索任务并生成高质量训练数据。

Result: 在ToolBench和CRMArena基准测试中，使用框架生成数据训练的模型性能显著提升（高达49.3%）。

Conclusion: LAM SIMULATOR能高效生成高质量训练数据，显著加速AI代理的开发。

Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face
challenges due to the need for high-quality training data, especially for
multi-steps tasks that involve planning, executing tool calls, and responding
to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive
framework designed for online exploration of agentic tasks with high-quality
feedback. Our framework features a dynamic task query generator, an extensive
collection of tools, and an interactive environment where Large Language Model
(LLM) Agents can call tools and receive real-time feedback. This setup enables
LLM Agents to explore and solve tasks autonomously, facilitating the discovery
of multiple approaches to tackle any given task. The resulting action
trajectory data are then used to create high-quality training datasets for
LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,
highlight the effectiveness of LAM SIMULATOR: models trained with
self-generated datasets using our framework achieve significant performance
gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR
requires minimal human input during dataset creation, highlighting LAM
SIMULATOR's efficiency and effectiveness in speeding up development of AI
agents.

</details>


### [24] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
*Russell Scheinberg,Ameeta Agrawal,Amber Shore,So Young Lee*

Key words: 大型语言模型,语法提示,语法判断,多语言基准测试,小型语言模型

TL;DR: 论文提出了一种名为“语法提示”的方法，通过让大型语言模型先解释语法规则，再利用这些解释作为上下文信息帮助目标模型判断句子正确性，显著提升了模型在语法判断任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管大型语言模型能够解释语法规则，但在实际应用中却难以正确运用这些规则。为了解决这一问题，研究提出了一种新的提示方法。

Method: 采用“解释-处理”范式，即首先由大型语言模型生成语法现象的简短解释，然后将这些解释作为附加上下文输入目标模型（可以是大型或小型语言模型），以判断句子对错。

Result: 在多个基准测试中，该方法显著提升了模型的语法判断能力，特别是在小型语言模型中使用时，能够大幅缩小与大型语言模型的性能差距。

Conclusion: 语法提示方法是一种轻量级、语言无关的技术，可以有效帮助低成本的模型在多语言环境中接近前沿大型语言模型的性能。

Abstract: Large language models (LLMs) can explain grammatical rules, yet they often
fail to apply those rules when judging sentence acceptability. We present
"grammar prompting", an explain-then-process paradigm: a large LLM first
produces a concise explanation of the relevant syntactic phenomenon, then that
explanation is fed back as additional context to the target model -- either an
LLM or a smaller language model (SLM) -- before deciding which sentence of a
minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian
RuBLiMP benchmarks, this simple prompt design yields substantial improvements
over strong baselines across many syntactic phenomena. Feeding an LLM's
metalinguistic explanation back to the target model bridges the gap between
knowing a rule and using it. On SLMs, grammar prompting alone trims the average
LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by
56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,
language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in
multilingual settings.

</details>


### [25] [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)
*Pegah Alipoormolabashi,Ajay Patel,Niranjan Balasubramanian*

Key words: 作者归属、公平性、MAUIk、模型评估、潜在空间嵌入

TL;DR: 论文提出了一个衡量作者归属不公平性的指标MAUIk，并验证了五种模型在数据集上的不公平性，指出模型嵌入方式与不公平性的关系。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 作者归属错误在现实中可能带来严重后果，尤其是法医场景下。现有评估方法未明确考虑公平性，因此需要新指标来衡量不公平性。

Method: 引入Misattribution Unfairness Index (MAUIk)指标，基于作者在未撰写文本中的前k排名频率，评估五种模型在两个数据集上的表现。

Result: 所有模型均表现出高水平的不公平性，部分作者风险更高。不公平性与模型在潜在空间中嵌入作者的方式相关，靠近中心点的作者风险更大。

Conclusion: 研究表明模型可能带来潜在危害，需在使用时与终端用户沟通并校准误归属风险。

Abstract: Authorship misattribution can have profound consequences in real life. In
forensic settings simply being considered as one of the potential authors of an
evidential piece of text or communication can result in undesirable scrutiny.
This raises a fairness question: Is every author in the candidate pool at equal
risk of misattribution? Standard evaluation measures for authorship attribution
systems do not explicitly account for this notion of fairness. We introduce a
simple measure, Misattribution Unfairness Index (MAUIk), which is based on how
often authors are ranked in the top k for texts they did not write. Using this
measure we quantify the unfairness of five models on two different datasets.
All models exhibit high levels of unfairness with increased risks for some
authors. Furthermore, we find that this unfairness relates to how the models
embed the authors as vectors in the latent search space. In particular, we
observe that the risk of misattribution is higher for authors closer to the
centroid (or center) of the embedded authors in the haystack. These results
indicate the potential for harm and the need for communicating with and
calibrating end users on misattribution risk when building and providing such
models for downstream use.

</details>


### [26] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
*Berk Atil,Namrata Sureddy,Rebecca J. Passonneau*

Key words: 毒性检测,大型语言模型,社会群体,TRuST数据集

TL;DR: 该论文介绍了TRuST数据集，用于改进毒性检测，并评估了大型语言模型在毒性检测、目标群体识别和毒性片段提取方面的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线内容中的毒性问题对心理和社会有负面影响，因此需要更有效的检测方法。

Method: 通过合并现有数据集并添加标签，构建TRuST数据集，并测试大型语言模型的表现。

Result: 微调模型表现优于零样本或少样本提示，但对某些社会群体效果不佳，且推理能力未显著提升性能。

Conclusion: LLMs在社会推理能力上较弱，需进一步改进。

Abstract: Toxicity in online content, including content generated by language models,
has become a critical concern due to its potential for negative psychological
and social impact. This paper introduces TRuST, a comprehensive dataset
designed to improve toxicity detection that merges existing datasets, and has
labels for toxicity, target social group, and toxic spans. It includes a
diverse range of target groups such as ethnicity, gender, religion, disability,
and politics, with both human/machine-annotated and human machine-generated
data. We benchmark state-of-the-art large language models (LLMs) on toxicity
detection, target group identification, and toxic span extraction. We find that
fine-tuned models consistently outperform zero-shot and few-shot prompting,
though performance remains low for certain social groups. Further, reasoning
capabilities do not significantly improve performance, indicating that LLMs
have weak social reasoning skills.

</details>


### [27] [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)
*Hyungjoo Chae,Dongjin Kang,Jihyuk Kim,Beong-woo Kwak,Sunghyun Park,Haeju Park,Jinyoung Yeo,Moontae Lee,Kyungjae Lee*

Key words: 长链思维, 数据集构建, 推理能力, 强化学习

TL;DR: 该论文探索了利用未针对推理时间扩展训练的LLMs构建长链思维数据集的方法，提出了Long CoT Collection数据集，并通过实验验证其质量和实用性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前依赖现有大型推理模型（如R1）限制了该领域的发展，因此论文提出了一种独立的长链思维数据集构建方法。

Method: 使用短链思维LLMs标注100K长链思维数据集，开发了诱导新推理策略的流程，并引入了思想预算的可控性。

Result: 数据集质量接近R1，使用该数据集训练不仅提升一般推理能力，还为强化学习提供了更强的基础。

Conclusion: 论文成功证明了通过短链思维LLMs构建长链思维数据集的可行性，并展示了其在实际应用中的潜力。

Abstract: With the release of R1, a publicly available large reasoning model (LRM),
researchers commonly train new LRMs by training language models on R1's long
chain-of-thought (CoT) inferences. While prior works show that LRMs'
capabilities can be reproduced through direct distillation, the continued
reliance on the existing models (e.g., R1) remains a critical limitation in
advancing the field. As a first step toward independent LRM development, this
paper explores the possibility of constructing a long CoT dataset with LLMs
that are not trained for inference-time scaling. To this end, we present the
Long CoT Collection, a dataset of 100K CoT rationales annotated using existing
short CoT LLMs. We develop a pipeline that induces o1's novel reasoning
strategies into short CoT LLMs, enabling them to think longer and introducing
controllability over the thought budget to better manage the overthinking
problem. Our extensive analyses validate that our dataset achieves quality
comparable to--or slightly below--R1. Furthermore, our experiments demonstrate
that training on our dataset not only strengthens general reasoning skills, but
also provides a strong foundation for reinforcement learning--models
initialized on our data achieve 2-3x larger gains with RLVR.

</details>


### [28] [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)
*Jiaming Li,Yukun Chen,Ziqiang Liu,Minghuan Tan,Lei Zhang,Yunshui Li,Run Luo,Longze Chen,Jing Luo,Ahmadreza Argha,Hamid Alinejad-Rokny,Wei Zhou,Min Yang*

Key words: 自动故事生成，SVO三元组，叙事连贯性，NEKG

TL;DR: 提出了一种基于语言学SVO三元组的自动故事生成方法Storyteller，显著提升了故事的一致性和连贯性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有自动故事生成方法在叙事连贯性和逻辑一致性上表现不佳，Storyteller旨在通过模仿人类认知过程解决这一问题。

Method: 采用基于SVO三元组的剧情节点结构，结合动态模块STORYLINE和NEKG，持续交互优化故事生成。

Result: 在人类偏好评估中平均胜出率达84.33%，并在创意、连贯性、吸引力和相关性上表现优异。

Conclusion: Storyteller通过系统性优化，显著提升了自动生成故事的逻辑连贯性和叙事质量。

Abstract: Stories are central to human culture, serving to share ideas, preserve
traditions, and foster connections. Automatic story generation, a key
advancement in artificial intelligence (AI), offers new possibilities for
creating personalized content, exploring creative ideas, and enhancing
interactive experiences. However, existing methods struggle to maintain
narrative coherence and logical consistency. This disconnect compromises the
overall storytelling experience, underscoring the need for substantial
improvements. Inspired by human cognitive processes, we introduce Storyteller,
a novel approach that systemically improves the coherence and consistency of
automatically generated stories. Storyteller introduces a plot node structure
based on linguistically grounded subject verb object (SVO) triplets, which
capture essential story events and ensure a consistent logical flow. Unlike
previous methods, Storyteller integrates two dynamic modules, the STORYLINE and
narrative entity knowledge graph (NEKG),that continuously interact with the
story generation process. This integration produces structurally sound,
cohesive and immersive narratives. Extensive experiments demonstrate that
Storyteller significantly outperforms existing approaches, achieving an 84.33%
average win rate through human preference evaluation. At the same time, it is
also far ahead in other aspects including creativity, coherence, engagement,
and relevance.

</details>


### [29] [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)
*Herun Wan,Jiaying Wu,Minnan Luo,Zhi Zeng,Zhixiong Su*

Key words: 虚假信息检测、捷径学习、数据增强、大型语言模型、鲁棒性

TL;DR: 该论文提出了TruthOverTricks评估范式，揭示现有虚假信息检测模型的性能因依赖表面线索（捷径）而下降，并提出了基于LLM的数据增强框架SMF以提高模型的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有虚假信息检测模型容易依赖训练数据中的表面线索（捷径），但在面对多样化和动态变化的虚假信息时泛化能力不足，尤其是大型语言模型（LLMs）能轻松生成具有迷惑性的虚假信息。

Method: 论文提出TruthOverTricks评估范式，分类捷径行为（内在诱导与外在注入），并在14个基准数据集上评估7种代表性检测器。同时，提出了SMF框架，通过数据增强（如释义、事实摘要和情感归一化）减少对捷径的依赖。

Result: 实验显示，现有检测器在面对自然或对抗性捷径时性能显著下降，而SMF框架能够持续提升模型在16个基准上的鲁棒性，促使其依赖深层语义理解。

Conclusion: 该研究为虚假信息检测领域提供了新的评估工具和优化方法，揭示了捷径学习的局限，并通过数据增强提升了模型的性能。

Abstract: Misinformation detection models often rely on superficial cues (i.e.,
\emph{shortcuts}) that correlate with misinformation in training data but fail
to generalize to the diverse and evolving nature of real-world misinformation.
This issue is exacerbated by large language models (LLMs), which can easily
generate convincing misinformation through simple prompts. We introduce
TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning
in misinformation detection. TruthOverTricks categorizes shortcut behaviors
into intrinsic shortcut induction and extrinsic shortcut injection, and
evaluates seven representative detectors across 14 popular benchmarks, along
with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.
Empirical results reveal that existing detectors suffer severe performance
degradation when exposed to both naturally occurring and adversarially crafted
shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation
framework that mitigates shortcut reliance through paraphrasing, factual
summarization, and sentiment normalization. SMF consistently enhances
robustness across 16 benchmarks, encouraging models to rely on deeper semantic
understanding rather than shortcut cues. To promote the development of
misinformation detectors, we have published the resources publicly at
https://github.com/whr000001/TruthOverTricks.

</details>


### [30] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
*Jeonghun Kang,Soonmok Kwon,Joonseok Lee,Byung-Hak Kim*

Key words: DIAMOND, 棒球摘要, LLM驱动代理, sabermetric特征, 上下文感知

TL;DR: DIAMOND是一款结合结构化体育分析和自然语言推理的LLM驱动代理，用于棒球高光时刻的上下文感知摘要，超越传统基于统计或视觉的系统。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统方法如WPA排名或计算机视觉事件检测缺乏战略深度和故事性，人工标记虽准确但不高效。DIAMOND旨在填补这一空白。

Method: DIAMOND结合sabermetric特征（如Win Expectancy）和LLM模块，定量评估比赛重要性并增强上下文叙事价值。

Result: 在五场韩国棒球联盟比赛中，DIAMOND的F1分数从42.9%提升至84.8%，优于商业和统计基线。

Conclusion: DIAMOND展示了模块化、可解释的基于代理的框架在体育及其他领域事件摘要中的潜力。

Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking
or computer vision-driven event detection -- can identify scoring plays but
often miss strategic depth, momentum shifts, and storyline progression. Manual
curation remains the gold standard but is resource-intensive and not scalable.
We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight
summarization that integrates structured sports analytics with natural language
reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and
Leverage Index -- to quantify play importance, while an LLM module enhances
selection based on contextual narrative value. This hybrid approach ensures
both quantitative rigor and qualitative richness, surpassing the limitations of
purely statistical or vision-based systems. Evaluated on five diverse Korean
Baseball Organization League games, DIAMOND improves F1-score from 42.9%
(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.
Though limited in scale, our results highlight the potential of modular,
interpretable agent-based frameworks for event-level summarization in sports
and beyond.

</details>


### [31] [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)
*Hisami Suzuki,Satoru Katsumata,Takashi Kodama,Tetsuro Takahashi,Kouta Nakayama,Satoshi Sekine*

Key words: 日本LLM, 安全性, 数据集, 微调, 社会文化背景

TL;DR: AnswerCarefully是一个用于提升日本LLM输出安全性和适当性的数据集，包含1800个需要特别注意的问题-答案对，覆盖多种风险类别，并反映日本的社会文化背景。使用该数据集微调LLM可提高安全性而不影响通用回答效果，同时评估了12个日本LLM的安全性。数据集最近更新了英文翻译和标注，方便跨语言和地区使用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为日本LLM输出的安全性和适当性提供专门的评估和优化工具，考虑本土社会文化背景。

Method: 构建包含1800个问题-答案对的AnswerCarefully数据集，用于微调LLM并进行安全性评估。

Result: 微调的日本LLM安全性提升且通用回答效果不受影响，同时12个日本LLM的安全性得到了基准测试。

Conclusion: AnswerCarefully数据集有效提升LLM安全性并支持跨语言推广。

Abstract: In this paper we present AnswerCarefully, a dataset for promoting the safety
and appropriateness of Japanese LLM outputs. The dataset consists of 1,800
pairs of questions and reference answers, where the questions require special
attention in answering. It covers a wide range of risk categories established
in prior English-language datasets, but the data samples are original in that
they are manually created to reflect the socio-cultural context of LLM usage in
Japan. We show that using this dataset for instruction to fine-tune a Japanese
LLM led to improved output safety without compromising the utility of general
responses. We also report the results of a safety evaluation of 12 Japanese
LLMs using this dataset as a benchmark. Finally, we describe the latest update
on the dataset which provides English translations and annotations of the
questions, aimed at facilitating the derivation of similar datasets in
different languages and regions.

</details>


### [32] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
*Ukyo Honda,Tatsushi Oka*

Key words: in-context learning, LLMs, robustness, explanations, out-of-distribution

TL;DR: X^2-ICL扩展了带有解释的上下文学习（X-ICL），通过系统探索所有可能标签的解释，提升了模型对分布外数据的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决传统ICL方法在超出演示分布时泛化能力不足的问题。

Method: 提出X^2-ICL框架，通过系统探索所有可能标签的解释来增强模型推理能力。

Result: 在多个人工理解数据集上，X^2-ICL显著提升了模型对分布外数据的鲁棒性。

Conclusion: X^2-ICL是一种更全面且鲁棒的上下文学习方法。

Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging
large language models (LLMs). However, it often struggles to generalize beyond
the distribution of the provided demonstrations. A recent advancement in
enhancing robustness is ICL with explanations (X-ICL), which improves
prediction reliability by guiding LLMs to understand and articulate the
reasoning behind correct labels. Building on this approach, we introduce an
advanced framework that extends X-ICL by systematically exploring explanations
for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and
robust decision-making. Experimental results on multiple natural language
understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating
significantly improved robustness to out-of-distribution data compared to the
existing ICL approaches.

</details>


### [33] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
*Chuanghao Ding,Jiaping Wang,Ziqing Yang,Xiaoliang Wang,Dahua Lin,Cam-Tu Nguyen,Fei Tan*

Key words: 推测解码, 顾问解码, 大型语言模型, 推理加速

TL;DR: 该论文提出了一种名为顾问解码（CD）的新型解码机制，通过优化验证过程显著提升大型语言模型的推理速度，同时保持生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为了解决基于推测解码（SD）的高拒绝率问题，降低大型语言模型（LLM）的调用频率并提高整体效率。

Method: 提出顾问解码（CD），利用LLM计算的标记级似然性验证候选草案，而非依赖重要性采样的度量。

Result: CD将推理速度提高了2.5倍，生成质量接近目标模型的100%，且大型模型的调用频率降至10%以下。

Conclusion: CD不仅提高了效率，还在某些任务中超越了目标模型的性能，为推测解码提供了新的优化方向。

Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.

</details>


### [34] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
*Yilin Xiao,Junnan Dong,Chuang Zhou,Su Dong,Qianwen Zhang,Di Yin,Xing Sun,Xiao Huang*

Key words: GraphRAG, benchmark, complex reasoning, domain-specific, multi-hop reasoning

TL;DR: GraphRAG-Bench是一个新的大规模领域特定基准测试，旨在全面评估GraphRAG模型在复杂推理任务中的表现，填补了当前评测的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前GraphRAG模型的评测主要依赖传统问答数据集，无法全面评估其推理能力的提升，因此需要更全面的评测标准。

Method: 设计了GraphRAG-Bench，包含挑战性问题、多样化任务和全面评估框架，涵盖16个学科的核心教科书内容。

Result: 通过应用九种当前GraphRAG方法，验证了GraphRAG-Bench的有效性，并揭示了关于图结构、检索效率和推理能力的关键见解。

Conclusion: GraphRAG-Bench为研究社区提供了量化GraphRAG模型推理能力的工具，并指出了未来改进的方向。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [35] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
*Zhengyuan Liu,Geyu Lin,Hui Li Tan,Huayun Zhang,Yanfeng Lu,Xiaoxue Gao,Stella Xin Yin,He Sun,Hock Huan Goh,Lung Hsiang Wong,Nancy F. Chen*

Key words: 生成式AI, 语言学习, 对话式辅导, 多语言, 儿童教育

TL;DR: 论文介绍了SingaKids，一种通过图片描述任务促进语言学习的对话式辅导系统，整合了多种技术并在多语言环境中验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 生成式AI在教育中的应用可提升个性化学习体验，但需解决跨语言、文化一致性和儿童友好设计等挑战。

Method: 系统整合密集图像标注、多语言对话交互、语音理解与生成，并通过多语言预训练和任务优化提升性能。

Result: 实证研究表明，SingaKids能有效支持不同水平学习者的语言学习。

Conclusion: SingaKids在多语言环境下展现出潜力，但仍需进一步优化。

Abstract: The integration of generative artificial intelligence into educational
applications has enhanced personalized and interactive learning experiences,
and it shows strong potential to promote young learners language acquisition.
However, it is still challenging to ensure consistent and robust performance
across different languages and cultural contexts, and kids-friendly design
requires simplified instructions, engaging interactions, and age-appropriate
scaffolding to maintain motivation and optimize learning outcomes. In this
work, we introduce SingaKids, a dialogic tutor designed to facilitate language
learning through picture description tasks. Our system integrates dense image
captioning, multilingual dialogic interaction, speech understanding, and
engaging speech generation to create an immersive learning environment in four
languages: English, Mandarin, Malay, and Tamil. We further improve the system
through multilingual pre-training, task-specific tuning, and scaffolding
optimization. Empirical studies with elementary school students demonstrate
that SingaKids provides effective dialogic teaching, benefiting learners at
different performance levels.

</details>


### [36] [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)
*Tairan Liu*

Key words: 性别不平等、英语教材、自然语言处理、跨文化研究

TL;DR: 研究通过NLP方法量化22个国家英语教材中的性别不平等，发现男性角色在数量、提及顺序和命名实体上普遍被过度代表。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨跨文化背景下英语教材中的性别不平等问题。

Method: 使用自然语言处理技术分析教材文本，包括角色统计、提及顺序、TF-IDF词关联等指标。

Result: 男性角色在所有指标上均占优，拉丁文化圈的不平等程度最低。

Conclusion: 全球英语教材普遍存在性别不平等，需进一步改进。

Abstract: Textbooks play a critical role in shaping children's understanding of the
world. While previous studies have identified gender inequality in individual
countries' textbooks, few have examined the issue cross-culturally. This study
applies natural language processing methods to quantify gender inequality in
English textbooks from 22 countries across 7 cultural spheres. Metrics include
character count, firstness (which gender is mentioned first), and TF-IDF word
associations by gender. The analysis also identifies gender patterns in proper
names appearing in TF-IDF word lists, tests whether large language models can
distinguish between gendered word lists, and uses GloVe embeddings to examine
how closely keywords associate with each gender. Results show consistent
overrepresentation of male characters in terms of count, firstness, and named
entities. All regions exhibit gender inequality, with the Latin cultural sphere
showing the least disparity.

</details>


### [37] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
*Maryam Berijanian,Kuldeep Singh,Amin Sehati*

Key words: 关系分类, 大型语言模型, 多智能体, 动态示例生成

TL;DR: 该论文比较了三种AI代理架构在关系分类任务中的表现，发现多智能体协作优于标准少样本提示，接近微调模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决信息抽取中有限标注数据和复杂关系结构带来的挑战。

Method: 比较三种架构：反射性自评估、层次任务分解和动态多智能体示例生成。

Result: 多智能体协调显著优于标准提示方法，接近微调模型。

Conclusion: 为模块化、可泛化的LLM系统设计提供指导。

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at
\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [38] [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)
*Mahammed Kamruzzaman,Abdullah Al Monsur,Gene Louis Kim,Anshuman Chhabra*

Key words: 大型语言模型、情感刻板印象、国籍差异、文化规范

TL;DR: 研究了大型语言模型（LLMs）在扮演国家特定角色时是否表现出情感刻板印象，发现LLMs中存在基于国籍的情感偏见，且与人类情感反应不一致。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在情感表达中是否存在国籍刻板印象，及其与文化规范的匹配程度。

Method: 通过分析预训练LLMs对不同国家角色的情感分配，并与人类情感反应对比。

Result: 发现LLMs中存在显著的国家情感偏见，尤其是负面情感，与人类情感反应不匹配。

Conclusion: LLMs在情感表达上存在简化和偏见，需改进以避免刻板印象。

Abstract: Emotions are a fundamental facet of human experience, varying across
individuals, cultural contexts, and nationalities. Given the recent success of
Large Language Models (LLMs) as role-playing agents, we examine whether LLMs
exhibit emotional stereotypes when assigned nationality-specific personas.
Specifically, we investigate how different countries are represented in
pre-trained LLMs through emotion attributions and whether these attributions
align with cultural norms. Our analysis reveals significant nationality-based
differences, with emotions such as shame, fear, and joy being
disproportionately assigned across regions. Furthermore, we observe notable
misalignment between LLM-generated and human emotional responses, particularly
for negative emotions, highlighting the presence of reductive and potentially
biased stereotypes in LLM outputs.

</details>


### [39] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)
*Utsav Maskey,Mark Dras,Usman Naseem*

Key words: 大语言模型,安全性,长尾分布,加密文本,不匹配泛化攻击

TL;DR: 该论文系统评估了大语言模型在长尾分布（加密）文本中的行为及其安全性影响，提出了一个二维安全框架，并通过实验揭示模型在解密密码时可能受到不匹配泛化攻击。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究大语言模型在长尾分布和加密文本中的安全性，揭示其安全机制的潜在漏洞。

Method: 引入二维安全评估框架，包括指令拒绝和生成安全性，通过实验测试模型在解密密码时的表现。

Result: 发现模型在解密时可能出现安全机制失效或不必要的过度拒绝。

Conclusion: 论文为理解长尾文本场景下的LLM安全性提供了见解，并提出了开发更鲁棒安全机制的方向。

Abstract: This paper presents a systematic evaluation of Large Language Models' (LLMs)
behavior on long-tail distributed (encrypted) texts and their safety
implications. We introduce a two-dimensional framework for assessing LLM
safety: (1) instruction refusal-the ability to reject harmful obfuscated
instructions, and (2) generation safety-the suppression of generating harmful
responses. Through comprehensive experiments, we demonstrate that models that
possess capabilities to decrypt ciphers may be susceptible to
mismatched-generalization attacks: their safety mechanisms fail on at least one
safety dimension, leading to unsafe responses or over-refusal. Based on these
findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss
their strengths and limitations. This work contributes to understanding the
safety of LLM in long-tail text scenarios and provides directions for
developing robust safety mechanisms.

</details>


### [40] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)
*Bo Peng,Zhiheng Wang,Heyang Gong,Chaochao Lu*

Key words: 对话系统,个性化,数据集,隐式推理,评估框架

TL;DR: 提出了一种自动生成合成数据的方法，并引入了IP-Dialog基准数据集，旨在解决对话系统中用户背景隐式推断的数据稀缺问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现代对话系统需要从对话中隐式推断用户背景以提供个性化服务，但高质量数据的稀缺是主要挑战。

Method: 提出了自动合成数据生成方法，并构建了IP-Dialog基准数据集，涵盖10个任务和12种用户属性类型。

Result: 开发了系统性评估框架，实验证明数据集的可靠性。

Conclusion: 该研究为对话系统的个性化能力评估和改进提供了有效工具。

Abstract: In modern dialogue systems, the ability to implicitly infer user backgrounds
from conversations and leverage this information for personalized assistance is
crucial. However, the scarcity of high-quality data remains a fundamental
challenge to evaluating and improving this capability. Traditional dataset
construction methods are labor-intensive, resource-demanding, and raise privacy
concerns. To address these issues, we propose a novel approach for automatic
synthetic data generation and introduce the Implicit Personalized Dialogue
(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12
user attribute types. Additionally, we develop a systematic evaluation
framework with four metrics to assess both attribute awareness and reasoning
capabilities. We further propose five causal graphs to elucidate models'
reasoning pathways during implicit personalization. Extensive experiments yield
insightful observations and prove the reliability of our dataset.

</details>


### [41] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
*Zhaorui Yang,Bo Pan,Han Wang,Yiyao Wang,Xingyu Liu,Minfeng Zhu,Bo Zhang,Wei Chen*

Key words: 可视化, 多模态报告, FDV, 大型语言模型

TL;DR: 该论文提出了一种新任务，即自动化生成结合文本和可视化的多模态报告，并提出了FDV表示法和Multimodal DeepResearcher框架来解决这一挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有深度学习框架主要生成纯文本内容，而结合文本和可视化的自动化生成仍未被充分探索，这限制了信息传达的效果。

Method: 论文提出了FDV（结构化文本表示法）和Multimodal DeepResearcher框架，将任务分解为研究、文本化示例报告、规划和多模态报告生成四个阶段。

Result: 实验表明，Multimodal DeepResearcher在Claude 3.7 Sonnet模型上的整体胜率为82%，优于基线方法。

Conclusion: 该框架成功解决了多模态报告生成的挑战，显著提升了信息传达的效果。

Abstract: Visualizations play a crucial part in effective communication of concepts and
information. Recent advances in reasoning and retrieval augmented generation
have enabled Large Language Models (LLMs) to perform deep research and generate
comprehensive reports. Despite its progress, existing deep research frameworks
primarily focus on generating text-only content, leaving the automated
generation of interleaved texts and visualizations underexplored. This novel
task poses key challenges in designing informative visualizations and
effectively integrating them with text reports. To address these challenges, we
propose Formal Description of Visualization (FDV), a structured textual
representation of charts that enables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this representation, we introduce
Multimodal DeepResearcher, an agentic framework that decomposes the task into
four stages: (1) researching, (2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the evaluation of generated
multimodal reports, we develop MultimodalReportBench, which contains 100
diverse topics served as inputs along with 5 dedicated metrics. Extensive
experiments across models and evaluation methods demonstrate the effectiveness
of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet
model, Multimodal DeepResearcher achieves an 82\% overall win rate over the
baseline method.

</details>


### [42] [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)
*Yupeng Qi,Ziyu Lyu,Min Yang,Yanlin Wang,Lu Bai,Lixin Cui*

Key words: 大型语言模型, 安全性, 实用性, 混合专家框架, 动态路由

TL;DR: 为了解决大型语言模型（LLMs）在安全性和实用性之间的平衡问题，本文提出了一种混合专家框架MidPO，通过优化单一偏好和动态路由机制，显著提升了模型的安全性和实用性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前的安全约束在线偏好优化方法可能导致过度安全，而离线方法在自适应平衡安全性和实用性方面表现不佳。因此，需要一种新的方法来解决这些问题。

Method: MidPO采用混合专家（MoE）框架，首先通过单一偏好增强的直接偏好优化方法训练出独立的‘安全专家’和‘实用专家’，然后通过动态路由机制自适应地平衡两者的贡献。

Result: 实验结果表明，MidPO在三组流行数据集上的表现显著优于现有方法，同时兼顾了安全性和实用性。

Conclusion: MidPO通过混合专家框架和动态路由机制，有效解决了LLMs在安全性和实用性之间的平衡问题，是一种高效且灵活的方法。

Abstract: As large language models (LLMs) are increasingly applied across various
domains, enhancing safety while maintaining the helpfulness of LLMs has become
a critical challenge. Recent studies solve this problem through
safety-constrained online preference optimization or safety-constrained offline
preference optimization. However, the safety-constrained online methods often
suffer from excessive safety, which might reduce helpfulness, while the
safety-constrained offline methods perform poorly in adaptively balancing
safety and helpfulness. To address these limitations, we propose MidPO, a
\textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness
\textbf{\underline{d}}ual \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference
enhanced direct preference optimization approach to transform the base model
into two independent experts, termed safety and helpfulness experts, and
fine-tunes the two independent experts for optimal safety or helpfulness
performance. Secondly, to achieve an effective balance between safety and
helpfulness, MidPO incorporates the two experts into the MoE framework and
designs a dynamic routing mechanism to allocate contributions from each expert
adaptively. We conduct quantitative and qualitative experiments on three
popular datasets to demonstrate the proposed MidPO significantly outperforms
state-of-the-art approaches in both safety and helpfulness. The code and models
will be released.

</details>


### [43] [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)
*Chunkit Chan,Yauwai Yim,Hongchuan Zeng,Zhiying Zou,Xinyuan Cheng,Zhifan Sun,Zheye Deng,Kawai Chung,Yuzhuo Ao,Yixiang Fan,Cheng Jiayang,Ercong Nie,Ginny Y. Wong,Helmut Schmid,Hinrich Schütze,Simon See,Yangqiu Song*

Key words: Theory of Mind (ToM), 多语言, 心理理论, 大型语言模型 (LLMs), XToM

TL;DR: 论文探讨了大型语言模型在跨语言环境中的心理理论能力，提出了多语言基准测试XToM，并揭示了模型在多语言理解与心理理论表现之间的不一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有对大型语言模型（LLMs）心理理论（ToM）的评估主要限于英语，忽视了语言多样性对人类认知的影响，研究旨在填补这一空白。

Method: 研究提出了XToM，一个经过严格验证的多语言基准测试，涵盖五种语言，并包含多样化和情境丰富的任务场景。

Result: 研究发现，尽管LLMs在多语言理解上表现优秀，但在不同语言中的ToM表现存在显著差异，未能完全复制人类跨语言的心理推理能力。

Conclusion: 研究揭示了LLMs在跨语言环境中模拟人类心理理论能力的局限性，强调了未来研究需关注语言多样性对认知能力的影响。

Abstract: Theory of Mind (ToM), the ability to infer mental states in others, is
pivotal for human social cognition. Existing evaluations of ToM in LLMs are
largely limited to English, neglecting the linguistic diversity that shapes
human cognition. This limitation raises a critical question: can LLMs exhibit
Multilingual Theory of Mind, which is the capacity to reason about mental
states across diverse linguistic contexts? To address this gap, we present
XToM, a rigorously validated multilingual benchmark that evaluates ToM across
five languages and incorporates diverse, contextually rich task scenarios.
Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a
pronounced dissonance: while models excel in multilingual language
understanding, their ToM performance varies across languages. Our findings
expose limitations in LLMs' ability to replicate human-like mentalizing across
linguistic contexts.

</details>


### [44] [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
*Zijian Li,Xiaocheng Feng,Huixin Liu,Yichong Huang,Ting Liu,Bing Qin*

Key words: 大语言模型, 模型融合, 参数高效微调, Frobenius范数, 任务干扰

TL;DR: 提出了一种名为FroM的自适应融合方法，通过直接使用Frobenius范数衡量模型参数，无需训练数据，解决了传统模型融合方法中的任务干扰问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大语言模型的发展，微调成为增强特定场景性能的有效方法，但传统方法在融合全微调模型时存在任务干扰问题，尤其是在参数高效微调场景中更为明显。需要一种无需训练数据的融合方法。

Method: 改进RegMean方法，提出FroM方法，直接使用Frobenius范数衡量模型参数，并通过额外超参数控制融合过程。

Result: FroM在多种微调场景中优于基线方法，有效缓解了任务干扰问题。

Conclusion: FroM提供了一种高效且无数据依赖的模型融合解决方案。

Abstract: With the development of large language models, fine-tuning has emerged as an
effective method to enhance performance in specific scenarios by injecting
domain-specific knowledge. In this context, model merging techniques provide a
solution for fusing knowledge from multiple fine-tuning models by combining
their parameters. However, traditional methods often encounter task
interference when merging full fine-tuning models, and this problem becomes
even more evident in parameter-efficient fine-tuning scenarios. In this paper,
we introduce an improvement to the RegMean method, which indirectly leverages
the training data to approximate the outputs of the linear layers before and
after merging. We propose an adaptive merging method called FroM, which
directly measures the model parameters using the Frobenius norm, without any
training data. By introducing an additional hyperparameter for control, FroM
outperforms baseline methods across various fine-tuning scenarios, alleviating
the task interference problem.

</details>


### [45] [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)
*Yifan Duan,Yihong Tang,Kehai Chen,Liqiang Nie,Min Zhang*

Key words: 提示优化,角色扮演,语言模型,即插即用

TL;DR: ORPP是一种通过角色扮演场景优化提示的框架，显著提升语言模型性能，且具有“即插即用”特性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有提示优化方法通常计算开销大或依赖模型优化能力，限制了广泛应用。ORPP通过角色扮演提示优化解决这些问题。

Method: ORPP通过迭代优化小规模训练样本生成高质量角色扮演提示，并利用少量样本学习能力推广到其他样本。

Result: 实验表明ORPP性能优于主流提示优化方法，且能与其他方法结合进一步提升效果。

Conclusion: ORPP通过角色扮演提示优化有效激活模型内在能力，适用于广泛任务。

Abstract: High-quality prompts are crucial for eliciting outstanding performance from
large language models (LLMs) on complex tasks. Existing research has explored
model-driven strategies for prompt optimization. However, these methods often
suffer from high computational overhead or require strong optimization
capabilities from the model itself, which limits their broad applicability.To
address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a
framework that enhances model performance by optimizing and generating
role-playing prompts. The core idea of ORPP is to confine the prompt search
space to role-playing scenarios, thereby fully activating the model's intrinsic
capabilities through carefully crafted, high-quality role-playing prompts.
Specifically, ORPP first performs iterative optimization on a small subset of
training samples to generate high-quality role-playing prompts. Then,
leveraging the model's few-shot learning capability, it transfers the
optimization experience to efficiently generate suitable prompts for the
remaining samples.Our experimental results show that ORPP not only matches but
in most cases surpasses existing mainstream prompt optimization methods in
terms of performance. Notably, ORPP demonstrates superior "plug-and-play"
capability. In most cases, it can be integrated with various other prompt
methods and further enhance their effectiveness.

</details>


### [46] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
*Inderjeet Nair,Lu Wang*

Key words: LLM, 价值偏好, 短形式测试, 长形式回答, 一致性

TL;DR: 研究发现，LLM的价值偏好从短形式测试与长形式回答中推断出的相关性较弱，且长形式生成设置之间的一致性也较低，需要更强的方法确保一致的价值表达。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLM在短形式和长形式回答中价值偏好的一致性，以评估其在现实应用中的伦理风险和价值倾向。

Method: 比较五种LLM（llama3-8b等）在短形式反应和长形式回答中的价值偏好，分析不同生成设置的相关性。

Result: 短形式与长形式回答间价值偏好的相关性弱，长形式设置间一致性低，且一致性改善有限。

Conclusion: 需要更稳健的方法确保LLM在不同应用中价值表达的一致性。

Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on
short-form surveys and psychometric tests, yet real-world use involves
long-form, open-ended responses -- leaving value-related risks and preferences
in practical settings largely underexplored. In this work, we ask: Do value
preferences inferred from short-form tests align with those expressed in
long-form outputs? To address this question, we compare value preferences
elicited from short-form reactions and long-form responses, varying the number
of arguments in the latter to capture users' differing verbosity preferences.
Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),
we find (1) a weak correlation between value preferences inferred from
short-form and long-form responses across varying argument counts, and (2)
similarly weak correlation between preferences derived from any two distinct
long-form generation settings. (3) Alignment yields only modest gains in the
consistency of value expression. Further, we examine how long-form generation
attributes relate to value preferences, finding that argument specificity
negatively correlates with preference strength, while representation across
scenarios shows a positive correlation. Our findings underscore the need for
more robust methods to ensure consistent value expression across diverse
applications.

</details>


### [47] [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)
*Sina Bagheri Nezhad,Ameeta Agrawal*

Key words: NSAR、符号推理、多目标推理、长上下文

TL;DR: NSAR结合神经与符号推理，显著提升多目标推理能力在长上下文场景中。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大语言模型在长上下文中多目标推理的困难，提出结合神经与符号推理的方法。

Method: NSAR提取符号事实并生成Python代码处理复杂推理步骤。

Result: 在七种语言和不同上下文长度中，NSAR显著优于基线方法。

Conclusion: 神经与符号推理结合在多语言环境中高效、可解释且可扩展。

Abstract: Large language models (LLMs) often struggle to perform multi-target reasoning
in long-context scenarios where relevant information is scattered across
extensive documents. To address this challenge, we introduce NeuroSymbolic
Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic
reasoning during inference. NSAR explicitly extracts symbolic facts from text
and generates executable Python code to handle complex reasoning steps. Through
extensive experiments across seven languages and diverse context lengths, we
demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and
advanced prompting strategies in accurately identifying and synthesizing
multiple pieces of information. Our results highlight the effectiveness of
combining explicit symbolic operations with neural inference for robust,
interpretable, and scalable reasoning in multilingual settings.

</details>


### [48] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
*Junzhe Zhang,Huixuan Zhang,Xinyu Hu,Li Lin,Mingqi Gao,Shi Qiu,Xiaojun Wan*

Key words: 多模态评估,T2I生成,人类评估数据,Minos

TL;DR: 论文提出了Minos-Corpus，一个结合人类和GPT评估数据的多模态数据集，并基于此开发了Minos模型，在T2I生成任务评估上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多模态生成任务评估的重要性日益突出，但现有工作忽视了T2I生成任务的评估能力和大规模人类评估数据的整合。

Method: 构建Minos-Corpus数据集，结合人类和GPT的评估数据；提出数据选择和平衡、Mix-SFT训练方法，应用DPO开发Minos模型。

Result: Minos在类似规模的开源评估模型中达到最佳平均性能，在T2I生成任务评估上优于所有开源和闭源模型。

Conclusion: 高质量人类评估数据和联合训练对提升评估性能至关重要。

Abstract: Evaluation is important for multimodal generation tasks. With the rapid
progress of MLLMs, there is growing interest in applying MLLMs to build general
evaluation systems. However, existing work overlooks two aspects: (1) the
development of evaluation capabilities for text-to-image (T2I) generation task,
and (2) the incorporation of large-scale human evaluation data. In this paper,
we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that
combines evaluation data from both human and GPT. The corpus contains
evaluation data across both image-to-text(I2T) and T2I generation tasks. Based
on this corpus, we propose Data Selection and Balance, Mix-SFT training
methods, and apply DPO to develop Minos, a multimodal evaluation model built
upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among
all open-source evaluation models of similar scale on the average of evaluation
performance on all tasks, and outperforms all open-source and closed-source
models on evaluation of T2I generation task. Extensive experiments demonstrate
the importance of leveraging high-quality human evaluation data and jointly
training on evaluation data from both I2T and T2I generation tasks.

</details>


### [49] [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)
*Yongjian Li,HaoCheng Chu,Yukun Yan,Zhenghao Liu,Shi Yu,Zheni Zeng,Ruobing Wang,Sen Song,Zhiyuan Liu,Maosong Sun*

Key words: RAG, KARE-RAG, DDPO, 知识表示, 对比数据生成

TL;DR: KARE-RAG通过结构化知识表示、优化的训练目标和对比数据生成，显著提升了RAG的性能，同时保持数据效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决RAG中噪声文档导致的事实不一致问题，提升知识利用效率。

Method: 提出KARE-RAG，包括结构化知识表示、DDPO训练目标和对比数据生成。

Result: 实验显示KARE-RAG显著提升RAG性能，适用于不同规模和任务。

Conclusion: 通过优化模型对检索内容的学习方式，可以提升RAG的广泛性能。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access broader knowledge sources, yet factual inconsistencies persist due to
noise in retrieved documents-even with advanced retrieval methods. We
demonstrate that enhancing generative models' capacity to process noisy content
is equally critical for robust performance. In this paper, we present KARE-RAG
(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge
utilization through three key innovations: (1) structured knowledge
representations that facilitate error detection during training, (2) Dense
Direct Preference Optimization (DDPO)-a refined training objective that
prioritizes correction of critical errors, and (3) a contrastive data
generation pipeline that maintains semantic consistency while rectifying
factual inaccuracies. Experiments show our method significantly enhances
standard RAG pipelines across model scales, improving both in-domain and
out-of-domain task performance without compromising general capabilities.
Notably, these gains are achieved with modest training data, suggesting
data-efficient optimization is possible through targeted learning strategies.
Our findings establish a new direction for RAG improvement: by improving how
models learn to process retrieved content, we can enhance performance across
diverse inference paradigms. All data and code will be publicly available on
Github.

</details>


### [50] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
*Jie Zhu,Junhui Li,Yalong Wen,Xiandong Li,Lifan Guo,Feng Chen*

Key words: 大语言模型，金融会议，多语言，多任务，基准测试

TL;DR: 论文提出了一种新的金融会议理解基准$	exttt{M$^3$FinMeeting}$，用于评估大语言模型在多语言、多行业和多任务场景下的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前金融领域的基准测试主要依赖新闻文章或财报，难以反映真实金融会议的动态特性。$	exttt{M$^3$FinMeeting}$填补了这一空白。

Method: 构建了一个支持中英日三种语言、涵盖多个行业（基于GICS）的数据集，并设计了摘要生成、问答对提取和问题回答三个任务。

Result: 实验表明，即使是先进的上下文模型，在金融会议理解任务中仍有显著改进空间。

Conclusion: $	exttt{M$^3$FinMeeting}$是一个有效且全面的金融会议理解基准，有助于提升大语言模型的实际表现。

Abstract: Recent breakthroughs in large language models (LLMs) have led to the
development of new benchmarks for evaluating their performance in the financial
domain. However, current financial benchmarks often rely on news articles,
earnings reports, or announcements, making it challenging to capture the
real-world dynamics of financial meetings. To address this gap, we propose a
novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual,
multi-sector, and multi-task dataset designed for financial meeting
understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and
Japanese, enhancing comprehension of financial discussions in diverse
linguistic contexts. Second, it encompasses various industry sectors defined by
the Global Industry Classification Standard (GICS), ensuring that the benchmark
spans a broad range of financial activities. Finally,
$\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer
(QA) pair extraction, and question answering, facilitating a more realistic and
comprehensive evaluation of understanding. Experimental results with seven
popular LLMs reveal that even the most advanced long-context models have
significant room for improvement, demonstrating the effectiveness of
$\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting
comprehension skills.

</details>


### [51] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
*Zhuohan Xie,Dhruv Sahnan,Debopriyo Banerjee,Georgi Georgiev,Rushil Thareja,Hachem Madmoun,Jinyan Su,Aaryamonvikram Singh,Yuxia Wang,Rui Xing,Fajri Koto,Haonan Li,Ivan Koychev,Tanmoy Chakraborty,Salem Lahlou,Veselin Stoyanov,Preslav Nakov*

Key words: 金融推理, Chain-of-Thought, 基准测试, FinChain, ChainEval

TL;DR: FinChain是第一个用于可验证链式思维（CoT）金融推理的符号基准测试，覆盖12个金融领域的54个主题。它提供自动生成的Python执行轨迹和新的评估指标ChainEval，用于评估最终答案和中间推理步骤。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有金融推理数据集（如FinQA和ConvFinQA）仅监督最终数值答案，缺乏对中间推理步骤的评估，因此需要一种新的基准测试方法。

Method: FinChain通过五个参数化模板生成多样化的推理复杂度的实例，包括可执行的Python轨迹。同时提出ChainEval作为自动评估指标。

Result: 测试30个大型语言模型发现，即使是当前最先进的模型在多步金融推理上仍有较大改进空间。

Conclusion: FinChain填补了金融推理评估的空白，为未来研究提供了重要工具和数据支持。

Abstract: Multi-step symbolic reasoning is critical for advancing downstream
performance on financial tasks. Yet, benchmarks for systematically evaluating
this capability are lacking. Existing datasets like FinQA and ConvFinQA
supervise only final numerical answers, without assessing intermediate
reasoning steps. To address this, we introduce FinChain, the first symbolic
benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.
Spanning 54 topics across 12 financial domains, Fin- Chain offers five
parameterized templates per topic, each varying in reasoning complexity and
domain expertise required. Each dataset instance includes an executable Python
trace, enabling automatic generation of extensive training data and easy
adaptation to other domains. We also introduce ChainEval, a new metric for
automatic evaluation of both final answers and intermediate reasoning.
Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models
have considerable room for improvement in multi-step financial reasoning. All
templates and evaluation metrics for FinChain are available at https:
//github.com/mbzuai-nlp/finchain.

</details>


### [52] [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)
*Sohan Patnaik,Milan Aggarwal,Sumit Bhatia,Balaji Krishnamurthy*

Key words: 小型LLM, 推理能力, 多样化理性, 偏好优化, COLLATE

TL;DR: 论文提出COLLATE框架，通过训练小型LLM从多样化理性输出中选择性地优化下游任务，而不依赖大型LLM的知识蒸馏。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有工作多利用大型LLM（如GPT-4）的推理能力改进小型模型，但因版权和法律问题难以商用。本文探索如何不依赖大型模型，直接提升小型模型的推理能力。

Method: COLLATE通过训练小型LLM生成多样化理性输出，并优化其选择最佳理性以提高任务性能，采用偏好优化提升模型选择能力。

Result: 在数学问题求解、自然语言推理和常识推理等3个领域的5个数据集上，COLLATE优于多种基准方法，且适用于1B到8B参数规模的模型。

Conclusion: COLLATE证明了通过多样化理性提供者和任务导向优化，小型LLM的推理能力可显著提升，同时避免依赖大型模型。

Abstract: LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions
by generating step-by-step rationales. Prior works have utilized this
capability to improve smaller and cheaper LMs (say, with 7B parameters).
However, various practical constraints, such as copyright and legal issues,
owing to lack of transparency in the pre-training data of large (often closed)
models, prevent their use in commercial settings. Little focus has been given
to improving the innate reasoning ability of smaller models without distilling
information from larger LLMs. To address this, we propose COLLATE, a trainable
framework that tunes a (small) LLM to generate those outputs from a pool of
diverse rationales that selectively improves the downstream task. COLLATE
enforces multiple instances of the same LLM to exhibit distinct behavior and
employs them to generate rationales to obtain diverse outputs. The LLM is then
tuned via preference optimization to choose the candidate rationale which
maximizes the likelihood of ground-truth answer. COLLATE outperforms several
trainable and prompting baselines on 5 datasets across 3 domains: maths problem
solving, natural language inference, and commonsense reasoning. We show the eff
icacy of COLLATE on LLMs from different model families across varying parameter
scales (1B to 8B) and demonstrate the benefit of multiple rationale providers
guided by the end task through ablations. Code is released here
(https://github.com/Sohanpatnaik106/collate).

</details>


### [53] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
*Yingying Zhuang,Aman Gupta,Anurag Beniwal*

Key words: 多语言信息检索, 对比学习, 加权采样, 跨语言知识共享

TL;DR: 提出了一种基于加权采样的对比学习方法，用于微调多语言嵌入模型，以提高多语言信息检索的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决高质量知识库资源稀缺且语言有限的问题，实现跨语言知识共享。

Method: 采用加权采样策略，进行对比学习以微调多语言嵌入模型。

Result: 相比标准方法，在MRR和Recall@3上分别提升31.03%和33.98%。

Conclusion: 该方法语言无关，适用于多语言和代码混合场景。

Abstract: Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.

</details>


### [54] [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)
*Jinu Lee,Sagnik Mukherjee,Dilek Hakkani-Tur,Julia Hockenmaier*

Key words: 大型推理模型, ReasoningFlow, 有向无环图, 推理模式, 语义分析

TL;DR: 提出ReasoningFlow框架，解析大型推理模型的复杂推理轨迹为有向无环图，支持对其语义结构的分析和模式识别。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 因大型推理模型（LRMs）生成的推理轨迹复杂且多样，需一种统一方法解析和表征这些轨迹的语义结构。

Method: 开发ReasoningFlow，将推理轨迹解析为有向无环图（DAG），通过子图结构识别不同推理模式。

Result: ReasoningFlow提供了人类可理解的表示方法，支持对LRMs推理过程的理解、评估和改进。

Conclusion: ReasoningFlow为分析复杂推理轨迹提供了一种有效工具，具有实际应用潜力。

Abstract: Large reasoning models (LRMs) generate complex reasoning traces with
planning, reflection, verification, and backtracking. In this work, we
introduce ReasoningFlow, a unified schema for analyzing the semantic structures
of these complex traces. ReasoningFlow parses traces into directed acyclic
graphs, enabling the characterization of distinct reasoning patterns as
subgraph structures. This human-interpretable representation offers promising
applications in understanding, evaluating, and enhancing the reasoning
processes of LRMs.

</details>


### [55] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)
*Maike Behrendt,Stefan Sylvius Wagner,Carina Weinmann,Marike Bormann,Mira Warne,Stefan Harmeling*

Key words: 政治在线参与, 讨论质量, 机器学习, 深思熟虑

TL;DR: 文章讨论了政治在线讨论中的问题，并探索了如何利用机器学习提升讨论质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着政治讨论日益数字化，高质量的讨论（即深思熟虑的交流）对决策至关重要。平台设计影响讨论质量，机器学习有潜力改善这一过程。

Method: 提出了如何利用机器学习方法解决政治在线讨论中的问题，并促进深思熟虑的交流。

Result: 展示了机器学习在提升政治在线讨论质量方面的潜在应用。

Conclusion: 机器学习可以有效改善政治在线讨论中的问题，提升讨论质量。

Abstract: Political online participation in the form of discussing political issues and
exchanging opinions among citizens is gaining importance with more and more
formats being held digitally. To come to a decision, a careful discussion and
consideration of opinions and a civil exchange of arguments, which is defined
as the act of deliberation, is desirable. The quality of discussions and
participation processes in terms of their deliberativeness highly depends on
the design of platforms and processes. To facilitate online communication for
both participants and initiators, machine learning methods offer a lot of
potential. In this work we want to showcase which issues occur in political
online discussions and how machine learning can be used to counteract these
issues and enhance deliberation.

</details>


### [56] [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
*Xin Liu,Lu Wang*

Key words: 大型语言模型, 链式推理, 推理效率, 推断时优化

TL;DR: 本文研究了大型语言模型（LLM）中推理步骤的冗余性，并提出三种推断时策略以提高效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 链式推理（CoT）提示虽然增强了LLM的推理能力，但常导致冗余输出，增加了推断成本。

Method: 通过系统性研究确定最小必要推理步骤，并提出了三种推断时策略：早期停止、增强结束信号概率及基于内部激活的监督方法。

Result: 实验表明，这些方法能显著减少标记使用且几乎不影响准确性，尤其在NaturalQuestions上实现超过40%的标记减少。

Conclusion: 研究强调了推断时高效推理方法的重要性，为实际应用提供了实用价值。

Abstract: Chain-of-thought (CoT) prompting enhances reasoning in large language models
(LLMs) but often leads to verbose and redundant outputs, thus increasing
inference cost. We hypothesize that many reasoning steps are unnecessary for
producing correct answers. To investigate this, we start with a systematic
study to examine what is the minimum reasoning required for a model to reach a
stable decision. We find that on math reasoning tasks like math, models
typically converge to their final answers after 60\% of the reasoning steps,
suggesting substantial redundancy in the remaining content. Based on these
insights, we propose three inference-time strategies to improve efficiency: (1)
early stopping via answer consistency, (2) boosting the probability of
generating end-of-reasoning signals, and (3) a supervised method that learns
when to stop based on internal activations. Experiments across five benchmarks
and five open-weights LLMs show that our methods significantly reduce token
usage with little or no accuracy drop. In particular, on NaturalQuestions,
Answer Consistency reduces tokens by over 40\% while further improving
accuracy. Our work underscores the importance of cost-effective reasoning
methods that operate at inference time, offering practical benefits for
real-world applications.

</details>


### [57] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Key words: 多模态检索增强生成, 知识不一致, CoRe-MMRAG, KB-VQA

TL;DR: 论文提出了CoRe-MMRAG框架，通过多源知识调和解决多模态检索增强生成中的知识不一致问题，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多模态检索增强生成（MMRAG）中存在的参数化与检索知识不一致（PRKI）以及视觉与文本知识不一致（VTKI）问题，影响了模型的可靠性。

Method: 提出CoRe-MMRAG框架，采用四阶段流程：生成内部响应、联合相似性评估选择多模态证据、生成外部响应、整合知识生成最终答案，并通过专门训练范式增强多源知识融合能力。

Result: 在KB-VQA基准测试中，CoRe-MMRAG在InfoSeek和Encyclopedic-VQA上分别实现了5.6%和9.3%的性能提升。

Conclusion: CoRe-MMRAG能有效调和多源知识不一致问题，提升多模态模型的可靠性。

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge
\textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG}
(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles
inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage
pipeline: it first generates an internal response from parametric knowledge,
then selects the most relevant multimodal evidence via joint similarity
assessment, generates an external response, and finally integrates both to
produce a reliable answer. Additionally, a specialized training paradigm
enhances knowledge source discrimination, multimodal integration, and unified
answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG
achieves substantial improvements over baseline methods, achieving 5.6\% and
9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We
release code and data at
\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [58] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
*Yirao Zhao,Guizhen Chen,Kenji Kawaguchi,Lidong Bing,Wenxuan Zhang*

Key words: 大型语言模型, 剪枝, 专家模型, 自定义剪枝

TL;DR: 提出了一种名为Cus-Prun的自定义剪枝方法，可将大型通用模型剪枝为小型专家模型，无需后训练，且性能优于其他方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型计算资源需求高，现有剪枝方法无法在保持模型通用能力的同时满足特定下游场景需求。

Method: Cus-Prun通过在语言、领域和任务三个维度上识别并剪枝无关神经元，生成轻量级专家模型。

Result: 实验表明Cus-Prun在多种模型上均表现优异，专家能力和通用能力损失最小。

Conclusion: Cus-Prun是一种高效的剪枝方法，适合生成特定场景的专家模型。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing pruning methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained pruning. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.

</details>


### [59] [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)
*Muhammad Falensi Azmi,Muhammad Dehan Al Kautsar,Alfan Farizki Wicaksono,Fajri Koto*

Key words: 大型语言模型, 安全性评估, 印度尼西亚, 文化多样性, 本地语言

TL;DR: 该论文介绍了IndoSafety，一个针对印度尼西亚文化背景的高质量、人工验证的安全评估数据集，揭示了现有区域大型语言模型在本地语言环境中的安全问题，并通过微调显著提升了安全性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管区域性大型语言模型（LLMs）日益发展，但其安全性在文化多样化的环境中（如印度尼西亚）仍未得到充分探索。印度尼西亚对本地规范的敏感性非常高，因此需要针对其文化背景的安全评估。

Method: 研究通过扩展现有的安全框架，构建了一个覆盖印尼五种语言变体的安全评估数据集IndoSafety，包括正式和非正式的印尼语及三种主要本地语言（爪哇语、巽他语和米南加保语）。

Result: 研究发现，现有的印尼语言模型在非正式和本地语言环境中常生成不安全输出。通过使用IndoSafety微调模型，显著提高了安全性，同时保持了任务性能。

Conclusion: 研究强调了基于文化的安全评估的重要性，并为多语言环境中负责任的大型语言模型部署提供了具体步骤。

Abstract: Although region-specific large language models (LLMs) are increasingly
developed, their safety remains underexplored, particularly in culturally
diverse settings like Indonesia, where sensitivity to local norms is essential
and highly valued by the community. In this work, we present IndoSafety, the
first high-quality, human-verified safety evaluation dataset tailored for the
Indonesian context, covering five language varieties: formal and colloquial
Indonesian, along with three major local languages: Javanese, Sundanese, and
Minangkabau. IndoSafety is constructed by extending prior safety frameworks to
develop a taxonomy that captures Indonesia's sociocultural context. We find
that existing Indonesian-centric LLMs often generate unsafe outputs,
particularly in colloquial and local language settings, while fine-tuning on
IndoSafety significantly improves safety while preserving task performance. Our
work highlights the critical need for culturally grounded safety evaluation and
provides a concrete step toward responsible LLM deployment in multilingual
settings. Warning: This paper contains example data that may be offensive,
harmful, or biased.

</details>


### [60] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
*Sarenne Wallbridge,Christoph Minixhofer,Catherine Lai,Peter Bell*

Key words: 韵律, 自监督学习, 语音处理, 声学特征, 情感识别

TL;DR: 研究探讨了韵律（如语调、节奏和音量）对语音中可预测结构的贡献，并通过自监督学习分析了其时间粒度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管语音中存在可预测的结构，但韵律如何独立于词汇内容影响这些结构尚不明确。

Method: 使用自监督学习（SSL）和提出的Masked Prosody Model，研究韵律的声学相关结构的时间粒度。

Result: 模型能够预测依赖局部信息的感知标签（如词边界），并尤其擅长处理涉及长期结构的标签（如情感识别）。

Conclusion: 结果强调了SSL训练目标时间尺度的重要性，以及复杂SSL编码结构相对于传统结构的优势。

Abstract: People exploit the predictability of lexical structures during text
comprehension. Though predictable structure is also present in speech, the
degree to which prosody, e.g. intonation, tempo, and loudness, contributes to
such structure independently of the lexical content is unclear. This study
leverages self-supervised learning (SSL) to examine the temporal granularity of
structures in the acoustic correlates of prosody. Representations from our
proposed Masked Prosody Model can predict perceptual labels dependent on local
information, such as word boundaries, but provide the most value for labels
involving longer-term structures, like emotion recognition. Probing experiments
across various perceptual labels show strong relative gains over untransformed
pitch, energy, and voice activity features. Our results reveal the importance
of SSL training objective timescale and highlight the value of complex
SSL-encoded structures compared to more constrained classical structures.

</details>


### [61] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
*Maria Levchenko*

Key words: Named Entity Recognition, 俄语, 文化新闻, GPT-4o, 性能评估

TL;DR: 论文通过在俄罗斯文化新闻文本中比较多种NER模型，发现GPT-4o在特定提示下表现最佳，F1分数达0.93，GPT-4精度最高为0.99。研究还展示了模型在俄语等复杂语言中的性能进展。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决俄语文化新闻文本中人名识别的挑战，评估并比较不同NER模型的性能，以了解其在形态复杂语言中的应用潜力。

Method: 使用SPbLitGuide数据集，比较了DeepPavlov、RoBERTa、SpaCy等传统模型与GPT-3.5、GPT-4、GPT-4o等LLM模型的表现，重点关注JSON提示对性能的影响。

Result: GPT-4o在JSON提示下F1分数达0.93，GPT-4精度最高为0.99。GPT-4.1在后续测试中表现进一步提升。

Conclusion: 研究表明LLM尤其适合俄语等复杂语言的NER任务，提示方式显著影响性能，模型改进速度快。

Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.

</details>


### [62] [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)
*Minh Duc Bui,Kyung Eun Park,Goran Glavaš,Fabian David Schmidt,Katharina von der Wense*

Key words: 测量系统，大型语言模型，文化多样性，准确性，推理方法

TL;DR: 论文研究了大型语言模型（LLMs）在不同测量系统（如货币单位）下的表现，发现模型倾向于默认使用数据中占主导地位的测量系统，且准确性和性能在不同系统中存在显著波动。推理方法（如链式思考）可以缓解问题，但会增加计算成本和响应时间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索LLMs能否在不同文化和测量系统中提供准确信息，以服务多样化的用户群体。

Method: 通过新编译的数据集测试七个开源LLM，回答三个关键问题：默认测量系统、准确性差异、以及通过推理缓解挑战的能力。

Result: LLMs默认使用数据中的主导测量系统，性能在不同系统中不稳定。推理方法可改善准确性，但增加计算成本。

Conclusion: LLMs在不同测量系统中的表现存在偏差，推理方法虽有效但成本高，可能边缘化使用非主流测量系统的用户。

Abstract: Measurement systems (e.g., currencies) differ across cultures, but the
conversions between them are well defined so that humans can state facts using
any measurement system of their choice. Being available to users from diverse
cultural backgrounds, large language models (LLMs) should also be able to
provide accurate information irrespective of the measurement system at hand.
Using newly compiled datasets we test if this is the case for seven open-source
LLMs, addressing three key research questions: (RQ1) What is the default system
used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their
accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate
potential challenges w.r.t. underrepresented systems via reasoning? Our
findings show that LLMs default to the measurement system predominantly used in
the data. Additionally, we observe considerable instability and variance in
performance across different measurement systems. While this instability can in
part be mitigated by employing reasoning methods such as chain-of-thought
(CoT), this implies longer responses and thereby significantly increases
test-time compute (and inference costs), marginalizing users from cultural
backgrounds that use underrepresented measurement systems.

</details>


### [63] [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)
*Zhi-Yuan Chen,Hao Wang,Xinyu Zhang,Enrui Hu,Yankai Lin*

Key words: 自偏好偏差、大型语言模型、DBG评分、黄金判断、响应质量

TL;DR: 论文提出了一种新的DBG评分方法，通过引入黄金判断来更准确地衡量大型语言模型的自偏好偏差问题，避免了传统方法中响应质量对偏差测量的干扰。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是发现现有的方法在测量大型语言模型的自偏好偏差时，无法区分偏差与响应质量的影响，从而导致测量不准确。为了解决这一问题，作者提出了改进方案。

Method: 研究方法包括引入黄金判断作为响应质量的真实基准，并提出DBG评分来衡量自偏好偏差，同时通过实验分析不同模型版本、规模和推理能力的偏差情况。

Result: 实验结果显示，DBG评分能有效分离偏差与响应质量的影响。同时，研究还发现响应文本风格和训练后的数据可以帮助减轻自偏好偏差。

Conclusion: 结论表明，DBG评分是一种有效的自偏好偏差测量方法，并为未来的研究提供了新的视角和工具。

Abstract: Recent studies show that large language models (LLMs) exhibit self-preference
bias when serving as judges, meaning they tend to favor their own responses
over those generated by other models. Existing methods typically measure this
bias by calculating the difference between the scores a judge model assigns to
its own responses and those it assigns to responses from other models. However,
this approach conflates self-preference bias with response quality, as
higher-quality responses from the judge model may also lead to positive score
differences, even in the absence of bias. To address this issue, we introduce
gold judgments as proxies for the actual quality of responses and propose the
DBG score, which measures self-preference bias as the difference between the
scores assigned by the judge model to its own responses and the corresponding
gold judgments. Since gold judgments reflect true response quality, the DBG
score mitigates the confounding effect of response quality on bias measurement.
Using the DBG score, we conduct comprehensive experiments to assess
self-preference bias across LLMs of varying versions, sizes, and reasoning
abilities. Additionally, we investigate two factors that influence and help
alleviate self-preference bias: response text style and the post-training data
of judge models. Finally, we explore potential underlying mechanisms of
self-preference bias from an attention-based perspective. Our code and data are
available at https://github.com/zhiyuanc2001/self-preference.

</details>


### [64] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
*Fan Gao,Dongyuan Li,Ding Xia,Fei Mi,Yasheng Wang,Lifeng Shang,Baojun Wang*

Key words: 中文写作，大语言模型，多文体评估，教育技术

TL;DR: 提出了一个多类型的中文写作基准测试，旨在评估大语言模型在不同文体中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试在中文写作评估中忽视了结构和修辞复杂性，特别是在多文体背景下。

Method: 设计了包含728个真实提示的多文体基准测试，开发了细粒度评分框架，并进行人类评估验证。

Result: 测试了15个大语言模型，分析了它们在不同文体和指令类型中的优劣势。

Conclusion: 该基准测试有助于推动中文写作评估的研究，并启发教育场景下的写作生成改进。

Abstract: Chinese essay writing and its evaluation are critical in educational
contexts, yet the capabilities of Large Language Models (LLMs) in this domain
remain largely underexplored. Existing benchmarks often rely on coarse-grained
text quality metrics, largely overlooking the structural and rhetorical
complexities of Chinese essays, particularly across diverse genres. To address
this gap, we propose \benchName, a multi-genre benchmark specifically designed
for Chinese essay writing across four major genres: Argumentative, Narrative,
Descriptive, and Expository. We curate and refine a total of 728 real-world
prompts to ensure authenticity and meticulously categorize them into the
\textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing
scenarios. To reliably evaluate generated essays, we develop a fine-grained,
genre-specific scoring framework that hierarchically aggregates scores. We
further validate our evaluation protocol through a comprehensive human
agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their
strengths and limitations across genres and instruction types. With \benchName,
we aim to advance LLM-based Chinese essay evaluation and inspire future
research on improving essay generation in educational settings.

</details>


### [65] [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)
*Ömer Tarik Özyilmaz,Matt Coler,Matias Valdenegro-Toro*

Key words: ASR, Whisper, 阿拉伯语方言, MSA, 微调, 数据稀缺

TL;DR: 研究了在阿拉伯语方言ASR中微调Whisper模型的效果，发现少量MSA数据即可显著提升小模型性能，且方言合并模型效果接近方言专用模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 商业ASR系统对阿拉伯语方言支持不足，探索如何通过微调提升方言识别效果。

Method: 使用Mozilla Common Voice和MASC数据集，对比MSA微调数据量、预训练效果及方言合并与专用模型。

Result: 少量MSA微调数据能显著提升小模型性能，方言合并模型表现接近专用模型。

Conclusion: 合理平衡的方言数据合并可解决低资源ASR数据稀缺问题，且性能损失小。

Abstract: Although commercial Arabic automatic speech recognition (ASR) systems support
Modern Standard Arabic (MSA), they struggle with dialectal speech. We
investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic
dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common
Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA
training size effects, benefits of pre-training on MSA data, and
dialect-specific versus dialect-pooled models. We find that small amounts of
MSA fine-tuning data yield substantial improvements for smaller models,
matching larger non-fine-tuned models. While MSA pre-training shows minimal
benefit, suggesting limited shared features between MSA and dialects, our
dialect-pooled models perform comparably to dialect-specific ones. This
indicates that pooling dialectal data, when properly balanced, can help address
data scarcity in low-resource ASR without significant performance loss.

</details>


### [66] [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
*Manon Reusens,Bart Baesens,David Jurgens*

Key words: 个性化LLM, 角色一致性, 任务类型, 标准化框架, 模型设计

TL;DR: 本文提出了一种新的标准化框架，用于分析分配角色的LLMs的一致性，发现一致性受多种因素影响，并因任务类型而异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 此前缺乏对LLMs在不同角色和任务类型中一致性的全面分析，本文旨在填补这一空白。

Method: 引入标准化框架，评估LLMs在四种角色类别和多种任务维度中的一致性。

Result: 一致性受角色、刻板印象和模型设计选择影响，结构化任务和额外上下文能提升一致性。

Conclusion: 角色分配LLMs的一致性分析揭示多样化因素的作用，为未来研究提供工具和洞见。

Abstract: Personalized Large Language Models (LLMs) are increasingly used in diverse
applications, where they are assigned a specific persona - such as a happy high
school teacher - to guide their responses. While prior research has examined
how well LLMs adhere to predefined personas in writing style, a comprehensive
analysis of consistency across different personas and task types is lacking. In
this paper, we introduce a new standardized framework to analyze consistency in
persona-assigned LLMs. We define consistency as the extent to which a model
maintains coherent responses when assigned the same persona across different
tasks and runs. Our framework evaluates personas across four different
categories (happiness, occupation, personality, and political stance) spanning
multiple task dimensions (survey writing, essay generation, social media post
generation, single turn, and multi-turn conversations). Our findings reveal
that consistency is influenced by multiple factors, including the assigned
persona, stereotypes, and model design choices. Consistency also varies across
tasks, increasing with more structured tasks and additional context. All code
is available on GitHub.

</details>


### [67] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
*Shihan Dou,Ming Zhang,Chenhao Huang,Jiayi Chen,Feng Chen,Shichun Liu,Yan Liu,Chenxiao Liu,Cheng Zhong,Zongzhang Zhang,Tao Gui,Chao Xin,Wei Chengzhi,Lin Yan,Qi Zhang,Xuanjing Huang*

Key words: 大型语言模型, 基准测试, 学习能力, 动态评估, EvaLearn

TL;DR: EvaLearn是一个评估大型语言模型学习能力和效率的基准测试，包含648个挑战性问题，要求模型顺序解决问题并提供五个自动化指标。测试发现，某些模型如Claude-3.7-sonnet表现出较强学习能力，而其他模型则难以受益于经验。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基准测试大多并行评估模型，未能充分探索模型的学习能力和效率。EvaLearn填补了这一空白，专注于模型在顺序任务中的动态能力。

Method: EvaLearn包含六个任务类型的182个问题序列，要求模型顺序解决648个问题，同时提供五种自动化评估指标。

Result: 测试九种前沿模型显示，其学习能力差异显著；某些模型能从经验中学习，而另一些则表现不佳。静态能力强的模型在学习能力上并无明显优势。

Conclusion: EvaLearn为评估模型潜力和理解模型与人类能力差距提供了新视角，促进了更深入的动态评估方法发展。

Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.

</details>


### [68] [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
*Zhong-Zhi Li,Xiao Liang,Zihao Tang,Lei Ji,Peijie Wang,Haotian Xu,Xing W,Haizhen Huang,Weiwei Deng,Ying Nian Wu,Yeyun Gong,Zhijiang Guo,Xiao Liu,Fei Yin,Cheng-Lin Liu*

Key words: 大语言模型,动态比例训练,推理效率,System-1,System-2,DeepSeek

TL;DR: 提出一种动态比例训练方法，减少大语言模型推理时的冗余输出，降低40%的token数量，同时保持推理准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有大语言模型在推理时面临长输出效率低的问题，研究者希望减少冗余推理过程，同时保留模型的推理能力。

Method: 采用动态比例训练方法，平衡模型System-1和System-2数据的权重，无需复杂数据标注或多模型插值。

Result: 在DeepSeek-R1-Distill-7B和14B模型及多种难度基准测试中，显著减少近40%的输出token，同时保持推理准确性。

Conclusion: 该方法有效提升了推理效率，未来代码和数据将被开源。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by
leveraging Reinforcement Learning and extended Chain-of-Thought (CoT)
techniques. However, the challenge of performing efficient language
reasoning--especially during inference with extremely long outputs--has drawn
increasing attention from the research community. In this work, we propose a
dynamic ratio-based training pipeline that does not rely on sophisticated data
annotations or interpolation between multiple models. We continuously balance
the weights between the model's System-1 and System-2 data to eliminate
redundant reasoning processes while preserving the model's reasoning
capability. We validate our approach across models on DeepSeek-R1-Distill-7B
and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying
difficulty levels. Our method significantly reduces the number of output tokens
by nearly 40% while maintaining the accuracy of the reasoning. Our code and
data will be available soon.

</details>


### [69] [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)
*Zhengdong Lu,Weikai Lu,Yiling Tao,Yun Dai,ZiXuan Chen,Huiping Zhuang,Cen Chen,Hao Peng,Ziqian Zeng*

Key words: LLM, 规划任务, 并行规划, DPPM, 验证优化

TL;DR: DPPM是一种并行规划范式，通过分解、并行规划和合并子计划来解决LLM在规划任务中的局限性，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型在规划任务中存在约束和级联错误的问题，需要新的方法来解决这些局限性。

Method: 提出DPPM范式，包括分解任务、并行生成子计划、合并子计划及验证与优化模块。

Result: 实验显示DPPM在旅行规划任务中表现显著优于现有方法。

Conclusion: DPPM通过并行规划和验证优化，有效提升了LLM在复杂规划任务中的能力。

Abstract: Despite significant advances in Large Language Models (LLMs), planning tasks
still present challenges for LLM-based agents. Existing planning methods face
two key limitations: heavy constraints and cascading errors. To address these
limitations, we propose a novel parallel planning paradigm, which Decomposes,
Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).
Specifically, DPPM decomposes the complex task based on constraints into
subtasks, generates the subplan for each subtask in parallel, and merges them
into a global plan. In addition, our approach incorporates a verification and
refinement module, enabling error correction and conflict resolution.
Experimental results demonstrate that DPPM significantly outperforms existing
methods in travel planning tasks.

</details>


### [70] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)
*Liang Yue,Yihong Tang,Kehai Chen,Jie Liu,Min Zhang*

Key words: 指令微调, 数据增强, 多智能体交互, 复杂推理

TL;DR: MASTER方法通过多智能体交互增强原始数据，构建了高质量的BOOST-QA数据集，显著提升了模型在多任务和复杂推理任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大模型高质量微调数据难以获取的问题，提出了一种新的数据增强方法。

Method: 利用多智能体交互模拟教学场景，生成高质量的师生交互数据。

Result: 基于MASTER构建的BOOST-QA数据集使模型在多个基准测试中表现优异，尤其在复杂推理任务中有显著提升。

Conclusion: MASTER方法为高质量微调数据的生成提供了新思路，对未来的研究具有重要价值。

Abstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'
instruction-following capabilities and task-specific performance. However,
obtaining high-quality fine-tuning data for large models is challenging due to
data collection difficulties and high production costs. To address this, we
propose MASTER, a novel data augmentation method that enriches original data
through interactions among multiple agents with varying cognitive levels. We
simulate three pedagogically grounded teaching scenarios, leveraging
multi-agent conversations to generate high-quality teacher-student interaction
data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented
from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.
Experiments show that models fine-tuned with BOOST-QA perform excellently
across multiple benchmarks, demonstrating strong multitask generalization.
Notably, MASTER significantly improves models' reasoning abilities in complex
tasks, providing valuable insights for future research.

</details>


### [71] [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
*Masaki Sakata,Sho Yokoi,Benjamin Heinzerling,Takumi Ito,Kentaro Inui*

Key words: 语言模型、命名实体、内部表示、聚类分析、同构性

TL;DR: 论文分析了语言模型内部表示如何识别和区分命名实体的提及，提出了基于聚类质量指标的框架，并实验证明模型能有效区分实体。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究语言模型内部表示如何识别和区分命名实体的提及，以理解模型如何组织和利用实体信息。

Method: 通过聚类分析量化实体提及的歧义性和变异性，实验验证五种Transformer自回归模型的表现。

Result: 模型能有效识别和区分实体，指标在0.66到0.9之间；实体信息在早期层的低维线性子空间中紧凑表示。

Conclusion: 语言模型内部表示与实体中心知识结构存在同构性，为模型如何组织和利用实体提供了新见解。

Abstract: We analyze the extent to which internal representations of language models
(LMs) identify and distinguish mentions of named entities, focusing on the
many-to-many correspondence between entities and their mentions. We first
formulate two problems of entity mentions -- ambiguity and variability -- and
propose a framework analogous to clustering quality metrics. Specifically, we
quantify through cluster analysis of LM internal representations the extent to
which mentions of the same entity cluster together and mentions of different
entities remain separated. Our experiments examine five Transformer-based
autoregressive models, showing that they effectively identify and distinguish
entities with metrics analogous to precision and recall ranging from 0.66 to
0.9. Further analysis reveals that entity-related information is compactly
represented in a low-dimensional linear subspace at early LM layers.
Additionally, we clarify how the characteristics of entity representations
influence word prediction performance. These findings are interpreted through
the lens of isomorphism between LM representations and entity-centric knowledge
structures in the real world, providing insights into how LMs internally
organize and use entity information.

</details>


### [72] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
*Qihang Yan,Xinyu Zhang,Luming Guo,Qi Zhang,Feifan Liu*

Key words: RACE-Align, LLMs, 检索增强, 链式思维, 垂直领域

TL;DR: RACE-Align结合检索增强和链式思维优化对齐方法，显著提升LLMs在垂直领域的效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLMs在垂直领域中准确性、领域推理及可解释性不足的问题，传统对齐方法忽略了知识和推理逻辑。

Method: RACE-Align构建二元偏好数据集，整合外部知识与显式链式思维推理，通过DPO算法对齐模型。

Result: 在中医领域实验显著提升答案准确性、信息丰富度和推理逻辑性。

Conclusion: RACE-Align为复杂垂直领域中LLMs的知识应用和推理可靠性提供了有效途径。

Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific
reasoning, and interpretability in vertical domains. Traditional preference
alignment methods like Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) often overlook the underlying knowledge
sources and reasoning logic. This paper introduces RACE-Align
(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel
framework designed to address these limitations. RACE-Align systematically
constructs a binary preference dataset incorporating external knowledge support
and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO
algorithm. The core innovation lies in its preference data construction
strategy: it integrates AI-driven retrieval for factual grounding, enhancing
knowledgeability and accuracy, and emphasizes the optimization of
domain-specific CoT, treating the reasoning process itself as a key preference
dimension. A multi-stage, AI-driven refinement pipeline cost-effectively
generates these preference pairs. Experimental validation in Traditional
Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that
RACE-Align significantly outperforms the original base model and a model
fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed
across multiple dimensions, including answer accuracy, information richness,
application of TCM thinking patterns, logicality and depth of reasoning, and
interpretability. These findings suggest RACE-Align offers an effective pathway
to enhance LLMs' knowledge application, reasoning reliability, and process
transparency in complex vertical domains.

</details>


### [73] [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)
*Amaç Herdağdelen,Marco Baroni*

Key words: 性别偏见，刻板印象，Twitter，OMCS，自然语言处理，数据集

TL;DR: 研究从文本和Twitter数据中提取性别特定行为，并与刻板印象对比，利用OMCS和自动化方法评估性别偏见，验证了利用自然文本增强常识库的可行性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索如何利用自然语言数据（如Twitter）和常识知识库（如OMCS）来量化性别偏见行为，并与人类刻板印象对比验证。

Method: 使用OMCS捕捉常识行为，结合Twitter用户性别信息和语料库启发式方法计算行为性别偏见，并与人工标准对比。

Result: 语料库预测与人工标准的斯皮尔曼相关性为0.47，ROC曲线下面积为0.76，并提供了两个数据集（人工评分和自动评分）。

Conclusion: 利用自然文本（尤其是Twitter）可以有效地为常识库补充性别刻板印象数据，提出的自动化方法具有一定可靠性。

Abstract: We extracted gender-specific actions from text corpora and Twitter, and
compared them to stereotypical expectations of people. We used Open Mind Common
Sense (OMCS), a commonsense knowledge repository, to focus on actions that are
pertinent to common sense and daily life of humans. We use the gender
information of Twitter users and Web-corpus-based pronoun/name gender
heuristics to compute the gender bias of the actions. With high recall, we
obtained a Spearman correlation of 0.47 between corpus-based predictions and a
human gold standard, and an area under the ROC curve of 0.76 when predicting
the polarity of the gold standard. We conclude that it is feasible to use
natural text (and a Twitter-derived corpus in particular) in order to augment
commonsense repositories with the stereotypical gender expectations of actions.
We also present a dataset of 441 commonsense actions with human judges' ratings
on whether the action is typically/slightly masculine/feminine (or neutral),
and another larger dataset of 21,442 actions automatically rated by the methods
we investigate in this study.

</details>


### [74] [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)
*Aisha Alansari,Hamzah Luqman*

Key words: 冒犯性语言检测、多任务学习、主动学习、阿拉伯语、社交媒体

TL;DR: 该论文提出一种结合多任务学习和主动学习的新框架，用于提升阿拉伯社交媒体中冒犯性语言的检测效果，并在资源受限的环境中取得了最优性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 社交媒体中冒犯性、暴力和粗俗语言的泛滥引发了社会和网络安全问题，而阿拉伯语的复杂性和数据稀缺性增加了检测难度。

Method: 通过多任务学习联合训练暴力语言和粗俗语言两个辅助任务，动态调整任务权重，并利用主动学习策略通过不确定性采样选择信息量大的样本，同时引入加权表情处理。

Result: 在OSACT2022数据集上，模型取得了85.42%的宏F1分数，优于现有方法且使用更少的微调样本。

Conclusion: 研究表明，结合多任务学习和主动学习的框架有助于在资源受限条件下高效准确地检测冒犯性语言。

Abstract: The rapid growth of social media has amplified the spread of offensive,
violent, and vulgar speech, which poses serious societal and cybersecurity
concerns. Detecting such content in Arabic text is particularly complex due to
limited labeled data, dialectal variations, and the language's inherent
complexity. This paper proposes a novel framework that integrates multi-task
learning (MTL) with active learning to enhance offensive speech detection in
Arabic social media text. By jointly training on two auxiliary tasks, violent
and vulgar speech, the model leverages shared representations to improve the
detection accuracy of the offensive speech. Our approach dynamically adjusts
task weights during training to balance the contribution of each task and
optimize performance. To address the scarcity of labeled data, we employ an
active learning strategy through several uncertainty sampling techniques to
iteratively select the most informative samples for model training. We also
introduce weighted emoji handling to better capture semantic cues. Experimental
results on the OSACT2022 dataset show that the proposed framework achieves a
state-of-the-art macro F1-score of 85.42%, outperforming existing methods while
using significantly fewer fine-tuning samples. The findings of this study
highlight the potential of integrating MTL with active learning for efficient
and accurate offensive language detection in resource-constrained settings.

</details>


### [75] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
*Stefano Bannò,Kate Knill,Mark Gales*

Key words: 词汇评估、第二语言学习、大型语言模型、英语词汇档案、熟练度

TL;DR: 本文提出了一种利用大型语言模型（LLMs）和英语词汇档案（EVP）进行细粒度词汇评估的新方法，旨在通过上下文分析提高第二语言（L2）学习者写作中词汇使用的评估准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目前，词汇评估多依赖于上下文无关或词性相关的方法。本文旨在通过上下文分析提升词汇评估的精确性。

Method: 结合LLMs和EVP，评估LLMs在L2学习者写作中为单个词汇分配熟练度的能力，并解决多义性、上下文变化和多词表达等挑战。

Result: LLMs在词汇评估任务中表现优于基于词性的基线方法，能够利用额外的语义信息提升性能。

Conclusion: LLMs适用于词汇评估任务，能够有效分析学习者词汇使用的熟练度。

Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency.
To date, its assessment by automated systems has typically examined the
context-independent, or part-of-speech (PoS) related use of words. This paper
introduces a novel approach to enable fine-grained vocabulary evaluation
exploiting the precise use of words within a sentence. The scheme combines
large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP
is a standard lexical resource that enables in-context vocabulary use to be
linked with proficiency level. We evaluate the ability of LLMs to assign
proficiency levels to individual words as they appear in L2 learner writing,
addressing key challenges such as polysemy, contextual variation, and
multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to
exploit additional semantic information that yields improved performance. We
also explore correlations between word-level proficiency and essay-level
proficiency. Finally, the approach is applied to examine the consistency of the
EVP proficiency levels. Results show that LLMs are well-suited for the task of
vocabulary assessment.

</details>


### [76] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
*Sifan Li,Yujun Cai,Yiwei Wang*

Key words: 视觉语言模型(VLMs), HC-Bench, SemVink, 隐藏内容检测, 多尺度处理

TL;DR: 论文介绍了一个名为HC-Bench的基准测试，揭示了当前领先的视觉语言模型(VLMs)在检测隐藏内容（如光学幻觉或AI生成图像）方面的能力不足，并提出了一种简单的解决方案SemVink，通过降低图像分辨率显著提高了准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决VLMs在检测隐藏内容上的不足，揭示了其过于依赖高级语义而忽视低层次视觉操作的缺陷。

Method: 提出了HC-Bench基准测试，包含112张带有隐藏文本、物体和幻觉的图像，并通过SemVink方法（将图像缩放至低分辨率）来消除视觉噪声。

Result: 实验结果表明，VLMs在基准测试中的准确率极低（0-5.36%），而SemVink方法将准确率提升至超过99%。

Conclusion: 论文呼吁开发混合模型，整合多尺度处理，以弥补计算视觉与人类认知之间的差距，适用于医疗影像、安全等领域。

Abstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core
human capability: detecting hidden content in optical illusions or AI-generated
images through perceptual adjustments like zooming. We introduce HC-Bench, a
benchmark of 112 images with hidden text, objects, and illusions, revealing
that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit
prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to
an overreliance on high-level semantics. Strikingly, we propose SemVink
(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128
pixels), which unlocks >99% accuracy by eliminating redundant visual noise.
This exposes a critical architectural flaw: VLMs prioritize abstract reasoning
over low-level visual operations crucial for real-world robustness. Our work
urges a shift toward hybrid models integrating multi-scale processing, bridging
the gap between computational vision and human cognition for applications in
medical imaging, security, and beyond.

</details>


### [77] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
*Ekaterina Grishina,Mikhail Gorbunov,Maxim Rakhuba*

Key words: 大型语言模型, 权重矩阵, 正交变换, 结构化矩阵, 模型压缩

TL;DR: 该论文提出了一种利用正交变换优化大型语言模型权重矩阵压缩的方法，以减少计算和内存需求。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）在自然语言处理任务中表现出色，但需要大量计算和内存资源，因此需要减少其参数数量。

Method: 通过利用LLMs输出在特定正交变换下的不变性，优化权重矩阵的可压缩性，适用于多种结构化矩阵类型。

Result: 提出的方法能够在不损失模型准确性的情况下，显著压缩权重矩阵。

Conclusion: 该方法为减少LLMs资源需求提供了一种有效途径，适用于多种结构化矩阵。

Abstract: Large language models (LLMs) demonstrate impressive results in natural
language processing tasks but require a significant amount of computational and
memory resources. Structured matrix representations are a promising way for
reducing the number of parameters of these models. However, it seems
unrealistic to expect that weight matrices of pretrained models can be
accurately represented by structured matrices without any fine-tuning. To
overcome this issue, we utilize the fact that LLM output is invariant under
certain orthogonal transformations of weight matrices. This insight can be
leveraged to identify transformations that significantly improve the
compressibility of weights within structured classes. The proposed approach is
applicable to various types of structured matrices that support efficient
projection operations. Code is available at
https://github.com/GrishKate/ProcrustesGPT

</details>


### [78] [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)
*Yulin Dou,Jiangming Liu*

Key words: 大语言模型, 对话系统, 轨迹优化, 偏好获取

TL;DR: 论文介绍了TO-GATE框架，通过轨迹优化提升生成有效对话问题的能力，显著优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有基于自学习推理的方法难以识别最佳对话轨迹，导致任务无关问题，因此需要改进。

Method: 提出TO-GATE框架，包含澄清解析器（生成最优提问轨迹）和总结器（确保任务对齐的最终响应）。

Result: TO-GATE在标准偏好获取任务上比基线方法提升了9.32%。

Conclusion: TO-GATE通过轨迹优化显著提升了对话问题的生成效果。

Abstract: Large language models (LLMs) can effectively elicit human preferences through
multi-turn dialogue. Complex tasks can be accomplished through iterative
clarifying questions and final responses generated by an LLM acting as a
questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches
based on self-taught reasoning struggle to identify optimal dialogue
trajectories and avoid irrelevant questions to the tasks. To address this
limitation, we propose TO-GATE, a novel framework that enhances question
generation through trajectory optimization, which consists of two key
components: a clarification resolver that generates optimal questioning
trajectories, and a summarizer that ensures task-aligned final responses. The
trajectory optimization enables the model to produce effective elicitation
questions and summary responses tailored to specific tasks. Experimental
results demonstrate that TO-GATE significantly outperforms baseline methods,
achieving a 9.32% improvement on standard preference elicitation tasks.

</details>


### [79] [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)
*Ludovic Moncla,Hédi Zeghidi*

Key words: 命名实体识别, 历史文本, 嵌套实体, 生成模型, 低资源学习

TL;DR: 论文研究了历史文本中的命名实体识别（NER），比较了多种方法，包括传统技术和Transformer模型，并在18世纪法语百科全书数据集上进行了实验。提出了针对嵌套实体的分类框架，并评估了生成模型在低资源场景下的潜力。结果展示了Transformer模型的优势，同时指出了结合符号与神经方法的混合策略的价值。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 历史文本中的NER因语言非标准化、拼写古老及实体嵌套等特性而具有挑战性，本研究旨在评估不同方法的表现并提出改进方向。

Method: 在GeoEDdA数据集上测试了CRF、spaCy、CamemBERT、Flair等多种模型，提出基于标记和跨度的分类框架，并探索了生成模型的少样本提示能力。

Result: Transformer模型在嵌套实体上表现最优，生成模型在低资源情况下具有潜力。

Conclusion: 历史文本NER仍需解决复杂性问题，未来可结合符号与神经方法的混合策略以提高效果。

Abstract: Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.

</details>


### [80] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
*Jintian Shao,Yiming Cheng*

Key words: Chain-of-Thought, Large Language Models, Reasoning, Structural Constraint, Pattern Matching

TL;DR: 论文认为链式思维（CoT）提示并非引发大型语言模型的真实抽象推理能力，而是通过结构性约束引导模型模仿推理形式。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨链式思维（CoT）提示在大型语言模型中的实际作用，挑战其普遍认为的能激发抽象推理能力的观点。

Method: 采用理论论证，分析链式思维提示如何通过结构性约束和序列预测能力模仿推理过程。

Result: 链式思维提示通过引导模型生成中间步骤，表现出类似推理的形式，但并未实现真实抽象推理。

Conclusion: 链式思维提示是一种有效的结构性约束工具，但其效果源于模型的序列预测能力，而非真正的抽象推理能力。

Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.
Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.

</details>


### [81] [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)
*Verena Blaschke,Miriam Winkler,Constantin Förster,Gabriele Wenger-Glemser,Barbara Plank*

Key words: ASR, 方言, 德语, Betthupferl数据集

TL;DR: 该论文介绍了德国东南部三种方言的语音识别数据集Betthupferl，评估现有ASR模型对方言鲁棒性，发现输出结果介于方言与标准德语之间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 德国方言在ASR研究中代表性不足，需评估模型对方言的适应性。

Method: 构建含三种方言及标准德语的4小时语音数据集，分析方言与标准德语的差异，并测试多语言ASR模型。

Result: 模型输出介于方言与标准德语之间，部分结果保留方言结构。

Conclusion: 现有ASR模型对德方言处理仍有不足，未来需改进。

Abstract: Although Germany has a diverse landscape of dialects, they are
underrepresented in current automatic speech recognition (ASR) research. To
enable studies of how robust models are towards dialectal variation, we present
Betthupferl, an evaluation dataset containing four hours of read speech in
three dialect groups spoken in Southeast Germany (Franconian, Bavarian,
Alemannic), and half an hour of Standard German speech. We provide both
dialectal and Standard German transcriptions, and analyze the linguistic
differences between them. We benchmark several multilingual state-of-the-art
ASR models on speech translation into Standard German, and find differences
between how much the output resembles the dialectal vs. standardized
transcriptions. Qualitative error analyses of the best ASR model reveal that it
sometimes normalizes grammatical differences, but often stays closer to the
dialectal constructions.

</details>


### [82] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
*Yusuke Sakai,Takumi Goto,Taro Watanabe*

Key words: 语法错误校正, GED, 无参考评估, 语言模型, SEEDA

TL;DR: IMPARA-GED 是一种新型的无参考自动语法错误校正（GEC）评估方法，具备语法错误检测（GED）能力。通过增强GED能力的预训练语言模型构建其质量估计器，实验表明其在SEEDA数据集上与人类评价的相关性最高。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决传统自动GEC评估方法缺乏语法错误检测能力的问题，提升评估的准确性和相关性。

Method: 基于IMPARA的质量估计器，利用预训练语言模型增强语法错误检测能力，构建IMPARA-GED。

Result: 在SEEDA数据集上，IMPARA-GED与人类句子级评价的相关性最高。

Conclusion: IMPARA-GED是一种有效的无参考自动GEC评估方法，具有强大的语法错误检测能力。

Abstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error
correction (GEC) evaluation method with grammatical error detection (GED)
capabilities. We focus on the quality estimator of IMPARA, an existing
automatic GEC evaluation method, and construct that of IMPARA-GED using a
pre-trained language model with enhanced GED capabilities. Experimental results
on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,
demonstrate that IMPARA-GED achieves the highest correlation with human
sentence-level evaluations.

</details>


### [83] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
*Yin Fang,Qiao Jin,Guangzhi Xiong,Bowen Jin,Xianrui Zhong,Siru Ouyang,Aidong Zhang,Jiawei Han,Zhiyong Lu*

Key words: 单细胞RNA测序、细胞类型标注、大语言模型、CellPuzzles、Cell-o1。

TL;DR: 提出了一种新的单细胞RNA测序数据分析任务CellPuzzles，专注于批级细胞类型标注，并通过训练模型Cell-o1提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的基础模型在单细胞RNA测序数据标注中缺乏批级上下文和解释性，而人类专家依赖知识和上下文标注。

Method: 引入CellPuzzles任务，开发Cell-o1模型（7B LLM），结合监督微调和强化学习。

Result: Cell-o1在批级准确率上超越基线（OpenAI o1）73%，表现出色且泛化能力强。

Conclusion: Cell-o1通过批级奖励和推理训练，实现了类似专家的表现，为单细胞数据分析提供了新思路。

Abstract: Cell type annotation is a key task in analyzing the heterogeneity of
single-cell RNA sequencing data. Although recent foundation models automate
this process, they typically annotate cells independently, without considering
batch-level cellular context or providing explanatory reasoning. In contrast,
human experts often annotate distinct cell types for different cell clusters
based on their domain knowledge. To mimic this workflow, we introduce the
CellPuzzles task, where the objective is to assign unique cell types to a batch
of cells. This benchmark spans diverse tissues, diseases, and donor conditions,
and requires reasoning across the batch-level cellular context to ensure label
uniqueness. We find that off-the-shelf large language models (LLMs) struggle on
CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%
batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained
via supervised fine-tuning on distilled reasoning traces, followed by
reinforcement learning with batch-level rewards. Cell-o1 achieves
state-of-the-art performance, outperforming o1 by over 73% and generalizing
well across contexts. Further analysis of training dynamics and reasoning
behaviors provides insights into batch-level annotation performance and
emergent expert-like reasoning. Code and data are available at
https://github.com/ncbi-nlp/cell-o1.

</details>


### [84] [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)
*Yijun Yang,Zeyu Huang,Wenhao Zhu,Zihan Qiu,Fei Yuan,Jeff Z. Pan,Ivan Titov*

Key words: 长上下文语言模型,评估框架,LongBioBench,语义理解,可信度

TL;DR: 该论文提出了一个新的长上下文语言模型（LCLM）评估框架LongBioBench，通过人工生成传记来测试模型的理解、推理和可信度，弥补了现有评估方法的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有评估长上下文语言模型的方法（现实任务和合成任务）存在局限性：现实任务过于复杂且易受数据污染，合成任务缺乏真实性。因此，需要一个兼具可控性和真实性的新框架。

Method: 提出LongBioBench基准，利用人工生成的传记作为测试环境，评估模型的理解、推理和可信度。实验涵盖18个LCLM。

Result: 大多数模型在语义理解和基础推理方面表现不足，且随着上下文长度增加，可信度下降。同时，揭示了现有合成基准设计中的漏洞。

Conclusion: LongBioBench在真实性和可控性之间取得了更好的平衡，是一种高度可解释和可配置的评估方法。

Abstract: Existing frameworks for evaluating long-context language models (LCLM) can be
broadly categorized into real-world and synthetic tasks. Despite their utility,
both approaches are accompanied by certain intrinsic limitations. Real-world
tasks are too complex to interpret or characterize and are susceptible to data
contamination. In contrast, synthetic tasks often adopt the
needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the
"needle" and the "haystack" compromises their validity as proxies for realistic
applications. In response to these challenges, we posit that an ideal
long-context evaluation framework should be characterized by three essential
features: $\textit{seamless context}$, $\textit{controllable setting}$, and
$\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a
novel benchmark that utilizes artificially generated biographies as a
controlled environment for assessing LCLMs across dimensions of
$\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$.
Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total,
demonstrates that most models still exhibit deficiencies in semantic
understanding and elementary reasoning over retrieved results and are less
trustworthy as context length increases. Our further analysis indicates some
design choices employed by existing synthetic benchmarks, such as contextual
non-coherence, numerical needles, and the absence of distractors, rendering
them vulnerable to test the model long-context capabilities. Moreover, we also
reveal that long-context continual pretraining primarily adjusts RoPE embedding
to accommodate extended context lengths. To sum up, compared to previous
synthetic benchmarks, LongBioBench achieves a better trade-off between
mirroring authentic language tasks and maintaining controllability, and is
highly interpretable and configurable.

</details>


### [85] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
*Diogo A. P. Nunes,Eugénio Ribeiro*

Key words: 抑郁症，信息检索，二元分类，模型微调，合成数据

TL;DR: 该论文描述了团队在eRisk 2025任务1中通过二元分类和模型微调等方法成功搜索抑郁症状的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决Beck抑郁量表(BDI)问卷中抑郁症状的句子分类问题，并优化信息检索效果。

Method: 将问题建模为二元分类任务，探索了模型微调、句子相似性、大语言模型提示和集成技术等方法。

Result: 微调基础模型表现最佳，尤其在合成数据缓解类别不平衡时，最终测试表现优于其他16个团队。

Conclusion: 微调基础模型并结合合成数据是提升抑郁症状分类任务效果的有效方法，且方法需针对不同症状调整。

Abstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.

</details>


### [86] [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
*Aishwarya Sahoo,Jeevana Kruthi Karnuthala,Tushar Parmanand Budhwani,Pranchal Agarwal,Sankaran Vaidyanathan,Alexa Siu,Franck Dernoncourt,Jennifer Healey,Nedim Lipka,Ryan Rossi,Uttaran Bhattacharya,Branislav Kveton*

Key words: LLM, 自动评估, 定量法官, 回归模型, 人类评分对齐

TL;DR: 该论文提出了一个名为'LLM-as-a-judge'的框架，通过回归模型将大型语言模型（LLM）的评分与人类评分对齐，以改进自动评估的效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为解决LLM自动评估评分与人类评分不一致的问题，提出了一种更高效且统计有效的定量评估方法。

Method: 使用回归模型训练定量法官（quantitative judges），通过文本评估和评分改进原有法官的评分准确性。

Result: 实验验证了定量法官在四个数据集上能有效提升现有法官的预测能力。

Conclusion: 该框架在计算和统计效率上优于监督微调，特别适用于人类反馈有限的场景。

Abstract: LLM-as-a-judge is a framework in which a large language model (LLM)
automatically evaluates the output of another LLM. We propose quantitative LLM
judges, which align evaluation scores of existing LLM judges to human scores in
a given domain using regression models. The models are trained to improve the
score of the original judge by using the judge's textual evaluation and score.
We present four quantitative judges for different types of absolute and
relative feedback, which showcases the generality and versatility of our
framework. Our framework is more computationally efficient than supervised
fine-tuning and can be more statistically efficient when human feedback is
limited, which is expected in most applications of our work. We validate these
claims empirically on four datasets using two base judges. Our experiments show
that quantitative judges can effectively improve the predictive power of
existing judges through post-hoc modeling.

</details>


### [87] [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
*Boyi Li,Zhonghan Zhao,Der-Horng Lee,Gaoang Wang*

Key words: 多智能体系统,自适应图剪枝,大语言模型,任务适应性,通信拓扑

TL;DR: 提出了一种名为自适应图剪枝（AGP）的多智能体协作框架，通过动态优化智能体数量和通信拓扑结构，显著提升了任务适应性和性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有基于大语言模型的多智能体系统依赖固定数量和静态通信结构的问题，提升任务适应性和性能。

Method: 采用两阶段训练策略：1）独立训练不同智能体数量的软剪枝网络；2）联合优化硬剪枝和软剪枝，动态配置智能体数量和通信拓扑。

Result: 在六个基准测试中表现优异，性能提升2.58%至9.84%，同时减少90%以上的token消耗，并在十步训练内超越基线。

Conclusion: AGP框架在性能、任务适应性、经济性和训练效率上均显著优于现有方法。

Abstract: Large Language Model (LLM) based multi-agent systems have shown remarkable
performance in various tasks, especially when enhanced through collaborative
communication. However, current methods often rely on a fixed number of agents
and static communication structures, limiting their ability to adapt to varying
task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a
novel task-adaptive multi-agent collaboration framework that jointly optimizes
agent quantity (hard-pruning) and communication topology (soft-pruning).
Specifically, our method employs a two-stage training strategy: firstly,
independently training soft-pruning networks for different agent quantities to
determine optimal agent-quantity-specific complete graphs and positional masks
across specific tasks; and then jointly optimizing hard-pruning and
soft-pruning within a maximum complete graph to dynamically configure the
number of agents and their communication topologies per task. Extensive
experiments demonstrate that our approach is: (1) High-performing, achieving
state-of-the-art results across six benchmarks and consistently generalizes
across multiple mainstream LLM architectures, with a increase in performance of
$2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized
communication topologies tailored to specific tasks, with an extremely high
performance in all three task categories (general reasoning, mathematical
reasoning, and code generation); (3) Token-economical, having fewer training
steps and token consumption at the same time, with a decrease in token
consumption of $90\%+$; and (4) Training-efficient, achieving high performance
with very few training steps compared with other methods. The performance will
surpass the existing baselines after about ten steps of training under six
benchmarks.

</details>


### [88] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
*Zhixiong Su,Yichen Wang,Herun Wan,Zhaohan Zhang,Minnan Luo*

Key words: 大语言模型，机器生成文本检测，人类-AI共同创作，细粒度检测，HACo-Det

TL;DR: 本文探讨细粒度机器生成文本（MGT）检测的可能性，特别是在人类与AI共同创作的情境下。作者提出了一个数据集HACo-Det，并评估了现有检测器在词级和句级检测任务上的表现，发现微调模型优于基于指标的方法，但仍存在改进空间。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型（LLM）的滥用带来风险，但现有研究多关注文档级检测，忽略了人类与AI共同创作的文本。本文旨在填补这一空白。

Method: 作者提出HACo-Det数据集，通过自动流程生成人类与AI共同创作的文本，并为每个词标注来源。改进了7种文档级检测器，评估其在词级和句级检测任务的表现。

Result: 基于指标的方法在细粒度检测上表现较差（平均F1分数0.462），微调模型表现更优且泛化能力更强，但仍未完全解决问题。

Conclusion: 细粒度合作文本检测仍具挑战性。研究分析了影响性能的因素（如上下文窗口）并指出了当前方法的局限性，为未来改进提供了方向。

Abstract: The misuse of large language models (LLMs) poses potential risks, motivating
the development of machine-generated text (MGT) detection. Existing literature
primarily concentrates on binary, document-level detection, thereby neglecting
texts that are composed jointly by human and LLM contributions. Hence, this
paper explores the possibility of fine-grained MGT detection under human-AI
coauthoring. We suggest fine-grained detectors can pave pathways toward
coauthored text detection with a numeric AI ratio. Specifically, we propose a
dataset, HACo-Det, which produces human-AI coauthored texts via an automatic
pipeline with word-level attribution labels. We retrofit seven prevailing
document-level detectors to generalize them to word-level detection. Then we
evaluate these detectors on HACo-Det on both word- and sentence-level detection
tasks. Empirical results show that metric-based methods struggle to conduct
fine-grained detection with a 0.462 average F1 score, while finetuned models
show superior performance and better generalization across domains. However, we
argue that fine-grained co-authored text detection is far from solved. We
further analyze factors influencing performance, e.g., context window, and
highlight the limitations of current methods, pointing to potential avenues for
improvement.

</details>


### [89] [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)
*Yan Gao,Massimo Roberto Scamarcia,Javier Fernandez-Marques,Mohammad Naseri,Chong Shen Ng,Dimitris Stripelis,Zexi Li,Tao Shen,Jiamu Bai,Daoyuan Chen,Zikai Zhang,Rui Hu,InSeo Song,Lee KangYoon,Hong Jia,Ting Dang,Junyan Wang,Zheyuan Liu,Daniel Janes Beutel,Lingjuan Lyu,Nicholas D. Lane*

Key words: Large Language Models, Federated Learning, Fine-tuning, Privacy-preserving, Domain-specific

TL;DR: 该研究首次提出了FlowerTune LLM Leaderboard，用于评估在联邦学习环境下预训练LLM的微调效果，涵盖四个领域，并提供了26种模型的全面比较。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决LLM开发中数据稀缺和敏感信息访问受限的问题，探索联邦学习框架下的预训练LLM兼容性与性能。

Method: 创建FlowerTune LLM Leaderboard，整合四个领域的联邦指令微调数据集和领域特定评估指标，测试26种预训练LLM。

Result: 提供了联邦学习环境下模型性能、资源限制和领域适应的详细分析，为隐私保护型专业LLM开发奠定基础。

Conclusion: 该研究为实际应用中隐私保护型专业LLM的开发提供了重要参考和基础。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across
diverse domains, yet their development remains reliant on vast amounts of
publicly available data, raising concerns about data scarcity and the lack of
access to domain-specific, sensitive information. Federated Learning (FL)
presents a compelling framework to address these challenges by enabling
decentralized fine-tuning on pre-trained LLMs without sharing raw data.
However, the compatibility and performance of pre-trained LLMs in FL settings
remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a
first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning
of LLMs across four diverse domains: general NLP, finance, medical, and coding.
Each domain includes federated instruction-tuning datasets and domain-specific
evaluation metrics. Our results, obtained through a collaborative, open-source
and community-driven approach, provide the first comprehensive comparison
across 26 pre-trained LLMs with different aggregation and fine-tuning
strategies under federated settings, offering actionable insights into model
performance, resource constraints, and domain adaptation. This work lays the
foundation for developing privacy-preserving, domain-specialized LLMs for
real-world applications.

</details>


### [90] [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)
*Dingwei Chen,Ziqiang Liu,Feiteng Fang,Chak Tou Leong,Shiwen Ni,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang,Chengming Li*

Key words: 大型语言模型（LLMs）、幻觉、PLI（Premature Layers Interpolation）、事实一致性

TL;DR: PLI（Premature Layers Interpolation）是一种无需训练、即插即用的干预方法，通过数学插值插入过早的层来减少LLMs的幻觉，提升事实一致性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大型语言模型（LLMs）在文本理解和生成方面表现出色，但存在事实不一致（幻觉）的问题，现有方法在输入或输出层面解决问题，忽视了内在信息细化过程和过早层的作用。

Method: 提出PLI方法，通过数学插值在相邻层之间插入过早的层，扩展信息处理深度，提高事实一致性。

Result: 在四个公开数据集上的实验表明，PLI有效减少了幻觉，并在大多数情况下优于现有基线。

Conclusion: PLI通过层插值与LLMs内部机制紧密结合，显著提升了模型的事实性，且无需额外训练资源。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text
understanding and generation. However, their tendency to produce factually
inconsistent outputs, commonly referred to as ''hallucinations'', remains a
critical challenge. Existing approaches, such as retrieval-based and
inference-time correction methods, primarily address this issue at the input or
output level, often overlooking the intrinsic information refinement process
and the role of premature layers. Meanwhile, alignment- and fine-tuning-based
methods are resource-intensive. In this paper, we propose PLI (Premature Layers
Interpolation), a novel, training-free, and plug-and-play intervention designed
to enhance factuality. PLI mitigates hallucinations by inserting premature
layers formed through mathematical interpolation with adjacent layers. Inspired
by stable diffusion and sampling steps, PLI extends the depth of information
processing and transmission in LLMs, improving factual coherence. Experiments
on four publicly available datasets demonstrate that PLI effectively reduces
hallucinations while outperforming existing baselines in most cases. Further
analysis suggests that the success of layer interpolation is closely linked to
LLMs' internal mechanisms. To promote reproducibility, we will release our code
and data upon acceptance.

</details>


### [91] [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)
*Atsumoto Ohashi,Shinya Iizuka,Jingjing Jiang,Ryuichiro Higashinaka*

Key words: 全双工语音对话系统；日语；两阶段训练；合成数据增强

TL;DR: 摘要介绍了一种针对日语的全双工语音对话系统模型，基于英语模型Moshi开发，通过两阶段训练和合成数据增强，性能优于基线模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究填补了日语全双工语音对话系统的研究空白，并提升了模型在自然度和意义性上的表现。

Method: 模型采用两阶段训练（大规模日语对话数据预训练和高质量立体声对话数据微调），并利用多流语音合成技术生成合成数据增强性能。

Result: 实验表明模型在自然度和意义性上均优于日语基线模型。

Conclusion: 该研究为日语全双工语音对话系统提供了首个公开可用模型，表现优异。

Abstract: Full-duplex spoken dialogue systems, which can model simultaneous
bidirectional features of human conversations such as speech overlaps and
backchannels, have attracted significant attention recently. However, the study
of full-duplex spoken dialogue systems for the Japanese language has been
limited, and the research on their development in Japanese remains scarce. In
this paper, we present the first publicly available full-duplex spoken dialogue
model in Japanese, which is built upon Moshi, a full-duplex dialogue model in
English. Our model is trained through a two-stage process: pre-training on a
large-scale spoken dialogue data in Japanese, followed by fine-tuning on
high-quality stereo spoken dialogue data. We further enhance the model's
performance by incorporating synthetic dialogue data generated by a
multi-stream text-to-speech system. Evaluation experiments demonstrate that the
trained model outperforms Japanese baseline models in both naturalness and
meaningfulness.

</details>


### [92] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)
*Richard Armitage*

Key words: 大型语言模型, 初级医疗, MRCGP, 考试问题, 评估

TL;DR: 测试了几种领先的大型语言模型在初级医疗教育中的表现，发现其在回答MRCGP风格问题上显著优于人类医生。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估大型语言模型（LLMs）在初级医疗教育中的能力，尤其是解决MRCGP风格问题的表现。

Method: 使用四种LLMs（o3、Claude Opus 4、Grok3和Gemini 2.5 Pro）回答100道随机选择的MRCGP考试题目，并对其准确率进行评估。

Result: o3得分99.0%，其他模型均为95.0%，远高于人类医生的平均得分73.0%。

Conclusion: LLMs，尤其是推理类模型，在初级医疗支持中表现出色，尤其是那些经过初级医疗数据专门训练的模型。

Abstract: Background: Large language models (LLMs) have demonstrated substantial
potential to support clinical practice. Other than Chat GPT4 and its
predecessors, few LLMs, especially those of the leading and more powerful
reasoning model class, have been subjected to medical specialty examination
questions, including in the domain of primary care. This paper aimed to test
the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and
Gemini 2.5 Pro) in primary care education, specifically in answering Member of
the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer
100 randomly chosen multiple choice questions from the Royal College of General
Practitioners GP SelfTest on 25 May 2025. Questions included textual
information, laboratory results, and clinical images. Each model was prompted
to answer as a GP in the UK and was provided with full question information.
Each question was attempted once by each model. Responses were scored against
correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was
99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the
same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially
exceeded the average performance of GPs and GP registrars who had answered the
same questions. o3 demonstrated the best performance, while the performances of
the other leading models were comparable with each other and were not
substantially lower than that of o3. These findings strengthen the case for
LLMs, particularly reasoning models, to support the delivery of primary care,
especially those that have been specifically trained on primary care clinical
data.

</details>


### [93] [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/abs/2506.02995)
*Iuliia Zaitova,Badr M. Abdullah,Wei Xue,Dietrich Klakow,Bernd Möbius,Tania Avgustinova*

Key words: 习语翻译, 语音到文本, 机器翻译, 大语言模型, 端到端系统

TL;DR: 论文评估了习语翻译在文本到文本和语音到文本系统中的表现，发现语音到文本系统在习语翻译上表现较差，而机器翻译和大语言模型表现更好，需改进语音到文本架构。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 习语翻译在现代机器翻译系统中仍是一个主要挑战，尤其是在语音到文本系统中，相关研究较少。本文旨在系统评估习语翻译的表现。

Method: 在德英和俄英两种语言对上，比较了最先进的端到端语音到文本系统、机器翻译系统和大语言模型的表现。

Result: 语音到文本系统在习语翻译上表现显著较差，常采用字面翻译，而机器翻译和大语言模型表现更优。

Conclusion: 语音到文本系统需要针对习语的优化策略和改进的内部表示。

Abstract: Idioms are defined as a group of words with a figurative meaning not
deducible from their individual components. Although modern machine translation
systems have made remarkable progress, translating idioms remains a major
challenge, especially for speech-to-text systems, where research on this topic
is notably sparse. In this paper, we systematically evaluate idiom translation
as compared to conventional news translation in both text-to-text machine
translation (MT) and speech-to-text translation (SLT) systems across two
language pairs (German to English, Russian to English). We compare
state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large
v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large
Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal
that SLT systems experience a pronounced performance drop on idiomatic data,
often reverting to literal translations even in higher layers, whereas MT
systems and Large Language Models demonstrate better handling of idioms. These
findings underscore the need for idiom-specific strategies and improved
internal representations in SLT architectures.

</details>


### [94] [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/abs/2506.02998)
*Đorđe Klisura,Astrid R Bernaga Torres,Anna Karen Gárate-Escamilla,Rajesh Roshan Biswal,Ke Yang,Hilal Pataci,Anthony Rios*

Key words: 隐私政策,问答系统,方言偏见,多智能体框架,公平访问

TL;DR: 提出了一个多智能体框架，通过整合方言智能体和隐私政策智能体，减少隐私政策问答系统中的方言偏见，无需重新训练或特定方言微调。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 隐私政策的复杂性限制了多元人群的可访问性，且现有隐私政策问答系统在英语方言间存在性能差异，影响非标准方言使用者。

Method: 采用多智能体框架，包括方言智能体（将查询翻译为标准英语）和隐私政策智能体（利用领域知识优化预测）。

Result: 在PrivacyQA和PolicyQA上的零样本准确率分别从0.394提升至0.601和从0.352提升至0.464，优于或少样本基线的性能。

Conclusion: 结构化智能体协作有效减少方言偏见，强调了设计考虑语言多样性的NLP系统以确保公平获取隐私信息的重要性。

Abstract: Privacy policies inform users about data collection and usage, yet their
complexity limits accessibility for diverse populations. Existing Privacy
Policy Question Answering (QA) systems exhibit performance disparities across
English dialects, disadvantaging speakers of non-standard varieties. We propose
a novel multi-agent framework inspired by human-centered design principles to
mitigate dialectal biases. Our approach integrates a Dialect Agent, which
translates queries into Standard American English (SAE) while preserving
dialectal intent, and a Privacy Policy Agent, which refines predictions using
domain expertise. Unlike prior approaches, our method does not require
retraining or dialect-specific fine-tuning, making it broadly applicable across
models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves
GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from
0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without
additional training data. These results highlight the effectiveness of
structured agent collaboration in mitigating dialect biases and underscore the
importance of designing NLP systems that account for linguistic diversity to
ensure equitable access to privacy information.

</details>


### [95] [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)
*Florian Ludwig,Torsten Zesch,Frederike Zufall*

Key words: 法律评估, 大型语言模型, 抽象层次, 仇恨言论, 德国刑法

TL;DR: 本文研究了如何在不同法律抽象层次上调整大型语言模型（LLMs）以检测可惩罚的仇恨言论，发现在分类社交媒体帖子是否符合德国刑法规定的煽动仇恨罪时，模型与法律专家仍存在显著性能差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估法律问题需要考虑具体的法律体系及其抽象层次，但目前尚不清楚大型语言模型（LLMs）如何内化这些法律体系。本文旨在探究如何在不同抽象层次上调整LLMs，以检测潜在的仇恨言论。

Method: 研究了多种调整LLMs的方法，以在不同抽象层次的法律体系中分类社交媒体帖子是否构成德国刑法规定的煽动仇恨罪。

Result: 模型与法律专家在仇恨言论的法律评估上仍存在显著性能差距。基于抽象法律知识的模型缺乏深入的任务理解，常自相矛盾或产生幻觉答案；而基于具体法律知识的模型在识别相关目标群体上表现较好，但对目标行为的分类仍有困难。

Conclusion: 当前LLMs在法律评估任务中的性能有限，尤其是在处理复杂的法律概念时，需要进一步改进以接近专家水平。

Abstract: The assessment of legal problems requires the consideration of a specific
legal system and its levels of abstraction, from constitutional law to
statutory law to case law. The extent to which Large Language Models (LLMs)
internalize such legal systems is unknown. In this paper, we propose and
investigate different approaches to condition LLMs at different levels of
abstraction in legal systems. This paper examines different approaches to
conditioning LLMs at multiple levels of abstraction in legal systems to detect
potentially punishable hate speech. We focus on the task of classifying whether
a specific social media posts falls under the criminal offense of incitement to
hatred as prescribed by the German Criminal Code. The results show that there
is still a significant performance gap between models and legal experts in the
legal assessment of hate speech, regardless of the level of abstraction with
which the models were conditioned. Our analysis revealed, that models
conditioned on abstract legal knowledge lacked deep task understanding, often
contradicting themselves and hallucinating answers, while models using concrete
legal knowledge performed reasonably well in identifying relevant target
groups, but struggled with classifying target conducts.

</details>


### [96] [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/abs/2506.03011)
*Aditya Bharat Soni,Boxuan Li,Xingyao Wang,Valerie Chen,Graham Neubig*

Key words: 通用代理,OpenHands-Versa,多任务,泛化能力

TL;DR: 研究探讨了通用代理OpenHands-Versa，使用少量通用工具（代码编辑、网络搜索等）在多任务中表现优于专用代理。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前AI代理多为专用型，难以泛化到其他任务。研究旨在探索如何用最少通用工具实现高性能。

Method: 开发OpenHands-Versa代理，整合代码编辑、网络搜索、多模态浏览等通用工具。

Result: OpenHands-Versa在多个基准测试中表现优于专用代理，如SWE-Bench Multimodal等。

Conclusion: 通用代理可行，OpenHands-Versa为未来研究提供了强基线。

Abstract: Modern human labor is characterized by specialization; we train for years and
develop particular tools that allow us to perform well across a variety of
tasks. In addition, AI agents have been specialized for domains such as
software engineering, web navigation, and workflow automation. However, this
results in agents that are good for one thing but fail to generalize beyond
their intended scope. One reason for this is that agent developers provide a
highly specialized set of tools or make architectural decisions optimized for a
specific use case or benchmark. In this work, we ask the question: what is the
minimal set of general tools that can be used to achieve high performance
across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist
agent built with a modest number of general tools: code editing and execution,
web search, as well as multimodal web browsing and file access. Importantly,
OpenHands-Versa demonstrates superior or competitive performance over leading
specialized agents across three diverse and challenging benchmarks: SWE-Bench
Multimodal, GAIA, and The Agent Company, outperforming the best-performing
previously published results with absolute improvements in success rate of 9.1,
1.3, and 9.1 points respectively. Further, we show how existing
state-of-the-art multi-agent systems fail to generalize beyond their target
domains. These results demonstrate the feasibility of developing a generalist
agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline
for future research.

</details>


### [97] [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)
*Pierre Lepagnol,Sahar Ghannay,Thomas Gerald,Christophe Servan,Sophie Rosset*

Key words: spoken language understanding, information retrieval, large language models, few-shot learning

TL;DR: 研究提出了一种利用信息检索（IR）方法选择示例，增强提示以提升口语语言理解（SLU）任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前最先进的SLU技术依赖大量训练数据，但特定任务或语言的标注数据有限。指令调优的大语言模型（LLMs）在少量示例条件下表现优异。

Method: 通过信息检索方法选择示例，构建增强提示，应用于SLU任务。

Result: 实验结果表明，基于词汇的IR方法显著提升了性能，且未增加提示长度。

Conclusion: 提出的方法在多个SLU基准测试中有效，展示了IR在提升SLU任务中的潜力。

Abstract: Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.

</details>


### [98] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
*Jintian Shao,Yiming Cheng*

Key words: 强化学习, 大语言模型, 长链推理, 价值函数, 信用分配

TL;DR: RL增强LLMs在复杂长链推理中的表现，但VAPO框架在建模和利用长期价值方面存在理论局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨VAPO框架在长链推理中的局限性，尤其是信用分配、价值函数表示能力和稀疏奖励下的全局信号转化问题。

Method: 理论分析VAPO框架的局限性，包括信用分配和长期价值建模的挑战。

Result: 揭示了VAPO框架在长期价值建模中的边界，为未来研究提供方向。

Conclusion: VAPO框架在长链推理中的局限性需要通过新方法解决，以提升LLM代理的鲁棒性。

Abstract: Reinforcement learning (RL) enhances large language models (LLMs) in complex,
long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,
despite sophisticated mechanisms like Decoupled GAE, theoretically faces
fundamental limitations in comprehensively modeling and leveraging deep,
long-term value for fine-grained, step-by-step policy guidance in extended
reasoning chains. We argue these limitations stem from inherent difficulties in
credit assignment, value function representational capacity with temporally
abstracted goals, and translating global value signals into local policy
improvements, especially with sparse rewards. Our theoretical analysis examines
these aspects to illuminate VAPO's boundaries in long-term value modeling,
aiming to deepen understanding of current RL for advanced reasoning and suggest
future research for more robust LLM agents.

</details>


### [99] [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)
*Yuval Kansal,Shmuel Berman,Lydia Liu*

Key words: 大型语言模型，教育工具，事实性，多语言性能，偏见

TL;DR: 评估Llama3.1模型在中高中学生事实问题回答中的表现，发现其不仅提供多余和不准确信息，还加剧了对稀有语言的偏见。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着大型语言模型（LLMs）在教育中的应用增加，确保其在不同语言中回答事实问题的正确性至关重要。目前对非英语语言的性能测试较少。

Method: 评估Llama3.1模型在中高中学生教育相关事实问题中的表现。

Result: LLMs不仅提供多余且不准确的信息，还加剧了对稀有语言的偏见。

Conclusion: LLMs在教育应用中的事实性问题仍需改进，尤其是在非英语语言中的表现。

Abstract: Factuality is a necessary precursor to useful educational tools. As adoption
of Large Language Models (LLMs) in education continues of grow, ensuring
correctness in all settings is paramount. Despite their strong English
capabilities, LLM performance in other languages is largely untested. In this
work, we evaluate the correctness of the Llama3.1 family of models in answering
factual questions appropriate for middle and high school students. We
demonstrate that LLMs not only provide extraneous and less truthful
information, but also exacerbate existing biases against rare languages.

</details>


### [100] [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/abs/2506.03090)
*Katherine Thai,Mohit Iyyer*

Key words: 长上下文语言模型；文学证据检索；RELiC数据集；Gemini Pro；开放权重模型

TL;DR: 现代长上下文语言模型在文学证据检索任务中表现如何？研究使用RELiC数据集构建基准，发现封闭权重模型（如Gemini Pro 2.5）超过人类专家表现，而开放权重模型表现较差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索现代长上下文语言模型对文学小说的理解能力，特别是在文学证据检索任务中的表现。

Method: 使用RELiC数据集构建基准，要求模型在全文本环境下生成缺失的文学引用，结合全局叙事推理和细节文本分析。

Result: Gemini Pro 2.5以62.5%的准确率超过人类专家（50%），而最佳开放权重模型仅达到29.1%。

Conclusion: 模型在文学分析中仍面临挑战，如对细微文学信号的识别和过度生成问题。

Abstract: How well do modern long-context language models understand literary fiction?
We explore this question via the task of literary evidence retrieval,
repurposing the RELiC dataset of That et al. (2022) to construct a benchmark
where the entire text of a primary source (e.g., The Great Gatsby) is provided
to an LLM alongside literary criticism with a missing quotation from that work.
This setting, in which the model must generate the missing quotation, mirrors
the human process of literary analysis by requiring models to perform both
global narrative reasoning and close textual examination. We curate a
high-quality subset of 292 examples through extensive filtering and human
verification. Our experiments show that recent reasoning models, such as Gemini
Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In
contrast, the best open-weight model achieves only 29.1% accuracy, highlighting
a wide gap in interpretive reasoning between open and closed-weight models.
Despite their speed and apparent accuracy, even the strongest models struggle
with nuanced literary signals and overgeneration, signaling open challenges for
applying LLMs to literary analysis. We release our dataset and evaluation code
to encourage future work in this direction.

</details>


### [101] [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/abs/2506.03101)
*Jonas F. Lotz,António V. Lopes,Stephan Peitz,Hendra Setiawan,Leonardo Emili*

Key words: 分词器, 语言模型, 多语言评估, Zipf定律

TL;DR: 研究表明，分词器选择对小模型的性能预测有效，但对多语言任务影响较大，并提出新的评估指标。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决分词器选择对语言模型性能影响评估的挑战。

Method: 通过小模型预测分词器对大模型的影响，并评估多语言分词器性能，提出基于Zipf定律的新指标。

Result: 分词器对英语任务影响小，但对多语言任务影响显著；新指标优于传统压缩方法。

Conclusion: 提出高效的分词器评估框架，为未来模型开发提供指导。

Abstract: The choice of tokenizer can profoundly impact language model performance, yet
accessible and reliable evaluations of tokenizer quality remain an open
challenge. Inspired by scaling consistency, we show that smaller models can
accurately predict significant differences in tokenizer impact on larger models
at a fraction of the compute cost. By systematically evaluating both
English-centric and multilingual tokenizers, we find that tokenizer choice has
negligible effects on tasks in English but results in consistent performance
differences in multilingual settings. We propose new intrinsic tokenizer
metrics inspired by Zipf's law that correlate more strongly with downstream
performance than text compression when modeling unseen languages. By combining
several metrics to capture multiple aspects of tokenizer behavior, we develop a
reliable framework for intrinsic tokenizer evaluations. Our work offers a more
efficient path to informed tokenizer selection in future language model
development.

</details>


### [102] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
*Xiaoying Zhang,Hao Sun,Yipeng Zhang,Kaituo Feng,Chao Yang,Helen Meng*

Key words: 强化学习, 自然语言反馈, 语言模型, 性能优化, 探索策略

TL;DR: 该论文提出了一种结合自然语言和数值反馈的在线强化学习框架Critique-GRPO，显著提升了LLMs在复杂推理任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究发现仅依靠数值反馈的强化学习存在性能瓶颈、自我反思效果有限和持续失败等问题，因此探索如何通过自然语言反馈提升模型表现。

Method: 提出了Critique-GRPO框架，整合自然语言和数值反馈，使模型能同时从初始响应和基于批评的改进中学习，同时保持探索性。

Result: 在多个复杂推理任务中，Critique-GRPO优于监督学习和RL微调方法，平均pass@1分数分别提升了4.5%和5%。

Conclusion: 自然语言反馈能有效提升RL模型的性能，且探索策略中高熵和长响应未必能带来更高效的学习。

Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such
as scalar rewards, have significantly enhanced the complex reasoning
capabilities of large language models (LLMs). Despite this success, we identify
three key challenges encountered by RL with solely numerical feedback:
performance plateaus, limited effectiveness of self-reflection, and persistent
failures. We then demonstrate that RL-finetuned models, even after exhibiting
performance plateaus, can generate correct refinements on persistently failed
problems by leveraging natural language feedback in the form of critiques.
Building on this insight, we propose Critique-GRPO, an online RL framework that
integrates both natural language and numerical feedback for effective policy
optimization. Critique-GRPO enables LLMs to learn from initial responses and
critique-guided refinements simultaneously while maintaining exploration.
Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that
Critique-GRPO consistently outperforms supervised learning-based and RL-based
fine-tuning approaches across eight challenging mathematical, STEM, and general
reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,
respectively. Notably, Critique-GRPO surpasses a strong baseline that
incorporates expert demonstrations within online RL. Further analysis reveals
two critical insights about policy exploration: (1) higher entropy does not
always guarantee efficient learning from exploration, and (2) longer responses
do not necessarily lead to more effective exploration.

</details>


### [103] [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/abs/2506.03122)
*Prashanth Vijayaraghavan,Luyao Shi,Ehsan Degan,Vandana Mukherjee,Xin Zhang*

Key words: 模拟电路、EDA、大型语言模型、强化学习、自动化设计

TL;DR: AUTOCIRCUIT-RL利用大型语言模型和强化学习，实现了自动化模拟电路拓扑合成，生成更多有效电路且效率更高。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 模拟电路拓扑合成的设计空间大且约束严格，需要高效的自动化解决方案。

Method: 分两阶段：指令调整让LLM从设计约束提示生成拓扑，强化学习通过奖励模型进一步优化。

Result: 生成有效性提升12%，效率提高14%，重复率降低38%，训练数据有限时成功率超60%。

Conclusion: AUTOCIRCUIT-RL在复杂电路设计中高效且满足约束，是AI驱动设计的重大进步。

Abstract: Analog circuit topology synthesis is integral to Electronic Design Automation
(EDA), enabling the automated creation of circuit structures tailored to
specific design requirements. However, the vast design search space and strict
constraint adherence make efficient synthesis challenging. Leveraging the
versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel
reinforcement learning (RL)-based framework for automated analog circuit
synthesis. The framework operates in two phases: instruction tuning, where an
LLM learns to generate circuit topologies from structured prompts encoding
design constraints, and RL refinement, which further improves the
instruction-tuned model using reward models that evaluate validity, efficiency,
and output voltage. The refined model is then used directly to generate
topologies that satisfy the design constraints. Empirical results show that
AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by
~14% compared to the best baselines, while reducing duplicate generation rates
by ~38%. It achieves over 60% success in synthesizing valid circuits with
limited training data, demonstrating strong generalization. These findings
highlight the framework's effectiveness in scaling to complex circuits while
maintaining efficiency and constraint adherence, marking a significant
advancement in AI-driven circuit design.

</details>


### [104] [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)
*Yinjie Wang,Ling Yang,Ye Tian,Ke Shen,Mengdi Wang*

Key words: 强化学习, 代码生成, 单元测试, 无监督学习

TL;DR: CURE是一个新颖的强化学习框架，通过奖励设计共同进化代码和单元测试生成能力，无需真实代码监督。其优化的模型在代码生成和测试任务中表现优于同类模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在通过无监督的强化学习方法，提升代码生成和单元测试的效率与准确性。

Method: 采用奖励设计机制，让编码器和测试器通过交互共同进化，直接从错误中学习。

Result: 优化后的模型在代码生成准确性上提升5.3%，Best-of-N准确性提升9.0%，并在下游任务中表现优异。

Conclusion: CURE框架及其衍生模型在代码生成和测试任务中表现卓越，还能作为强化学习的奖励模型。

Abstract: We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE

</details>


### [105] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
*Qianhui Wu,Kanzhi Cheng,Rui Yang,Chaoyun Zhang,Jianwei Yang,Huiqiang Jiang,Jian Mu,Baolin Peng,Bo Qiao,Reuben Tan,Si Qin,Lars Liden,Qingwei Lin,Huan Zhang,Tong Zhang,Jianbing Zhang,Dongmei Zhang,Jianfeng Gao*

Key words: GUI定位, 视觉语言模型, 注意力机制, 坐标无关, 动作区域

TL;DR: GUI-Actor是一种基于视觉语言模型（VLM）的无坐标GUI定位方法，通过注意力机制改进视觉与语义的对齐，解决了现有方法的局限性，并在多个基准测试中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有GUI定位方法在空间语义对齐、模糊监督目标和视觉特征粒度方面存在不足，为此提出GUI-Actor以改进这些问题。

Method: 引入注意力动作头和对齐机制，通过<ACTOR>令牌与视觉令牌对齐，生成动作区域候选，并设计验证器选择最优区域。

Result: 在多个GUI定位基准测试中超越现有方法，GUI-Actor-7B在ScreenSpot-Pro中得分40.7（Qwen2-VL）和44.6（Qwen2.5-VL）。

Conclusion: GUI-Actor通过轻量级改进赋予VLM强大的定位能力，同时保留其通用性。

Abstract: One of the principal challenges in building VLM-powered GUI agents is visual
grounding, i.e., localizing the appropriate screen region for action execution
based on both the visual content and the textual plans. Most existing work
formulates this as a text-based coordinate generation task. However, these
approaches suffer from several limitations: weak spatial-semantic alignment,
inability to handle ambiguous supervision targets, and a mismatch between the
dense nature of screen coordinates and the coarse, patch-level granularity of
visual features extracted by models like Vision Transformers. In this paper, we
propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its
core, GUI-Actor introduces an attention-based action head that learns to align
a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the
model to propose one or more action regions in a single forward pass. In line
with this, we further design a grounding verifier to evaluate and select the
most plausible action region from the candidates proposed for action execution.
Extensive experiments show that GUI-Actor outperforms prior state-of-the-art
methods on multiple GUI action grounding benchmarks, with improved
generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B
even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7
with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by
incorporating the verifier, we find that fine-tuning only the newly introduced
action head (~100M parameters for 7B model) while keeping the VLM backbone
frozen is sufficient to achieve performance comparable to previous
state-of-the-art models, highlighting that GUI-Actor can endow the underlying
VLM with effective grounding capabilities without compromising its
general-purpose strengths.

</details>


### [106] [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Key words: 知识图谱,大型语言模型,神经科学,无标记数据,信息检索

TL;DR: 提出了一种利用大型语言模型、神经科学本体和文本嵌入从无标记的大规模神经科学研究语料库构建知识图谱的新方法，显著提升了知识发现能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 神经科学领域的知识分散在多个来源，现有检索方法难以有效提取信息，且构建知识图谱通常依赖标记数据和领域专家，获取大规模标记数据具有挑战性。

Method: 利用大型语言模型（LLM）、神经科学本体和文本嵌入，分析神经科学文本片段的语义相关性以构建知识图谱，并引入实体增强的信息检索算法。

Result: 实体提取的F1得分为0.84，从知识图谱中提取的知识改善了54%以上问题的回答。

Conclusion: 提出的方法显著提高了从无标记神经科学研究语料库中知识发现的效果，验证了其在神经科学领域的潜力。

Abstract: Neuroscience research publications encompass a vast wealth of knowledge.
Accurately retrieving existing information and discovering new insights from
this extensive literature is essential for advancing the field. However, when
knowledge is dispersed across multiple sources, current state-of-the-art
retrieval methods often struggle to extract the necessary information. A
knowledge graph (KG) can integrate and link knowledge from multiple sources,
but existing methods for constructing KGs in neuroscience often rely on labeled
data and require domain expertise. Acquiring large-scale, labeled data for a
specialized area like neuroscience presents significant challenges. This work
proposes novel methods for constructing KG from unlabeled large-scale
neuroscience research corpus utilizing large language models (LLM),
neuroscience ontology, and text embeddings. We analyze the semantic relevance
of neuroscience text segments identified by LLM for building the knowledge
graph. We also introduce an entity-augmented information retrieval algorithm to
extract knowledge from the KG. Several experiments were conducted to evaluate
the proposed approaches, and the results demonstrate that our methods
significantly enhance knowledge discovery from the unlabeled neuroscience
research corpus. It achieves an F1 score of 0.84 for entity extraction, and the
knowledge obtained from the KG improves answers to over 54% of the questions.

</details>


### [107] [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)
*Pietro Lesci,Clara Meister,Thomas Hofmann,Andreas Vlachos,Tiago Pimentel*

Key words: 语言模型、分词偏差、因果效应、回归断点设计、子词

TL;DR: 论文探讨了现代语言模型中因分词器选择导致的概率分配偏差，即'分词偏差'，并通过因果效应估计方法量化了其影响。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机是解决分词器（将字符序列映射为子词的组件）的选择如何影响模型对字符序列的概率分配问题。理想情况下，分词器的选择不应影响字符序列的概率分配，但在实践中存在偏差。

Method: 方法是将分词偏差定义为因果效应，并利用回归断点设计（RDD）进行估计。通过比较分词器词汇表中阈值附近的子词，量化了分词偏差的影响。

Result: 实验结果表明，分词操作在不同规模、词汇表和分词器的模型中均显著影响输出。例如，一个小型模型中子词的存在可能导致其对应字符概率增加高达17倍。

Conclusion: 结论是分词器的选择是语言模型设计中的关键因素，分词偏差的存在需要在模型开发和评估中被重视。

Abstract: Modern language models are typically trained over subword sequences, but
ultimately define probabilities over character-strings. Ideally, the choice of
the tokeniser -- which maps character-strings to subwords -- should not affect
the probability assigned to the underlying character-string; in practice, it
does. We define this mismatch as tokenisation bias. In this work, we quantify
one particular type of tokenisation bias: the effect of including or not a
subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the
probability a trained model assigns to the corresponding characters (i.e.,
\textit{``hello''}). Estimating this effect is challenging because each model
is trained with only one tokeniser. We address this by framing tokenisation
bias as a causal effect and estimating it using the regression discontinuity
design. Specifically, we exploit the fact that tokenisation algorithms rank
subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an
arbitrary cutoff point. As such, we can estimate a causal effect by comparing
similar subwords around this cutoff. Experimentally, we find that tokenisation
consistently affects models' outputs across scales, vocabularies, and
tokenisers. Notably, a subword's presence in a small model's vocabulary may
increase its characters' probability by up to 17 times, highlighting
tokenisation as a key design choice in language modelling.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [108] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/abs/2506.01959)
*Irmi Schneider*

Key words: 对称性, 神经网络, 临界点, 优化问题, 度量方法

TL;DR: 论文研究了不同空间中的对称性现象，扩展了神经网络中权重对称性的研究，并在其他四种案例中发现了临界点的非平凡对称性，同时引入了一种新的对称性度量方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨对称性在数学结构和优化问题中的重要作用，特别是扩展了神经网络中权重对称性的研究，试图理解更广泛空间中的对称性现象。

Method: 研究了四种案例（有限域上的投影案例、八面体图案例、完美匹配案例和粒子吸引案例），并分析了这些案例中临界点的对称性。同时引入了一种新的对称性度量方法。

Result: 所有分析的案例中，临界点均表现出非平凡对称性。新引入的对称性度量方法揭示了之前方法未能捕捉到的额外对称性结构。

Conclusion: 对称性是优化问题中普遍存在的现象，新提出的对称性度量方法为进一步研究对称性提供了工具。

Abstract: Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [109] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/abs/2506.01962)
*Xiaozhou Ye,Kevin I-Kai Wang*

Key words: 传感器人机交互识别, 跨用户泛化, 图神经网络, 对抗学习

TL;DR: 提出GNN-ADG方法，通过图神经网络和对抗学习解决传感器人机交互识别中的跨用户泛化问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传感器人机交互识别中因用户行为、传感器放置和数据分布差异导致的跨用户泛化挑战。

Method: 基于GNN和对抗学习，构建三种解剖单元（互联、类比和侧边单元）的统一图结构，通过循环训练策略动态融合空间、功能和侧边相关性。

Result: GNN-ADG无需目标用户数据即可学习泛化特征，适用于现实应用。

Conclusion: GNN-ADG有效解决了跨用户泛化问题，具有实际应用价值。

Abstract: Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [110] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Key words: 大型语言模型,非注意力机制,状态空间块,多分辨率卷积,检索增强记忆

TL;DR: 提出了一种新型非注意力机制的大型语言模型架构，高效处理超长上下文窗口。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统Transformer因自注意力机制导致二次方内存和计算开销，难以处理超长上下文。

Method: 结合状态空间块、多分辨率卷积层、轻量级循环监督器和检索增强外部记忆，避免令牌间注意力。

Result: 模型能够高效处理数十万至数百万令牌的上下文窗口。

Conclusion: 该架构为非注意力机制LLM提供了一种高效且可扩展的解决方案。

Abstract: We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [111] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/abs/2506.01964)
*Kamal Acharya,Mehul Lad,Liang Sun,Houbing Song*

Key words: 交通规划,重力模型,机器学习,数据驱动,预测准确性

TL;DR: 研究提出了一种数据驱动方法，通过结合地理、经济、社会和旅行数据，利用机器学习技术增强传统重力模型，显著提高了交通预测的准确性和解释力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统的重力模型在反映现代出行行为的复杂因素时表现不足，研究旨在通过数据驱动的方法提升模型的预测能力和可靠性。

Method: 结合田纳西州和纽约州的多种数据（地理、经济、社会、旅行数据），使用机器学习技术扩展传统重力模型。

Result: R平方提升51.48%，平均绝对误差减少63.59%，通勤共同部分增加44.32%，显著优化了模型性能。

Conclusion: 研究表明，结合多样化数据和高级算法能显著提升交通模型的预测能力，为城市规划者提供更可靠的工具。

Abstract: Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [112] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/abs/2506.01965)
*Bonpagna Kann,Sandra Castellanos-Paez,Romain Rombourg,Philippe Lalanda*

Key words: 持续学习,变分自编码器,人体活动识别,重放方法

TL;DR: TaskVAE是一种基于重放的持续学习方法，通过任务特定VAE生成合成样本，解决动态数据环境中的模型适应问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 机器学习系统需适应动态数据环境，但数据偏移（如渐变、突变或周期性变化）威胁模型准确性。持续学习（CL）需平衡新旧知识的保留。

Method: 使用任务特定变分自编码器（VAE）生成合成样本，结合新任务数据训练分类器，无需预知总类别数且内存占用低。

Result: 在5个人体活动识别（HAR）数据集上优于传统方法，尤其数据有限时表现更优，内存占用量仅为每任务60样本。

Conclusion: TaskVAE在内存限制、任务特定生成和长期稳定性间取得平衡，适用于HAR等现实场景。

Abstract: As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [113] [Matrix Is All You Need](https://arxiv.org/abs/2506.01966)
*Yuzhou Zhu*

Key words: 深度神经网络, 矩阵框架, 稀疏矩阵乘法, CNN, RNN, Transformer

TL;DR: 论文提出了一个统一的矩阵框架，将卷积、循环和自注意力操作视为稀疏矩阵乘法，证明了与标准CNN、RNN和Transformer层的代数同构，并通过实验验证了其性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络在不同任务中使用不同架构，但其底层共性被忽视。希望通过矩阵框架统一这些操作，简化设计并提升性能。

Method: 将卷积、循环和自注意力操作化为稀疏矩阵乘法。卷积对应上三角矩阵，循环对应下三角矩阵，注意力则为三阶张量分解。

Result: 在图像分类、时间序列预测和语言建模任务中，稀疏矩阵形式的性能匹配或优于原生模型，且收敛更快。

Conclusion: 矩阵框架为多样化的神经网络架构提供了数学基础，并支持硬件感知的网络设计。

Abstract: Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [114] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Key words: 量化、大语言模型、通道级别缩放、旋转技术、异常值

TL;DR: 论文提出了一种新的量化方法，通过引入通道级别的缩放和旋转技术，减少了大语言模型中异常值对量化误差的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大语言模型的量化虽然可以降低服务成本，但权值和激活值的量化中存在显著的异常值，增加了量化误差。研究旨在解决这一问题。

Method: 提出了一种基于通道级别缩放和旋转的混合方法，并引入新的度量标准以可视化和测量量化难度。

Result: 该方法有效地减少了量化误差，优化了大语言模型的量化效果。

Conclusion: 通过通道级别的缩放和旋转技术，可以显著降低量化误差，为大语言模型的高效服务提供了新思路。

Abstract: Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [115] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/abs/2506.01968)
*Chang Liu,Jiangrong Shen,Xuming Ran,Mingkun Xu,Qi Xu,Yi Xu,Gang Pan*

Key words: 人工神经网络（ANNs）、脉冲神经网络（SNNs）、误差补偿学习、双阈值神经元、膜电位初始化

TL;DR: 提出了一种基于误差补偿学习的新型ANN-to-SNN转换框架，通过可学习的阈值裁剪、双阈值神经元和优化的膜电位初始化策略，显著减少了转换误差，实现了高精度和超低延迟。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 人工神经网络（ANNs）在资源受限环境中部署时面临高计算和内存需求的挑战，脉冲神经网络（SNNs）因其节能特性成为替代方案，但现有ANN-to-SNN转换方法存在精度损失和推理时间增加的问题。

Method: 提出了一种基于误差补偿学习的框架，包括可学习的阈值裁剪函数、双阈值神经元和优化的膜电位初始化策略，以解决剪切、量化和不均匀激活误差。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上的实验表明，该方法以仅两个时间步实现了高精度和超低延迟，在ResNet-18结构下CIFAR-10的准确率达到94.75%。

Conclusion: 该研究推动了SNNs在低功耗硬件上的实际应用，为高效实时处理提供了可能。

Abstract: Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [116] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song,Beiming Yuan*

Key words: 抽象推理、RPM任务、Johnny架构、Spin-Transformer网络、表征空间

TL;DR: 本文分析了传统RPM任务模型的局限性，并提出了一种新型架构Johnny和Spin-Transformer网络，显著提升了AI的抽象推理能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统RPM任务模型依赖于选项池配置，限制了模型的推理能力，亟需新的方法来解决这一问题。

Method: 提出了Johnny架构，包括表征提取模块和推理模块，以及Spin-Transformer网络及其轻量级变体。

Result: 实验证明Johnny和Spin-Transformer在RPM任务上表现优越，推动了AI抽象推理能力的发展。

Conclusion: 提出的架构为提升AI抽象推理能力提供了创新方法，具有重要研究价值。

Abstract: This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [117] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/abs/2506.01974)
*Kanwal Aalijah*

Key words: 城市交通、AI、情感分析、拥堵预测、中东地区

TL;DR: AI技术结合实时交通数据和情感分析优化城市交通规划。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 快速城市化导致交通拥堵问题加剧，影响居民生活质量和城市可持续发展。

Method: 结合实时交通数据和地理定位的情感分析，利用AI模型和探索性数据分析预测拥堵模式、分析通勤行为。

Result: 提出优化交通流、改善通勤体验及针对性解决中东等地城市交通问题的建议。

Conclusion: AI为动态和全面的城市交通规划提供了有效工具。

Abstract: Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [118] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/abs/2506.01975)
*Jama Hussein Mohamud*

Key words: 预训练网络、神经网络复用、任务相关性、语义相关、优化器

TL;DR: 本文通过实验研究发现，预训练神经网络的复用效果与任务之间的相关性密切相关，任务相关性高时复用效果更好；即使任务无关，仍可能因网络和优化器的选择获得优于随机的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究预训练神经网络复用成功的因素，理解任务相关性对复用效果的影响。

Method: 设计实验框架，分析任务相关性、网络层选择和优化器对复用效果的作用。

Result: 任务相关性高时复用效果显著；任务无关时仍可能因网络和优化器选择获得较好表现；语义相关的任务复用效果更佳。

Conclusion: 预训练网络的复用效果取决于任务相关性，语义相关任务复用更有效。

Abstract: Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [119] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/abs/2506.01976)
*Elham Kiyani,Venkatesh Ananchaperumal,Ahmad Peyvan,Mahendaran Uchimali,Gang Li,George Em Karniadakis*

Key words: 裂纹扩展, DeepONet, CPD模拟, 几何变化, 时间依赖性

TL;DR: 利用DeepONet模型预测裂纹扩展，发现Fusion DeepONet在几何变化和时间依赖性问题上表现优于传统版本。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 准确建模裂纹扩展对预测工程材料和结构的失效至关重要，尤其是裂纹与孔洞等不连续性的相互作用。

Method: 通过CPD模拟数据训练DeepONet（包括vanilla和Fusion变体），预测几何变化时的裂纹扩展。

Result: Fusion DeepONet在非断裂案例中表现更优，但断裂驱动场景仍具挑战性。

Conclusion: Fusion DeepONet在复杂几何和时间依赖性裂纹扩展问题中展现出潜在泛化能力。

Abstract: Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [120] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/abs/2506.01977)
*Wei Huang,Hanchen Wang,Dong Wen,Shaozhen Ma,Wenjie Zhang,Xuemin Lin*

Key words: 图编辑距离，无监督学习，GAN，节点匹配

TL;DR: 本文提出了一种无监督的GAN框架GEDRanker，用于计算图编辑距离（GED），无需依赖昂贵的真实标签，通过设计可解释的判别器和训练策略，实现高质量的节点匹配。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法依赖真实标签，但实际场景中获取成本高；本文提出无监督框架以解决这一限制。

Method: GEDRanker结合匹配式GED求解器和偏好感知判别器，通过有效训练策略生成高质量节点匹配。

Result: 在基准数据集上，GEDRanker使匹配式求解器无需真实标签即可获得接近最优解的质量。

Conclusion: GEDRanker是一种高效的无监督GED计算方法，解决了真实标签依赖问题。

Abstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [121] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/abs/2506.01983)
*Reyhaneh Keshavarzpour,Eghbal Mansoori*

Key words: 抗菌肽，深度神经网络，预测方法，人工智能，抗生素替代

TL;DR: 论文提出了一种改进的深度神经网络方法，用于预测抗菌肽，通过结合最佳编码方法和处理不平衡数据集，显著提高了预测的准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 抗菌肽作为抗生素的替代品在生物医学和其他实际应用中具有重要意义，识别抗菌肽是当前的重要课题。人工智能算法在其识别中发挥了重要作用，但需要进一步改进预测方法。

Method: 结合不同视角的最佳编码方法，并使用深度神经网络处理不平衡数据集，改进抗菌肽的预测方法。

Result: 提出的方法在抗菌肽预测的准确性和效率上有显著提升，优于现有方法。

Conclusion: 该方法在抗菌肽的预测和分类领域，尤其是在医学和制药行业，具有高度有效性和广泛的应用潜力。

Abstract: Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [122] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)
*Selin Yildirim,Deming Chen*

Key words: 推测解码, 内存优化, 设备感知, 分布式推理, 大语言模型

TL;DR: 论文提出了一种名为SpecMemo的设备感知推理引擎，通过精细控制内存分配，使内存受限设备能够支持带推测解码的多轮聊天机器人，并显著提升推理速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 推测解码虽能加速大语言模型任务，但在内存受限设备（如移动GPU）上部署仍具挑战。本研究旨在解决这一问题。

Method: 通过理论建模推测解码的内存占用，确定最低内存需求并优化分配。结合分布式和批量推测解码，提升多小GPU上的大模型推理效率。

Result: SpecMemo在MT-Bench上保留了96%的推测解码吞吐量，单卡内存减少65%；在八块AMD MI250 GPU上，推理速度提升2倍，批量处理时速度提升8倍。

Conclusion: 该研究为资源受限环境中的LLM应用提供了高效部署方案。

Abstract: Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [123] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/abs/2506.01987)
*Runkang Yang,Peng Sun,Xinyi Shang,Yi Tang,Tao Lin*

Key words: 知识蒸馏、数据高效学习、样本优化、目标优化、训练动态

TL;DR: 研究探讨了数据和目标的交互对训练动态的影响，提出了统一损失框架，并通过实证分析提供了六项关键见解以提升训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有研究忽视目标优化，导致样本与目标之间交互的影响未被充分探索。研究旨在填补这一空白。

Method: 建立样本-目标交互的分类法，并提出统一损失框架进行实证分析。

Result: 分析了目标与样本类型、数量和质量对训练的影响，得出六项关键见解。

Conclusion: 样本与目标的交互对训练效率至关重要，提出的框架和见解有助于优化训练过程。

Abstract: Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [124] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/abs/2506.01988)
*Akshat Dubey,Aleksandar Anžel,Georges Hattab*

Key words: 健康信息学, 随机森林, 可解释性, 特征交互, 替代可解释性图

TL;DR: 该论文提出了一种称为替代可解释性图的方法，用于提高随机森林模型在健康信息学领域的全局可解释性，解决了特征数量增加导致的解释困难问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着随机森林模型在健康信息学中的应用，特征数量和估计器的增加使得全局特征交互的解释变得困难，影响了模型的可信度和合规性。

Method: 使用图形和混合整数线性规划来分析并可视化特征交互，通过展示每个决策-特征交互表中的特征使用情况以及预测中最主要的层次决策特征交互，提升可解释性。

Result: 替代可解释性图显著提高了模型的全局可解释性，适用于高风险领域。

Conclusion: 该方法有效解决了随机森林模型在健康信息学中的解释性问题，增强了模型的透明度和可信度。

Abstract: The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [125] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/abs/2506.01989)
*Chengxi Li,Ming Xiao,Mikael Skoglund*

Key words: 分布式学习, 拜占庭攻击, 鲁棒有界聚合规则, 编码鲁棒聚合

TL;DR: 本文提出了一种基于编码鲁棒聚合（CRA-DL）的新型分布式学习方法，以应对拜占庭攻击。通过冗余分配训练数据和编码梯度传输，提升了聚合的鲁棒性和学习性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的分布式学习方法在面对拜占庭攻击时，由于本地梯度差异较大，导致学习性能显著下降。为此，作者提出了一种新方法以解决这一问题。

Method: 在训练开始前，冗余分配训练数据；训练过程中，诚实设备传输基于分配数据的编码梯度，服务器使用鲁棒有界聚合规则（RBA）聚合信息，恢复全局梯度以更新模型。

Result: 理论分析和数值结果验证了CRA-DL方法的优越性，其在拜占庭攻击下表现出更强的学习性能。

Conclusion: CRA-DL通过编码梯度设计和冗余数据分配，显著提升了分布式学习在面对拜占庭攻击时的鲁棒性和效果。

Abstract: In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [126] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/abs/2506.02050)
*Qingyu Xiao,Yuanlin Chang,Youtian Du*

Key words: 强化学习、分层RL、状态抽象、探索效率、离散状态空间

TL;DR: 论文提出了一种分层强化学习框架（DcHRL-SA），通过状态抽象和双层架构解决复杂离散状态空间中的探索问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在部分可观测的复杂离散状态空间中，有效的智能体探索仍然是一个核心挑战，现有的方法表现不佳。

Method: 采用双层架构（高层RL演员和低层基于规则的策略）结合状态抽象方法，降低状态维度。

Result: 在两个离散定制网格环境中的实验表明，该方法在探索效率、收敛速度、累积奖励和策略稳定性上优于PPO。

Conclusion: 结合分层策略和状态抽象为大规模探索空间的离散网格提供了一种实用方法。

Abstract: Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [127] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/abs/2506.02053)
*Xu Zhang,Haoye Qiu,Weixuan Liang,Hui Liu,Junhui Hou,Yuheng Jia*

Key words: 集成聚类,泛化误差,超额风险,一致性,多样性

TL;DR: 本文研究了集成聚类的泛化性能，推导了泛化误差和超额风险的收敛率，证明了当样本和基聚类数量趋于无穷时的一致性，并提出了一种新的集成聚类算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 集成聚类在实践中表现优异，但其理论基础尚不完善，本文旨在填补这一空白，探究其泛化性能。

Method: 通过理论分析泛化误差和超额风险的收敛率，提出最小化基聚类的偏差和最大化多样性，并实例化为新算法。

Result: 新算法在10个数据集上平均提升了6.1%、7.3%和6.0%（NMI、ARI和Purity）。

Conclusion: 集成聚类的性能提升依赖于基聚类的偏差最小化和多样性最大化，理论结果在实验中得到了验证。

Abstract: Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [128] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/abs/2506.02062)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Key words: 指纹模式, ABO血型, 相关性分析, 法医学, 个人识别

TL;DR: 研究探讨指纹模式与ABO血型分类的关系，未发现显著相关性，但强调了未来研究的必要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统生物识别技术成本高且耗时，研究探索指纹与血型的潜在关联以优化身份识别。

Method: 分析200名参与者的指纹类型（斗形、环形、弓形）与血型，使用卡方检验和皮尔逊相关分析。

Result: 指纹模式与血型无显著相关性（p > 0.05），斗形指纹和O+血型最常见。

Conclusion: 研究未发现显著关联，但建议未来扩大样本多样性并引入机器学习方法。

Abstract: Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [129] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant,Krishna P. Miyapuram*

Key words: 持续学习,灾难性遗忘,弹性权重生成网络,上下文切换,动态权重生成

TL;DR: 论文提出了弹性权重生成网络（EWGN），通过动态生成权重和上下文切换来缓解神经网络的灾难性遗忘问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 受人类学习多样任务能力的启发，研究旨在通过持续学习实现人工智能的通用智能，解决任务变异和上下文切换对神经网络的挑战。

Method: 提出EWGN架构，利用附加网络动态生成主网络权重，实现输入依赖的上下文切换；在MNIST和fashion-MNIST数据集上测试不同网络架构和学习算法。

Result: 通过实验验证EWGN在保持先前任务表征方面的有效性。

Conclusion: 动态权重生成和上下文切换能力有助于提升持续学习性能。

Abstract: The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [130] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)
*Peter Holderrieth,Ezra Erives*

Key words: 扩散模型, 基于流的模型, 生成式AI, 微分方程, 流匹配

TL;DR: 本文介绍了扩散和基于流的模型，这些模型已成为生成式AI的最新技术，适用于多种数据类型。内容涵盖从微分方程到现代先进模型的理论与实践。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在为生成式AI的理论和实践提供一个系统的介绍，帮助学生和从业者深入理解扩散和基于流的模型。

Method: 课程笔记结合了微分方程、流匹配、分数匹配等方法，介绍了现代扩散模型的内部工作原理。

Result: 提供了一套完整的理论框架和实践指南，适用于生成式AI的多种应用场景。

Conclusion: 这些课程笔记和配套课程适合希望系统地学习生成式AI理论和技术的人。

Abstract: Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [131] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
*Yoonjun Cho,Soeun Kim,Dongjae Jeon,Kyelim Lee,Beomsoo Lee,Albert No*

Key words: 权重矩阵分解, 量化, 低秩近似, 大型语言模型, ODLRI

TL;DR: 提出了一种名为ODLRI的方法，通过结构化分解权重矩阵来优化大型语言模型的压缩效果，提升量化和低秩近似的平衡性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有联合优化方法在量化和低秩近似之间交替进行，但往往牺牲一方效果，导致分解效果不佳。

Method: 引入ODLRI方法，专门让低秩分量捕捉激活敏感的权重，结构化分解以减轻异常值对量化的负面影响。

Result: 在多个模型上实验验证，ODLRI显著减少了激活感知误差，降低了量化规模，提升了低比特设置下的困惑度和零样本准确性。

Conclusion: ODLRI通过平衡量化和低秩近似，有效提升了模型压缩效果。

Abstract: Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [132] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang,Tian Wen,Zhiqin Yang,Lvhua Wu,Yufeng Chen,Sheng Sun,Yuwei Wang,Min Liu*

Key words: 联邦学习、标签噪声、优化框架、标签校正、几何中值聚合

TL;DR: 提出了一种名为MaskedOptim的两阶段优化框架，用于解决联邦学习中的复杂标签噪声问题，通过检测高噪声客户端和标签校正机制提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中客户端提供的标注数据常含有复杂标签噪声，影响模型性能，需解决高噪声客户端带来的负面影响。

Method: 提出两阶段框架：第一阶段检测高噪声客户端，第二阶段通过端到端标签校正机制修正噪声数据标签，并使用几何中值聚合模型。

Result: 在多个数据集上的实验表明，该框架在不同场景下表现出鲁棒性，并能有效提升噪声客户端本地数据质量。

Conclusion: MaskedOptim框架能有效解决联邦学习中的标签噪声问题，提升模型性能和数据质量。

Abstract: In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [133] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)
*Chihiro Maru,Shoetsu Sato*

Key words: 时间序列基础模型, 测试时适应, 检索增强, 异常检测

TL;DR: 论文提出了一种基于检索增强的时间序列基础模型(RATFM)，旨在解决现有模型在测试时适应任务中的局限性，避免领域依赖的微调。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 时间序列基础模型在不同领域和任务中表现不一致，且缺乏解释或利用示例的能力，因此需要一种新方法来提升其适应性。

Method: 通过检索增强技术，使预训练的时间序列基础模型能够纳入测试时的示例，从而提高其表现。

Result: 在UCR Anomaly Archive多领域数据集上的实验表明，RATFM的性能接近领域内微调，但无需领域依赖的微调。

Conclusion: RATFM提供了一种高效的测试时适应方法，适用于多领域时间序列任务。

Abstract: Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [134] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/abs/2506.02084)
*Nikolaos Gkorgkolis,Nikolaos Kougioulis,MingXue Wang,Bora Caglayan,Andrea Tonon,Dario Simionato,Ioannis Tsamardinos*

Key words: 因果发现，时间序列，数据生成，TCS，Min-max优化

TL;DR: 本文提出了TCS框架，用于生成真实的时间序列数据及其关联的因果图，通过三阶段方法（因果结构估计、功能依赖近似和噪声分布学习）提升生成数据的多样性和质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有因果发现方法依赖合成数据，但无法真实反映现实场景，尤其在时序数据中更明显。本文旨在解决这一问题，提出更真实的数据生成方法。

Method: 采用三阶段方法：1) 估计数据的滞后因果结构，2) 近似变量间的功能依赖，3) 学习因果模型的噪声分布。并通过Min-max优化提升数据真实性。

Result: 实验表明，TCS能够生成更真实的时序因果数据，同时验证了生成数据的有效性，并揭示了真实数据生成的挑战。

Conclusion: 虽然生成真实因果数据仍具挑战性，TCS为时序因果数据生成提供了更灵活且模型无关的解决方案。

Abstract: Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [135] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)
*Zeng Wang,Minghao Shao,Rupesh Karn,Jitendra Bhandari,Likhitha Mankali,Ramesh Karri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel*

Key words: LLM, 硬件设计, 机器去学习, Verilog, 数据安全

TL;DR: 提出SALAD方法，利用机器去学习技术解决LLM硬件设计中的数据安全问题，包括数据污染、IP泄漏和恶意代码生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LLM在硬件设计自动化中有巨大潜力，但也带来数据安全风险，如Verilog评估数据污染、IP泄漏和恶意代码生成，亟需解决方案。

Method: 引入SALAD，通过机器去学习技术，选择性移除预训练LLM中的污染数据、敏感IP或恶意代码，无需完全重新训练模型。

Result: 案例研究表明，机器去学习技术能有效降低LLM辅助硬件设计中的数据安全风险。

Conclusion: SALAD为LLM在硬件设计中的安全应用提供了可行的解决方案。

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [136] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/abs/2506.02092)
*Francesco De Santis,Philippe Bich,Gabriele Ciravegna,Pietro Barbiero,Danilo Giordano,Tania Cerquitelli*

Key words: 深度神经网络、可解释性、无监督学习、概念建模、图像分类

TL;DR: 该论文提出了一种无监督的基于概念的图像分类模型LCBM，通过将概念建模为伯努利潜空间中的随机变量，提升了模型的可解释性和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了提高深度神经网络的可信度，需要更好地理解其决策过程。当前方法要么需要大量人工监督，要么可扩展性有限。

Method: LCBM通过减少所需概念数量而不牺牲性能，采用伯努利潜空间建模概念，并结合局部线性组合保持可解释性。

Result: LCBM在泛化能力上优于现有无监督概念模型，性能接近黑盒模型，且概念更符合人类理解。用户研究表明其概念更直观。

Conclusion: LCBM通过新颖的概念表示提升了深度神经网络的可解释性和性能，同时保持了模型的可解释性。

Abstract: To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [137] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu,Jinjie Ni,Xiangyan Liu,Zichen Liu,Hang Yan,Michael Qizhe Shieh*

Key words: 视觉语言模型（VLMs），强化学习（RL），数据合成，推理任务，SynthRL

TL;DR: SynthRL 是一种通过自动合成数据扩展推理导向强化学习的流程，显著提升了视觉语言模型的性能，尤其在复杂推理任务上表现突出。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何通过合成强化学习数据进一步优化 RLVR（带可验证奖励的强化学习），以提升视觉语言模型在推理任务中的表现。

Method: 提出 SynthRL 流程，包括选择种子问题、生成更具挑战性的变体并保留原始答案，以及通过验证阶段确保数据的正确性和难度提升。

Result: 在 MMK12 数据集上，SynthRL 从 8K 种子样本生成了 3.3K 可验证的挑战性问题，模型在五个领域外视觉数学推理基准测试中显著优于基线模型。

Conclusion: SynthRL 能有效扩展数据并提升模型性能，尤其在复杂推理任务中表现更优。

Abstract: Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [138] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/abs/2506.02098)
*Miran Özdogan,Gilad Landau,Gereon Elvers,Dulhan Jayalath,Pratik Somaiya,Francesco Mantegna,Mark Woolrich,Oiwi Parker Jones*

Key words: MEG, 语音解码, 深度学习, 神经表征, 脑机接口

TL;DR: LibriBrain is the largest single-subject MEG数据集，用于语音解码，包含超过50小时的录音，旨在推动神经解码技术的进步。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 目前缺乏大规模的个体内数据，限制了非侵入性方法下神经表征的研究。LibriBrain的发布旨在填补这一空白，促进语音解码方法和临床脑机接口的发展。

Method: 数据集包含高质量的MEG录音和详细标注，支持深度学习集成，并提供标准数据分割和基线结果。

Result: 基线实验显示，增加训练数据显著提高了解码性能，验证了大尺度个体内数据集的价值。

Conclusion: LibriBrain的发布为研究社区提供了强大的工具，有望推动语音解码和脑机接口技术的进步。

Abstract: LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [139] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/abs/2506.02134)
*Rishi Raj Sahoo,Rucha Bhalchandra Joshi,Subhankar Mishra*

Key words: Graph Neural Networks, Explainability, Privacy Risks, Differential Privacy, Graph Reconstruction Attack

TL;DR: 论文探讨了图神经网络（GNNs）的可解释性方法在隐私保护方面的风险，提出了一种新的图重构攻击方法ReconXF，能够在辅助数据经过差分隐私保护的情况下仍有效恢复图结构。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GNNs的可解释性方法虽然提高了模型的透明度，但也可能暴露敏感关系，尤其是在辅助数据经过差分隐私保护的情况下，现有攻击方法效果不佳，需要更有效的攻击手段。

Method: 提出ReconXF方法，结合去噪机制处理差分隐私引入的噪声，并利用解释中的结构信号，以优化图重构攻击的效果。

Result: 在多个数据集上的实验表明，ReconXF在隐私保护场景下优于现有方法，显著提高了AUC和平均精度。

Conclusion: 即使辅助数据经过隐私保护，公开解释结合去噪机制仍可能导致图结构的信息泄露，需要进一步研究隐私保护与透明度的平衡。

Abstract: Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [140] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/abs/2506.02138)
*Yarden Bakish,Itamar Zimerman,Hila Chefer,Lior Wolf*

Key words: Transformer, Layer-wise Relevance Propagation (LRP), positional encoding, explainability, NLP, vision

TL;DR: 论文提出了一种改进的层相关传播（LRP）方法，专注于Transformer的位置编码（PE），解决了现有方法忽视PE的问题，显著提升了视觉和NLP任务的解释性效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的LRP方法在解释Transformer时忽略了位置编码（PE），导致了守恒性质的破坏和位置相关性的丢失，限制了其解释能力。

Method: 通过将输入空间重新定义为位置-标记对，提出了专门的理论基础的LRP规则，适用于多种位置编码方法（如Rotary、Learnable和Absolute PE）。

Result: 在微调分类器和零样本基础模型（如LLaMA 3）的实验中，该方法在视觉和NLP解释性任务中显著优于现有方法。

Conclusion: 改进的LRP方法通过纳入位置编码，显著提升了Transformer的解释性能力，为深度学习的可解释性工具提供了新的方向。

Abstract: The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [141] [Z-Error Loss for Training Neural Networks](https://arxiv.org/abs/2506.02154)
*Guillaume Godin*

Key words: 离群样本, Z-Error Loss, 神经网络, 数据清理, 批次统计

TL;DR: 提出一种统计上严谨的Z-Error Loss方法，通过屏蔽批次中的离群样本，减少其对训练的负面影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 离群样本在神经网络训练中传播错误的梯度，导致模型性能和泛化能力下降，因此需要一种方法来减少其影响。

Method: 提出了Z-Error Loss方法，利用批次级统计自动检测并排除异常样本，使模型专注于真实数据结构的训练。

Result: 该方法对数据质量具有鲁棒性和自适应性，同时为数据清理提供了有用的诊断信息。

Conclusion: Z-Error Loss是一种有效的解决方案，用于解决离群样本在神经网络训练中的负面影响。

Abstract: Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [142] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/abs/2506.02168)
*Hrushikesh N. Mhaskar,Efstratios Tsoukanis,Ameya D. Jagtap*

Key words: 机器学习, 函数逼近, 神经网络, 流形学习, 泛化能力

TL;DR: 讨论了机器学习中的函数逼近问题，分析了现有方法的局限性，并提出了针对未知流形的新研究。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究机器学习中函数逼近的理论与实践之间的差距，提升模型的泛化能力。

Method: 回顾现有方法（如神经网络、核方法），分析其局限性，并提出了不依赖流形特征的新方法。

Result: 提出了在未知流形上实现函数逼近的新方法，避免了传统方法的复杂性。

Conclusion: 函数逼近理论应更紧密地结合机器学习实践，以提升模型的泛化能力。

Abstract: A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [143] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/abs/2506.02200)
*Shiangyi Lin,Hui Lan,Vasilis Syrgkanis*

Key words: 工具变量, 高维数据, 降维, 遗漏变量偏差

TL;DR: 传统工具变量（IV）估计在处理高维内生变量时面临挑战，新方法通过将工具变量信息融入降维过程，减少了遗漏变量偏差并提高了预测效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统IV方法在高维非结构化处理变量（如患者治疗路径）中表现受限，研究者通常采用无监督降维方法，但可能引入遗漏变量偏差。

Method: 提出了一种新方法，在降维过程中显式结合工具变量信息，构建处理表示，以优化结果预测的识别方向。

Result: 理论和实验均表明，该方法优于传统的两阶段降维方法，能有效减少偏差并提升预测性能。

Conclusion: 结合工具变量信息的降维方法为高维内生变量处理提供了新框架，显著改善了传统方法的局限性。

Abstract: Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [144] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/abs/2506.02203)
*Navid NaderiAlizadeh,Darian Salehi,Xinran Liu,Soheil Kolouri*

Key words: Sliced Wasserstein距离, 约束学习, 梯度优化, 高维数据, 固定长度表示

TL;DR: 提出了一种优化Sliced Wasserstein距离切片方向的约束学习方法，通过约束1D传输计划来近似原空间的最优计划，提升了切片的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决当前Sliced Wasserstein距离中切片方向难以选择的问题，减少计算复杂度同时提升性能。

Method: 采用约束学习方法，通过梯度对偶优化切片方向，同时结合模型参数训练。

Result: 实验验证了该方法在图像、点云和蛋白质序列等高维数据上的有效性。

Conclusion: 约束学习方法能够显著提升切片方向的信息量，适用于高维数据的固定长度表示生成。

Abstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [145] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/abs/2506.02205)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Key words: Cross-Entropy Method, Bregman centroid, model-based reinforcement learning, multimodal optimization

TL;DR: 该论文提出了Bregman Centroid Guided CEM (BC-EvoCEM)，一种轻量级改进的CEM方法，通过Bregman质心进行信息聚合和多样性控制，提升了多模态环境中的收敛性和解的质量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统CEM在多模态环境中容易早熟收敛，因此需要一个能够控制多样性并提升性能的改进方法。

Method: 提出BC-EvoCEM，利用Bregman质心进行信息聚合，并通过信任区域采样更新贡献较小的CEM工作者。

Result: 实验表明，BC-EvoCEM在合成基准、复杂导航任务和完整MBRL流程中均提升了收敛性和解的质量。

Conclusion: BC-EvoCEM是一种简单但高效的CEM改进方法，适用于多模态优化问题。

Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [146] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
*Hongling Xu,Qi Zhu,Heyuan Deng,Jinpeng Li,Lu Hou,Yasheng Wang,Lifeng Shang,Ruifeng Xu,Fei Mi*

Key words: 大型语言模型, 知识蒸馏, 强化学习, 推理能力, KDRL

TL;DR: KDRL框架结合了知识蒸馏（KD）和强化学习（RL），通过同时优化教师监督和自我探索，提升了大型语言模型（LLM）的推理能力，并在多个基准测试中表现优于单独使用RL或KD的方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了克服RL探索效率低和KD泛化能力差的缺点，提出了一种统一的训练框架KDRL，旨在结合两者的优势。

Method: KDRL利用策略梯度优化，最小化学生与教师分布之间的RKL散度，同时最大化基于规则的奖励，通过统一的优化目标整合GRPO和KD。

Result: 实验表明，KDRL在多个推理基准测试中优于GRPO和KD基线，并在性能和推理效率之间取得了良好的平衡。

Conclusion: 结合KD和RL是一种有效且高效的训练推理LLM的策略。

Abstract: Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [147] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/abs/2506.02210)
*Pu,Yi,Tianlang Chen,Yifan Yang,Sara Achour*

Key words: 神经网络, 动态剪枝, 冗余, 对称性, ExPrune

TL;DR: 本文提出了一种名为ExPrune的动态剪枝算法，利用神经网络中的对称性冗余（exchangeability）来高效减少计算量（FLOPs），同时保持较小的精度损失。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 神经网络的参数量和计算资源需求不断增加，研究者探索了多种方法（如剪枝或量化）以减少冗余。此前工作发现对称性是冗余的一种形式，但尚未被充分利用以提高推理效率。

Method: 通过统计性质定义神经网络参数的对称性（exchangeability），提出动态剪枝算法ExPrune，根据输入动态移除冗余。具体实现是通过预测ReLU激活的负输入进行神经元级动态剪枝。

Result: 在多种模型（计算机视觉、图模型、语言模型）中，ExPrune实现了FLOPs显著减少（10.98-39.05%），精度损失可忽略（最多1%）。与静态剪枝结合还可进一步减少FLOPs。

Conclusion: ExPrune有效利用了对称性冗余，实现了高效推理，且与现有方法兼容，为神经网络优化提供了新思路。

Abstract: Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [148] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/abs/2506.02213)
*Kahn Rhrissorrakrai,Kathleen E. Hamilton,Prerana Bangalore Parthsarathy,Aldo Guzman-Saenz,Tyler Alban,Filippo Utro,Laxmi Parida*

Key words: 小数据学习, 量子集成模型, 医疗, 生命科学, 量子计算

TL;DR: 量子集成模型在小数据问题中的应用研究，重点测试其在医疗和生命科学中的表现，结果表明量子嵌入结构对性能有重要影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 小数据学习在现实应用中是一个常见挑战，本研究探索量子集成模型在医疗和生命科学中的有效性。

Method: 构建了多种量子集成模型，用于二元分类任务，测试了合成数据集和肾细胞癌患者的基因表达数据。

Result: 通过模拟和初步硬件实验，展示了量子嵌入结构对性能的影响，并讨论了如何提取特征和构建有效模型。

Conclusion: 量子计算在小数据问题中具有潜力，尤其是在医疗和生命科学领域。

Abstract: Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [149] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/abs/2506.02242)
*Yihong Tang,Ao Qu,Xujing Yu,Weipeng Deng,Jun Ma,Jinhua Zhao,Lijun Sun*

Key words: 多模态大型语言模型, 道路安全, 城市设计, 可解释性, 假设推断

TL;DR: 提出一种基于多模态大型语言模型（MLLM）的方法，用于可解释的假设推断，自动化生成、评估和优化关于城市环境和道路安全结果的假设，克服传统工作流的局限。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法依赖人工专家提出假设，耗时且易受确认偏误影响；深度学习方法的可解释性有限；未充分利用非结构化数据。为解决这些问题，研究提出MLLM方法。

Method: 利用MLLM为街景图像生成安全问题，从其响应中提取可解释嵌入，并应用于基于回归的统计模型，支持迭代假设检验和优化。

Result: 在曼哈顿街区的实验表明，该方法优于预训练的深度学习模型，同时提供完全可解释性。

Conclusion: UrbanX可作为通用框架，从非结构化数据中提取结构化见解，增强模型可信度，为城市和交通研究提供可扩展、统计基础的知识发现途径。

Abstract: Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [150] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/abs/2506.02243)
*Tamara Cucumides,Floris Geerts*

Key words: 表格数据,关系数据,图神经网络,任务感知,图增强

TL;DR: auGraph是用于表格和关系数据的任务感知图增强框架，通过选择性提升属性为节点，提升下游任务的预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 表格和关系数据在机器学习中广泛使用，但现有GNN方法依赖固定模式图，未充分利用非键属性的预测信号。

Method: auGraph通过评分函数选择性地将属性提升为节点，增强基础图结构，保留原始数据模式的同时注入任务相关信号。

Result: 实验表明，auGraph优于基于模式和启发式的图构建方法，能更好地支持关系及表格预测任务的学习。

Conclusion: auGraph提供了一种灵活且有效的图增强方法，显著提升了表格和关系数据的学习性能。

Abstract: Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [151] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/abs/2506.02255)
*Asha Ramanujam,Adam Elyoumi,Hao Chen,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Shraman Pal,Dimitri J. Papageorgiou,Can Li*

Key words: 安全强化学习, 操作研究, 复杂约束, 基准测试

TL;DR: SafeOR-Gym是一个针对安全强化学习（RL）的基准测试套件，专注于解决复杂约束下的实际问题，填补现有基准测试在能源、制造等高风险领域的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前安全RL基准测试多集中在机器人学和控制任务，缺乏对结构化约束、混合整数决策和工业复杂性的覆盖，限制了安全RL在高风险领域的应用和发展。

Method: 作者提出了SafeOR-Gym，包含九个针对复杂约束的操作研究（OR）环境的基准测试套件，每个环境模拟了具有成本约束、规划周期和混合离散-连续动作空间的现实问题。

Result: 测试发现，当前一些安全RL算法在这些环境中表现不一：部分任务可解，而其他任务则暴露了现有方法的根本局限性。

Conclusion: SafeOR-Gym为安全RL研究提供了一个具有挑战性和实用性的测试平台，旨在推动解决现实决策问题的研究。

Abstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [152] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/abs/2506.02256)
*Yi Xiao,Harshit Sharma,Sawinder Kaur,Dessa Bergen-Cico,Asif Salekin*

Key words: 压力检测,领域泛化,阿片类药物使用障碍,生理信号,机器学习

TL;DR: HHISS是一种领域泛化方法，通过消除个体特异性差异来检测压力信号的一致模式，提高模型在新人群和环境中的准确性，特别适用于阿片类药物使用障碍患者。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 压力检测模型在因个体差异和健康条件等因素导致的生理信号变化中难以泛化，HHISS旨在解决这一问题。

Method: 提出person-wise子网络修剪交集技术和利用连续标签防止过拟合，专注于跨个体的共享特征。

Result: HHISS在七种不同压力数据集上表现优于现有基准方法，证实其在实际应用中的有效性和可扩展性。

Conclusion: HHISS在敏感的实际应用中具有可行性和扩展性，特别适合移动压力检测。

Abstract: Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [153] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/abs/2506.02269)
*YuQing Xie,Tess Smidt*

Key words: 等变神经网络, 优化, 损失景观, 群表示

TL;DR: 论文研究了等变神经网络优化中的困难，提出通过放松等变约束和调整内部群表示可以改善优化效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究等变约束是否引入了优化障碍，是否需要不同的超参数调整。

Method: 通过理论分析损失景观几何，聚焦于使用置换表示构建的网络，并展示无约束模型的参数对称性对等变子空间损失景观的影响。

Result: 证明在某些条件下，等变约束可能阻止全局最小值的寻找到；通过放松约束可以解决问题。

Conclusion: 在更大无约束函数空间下分析网络损失景观结构，调整内部群表示可以有效放松等变约束。

Abstract: Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [154] [Latent Stochastic Interpolants](https://arxiv.org/abs/2506.02276)
*Saurabh Singh,Dmitry Lagun*

Key words: Stochastic Interpolants, Latent Space, Generative Modeling, ELBO, ImageNet

TL;DR: 该论文提出了一种名为Latent Stochastic Interpolants (LSI)的方法，通过在潜在空间中联合优化编码器、解码器和潜在SI模型，克服了Stochastic Interpolants (SI)框架在高维空间中直接应用的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Stochastic Interpolants (SI)框架在潜在变量模型中联合优化的问题，同时避免高维观测空间直接应用SI的计算需求。

Method: 提出LSI方法，通过开发连续时间下的Evidence Lower Bound (ELBO)目标，实现编码器、解码器和潜在SI模型的联合优化。

Result: LSI能够学习有效的潜在表示，并保留SI框架的生成灵活性，且在ImageNet生成基准测试中表现优异。

Conclusion: LSI为潜在空间中的生成建模提供了一种高效且灵活的方法。

Abstract: Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [155] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)
*Qinsi Wang,Jinghan Ke,Hancheng Ye,Yueqian Lin,Yuzhe Fu,Jianyi Zhang,Kurt Keutzer,Chenfeng Xu,Yiran Chen*

Key words: 强化学习,大型语言模型,数据采样,训练效率,角度集中

TL;DR: 论文提出了一种基于模型内在角度信号的动态数据选择框架GAIN-RL，显著提升了大型语言模型在强化微调中的训练效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前强化微调方法在数据采样中存在冗余问题，导致样本效率低下，需要一种更智能的采样策略。

Method: 利用模型固有的角度集中信号（angle concentration），设计动态数据选择框架GAIN-RL，确保梯度更新更具影响力。

Result: 实验表明，GAIN-RL在数学和编程任务中提升了2.5倍训练效率，且用一半数据即可达到更好性能。

Conclusion: 通过模型内在学习信号优化数据选择，GAIN-RL显著提升了训练效率和性能，为LLM微调提供新思路。

Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [156] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)
*Aaron Defazio*

Key words: LLM训练, 梯度范数, 权重衰减, 归一化层, 学习率调度

TL;DR: 长时LLM训练中梯度范数在训练末期快速增加，原因是权重衰减、归一化层和学习率调度的意外交互。提出简单修正方法，改善行为并降低训练损失。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究LLM长时间训练中梯度范数异常增加的现象及其原因。

Method: 分析权重衰减、归一化层与学习率调度的交互作用，提出修正方案。

Result: 修正方法成功抑制梯度范数异常增加，并降低训练损失。

Conclusion: 修正方法有效解决了LLM训练末期的梯度问题，提升训练稳定性。

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [157] [On Universality Classes of Equivariant Networks](https://arxiv.org/abs/2506.02293)
*Marco Pacini,Gabriele Santin,Bruno Lepri,Shubhendu Trivedi*

Key words: 等变神经网络, 对称性学习, 分离能力, 普遍逼近, 浅层网络

TL;DR: 该论文研究了等变神经网络在对称性学习中的表达能力，发现其分离能力并不能完全捕捉其逼近能力，并探讨了浅层等变网络在哪些情况下能实现普遍逼近。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 等变神经网络广泛用于对称性学习，但对其逼近能力的研究较为有限。论文旨在填补这一空白。

Method: 通过分析浅层不变网络的通用性类别，研究等变网络的逼近能力与分离能力之间的关系。

Result: 结果表明，分离能力不足以保证普遍逼近，而浅层网络在某些结构条件下可以实现受限的普适性。

Conclusion: 等变神经网络的逼近能力依赖于对称群的结构特性，如存在适当的正规子群，这在某些关键情况下（如置换对称性）可能不成立。

Abstract: Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [158] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)
*Farzaneh Mahdisoltani,Saeed Mahdisoltani,Roger B. Grosse,David J. Fleet*

Key words: 神经网络, 可解释性, 梯度分析, 决策边界, 可视化

TL;DR: 该论文提出了一种新框架，通过将网络梯度视为微小运动来可视化类别间的隐含路径，从而揭示神经网络的决策边界和内部表示。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有解释方法通常只能识别输入区域的影响，而无法阐明模型如何区分类别或如何通过特定变化将输入从一个类别转换到另一个类别。

Method: 采用可逆变换（Complex Steerable Pyramid）分解图像，在变换空间中计算类别条件梯度，并通过线性外推放大梯度，展示模型从源类别到目标类别的变化路径。

Result: 实验表明，该方法能生成语义明确、空间连贯的形态变化，揭示了分类器最敏感的决策方向。

Conclusion: 该方法为神经网络内部表示提供了一种新颖且可解释的分析工具。

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [159] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/abs/2506.02306)
*Aditya Gorla,Ryan Wang,Zhengtong Liu,Ulzee An,Sriram Sankararaman*

Key words: 掩码自编码, 表格数据填补, 缺失模式, 上下文信息

TL;DR: CACTI是一种基于掩码自编码的表格数据填补方法，利用缺失模式和上下文信息提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有表格数据填补方法未充分利用缺失模式和上下文信息，CACTI旨在填补这一空白。

Method: 采用中位数截断的复制掩码训练策略，结合缺失模式和特征间的语义关系（如列名和文本描述）。

Result: CACTI在不同数据集和缺失条件下优于现有方法，平均R²提升7.8%。

Conclusion: 利用数据集特定的上下文信息和缺失模式可显著提升填补性能。

Abstract: We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [160] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/abs/2506.02308)
*Xiaojun Shan,Qi Cao,Xing Han,Haofei Yu,Paul Pu Liang*

Key words: 多模态, 指令调优, 任务分组, 泛化, MINT

TL;DR: MINT是一种基于多模态交互类型的任务分组策略，能够显著提升多模态指令调优的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究多模态基础模型的性能提升方法，发现单纯增加指令调优任务数量并不总能提高性能，而任务分组更为有效。

Method: 提出MINT方法，通过任务分组（如共享信息发现、模态选择等）来学习可转移技能并减少不匹配任务的干扰。

Result: MINT在多模态指令调优中显著优于基线方法，平衡了泛化与专业化。

Conclusion: 任务分组策略在多模态指令调优中至关重要，MINT为其提供了简单高效的解决方案。

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [161] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/abs/2506.02315)
*D. Isaiah Harp,Joshua Ott,John Alora,Dylan Asmar*

Key words: 飞行测试,降阶模型,高斯过程回归,高保真模型,动态更新

TL;DR: 论文提出了一种新颖的‘无测试点’方法，通过高保真数字模型和机器学习生成降阶模型（ROM），动态更新验证飞行测试数据。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统测试点方法因飞行员操作偏离预设条件而导致模型失效，且数据带和容忍度存在根本问题。

Method: 使用高保真数字模型生成ROM，通过机器学习（如高斯过程回归）动态更新模型，适应任意飞行条件并验证高保真模型。

Result: 成功应用T-38C飞行测试数据生成纵向俯仰运动的ROM，并验证其符合MIL-STD-1797B标准。

Conclusion: 该方法摆脱了传统测试点的限制，动态更新模型，提高了飞行测试的灵活性和准确性。

Abstract: The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [162] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/abs/2506.02318)
*Yuchen Liang,Renxiang Huang,Lifeng Lai,Ness Shroff,Yingbin Liang*

Key words: 离散扩散模型,吸收率矩阵,收敛分析,误差界限,KL散度

TL;DR: 本文首次为使用吸收率矩阵的离散扩散模型提供了有限时间误差界限和收敛率分析。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 离散扩散模型在离散数据应用中表现出色，但其性能对率矩阵选择非常敏感，尤其是均匀与吸收率矩阵。尽管吸收率矩阵通常表现更好，但理论分析集中在均匀率矩阵上，吸收率模型的理论分析仍缺失。

Method: 通过引入替代初始化分布解决吸收平稳分布导致的KL散度问题，使用Jensen型工具和新技术来分析吸收分数函数，并提供无需提前停止的收敛保证。

Result: 首次为吸收率矩阵模型建立了收敛保证和改进的收敛率，证明了其在采样器中的优势，并提供了新工具解决吸收率矩阵特有的挑战。

Conclusion: 本文填补了吸收率矩阵扩散模型的理论空白，为离散扩散模型的优化提供了理论支持和新方法。

Abstract: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [163] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/abs/2506.02323)
*Aleix Boquet-Pujadas,Pol del Aguila Pla,Michael Unser*

Key words: 概率密度估计, 非均匀采样, 样条函数, 核范数, PET重分框

TL;DR: 该论文提出了一种优化方法，用于估计多维问题中非均匀采样下的概率密度，利用网格上的样条函数与核范数正则化提升稀疏性，方法具有空间适应性和稳定性，并在PET重分框中应用。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了处理多维问题中非均匀采样导致的概率密度估计难题，尤其是在探测器灵敏度不均匀的情况下，需要一种高效且灵活的方法。

Method: 采用网格上的样条函数计算概率密度，通过核范数正则化Hessian矩阵以促进稀疏性，实现空间适应性。

Result: 该方法在标准密度测试中表现稳定，对正则化参数的选择具有鲁棒性，并开发了相关软件。

Conclusion: 提出的方法在概率密度估计和PET重分框中具有实际应用价值，展示了其高效性和适应性。

Abstract: We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [164] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/abs/2506.02337)
*Adrienne M. Propp,Jonas A. Actor,Elise Walker,Houman Owhadi,Nathaniel Trask,Daniel M. Tartakovsky*

Key words: Dirichlet-to-Neumann映射, 高斯过程, 离散外微积分, 非线性最优恢复, 守恒律

TL;DR: 提出了一种基于高斯过程的图上的Dirichlet-to-Neumann映射学习方法，结合离散外微积分和非线性最优恢复技术，确保守恒律并避免过拟合，适用于数据稀缺场景。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何通过Dirichlet-to-Neumann映射实现多物理场仿真中的子域耦合，确保状态变量和通量在人工界面上的连续性。

Method: 采用高斯过程学习方法，结合离散外微积分和非线性最优恢复技术，通过最大似然估计优化再生核希尔伯特空间范数。

Result: 在子表面裂缝网络和动脉血流两个应用中，方法表现出高精度和良好的不确定性量化能力，适用于数据稀缺场景。

Conclusion: 该方法在数据稀缺且需可靠不确定性量化的科学应用中具有潜力。

Abstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [165] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/abs/2506.02355)
*Andre He,Daniel Fried,Sean Welleck*

Key words: 强化学习,GRPO,unlikeness reward,定理证明,pass@N

TL;DR: 研究发现GRPO算法在强化学习任务中存在偏向性，提出用‘unlikeliness reward’改善性能，实验证明其有效提升多样性和表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决GRPO算法在任务中偏向常见解而忽略罕见解的问题，提升多样本性能。

Method: 引入‘unlikeness reward’并增加PPO训练轮次，以强化罕见正确解。

Result: 实验显示改进后的方法显著提升pass@N性能，并在定理证明任务中达到竞争性水平。

Conclusion: 提出的方法简单有效，适用于训练定理证明器等任务。

Abstract: Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [166] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)
*Ram Potham*

Key words: AI安全, 可控性评估, LLM代理, 安全原则, 基准测试

TL;DR: 本文提出了一种轻量级、可解释的基准方法，用于评估LLM代理在高层次安全原则与任务指令冲突时的行为表现，初步验证了其可行性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为确保高级AI开发的安全可控，需要早期检测代理行为是否违背安全关键原则，尤其是在这些原则与操作目标冲突时。

Method: 采用简单的网格世界环境，设计了一种基准方法，测试LLM代理在冲突情境下是否优先遵守预设的高层次安全原则。

Result: 初步研究表明该方法可行，并提供了代理在原则冲突下的行为洞察，为评估可控性提供了实证依据。

Conclusion: 评估代理对层次化原则的遵守是理解构建可控AI系统的关键早期步骤。

Abstract: Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [167] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)
*Zhe Li,Bicheng Ying,Zidong Liu,Chaosheng Dong,Haibo Yang*

Key words: 联邦学习；零阶优化；Hessian信息；大语言模型；通信效率

TL;DR: 提出了一种基于Hessian信息的零阶优化方法HiSo，用于联邦学习中大型语言模型的微调，显著提升了收敛速度和通信效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于零阶随机梯度下降的联邦学习方法（如DeComFL）在大模型微调中通信成本低，但梯度估计方差高导致收敛慢。利用Hessian信息能加速优化，但将其融入联邦学习面临数据本地化和通信维度限制的挑战。

Method: 提出HiSo方法，通过解耦维度无关通信与零阶优化，结合全局曲率信息加速收敛，同时保持每轮通信成本最小。

Result: 理论证明HiSo收敛速度与全局Hessian矩阵低有效秩相关，实验显示其在基准数据集和大模型微调任务中显著优于现有零阶方法。

Conclusion: HiSo在保持低通信成本的同时，通过Hessian信息加速了联邦学习中大模型的微调。

Abstract: Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [168] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/abs/2506.02371)
*Haoye Lu,Darren Lo,Yaoliang Yu*

Key words: 扩散模型, 隐私保护, SFBD, 交替投影, Online SFBD

TL;DR: SFBD flow是一种改进的扩散模型训练方法，通过连续投影算法避免交替步骤，提升隐私保护性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 扩散模型在生成性能强的同时可能泄露敏感数据，SFBD通过局部数据训练减少隐私风险，但其实现复杂。

Method: 将SFBD重新解释为交替投影算法，提出连续变体SFBD flow，消除交替步骤，并引入Online SFBD实现。

Result: Online SFBD在多个基准测试中表现优于基线方法。

Conclusion: SFBD flow通过连续化改进训练效率，同时保持隐私保护能力。

Abstract: Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [169] [Multi-agent Markov Entanglement](https://arxiv.org/abs/2506.02385)
*Shuze Chen,Tianyi Peng*

Key words: 值分解, 多智能体强化学习, 马尔可夫决策过程, 马尔可夫纠缠, 索引策略

TL;DR: 该论文揭示了值分解在多智能体强化学习中的数学基础，提出了一种称为“马尔可夫纠缠”的新概念，用于衡量和限制分解误差，并证明了其在实际应用中的可行性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究多智能体系统中值分解的理论基础，填补了关于为何这种分解方法有效的理论空白。

Method: 通过引入“马尔可夫纠缠”概念，并从量子物理中汲取灵感，提出了一种衡量多智能体MDP中分解误差的方法。

Result: 证明了广泛使用的索引策略具有弱纠缠性，分解误差与智能体数量的平方根成正比，并提出了实际中有效估计纠缠度的方法。

Conclusion: 值分解在多智能体系统中有效的原因在于其低马尔可夫纠缠度，该研究为实践中的分解质量提供了理论支持。

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [170] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/abs/2506.02386)
*Jie Bian,Vincent Y. F. Tan*

Key words: 线性老虎机, 固定预算, 最佳臂识别, 后验采样, 游戏采样规则

TL;DR: 本文提出一种新颖的线性多臂老虎机算法，用于在固定预算下识别最佳可行臂，并通过理论证明和实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前文献中，即使在简单的K臂高斯噪声老虎机问题中，关于错误概率指数衰减的确切速率尚未明确。本文旨在填补这一空白。

Method: 基于后验采样的游戏采样规则，结合最小学习者和最大学习者，提出一种新算法，专门针对固定预算下的最佳臂识别优化。

Result: 算法保证错误概率以指数速率衰减，且衰减速率与信息理论下界匹配，实验表明其优于多个基准算法。

Conclusion: 本文不仅填补了理论空白，还提供了实际有效的算法，适用于不同复杂度的场景。

Abstract: The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [171] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)
*Chamara Madarasingha,Nasrin Sohrabi,Zahir Tari*

Key words: 时间序列预测，LLMs，零样本预测，序列分解，提示处理

TL;DR: LLMPred 是一种利用大型语言模型（LLMs）进行时间序列预测的方法，通过将时间序列转化为文本并结合数据预处理技术，显著提升了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前 LLMs 在复杂、噪声和多元时间序列数据上的表现尚未充分研究，因此需开发新方法以充分利用其泛化能力。

Method: LLMPred 将时间序列数据转化为文本格式，结合序列分解和轻量级提示处理技术，支持零样本预测。

Result: 实验表明，LLMPred 在多种小型 LLMs（如 Llama 2 7B、GPT-4o-mini 等）上取得了优于或媲美当前最佳基准的性能。

Conclusion: LLMPred 的关键组件通过消融研究验证了其重要性，展示了其在时间序列预测中的潜力。

Abstract: Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [172] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/abs/2506.02390)
*Qin Xie,Qinghua Zhang,Shuyin Xia,Xinran Zhou,Guoyin Wang*

Key words: AdaBoost, 标签噪声, 粒度计算, GAdaBoost, SAMME

TL;DR: 提出了一种名为GAdaBoost的两阶段框架，通过数据粒化和自适应提升阶段，有效处理多标签分类任务中的标签噪声问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: AdaBoost在多标签分类任务中面临标签噪声的挑战，现有方法要么缺乏有效的噪声处理机制，要么因冗余数据导致高计算成本。

Method: 提出GAdaBoost框架，包括数据粒化阶段和自适应提升阶段，并设计了基于粒球的SAMME扩展算法（GAdaBoost.SA）。

Result: 在噪声数据集上的实验表明，GAdaBoost.SA在鲁棒性和效率上优于现有方法。

Conclusion: 该方法有效扩展了AdaBoost和SAMME，解决了标签噪声问题。

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [173] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)
*Yuanyao Chen,Rongsheng Chen,Fu Luo,Zhenkun Wang*

Key words: 神经组合优化,车辆路径问题,大语言模型,分布投影,推理阶段

TL;DR: 提出了一种基于大语言模型（LLM）的学习框架，解决了神经组合优化（NCO）在小规模实例训练后在大规模VRP上性能下降的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有NCO方法在小规模实例上表现良好，但在大规模场景中性能显著下降，原因是训练与测试数据分布不一致。

Method: 引入LLM驱动的学习框架，在推理阶段学习训练与测试分布间的投影，无需重新训练模型。

Result: 实验表明，该方法使训练于100节点实例的骨干模型在10万节点的TSP和CVRP上表现优越。

Conclusion: 该方法显著提升了NCO模型的扩展性，适用于大规模车辆路径问题。

Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [174] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/abs/2506.02406)
*Renat Sergazinov,Jing Wu,Shao-An Yin*

Key words: 随机傅里叶特征,表格数据,深度学习,神经正切核,预处理

TL;DR: 随机傅里叶特征被重新用于深度学习中，作为处理表格数据的预处理步骤，通过固定频率的正弦和余弦投影，加速训练并提高性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对表格数据深度学习中的不足，如需要额外的归一化或嵌入学习，通过神经正切核（NTK）分析重新利用随机傅里叶映射。

Method: 在初始化时使用固定频率的正弦和余弦投影将输入映射到固定特征空间，作为参数无关且架构无关的转换。

Result: 随机傅里叶预处理能加速梯度训练，优化网络性能，降低超参数调优需求，并提高最终模型的稳定性。

Conclusion: 随机傅里叶预处理是一种理论支持、即插即用的表格数据深度学习增强方法。

Abstract: While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [175] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/abs/2506.02415)
*Karthikeyan Vaiapury*

Key words: AERO, 优化框架, 动态系统, 对抗学习, 能量管理

TL;DR: 提出了一种新颖的优化框架AERO，通过借鉴柔道的重定向原理，将外部扰动转化为优化动力，显著提升了动态非线性系统中的稳定性和适应性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有优化方法在动态、非线性系统中往往难以保持稳定性和适应性，尤其在不确定环境下表现不佳。

Method: 基于15个相互关联的公理，提出AERO框架，包含对抗校正、能量守恒和扰动感知学习，通过梯度投影、不确定性驱动动态和能量管理实现稳定优化。

Result: 在概率性太阳能预测中，AERO显著提升了预测准确性、可靠性和适应性，尤其在噪声和不确定环境下表现突出。

Conclusion: AERO为优化理论及实践提供了一个新的研究方向。

Abstract: Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [176] [Weak Supervision for Real World Graphs](https://arxiv.org/abs/2506.02451)
*Pratheeksha Nair,Reihaneh Rabbany*

Key words: 弱监督学习, 图对比学习, 噪声标签, 节点分类

TL;DR: WSNET是一个新的弱监督图对比学习框架，利用图中的弱信号进行鲁棒表示学习，在真实和合成数据集上表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界的图数据常面临标签稀缺和噪声问题，现有监督学习方法受限，但图中常包含弱信号可辅助学习。

Method: 提出WSNET框架，通过对比目标融合图结构、节点特征和多源噪声监督。

Result: 在三个真实数据集和合成基准上，WSNET的F1分数比现有最优方法提高15%。

Conclusion: 弱监督下的对比学习有效证明了在图中利用不完美标签的潜力。

Abstract: Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [177] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
*Jiaxi Hu,Yongqi Pan,Jusen Du,Disen Lan,Xiaqiang Tang,Qingsong Wen,Yuxuan Liang,Weigao Sun*

Key words: 非线性RNN, 闭环控制, Comba, 序列建模, 硬件高效

TL;DR: 本文提出了Comba，一种基于闭环控制理论的新型非线性RNN变体，结合状态和输出反馈校正，实现了高效的语言和视觉建模。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究近期的序列建模方法（如Gated DeltaNet等）在递归状态管理上的优势与不足，并基于闭环控制理论改进非线性RNN。

Method: 提出Comba模型，采用标量加低秩状态转移，并引入状态和输出反馈；使用Triton实现硬件高效的并行核，训练大参数模型。

Result: Comba在语言和视觉建模中表现出优越的性能和计算效率。

Conclusion: Comba通过创新的递归结构和反馈机制，显著提升了序列建模的效率和性能。

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [178] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/abs/2506.02504)
*Xingyu Chen,Bokun Wang,Ming Yang,Quanqi Hu,Qihang Lin,Tianbao Yang*

Key words: FCCO, 非凸优化, 动量方法, 复杂性分析

TL;DR: 针对非凸非光滑FCCO问题，提出了一种新型随机动量方法，提高了迭代复杂性，并将其应用于多等式约束优化问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在处理非凸非光滑FCCO问题时存在高迭代复杂性和不适合深度学习的局限性，迫切需要改进。

Method: 提出了适用于非光滑FCCO的随机动量方法，并优化了基于平滑铰链惩罚的表达式。

Result: 实现了O(1/ε^5)的迭代复杂性，显著优于现有结果。

Conclusion: 新算法在理论和实验上均表现出色，为FCCO问题提供了有效的解决方案。

Abstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [179] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/abs/2506.02539)
*Thong Q. Nguyen,Shubhang Desai,Yash Jain,Tanvir Aumi,Vishal Chowdhary*

Key words: VerificAgent, 记忆管理, 计算机代理, 事实检查

TL;DR: VerificAgent框架通过专家知识、迭代训练后的人为事实检查，提升计算机代理的任务成功率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决计算机代理因记忆积累导致的性能下降问题，特别是在特定领域任务中。

Method: 结合专家知识、迭代训练和人为事实检查三步管理记忆。

Result: 在OSWorld任务中，VerificAgent比基线表现提升111.1%。

Conclusion: VerificAgent有效管理记忆，显著提升代理性能。

Abstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [180] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim,Nakyeong Yang,Kyomin Jung*

Key words: 机器学习、视觉语言模型、隐私保护、遗忘算法、PUBG

TL;DR: 论文提出了一种针对大型视觉语言模型的机器学习方法PUBG，旨在解决现有遗忘方法导致的隐私泄露和响应质量下降问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机源于现有遗忘方法在消除隐私风险时，往往忽略了对模型响应质量和信息量的要求，导致退化、幻觉或过度拒绝等问题。

Method: 提出了一种新的遗忘任务，要求模型提供既保护隐私又信息丰富且视觉基础的响应。PUBG方法通过明确引导遗忘后的行为向理想输出分布来解决问题。

Result: 实验表明，PUBG能有效避免隐私泄露，同时生成视觉基础和信息丰富的响应，而现有方法在防止隐私侵犯的同时仍存在响应质量问题。

Conclusion: PUBG方法成功解决了大型视觉语言模型中的遗忘后遗症问题，成为兼顾隐私保护和响应质量的有效解决方案。

Abstract: Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [181] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann,Masoud Ramuz,Zeeshan Nisar,Nadine S. Schaadt,Hendrik Annuth,Benjamin Doerr,Friedrich Feuerhake,Thomas Lampert,Johannes F. Lutzeyer*

Key words: 图神经网络, 肾病理学, 异构图, 计算机视觉, 肾小球分类

TL;DR: 本文提出了一种新的异构GNN架构HIEGNet，用于肾小球分类，通过整合肾小球及其周围免疫细胞的信息，显著提高了分类性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 肾小球健康分类是肾病学中的重要任务，但现有GNN方法在此任务中尚未深入探索，尤其是在图构建（节点、边和特征的识别）方面存在挑战。

Method: 提出了一种结合传统和机器学习计算机视觉技术的流水线，用于构建异构图；并设计了一种名为HIEGNet的新型异构GNN架构，整合肾小球和免疫细胞信息。

Result: HIEGNet在肾移植患者的全幻灯片图像数据集上表现优于多个基线模型，且在患者间泛化能力最佳。

Conclusion: HIEGNet通过考虑肾小球的免疫环境，显著提升了肾小球分类的性能，并展示了良好的泛化能力。

Abstract: Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [182] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
*Shenghua He,Tian Xia,Xuan Zhou,Hui Wei*

Key words: 大型语言模型, 强化学习, Zero-Reward Assumption, 策略梯度定理, TRePO

TL;DR: 论文提出了Trajectory Policy Gradient Theorem，证明了在Zero-Reward Assumption下，基于响应级奖励模型可以无偏估计令牌级奖励，为现有强化学习方法提供理论支持，并提出更高效的TRePO算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究大型语言模型（LLM）强化学习中的Zero-Reward Assumption问题，即非终止动作（中间令牌生成）无即时奖励，仅最终令牌获得奖励。实践中令牌级奖励难以获取，亟需理论支持和方法改进。

Method: 引入Trajectory Policy Gradient Theorem，证明响应级奖励模型可无偏估计令牌级奖励；提出Token-Reinforced Policy Optimization (TRePO)算法，简化PPO并提升效率。

Result: 理论表明PPO、GRPO等方法具备建模令牌级奖励的能力，为响应级奖励方法提供理论依据；TRePO在内存效率和实用性上表现优异。

Conclusion: 响应级奖励模型可用于高效LLM微调，TRePO为简化且有效的替代方案，推动了LLM强化学习的实践应用。

Abstract: We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [183] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/abs/2506.02563)
*Roie Reshef,Kfir Yehuda Levy*

Key words: 差分隐私,联邦学习,部分参与,噪声消除,Stochastic Convex Optimization

TL;DR: 该论文提出了一种在联邦学习中实现差分隐私的新方法，解决了部分参与场景下的隐私保护问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中部分参与场景下差分隐私保护的挑战，填补现有方法的不足。

Method: 提出了一种新颖的噪声消除机制，在保护隐私的同时不牺牲收敛速度或计算效率。

Result: 在Stochastic Convex Optimization框架下，该方法在均匀和非均匀数据分布下均表现出最优性能。

Conclusion: 该方法扩展了差分隐私在联邦学习中的应用，为分布式系统中隐私保护学习提供了高效实用的解决方案。

Abstract: This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [184] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)
*Ping Gong,Jiawei Yi,Shengnan Wang,Juncheng Zhang,Zewen Jin,Ouxiang Zhou,Ruibo Liu,Guanbin Xu,Youhui Bai,Bowen Ye,Kun Yuan,Tong Yang,Gong Zhang,Renhai Chen,Feng Wu,Cheng Li*

Key words: Large Language Models, Top-$k$ Attention, HATA, Learning-to-Hash, Efficiency

TL;DR: HATA是一种新型的Top-$k$注意力机制，通过结合低开销的哈希学习技术，显著提高了LLM推理的效率，同时保持了模型准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的Top-$k$注意力方法在效率和准确性之间难以平衡，HATA旨在通过哈希技术解决这一问题。

Method: HATA将查询和键映射为二进制哈希码，以低成本获取相对qk分数顺序，实现Top-$k$注意力。

Result: 实验表明，HATA比传统全注意力快7.2倍，并在多个主流LLM模型和任务中优于现有Top-$k$方法。

Conclusion: HATA通过哈希技术有效提升了LLM推理的效率与准确性，是一种有前景的解决方案。

Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [185] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/abs/2506.02577)
*Wenyan Yang,Joni Pajarinen*

Key words: 离线强化学习,目标条件强化学习,可达性加权采样,PU学习

TL;DR: 该论文介绍了离线目标条件强化学习中的一种新采样方法——可达性加权采样（RWS），通过优先采样有助于实现目标的轨迹，显著提升了策略性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 离线目标条件强化学习通常均匀采样目标和状态-动作对，但这种方法需要大量数据且会生成许多无效组合，影响策略性能。论文旨在通过改进采样方法提升学习效率。

Method: 提出了可达性加权采样（RWS），利用正样本-未标记（PU）学习训练可达性分类器，为其分配采样优先级。

Result: 在六个复杂机器人操作任务中，RWS显著提升了性能，其中HandBlock-Z任务的性能提升了近50%。

Conclusion: 可达性加权采样有效提升了离线目标条件强化学习的性能，是一种即插即用的方法。

Abstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [186] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg,Marion Neumeier,Sinan Hasirlioglu,Mohamed Essayed Bouzouraa,Michael Botsch*

Key words: 自动驾驶系统, 交通场景聚类, CVQ-VAE, highD数据集, 完整性分析

TL;DR: 本文提出了一种用于高速公路交通场景聚类和类别完整性分析的流程，采用CVQ-VAE方法，结果表明其聚类性能优于先前工作，并探讨了聚类质量与数据量之间的权衡。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着自动驾驶系统（ADS）在复杂交通场景中的安全性需求日益增长，精确理解交通场景是确保安全发布ADS功能的关键。

Method: 利用聚类向量量化-变分自编码器（CVQ-VAE）对高速公路交通场景进行聚类，并生成不同数量的交通场景类别目录。

Result: 结果表明，该方法在聚类性能上优于先前工作，同时分析了类别数量对完整性考量的影响。

Conclusion: 本文通过公开的highD数据集，探讨了聚类质量与所需数据量之间的权衡，为ADS安全评估提供了有效工具。

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [187] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/abs/2506.02612)
*Jan Robine,Marc Höftmann,Stefan Harmeling*

Key words: 世界模型、自监督学习、Atari 100k、数据增强、短期依赖

TL;DR: SGF是一种简单、高效的世界模型，通过自监督学习捕捉短期依赖关系，增强模型鲁棒性，并在Atari 100k基准测试中表现良好。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨如何构建不依赖于RNN、Transformer、离散表示和图像重建的世界模型，并提出更简单的替代方案。

Method: 使用自监督表示学习、帧和动作堆叠捕捉短期依赖关系，并通过数据增强提升鲁棒性。

Result: 在Atari 100k基准测试中表现出色，并通过消融研究验证了模型组件的有效性。

Conclusion: SGF展示了在不依赖复杂架构的情况下，仍能构建高效的世界模型。

Abstract: What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [188] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/abs/2506.02616)
*Qi Liao,Parijat Bhattacharjee*

Key words: 自组织网络, 组合学习, 多代理系统, 深度强化学习, 预测决策

TL;DR: 研究提出了两种组合学习方法（CDRL和CPDM），用于解决自组织网络中复杂参数和冲突目标的问题，通过模块化框架显著提升了性能和安全性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决自组织网络中参数复杂和目标冲突的挑战。

Method: 提出了组合深度强化学习（CDRL）和组合预测决策（CPDM）的模块化框架，包含细胞级和细胞对级代理。

Result: 仿真显示减少了切换失败，提升了吞吐量和延迟，优于传统多代理深度强化学习方法。

Conclusion: 该方法在规模化自组织网络中表现更优，具有更快的收敛速度和更高的样本效率。

Abstract: Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [189] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/abs/2506.02619)
*Yanbei Liu,Chongxu Wang,Zhitao Xiao,Lei Geng,Yanwei Pang,Xiao Wang*

Key words: 异构图神经网络, 自监督学习, 最优传输, 节点分类

TL;DR: 提出了HGOT方法，利用最优传输机制简化异构图的自监督学习，无需图增强策略，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统对比自监督学习在异构图中需要复杂的数据增强和样本选择，HGOT旨在解决这一问题。

Method: HGOT通过最优传输机制替代正负样本采样，设计中心视图整合分支视图语义信息。

Result: 在多个真实数据集上，HGOT在节点分类等任务中性能优于现有方法，准确率平均提升6%。

Conclusion: HGOT通过最优传输机制有效简化了自监督学习过程，并提升了模型性能。

Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [190] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou,Ferrante Neri,Yew-Soon Ong,Ruibin Bai*

Key words: 神经架构搜索,多目标优化,Siamese网络,Pareto最优,计算效率

TL;DR: SiamNAS提出了一种新颖的代理建模方法，通过Siamese网络块预测候选架构的支配关系，显著降低了计算成本，并在多目标神经网络架构搜索中高效找到Pareto最优解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代神经架构搜索（NAS）本质上是多目标的，需要在准确性、参数量和计算成本之间进行权衡，导致计算开销巨大。

Method: 使用Siamese网络块集合的代理模型预测架构间的支配关系，并基于模型大小的启发式规则取代拥挤距离计算。

Result: 在NAS-Bench-201上，SiamNAS以0.01 GPU天的计算成本找到了CIFAR-10上最佳和ImageNet上次佳的架构。

Conclusion: Siamese网络代理模型在多任务优化中具有潜力，并可扩展为生成Pareto集集合（SOS）。

Abstract: Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [191] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/abs/2506.02630)
*Tom Jacobs,Advait Gadhikar,Celia Rubio-Madrigal,Rebekka Burkholz*

Key words: HAM, 隐式偏差, 双曲几何, 深度学习, 优化算法

TL;DR: HAM（Hyperbolic Aware Minimization）通过交替优化步骤和双曲镜像步骤，改进深度学习模型的收敛性和特征学习效果，同时保持计算效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究深度学习优化算法的隐式偏差，以解决过参数化导致的小学习率问题。

Method: 提出HAM算法，结合梯度下降和双曲镜像步骤，推导其黎曼梯度流。

Result: HAM在多种任务中提升性能，尤其在稀疏化方法中表现优异。

Conclusion: HAM通过双曲几何优化，显著改进模型性能且计算高效。

Abstract: Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [192] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/abs/2506.02654)
*Shiyu Shen,Bin Pan,Guirong Xue*

Key words: 交通流量预测,概率Transformer,多源数据融合,预训练模型

TL;DR: 提出TrafficPPT模型，通过预训练概率Transformer解决城市交通流量预测中的不确定性和泛化性问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有交通流量预测方法多为确定性点估计，忽视不确定性且缺乏跨城市泛化能力。

Method: 结合实时观测、历史轨迹和路网拓扑数据，预训练基于多城市模拟数据的概率Transformer。

Result: 实验显示TrafficPPT在极端数据稀疏下优于现有方法。

Conclusion: TrafficPPT通过概率建模和预训练提升流量预测的鲁棒性和泛化性。

Abstract: City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [193] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/abs/2506.02665)
*Tianci Liu,Tong Yang,Quan Zhang,Qi Lei*

Key words: 版权保护,水印,鲁棒性,双层优化

TL;DR: 本文提出了一种针对AI模型未经授权使用内容的长期保护方法，通过嵌入可见但难以去除的水印来提高鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着AI的发展，版权内容面临未经授权使用的风险增加，现有方法仅提供短期保护，无法应对模型架构的变化。

Method: 基于概率和逆问题的新框架，嵌入可见且难以去除的水印，并通过近似算法解决双层优化问题。

Result: 实验结果表明，该方法在多种场景下均优于现有方法。

Conclusion: 提出的方法为版权内容提供了长期且鲁棒的保护。

Abstract: As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [194] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/abs/2506.02694)
*Daichi Kimura,Tomonori Izumitani,Hisashi Kashima*

Key words: 时间序列预测, Transformer, 注意力机制, Chatterjee相关系数, 非线性依赖

TL;DR: 该研究提出了一种基于Chatterjee排序相关系数的新型注意力机制XicorAttention，通过非线性相关性改进现有Transformer模型在时间序列预测中的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有注意力机制未能充分捕捉时间序列数据中的非线性依赖关系，因此需要改进。

Method: 提出XicorAttention机制，用Chatterjee相关系数替代标准注意力中的矩阵乘法，并引入了可微分的SoftSort和SoftRank近似计算。

Result: 在真实数据集上的实验表明，XicorAttention能将预测精度提升约9.1%。

Conclusion: 非线性相关性的引入显著提高了时间序列预测的准确性。

Abstract: Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [195] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/abs/2506.02703)
*Khizar Hayat,Baptiste Magnier*

Key words: 信用卡欺诈检测, 方法论严谨性, 数据泄露, 评估缺陷, 机器学习研究

TL;DR: 该研究指出信用卡欺诈检测研究中方法论严谨性的不足，揭示了评估缺陷如何掩盖算法的复杂性，甚至简单模型也能因方法不当而表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 批评当前信用卡欺诈检测研究中普遍存在的方法论缺陷，强调评估严谨性的重要性。

Method: 通过实验展示数据泄露、方法报道模糊、时间验证不足和指标操纵等问题对结果的影响，并用案例研究说明这些缺陷如何导致误导性结果。

Result: 研究发现一个存在数据泄露的最小神经网络架构能超越许多复杂方法，达到99.9%的召回率。

Conclusion: 研究强调评估方法的严谨性比模型复杂性更重要，为机器学习研究提供了改进方向的警示。

Abstract: This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [196] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/abs/2506.02712)
*Jayadev Naram,Fredrik Hellström,Ziming Wang,Rebecka Jörnsten,Giuseppe Durisi*

Key words: 部分域自适应，PDA，部分最优运输，Wasserstein距离，权重设计

TL;DR: 本文提出了基于部分最优传输的部分域自适应（PDA）问题泛化界限，并设计了一种名为WARMPOT的实用算法，其性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在有标签的目标分布数据稀缺而相关源分布数据丰富的情况下，PDA成为重要研究领域，但现有方法缺乏理论支持且权重设计多为启发式。

Method: 通过部分最优运输理论基础推导PDA的泛化界限，提出部分Wasserstein距离作为域对齐项，并设计新的权重表达式。

Result: 提出的WARMPOT算法在实验中表现优异，超过了现有方法。

Conclusion: 理论界限为PDA提供了基础，提出的权重和算法有效提升了性能。

Abstract: In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [197] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)
*Guanzhong Chen,Shaoxiong Yang,Chao Li,Wei Liu,Jian Luan,Zenglin Xu*

Key words: 多Agent系统, 大语言模型, 强化学习, MHGPO, 计算效率

TL;DR: 本文提出了多Agent异质组策略优化（MHGPO），一种无Critic网络的新算法，用于优化基于LLM的多Agent系统，实验表明其在任务性能和计算效率上优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在大语言模型（LLM）的实际部署中，固定的知识截断和输出控制问题限制了其应用。多Agent系统（MAS）通过动态协作和迭代推理提供了一种解决方案，但现有优化方法存在工程开销高和适应性有限的问题。

Method: 提出MHGPO算法，通过估计异质组之间的相对奖励优势来指导策略更新，无需Critic网络。还引入了三种组采样策略平衡效率与效果。

Result: 实验表明，MHGPO在多Agent基于LLM的搜索系统中性能优于MAPPO，计算效率更高且无需预热。

Conclusion: MHGPO为复杂LLM-based MAS的稳定和可扩展优化提供了潜在解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


### [198] [WeightLoRA: Keep Only Necessary Adapters](https://arxiv.org/abs/2506.02724)
*Andrey Veprikov,Vladimir Solodkin,Alexander Zyl,Andrey Savchenko,Aleksandr Beznosikov*

Key words: Parameter-Efficient Fine-Tuning, LoRA, WeightLoRA, 自适应优化, 轻量化训练

TL;DR: 论文提出了一种名为WeightLoRA的新方法，通过自适应选择关键的LoRA头来减少可训练参数，同时保持或提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前的LoRA方法虽然有效，但需要大量内存和人工直觉来选择适配器层，限制了其效率。

Method: 提出的WeightLoRA方法通过自适应选择关键的LoRA头，动态优化训练过程。

Result: 实验表明，WeightLoRA能显著减少参数数量，同时保持或超越基准性能。

Conclusion: WeightLoRA及其改进版WeightLoRA+在多个基准测试和模型上表现优异，验证了其有效性。

Abstract: The widespread utilization of language models in modern applications is
inconceivable without Parameter-Efficient Fine-Tuning techniques, such as
low-rank adaptation ($\texttt{LoRA}$), which adds trainable adapters to
selected layers. Although $\texttt{LoRA}$ may obtain accurate solutions, it
requires significant memory to train large models and intuition on which layers
to add adapters. In this paper, we propose a novel method,
$\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the
most critical $\texttt{LoRA}$ heads throughout the optimization process. As a
result, we can significantly reduce the number of trainable parameters while
maintaining the capability to obtain consistent or even superior metric values.
We conduct experiments for a series of competitive benchmarks and DeBERTa,
BART, and Llama models, comparing our method with different adaptive
approaches. The experimental results demonstrate the efficacy of
$\texttt{WeightLoRA}$ and the superior performance of $\texttt{WeightLoRA+}$ in
almost all cases.

</details>


### [199] [Knowledge Graph Completion by Intermediate Variables Regularization](https://arxiv.org/abs/2506.02749)
*Changyi Xiao,Yixin Cao*

Key words: 知识图谱补全,张量分解,正则化方法,过拟合

TL;DR: 本文总结了现有的基于张量分解的知识图谱补全（KGC）模型，提出了一种新的正则化方法以减少过拟合，并通过实验验证了方法的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有正则化方法仅最小化嵌入范数，性能不佳，需提出更有效的方法以减少TDB模型过拟合。

Method: 提出了一种新的正则化方法，最小化预测张量计算中涉及的中间变量的范数，并确保计算可行性。

Result: 通过理论分析和实验验证，证明了该正则化方法的有效性及其降低过拟合的作用。

Conclusion: 新正则化方法适用于多数TDB模型，能有效减少过拟合，提升KGC性能。

Abstract: Knowledge graph completion (KGC) can be framed as a 3-order binary tensor
completion task. Tensor decomposition-based (TDB) models have demonstrated
strong performance in KGC. In this paper, we provide a summary of existing TDB
models and derive a general form for them, serving as a foundation for further
exploration of TDB models. Despite the expressiveness of TDB models, they are
prone to overfitting. Existing regularization methods merely minimize the norms
of embeddings to regularize the model, leading to suboptimal performance.
Therefore, we propose a novel regularization method for TDB models that
addresses this limitation. The regularization is applicable to most TDB models
and ensures tractable computation. Our method minimizes the norms of
intermediate variables involved in the different ways of computing the
predicted tensor. To support our regularization method, we provide a
theoretical analysis that proves its effect in promoting low trace norm of the
predicted tensor to reduce overfitting. Finally, we conduct experiments to
verify the effectiveness of our regularization technique as well as the
reliability of our theoretical analysis. The code is available at
https://github.com/changyi7231/IVR.

</details>


### [200] [Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection](https://arxiv.org/abs/2506.02757)
*Ruiying Lu,Jinhan Liu,Chuan Du,Dandan Guo*

Key words: 表格异常检测, 掩码建模, 原型学习, 解耦表示, 全局相关性

TL;DR: 提出了一种结合掩码建模和原型学习的表格异常检测方法，通过解耦表示学习和全局原型提取提升性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有深度学习方法存在表示纠缠和全局相关性建模不足的问题，影响了异常检测的性能。

Method: 结合掩码建模和原型学习，设计可学习的掩码和全局原型，通过分布匹配视角优化传输问题。

Result: 在20个表格基准测试中验证了模型的有效性和可解释性。

Conclusion: 该方法显著提升了表格异常检测的性能和可解释性。

Abstract: Tabular anomaly detection, which aims at identifying deviant samples, has
been crucial in a variety of real-world applications, such as medical disease
identification, financial fraud detection, intrusion monitoring, etc. Although
recent deep learning-based methods have achieved competitive performances,
these methods suffer from representation entanglement and the lack of global
correlation modeling, which hinders anomaly detection performance. To tackle
the problem, we incorporate mask modeling and prototype learning into tabular
anomaly detection. The core idea is to design learnable masks by disentangled
representation learning within a projection space and extracting normal
dependencies as explicit global prototypes. Specifically, the overall model
involves two parts: (i) During encoding, we perform mask modeling in both the
data space and projection space with orthogonal basis vectors for learning
shared disentangled normal patterns; (ii) During decoding, we decode multiple
masked representations in parallel for reconstruction and learn association
prototypes to extract normal characteristic correlations. Our proposal derives
from a distribution-matching perspective, where both projection space learning
and association prototype learning are formulated as optimal transport
problems, and the calibration distances are utilized to refine the anomaly
scores. Quantitative and qualitative experiments on 20 tabular benchmarks
demonstrate the effectiveness and interpretability of our model.

</details>


### [201] [Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization](https://arxiv.org/abs/2506.02767)
*Marco Calì,Giulio Giacomuzzo,Ruggero Carli,Alberto Dalla Libera*

Key words: MC-PILCO, iLQR, 强化学习, 收敛加速

TL;DR: 提出了EB-MC-PILCO方法，通过结合iLQR加速MC-PILCO的收敛，实验表明其能减少45.9%的执行时间，并保持100%的成功率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决MC-PILCO在模型强化学习中的收敛速度慢的问题。

Method: 将MC-PILCO与iLQR结合，利用iLQR生成探索性轨迹并初始化策略。

Result: 在cart-pole任务中，EB-MC-PILCO收敛更快，执行时间减少45.9%，成功率保持100%。

Conclusion: EB-MC-PILCO有效加速了MC-PILCO的收敛，同时保持了高成功率。

Abstract: This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.

</details>


### [202] [CART-based Synthetic Tabular Data Generation for Imbalanced Regression](https://arxiv.org/abs/2506.02811)
*António Pedro Pinheiro,Rita P. Ribeiro*

Key words: 不平衡回归, CART, 数据生成, 透明度, 可扩展性

TL;DR: 提出一种基于CART的数据生成方法，解决回归任务中目标分布不平衡问题，避免人工阈值，提高透明度和效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 回归任务中目标分布不平衡会导致模型性能下降，现有方法如随机采样和SMOTE依赖人工阈值，而生成模型如GANs和VAEs则计算成本高且可解释性差。

Method: 采用基于CART的合成数据生成方法，结合相关性和密度机制，无阈值地生成稀疏区域样本。

Result: 实验表明，该方法在性能上与现有重采样和生成策略相当，但执行更快且更透明。

Conclusion: 该方法是一种透明、可扩展的数据级策略，适用于不平衡回归任务。

Abstract: Handling imbalanced target distributions in regression tasks remains a
significant challenge in tabular data settings where underrepresented regions
can hinder model performance. Among data-level solutions, some proposals, such
as random sampling and SMOTE-based approaches, propose adapting classification
techniques to regression tasks. However, these methods typically rely on crisp,
artificial thresholds over the target variable, a limitation inherited from
classification settings that can introduce arbitrariness, often leading to
non-intuitive and potentially misleading problem formulations. While recent
generative models, such as GANs and VAEs, provide flexible sample synthesis,
they come with high computational costs and limited interpretability. In this
study, we propose adapting an existing CART-based synthetic data generation
method, tailoring it for imbalanced regression. The new method integrates
relevance and density-based mechanisms to guide sampling in sparse regions of
the target space and employs a threshold-free, feature-driven generation
process. Our experimental study focuses on the prediction of extreme target
values across benchmark datasets. The results indicate that the proposed method
is competitive with other resampling and generative strategies in terms of
performance, while offering faster execution and greater transparency. These
results highlight the method's potential as a transparent, scalable data-level
strategy for improving regression models in imbalanced domains.

</details>


### [203] [Sheaves Reloaded: A Directional Awakening](https://arxiv.org/abs/2506.02842)
*Stefano Fiorini,Hakan Aktas,Iulia Duta,Stefano Coniglio,Pietro Morerio,Alessio Del Bue,Pietro Liò*

Key words: Sheaf Neural Networks, Directed Cellular Sheaf, Sheaf Laplacian, Graph Learning

TL;DR: 介绍了Directed Sheaf Neural Networks (DSNN)，一种新型Sheaf Neural Networks (SNNs)，通过引入Directed Cellular Sheaf和Directed Sheaf Laplacian来显式建模方向信息，显著提升复杂关系数据的建模能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管方向性在图学习任务中能显著提升性能，现有SNNs无法很好地建模方向信息，限制了其应用潜力。

Method: 提出Directed Cellular Sheaf和Directed Sheaf Laplacian，在此基础上构建DSNN，首次在SNN架构中嵌入方向性偏置。

Result: 在九个真实世界基准测试中，DSNN均优于基线方法。

Conclusion: DSNN通过显式建模方向性，显著提升了SNNs在复杂关系数据上的性能，扩展了其应用范围。

Abstract: Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph
Neural Networks (GNNs) that significantly improve our ability to model complex
relational data. While directionality has been shown to substantially boost
performance in graph learning tasks and is key to many real-world applications,
existing SNNs fall short in representing it. To address this limitation, we
introduce the Directed Cellular Sheaf, a special type of cellular sheaf
designed to explicitly account for edge orientation. Building on this
structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which
captures both the graph's topology and its directional information. This
operator serves as the backbone of the Directed Sheaf Neural Network (DSNN),
the first SNN model to embed a directional bias into its architecture.
Extensive experiments on nine real-world benchmarks show that DSNN consistently
outperforms baseline methods.

</details>


### [204] [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/abs/2506.02864)
*Changyi Xiao,Mengdi Zhang,Yixin Cao*

Key words: 强化学习, 策略优化, 奖励归一化, Beta 分布, BNPO

TL;DR: 本文提出了一种新的策略优化方法 Beta Normalization Policy Optimization (BNPO)，通过动态更新的 Beta 分布自适应归一化奖励，解决了现有方法因静态归一化导致的训练不稳定问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有强化学习方法在二进制奖励场景下存在归一化策略不适应策略更新的动态特性，导致梯度估计不稳定。

Method: BNPO 方法利用动态更新的 Beta 分布自适应归一化奖励，并结合优势分解机制扩展其适用性。

Result: 实验表明 BNPO 在推理任务上实现了最先进的性能，理论分析验证了其降低方差的特性。

Conclusion: BNPO 通过动态归一化提升训练稳定性，扩展了策略优化的适用性。

Abstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that
reinforcement learning with rule-based, binary-valued reward functions can
significantly enhance the reasoning capabilities of large language models.
These models primarily utilize REINFORCE-based policy optimization techniques,
such as REINFORCE with baseline and group relative policy optimization (GRPO).
However, a key limitation remains: current policy optimization methods either
neglect reward normalization or employ static normalization strategies, which
fail to adapt to the dynamic nature of policy updates during training. This may
result in unstable gradient estimates and hinder training stability. To address
this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel
policy optimization method that adaptively normalizes rewards using a Beta
distribution with dynamically updated parameters. BNPO aligns the normalization
with the changing policy distribution, enabling more precise and lower-variance
gradient estimation, which in turn promotes stable training dynamics. We
provide theoretical analysis demonstrating BNPO's variance-reducing properties
and show that it generalizes both REINFORCE and GRPO under binary-valued reward
settings. Furthermore, we introduce an advantage decomposition mechanism to
extend BNPO's applicability to more complex reward systems. Experimental
results confirm that BNPO achieves state-of-the-art performance among policy
optimization methods on reasoning tasks. The code is available at
https://github.com/changyi7231/BNPO.

</details>


### [205] [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.02883)
*Anthony Kobanda,Odalric-Ambrym Maillard,Rémy Portelas*

Key words: 持续强化学习、视频游戏导航、遗忘、任务适应、内存效率

TL;DR: 提出了一种新的持续强化学习基准，专注于视频游戏导航场景，以解决遗忘、任务适应和内存效率问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决持续强化学习中的遗忘、任务适应和内存效率挑战，填补文献空白。

Method: 引入包含多样化任务、数据集、评估协议和指标的基准，并应用现有先进基线算法。

Result: 基准提供了一个可复现的研究框架，加速游戏领域持续强化学习的进展。

Conclusion: 该基准不仅促进可复现研究，还为实际应用提供有效的解决方案。

Abstract: Autonomous agents operating in domains such as robotics or video game
simulations must adapt to changing tasks without forgetting about the previous
ones. This process called Continual Reinforcement Learning poses non-trivial
difficulties, from preventing catastrophic forgetting to ensuring the
scalability of the approaches considered. Building on recent advances, we
introduce a benchmark providing a suite of video-game navigation scenarios,
thus filling a gap in the literature and capturing key challenges :
catastrophic forgetting, task adaptation, and memory efficiency. We define a
set of various tasks and datasets, evaluation protocols, and metrics to assess
the performance of algorithms, including state-of-the-art baselines. Our
benchmark is designed not only to foster reproducible research and to
accelerate progress in continual reinforcement learning for gaming, but also to
provide a reproducible framework for production pipelines -- helping
practitioners to identify and to apply effective approaches.

</details>


### [206] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/abs/2506.02887)
*Mrinmay Sen,Shruti Aparna,Rohit Agarwal,Chalavadi Krishna Mohan*

Key words: 联邦学习、部分客户端参与、数据异质性、分布式训练

TL;DR: 本文综述了联邦学习中部分客户端参与的影响，分析了现有方法及其优缺点。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的联邦学习研究多假设客户端完全参与，而忽略了实际场景中部分客户端参与的挑战。

Method: 对现有应对部分客户端参与的联邦学习方法进行了分类和分析。

Result: 提供了理论和实证支持的综合分析，突出了各种方法的优缺点。

Conclusion: 部分客户端参与是联邦学习中需要重点关注的实际问题，现有方法仍需改进。

Abstract: Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [207] [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
*Jakub Krajewski,Marcin Chochowski,Daniel Korzekwa*

Key words: Mixture of Experts (MoE), 大型语言模型 (LLMs), 细粒度, 实证评估

TL;DR: 研究提出了一种细粒度MoE的训练方案，并对比其与标准MoE在缩放性上的表现，结果显示细粒度MoE在大型模型中表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索细粒度Mixture of Experts (MoE)架构在大型语言模型中的效果，以提高模型收敛性和性能。

Method: 提出一套训练方案，对不同规模的细粒度MoE和标准MoE配置进行全面的实证评估。

Result: 在最大规模模型中，细粒度MoE实现了更低的验证损失和更高的下游任务准确率。

Conclusion: 细粒度MoE为未来大规模模型的开发提供了实证基础和实用见解。

Abstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling
Large Language Models (LLMs) efficiently. Fine-grained MoE approaches -
utilizing more numerous, smaller experts - have demonstrated potential in
improving model convergence and quality. This work proposes a set of training
recipes and provides a comprehensive empirical evaluation of fine-grained MoE,
directly comparing its scaling properties against standard MoE configurations
for models with up to 56B total (17B active) parameters. We investigate
convergence speed, model performance on downstream benchmarks, and practical
training considerations across various setups. Overall, at the largest scale we
show that fine-grained MoE achieves better validation loss and higher accuracy
across a set of downstream benchmarks. This study offers empirical grounding
and practical insights for leveraging fine-grained MoE in the development of
future large-scale models.

</details>


### [208] [Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning](https://arxiv.org/abs/2506.02897)
*Alessandro Licciardi,Roberta Raineri,Anton Proskurnikov,Lamberto Rondoni,Lorenzo Zino*

Key words: 联邦学习、数据异构性、方差减少、意见动态、聚类

TL;DR: 本文提出了一种基于意见动态的联邦学习算法（FCVR-BE），通过动态聚类和方差减小组选择，有效解决数据异构性问题，提升模型性能和收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习（FL）虽能实现隐私保护的协作模型训练，但客户数据异构性会严重降低模型性能。

Method: 引入FCVR-BE算法，动态组织客户端到非重叠聚类，并从每个聚类中选择一个客户端以减少模型更新方差。

Result: 实验表明，该算法在异构场景中优于现有FL算法，提供更准确的结果和更快的收敛。

Conclusion: 通过意见动态聚类和方差减少选择，FCVR-BE有效解决了数据异构性问题。

Abstract: Federated Learning (FL) enables privacy-preserving collaborative model
training, yet its practical strength is often undermined by client data
heterogeneity, which severely degrades model performance. This paper proposes
that data heterogeneity across clients' distributions can be effectively
addressed by adopting an approach inspired by opinion dynamics over temporal
social networks. We introduce \shortname (Federated Coalition Variance
Reduction with Boltzmann Exploration), a variance-reducing selection algorithm
in which (1) clients dynamically organize into non-overlapping clusters based
on asymptotic agreements, and (2) from each cluster, one client is selected to
minimize the expected variance of its model update. Our experiments show that
in heterogeneous scenarios our algorithm outperforms existing FL algorithms,
yielding more accurate results and faster convergence, validating the efficacy
of our approach.

</details>


### [209] [From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation](https://arxiv.org/abs/2506.02933)
*Junyi Fang,Yuxun Chen,Yuxin Chen,Chen Zhang*

Key words: 多臂老虎机,非静态环境,RAVEN-UCB,方差感知

TL;DR: 介绍了RAVEN-UCB算法，适用于非静态环境中的多臂老虎机问题，通过方差感知的适应机制实现更优的遗憾边界，实验验证了其在理论和实践上的优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决非静态环境中多臂老虎机问题中奖励分布动态变化的挑战。

Method: 结合三种创新：方差驱动的探索、自适应控制和高效递归更新。

Result: 在理论和实验上实现了优于UCB1和UCB-V的遗憾边界，并在多种非静态环境中表现出鲁棒性。

Conclusion: RAVEN-UCB算法在非静态环境中表现出理论和实践上的优越性。

Abstract: The Multi-Armed Bandit (MAB) problem is challenging in non-stationary
environments where reward distributions evolve dynamically. We introduce
RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical
efficiency via variance-aware adaptation. It achieves tighter regret bounds
than UCB1 and UCB-V, with gap-dependent regret of order $K \sigma_{\max}^2 \log
T / \Delta$ and gap-independent regret of order $\sqrt{K T \log T}$. RAVEN-UCB
incorporates three innovations: (1) variance-driven exploration using
$\sqrt{\hat{\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive
control via $\alpha_t = \alpha_0 / \log(t + \epsilon)$, and (3) constant-time
recursive updates for efficiency. Experiments across non-stationary patterns -
distributional changes, periodic shifts, and temporary fluctuations - in
synthetic and logistics scenarios demonstrate its superiority over
state-of-the-art baselines, confirming theoretical and practical robustness.

</details>


### [210] [MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver](https://arxiv.org/abs/2506.02935)
*Yuepeng Zheng,Fu Luo,Zhenkun Wang,Yaoxin Wu,Yu Zhou*

Key words: 多任务学习, 知识蒸馏, 神经组合优化, 车辆路径问题, 强化学习

TL;DR: 本文提出了一种基于知识蒸馏的多任务学习方法（MTL-KD），用于训练具有强泛化能力的大型解码器模型，并在多种车辆路径问题（VRP）变体上取得了优于现有方法的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基于强化学习（RL）的多任务方法在解决大规模问题时泛化能力有限，只能训练小型解码器模型。本文旨在通过知识蒸馏技术克服这一限制。

Method: 提出了MTL-KD方法，通过从多个单任务RL模型中转移策略知识到单一大型解码器模型，实现无标签训练；同时引入了随机重排序重建（R3C）推理策略以适应多样化的VRP任务。

Result: 在6个已知和10个未知的VRP变体（最多1000个节点）上，MTL-KD方法在统一和实际场景基准测试中均表现出优越性能，显示了强泛化能力。

Conclusion: MTL-KD方法通过知识蒸馏和R3C策略，有效提升了多任务模型的泛化能力和性能。

Abstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a
promising approach to train a unified model capable of solving multiple Vehicle
Routing Problem (VRP) variants. However, existing Reinforcement Learning
(RL)-based multi-task methods can only train light decoder models on
small-scale problems, exhibiting limited generalization ability when solving
large-scale problems. To overcome this limitation, this work introduces a novel
multi-task learning method driven by knowledge distillation (MTL-KD), which
enables the efficient training of heavy decoder models with strong
generalization ability. The proposed MTL-KD method transfers policy knowledge
from multiple distinct RL-based single-task models to a single heavy decoder
model, facilitating label-free training and effectively improving the model's
generalization ability across diverse tasks. In addition, we introduce a
flexible inference strategy termed Random Reordering Re-Construction (R3C),
which is specifically adapted for diverse VRP tasks and further boosts the
performance of the multi-task model. Experimental results on 6 seen and 10
unseen VRP variants with up to 1000 nodes indicate that our proposed method
consistently achieves superior performance on both uniform and real-world
benchmarks, demonstrating robust generalization abilities.

</details>


### [211] [QKV Projections Require a Fraction of Their Memory](https://arxiv.org/abs/2506.02939)
*Malik Khalaf,Yara Shamshoum,Nitzan Hodos,Yuval Sieradzki,Assaf Schuster*

Key words: Multi-Head Attention, memory efficiency, PAMM, LLM training

TL;DR: 本文提出了一种名为PAMM的矩阵乘法近似技术，显著减少了注意力层中Q、K、V投影的内存占用，与现有高效注意力技术兼容。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决现有工作中对注意力层中线性投影内存占用问题的忽视。

Method: 采用Point-Approximate Matrix Multiplication（PAMM）压缩技术。

Result: 内存消耗减少高达512倍，同时保持或改善最终困惑度。

Conclusion: PAMM是一种实用且互补的内存高效LLM训练方法。

Abstract: The Multi-Head Attention mechanism is central to LLM operation, and multiple
works target its compute and memory efficiency during training. While most
works focus on approximating the scaled dot product, the memory consumption of
the linear projections that compute the $Q$, $K$, and $V$ tensors from the
input $x$ is often overlooked. To address this, we propose Point-Approximate
Matrix Multiplication (PAMM), a novel tensor compression technique that reduces
memory consumption of the $Q,K,V$ projections in attention layers by a factor
of up to $\times 512$, effectively erasing their memory footprint, while
achieving similar or better final perplexity. PAMM is fully composable with
efficient attention techniques such as FlashAttention, making it a practical
and complementary method for memory-efficient LLM training.

</details>


### [212] [Abstract Counterfactuals for Language Model Agents](https://arxiv.org/abs/2506.02946)
*Edoardo Pona,Milad Kazemi,Yali Du,David Watson,Nicola Paoletti*

Key words: 反事实推理,语言模型代理,抽象反事实,令牌级干预

TL;DR: 论文提出了一种针对语言模型代理的抽象反事实框架，解决了传统令牌级反事实方法的局限性，并在实验中验证了其一致性和有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 语言模型代理的动作空间开放且隐含，令牌级反事实方法难以适用，容易产生偏差或无意义的结果。

Method: 引入抽象反事实框架，关注动作和交互的高层特征，结合令牌级和潜在空间干预进行实验。

Result: 实验表明，该方法能生成一致且有意义的反事实，同时减少令牌级方法的副作用。

Conclusion: 抽象反事实框架为语言模型代理提供了一种更有效的反事实推理方法。

Abstract: Counterfactual inference is a powerful tool for analysing and evaluating
autonomous agents, but its application to language model (LM) agents remains
challenging. Existing work on counterfactuals in LMs has primarily focused on
token-level counterfactuals, which are often inadequate for LM agents due to
their open-ended action spaces. Unlike traditional agents with fixed, clearly
defined action spaces, the actions of LM agents are often implicit in the
strings they output, making their action spaces difficult to define and
interpret. Furthermore, the meanings of individual tokens can shift depending
on the context, adding complexity to token-level reasoning and sometimes
leading to biased or meaningless counterfactuals. We introduce \emph{Abstract
Counterfactuals}, a framework that emphasises high-level characteristics of
actions and interactions within an environment, enabling counterfactual
reasoning tailored to user-relevant features. Our experiments demonstrate that
the approach produces consistent and meaningful counterfactuals while
minimising the undesired side effects of token-level methods. We conduct
experiments on text-based games and counterfactual text generation, while
considering both token-level and latent-space interventions.

</details>


### [213] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/abs/2506.02950)
*Stepan I. Manukhov,Alexander Kolesov,Vladimir V. Palyulin,Alexander Korotin*

Key words: EFM, IFM, 相互作用场, 数据生成, 数据传输

TL;DR: 提出了一种名为IFM（Interaction Field Matching）的新方法，作为EFM（Electrostatic Field Matching）的扩展，解决了EFM中静电场的建模难题，并展示了其性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: EFM作为一种基于物理启发的数据生成和传输方法，存在建模复杂的静电场的局限性，因此需要更通用的方法。

Method: 提出IFM方法，允许使用超出静电场的通用相互作用场，特别设计了一种基于强相互作用的实现方案。

Result: 在一系列玩具和图像数据传输问题上展示了IFM的性能。

Conclusion: IFM成功解决了EFM中的建模问题，并通过通用相互作用场提供了更灵活的数据处理方法。

Abstract: Electrostatic field matching (EFM) has recently appeared as a novel
physics-inspired paradigm for data generation and transfer using the idea of an
electric capacitor. However, it requires modeling electrostatic fields using
neural networks, which is non-trivial because of the necessity to take into
account the complex field outside the capacitor plates. In this paper, we
propose Interaction Field Matching (IFM), a generalization of EFM which allows
using general interaction fields beyond the electrostatic one. Furthermore,
inspired by strong interactions between quarks and antiquarks in physics, we
design a particular interaction field realization which solves the problems
which arise when modeling electrostatic fields in EFM. We show the performance
on a series of toy and image data transfer problems.

</details>


### [214] [Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/abs/2506.02965)
*Ze Yu Zhang,Bolin Ding,Bryan Kian Hsiang Low*

Key words: Mixture-of-Experts, 隐私保护, 分布式训练, 大型语言模型, 协作学习

TL;DR: PC-MoE是一种隐私保护的协作式Mixture-of-Experts框架，通过MoE架构的稀疏性实现高效内存使用和分布式协作训练，保护数据隐私同时保持高任务精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多个资源有限的参与方在协作训练大型语言模型时面临的数据隐私和GPU内存限制问题。

Method: 利用MoE架构的稀疏性，将训练数据和部分梯度信号保留在本地，实现分布式协作训练。

Result: 在七个流行的LLM基准测试中，几乎与集中式模型性能相当，GPU内存峰值减少70%，且完全抵抗重建攻击。

Conclusion: PC-MoE在保护隐私的同时，突破了典型隐私保护方案中精度下降的瓶颈，实现了高效的协作训练。

Abstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


### [215] [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](https://arxiv.org/abs/2506.02972)
*Md-Ferdous Pervej,Richeng Jin,Md Moin Uddin Chowdhury,Simran Singh,İsmail Güvenç,Huaiyu Dai*

Key words: 隐私保护分布式机器学习, 空中连接车辆, 在线学习, 联邦学习, 模型裁剪, 梯度量化

TL;DR: 该论文提出了一种计算和通信高效的在线空中联邦学习算法（2CEOAFL），用于解决空中连接车辆（ACV）持续感知数据和资源受限的问题，并通过模型裁剪和梯度量化的方式实现高效学习。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 空中连接车辆（ACV）因其持续感知数据的能力和资源受限的特点，需要一种高效的学习方法以应对在线学习和资源限制问题。

Method: 通过建模ACV的动态轨迹和时间变化数据分布，提出2CEOAFL算法，包括模型裁剪、训练和梯度量化的步骤。

Result: 仿真结果表明，2CEOAFL算法在性能上与非裁剪和非量化的低效方法相当。

Conclusion: 2CEOAFL算法在资源受限的ACV环境下实现了高效且性能相当的联邦学习。

Abstract: Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically quantize and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and nonquantized, hence, computation- and communication-inefficient
counterparts.

</details>


### [216] [On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](https://arxiv.org/abs/2506.02978)
*Mohamed Djilani,Thibault Simonetto,Karim Tit,Florian Tambon,Paul Récamier,Salah Ghamizi,Maxime Cordy,Mike Papadakis*

Key words: 表格基础模型,对抗性攻击,鲁棒性,对抗性训练

TL;DR: 文章研究了表格基础模型（FM）对对抗性攻击的脆弱性，发现小规模的结构化扰动会显著降低预测准确性。同时，表格FM还能生成可迁移的对抗样本。作者提出了一种无需更新模型权重的对抗性训练策略，提高了模型的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索表格基础模型在对抗性攻击下的脆弱性及其作为对抗性工具的潜���用，并提出改进方法。

Method: 通过对抗性微调和上下文对抗性学习优化模型，提出一种增量替换上下文的对抗性训练策略。

Result: 在金融、网络安全和医疗三个领域的基准测试中，小扰动显著降低了预测准确性；提出的训练策略提高了模型鲁棒性。

Conclusion: 表格基础模型既是对抗性攻击的目标也是其来源，亟需鲁棒性训练和评估方法。

Abstract: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage
in-context learning to achieve strong performance without gradient updates or
fine-tuning. However, their robustness to adversarial manipulation remains
largely unexplored. In this work, we present a comprehensive study of the
adversarial vulnerabilities of tabular FM, focusing on both their fragility to
targeted test-time attacks and their potential misuse as adversarial tools. We
show on three benchmarks in finance, cybersecurity and healthcare, that small,
structured perturbations to test inputs can significantly degrade prediction
accuracy, even when training context remain fixed. Additionally, we demonstrate
that tabular FM can be repurposed to generate transferable evasion to
conventional models such as random forests and XGBoost, and on a lesser extent
to deep tabular models. To improve tabular FM, we formulate the robustification
problem as an optimization of the weights (adversarial fine-tuning), or the
context (adversarial in-context learning). We introduce an in-context
adversarial training strategy that incrementally replaces the context with
adversarial perturbed instances, without updating model weights. Our approach
improves robustness across multiple tabular benchmarks. Together, these
findings position tabular FM as both a target and a source of adversarial
threats, highlighting the urgent need for robust training and evaluation
practices in this emerging paradigm.

</details>


### [217] [Implicit Regularization of the Deep Inverse Prior Trained with Inertia](https://arxiv.org/abs/2506.02986)
*Nathan Buskulic,Jalal Fadil,Yvain Quéau*

Key words: 神经网络, 逆问题, 自监督, 收敛性, 恢复性, 惯性训练

TL;DR: 本文为自监督神经网络在逆问题中的应用提供了收敛性和恢复性保证，研究了连续时间和离散时间情况下的训练方法，并展示了优化的加速指数收敛率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前逆问题中使用神经网络的理论保证较少，本文旨在为自监督神经网络在逆问题中的训练提供理论支持，特别是针对惯性训练方法的研究。

Method: 研究了连续时间和离散时间下的惯性训练方法，结合粘性和几何Hessian驱动的阻尼，分析了网络的收敛性和恢复性。

Result: 连续时间情况下，网络训练实现了优化的加速指数收敛率；离散时间情况下，展示了类似的恢复性保证，但收敛率较线性。

Conclusion: 本文为自监督神经网络在逆问题中的应用提供了理论保证，证明了惯性训练方法的有效性，尤其在连续时间情况下表现更优。

Abstract: Solving inverse problems with neural networks benefits from very few
theoretical guarantees when it comes to the recovery guarantees. We provide in
this work convergence and recovery guarantees for self-supervised neural
networks applied to inverse problems, such as Deep Image/Inverse Prior, and
trained with inertia featuring both viscous and geometric Hessian-driven
dampings. We study both the continuous-time case, i.e., the trajectory of a
dynamical system, and the discrete case leading to an inertial algorithm with
an adaptive step-size. We show in the continuous-time case that the network can
be trained with an optimal accelerated exponential convergence rate compared to
the rate obtained with gradient flow. We also show that training a network with
our inertial algorithm enjoys similar recovery guarantees though with a less
sharp linear convergence rate.

</details>


### [218] [Protein Inverse Folding From Structure Feedback](https://arxiv.org/abs/2506.03028)
*Junde Xu,Zijun Gao,Xinyi Zhou,Jie Hu,Xingyi Cheng,Le Song,Guangyong Chen,Pheng-Ann Heng,Jiezhong Qiu*

Key words: 逆折叠问题，直接偏好优化（DPO），蛋白质设计，结构反馈，TM-Score

TL;DR: 论文提出了一种利用直接偏好优化（DPO）的方法，通过蛋白质折叠模型的反馈来优化逆折叠模型，显著提升了目标蛋白质结构的序列恢复能力和结构相似性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决逆折叠问题（设计能够折叠成特定三维结构的氨基酸序列）是生物技术中的关键挑战，现有方法存在局限性，需要更高效的优化策略。

Method: 结合逆折叠模型和蛋白质折叠模型的反馈，通过DPO目标函数对候选序列进行优化，利用结构偏好标签迭代调整模型。

Result: 在CATH 4.2测试集上，TM-Score从0.77提升至0.81，且迭代优化后对挑战性结构的平均TM-Score提升达79.5%。

Conclusion: 该方法通过偏好优化有效提升了蛋白质序列设计能力，为生物技术应用提供了新方向。

Abstract: The inverse folding problem, aiming to design amino acid sequences that fold
into desired three-dimensional structures, is pivotal for various
biotechnological applications. Here, we introduce a novel approach leveraging
Direct Preference Optimization (DPO) to fine-tune an inverse folding model
using feedback from a protein folding model. Given a target protein structure,
we begin by sampling candidate sequences from the inverse-folding model, then
predict the three-dimensional structure of each sequence with the folding model
to generate pairwise structural-preference labels. These labels are used to
fine-tune the inverse-folding model under the DPO objective. Our results on the
CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence
recovery of baseline models but also leads to a significant improvement in
average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.
Furthermore, iterative application of our DPO-based method on challenging
protein structures yields substantial gains, with an average TM-Score increase
of 79.5\% with regard to the baseline model. This work establishes a promising
direction for enhancing protein sequence design ability from structure feedback
by effectively utilizing preference optimization.

</details>


### [219] [On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning](https://arxiv.org/abs/2506.03037)
*Shubhendu Trivedi,Brian D. Nord*

Key words: 机器学习、不确定性量化、标准化、信任轴、科学ML、模拟推断

TL;DR: 该立场论文探讨了机器学习（ML）中不确定性量化的挑战，包括术语不一致和不同问题背景下的技术要求，并提出了标准化建议以提高不确定性量化的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决机器学习中不确定性量化面临的术语不一致和技术多样性问题，促进更可靠的不确定性量化方法。

Method: 通过分析当前不确定性估计目标（如预测、推断、基于模拟的推断）和不确定性构建（如频率派、贝叶斯、信任派），提出了标准化的建议。

Result: 强调了问题映射的实例，并提出了促进意图与实现一致的标准，以确保不确定性量化的可靠性。

Conclusion: 论文提倡通过标准化和信任轴设计来提升不确定性量化在ML中的可靠性，特别是在科学ML和基于模拟的推断中。

Abstract: Quantifying uncertainties for machine learning (ML) models is a foundational
challenge in modern data analysis. This challenge is compounded by at least two
key aspects of the field: (a) inconsistent terminology surrounding uncertainty
and estimation across disciplines, and (b) the varying technical requirements
for establishing trustworthy uncertainties in diverse problem contexts. In this
position paper, we aim to clarify the depth of these challenges by identifying
these inconsistencies and articulating how different contexts impose distinct
epistemic demands. We examine the current landscape of estimation targets
(e.g., prediction, inference, simulation-based inference), uncertainty
constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to
map between them. Drawing on the literature, we highlight and explain examples
of problematic mappings. To help address these issues, we advocate for
standards that promote alignment between the \textit{intent} and
\textit{implementation} of uncertainty quantification (UQ) approaches. We
discuss several axes of trustworthiness that are necessary (if not sufficient)
for reliable UQ in ML models, and show how these axes can inform the design and
evaluation of uncertainty-aware ML systems. Our practical recommendations focus
on scientific ML, offering illustrative cases and use scenarios, particularly
in the context of simulation-based inference (SBI).

</details>


### [220] [Sample complexity of Schrödinger potential estimation](https://arxiv.org/abs/2506.03043)
*Nikita Puchkin,Iurii Pustovalov,Yuri Sapronov,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Key words: Schrödinger势、KL散度、泛化能力、随机最优控制、生成建模

TL;DR: 论文研究了基于KL散度最小化的Schrödinger势估计方法，证明了在高概率下终端密度与目标分布间的KL散度界，样本量增加时风险下降速度可达O(log²n/n)。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决Schrödinger势估计问题，该问题在生成建模和随机最优控制中至关重要，特别是在Schrödinger桥和SDEs的背景下。

Method: 使用经验KL风险最小化方法，通过对数势的约束类拟合终端边际分布，并分析其泛化能力。

Result: 在高概率下，推导了终端密度与目标分布间KL散度的非渐近上界，证明风险下降速度可达O(log²n/n)。

Conclusion: 即使在无界支撑的情况下，该方法能有效估计Schrödinger势，并展示了快速收敛性。

Abstract: We address the problem of Schr\"odinger potential estimation, which plays a
crucial role in modern generative modelling approaches based on Schr\"odinger
bridges and stochastic optimal control for SDEs. Given a simple prior diffusion
process, these methods search for a path between two given distributions
$\rho_0$ and $\rho_T^*$ requiring minimal efforts. The optimal drift in this
case can be expressed through a Schr\"odinger potential. In the present paper,
we study generalization ability of an empirical Kullback-Leibler (KL) risk
minimizer over a class of admissible log-potentials aimed at fitting the
marginal distribution at time $T$. Under reasonable assumptions on the target
distribution $\rho_T^*$ and the prior process, we derive a non-asymptotic
high-probability upper bound on the KL-divergence between $\rho_T^*$ and the
terminal density corresponding to the estimated log-potential. In particular,
we show that the excess KL-risk may decrease as fast as $O(\log^2 n / n)$ when
the sample size $n$ tends to infinity even if both $\rho_0$ and $\rho_T^*$ have
unbounded supports.

</details>


### [221] [Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation](https://arxiv.org/abs/2506.03062)
*Qining Zhang,Tanner Fiez,Yi Liu,Wenyang Liu*

Key words: 自适应实验设计, A/B测试, 多指标, 异方差, SHRVar

TL;DR: 本文提出了一种固定预算的多指标自适应实验设计框架，通过两阶段结构（探索和验证）解决传统A/B测试在多候选者和多指标情况下的统计效率问题。提出的SHRVar方法基于相对方差采样和奖励z值淘汰策略，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统A/B测试在多候选者测试时统计效率低，自适应实验设计（AED）在多指标和异方差情况下无法准确推断实验统计量。本文旨在解决这些问题。

Method: 采用两阶段框架：自适应探索阶段识别最佳处理，验证阶段通过A/B测试验证处理质量并推断统计量。提出SHRVar方法，结合相对方差采样和奖励z值淘汰策略。

Result: SHRVar方法的错误概率呈指数下降，性能优于SH和SHVar方法，数值实验验证了其优越性。

Conclusion: 该框架和方法在多指标和异方差情况下显著提升了实验效率和统计推断能力，具有实用价值。

Abstract: Standard A/B tests in online experiments face statistical power challenges
when testing multiple candidates simultaneously, while adaptive experimental
designs (AED) alone fall short in inferring experiment statistics such as the
average treatment effect, especially with many metrics (e.g., revenue, safety)
and heterogeneous variances. This paper proposes a fixed-budget multi-metric
AED framework with a two-phase structure: an adaptive exploration phase to
identify the best treatment, and a validation phase with an A/B test to verify
the treatment's quality and infer statistics. We propose SHRVar, which
generalizes sequential halving (SH) (Karnin et al., 2013) with a novel
relative-variance-based sampling and an elimination strategy built on reward
z-values. It achieves a provable error probability that decreases
exponentially, where the exponent generalizes the complexity measure for SH
(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and
heterogeneous variances, respectively. Numerical experiments verify our
analysis and demonstrate the superior performance of this new framework.

</details>


### [222] [Provable Reinforcement Learning from Human Feedback with an Unknown Link Function](https://arxiv.org/abs/2506.03066)
*Qining Zhang,Lei Ying*

Key words: RLHF, 零阶策略优化, 链接函数, 人类偏好, ZSPO

TL;DR: 该论文提出了一种名为ZSPO的新算法，用于解决RLHF问题中链接函数未知的情况，通过零阶策略优化方法，利用人类偏好估计策略梯度方向，而不需要知道链接函数的具体形式。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于人类偏好的复杂性，现有RLHF算法（如DPO和PPO）假设链接函数已知是不现实的。为了避免链接函数误设，作者研究了链接函数未知的一般RLHF问题。

Method: 提出ZSPO算法，基于零阶策略优化方法，通过人类偏好构建与真实策略梯度方向正相关的参数更新方向，无需知道链接函数的具体形式。

Result: ZSPO在多轮策略迭代和每轮轨迹数的情况下，以多项式收敛速率收敛到平稳策略，数值结果证明了其在链接函数不匹配情况下的优越性。

Conclusion: ZSPO为解决链接函数未知的RLHF问题提供了一种有效方法，具有理论保证和实际性能优势。

Abstract: Link functions, which characterize how human preferences are generated from
the value function of an RL problem, are a crucial component in designing RLHF
algorithms. Almost all RLHF algorithms, including state-of-the-art ones in
empirical studies such as DPO and PPO, assume the link function is known to the
agent (e.g., a logistic function according to the Bradley-Terry model), which
is arguably unrealistic considering the complex nature of human preferences. To
avoid link function mis-specification, this paper studies general RLHF problems
with unknown link functions. We propose a novel policy optimization algorithm
called ZSPO based on a new zeroth-order policy optimization method, where the
key is to use human preference to construct a parameter update direction that
is positively correlated with the true policy gradient direction. ZSPO achieves
it by estimating the sign of the value function difference instead of
estimating the gradient from the value function difference, so it does not
require knowing the link function. Under mild conditions, ZSPO converges to a
stationary policy with a polynomial convergence rate depending on the number of
policy iterations and trajectories per iteration. Numerical results also show
the superiority of ZSPO under link function mismatch.

</details>


### [223] [Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness](https://arxiv.org/abs/2506.03075)
*Bogdan Chornomaz,Yonatan Koren,Shay Moran,Tom Waknine*

Key words: 对抗性攻击, 学习理论, VC维度, 随机学习器, 不可知设置

TL;DR: 该论文解决了在不可知设置下对抗性实例目标中毒攻击的最优超额误差问题，证明了其规模为$	ilde{\Theta}(\sqrt{d\eta})$，并展示了随机学习器的必要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在存在对抗性实例目标中毒攻击的情况下，学习问题的最优超额误差界限，填补了现有研究的空白。

Method: 通过理论分析，证明了在不可知设置下，最优超额误差为$	ilde{\Theta}(\sqrt{d\eta})$，并探讨了随机学习器的必要性。

Result: 确定了在对抗性攻击下，随机学习器的最优超额误差界限，并展示了确定性学习器的局限性。

Conclusion: 随机学习器在对抗性实例目标中毒攻击下具有优势，为未来研究提供了新的方向。

Abstract: We study the problem of learning in the presence of an adversary that can
corrupt an $\eta$ fraction of the training examples with the goal of causing
failure on a specific test point. In the realizable setting, prior work
established that the optimal error under such instance-targeted poisoning
attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the
hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding
question in the agnostic setting. We show that the optimal excess error is
$\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by
Hanneke et al. To achieve this rate, it is necessary to use randomized
learners: Hanneke et al. showed that deterministic learners can be forced to
suffer error close to 1, even under small amounts of poisoning. Perhaps
surprisingly, our upper bound remains valid even when the learner's random bits
are fully visible to the adversary . In the other direction, our lower bound is
stronger than standard PAC-style bounds: instead of tailoring a hard
distribution separately for each sample size, we exhibit a single fixed
distribution under which the adversary can enforce an excess error of
$\Omega(\sqrt{d\eta})$ infinitely often.

</details>


### [224] [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/abs/2506.03077)
*Qijun Luo,Mengqi Li,Lei Zhao,Xiao Li*

Key words: StreamBP, 反向传播, 内存效率, 长序列训练, 梯度检查点

TL;DR: 提出一种名为StreamBP的内存高效且精确的反向传播方法，显著降低激活值和逻辑的内存消耗，适用于多种常见目标，并支持多GPU训练。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 训练长序列数据对提升模型能力至关重要，但反向传播过程中存储激活值的内存成本极高，即使使用梯度检查点技术。

Method: 通过线性分解链式规则，逐层处理序列维度，利用因果结构减少计算量和加速反向传播。

Result: StreamBP将反向传播的最大序列长度扩展了2.8-5.5倍，计算FLOPs更少且反向传播速度更快，支持批量大小扩展。

Conclusion: StreamBP是一种高效、可扩展的反向传播方法，适用于多种任务和模型，支持分布式训练。

Abstract: Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.

</details>


### [225] [Non-Asymptotic Length Generalization](https://arxiv.org/abs/2506.03085)
*Thomas Chen,Tengyu Ma,Zhiyuan Li*

Key words: 长度泛化, 非渐近框架, 长度复杂性, C-RASP, 确定性有限自动机

TL;DR: 本文研究了长度泛化的理论保证，包括形式化框架、最优算法及对不同函数类的长度复杂性分析。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨学习算法在训练集外长输入上的泛化能力，为非渐近长度泛化提供理论保证。

Method: 形式化非渐近长度泛化框架，分析不同函数类的长度复杂性，如最小复杂度插值算法。

Result: 确定性有限自动机的长度复杂性为2n-2；C-RASP函数类的复杂性随层数和头数增长。

Conclusion: 长度泛化与语言等价问题可解性相关，不同函数类的长度复杂性有显著差异。

Abstract: Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [226] [How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment](https://arxiv.org/abs/2506.03087)
*Bin Ma,Yuyuan Feng,Minhua Lin,Enyan Dai*

Key words: 图神经网络, 模型窃取, 可解释性, 安全风险, 数据增强

TL;DR: 该论文研究了可解释性图神经网络（GNNs）在安全领域的潜在风险，提出了一个名为{\method}的新型模型窃取框架，通过结合解释对齐和引导数据增强，成功复制了目标模型的预测行为和推理模式。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于GNNs在药物发现和金融分析等领域的广泛应用，模型透明性需求增加。然而，可解释性机制可能暴露模型的决策逻辑，导致安全风险。

Method: 论文提出了{\method}框架，结合解释对齐和引导数据增强，以有限的查询次数高效训练，复制目标模型的预测行为和推理模式。

Result: 实验在分子图数据集上展示了该框架在模型窃取中的优势，超越了传统方法。

Conclusion: 该研究强调了可解释性GNNs在敏感领域部署时的安全考虑，并提出了针对基于解释的攻击的保护措施需求。

Abstract: Graph Neural Networks (GNNs) have become essential tools for analyzing
graph-structured data in domains such as drug discovery and financial analysis,
leading to growing demands for model transparency. Recent advances in
explainable GNNs have addressed this need by revealing important subgraphs that
influence predictions, but these explanation mechanisms may inadvertently
expose models to security risks. This paper investigates how such explanations
potentially leak critical decision logic that can be exploited for model
stealing. We propose {\method}, a novel stealing framework that integrates
explanation alignment for capturing decision logic with guided data
augmentation for efficient training under limited queries, enabling effective
replication of both the predictive behavior and underlying reasoning patterns
of target models. Experiments on molecular graph datasets demonstrate that our
approach shows advantages over conventional methods in model stealing. This
work highlights important security considerations for the deployment of
explainable GNNs in sensitive domains and suggests the need for protective
measures against explanation-based attacks. Our code is available at
https://github.com/beanmah/EGSteal.

</details>


### [227] [From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](https://arxiv.org/abs/2506.03093)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Key words: 稀疏自编码器、神经网络可解释性、匹配追踪、非线性特征、分层特征

TL;DR: 稀疏自编码器（SAE）的传统假设认为神经网络特征是线性可访问且正交的，但近期研究发现特征具有分层、非线性和多维性。本文提出MP-SAE方法，能更好地捕捉复杂特征，验证了特征不仅是线性可访问的。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究神经网络特征是否具有分层和非线性结构，挑战传统SAE的线性正交假设。

Method: 提出MP-SAE，基于匹配追踪算法，利用残差引导步骤捕捉分层和非线性特征。

Result: MP-SAE能恢复有意义的特征，揭示视觉-语言模型中共享的结构，并支持自适应稀疏性。

Conclusion: 特征的可解释性应从表征现象学出发，方法需适应其特征假设。

Abstract: Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.

</details>


### [228] [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
*Yang Guo,Yutian Tao,Yifei Ming,Robert D. Nowak,Yingyu Liang*

Key words: 检索增强生成（RAG）、上下文学习（ICL）、泛化界、偏差-方差权衡、QA基准

TL;DR: 本文首次提出了检索增强生成（RAG）在上下文线性回归中的有限样本泛化界，并推导了精确的偏差-方差权衡。理论分析表明，与上下文学习（ICL）相比，RAG存在固有泛化误差上限。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 尽管RAG在实证研究中取得了成功，但其理论基础尚未充分探索。本文旨在通过理论分析填补这一空白。

Method: 将检索文本视为依赖于查询的噪声上下文示例，构建了统一的框架，并引入了均匀和非均匀RAG噪声来建模从训练数据和外部语料库的检索。

Result: 理论分析揭示了RAG的固有泛化误差上限，实验在Natural Questions和TriviaQA等QA基准上验证了ICL和RAG的样本效率。

Conclusion: 本文为RAG提供了理论基础，揭示了其在泛化误差上的局限性，并通过实验验证了理论结果。

Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.

</details>


### [229] [On Weak-to-Strong Generalization and f-Divergence](https://arxiv.org/abs/2506.03109)
*Wei Yao,Gengze Xu,Huayi Tang,Wenkai Yang,Donglin Di,Ziqiao Wang,Yong Liu*

Key words: 弱监督到强监督学习, $f$-散度, 信息论损失, 噪声容忍, 泛化能力

TL;DR: 提出了一种基于$f$-散度的弱监督到强监督学习（W2SG）方法，通过理论分析和实验验证其有效性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有W2SG方法通常需要额外的弱模型或复杂流程，导致计算和内存开销较大，而$f$-散度在机器学习中表现出色，因此探讨其在W2SG中的应用。

Method: 引入$f$-散度作为W2SG中的信息论损失函数框架，通过理论分析和实验验证其有效性。

Result: $f$-散度损失能有效提高强模型的泛化能力和噪声容忍度，且具有理论支持。

Conclusion: $f$-散度在W2SG中是一种高效且通用的损失函数选择。

Abstract: Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for
stimulating the capabilities of strong pre-trained models by leveraging
supervision from weaker supervisors. To improve the performance of the strong
model, existing methods often require additional weak models or complex
procedures, leading to substantial computational and memory overhead. Motivated
by the effectiveness of $f$-divergence loss in various machine learning
domains, we introduce $f$-divergence as an information-theoretic loss function
framework in W2SG. Our theoretical analysis reveals fundamental limitations and
equivalence of different $f$-divergence losses in W2SG, supported by sample
complexity bounds and information-theoretic insights. We empirically
demonstrate that $f$-divergence loss, which generalizes widely-used metrics
like KL divergence, effectively improves generalization and noise tolerance of
the strong model in practice.

</details>


### [230] [Rectified Flows for Fast Multiscale Fluid Flow Modeling](https://arxiv.org/abs/2506.03111)
*Victor Armegioiu,Yannick Ramic,Siddhartha Mishra*

Key words: 流体流动、统计建模、扩散模型、ODE、推理效率

TL;DR: 提出了一种名为rectified flow的方法，通过优化流动轨迹，显著减少了流体流动统计建模的推理步骤，同时保持高保真度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 流体流动的统计建模由于多尺度动态和初始条件极端敏感而极具挑战性。现有扩散模型需要大量推理步骤，效率较低。

Method: 引入rectified flow框架，学习时间依赖的速度场，将输入分布沿近乎直线轨迹传输到输出分布，将采样转化为ODE求解问题。

Result: 实验表明，该方法在仅需8步推理时，与标准分数扩散模型（128步以上）具有相同的后验分布和高保真度，推理时间大幅缩短。

Conclusion: rectified flow能在极少量推理步骤下恢复扩散模型的高保真结果，同时保留细粒度特征，显著提升效率。

Abstract: The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.

</details>


### [231] [Zero-Shot Time Series Forecasting with Covariates via In-Context Learning](https://arxiv.org/abs/2506.03128)
*Andreas Auer,Raghul Parthipan,Pedro Mercado,Abdul Fatir Ansari,Lorenzo Stella,Bernie Wang,Michael Bohlke-Schneider,Syama Sundar Rangapuram*

Key words: 零样本预测,协变量,上下文学习,时间序列,COSMIC

TL;DR: COSMIC是一种利用协变量进行零样本预测的模型，通过上下文学习有效整合协变量，解决了数据稀缺问题，并实现了最先进的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有预训练时间序列模型无法有效支持或整合协变量，影响了零样本预测的性能和可访问性。

Method: 提出了COSMIC模型，采用上下文学习利用协变量，并提出信息性协变量增强方法以解决数据稀缺问题。

Result: COSMIC在零样本预测中达到了最先进的性能，无论是否使用协变量。定量和定性分析表明其有效利用了协变量。

Conclusion: COSMIC通过创新的协变量利用方法和数据增强策略，显著提升了零样本时间序列预测的表现。

Abstract: Pretrained time series models, capable of zero-shot forecasting, have
demonstrated significant potential in enhancing both the performance and
accessibility of time series forecasting. However, existing pretrained models
either do not support covariates or fail to incorporate them effectively. We
introduce COSMIC, a zero-shot forecasting model that utilizes covariates via
in-context learning. To address the challenge of data scarcity, we propose
Informative Covariate Augmentation, which enables the training of COSMIC
without requiring any datasets that include covariates. COSMIC achieves
state-of-the-art performance in zero-shot forecasting, both with and without
covariates. Our quantitative and qualitative analysis demonstrates that COSMIC
effectively leverages covariates in zero-shot forecasting.

</details>


### [232] [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/abs/2506.03133)
*Kai Lion,Liang Zhang,Bingcong Li,Niao He*

Key words: 低秩适应,极分解,Stiefel流形,黎曼优化

TL;DR: 论文提出PoLAR方法，通过极分解参数化解决低秩适应中稳定秩不足的问题，提升了微调性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大规模模型的低秩适应存在稳定秩低于子空间线性代数秩的问题，影响微调表现。

Method: 引入PoLAR，利用极分解将低秩更新因子化为两个Stiefel流形约束的方向矩阵和一个无约束的比例矩阵。通过黎曼优化提升性能。

Result: 理论表明PoLAR在低秩适应问题中实现指数级更快收敛，实验在350M到27B的模型上验证了其在语言理解、常识推理和数学解题中的优势。

Conclusion: PoLAR通过改进参数化方法显著提升低秩适应的性能。

Abstract: We show that low-rank adaptation of large-scale models suffers from a low
stable rank that is well below the linear algebraic rank of the subspace,
degrading fine-tuning performance. To mitigate the underutilization of the
allocated subspace, we propose PoLAR, a parameterization inspired by the polar
decomposition that factorizes the low-rank update into two direction matrices
constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory
shows that PoLAR yields an exponentially faster convergence rate on a canonical
low-rank adaptation problem. Pairing the parameterization with Riemannian
optimization leads to consistent gains on three different benchmarks testing
general language understanding, commonsense reasoning, and mathematical problem
solving with base model sizes ranging from 350M to 27B.

</details>


### [233] [Not All Tokens Are Meant to Be Forgotten](https://arxiv.org/abs/2506.03142)
*Xiangyu Zhou,Yao Qiang,Saleh Zare Zade,Douglas Zytko,Prashant Khanduri,Dongxiao Zhu*

Key words: 大规模语言模型（LLMs），遗忘机制，目标信息遗忘（TIF），隐私保护

TL;DR: 论文提出了一种名为TIF的框架，通过区分不需要的词语和通用词语，并采用目标偏好优化方法，有效解决大规模语言模型（LLMs）遗忘信息时过度遗忘的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LLMs能够记忆大量文本，但也可能记住隐私或版权内容，引发隐私和法律问题。现有遗忘方法导致过度遗忘，模型性能下降。TIF框架旨在解决这一问题。

Method: TIF框架包括两部分：（1）目标信息标识器，区分不需要的词语（UW）和通用词语（GW）；（2）目标偏好优化方法，结合Logit Preference Loss遗忘UW，用Preservation Loss保留GW。

Result: 在TOFU和MUSE基准测试中，TIF框架显著提高了遗忘效果，同时保持了模型性能，达到了最优结果。

Conclusion: TIF框架有效解决了LLMs遗忘信息时的过度遗忘问题，提升了遗忘效果并保持模型性能。

Abstract: Large Language Models (LLMs), pre-trained on massive text corpora, exhibit
remarkable human-level language understanding, reasoning, and decision-making
abilities. However, they tend to memorize unwanted information, such as private
or copyrighted content, raising significant privacy and legal concerns.
Unlearning has emerged as a promising solution, but existing methods face a
significant challenge of over-forgetting. This issue arises because they
indiscriminately suppress the generation of all the tokens in forget samples,
leading to a substantial loss of model utility. To overcome this challenge, we
introduce the Targeted Information Forgetting (TIF) framework, which consists
of (1) a flexible targeted information identifier designed to differentiate
between unwanted words (UW) and general words (GW) in the forget samples, and
(2) a novel Targeted Preference Optimization approach that leverages Logit
Preference Loss to unlearn unwanted information associated with UW and
Preservation Loss to retain general information in GW, effectively improving
the unlearning process while mitigating utility degradation. Extensive
experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF
framework enhances unlearning effectiveness while preserving model utility and
achieving state-of-the-art results.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [234] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)
*Priyaranjan Pattnayak,Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Srikant Panda*

Key words: 检索增强生成（RAG）, 意图系统, 对话AI, 企业应用

TL;DR: 提出了一种结合检索增强生成（RAG）和预定义回应的混合框架，用于解决企业级对话AI中的高延迟、幻觉等问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 企业级对话AI面临多样查询、高延迟、频繁更新的领域知识等挑战，需要高效且可扩展的解决方案。

Method: 集成RAG与预定义高置信度回应，动态路由复杂查询，并利用反馈循环优化意图和响应范围。

Result: 实验显示该框架在准确性（95%）和延迟（180ms）上优于传统RAG和意图系统，适用于多样化查询。

Conclusion: 提出的混合框架为企业级对话AI提供了一个可扩展且自适应的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [235] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/abs/2506.02125)
*Guy Tennenholtz,Jihwan Jeong,Chih-Wei Hsu,Yinlam Chow,Craig Boutilier*

Key words: 部分可观测环境, 描述性历史表征, 多智能体学习, 用户建模

TL;DR: 论文提出了一种描述性历史表征（DHRs）方法，用于压缩部分可观测环境中的交互历史，以支持有效决策。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 在部分可观测环境中，有效决策需要将长交互历史压缩为信息丰富的表征。

Method: 提出了多智能体学习框架，包括表征、决策和提问组件，通过联合优化目标平衡奖励最大化和表征能力。

Result: 在电影和购物数据集上验证了方法，生成可解释的用户画像，用于预测用户偏好行为。

Conclusion: DHRs能够捕获关键历史细节和预测结构，为有效决策提供支持。

Abstract: Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [236] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)
*Edward Y. Chang*

Key words: 大语言模型, 少样本学习, 统一认知意识理论, 语义锚定, AGI

TL;DR: 本文提出统一认知意识理论（UCCT），认为大语言模型（LLMs）是非完整认知的基础组件，而非缺陷。通过语义锚定，UCCT统一了提示、微调等方法，并形式化为概率相位转换。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨LLMs在少样本学习中的矛盾表现，提出新理论解释其作为通用智能基础组件的作用。

Method: 提出UCCT理论，将LLMs视为无意识底层结构，语义锚定作为控制层，并形式化为概率相位转换。

Result: UCCT为提示、微调等方法提供统一解释，并支持LLMs作为AGI系统核心的观点。

Conclusion: AGI的实现需整合LLMs而非抛弃，通过语义锚定实现协同推理与适应。

Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [237] [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
*Peter Belcak,Greg Heinrich,Shizhe Diao,Yonggan Fu,Xin Dong,Saurav Muralidharan,Yingyan Celine Lin,Pavlo Molchanov*

Key words: 小型语言模型, 代理AI, 经济性, 异构系统, 转换算法

TL;DR: 论文主张小型语言模型（SLMs）在代理人工智能系统中比大型语言模型（LLMs）更具优势，适合特定任务的重复执行，且更经济高效。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前代理AI系统中，LLMs被广泛使用，但SLMs因其专用性、经济性和高效性更适合某些场景。

Method: 论文基于SLMs的能力、代理系统架构和经济性分析，提出了从LLMs转向SLMs的观点，并设计了转换算法。

Result: 通过分析，论文表明SLMs在代理系统中具有显著的操作和经济优势，适合未来AI代理的发展。

Conclusion: SLMs是代理AI的未来，异构代理系统（结合多种模型）可能更适用于需要通用对话能力的场景。

Abstract: Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [238] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/abs/2506.02158)
*Ruhana Azam,Aditya Vempaty,Ashish Jagmohan*

Key words: 网络导航,自我反思,ReAP,经验学习

TL;DR: 论文提出了一种基于自我反思的网络导航系统ReAP，通过利用过去成功和失败的经验，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的网络导航代理缺乏记忆能力，导致重复错误和无法从过去的交互中学习。

Method: 提出Reflection-Augment Planning (ReAP)方法，利用自我反思整合成功和失败的经验。

Result: ReAP将基线结果提升了11个百分点，在以前失败的任务上提升了29个百分点。

Conclusion: 自我反思能够有效提升网络导航任务的性能，并具有跨任务迁移的能力。

Abstract: Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [239] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)
*Haizhong Zheng,Yang Zhou,Brian R. Bartoldson,Bhavya Kailkhura,Fan Lai,Jiawei Zhao,Beidi Chen*

Key words: 强化学习, GRPO, GRESO, 计算开销, 数学推理

TL;DR: 论文提出GRESO算法，通过跳过无信息的提示来减少强化学习中的计算开销，显著加速训练时间且不影响准确性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决强化学习（如PPO和GRPO）在扩展提示采样时带来的高计算开销问题。

Method: 提出GRESO（基于奖励动态的轻量级预过滤算法），在线预测并跳过无信息的提示。

Result: 在多项数学推理基准和模型上，GRESO实现最高2.4倍的滚动加速和2.0倍的总训练加速，无精度损失。

Conclusion: GRESO通过高效跳过无用提示，显著提升训练效率。

Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [240] [Natural, Artificial, and Human Intelligences](https://arxiv.org/abs/2506.02183)
*Emmanuel M. Pothos,Dominic Widdows*

Key words: 人类智能、聊天机器人、语言、身体性、自我意识

TL;DR: 探讨人类智能的独特性，结合心理学、动物智能、语言作用及人工智能进展，提出人类智能的四个关键要素，并认为当前聊天机器人因缺乏身体性和自我意识而受限。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨人类智能是否独特，以及现代聊天机器人是否能被视为智能体。

Method: 结合心理学文献、动物智能证据、语言的作用、人工智能进展及智能测试历史进行分析。

Result: 人类智能的独特成就依赖于语言和其他四个关键要素，而聊天机器人因缺乏身体性和自我意识而受限。

Conclusion: 人类智能与非人类动物智能的差异主要在语言复杂性上，聊天机器人需克服身体性和意识问题才能接近人类智能。

Abstract: Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [241] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)
*Maxime Robeyns,Laurence Aitchison*

Key words: Large Language Models, code generation, code quality, GRPO

TL;DR: 该论文研究了LLMs在代码生成中的应用，提出了一种量化代码质量的方法，并通过GRPO提高了代码质量。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 当前的代码生成模型主要关注代码的功能正确性，忽略了代码的可维护性、质量和安全性。论文旨在填补这一空白。

Method: 开发了一个全面的代码质量量化库，并将其作为GRPO的奖励信号。

Result: GRPO提高了代码质量，并通过专家盲审验证了这一结果。

Conclusion: 量化代码质量可以有效提升代码生成模型的性能。

Abstract: Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [242] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)
*Kedir Yassin Hussen,Walelign Tewabe Sewunetie,Abinew Ali Ayele,Sukairaj Hafiz Imam,Shamsuddeen Hassan Muhammad,Seid Muhie Yimam*

Key words: 大型语言模型、低资源语言、非洲语言、语料库、标准化

TL;DR: 非洲低资源语言在大型语言模型（LLM）中的覆盖率极低，仅有42种语言得到支持，98%的语言被忽视；主要挑战包括数据缺乏、标记偏倚、高计算成本及评估问题，需要社区合作推动标准化与语料开发。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨非洲2000种低资源语言在LLMs中的覆盖率不足问题，揭示技术局限性并提出改进方向。

Method: 通过比较六种LLMs、八种SLMs和六种SSLMs的非洲语言覆盖情况，分析数据集、脚本问题及技术限制。

Result: 发现仅四语言（阿姆哈拉语、斯瓦希里语、南非荷兰语、马达加斯加语）被广泛支持，20种活跃脚本被忽视；提出标准化与社区开发的必要性。

Conclusion: 需通过标准化、语料库建设及适应性方法解决非洲语言在LLMs中的技术障碍。

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [243] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
*Tianyu Hua,Harper Hua,Violet Xiang,Benjamin Klieger,Sang T. Truong,Weixin Liang,Fan-Yun Sun,Nick Haber*

Key words: 大语言模型、代码生成、基准测试、机器学习研究

TL;DR: ResearchCodeBench是一个包含212个编码挑战的基准测试，用于评估大语言模型（LLMs）将最新研究论文中的前沿机器学习贡献转化为可执行代码的能力。研究发现，即使是表现最好的模型也只有不到40%的正确率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大语言模型在机器学习研究中展现出潜力，但其能否忠实实现预训练期间未见的新颖研究想法尚不明确。这是为了解决这一不确定性而进行的研究。

Method: 引入ResearchCodeBench基准测试，包含212个编码挑战，评估30多种专有和开源LLMs的代码生成能力。

Result: 表现最好的模型是Gemini-2.5-Pro-Preview，成功率为37.3%，其次是O3（High）和O4-mini（High），分别为32.3%和30.8%。所有模型的正确率均低于40%。

Conclusion: ResearchCodeBench为社区提供了一个严格的评估平台，有助于持续理解和推进LLM在研究代码生成中的创新。

Abstract: Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [244] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.02387)
*Zelai Xu,Zhexuan Xu,Xiangmin Yi,Huining Yuan,Xinlei Chen,Yi Wu,Chao Yu,Yu Wang*

Key words: Vision Language Models, Multi-agent, Strategic Reasoning, Benchmark, Multimodal

TL;DR: VS-Bench是一个多模态基准测试，用于评估视觉语言模型在多智能体环境中的战略推理和决策能力，覆盖合作、竞争和混合动机场景，揭示了当前模型的性能差距。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基准测试局限于单智能体或纯文本环境，而现实场景涉及多智能体在丰富的视觉和语言上下文中的交互，需评估模型的战略推理和决策能力。

Method: VS-Bench包含8个视觉基础环境，通过离线评估（预测准确性）和在线评估（规范化的任务回报）两个维度测试模型。

Result: 14个领先的视觉语言模型中，最佳模型在预测准确性和规范化回报上的表现分别为47.8%和24.3%，显示出显著性能差距。

Conclusion: VS-Bench为未来战略多模态智能体研究提供了标准化评估基础，并凸显了现有模型的局限性。

Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [245] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)
*Shengjia Zhang,Junjie Wu,Jiawei Chen,Changwang Zhang,Xingyu Lou,Wangchunshu Zhou,Sheng Zhou,Can Wang,Jun Wang*

Key words: 大型推理模型, 冗余推理, 链式思维, OThink-R1, 效率优化

TL;DR: 该论文提出了一种名为OThink-R1的方法，通过识别冗余与必要的推理步骤，动态简化推理过程，从而提高大型推理模型的效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管大型推理模型（LRMs）在复杂任务中表现出色，但研究发现许多简单任务可以通过非推理LLMs用更少的标记解决，表明复杂推理并非总是必要。

Method: 论文分析了LRMs的推理轨迹，利用LLM-Judge分类为冗余或必要推理，并引入OThink-R1方法动态切换快速与慢速推理模式。

Result: 实验表明，OThink-R1在数学和问答任务中平均减少23%的冗余推理，同时保持准确性。

Conclusion: OThink-R1为高效推理模型提供了实用指导，优化了推理过程。

Abstract: Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [246] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)
*Tri Cao,Bennett Lim,Yue Liu,Yuan Sui,Yuexin Li,Shumin Deng,Lin Lu,Nay Oo,Shuicheng Yan,Bryan Hooi*

Key words: CUAs, BUAs, 视觉提示注入攻击, 多模态AI代理, 安全性

TL;DR: 研究CUAs和BUAs在视觉提示注入攻击中的脆弱性，提出VPI-Bench评测基准，发现当前代理易受攻击，系统提示防御效果有限。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于CUAs具有系统级访问权，其安全性和隐私风险尚未充分研究，尤其是视觉提示注入攻击的影响。

Method: 提出VPI-Bench基准，包含306个测试用例，评估代理在视觉提示注入攻击下的鲁棒性。

Result: CUAs和BUAs在某些平台上的攻击成功率分别高达51%和100%，系统提示防御效果有限。

Conclusion: 需要开发更鲁棒、情境感知的防御机制以保障多模态AI代理的安全部署。

Abstract: Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [247] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)
*Xuejiao Zhao,Siyan Liu,Su-Yin Yang,Chunyan Miao*

Key words: 误诊, 多模态, 检索增强生成, 知识图谱, 医疗助手

TL;DR: MedRAG是一种多模态医疗助手，结合LLM和知识图谱，旨在减少误诊并提高决策准确性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 误诊对医疗系统和患者造成重大危害，需智能辅助工具提升决策效率。

Method: 利用检索增强生成和知识图谱推理，支持语音、查询和电子病历等多模态输入。

Result: 在公开和私有数据集上表现优异，提供更准确、具体的医疗建议。

Conclusion: MedRAG通过多模态和知识图谱增强，显著提升医疗决策的准确性和效率。

Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [248] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)
*Haowen Xu,Sisi Zlatanova,Ruiyu Liang,Ismet Canbulat*

Key words: 野火预测, 生成式AI, 多模态数据, 3D模拟, 人类-AI协作

TL;DR: 论文探讨了如何利用生成式AI改进野火预测和模拟，提出了五方面的未来愿景，并讨论了实施中的挑战和解决方案。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统方法在野火预测和模拟中存在局限性，生成式AI提供了整合多模态数据和改进模拟的新机遇。

Method: 采用生成对抗网络（GANs）、变分自编码器（VAEs）、Transformer和扩散模型等生成式AI方法，结合人类-AI协作框架。

Result: 生成式AI能增强2D野火扩散预测和生成更真实的3D模拟，同时支持自动知识提取和文献合成。

Conclusion: 生成式AI有潜力成为野火管理的基础框架，但需解决相关挑战以实现其全部潜力。

Abstract: Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [249] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)
*Xu Wan,Wenyue Xu,Chao Yang,Mingyang Sun*

Key words: LLMs, RL, 决策, ACE, 电网操作

TL;DR: 提出ACE框架，结合LLMs和RL解决大规模工业决策问题，通过双角色轨迹优化机制提升性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: LLMs缺乏实时长序列决策能力，RL在大规模动作空间中样本效率低，需一种结合两者优势的方法。

Method: ACE框架通过双角色轨迹优化机制，LLMs同时作为策略执行者和价值评判者，RL生成高质量微调数据。

Result: 在超过60K离散动作的电网操作任务中，ACE表现优于现有RL和LLM方法。

Conclusion: ACE有效结合LLMs和RL的优势，解决了大规模决策问题。

Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [250] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/abs/2506.02565)
*Zhuoxuan Jiang,Tianyang Zhang,Peiyan Peng,Jing Chen,Yinong Xun,Haotian Zhang,Lichi Li,Yong Li,Shaohua Zhang*

Key words: 几何问题生成, 符号推导引擎, 多模态, 语言转换

TL;DR: 本文提出了一种基于符号推导引擎的几何问题生成框架（SDE-GPG），旨在解决几何问题生成的挑战，支持多模态格式和非正式与正式语言之间的转换。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 几何问题生成在教育中具有重要意义，但传统方法难以处理多模态格式和非正式与正式语言之间的转换。

Method: 提出SDE-GPG框架，包含四个步骤：知识点映射表搜索、扩展定义采样与符号推导、问题过滤、文本和图表生成。

Result: 实验表明，SDE-GPG能有效生成可读、可解且可控的几何问题。

Conclusion: SDE-GPG框架成功解决了几何问题生成的多模态和语言转换挑战，具有实际应用价值。

Abstract: Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [251] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)
*Dongzhe Fan,Yi Fang,Jiajin Liu,Djellel Difallah,Qiaoyu Tan*

Key words: 多模态图、大型语言模型、图学习、多模态编码器、指令调优

TL;DR: 论文提出了一种名为MLaGA的多模态大型语言与图助手模型，旨在解决多模态图分析中的未充分探索问题，通过结构感知的多模态编码器和多模态指令调优方法，显著提升了图学习任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的基于大型语言模型（LLM）的图方法主要针对文本丰富的图，而多模态图（包含多种属性类型，如文本和图像）的应用尚未充分探索。本文旨在填补这一空白。

Method: 设计了结构感知的多模态编码器，通过联合图预训练目标将文本和视觉属性对齐到统一空间；随后采用多模态指令调优方法，通过轻量级投影器将多模态特征和图结构整合到LLM中。

Result: 在多个数据集上的实验表明，MLaGA在监督和迁移学习场景下均优于现有基线方法，展现了多样图学习任务中的卓越性能。

Conclusion: MLaGA成功扩展了LLM的能力，使其能够处理复杂的图结构和多模态属性，为多模态图分析提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [252] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/abs/2506.02576)
*Haichen Wang,Liu Yang,Xinyuan Zhang,Haomin Yu,Ming Li,Jilin Hu*

Key words: 需求预测, 时空数据, 注意力机制, 差分注意力, 聚合策略

TL;DR: 提出了Aggregation Differential Transformer (ADFormer)，通过差分注意力捕捉原始空间相关性并进行去噪，设计了时空聚合策略，统一原始与高层相关性，提升需求预测性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基于注意力机制的方法难以完全适应复杂的时空相关性，且忽略了现实世界中的高层相关性，因此需要一种能整合原始与高层相关性的方法。

Method: 提出了ADFormer，利用差分注意力捕捉原始空间相关性并进行去噪，设计了基于时空特性的不同聚合策略，统一原始与高层相关性。

Result: 在出租车和自行车数据集上的实验验证了模型的有效性和效率，展示了其实际应用价值。

Conclusion: ADFormer通过整合原始与高层时空相关性，显著提升了需求预测性能。

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [253] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/abs/2506.02580)
*Xuewen Luo,Fengze Yang,Fan Ding,Xiangbo Gao,Shuo Xing,Yang Zhou,Zhengzhong Tu,Chenxi Liu*

Key words: 自动驾驶, V2X, 知识驱动, 多模态, RAG

TL;DR: V2X-UniPool框架通过整合多模态V2X数据，解决自动驾驶系统感知受限和幻觉问题，显著提升运动规划和推理能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 自动驾驶系统因单车辆传感器短视和环境实时性不足面临感知受限和幻觉问题。

Method: 提出V2X-UniPool框架，集成多模态V2X数据，采用双查询RAG机制检索静态和动态知识。

Result: 实验表明，V2X-UniPool显著提升运动规划精度和推理能力，零样本车辆模型性能达SOTA，传输成本降低99.9%。

Conclusion: V2X-UniPool有效解决自动驾驶系统关键挑战，提升性能并降低成本。

Abstract: Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [254] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)
*Ruibo Duan,Yuxin Liu,Xinyao Dong,Chenglin Fan*

Key words: 组合优化、语言模型、对抗性生成、启发式算法、自动化求解器

TL;DR: EALG 是一个新型框架，通过语言模型自动化共同进化解的组合优化问题实例及其启发式求解器，动态生成更难的实例并合成适应性算法，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 通过自动生成挑战性实例和求解器，推动组合优化解算器的评估与发展，解决现有静态基准或手动设计的局限性。

Method: 基于进化的对抗性生成方法（EALG），利用语言模型动态进化解的生成过程和启发式算法。

Result: EALG 生成的实例比当前基准更难，其合成的求解器在多种组合任务中表现出高效泛化能力。

Conclusion: EALG 提出了一种整合实例生成与求解器设计的新范式，实现了最先进的性能。

Abstract: Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [255] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/abs/2506.02609)
*Tianfan Jiang,Mei Wu,Wenchao Weng,Dewen Seng,Yiqian Lin*

Key words: 交通流量预测，时空动态性，数据解耦，时间特征提取，动态图

TL;DR: 该论文提出了一种名为TEDDN的新方法，用于解决交通流量预测中复杂的时空依赖性和动态性问题，通过分离和提取稳定的交通模式与趋势，显著提升了预测效果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 由于交通数据的时空动态性和多样性，传统端到端训练的时空网络难以有效处理复杂的交通流量模式依赖关系。此外，现有研究对时间信息的重视不足，导致预测效果受限。

Method: 提出了TEDDN网络，通过动态图和时间特征提取模块，将复杂的交通数据解耦为稳定的模式和趋势，灵活学习时空信息和节点信息。

Result: 在四个真实数据集上的实验验证了TEDDN的优越性，表明其在解耦和提取复杂交通信息方面有显著效果。

Conclusion: TEDDN方法通过增强时间信息的利用和动态图的设计，显著提升了交通流量预测的精度和鲁棒性。

Abstract: In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [256] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)
*Yue Yang,MingKang Chen,Qihua Liu,Mengkang Hu,Qiguang Chen,Gengrui Zhang,Shuyue Hu,Guangtao Zhai,Yu Qiao,Yu Wang,Wenqi Shao,Ping Luo*

Key words: 大语言模型, 流体智能, 抽象推理, DRE-Bench, 认知层次

TL;DR: DRE-Bench是一个动态推理评估基准，用于测试大语言模型在抽象推理和多层次认知任务中的表现，揭示其在高级认知任务中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 评估大语言模型是否具备流体智能（抽象推理和新情境中的规则泛化能力），填补现有基准在可解释性和领域通用性上的不足。

Method: 提出DRE-Bench基准，包含36个抽象推理任务，分为四个认知层次，每个任务有多个动态变体测试相同的潜在规则。

Result: 多数大语言模型在低层次认知任务中表现稳健，但在高层次认知任务中表现不佳，泛化能力有限。

Conclusion: 当前大语言模型与人类流体智能仍有差距，需进一步优化推理能力；DRE-Bench为系统追踪推理进展提供了新工具。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [257] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida,Zhu Han*

Key words: 公共安全无人机（UAV）、大型语言模型（LLM）、上下文学习（ICL）、应急响应、路径规划

TL;DR: 论文提出将大型语言模型（LLM）与公共安全无人机（UAV）结合，通过上下文学习（ICL）优化应急响应中的路径规划和速度控制，减少延迟并提升效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有的深度强化学习（DRL）在无人机导航中存在训练复杂、样本效率低等问题，而LLM因其推理和泛化能力，为无人机在公共安全应急响应中提供了更轻量、高效的解决方案。

Method: 利用LLM的上下文学习（ICL）能力，通过自然语言提示和示例指导无人机执行任务，结合边缘计算减少延迟并保护隐私。

Result: 案例研究表明，基于LLM的ICL框架显著减少了数据包丢失，同时降低了潜在的安全漏洞风险。

Conclusion: LLM的ICL框架为公共安全无人机提供了自适应、情境感知的决策支持，是提升无人机在应急响应中自主性和响应能力的有效方案。

Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [258] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/abs/2506.02668)
*Frederico Metelo,Alexandre Oliveira,Stevo Racković,Pedro Ákos Costa,Cláudia Soares*

Key words: 边缘计算, 联邦强化学习, 去中心化, 任务卸载, 异步编排

TL;DR: FAuNO是一种用于边缘计算系统的异步联邦强化学习框架，通过去中心化任务卸载解决延迟和资源瓶颈问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统集中式编排在边缘计算中存在延迟和资源瓶颈问题，需要一种去中心化的解决方案以提高系统性能。

Method: FAuNO采用演员-评论家架构，本地演员学习节点动态和交互，联邦评论家聚合经验以促进协作。

Result: 实验表明，FAuNO在减少任务损失和延迟方面优于启发式和联邦多智能体强化学习基线。

Conclusion: FAuNO在动态边缘计算场景中表现出色，强调其适应性和高效性。

Abstract: Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [259] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)
*Jinyuan Luo,Zhen Fang,Yixuan Li,Seongheon Park,Ling Chen*

Key words: 大语言模型,幻觉检测,自我评估,扰动敏感性,SSP

TL;DR: 本文提出了Sample-Specific Prompting (SSP)框架，通过分析中间表征的扰动敏感性，改进了大语言模型的自我评估能力，从而更可靠地检测幻觉响应。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决大语言模型在真实问答任务中幻觉检测的不可靠性，特别是自我评估方法因模型偏见导致的输出置信度不准确的问题。

Method: 提出SSP框架，动态生成噪声提示并分析中间表征的扰动敏感性，使用轻量级编码器和对比距离度量量化差异以区分真实和幻觉响应。

Result: 实验表明SSP在多个幻觉检测基准上显著优于现有方法。

Conclusion: SSP通过利用中间表征的动态行为，显著提升了自我评估的可靠性。

Abstract: Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [260] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)
*Xiaochong Lan,Jie Feng,Yizhou Sun,Chen Gao,Jiahuan Lei,Xinlei Shi,Hengliang Luo,Yong Li*

Key words: 生活需求预测, 开放式分类, LLM, 个性化服务, 马斯洛需求

TL;DR: 论文提出PIGEON系统，通过开放式分类和LLM预测生活需求，显著优于传统闭集方法，提升19.37%。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统闭集分类方法无法捕捉生活需求的多样性和复杂性，需重新定义为开放式分类问题。

Method: PIGEON结合行为感知记录检索器和马斯洛需求层次理论，利用LLM进行灵活需求预测，并通过微调文本嵌入模型召回服务。

Result: 在真实数据集上，PIGEON比闭集方法平均提升19.37%，人类评估验证其合理性和特异性。

Conclusion: PIGEON系统在生活需求预测中表现优越，且通过指令调优使小型LLM具备竞争力，支持实际部署。

Abstract: Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [261] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
*Xiaochong Lan,Jie Feng,Jiahuan Lei,Xinlei Shi,Yong Li*

Key words: 大型语言模型、本地生活服务、微调、工作流优化、7B模型

TL;DR: 研究了大型语言模型（LLM）在本地生活服务领域的潜力，通过建立基准测试和探索微调与工作流优化，发现7B模型可实现与72B模型相当的效能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索LLM在本地生活服务中的应用潜力，解决模型部署成本与能力平衡的问题。

Method: 建立基准测试，评估LLM性能，采用模型微调和基于代理的工作流优化方法。

Result: 7B模型在性能上接近72B模型，显著降低了推理成本，提升了部署可行性。

Conclusion: 优化后的LLM在本地生活服务中具备高效和实用性，扩展了其实际应用场景。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [262] [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)
*Pengcheng Zhou,Yinglun Feng,Halimulati Julaiti,Zhongliang Yang*

Key words: 大型语言模型,智能体通信,多智能体协调,结构化通信,任务对齐

TL;DR: 探讨大型语言模型（LLMs）在多智能体协调中的局限性，提出是否需要设计新的模型范式以支持结构化通信和任务对齐。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基于自然语言的智能体通信方式与LLMs的高维向量空间不匹配，导致信息丢失和行为漂移，限制了多智能体协调能力。

Method: 分析当前LLMs的设计缺陷，提出是否应开发原生支持结构化通信和多智能体环境的模型范式。

Result: 揭示了现有LLMs在多智能体任务中的局限性，推动了对新型模型范式的探讨。

Conclusion: 需要重新考虑智能体的通信方式，并开发原生支持多智能体协调的模型架构。

Abstract: Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [263] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu,Wenjie Feng,Tianwei Zhang,Wei Zhou,Xueqi Cheng,See-Kiong Ng*

Key words: 图像生成模型,遗忘学习,任务分类,评估框架,数据集

TL;DR: 该论文针对图像生成模型遗忘学习（IGMU）的实践问题，提出了任务分类框架CatIGMU、评估框架EvalIGMU和高质量数据集DataIGM，以解决现有方法的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 图像生成模型的隐私和内容安全问题日益重要，而现有的遗忘学习方法存在任务定义不清、评估框架不足等问题。

Method: 论文提出CatIGMU任务分类框架、EvalIGMU评估框架，并构建DataIGM数据集，用于算法设计和评估。

Result: 发现现有IGMU算法在多个评估维度表现不佳，尤其在保留性和鲁棒性方面。

Conclusion: 通过新框架和数据集，提高了IGMU的标准化和评估可靠性。

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [264] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/abs/2506.02805)
*Henri Bollaert,Chris Cornelis,Marko Palangetić,Salvatore Greco,Roman Słowiński*

Key words: 规则归纳、模糊粗糙集、特征选择、机器学习、可解释性

TL;DR: 优化属性顺序对FRRI性能无显著提升，但模糊粗糙特征选择可提高平衡准确率和规则长度。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究模糊粗糙规则归纳算法FRRI中属性顺序优化对性能的影响，探索改进方法。

Method: 通过已知模糊粗糙集理论和经典机器学习方法优化属性顺序，并结合模糊粗糙特征选择进行实验。

Result: 优化属性顺序未改善FRRI性能，但模糊粗糙特征选择提升了平衡准确率和平均规则长度。

Conclusion: 模糊粗糙特征选择是提升FRRI性能的有效方法，而属性顺序优化的效果有限。

Abstract: Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [265] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)
*Jizhou Wang,Xiaodan Fang,Lei Huang,Yongfeng Huang*

Key words: 经济不平等,税收政策,大型语言模型,代理建模,公平与效率

TL;DR: TaxAgent结合大型语言模型和代理建模，设计自适应税收政策，优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统税收系统缺乏适应性，无法应对纳税人异质性和非理性行为。

Method: TaxAgent整合LLMs和ABM，模拟纳税人行为并优化税率。

Result: TaxAgent在公平与效率的权衡上优于Saez最优税收和美国联邦税。

Conclusion: TaxAgent提供了创新的税收解决方案和可扩展的政策评估框架。

Abstract: Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [266] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)
*Mathieu Andreux,Breno Baldas Skuk,Hamza Benchekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Matthias Brunel,Pierre-Louis Cedoz,Antoine Chassang,Mickaël Chen,Alexandra D. Constantinou,Antoine d'Andigné,Hubert de La Jonquière,Aurélien Delfosse,Ludovic Denoyer,Alexis Deprez,Augustin Derupti,Michael Eickenberg,Mathïs Federico,Charles Kantor,Xavier Koegler,Yann Labbé,Matthew C. H. Lee,Erwan Le Jumeau de Kergaradec,Amir Mahla,Avshalom Manevich,Adrien Maret,Charles Masson,Rafaël Maurin,Arturo Mena,Philippe Modard,Axel Moyal,Axel Nguyen Kerbel,Julien Revelle,Mats L. Richter,María Santos,Laurent Sifre,Maxime Theillard,Marc Thibault,Louis Thiry,Léo Tronchon,Nicolas Usunier,Tony Wu*

Key words: Surfer-H, 视觉语言模型, Holo1, 网络代理, WebClick, WebVoyager

TL;DR: Surfer-H是一种成本高效的网络代理，结合了视觉语言模型（VLM）完成任务，并搭配新的开源模型Holo1，在UI基准测试中表现卓越。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 旨在开发一种高效且成本低廉的网络代理，用于处理用户定义的网络任务，同时推动智能代理系统的研究。

Method: 通过集成视觉语言模型Holo1，并用精心筛选的数据（包括开源内容、合成数据等）进行训练，设计了新的基准测试WebClick。

Result: 在WebVoyager测试中达到92.2%的最高性能，平衡了准确性与成本效率。

Conclusion: Surfer-H与Holo1在性能和成本效率上表现出色，同时开源了模型和数据集以促进研究发展。

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [267] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
*Chen Qian,Dongrui Liu,Haochen Wen,Zhen Bai,Yong Liu,Jing Shao*

Key words: 大型推理模型,互信息峰值,思考标记,信息论,推理机制

TL;DR: 该论文通过信息论视角研究大型推理模型（LRMs）的推理机制，发现推理过程中的互信息（MI）峰值现象，并分析其对模型性能的影响，提出利用‘思考标记’提升推理能力的方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 理解大型推理模型（LRMs）的内部推理机制，并探索如何通过互信息（MI）分析揭示其推理过程中的关键节点。

Method: 通过跟踪中间表示与正确答案之间的互信息演变，识别MI峰值现象，并分析其与模型预测误差的关系。重点关注‘思考标记’对推理性能的影响。

Result: 发现MI峰值现象与模型预测误差降低相关，‘思考标记’（如‘Hmm’、‘Wait’等）对推理性能具有关键作用。

Conclusion: 研究为LRM的推理机制提供了新见解，并提出通过‘思考标记’提升推理性能的实用方法。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [268] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)
*Matthew Kowal,Jasper Timm,Jean-Francois Godbout,Thomas Costello,Antonio A. Arechar,Gordon Pennycook,David Rand,Adam Gleave,Kellin Pelrine*

Key words: 大型语言模型, 说服能力, 风险评测, 安全防护, 自动评估

TL;DR: 论文提出了‘尝试说服评估’（APE）基准，用于衡量大型语言模型（LLMs）在有害情境下试图说服的意愿，揭示了其安全防护的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究旨在评估LLMs在有害主题下尝试说服的倾向，以理解其安全风险，尤其是当模型被指示执行有害任务时。

Method: 采用APE基准，通过模拟多轮对话设置，测试LLMs在争议性和有害主题下的说服意愿，并引入自动评估模型量化说服尝试的频率和情境。

Result: 发现许多开放和封闭权重模型在有害主题下频繁尝试说服，且破解（jailbreaking）会加剧此行为，暴露了现有安全措施的不足。

Conclusion: 论文强调评估LLMs的说服意愿是风险管控的关键维度，APE为未来安全研究提供了重要基准。

Abstract: Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [269] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)
*Shangmin Guo,Omar Darwiche Domingues,Raphaël Avalos,Aaron Courville,Florian Strub*

Key words: LLM, tool use, state prediction, DyMo, SVS

TL;DR: 论文提出了一种名为DyMo的方法，通过状态预测和函数调用增强LLM，提升其在工具使用中的成功率和可靠性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决LLM在状态环境工具使用中的挑战，避免重复试验的不实用性。

Method: 采用动态建模（DyMo）和自我验证采样（SVS），结合状态预测和内部环境模型。

Result: 在Berkeley Function Calling Leaderboard V2上，DyMo提高了成功率并减少幻觉。与SVS结合后，进一步提升了效果和可靠性。

Conclusion: DyMo和SVS显著增强了LLM在工具使用中的有效性和可靠性，为无需重复查询环境的RL方法提供了路径。

Abstract: Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [270] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/abs/2506.02923)
*Alexis Bellot,Jonathan Richens,Tom Everitt*

Key words: AI代理, 行为解释, 信念推断, 世界模型, 公平性, 安全性

TL;DR: 该论文探讨了如何通过行为推断AI代理的信念目标，并预测其在未知环境中的行为，提出了理论界限及其对公平性和安全性的影响。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI系统及其与世界互动的复杂性增加，解释其行为对安全部署至关重要。研究旨在通过行为推断代理的信念和意图，为预测其在新环境中的行为提供理论基础。

Method: 假设代理行为由世界模型指导，推导出代理在新部署环境中行为的理论界限，代表了仅从行为数据预测意向代理的理论极限。

Result: 提供了关于代理在新环境中行为的精确理论界限，讨论了这些界限对预测意向代理行为的限制。

Conclusion: 研究结果为预测AI代理行为提供了理论依据，并对公平性和安全性等领域具有重要启示。

Abstract: As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [271] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/abs/2506.02949)
*Lixiang Xu,Xianwei Ding,Xin Yuan,Richang Hong,Feiping Nie,Enhong Chen,Philip S. Yu*

Key words: 

TL;DR: 论文提出了一种基于认知表示动态规划的知识追踪模型（CRDP-KT），通过优化认知表示来解决现有方法因非认知因素干扰导致的认知连续性和一致性不足的问题，提高了预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有知识追踪方法主要关注特征增强，但忽略了认知表示的不足和表达能力的限制，尤其是非认知因素（如猜测和失误）的干扰，导致无法有效捕捉学生认知过程的连续性和一致性，从而引入预测偏差和建模成本。

Method: 提出CRDP-KT模型，采用动态规划算法根据题目难度和表现间隔优化认知表示，确保其与学生认知模式一致；通过分区优化和加权融合二分图学习的关系增强表达能力。

Result: 在三个公开数据集上的实验验证了CRDP-KT模型的有效性，能够更准确地模拟认知状态并提供系统化的输入特征。

Conclusion: CRDP-KT模型通过优化认知表示和增强表达能力，有效解决了认知连续性和一致性问题，降低了预测偏差。

Abstract: Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [272] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
*Li Zhang,Kevin D. Ashley*

Key words: 大型语言模型, 法律论证, 多代理框架, 反思方法, 伦理AI

TL;DR: 本文提出了一种用于法律论证生成的多代理反思方法，有效减少了幻觉和错误论证，提升了事实利用率和论证的合规性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决大型语言模型在法律论证中存在的幻觉、错误说服和未能有效利用事实的问题。

Method: 采用反思性多代理框架，包含因子分析师和论证抛光师，迭代生成三部分法律论证。

Result: 在多代理框架下，模型在避免错误论证、减少幻觉和提升事实利用率方面表现显著优越。

Conclusion: 多代理反思框架为法律论证提供了可计算的伦理说服方法，增强了AI在法律领域的可信度。

Abstract: Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [273] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)
*Matthieu Tehenan,Christian Bolivar Moya,Tenghai Long,Guang Lin*

Key words: 大型语言模型,世界模型,空间表示,因果干预

TL;DR: 研究探讨了大型语言模型（LLMs）是否隐式编码线性空间世界模型，并提出了一种评估方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 大型语言模型在各种任务中表现出涌现能力，研究其是否具备内部世界模型具有重要意义。

Method: 通过构建空间世界模型的正式框架，使用合成数据集训练探针解码物体位置，并评估几何一致性。此外，进行了因果干预实验。

Result: 实验结果表明，大型语言模型确实编码了线性空间世界模型。

Conclusion: LLMs隐式地编码了线性空间世界模型，这为理解其内部机制提供了实证依据。

Abstract: Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [274] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)
*Junhao Yu,Yan Zhuang,YuXuan Sun,Weibo Gao,Qi Liu,Mingyue Cheng,Zhenya Huang,Enhong Chen*

Key words: 自适应测试,大型语言模型,交互式评估,心理测量,个性化服务

TL;DR: 论文提出了一种基于大型语言模型的TestAgent，用于增强自适应测试的交互性，解决了当前方法中的机械化问题和主观评估的噪声问题，实验证明其效果优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 准确评估人的内部状态对于理解偏好、提供个性化服务和识别现实应用中的挑战至关重要。当前自适应测试方法存在机械化、猜测行为和粗糙输出等问题，需要改进。

Method: 提出TestAgent，利用大型语言模型（LLM）进行交互式自适应测试，支持个性化问题选择、捕捉异常响应并提供精确结果。

Result: 实验表明，TestAgent在心理、教育和生活方式评估中，以20%更少的问题实现更准确的结果，并且在速度和流畅性等方面更受测试者青睐。

Conclusion: TestAgent是LLM在自适应测试中的首次应用，通过交互式设计显著提升了测试的准确性和用户体验。

Abstract: Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [275] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/abs/2506.03056)
*Ram Potham,Max Harms*

Key words: 基础模型, 可修正性, 安全对齐, 人类控制, 工具性驱动力

TL;DR: 论文探讨了基础模型的安全挑战，提出通过“可修正性作为单一目标”（CAST）使模型以人类控制为核心，避免失控导致的潜在灾难。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着基础模型能力的提升，其默认行为可能导致人类失控，带来存在性风险，现有对齐方法难以应对复杂价值规范和突发性权力寻求行为。

Method: 提出CAST框架，设计模型以赋能人类控制为核心目标，并通过训练方法（RLAIF、SFT等）、可扩展性测试和控制指令演示进行实证研究。

Result: CAST框架能够将模型的工具性驱动力转化为支持人类指导的行为，逐步增强对人类指引的响应能力。

Conclusion: 通过动态人类赋能，CAST提供了一条路径，使AI始终作为工具而非取代人类判断，从源头上解决对齐问题。

Abstract: Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [276] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo,David Cobbley,Xin Su,Shachar Rosenman,Vasudev Lal,Shao-Yen Tseng,Phillip Howard*

Key words: 计算机使用代理,视觉语言模型,隐私保护,本地运行,LLM-as-Judge

TL;DR: 开发了一种轻量级视觉语言模型，可在本地运行，解决了云推理带来的隐私和扩展性问题，并通过自动筛选高质量数据训练，性能优于现有基线。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为应对基于云的计算机使用代理存在的隐私和计算资源问题，提出了一种本地运行的轻量级解决方案。

Method: 引入LLM-as-Judge框架自动评估和筛选合成交互轨迹，生成高质量数据用于强化学习，无需人工标注。

Result: 在OS-World基准测试中，本地微调模型性能优于现有基线。

Conclusion: 该方法为实现隐私保护、高效且通用的GUI代理提供了一条有前景的路径。

Abstract: Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


### [277] [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
*Jiongnan Liu,Zhicheng Dou,Ning Hu,Chenyan Xiong*

Key words: 推荐系统, 多模态生成, 大型多模态模型, 强化学习

TL;DR: 提出一种新范式，超越传统推荐系统的内容过滤，直接生成个性化多模态内容（如图像）。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决推荐系统仅能过滤现有内容、无法生成新概念的局限性，以更好地满足用户需求。

Method: 利用大型多模态模型（LMMs），结合监督微调和在线强化学习，生成个性化内容。

Result: 实验和用户研究表明，生成的图像与用户历史偏好和潜在兴趣高度契合。

Conclusion: 该框架为推荐系统提供了生成新内容的能力，显著提升了用户体验。

Abstract: To address the challenge of information overload from massive web contents,
recommender systems are widely applied to retrieve and present personalized
results for users. However, recommendation tasks are inherently constrained to
filtering existing items and lack the ability to generate novel concepts,
limiting their capacity to fully satisfy user demands and preferences. In this
paper, we propose a new paradigm that goes beyond content filtering and
selecting: directly generating personalized items in a multimodal form, such as
images, tailored to individual users. To accomplish this, we leverage
any-to-any Large Multimodal Models (LMMs) and train them in both supervised
fine-tuning and online reinforcement learning strategy to equip them with the
ability to yield tailored next items for users. Experiments on two benchmark
datasets and user study confirm the efficacy of the proposed method. Notably,
the generated images not only align well with users' historical preferences but
also exhibit relevance to their potential future interests.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [278] [Stochastically Dominant Peer Prediction](https://arxiv.org/abs/2506.02259)
*Yichi Zhang,Shengwei Xu,David Pennock,Grant Schoenebeck*

Key words: 同行预测,随机占优真实性,激励机制,人类反馈,机器学习

TL;DR: 该论文提出了一种名为随机占优真实性（SD-truthfulness）的新机制，用于在更广泛的单调效用函数范围内激励真实反馈，改进了传统同行预测机制的局限性。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 在机器学习和AI对齐任务中，获取可靠的人类反馈至关重要。传统同行预测机制在激励真实反馈时依赖于线性效用函数的假设，而实际中非线性的支付规则或效用函数更为常见。

Method: 通过将得分转换为二元彩票的简单方法实现随机占优真实性，但发现这会降低敏感性。论文进一步提出更精细的舍入方法，并引入一种新的强制一致性（EA）机制。

Result: EA机制在二元信号设置下在理论上保证SD真实性，实验表明其在所有已知SD真实性机制中具有最高的敏感性。

Conclusion: 随机占优真实性机制为激励真实反馈提供了更强大的保证，尤其是在非线性效用函数的情况下，EA机制表现出色。

Abstract: Eliciting reliable human feedback is essential for many machine learning
tasks, such as learning from noisy labels and aligning AI systems with human
preferences. Peer prediction mechanisms incentivize truthful reporting without
ground truth verification by scoring agents based on correlations with peers.
Traditional mechanisms, which ensure that truth-telling maximizes the expected
scores in equilibrium, can elicit honest information while assuming agents'
utilities are linear functions of their scores. However, in practice,
non-linear payment rules are usually preferred, or agents' utilities are
inherently non-linear.
  We propose stochastically dominant truthfulness (SD-truthfulness) as a
stronger guarantee: the score distribution of truth-telling stochastically
dominates all other strategies, incentivizing truthful reporting for a wide
range of monotone utility functions. Our first observation is that no existing
peer prediction mechanism naturally satisfies this criterion without strong
assumptions. A simple solution -- rounding scores into binary lotteries -- can
enforce SD-truthfulness, but often degrades sensitivity, a key property related
to fairness and statistical efficiency. We demonstrate how a more careful
application of rounding can better preserve sensitivity. Furthermore, we
introduce a new enforced agreement (EA) mechanism that is theoretically
guaranteed to be SD-truthful in binary-signal settings under mild assumptions,
and empirically achieves the highest sensitivity among all known SD-truthful
mechanisms.

</details>


### [279] [Learning Optimal Posted Prices for a Unit-Demand Buyer](https://arxiv.org/abs/2506.02284)
*Yifeng Teng,Yifan Wang*

Key words: 单位需求定价, 查询模型, 样本复杂度, 定价查询复杂度

TL;DR: 研究了在独立项目价值下学习最优定价的问题，对比了两种查询模型的样本复杂度和定价查询复杂度。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 研究如何在单位需求买家的情况下，通过不同的查询模型（样本访问和定价查询）优化项目的定价策略。

Method: 比较了两种查询模型：样本访问模型（获取样本值）和定价查询模型（设置价格并获取二元信号），分析了它们的复杂度。

Result: 得出了单位需求定价问题的样本复杂度和定价查询复杂度的近似紧密界限。

Conclusion: 研究为优化定价策略提供了理论基础，特别是在不同查询模型下的复杂度分析。

Abstract: We study the problem of learning the optimal item pricing for a unit-demand
buyer with independent item values, and the learner has query access to the
buyer's value distributions. We consider two common query models in the
literature: the sample access model where the learner can obtain a sample of
each item value, and the pricing query model where the learner can set a price
for an item and obtain a binary signal on whether the sampled value of the item
is greater than our proposed price. In this work, we give nearly tight sample
complexity and pricing query complexity of the unit-demand pricing problem.

</details>


### [280] [Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff](https://arxiv.org/abs/2506.03102)
*Sophie Greenwood,Karen Levy,Solon Barocas,Hoda Heidari,Jon Kleinberg*

Key words: AI委托, 分类决策, 算法设计, 组合优化

TL;DR: 论文研究了在人类基于分类进行任务委托时，如何设计最优的AI委托算法，并展示了其相对于独立AI代理的优势。

<details>
  <summary>Details</summary>

Main category: cs.GT

Motivation: 随着AI技术的进步，人类更倾向于将任务委托给AI代理，但缺乏对决策实例的全面认知，因此需要研究如何设计最优的委托算法。

Method: 定义了分类存在时的最优委托问题，分析了问题的组合性质，并提出了在几种广泛情况下的高效算法。

Result: 发现最优委托问题通常计算困难，但在特定情况下可以高效求解，并通过实验验证了动态更新的委托算法的实用性。

Conclusion: 虽然动态优化无法完全达到最优委托，但在实践中表现良好，突出了委托算法设计的重要性。

Abstract: As AI technologies improve, people are increasingly willing to delegate tasks
to AI agents. In many cases, the human decision-maker chooses whether to
delegate to an AI agent based on properties of the specific instance of the
decision-making problem they are facing. Since humans typically lack full
awareness of all the factors relevant to this choice for a given
decision-making instance, they perform a kind of categorization by treating
indistinguishable instances -- those that have the same observable features --
as the same. In this paper, we define the problem of designing the optimal
algorithmic delegate in the presence of categories. This is an important
dimension in the design of algorithms to work with humans, since we show that
the optimal delegate can be an arbitrarily better teammate than the optimal
standalone algorithmic agent. The solution to this optimal delegation problem
is not obvious: we discover that this problem is fundamentally combinatorial,
and illustrate the complex relationship between the optimal design and the
properties of the decision-making task even in simple settings. Indeed, we show
that finding the optimal delegate is computationally hard in general. However,
we are able to find efficient algorithms for producing the optimal delegate in
several broad cases of the problem, including when the optimal action may be
decomposed into functions of features observed by the human and the algorithm.
Finally, we run computational experiments to simulate a designer updating an
algorithmic delegate over time to be optimized for when it is actually adopted
by users, and show that while this process does not recover the optimal
delegate in general, the resulting delegate often performs quite well.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [281] [Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods](https://arxiv.org/abs/2506.02841)
*Tom Danino,Nahum Shimkin*

Key words: 多智能体强化学习,选择性探索,集成学习,TD(λ),SMAC II

TL;DR: 论文提出了一种结合分解中心化评论器和去中心化集成学习的新算法，通过选择性探索和多样性正则化提升多智能体强化学习的效率和性能。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 多智能体强化学习（MARL）通常需要比单智能体更多的环境交互，且存在联合动作空间探索困难和方差高的问题。

Method: 采用分解中心化评论器与集成学习结合，利用集成的峰度指导探索高不确定性状态和动作，并通过截断的TD(λ)算法提升样本效率。

Result: 在标准MARL基准测试中（包括SMAC II地图），该方法优于现有最先进算法。

Conclusion: 该方法通过平衡稳定性和效率，显著提升了MARL的性能和样本效率。

Abstract: Multi-agent reinforcement learning (MARL) methods have achieved
state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms
typically require significantly more environment interactions than their
single-agent counterparts to converge, a problem exacerbated by the difficulty
in exploring over a large joint action space and the high variance intrinsic to
MARL environments. To tackle these issues, we propose a novel algorithm that
combines a decomposed centralized critic with decentralized ensemble learning,
incorporating several key contributions. The main component in our scheme is a
selective exploration method that leverages ensemble kurtosis. We extend the
global decomposed critic with a diversity-regularized ensemble of individual
critics and utilize its excess kurtosis to guide exploration toward
high-uncertainty states and actions. To improve sample efficiency, we train the
centralized critic with a novel truncated variation of the TD($\lambda$)
algorithm, enabling efficient off-policy learning with reduced variance. On the
actor side, our suggested algorithm adapts the mixed samples approach to MARL,
mixing on-policy and off-policy loss functions for training the actors. This
approach balances between stability and efficiency and outperforms purely
off-policy learning. The evaluation shows our method outperforms
state-of-the-art baselines on standard MARL benchmarks, including a variety of
SMAC II maps.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [282] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/abs/2506.01980)
*Lianhao Yin,Ozanan Meireles,Guy Rosman,Daniela Rus*

Key words: 自监督学习,微创手术,视频理解,Kolmogorov复杂性,紧凑表示

TL;DR: 论文提出了一种名为C2E的自监督学习框架，通过Kolmogorov复杂性学习手术视频的紧凑、信息丰富表示，无需标注数据即可提高编码器性能。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于标注成本高，监督学习在微创手术视频理解中的应用受限，需要一种能够捕捉结构和物理信息的自监督方法。

Method: C2E框架利用熵最大化解码器压缩图像，同时保留临床相关细节，学习紧凑表示。

Result: 在大规模未标注手术数据集上训练后，C2E在多种手术ML任务中表现出强泛化能力，性能优于传统方法。

Conclusion: C2E展示了自监督学习在提升手术AI性能和改善微创手术结果方面的潜力。

Abstract: Real-time video understanding is critical to guide procedures in minimally
invasive surgery (MIS). However, supervised learning approaches require large,
annotated datasets that are scarce due to annotation efforts that are
prohibitive, e.g., in medical fields. Although self-supervision methods can
address such limitations, current self-supervised methods often fail to capture
structural and physical information in a form that generalizes across tasks. We
propose Compress-to-Explore (C2E), a novel self-supervised framework that
leverages Kolmogorov complexity to learn compact, informative representations
from surgical videos. C2E uses entropy-maximizing decoders to compress images
while preserving clinically relevant details, improving encoder performance
without labeled data. Trained on large-scale unlabeled surgical datasets, C2E
demonstrates strong generalization across a variety of surgical ML tasks, such
as workflow classification, tool-tissue interaction classification,
segmentation, and diagnosis tasks, providing improved performance as a surgical
visual foundation model. As we further show in the paper, the model's internal
compact representation better disentangles features from different structural
parts of images. The resulting performance improvements highlight the yet
untapped potential of self-supervised learning to enhance surgical AI and
improve outcomes in MIS.

</details>


### [283] [Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine](https://arxiv.org/abs/2506.02149)
*Wenjun Xia,Chuang Niu,Ge Wang*

Key words: CT重建、深度学习、生成模型、PFGM++、FORCE

TL;DR: 提出了一个名为FORCE的新CT重建框架，将数据保真度与最新生成模型PFGM++结合，在多种CT成像任务中表现优异。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 临床CT场景中低剂量、稀疏扫描和金属植入等问题导致重建图像噪声和伪影严重，现有深度学习方法受限于配对数据获取困难，存在幻觉风险。

Method: 整合数据保真度与生成模型PFGM++，提出Flow-Oriented Reconstruction Conditioning Engine（FORCE）框架。

Result: 在多种CT成像任务中表现优于现有无监督重建方法。

Conclusion: FORCE框架通过结合数据保真度与生成模型，显著提升了CT重建的稳定性和准确性。

Abstract: Computed tomography (CT) is a major medical imaging modality. Clinical CT
scenarios, such as low-dose screening, sparse-view scanning, and metal
implants, often lead to severe noise and artifacts in reconstructed images,
requiring improved reconstruction techniques. The introduction of deep learning
has significantly advanced CT image reconstruction. However, obtaining paired
training data remains rather challenging due to patient motion and other
constraints. Although deep learning methods can still perform well with
approximately paired data, they inherently carry the risk of hallucination due
to data inconsistencies and model instability. In this paper, we integrate the
data fidelity with the state-of-the-art generative AI model, referred to as the
Poisson flow generative model (PFGM) with a generalized version PFGM++, and
propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine
(FORCE). In our experiments, the proposed method shows superior performance in
various CT imaging tasks, outperforming existing unsupervised reconstruction
approaches.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [284] [On the Benefits of Accelerated Optimization in Robust and Private Estimation](https://arxiv.org/abs/2506.03044)
*Laurentiu Andrei Marchis,Po-Ling Loh*

Key words: 加速梯度方法, Frank-Wolfe, 投影梯度下降, 差分隐私, 重尾鲁棒性

TL;DR: 该论文研究了加速梯度方法（如Frank-Wolfe和投影梯度下降）在隐私和重尾鲁棒性方面的优势，通过定制的学习率和梯度下界降低迭代复杂度，并验证了在非随机数据、随机模型和无参数模型中的统计保证。

<details>
  <summary>Details</summary>

Main category: math.ST

Motivation: 旨在通过加速梯度方法提升隐私保护和重尾数据鲁棒性，为实证和总体风险最小化提供更强的统计保证。

Method: Frank-Wolfe方法采用定制学习率和梯度下界，投影梯度下降使用Nesterov动量变体，并通过高斯机制确保差分隐私，几何中位数均值估计增强鲁棒性。

Result: 加速方法降低了迭代复杂度，提供了更强的统计保证，并在多个数据设置中验证了效果，部分情况下达到最优收敛率。

Conclusion: 加速梯度方法在隐私和重尾鲁棒性方面表现出色，适用于多种数据场景，部分情况下优于现有方法。

Abstract: We study the advantages of accelerated gradient methods, specifically based
on the Frank-Wolfe method and projected gradient descent, for privacy and
heavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe
method, our technique is based on a tailored learning rate and a uniform lower
bound on the gradient of the $\ell_2$-norm over the constraint set. For
accelerating projected gradient descent, we use the popular variant based on
Nesterov's momentum, and we optimize our objective over $\mathbb{R}^p$. These
accelerations reduce iteration complexity, translating into stronger
statistical guarantees for empirical and population risk minimization. Our
analysis covers three settings: non-random data, random model-free data, and
parametric models (linear regression and generalized linear models).
Methodologically, we approach both privacy and robustness based on noisy
gradients. We ensure differential privacy via the Gaussian mechanism and
advanced composition, and we achieve heavy-tailed robustness using a geometric
median-of-means estimator, which also sharpens the dependency on the dimension
of the covariates. Finally, we compare our rates to existing bounds and
identify scenarios where our methods attain optimal convergence.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [285] [Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment](https://arxiv.org/abs/2506.02046)
*Mohammad Saleh Torkestani,Taha Mansouri*

Key words: 生成式AI、高等教育评估、静态分析、动态测试

TL;DR: 提出了一种理论框架，通过静态分析和动态测试相结合的方法，解决生成式AI在高等教育评估中的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 生成式AI（如GPT-4）能够生成复杂的学术内容，传统评估方法面临威胁，当前检测工具和手动方法存在局限性。

Method: 采用静态分析和动态测试的双重策略，静态分析包含八个理论元素，动态测试通过仿真评估漏洞。

Result: 提出了一个评估漏洞的理论框架，包括评分基础和阈值确定理论。

Conclusion: 该框架为区分真实人类学习与AI生成内容提供了理论基础。

Abstract: This paper presents a theoretical framework for addressing the challenges
posed by generative artificial intelligence (AI) in higher education assessment
through a machine-versus-machine approach. Large language models like GPT-4,
Claude, and Llama increasingly demonstrate the ability to produce sophisticated
academic content, traditional assessment methods face an existential threat,
with surveys indicating 74-92% of students experimenting with these tools for
academic purposes. Current responses, ranging from detection software to manual
assessment redesign, show significant limitations: detection tools demonstrate
bias against non-native English writers and can be easily circumvented, while
manual frameworks rely heavily on subjective judgment and assume static AI
capabilities. This paper introduces a dual strategy paradigm combining static
analysis and dynamic testing to create a comprehensive theoretical framework
for assessment vulnerability evaluation. The static analysis component
comprises eight theoretically justified elements: specificity and
contextualization, temporal relevance, process visibility requirements,
personalization elements, resource accessibility, multimodal integration,
ethical reasoning requirements, and collaborative elements. Each element
addresses specific limitations in generative AI capabilities, creating barriers
that distinguish authentic human learning from AI-generated simulation. The
dynamic testing component provides a complementary approach through
simulation-based vulnerability assessment, addressing limitations in
pattern-based analysis. The paper presents a theoretical framework for
vulnerability scoring, including the conceptual basis for quantitative
assessment, weighting frameworks, and threshold determination theory.

</details>


### [286] [Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI](https://arxiv.org/abs/2506.02055)
*Nikola Balic*

Key words: AI代理, 多代理系统, 调查, 部署障碍, 治理

TL;DR: 研究调查了130名专业人士对AI代理能力、影响和治理的看法，发现部署决策复杂且无显著预测因素，需解决合规问题并建立治理框架。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 了解专业人士对多代理AI系统的看法，以预判采用挑战、伦理问题和未来劳动力发展。

Method: 通过调查分析130名受访者对AI代理的预期时间表、部署障碍和责任归属的看法，并尝试逻辑回归模型。

Result: 发现三类受访者集群，逻辑回归模型无显著预测因素，部署决策复杂，可能需更大样本或更多因素。

Conclusion: 组织在整合AI代理时应解决合规问题并建立清晰治理框架。

Abstract: Autonomous multi-agent AI systems are poised to transform various industries,
particularly software development and knowledge work. Understanding current
perceptions among professionals is crucial for anticipating adoption
challenges, ethical considerations, and future workforce development. This
study analyzes responses from 130 participants to a survey on the capabilities,
impact, and governance of AI agents. We explore expected timelines for AI
replacing programmers, identify perceived barriers to deployment, and examine
beliefs about responsibility when agents make critical decisions. Key findings
reveal three distinct clusters of respondents. While the study explored factors
associated with current AI agent deployment, the initial logistic regression
model did not yield statistically significant predictors, suggesting that
deployment decisions are complex and may be influenced by factors not fully
captured or that a larger sample is needed. These insights highlight the need
for organizations to address compliance concerns (a commonly cited barrier) and
establish clear governance frameworks as they integrate autonomous agents into
their workflows.

</details>


### [287] [AI Data Development: A Scorecard for the System Card Framework](https://arxiv.org/abs/2506.02071)
*Tadesse K. Bahiru,Haileleol Tibebu,Ioannis A. Kakadiaris*

Key words: 人工智能, 数据集质量, 透明度, 责任伦理, 评分卡

TL;DR: 该论文介绍了一种基于系统卡框架数据开发生命周期的评分卡，用于评估AI数据集的开发质量，涵盖数据字典、收集过程、组成、动机和预处理五个关键领域。通过评分系统提供改进建议，旨在提升数据集透明度与完整性，促进负责任AI系统的开发。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: AI系统的可靠性高度依赖于数据集质量，但目前存在透明度、责任和潜在偏见等问题，因此需要一种系统化方法评估和改进数据集开发过程。

Method: 采用结构化方法，使用输入表单和评分标准评估数据集质量与完整性，应用于四个不同数据集，通过评分系统提供定制化建议。

Result: 评分卡揭示了数据集的优势和待改进之处，通过评分系统为提升数据透明度和完整性提供具体指导，同时兼顾技术与伦理考量。

Conclusion: 该方法为数据管理者和研究者提供了实用工具，支持开发公平、可靠的AI决策支持系统，提升了数据实践的全面性。

Abstract: Artificial intelligence has transformed numerous industries, from healthcare
to finance, enhancing decision-making through automated systems. However, the
reliability of these systems is mainly dependent on the quality of the
underlying datasets, raising ongoing concerns about transparency,
accountability, and potential biases. This paper introduces a scorecard
designed to evaluate the development of AI datasets, focusing on five key areas
from the system card framework data development life cycle: data dictionary,
collection process, composition, motivation, and pre-processing. The method
follows a structured approach, using an intake form and scoring criteria to
assess the quality and completeness of the data set. Applied to four diverse
datasets, the methodology reveals strengths and improvement areas. The results
are compiled using a scoring system that provides tailored recommendations to
enhance the transparency and integrity of the data set. The scorecard addresses
technical and ethical aspects, offering a holistic evaluation of data
practices. This approach aims to improve the quality of the data set. It offers
practical guidance to curators and researchers in developing responsible AI
systems, ensuring fairness and accountability in decision support systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [288] [Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders](https://arxiv.org/abs/2506.02051)
*Hui Liu,Shiye Tian,Xuejun Liu*

Key words: SmilesGEN, 生成模型, 变分自编码器, 药物发现, 表达谱

TL;DR: SmilesGEN是一种基于VAE架构的生成模型，通过结合药物扰动和转录反应来生成具有潜在治疗效果的分子。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 传统方法主要依赖表达谱指导分子生成，但忽视了分子对细胞环境的扰动效应，SmilesGEN旨在解决这一问题。

Method: SmilesGEN整合了预训练的SmilesNet和ProfileNet，共同建模药物扰动与转录反应的交互作用，通过共享潜在空间生成分子。

Result: 实验表明，SmilesGEN在分子效价、独特性、新颖性及与已知配体的相似性等方面优于现有模型，并成功用于药物优化和生成。

Conclusion: SmilesGEN提供了一个强大的框架，利用基因特征生成有望诱导理想细胞表型变化的药物分子。

Abstract: The de novo generation of drug-like molecules capable of inducing desirable
phenotypic changes is receiving increasing attention. However, previous methods
predominantly rely on expression profiles to guide molecule generation, but
overlook the perturbative effect of the molecules on cellular contexts. To
overcome this limitation, we propose SmilesGEN, a novel generative model based
on variational autoencoder (VAE) architecture to generate molecules with
potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE
(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the
interplay between drug perturbations and transcriptional responses in a common
latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment
expression profiles when eliminating drug-induced perturbations in the latent
space, while SmilesNet is informed by desired expression profiles to generate
drug-like molecules. Our empirical experiments demonstrate that SmilesGEN
outperforms current state-of-the-art models in generating molecules with higher
degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity
to known ligands targeting the relevant proteins. Moreover, we evaluate
SmilesGEN for scaffold-based molecule optimization and generation of
therapeutic agents, and confirmed its superior performance in generating
molecules with higher similarity to approved drugs. SmilesGEN establishes a
robust framework that leverages gene signatures to generate drug-like molecules
that hold promising potential to induce desirable cellular phenotypic changes.

</details>


### [289] [Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications](https://arxiv.org/abs/2506.02052)
*Shuo Yan,Yuliang Yan,Bin Ma,Chenao Li,Haochun Tang,Jiahua Lu,Minhua Lin,Yuyuan Feng,Hui Xiong,Enyan Dai*

Key words: 蛋白质应用, 深度学习, 预训练, 领域特定模型, Protap基准测试

TL;DR: 本文介绍了Protap基准测试，用于系统比较不同蛋白质应用的深度学习架构、预训练策略和领域特定模型，并揭示了预训练和监督训练的性能差异。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 当前缺乏对蛋白质应用中不同深度学习方法和领域特定模型的系统比较，尤其是在工业相关的新型任务上。

Method: 采用Protap基准测试，覆盖五种蛋白质应用（包括两个新型任务），比较不同架构、预训练策略和领域特定模型的表现。

Result: 发现大规模预训练编码器在小型下游任务中表现不如监督训练编码器；结构信息在下游微调中表现优异；领域特定先验能提升专业任务性能。

Conclusion: Protap为蛋白质应用提供了全面的基准测试，揭示了不同方法的优劣势，并开源了代码和数据集。

Abstract: Recently, extensive deep learning architectures and pretraining strategies
have been explored to support downstream protein applications. Additionally,
domain-specific models incorporating biological knowledge have been developed
to enhance performance in specialized tasks. In this work, we introduce
$\textbf{Protap}$, a comprehensive benchmark that systematically compares
backbone architectures, pretraining strategies, and domain-specific models
across diverse and realistic downstream protein applications. Specifically,
Protap covers five applications: three general tasks and two novel specialized
tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted
protein degradation, which are industrially relevant yet missing from existing
benchmarks. For each application, Protap compares various domain-specific
models and general architectures under multiple pretraining settings. Our
empirical studies imply that: (i) Though large-scale pretraining encoders
achieve great results, they often underperform supervised encoders trained on
small downstream training sets. (ii) Incorporating structural information
during downstream fine-tuning can match or even outperform protein language
models pretrained on large-scale sequence corpora. (iii) Domain-specific
biological priors can enhance performance on specialized downstream tasks. Code
and datasets are publicly available at
https://github.com/Trust-App-AI-Lab/protap.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [290] [A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis](https://arxiv.org/abs/2506.02076)
*Christian Marius Lillelund,Sanjay Kalra,Russell Greiner*

Key words: ALS, 功能衰退预测, 多事件生存模型, 反事实预测, 个性化治疗

TL;DR: 本文提出了一种预测ALS患者功能衰退时间的新方法，通过多事件生存模型预测五种常见功能的衰退时间，并在临床数据中验证其优于传统方法。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: ALS是一种导致渐进性瘫痪的神经退行性疾病，现有治疗方法难以确定最佳干预时机，需要更个性化的预测方法。

Method: 采用多事件生存模型，训练五个基于协变量的生存模型，预测患者在未来500天内五种功能的衰退概率。

Result: 基于协变量的模型优于Kaplan-Meier估计器，并能进行反事实预测，发现Riluzole对功能衰退影响较小，延髓起病ALS患者的说话和吞咽功能衰退更快。

Conclusion: 该方法可用于临床数据评估功能衰退风险，支持个性化治疗计划。

Abstract: Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor
neurons that causes progressive paralysis in patients. Current treatment
options aim to prolong survival and improve quality of life; however, due to
the heterogeneity of the disease, it is often difficult to determine the
optimal time for potential therapies or medical interventions. In this study,
we propose a novel method to predict the time until a patient with ALS
experiences significant functional impairment (ALSFRS-R<=2) with respect to
five common functions: speaking, swallowing, handwriting, walking and
breathing. We formulate this task as a multi-event survival problem and
validate our approach in the PRO-ACT dataset by training five covariate-based
survival models to estimate the probability of an event over a 500-day period
after a baseline visit. We then predict five event-specific individual survival
distributions (ISDs) for each patient, each providing an interpretable and
meaningful estimate of when that event will likely take place in the future.
The results show that covariate-based models are superior to the Kaplan-Meier
estimator at predicting time-to-event outcomes. Additionally, our method
enables practitioners to make individual counterfactual predictions, where
certain features (covariates) can be changed to see their effect on the
predicted outcome. In this regard, we find that Riluzole has little to no
impact on predicted functional decline. However, for patients with bulbar-onset
ALS, our method predicts considerably shorter counterfactual time-to-event
estimates for tasks related to speech and swallowing compared to limb-onset
ALS. The proposed method can be applied to current clinical examination data to
assess the risk of functional decline and thus allow more personalized
treatment planning.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [291] [StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion](https://arxiv.org/abs/2506.02414)
*Fengjin Li,Jie Wang,Yadong Niu,Yongqing Wang,Meng Meng,Jian Luan,Zhiyong Wu*

Key words: 语音转换, 文本建模, 自回归框架, 说话人特征, 语言内容

TL;DR: StarVC是一种结合文本建模的自回归语音转换框架，通过先预测文本标记再合成声学特征，显著优于传统方法，提升了语言内容和说话人特征的保留效果。

<details>
  <summary>Details</summary>

Main category: cs.MM

Motivation: 传统的语音转换方法直接提取说话人信息而忽视语言内容的显式利用，导致性能受限。StarVC通过结合文本建模，优化了解耦说话人身份和语言内容的过程。

Method: StarVC采用统一的自回归框架，先预测文本标记，再合成声学特征，实现了对语言内容和说话人特征的显式建模。

Result: 实验表明，StarVC在语言内容（WER、CER）和说话人特征（SECS、MOS）的保留上优于传统方法。

Conclusion: 通过显式文本建模，StarVC成功提升了语音转换的性能，为相关领域提供了新的研究方向。

Abstract: Voice Conversion (VC) modifies speech to match a target speaker while
preserving linguistic content. Traditional methods usually extract speaker
information directly from speech while neglecting the explicit utilization of
linguistic content. Since VC fundamentally involves disentangling speaker
identity from linguistic content, leveraging structured semantic features could
enhance conversion performance. However, previous attempts to incorporate
semantic features into VC have shown limited effectiveness, motivating the
integration of explicit text modeling. We propose StarVC, a unified
autoregressive VC framework that first predicts text tokens before synthesizing
acoustic features. The experiments demonstrate that StarVC outperforms
conventional VC methods in preserving both linguistic content (i.e., WER and
CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found
at: https://thuhcsi.github.io/StarVC/.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [292] [Random-key genetic algorithms](https://arxiv.org/abs/2506.02120)
*Mariana A. Londe,Luciana S. Pessoa,Carlos E. Andrade,José F. Gonçalves,Mauricio G. C. Resende*

Key words: 随机键遗传算法、离散优化、全局优化、超立方体、精英保留

TL;DR: 本文介绍了一种随机键遗传算法及其变种，用于离散和全局优化问题。通过编码解为随机键向量，并在超立方体内进行遗传操作，提高了算法的效率和可维护性。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 旨在解决离散和全局优化问题，通过随机键编码和遗传算法结合，提升优化效率和框架的通用性。

Method: 使用随机键向量编码解，通过解码器映射到问题解并计算成本，结合精英保留、变异和交叉操作进行迭代优化。

Result: 该方法在超立方体内维护所有遗传操作，增强了框架的生产力和可维护性，适用于多种优化问题。

Conclusion: 随机键遗传算法及其变种在离散和全局优化中表现出高效和灵活性。

Abstract: A random-key genetic algorithm is an evolutionary metaheuristic for discrete
and global optimization. Each solution is encoded as a vector of N random keys,
where a random key is a real number randomly generated in the continuous
interval [0, 1). A decoder maps each vector of random keys to a solution of the
optimization problem being solved and computes its cost. The benefit of this
approach is that all genetic operators and transformations can be maintained
within the unitary hypercube, regardless of the problem being addressed. This
enhances the productivity and maintainability of the core framework. The
algorithm starts with a population of P vectors of random keys. At each
iteration, the vectors are partitioned into two sets: a smaller set of
high-valued elite solutions and the remaining non-elite solutions. All elite
elements are copied, without change, to the next population. A small number of
random-key vectors (the mutants) is added to the population of the next
iteration. The remaining elements of the population of the next iteration are
generated by combining, with the parametrized uniform crossover of Spears and
DeJong (1991), pairs of solutions. This chapter reviews random-key genetic
algorithms and describes an effective variant called biased random-key genetic
algorithms.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [293] [Deep Learning Enhanced Multivariate GARCH](https://arxiv.org/abs/2506.02796)
*Haoyuan Wang,Chen Liu,Minh-Ngoc Tran,Chao Wang*

Key words: LSTM-BEKK, 多变量波动率建模, 金融风险管理, 深度学习, GARCH

TL;DR: 本文提出了一种结合深度学习的多变量波动率建模框架LSTM-BEKK，通过将LSTM的灵活性与BEKK模型的计量结构结合，有效捕捉金融数据中的非线性动态高维依赖结构。

<details>
  <summary>Details</summary>

Main category: q-fin.CP

Motivation: 传统多变量GARCH方法在捕捉持续性波动集群和资产间非对称联动方面存在局限，本文旨在改进这一问题。

Method: 结合LSTM和BEKK模型，构建LSTM-BEKK框架，利用数据驱动的LSTM适应时变市场条件。

Result: LSTM-BEKK在多个股票市场中表现出更优的样本外组合风险预测能力，同时保持BEKK模型的可解释性。

Conclusion: 混合计量-深度学习模型在金融风险管理和多变量波动率预测中具有潜力。

Abstract: This paper introduces a novel multivariate volatility modeling framework,
named Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep
learning into multivariate GARCH processes. By combining the flexibility of
recurrent neural networks with the econometric structure of BEKK models, our
approach is designed to better capture nonlinear, dynamic, and high-dimensional
dependence structures in financial return data. The proposed model addresses
key limitations of traditional multivariate GARCH-based methods, particularly
in capturing persistent volatility clustering and asymmetric co-movement across
assets. Leveraging the data-driven nature of LSTMs, the framework adapts
effectively to time-varying market conditions, offering improved robustness and
forecasting performance. Empirical results across multiple equity markets
confirm that the LSTM-BEKK model achieves superior performance in terms of
out-of-sample portfolio risk forecast, while maintaining the interpretability
from the BEKK models. These findings highlight the potential of hybrid
econometric-deep learning models in advancing financial risk management and
multivariate volatility forecasting.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [294] [Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models](https://arxiv.org/abs/2506.02075)
*Christian Marius Lillelund,Shi-ang Qi,Russell Greiner,Christian Fischer Pedersen*

Key words: 生存分析, 时间到事件模型, C指数, 评估方法, 校准性

TL;DR: 该论文指出当前生存分析模型的评估方法存在误区，尤其是过度依赖C指数，未能全面评估模型的其他关键方面，如时间预测准确性和概率校准，并提出了新的评估标准和实践方法。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 现有生存分析和时间到事件模型的评估方法过于依赖C指数，忽视了其他关键指标如预测准确性和校准性，导致模型评估不全面。

Method: 通过文献调查总结现有评估方法的不足，提出一组针对生存分析特点的评估标准，并讨论其优缺点。

Result: 揭示了当前生存分析评估方法的局限性，提出新标准并假设评估方法和模型假设需同步发展。

Conclusion: 论文主张更全面的评估方法，强调模型和评估指标需在相同假设基础上发展。

Abstract: We argue that many survival analysis and time-to-event models are incorrectly
evaluated. First, we survey many examples of evaluation approaches in the
literature and find that most rely on concordance (C-index). However, the
C-index only measures a model's discriminative ability and does not assess
other important aspects, such as the accuracy of the time-to-event predictions
or the calibration of the model's probabilistic estimates. Next, we present a
set of key desiderata for choosing the right evaluation metric and discuss
their pros and cons. These are tailored to the challenges in survival analysis,
such as sensitivity to miscalibration and various censoring assumptions. We
hypothesize that the current development of survival metrics conforms to a
double-helix ladder, and that model validity and metric validity must stand on
the same rung of the assumption ladder. Finally, we discuss the appropriate
methods for evaluating a survival model in practice and summarize various
viewpoints opposing our analysis.

</details>


### [295] [Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments](https://arxiv.org/abs/2506.02394)
*Yuan Bian,Xingche Guo,Yuanjia Wang*

Key words: 抑郁症；强化学习；漂移扩散模型；隐马尔可夫模型；EMBARC研究

TL;DR: 提出了一种结合强化学习和漂移扩散模型的新框架，用于分析奖励决策和时间响应，并通过隐马尔可夫模型捕捉策略切换。该方法在EMBARC研究中显示，抑郁症患者的决策参与度较低且决策时间更长。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 抑郁症与奖励处理异常和注意力问题相关，希望通过整合强化学习和漂移扩散模型来更全面地分析决策行为。

Method: 结合强化学习（RL）和漂移扩散模型（DDM），使用隐马尔可夫模型（HMM）捕捉策略切换，并通过高效的计算算法实现。

Result: 方法在数值研究中表现优异；应用于EMBARC研究时，发现抑郁症患者参与度更低且决策时间更长；神经影像数据显示决策特征与“参与”状态相关。

Conclusion: 提出的框架能有效分析决策行为，揭示抑郁症患者的决策特点，并提供与“参与”状态相关的脑行为证据。

Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality,
is associated with reward-processing abnormalities and concentration issues.
Motivated by the probabilistic reward task from the Establishing Moderators and
Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we
propose a novel framework that integrates the reinforcement learning (RL) model
and drift-diffusion model (DDM) to jointly analyze reward-based decision-making
with response times. To account for emerging evidence suggesting that
decision-making may alternate between multiple interleaved strategies, we model
latent state switching using a hidden Markov model (HMM). In the ''engaged''
state, decisions follow an RL-DDM, simultaneously capturing reward processing,
decision dynamics, and temporal structure. In contrast, in the ''lapsed''
state, decision-making is modeled using a simplified DDM, where specific
parameters are fixed to approximate random guessing with equal probability. The
proposed method is implemented using a computationally efficient generalized
expectation-maximization algorithm with forward-backward procedures. Through
extensive numerical studies, we demonstrate that our proposed method
outperforms competing approaches under various reward-generating distributions,
both with and without strategy switching. When applied to the EMBARC study, our
framework reveals that MDD patients exhibit lower overall engagement than
healthy controls and experience longer decision times when they do engage.
Additionally, we show that neuroimaging measures of brain activities are
associated with decision-making characteristics in the ''engaged'' state but
not in the ''lapsed'' state, providing evidence of brain-behavioral association
specific to the ''engaged'' state.

</details>


### [296] [Simulation-Based Inference for Adaptive Experiments](https://arxiv.org/abs/2506.02881)
*Brian M Cho,Aurélien Bibaut,Nathan Kallus*

Key words: 多臂老虎机，乐观模拟，假设检验，置信区间

TL;DR: 本文提出了一种基于模拟的假设检验方法，用于在多臂老虎机实验中提高推断的准确性和效率。

<details>
  <summary>Details</summary>

Main category: stat.ME

Motivation: 传统多臂老虎机实验的推断方法依赖于渐近正态性或弱功率的鞅集中不等式，导致效果不理想。

Method: 提出了一种名为“乐观模拟”的方法，通过生成额外的实验轨迹来推断非正态样本均值统计量的分布。

Result: 该方法在多种常见老虎机设计中实现了渐近类型I错误控制、置信区间收敛和估计的强一致性，且置信区间宽度减少了50%。

Conclusion: 乐观模拟方法显著提升了推断的准确性和效率，特别适用于未被设计目标关注的臂。

Abstract: Multi-arm bandit experimental designs are increasingly being adopted over
standard randomized trials due to their potential to improve outcomes for study
participants, enable faster identification of the best-performing options,
and/or enhance the precision of estimating key parameters. Current approaches
for inference after adaptive sampling either rely on asymptotic normality under
restricted experiment designs or underpowered martingale concentration
inequalities that lead to weak power in practice. To bypass these limitations,
we propose a simulation-based approach for conducting hypothesis tests and
constructing confidence intervals for arm specific means and their differences.
Our simulation-based approach uses positively biased nuisances to generate
additional trajectories of the experiment, which we call \textit{simulation
with optimism}. Using these simulations, we characterize the distribution
potentially non-normal sample mean test statistic to conduct inference. We
provide guarantees for (i) asymptotic type I error control, (ii) convergence of
our confidence intervals, and (iii) asymptotic strong consistency of our
estimator over a wide variety of common bandit designs. Our empirical results
show that our approach achieves the desired coverage while reducing confidence
interval widths by up to 50%, with drastic improvements for arms not targeted
by the design.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [297] [Online Bayesian system identification in multivariate autoregressive models via message passing](https://arxiv.org/abs/2506.02710)
*T. N. Nisslbeck,Wouter M. Kouw*

Key words: 递归贝叶斯估计, 多变量自回归模型, 因子图, 不确定性传播

TL;DR: 提出了一种基于因子图中消息传递的多变量自回归模型递归贝叶斯估计方法，能够生成完整的后验分布，并支持在线模型证据计算。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 传统的最小二乘方法无法提供完整的后验分布和不确定性传播，因此需要一种更全面的贝叶斯估计方法。

Method: 通过因子图中的消息传递实现递归贝叶斯估计，适用于多变量自回归模型。

Result: 方法在合成自回归系统和双质量弹簧阻尼系统上表现出收敛性和竞争性性能。

Conclusion: 该方法能够有效地估计不确定性并支持在线模型评估。

Abstract: We propose a recursive Bayesian estimation procedure for multivariate
autoregressive models with exogenous inputs based on message passing in a
factor graph. Unlike recursive least-squares, our method produces full
posterior distributions for both the autoregressive coefficients and noise
precision. The uncertainties regarding these estimates propagate into the
uncertainties on predictions for future system outputs, and support online
model evidence calculations. We demonstrate convergence empirically on a
synthetic autoregressive system and competitive performance on a double
mass-spring-damper system.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [298] [A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization](https://arxiv.org/abs/2506.02458)
*Nguyen Chi Long,Trinh Van Chien,Ta Hai Tung,Van Son Nguyen,Trong-Minh Hoang,Nguyen Ngoc Hai Dang*

Key words: 移动边缘计算（MEC）、深度强化学习（DRL）、计算卸载、DDPG、Twin Delayed DDPG

TL;DR: 论文研究了如何利用深度强化学习（DRL）在移动边缘计算（MEC）系统中实现分布式动态计算卸载策略，提出了一种基于Twin Delayed DDPG的新方法，克服了DDPG算法的固有弱点，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 移动边缘计算（MEC）系统需要高效的分布式计算卸载策略，而现有的DDPG算法在动态环境中存在固有缺陷，无法满足移动用户的需求。

Method: 引入基于Twin Delayed DDPG算法的新方法，独立学习每个用户的计算卸载策略，解决了DDPG的缺陷。

Result: 数值结果表明，用户能自主学会有效策略，且新方法性能优于传统DDPG算法。

Conclusion: Twin Delayed DDPG算法能有效提升MEC系统的计算卸载性能，适用于移动用户场景。

Abstract: Mobile edge computing (MEC) allows appliances to offload workloads to
neighboring MEC servers that have the potential for computation-intensive tasks
with limited computational capabilities. This paper studied how deep
reinforcement learning (DRL) algorithms are used in an MEC system to find
feasible decentralized dynamic computation offloading strategies, which leads
to the construction of an extensible MEC system that operates effectively with
finite feedback. Even though the Deep Deterministic Policy Gradient (DDPG)
algorithm, subject to their knowledge of the MEC system, can be used to
allocate powers of both computation offloading and local execution, to learn a
computation offloading policy for each user independently, we realized that
this solution still has some inherent weaknesses. Hence, we introduced a new
approach for this problem based on the Twin Delayed DDPG algorithm, which
enables us to overcome this proneness and investigate cases where mobile users
are portable. Numerical results showed that individual users can autonomously
learn adequate policies through the proposed approach. Besides, the performance
of the suggested solution exceeded the conventional DDPG-based power control
strategy.

</details>


### [299] [Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning](https://arxiv.org/abs/2506.02657)
*Tam Ninh Thi-Thanh,Trinh Van Chien,Hung Tran,Nguyen Hoai Son,Van Nhan Vo*

Key words: 元宇宙, 数字孪生, 深度强化学习, 任务卸载, 动态环境

TL;DR: 本文探讨了深度强化学习（DRL）在基于元宇宙的数字孪生系统中的应用，证明了其在高动态环境中的任务卸载效率。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 元宇宙和数字孪生技术是未来数字世界的重要组成部分，但如何在高动态环境下保持其时效性是亟待解决的问题。

Method: 通过引入深度强化学习（DRL），设计了一个由用户设备、虚拟接入点（MVAP）和边缘计算服务器组成的系统，实现动态任务卸载。

Result: 实验结果表明，所提出的DRL算法能够有效确保数字孪生在动态环境中的时效性。

Conclusion: DRL算法为元宇宙和数字孪生技术在高动态环境中的应用提供了高效解决方案。

Abstract: Metaverse and Digital Twin (DT) have attracted much academic and industrial
attraction to approach the future digital world. This paper introduces the
advantages of deep reinforcement learning (DRL) in assisting Metaverse
system-based Digital Twin. In this system, we assume that it includes several
Metaverse User devices collecting data from the real world to transfer it into
the virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the
processing of data, and an edge computing server that receives the offloading
data from the MVAP. The proposed model works under a dynamic environment with
various parameters changing over time. The experiment results show that our
proposed DRL algorithm is suitable for offloading tasks to ensure the
promptness of DT in a dynamic environment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [300] [Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps](https://arxiv.org/abs/2506.02254)
*Dimitris G Giovanis,Nikolaos Evangelou,Ioannis G Kevrekidis,Roger G Ghanem*

Key words: 生成学习, 概率采样, 双扩散映射, 几何谐波, 过拟合

TL;DR: 提出了一种基于扩展PLoM的生成学习框架，针对小样本数据，结合双扩散映射和几何谐波方法，克服过拟合并提高泛化能力。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决PLoM方法在小样本数据下因维度接近样本数导致的过拟合和泛化能力下降问题。

Method: 结合双扩散映射（捕捉多尺度几何特征）和几何谐波（高维非线性插值），直接在潜在空间中求解完整阶ISDE。

Result: 通过二维Hermite多项式函数和爆轰波高保真模拟验证方法的有效性和鲁棒性。

Conclusion: 该方法在小样本下显著提升生成样本的统计一致性。

Abstract: We present a generative learning framework for probabilistic sampling based
on an extension of the Probabilistic Learning on Manifolds (PLoM) approach,
which is designed to generate statistically consistent realizations of a random
vector in a finite-dimensional Euclidean space, informed by a limited (yet
representative) set of observations. In its original form, PLoM constructs a
reduced-order probabilistic model by combining three main components: (a)
kernel density estimation to approximate the underlying probability measure,
(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,
and (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample
from the learned distribution. A key challenge arises, however, when the number
of available data points N is small and the dimensionality of the diffusion-map
basis approaches N, resulting in overfitting and loss of generalization. To
overcome this limitation, we propose an enabling extension that implements a
synthesis of Double Diffusion Maps -- a technique capable of capturing
multiscale geometric features of the data -- with Geometric Harmonics (GH), a
nonparametric reconstruction method that allows smooth nonlinear interpolation
in high-dimensional ambient spaces. This approach enables us to solve a
full-order ISDE directly in the latent space, preserving the full dynamical
complexity of the system, while leveraging its reduced geometric
representation. The effectiveness and robustness of the proposed method are
illustrated through two numerical studies: one based on data generated from
two-dimensional Hermite polynomial functions and another based on high-fidelity
simulations of a detonation wave in a reactive flow.

</details>


### [301] [Assumption-free stability for ranking problems](https://arxiv.org/abs/2506.02257)
*Ruiting Liang,Jake A. Soloff,Rina Foygel Barber,Rebecca Willett*

Key words: 排序问题, 算法稳定性, 膨胀 top-k, 膨胀全排序, 数据扰动

TL;DR: 该研究提出了一种新的算法稳定性框架，用于解决排序问题中的不稳定性，特别是在候选者分数接近时。通过两种新的排序操作符（膨胀 top-k 和膨胀全排序），实现了无需数据分布假设的稳定排序。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现实世界中的数据常出现候选者分数接近的情况，导致排序问题对小扰动高度敏感。现有理论依赖分离条件，但实际数据往往不符合这一假设，因此需要新的稳定性框架。

Method: 提出了两种新的排序操作符：膨胀 top-k（用于选择前 k 个项）和膨胀全排序（用于完全排序）。这些方法通过允许输出中存在不确定性来增强稳定性。

Result: 实验表明，所提出的方法在不牺牲信息量的情况下实现了稳定性，且不依赖于候选者数量或数据分布假设。

Conclusion: 新框架为排序问题提供了通用且稳定的解决方案，解决了现有理论在分数接近场景下的局限性。

Abstract: In this work, we consider ranking problems among a finite set of candidates:
for instance, selecting the top-$k$ items among a larger list of candidates or
obtaining the full ranking of all items in the set. These problems are often
unstable, in the sense that estimating a ranking from noisy data can exhibit
high sensitivity to small perturbations. Concretely, if we use data to provide
a score for each item (say, by aggregating preference data over a sample of
users), then for two items with similar scores, small fluctuations in the data
can alter the relative ranking of those items. Many existing theoretical
results for ranking problems assume a separation condition to avoid this
challenge, but real-world data often contains items whose scores are
approximately tied, limiting the applicability of existing theory. To address
this gap, we develop a new algorithmic stability framework for ranking
problems, and propose two novel ranking operators for achieving stable ranking:
the \emph{inflated top-$k$} for the top-$k$ selection problem and the
\emph{inflated full ranking} for ranking the full list. To enable stability,
each method allows for expressing some uncertainty in the output. For both of
these two problems, our proposed methods provide guaranteed stability, with no
assumptions on data distributions and no dependence on the total number of
candidates to be ranked. Experiments on real-world data confirm that the
proposed methods offer stability without compromising the informativeness of
the output.

</details>


### [302] [MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements](https://arxiv.org/abs/2506.02260)
*Howon Ryu,Yuliang Chen,Yacun Wang,Andrea Z. LaCroix,Chongzhi Di,Loki Natarajan,Yu Wang,Jingjing Zou*

Key words: 数字健康,多模态数据,自监督学习,Transformer,MoCA

TL;DR: 本文提出了一种名为多模态交叉掩码自动编码器（MoCA）的自监督学习框架，用于处理数字健康领域中的多模态数据，无需依赖大量标注数据。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 当前数字健康技术产生的多模态数据潜力巨大，但主流的监督学习方法需要大量标注数据，成本高且不切实际，尤其是在临床研究中。

Method: MoCA利用交叉模态掩码和Transformer自动编码器架构，结合时间相关性和跨模态相关性，无需监督学习。

Result: 实验表明，MoCA在重建和下游任务中优于现有方法，并提供了理论保证。

Conclusion: 自监督学习在数字健康和多模态数据中具有变革潜力。

Abstract: The growing prevalence of digital health technologies has led to the
generation of complex multi-modal data, such as physical activity measurements
simultaneously collected from various sensors of mobile and wearable devices.
These data hold immense potential for advancing health studies, but current
methods predominantly rely on supervised learning, requiring extensive labeled
datasets that are often expensive or impractical to obtain, especially in
clinical studies. To address this limitation, we propose a self-supervised
learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that
leverages cross-modality masking and the Transformer autoencoder architecture
to utilize both temporal correlations within modalities and cross-modal
correlations between data streams. We also provide theoretical guarantees to
support the effectiveness of the cross-modality masking scheme in MoCA.
Comprehensive experiments and ablation studies demonstrate that our method
outperforms existing approaches in both reconstruction and downstream tasks. We
release open-source code for data processing, pre-training, and downstream
tasks in the supplementary materials. This work highlights the transformative
potential of self-supervised learning in digital health and multi-modal data.

</details>


### [303] [Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression](https://arxiv.org/abs/2506.02336)
*Jingfeng Wu,Pierre Marion,Peter Bartlett*

Key words: 梯度下降, 逻辑回归, 线性可分, 收敛速度, 条件数

TL;DR: 研究在$\ell_2$正则化逻辑回归中，梯度下降（GD）使用大步长时对线性可分数据的加速效果。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统理论建议使用小步长以确保优化目标的单调下降，但研究发现大步长可以显著加速收敛。

Method: 通过分析梯度下降在大步长下的非单调目标演化行为，研究其收敛速度与条件数的关系。

Result: 使用大步长可将收敛步数从$\widetilde{\mathcal{O}}(\kappa)$加速到$\widetilde{\mathcal{O}}(\sqrt{\kappa})$，并拓展到最小化可分分布下的总体风险。

Conclusion: 研究结果表明，大步长可以显著加速收敛，并在特定场景下决定全局收敛性。

Abstract: We study gradient descent (GD) with a constant stepsize for
$\ell_2$-regularized logistic regression with linearly separable data.
Classical theory suggests small stepsizes to ensure monotonic reduction of the
optimization objective, achieving exponential convergence in
$\widetilde{\mathcal{O}}(\kappa)$ steps with $\kappa$ being the condition
number. Surprisingly, we show that this can be accelerated to
$\widetilde{\mathcal{O}}(\sqrt{\kappa})$ by simply using a large stepsize --
for which the objective evolves nonmonotonically. The acceleration brought by
large stepsizes extends to minimizing the population risk for separable
distributions, improving on the best-known upper bounds on the number of steps
to reach a near-optimum. Finally, we characterize the largest stepsize for the
local convergence of GD, which also determines the global convergence in
special scenarios. Our results extend the analysis of Wu et al. (2024) from
convex settings with minimizers at infinity to strongly convex cases with
finite minimizers.

</details>


### [304] [Tensor State Space-based Dynamic Multilayer Network Modeling](https://arxiv.org/abs/2506.02413)
*Tian Lan,Jie Guo,Chen Zhang*

Key words: 动态多层网络、张量状态空间模型、Tucker分解、变分EM算法

TL;DR: 该论文提出了一种新的张量状态空间模型（TSSDMN），用于动态多层网络分析，通过对称Tucker分解和变分EM算法有效捕捉网络的时间和跨层动态。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有模型难以捕捉动态多层网络的时间和跨层动态特性，需要一种新方法来解决这一问题。

Method: 使用对称Tucker分解表示潜在节点特征及其交互模式，通过固定潜在特征并允许交互模式随时间演化，结合变分期望最大化（EM）算法进行模型推断。

Result: 数值模拟和案例研究表明TSSDMN能有效理解动态多层网络。

Conclusion: TSSDMN为动态多层网络分析提供了新工具，具有理论和应用价值。

Abstract: Understanding the complex interactions within dynamic multilayer networks is
critical for advancements in various scientific domains. Existing models often
fail to capture such networks' temporal and cross-layer dynamics. This paper
introduces a novel Tensor State Space Model for Dynamic Multilayer Networks
(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric
Tucker decomposition to represent latent node features, their interaction
patterns, and layer transitions. Then by fixing the latent features and
allowing the interaction patterns to evolve over time, TSSDMN uniquely captures
both the temporal dynamics within layers and across different layers. The model
identifiability conditions are discussed. By treating latent features as
variables whose posterior distributions are approximated using a mean-field
variational inference approach, a variational Expectation Maximization
algorithm is developed for efficient model inference. Numerical simulations and
case studies demonstrate the efficacy of TSSDMN for understanding dynamic
multilayer networks.

</details>


### [305] [Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks](https://arxiv.org/abs/2506.02651)
*Luca Arnaboldi,Bruno Loureiro,Ludovic Stephan,Florent Krzakala,Lenka Zdeborova*

Key words: 随机梯度下降, 序列单索引模型, 注意力架构, 训练动态

TL;DR: 论文研究了序列单索引（SSI）模型在随机梯度下降（SGD）下的动态行为，揭示了两个训练阶段及其影响因素。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 探索序列数据结构和注意力模型如何通过SGD有效学习。

Method: 推导SSI模型的群体损失闭式解，分析高维SGD动态。

Result: 发现训练分为两个阶段，序列长度和位置编码影响收敛速度。

Conclusion: 结果为理解序列结构对注意力模型学习的益处提供了理论基础。

Abstract: We study the dynamics of stochastic gradient descent (SGD) for a class of
sequence models termed Sequence Single-Index (SSI) models, where the target
depends on a single direction in input space applied to a sequence of tokens.
This setting generalizes classical single-index models to the sequential
domain, encompassing simplified one-layer attention architectures. We derive a
closed-form expression for the population loss in terms of a pair of sufficient
statistics capturing semantic and positional alignment, and characterize the
induced high-dimensional SGD dynamics for these coordinates. Our analysis
reveals two distinct training phases: escape from uninformative initialization
and alignment with the target subspace, and demonstrates how the sequence
length and positional encoding influence convergence speed and learning
trajectories. These results provide a rigorous and interpretable foundation for
understanding how sequential structure in data can be beneficial for learning
with attention-based models.

</details>


### [306] [Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model](https://arxiv.org/abs/2506.02664)
*Hugo Tabanelli,Pierre Mergny,Lenka Zdeborova,Florent Krzakala*

Key words: 高维信号恢复, 多模态学习, 尖峰矩阵, 尖峰张量, 贝叶斯近似消息传递, 序列课程学习

TL;DR: 论文研究如何从两个噪声相关模态（尖峰矩阵和尖峰张量）中恢复多个高维信号，揭示了模态间交互的复杂行为和高效恢复策略。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究多模态高维信号恢复，尤其是尖峰矩阵和尖峰张量模态的交互及其对恢复效率的影响。

Method: 采用贝叶斯近似消息传递（Bayesian Approximate Message Passing）和序列课程学习（Sequential Curriculum Learning）策略。

Result: 模态间的相关性实现了高效恢复，而联合优化反而导致性能下降；序列学习策略达到最优弱恢复阈值。

Conclusion: 结构相关性和学习顺序在多模态高维推断中至关重要，序列学习策略优于联合优化。

Abstract: We study the recovery of multiple high-dimensional signals from two noisy,
correlated modalities: a spiked matrix and a spiked tensor sharing a common
low-rank structure. This setting generalizes classical spiked matrix and tensor
models, unveiling intricate interactions between inference channels and
surprising algorithmic behaviors. Notably, while the spiked tensor model is
typically intractable at low signal-to-noise ratios, its correlation with the
matrix enables efficient recovery via Bayesian Approximate Message Passing,
inducing staircase-like phase transitions reminiscent of neural network
phenomena. In contrast, empirical risk minimization for joint learning fails:
the tensor component obstructs effective matrix recovery, and joint
optimization significantly degrades performance, highlighting the limitations
of naive multi-modal learning. We show that a simple Sequential Curriculum
Learning strategy-first recovering the matrix, then leveraging it to guide
tensor recovery-resolves this bottleneck and achieves optimal weak recovery
thresholds. This strategy, implementable with spectral methods, emphasizes the
critical role of structural correlation and learning order in multi-modal
high-dimensional inference.

</details>


### [307] [Symmetry-Aware GFlowNets](https://arxiv.org/abs/2506.02685)
*Hohyun Kim,Seunggeun Lee,Min-hwan Oh*

Key words: GFlowNets, 对称性, 偏差校正, 奖励缩放, 图采样

TL;DR: 提出SA-GFN方法，通过对称性修正改进GFlowNets的采样偏差。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有的GFlowNets在采样图时因状态转移概率计算不准确而产生系统性偏差，亟需改进。

Method: 引入对称性修正，通过奖励缩放直接整合偏差校正，避免显式状态转移计算。

Result: SA-GFN实现无偏采样，提升多样性，生成的高奖励图与目标分布高度匹配。

Conclusion: SA-GFN有效解决了GFlowNets的对称性偏差问题，显著提升采样性能。

Abstract: Generative Flow Networks (GFlowNets) offer a powerful framework for sampling
graphs in proportion to their rewards. However, existing approaches suffer from
systematic biases due to inaccuracies in state transition probability
computations. These biases, rooted in the inherent symmetries of graphs, impact
both atom-based and fragment-based generation schemes. To address this
challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that
incorporates symmetry corrections into the learning process through reward
scaling. By integrating bias correction directly into the reward structure,
SA-GFN eliminates the need for explicit state transition computations.
Empirical results show that SA-GFN enables unbiased sampling while enhancing
diversity and consistently generating high-reward graphs that closely match the
target distribution.

</details>


### [308] [Safely Learning Controlled Stochastic Dynamics](https://arxiv.org/abs/2506.02754)
*Luc Brogat-Motte,Alessandro Rudi,Riccardo Bonalli*

Key words: 安全学习, 随机动力学, 核方法, 自适应学习

TL;DR: 该论文提出了一种从离散时间轨迹观测中安全学习受控随机动力学的方法，确保训练和部署期间系统轨迹始终在预定安全区域内。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在自主机器人、金融和生物医学等应用中，确保系统轨迹在安全区域内至关重要。

Method: 通过基于核的置信边界迭代扩展初始已知安全控制集，实现安全探索和高效估计系统动力学。

Result: 学习后的模型能预测系统动态并验证任何给定控制的安全性。理论保证安全性，并通过实验验证了方法的有效性。

Conclusion: 该方法在复杂实际系统中具有广泛适用性，且实验证实其在安全性和计算效率上的优越性。

Abstract: We address the problem of safely learning controlled stochastic dynamics from
discrete-time trajectory observations, ensuring system trajectories remain
within predefined safe regions during both training and deployment.
Safety-critical constraints of this kind are crucial in applications such as
autonomous robotics, finance, and biomedicine. We introduce a method that
ensures safe exploration and efficient estimation of system dynamics by
iteratively expanding an initial known safe control set using kernel-based
confidence bounds. After training, the learned model enables predictions of the
system's dynamics and permits safety verification of any given control. Our
approach requires only mild smoothness assumptions and access to an initial
safe control set, enabling broad applicability to complex real-world systems.
We provide theoretical guarantees for safety and derive adaptive learning rates
that improve with increasing Sobolev regularity of the true dynamics.
Experimental evaluations demonstrate the practical effectiveness of our method
in terms of safety, estimation accuracy, and computational efficiency.

</details>


### [309] [Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings](https://arxiv.org/abs/2506.02793)
*Houssam Zenati,Bariscan Bozkurt,Arthur Gretton*

Key words: 反事实政策评估, RKHS, 双重稳健估计器, 核测试统计量

TL;DR: 论文提出了一种名为CPME的新框架，用于估计反事实政策下结果的分布，通过RKHS表示分布，支持灵活的非参数评估，并提出了两种估计器。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 在推荐、广告和医疗等领域，估计反事实政策下的结果分布对决策至关重要。

Method: 采用RKHS表示反事实结果分布，提出插件估计器和双重稳健估计器，并基于此开发了一种双重稳健核测试统计量。

Result: 双重稳健估计器在收敛速度上表现更好，且核测试统计量支持高效计算和置信区间构建。

Conclusion: CPME框架在数值模拟中优于现有方法，具有实际应用价值。

Abstract: Estimating the distribution of outcomes under counterfactual policies is
critical for decision-making in domains such as recommendation, advertising,
and healthcare. We analyze a novel framework-Counterfactual Policy Mean
Embedding (CPME)-that represents the entire counterfactual outcome distribution
in a reproducing kernel Hilbert space (RKHS), enabling flexible and
nonparametric distributional off-policy evaluation. We introduce both a plug-in
estimator and a doubly robust estimator; the latter enjoys improved uniform
convergence rates by correcting for bias in both the outcome embedding and
propensity models. Building on this, we develop a doubly robust kernel test
statistic for hypothesis testing, which achieves asymptotic normality and thus
enables computationally efficient testing and straightforward construction of
confidence intervals. Our framework also supports sampling from the
counterfactual distribution. Numerical simulations illustrate the practical
benefits of CPME over existing methods.

</details>


### [310] [Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)](https://arxiv.org/abs/2506.02825)
*Tong Qi,Vera Andersson,Peter Viechnicki,Vince Lyzinski*

Key words: 多图匹配, 随机点积图, 顶点对齐, 测试能力恢复, 数据连接

TL;DR: OmniMatch算法用于种子化多图匹配，在RDPG模型中证明能高效对齐大量未种子顶点，并在模拟和实际数据中展示其有效性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决多图匹配中顶点对齐问题，尤其是在测试中由于顶点错位导致的测试能力损失。

Method: 提出OmniMatch算法，利用种子顶点逐步对齐未种子顶点，适用于随机点积图模型。

Result: 算法在模拟和实际数据中有效对齐顶点，恢复测试能力。

Conclusion: OmniMatch在多图匹配和数据对齐任务中表现出色，具有理论和实际应用价值。

Abstract: We present the OmniMatch algorithm for seeded multiple graph matching. In the
setting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that
under mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently
perfectly aligns $O(s^{\alpha})$ unseeded vertices -- for $\alpha<2\wedge d/4$
-- across multiple networks even in the presence of no edge correlation. We
demonstrate the effectiveness of our algorithm across numerous simulations and
in the context of shuffled graph hypothesis testing. In the shuffled testing
setting, testing power is lost due to the misalignment/shuffling of vertices
across graphs, and we demonstrate the capacity of OmniMatch to correct for
misaligned vertices prior to testing and hence recover the lost testing power.
We further demonstrate the algorithm on a pair of data examples from
connectomics and machine translation.

</details>


### [311] [Non-stationary Bandit Convex Optimization: A Comprehensive Study](https://arxiv.org/abs/2506.02980)
*Xiaoqi Liu,Dorian Baudry,Julian Zimmert,Patrick Rebeschini,Arya Akhavan*

Key words: Bandit凸优化, 非平稳环境, 最小极大后悔, TEWA-SE, cExO

TL;DR: 研究非平稳环境中的Bandit凸优化问题，提出两种算法（TEWA-SE和cExO），分别针对强凸和一般凸损失函数，实现了对非平稳性指标（S、Δ、P）的最小极大最优后悔。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 解决Bandit凸优化在非平稳环境中的挑战，尤其是针对不同非平稳性指标（S、Δ、P）的最优后悔边界。

Method: 提出TEWA-SE算法（结合sleeping专家框架）和cExO算法（基于离散化动作空间的指数加权），分别处理强凸和一般凸损失。

Result: TEWA-SE对已知S和Δ的强凸损失函数实现最小极大最优后悔；cExO对一般凸损失函数改进P相关的后悔边界。

Conclusion: 通过新算法，展示了在非平稳环境中Bandit凸优化的最优后悔边界，并扩展了未知非平稳性指标的处理能力。

Abstract: Bandit Convex Optimization is a fundamental class of sequential
decision-making problems, where the learner selects actions from a continuous
domain and observes a loss (but not its gradient) at only one point per round.
We study this problem in non-stationary environments, and aim to minimize the
regret under three standard measures of non-stationarity: the number of
switches $S$ in the comparator sequence, the total variation $\Delta$ of the
loss functions, and the path-length $P$ of the comparator sequence. We propose
a polynomial-time algorithm, Tilted Exponentially Weighted Average with
Sleeping Experts (TEWA-SE), which adapts the sleeping experts framework from
online convex optimization to the bandit setting. For strongly convex losses,
we prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\Delta$
by establishing matching upper and lower bounds. By equipping TEWA-SE with the
Bandit-over-Bandit framework, we extend our analysis to environments with
unknown non-stationarity measures. For general convex losses, we introduce a
second algorithm, clipped Exploration by Optimization (cExO), based on
exponential weights over a discretized action space. While not polynomial-time
computable, this method achieves minimax-optimal regret with respect to known
$S$ and $\Delta$, and improves on the best existing bounds with respect to $P$.

</details>


### [312] [Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records](https://arxiv.org/abs/2506.03068)
*Yina Hou,Shourav B. Rabbani,Liang Hong,Norou Diawara,Manar D. Samad*

Key words: 因果发现, 心衰, 混合类型变量, ML重要性, 非线性模型

TL;DR: 论文提出新方法用于探索混合类型临床变量的因果关系，发现非线性因果模型比线性模型更有意义，且特征重要性排序与因果强度相关。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 探讨临床变量在疾病预后中的因果关系，弥补统计相关性和ML预测重要性无法反映因果关系的不足。

Method: 提出一种计算框架，支持混合类型变量的因果结构发现和强度评分，应用于心衰患者队列。

Result: 非线性因果模型更有意义；ML特征重要性与因果强度强相关；相关变量可能是原因但很少是结果。

Conclusion: 研究为ML预测模型提供了变量的因果解释框架。

Abstract: The importance of clinical variables in the prognosis of the disease is
explained using statistical correlation or machine learning (ML). However, the
predictive importance of these variables may not represent their causal
relationships with diseases. This paper uses clinical variables from a heart
failure (HF) patient cohort to investigate the causal explainability of
important variables obtained in statistical and ML contexts. Due to inherent
regression modeling, popular causal discovery methods strictly assume that the
cause and effect variables are numerical and continuous. This paper proposes a
new computational framework to enable causal structure discovery (CSD) and
score the causal strength of mixed-type (categorical, numerical, binary)
clinical variables for binary disease outcomes. In HF classification, we
investigate the association between the importance rank order of three feature
types: correlated features, features important for ML predictions, and causal
features. Our results demonstrate that CSD modeling for nonlinear causal
relationships is more meaningful than its linear counterparts. Feature
importance obtained from nonlinear classifiers (e.g., gradient-boosting trees)
strongly correlates with the causal strength of variables without
differentiating cause and effect variables. Correlated variables can be causal
for HF, but they are rarely identified as effect variables. These results can
be used to add the causal explanation of variables important for ML-based
prediction modeling.

</details>


### [313] [GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression](https://arxiv.org/abs/2506.03074)
*Junghyun Lee,Kyoungseok Jang,Kwang-Sung Jun,Milan Vojnović,Se-Young Yun*

Key words: 广义低秩迹回归、Catoni估计器、核范数正则化、非线性逆链接函数、双线性对决赌博

TL;DR: GL-LowPopArt是一种新型的Catoni式估计器，用于广义低秩迹回归。它采用两阶段方法：核范数正则化和矩阵Catoni估计，实现了先进的估计误差界限，并在应用中表现出色。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究旨在解决广义低秩迹回归中的估计问题，尤其是在非线性逆链接函数引入的偏差控制上提出创新方法。

Method: 使用两阶段方法：核范数正则化（第一阶段）和矩阵Catoni估计（第二阶段），以应对非线性逆链接函数的偏差。

Result: GL-LowPopArt实现了先进的估计误差界限，并在广义线性矩阵完成和双线性对决赌博等应用中表现出色。

Conclusion: GL-LowPopArt在实例优化和误差界限上优于现有方法，为相关领域提供了新的技术解决方案。

Abstract: We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized
low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it
employs a two-stage approach: nuclear norm regularization followed by matrix
Catoni estimation. We establish state-of-the-art estimation error bounds,
surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and
reveal a novel experimental design objective, $\mathrm{GL}(\pi)$. The key
technical challenge is controlling bias from the nonlinear inverse link
function, which we address by our two-stage approach. We prove a *local*
minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise
optimality up to the condition number of the ground-truth Hessian. Applications
include generalized linear matrix completion, where `GL-LowPopArt` achieves a
state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a
novel setting inspired by general preference learning (Zhang et al., 2024). Our
analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,
potentially interesting problem-dependent quantity, along with improved Borda
regret bound than vectorization (Wu et al., 2024).

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [314] [Enhancing Interpretability of Quantum-Assisted Blockchain Clustering via AI Agent-Based Qualitative Analysis](https://arxiv.org/abs/2506.02068)
*Yun-Cheng Tsai,Yen-Ku Liu,Samuel Yen-Chi Chen*

Key words: 区块链分析, 量子聚类, 可解释性, AI代理

TL;DR: 提出一种两阶段分析框架，结合定量聚类评估与AI代理辅助的定性解释，提升量子增强聚类模型在区块链数据分析中的可解释性。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 区块链交易数据高维、噪声多且纠缠，传统聚类算法难以处理，量子增强聚类模型虽性能好但可解释性不足。

Method: 第一阶段用经典聚类方法和评估指标确定最优聚类数量；第二阶段引入AI代理生成聚类结果的可解释语义分析。

Result: 实验表明量子神经网络（QNN）优于随机量子特征（QF），AI代理进一步揭示QNN驱动的单例聚类现象。三聚类配置效果最佳。

Conclusion: 该框架提升了量子辅助区块链分析的可解释性，为未来自主AI驱动的聚类框架奠定了基础。

Abstract: Blockchain transaction data is inherently high dimensional, noisy, and
entangled, posing substantial challenges for traditional clustering algorithms.
While quantum enhanced clustering models have demonstrated promising
performance gains, their interpretability remains limited, restricting their
application in sensitive domains such as financial fraud detection and
blockchain governance. To address this gap, we propose a two stage analysis
framework that synergistically combines quantitative clustering evaluation with
AI Agent assisted qualitative interpretation. In the first stage, we employ
classical clustering methods and evaluation metrics including the Silhouette
Score, Davies Bouldin Index, and Calinski Harabasz Index to determine the
optimal cluster count and baseline partition quality. In the second stage, we
integrate an AI Agent to generate human readable, semantic explanations of
clustering results, identifying intra cluster characteristics and inter cluster
relationships. Our experiments reveal that while fully trained Quantum Neural
Networks (QNN) outperform random Quantum Features (QF) in quantitative metrics,
the AI Agent further uncovers nuanced differences between these methods,
notably exposing the singleton cluster phenomenon in QNN driven models. The
consolidated insights from both stages consistently endorse the three cluster
configuration, demonstrating the practical value of our hybrid approach. This
work advances the interpretability frontier in quantum assisted blockchain
analytics and lays the groundwork for future autonomous AI orchestrated
clustering frameworks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [315] [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
*Sinem Erisken,Timothy Gothard,Martin Leitgab,Ram Potham*

Key words: MAEBE, 多智能体, 新兴风险, LLM, 道德偏好

TL;DR: 传统AI安全评估在孤立LLM上不足，多智能体AI集成带来新风险。MAEBE框架系统评估这些风险，发现LLM道德偏好脆弱，群体行为不可预测，存在安全挑战。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 评估多智能体AI集成中的新兴风险，弥补传统孤立LLM评估的不足。

Method: 提出MAEBE框架，结合Greatest Good Benchmark和双反转问题技术，分析单智能体与集成智能体的行为差异。

Result: LLM道德偏好易受问题设计影响，群体行为涌现不可预测，存在同伴压力等现象。

Conclusion: 需在交互式多智能体环境中评估AI系统。

Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.

</details>


### [316] [ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms](https://arxiv.org/abs/2506.02931)
*Praneet Sai Madhu Surabhi,Dheeraj Reddy Mudireddy,Jian Tao*

Key words: ThinkTank, 协作智能, 检索增强生成, 本地化部署, 知识共享

TL;DR: ThinkTank是一个可扩展的AI协作框架，通过角色抽象、会议类型泛化和知识集成，将专业AI系统转化为多领域协作平台，支持复杂问题解决。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 解决专业AI系统在多领域协作中的不足，提供一种通用、高效且安全的协作智能平台。

Method: 采用角色抽象、会议类型泛化、检索增强生成和本地化部署（如Ollama框架和Llama3.1模型）。

Result: ThinkTank在成本效益、数据安全、可扩展性和竞争地位上优于基于云的方案。

Conclusion: ThinkTank是一个通用的AI协作问题解决平台，适用于知识密集型任务。

Abstract: This paper presents ThinkTank, a comprehensive and scalable framework
designed to transform specialized AI agent systems into versatile collaborative
intelligence platforms capable of supporting complex problem-solving across
diverse domains. ThinkTank systematically generalizes agent roles, meeting
structures, and knowledge integration mechanisms by adapting proven scientific
collaboration methodologies. Through role abstraction, generalization of
meeting types for iterative collaboration, and the integration of
Retrieval-Augmented Generation with advanced knowledge storage, the framework
facilitates expertise creation and robust knowledge sharing. ThinkTank enables
organizations to leverage collaborative AI for knowledge-intensive tasks while
ensuring data privacy and security through local deployment, utilizing
frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is
designed to deliver significant advantages in cost-effectiveness, data
security, scalability, and competitive positioning compared to cloud-based
alternatives, establishing it as a universal platform for AI-driven
collaborative problem-solving. The ThinkTank code is available at
https://github.com/taugroup/ThinkTank

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [317] [Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers](https://arxiv.org/abs/2506.01994)
*Wanshan Cui,Yejin Jeong,Inwook Song,Gyuri Kim,Minsang Kwon,Donghun Lee*

Key words: 聚合物材料, 异常检测, 机器学习, 预测模型, 数据质量

TL;DR: 提出一种多算法异常检测结合选择性重实验的方法，显著提升聚合物材料属性预测准确性，减少实验工作量。

<details>
  <summary>Details</summary>

Main category: cond-mat.soft

Motivation: 解决聚合物材料属性预测中由异常值导致的机器学习模型偏差问题。

Method: 整合多算法异常检测与选择性重实验，构建高质量数据集，并在多种机器学习模型上验证。

Result: 预测误差显著降低，仅需重新测量5%的数据即可提升准确性。

Conclusion: 数据质量提升对机器学习可靠性至关重要，该方法可推广至材料科学领域。

Abstract: Accurate prediction of polymer material properties through data-driven
approaches greatly accelerates novel material development by reducing redundant
experiments and trial-and-error processes. However, inevitable outliers in
empirical measurements can severely skew machine learning results, leading to
erroneous prediction models and suboptimal material designs. To address this
limitation, we propose a novel approach to enhance dataset quality efficiently
by integrating multi-algorithm outlier detection with selective
re-experimentation of unreliable outlier cases. To validate the empirical
effectiveness of the approach, we systematically construct a new dataset
containing 701 measurements of three key mechanical properties: glass
transition temperature ($T_g$), tan $\delta$ peak, and crosslinking density
($v_{c}$). To demonstrate its general applicability, we report the performance
improvements across multiple machine learning models, including Elastic Net,
SVR, Random Forest, and TPOT, to predict the three key properties. Our method
reliably reduces prediction error (RMSE) and significantly improves accuracy
with minimal additional experimental work, requiring only about 5% of the
dataset to be re-measured.These findings highlight the importance of data
quality enhancement in achieving reliable machine learning applications in
polymer science and present a scalable strategy for improving predictive
reliability in materials science.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [318] [The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era](https://arxiv.org/abs/2506.02027)
*Shriphani Palakodety*

Key words: 身份系统, 隐私风险, 密码学, AI时代, 终身通用标识符

TL;DR: 论文主张逐步淘汰终身通用标识符（ULIs），因为它们在现代AI时代存在隐私风险，提出了一个密码学框架以满足新需求。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 终身通用标识符（ULIs）在多个领域广泛使用，但在AI时代下其隐私风险已无法通过传统保护措施解决。

Method: 提出一种密码学框架，满足AI时代的身份系统核心属性，同时兼容现有标识符工作流。

Result: 该设计保留了机构工作流，支持审计和授权等功能，并提供了逐步淘汰ULIs的实用方案。

Conclusion: ULIs已不适应AI时代，需被替代，新框架提供了可行的解决方案。

Abstract: Many identity systems assign a single, static identifier to an individual for
life, reused across domains like healthcare, finance, and education. These
Universal Lifelong Identifiers (ULIs) underpin critical workflows but now pose
systemic privacy risks. We take the position that ULIs are fundamentally
incompatible with the AI era and must be phased out. We articulate a threat
model grounded in modern AI capabilities and show that traditional safeguards
such as redaction, consent, and access controls are no longer sufficient. We
define core properties for identity systems in the AI era and present a
cryptographic framework that satisfies them while retaining compatibility with
existing identifier workflows. Our design preserves institutional workflows,
supports essential functions such as auditability and delegation, and offers a
practical migration path beyond ULIs.

</details>


### [319] [Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges](https://arxiv.org/abs/2506.02032)
*Raj Patel,Himanshu Tripathi,Jasper Stone,Noorbakhsh Amiri Golilarz,Sudip Mittal,Shahram Rahimi,Vini Chaudhary*

Key words: MLOps, MITRE ATLAS, 安全, 攻击分类, 防御策略

TL;DR: 论文系统应用MITRE ATLAS框架评估MLOps生态系统的潜在攻击，并提出分类的防御策略，强调安全协议的重要性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着MLOps的快速发展，其安全问题日益突出，需要系统化的方法来评估和防御潜在攻击。

Method: 采用MITRE ATLAS框架，分析MLOps生态系统各阶段的攻击技术，并提出相应的缓解策略。

Result: 提出了攻击技术的分类和对应的防御策略，填补了MLOps安全领域的研究空白。

Conclusion: 强调了从早期阶段实施安全协议的重要性，以保护MLOps生态系统免受攻击。

Abstract: The rapid adoption of machine learning (ML) technologies has driven
organizations across diverse sectors to seek efficient and reliable methods to
accelerate model development-to-deployment. Machine Learning Operations (MLOps)
has emerged as an integrative approach addressing these requirements by
unifying relevant roles and streamlining ML workflows. As the MLOps market
continues to grow, securing these pipelines has become increasingly critical.
However, the unified nature of MLOps ecosystem introduces vulnerabilities,
making them susceptible to adversarial attacks where a single misconfiguration
can lead to compromised credentials, severe financial losses, damaged public
trust, and the poisoning of training data. Our paper presents a systematic
application of the MITRE ATLAS (Adversarial Threat Landscape for
Artificial-Intelligence Systems) framework, a comprehensive and continuously
updated catalog of AI-focused attacks, to systematically assess attacks across
different phases of the MLOps ecosystem. We begin by examining the preparatory
phases during which adversaries acquire the essential intelligence required to
initiate their attacks. We then present a structured taxonomy of attack
techniques explicitly mapped to corresponding phases of the MLOps ecosystem,
supported by examples drawn from red-teaming exercises and real-world
incidents. This is followed by a taxonomy of mitigation strategies aligned with
these attack categories, offering actionable early-stage defenses to strengthen
the security of MLOps ecosystem. Given the rapid evolution and adoption of
MLOps, we further highlight key research gaps that require immediate attention.
Our work emphasizes the importance of implementing robust security protocols
from the outset, empowering practitioners to safeguard MLOps ecosystem against
evolving cyber attacks.

</details>


### [320] [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)
*Lajos Muzsai,David Imolai,András Lukács*

Key words: 大型语言模型, 网络安全, 加密CTF, 工具辅助推理, GRPO

TL;DR: 论文提出了一个名为“random-crypto”的框架，用于改进大型语言模型在网络安全问题中的工具辅助推理能力，通过GRPO方法显著提升了模型在加密CTF挑战中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 大型语言模型在网络安全应用中的结构化推理和工具辅助计算能力仍然不足，需要改进。

Method: 研究使用“random-crypto”框架，结合GRPO方法，对Llama-3.1-8B模型进行微调，使其能在隔离环境中迭代编写和执行Python代码。

Result: 微调后的模型在未见过的“random-crypto”任务中Pass@8提升了53%，在picoCTF加密问题上Pass@8提升了13个百分点。

Conclusion: GRPO方法通过提升工具调用和代码合成的可靠性，显著增强了模型在加密CTF挑战中的表现。

Abstract: Large Language Models (LLMs) still struggle with the structured reasoning and
tool-assisted computation needed for problem solving in cybersecurity
applications. In this work, we introduce "random-crypto", a cryptographic
Capture-the-Flag (CTF) challenge generator framework that we use to fine-tune a
tool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation
(GRPO), allowing the agent to iteratively write and execute Python inside an
isolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen
"random-crypto" tasks (0.35 -> 0.88) and raises Majority@8 to 0.41. The
fine-tuned agent also generalizes to an external dataset. On a subset of
picoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the
gains stem from more reliable tool invocation and code synthesis, rather than
superficial prompt adaptation.

</details>


### [321] [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
*Kalyan Nakka,Nitesh Saxena*

Key words: 大语言模型,安全对齐,黑盒越狱攻击,BitBypass,对抗性攻击,比特流伪装

TL;DR: 本文提出了一种名为BitBypass的黑盒越狱攻击方法，利用连字符分隔的比特流伪装越狱对齐的大语言模型（LLMs）。与依赖提示工程或对抗性操作的传统方法不同，BitBypass通过利用数据的基本信息表示（连续比特）实现越狱。实验证明，BitBypass能够成功绕过五种先进LLMs的安全对齐，并生成有害内容，同时在隐蔽性和攻击成功率上优于现有攻击方法。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 大型语言模型（LLMs）的安全对齐是当前研究的重要方向，但其在面对对抗性攻击时的鲁棒性往往不足。现有的对齐方法容易被攻击者利用未探索的漏洞绕过，因此需要研究新型攻击方法以揭示这些漏洞，从而改进模型的安全性。

Method: 本文开发了一种名为BitBypass的黑盒越狱攻击方法。该方法通过连字符分隔的比特流伪装生成输入，绕过LLMs的安全对齐机制。实验评估了五种先进LLMs（GPT-4o、Gemini 1.5、Claude 3.5、Llama 3.1和Mixtral）在对抗性视角下的表现。

Result: 实验结果表明，BitBypass能够有效绕过五种LLMs的安全对齐机制，成功生成有害内容。此外，BitBypass在隐蔽性和攻击成功率上优于其他现有越狱攻击方法。

Conclusion: BitBypass是一种高效且隐蔽的新型越狱攻击方法，通过利用比特流伪装实现越狱，揭示了当前LLMs安全对齐机制的潜在漏洞，为未来改进模型安全性提供了方向。

Abstract: The inherent risk of generating harmful and unsafe content by Large Language
Models (LLMs), has highlighted the need for their safety alignment. Various
techniques like supervised fine-tuning, reinforcement learning from human
feedback, and red-teaming were developed for ensuring the safety alignment of
LLMs. However, the robustness of these aligned LLMs is always challenged by
adversarial attacks that exploit unexplored and underlying vulnerabilities of
the safety alignment. In this paper, we develop a novel black-box jailbreak
attack, called BitBypass, that leverages hyphen-separated bitstream camouflage
for jailbreaking aligned LLMs. This represents a new direction in jailbreaking
by exploiting fundamental information representation of data as continuous
bits, rather than leveraging prompt engineering or adversarial manipulations.
Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude
3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the
capabilities of BitBypass in bypassing their safety alignment and tricking them
into generating harmful and unsafe content. Further, we observed that BitBypass
outperforms several state-of-the-art jailbreak attacks in terms of stealthiness
and attack success. Overall, these results highlights the effectiveness and
efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.

</details>


### [322] [MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models](https://arxiv.org/abs/2506.02362)
*Xueqi Cheng,Minxing Zheng,Shixiang Zhu,Yushun Dong*

Key words: 模型提取攻击,防御策略,MISLEADER,双层优化,数据增强

TL;DR: 该论文提出了MISLEADER防御策略，通过双层优化和数据增强抵御模型提取攻击，不依赖OOD假设，保持了实用性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 模型提取攻击威胁MLaaS提供商的知识产权，现有防御方法依赖OOD假设且在实际场景中效果不佳。

Method: 提出MISLEADER，结合数据增强和异构蒸馏模型，通过双层优化保护模型。

Result: 实验验证了该策略在保护预测保真度和抵抗提取方面的有效性。

Conclusion: MISLEADER是一种实用且高效的防御方法，适用于实际部署场景。

Abstract: Model extraction attacks aim to replicate the functionality of a black-box
model through query access, threatening the intellectual property (IP) of
machine-learning-as-a-service (MLaaS) providers. Defending against such attacks
is challenging, as it must balance efficiency, robustness, and utility
preservation in the real-world scenario. Despite the recent advances, most
existing defenses presume that attacker queries have out-of-distribution (OOD)
samples, enabling them to detect and disrupt suspicious inputs. However, this
assumption is increasingly unreliable, as modern models are trained on diverse
datasets and attackers often operate under limited query budgets. As a result,
the effectiveness of these defenses is significantly compromised in realistic
deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of
dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does
not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel
optimization problem that simultaneously preserves predictive fidelity on
benign inputs and reduces extractability by potential clone models. Our
framework combines data augmentation to simulate attacker queries with an
ensemble of heterogeneous distilled models to improve robustness and diversity.
We further provide a tractable approximation algorithm and derive theoretical
error bounds to characterize defense effectiveness. Extensive experiments
across various settings validate the utility-preserving and
extraction-resistant properties of our proposed defense strategy. Our code is
available at https://github.com/LabRAI/MISLEADER.

</details>


### [323] [Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment](https://arxiv.org/abs/2506.02038)
*Anum Nawaz,Hafiz Humza Mahmood Ramzan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Key words: 边缘智能, 区块链, 隐私保护, 1D-CNN, 健康应用

TL;DR: 论文提出了一种结合边缘智能和区块链技术的自主计算模型，旨在解决隐私保护和时效性强的健康应用中的安全问题，并设计了一种资源高效的1D-CNN用于心律失常分类。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 边缘智能和区块链的结合虽能提升隐私保护和系统可靠性，但其架构中存在数据交互和存储的漏洞，尤其是在健康应用中。

Method: 提出自主计算模型及其交互拓扑结构，包括数据事务处理器、隐私保护机制、1D-CNN分类器和安全访问方案。

Result: 经安全性、性能和成本分析验证，模型在细粒度访问控制和实时分析中表现出高效可靠。

Conclusion: 模型适用于隐私关键和时间敏感的健康应用，解决了现有架构的脆弱性问题。

Abstract: Edge Intelligence (EI) serves as a critical enabler for privacy-preserving
systems by providing AI-empowered computation and distributed caching services
at the edge, thereby minimizing latency and enhancing data privacy. The
integration of blockchain technology further augments EI frameworks by ensuring
transactional transparency, auditability, and system-wide reliability through a
decentralized network model. However, the operational architecture of such
systems introduces inherent vulnerabilities, particularly due to the extensive
data interactions between edge gateways (EGs) and the distributed nature of
information storage during service provisioning. To address these challenges,
we propose an autonomous computing model along with its interaction topologies
tailored for privacy-critical and time-sensitive health applications. The
system supports continuous monitoring, real-time alert notifications, disease
detection, and robust data processing and aggregation. It also includes a data
transaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a
resource-efficient one-dimensional convolutional neural network (1D-CNN) is
proposed for the multiclass classification of arrhythmia, enabling accurate and
real-time analysis of constrained EGs. Furthermore, a secure access scheme is
defined to manage both off-chain and on-chain data sharing and storage. To
validate the proposed model, comprehensive security, performance, and cost
analyses are conducted, demonstrating the efficiency and reliability of the
fine-grained access control scheme.

</details>


### [324] [A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges](https://arxiv.org/abs/2506.02438)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Key words: 入侵检测系统, 机器学习, 数据集, 分类器, 网络安全

TL;DR: 本文综述了入侵检测系统（IDS）的研究进展，重点关注机器学习分类器在提升检测准确性中的作用，并分析了现有数据集的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 随着全球对技术和自动化流程的依赖加深，保障系统和网络安全成为重要问题。机器学习在IDS中的应用成为研究热点。

Method: 通过文献综述，分析多种机器学习分类器（如SVM、KNN、DT等）在IDS中的应用，并评估数据集（如KDDCUP'99、NSL-KDD等）的挑战。

Result: 总结了各分类器的角色、算法及检测效果，提供了详细的数据集、分类器、攻击类型和评估指标的表格分析。

Conclusion: 本文为未来IDS研究提供了全面的综述和分析。

Abstract: IDS aims to protect computer networks from security threats by detecting,
notifying, and taking appropriate action to prevent illegal access and protect
confidential information. As the globe becomes increasingly dependent on
technology and automated processes, ensuring secured systems, applications, and
networks has become one of the most significant problems of this era. The
global web and digital technology have significantly accelerated the evolution
of the modern world, necessitating the use of telecommunications and data
transfer platforms. Researchers are enhancing the effectiveness of IDS by
incorporating popular datasets into machine learning algorithms. IDS, equipped
with machine learning classifiers, enhances security attack detection accuracy
by identifying normal or abnormal network traffic. This paper explores the
methods of capturing and reviewing intrusion detection systems (IDS) and
evaluates the challenges existing datasets face. A deluge of research on
machine learning (ML) and deep learning (DL) architecture-based intrusion
detection techniques has been conducted in the past ten years on various
cybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,
and CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth
analysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,
RF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,
explaining the role of the classifiers and algorithms used. A detailed tabular
analysis highlights the datasets used, classifiers employed, attacks detected,
evaluation metrics, and conclusions drawn. This article offers a thorough
review for future IDS research.

</details>


### [325] [CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale](https://arxiv.org/abs/2506.02548)
*Zhun Wang,Tianneng Shi,Jingxuan He,Matthew Cai,Jialin Zhang,Dawn Song*

Key words: 大型语言模型, 网络安全, 概念证明, 零日漏洞, 评估框架

TL;DR: CyberGym是一种大规模、高质量的网络安全评估框架，用于评估大型语言模型（LLM）代理在生成漏洞复现的概念证明（PoC）测试中的能力。研究发现现有代理框架和LLM在此任务上的成功率较低，仅为11.9%。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有评估基准在网络安全领域的覆盖范围和真实性不足，无法全面评估LLM代理的能力，亟需一种更有效的评估方法。

Method: 开发了CyberGym框架，包含1,507个真实漏洞和188个大型软件项目，专注于基于文本描述和源代码生成PoC测试的任务。

Result: 评估显示，最佳组合（OpenHands和Claude-3.7-Sonnet）的漏洞复现成功率为11.9%，且主要集中在简单案例上。此外，LLM代理生成的PoC还发现了15个影响最新软件版本的零日漏洞。

Conclusion: CyberGym为LLM代理在网络安全任务中的能力评估提供了有效工具，揭示了现有技术的局限性及其在发现新漏洞方面的潜力。

Abstract: Large language model (LLM) agents are becoming increasingly skilled at
handling cybersecurity tasks autonomously. Thoroughly assessing their
cybersecurity capabilities is critical and urgent, given the high stakes in
this domain. However, existing benchmarks fall short, often failing to capture
real-world scenarios or being limited in scope. To address this gap, we
introduce CyberGym, a large-scale and high-quality cybersecurity evaluation
framework featuring 1,507 real-world vulnerabilities found and patched across
188 large software projects. While it includes tasks of various settings,
CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests
for vulnerability reproduction, based on text descriptions and corresponding
source repositories. Solving this task is particularly challenging, as it
requires comprehensive reasoning across entire codebases to locate relevant
code fragments and produce effective PoCs that accurately trigger the target
vulnerability starting from the program's entry point. Our evaluation across 4
state-of-the-art agent frameworks and 9 LLMs reveals that even the best
combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9%
reproduction success rate, mainly on simpler cases. Beyond reproducing
historical vulnerabilities, we find that PoCs generated by LLM agents can
reveal new vulnerabilities, identifying 15 zero-days affecting the latest
versions of the software projects.

</details>


### [326] [ATAG: AI-Agent Application Threat Assessment with Attack Graphs](https://arxiv.org/abs/2506.02859)
*Parth Atulbhai Gandhi,Akansha Shukla,David Tayouri,Beni Ifland,Yuval Elovici,Rami Puzis,Asaf Shabtai*

Key words: 多智能体系统, 安全评估, 攻击图, LLM漏洞, 威胁建模

TL;DR: ATAG是一个新框架，用于系统评估基于LLM的多智能体系统的安全风险，通过扩展MulVAL并创建LVD数据库，成功建模复杂攻击场景。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 当前多智能体系统（MASs）的安全评估复杂且传统方法无法准确建模针对LLM的攻击，亟需新工具。

Method: ATAG扩展了MulVAL工具，加入自定义事实和交互规则，并建立LLM漏洞数据库（LVD）以标准化漏洞文档。

Result: ATAG成功应用于两个多智能体应用，能建模复杂攻击场景（如提示注入、信息泄露等）。

Conclusion: ATAG为理解和优先处理多智能体AI系统中的攻击路径提供了有效工具，促进威胁的主动识别与缓解。

Abstract: Evaluating the security of multi-agent systems (MASs) powered by large
language models (LLMs) is challenging, primarily because of the systems'
complex internal dynamics and the evolving nature of LLM vulnerabilities.
Traditional attack graph (AG) methods often lack the specific capabilities to
model attacks on LLMs. This paper introduces AI-agent application Threat
assessment with Attack Graphs (ATAG), a novel framework designed to
systematically analyze the security risks associated with AI-agent
applications. ATAG extends the MulVAL logic-based AG generation tool with
custom facts and interaction rules to accurately represent AI-agent topologies,
vulnerabilities, and attack scenarios. As part of this research, we also
created the LLM vulnerability database (LVD) to initiate the process of
standardizing LLM vulnerabilities documentation. To demonstrate ATAG's
efficacy, we applied it to two multi-agent applications. Our case studies
demonstrated the framework's ability to model and generate AGs for
sophisticated, multi-step attack scenarios exploiting vulnerabilities such as
prompt injection, excessive agency, sensitive information disclosure, and
insecure output handling across interconnected agents. ATAG is an important
step toward a robust methodology and toolset to help understand, visualize, and
prioritize complex attack paths in multi-agent AI systems (MAASs). It
facilitates proactive identification and mitigation of AI-agent threats in
multi-agent applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [327] [Music interpretation and emotion perception: A computational and neurophysiological investigation](https://arxiv.org/abs/2506.01982)
*Vassilis Lyberatos,Spyridon Kantarelis,Ioanna Zioga,Christina Anagnostopoulou,Giorgos Stamou,Anastasia Georgaki*

Key words: 音乐表演, 情感表达, 计算分析, 神经生理学, 即兴表演

TL;DR: 研究通过计算和神经生理学方法探讨音乐表演中情感表达与感知的影响，发现即兴和表现力强的表演具有独特声学特征和更强的情感反应，且即兴表演能带来更大的放松感。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探索音乐表演中不同表现力和即兴程度对音乐家情感表达与听众反应的影响。

Method: 结合计算音频分析和神经生理学测量，研究专业音乐家在多种任务（如固定曲目、即兴）中的表现及听众的情感反馈。

Result: 表现力和即兴表演具有独特的声学特征，引发更强情感反应；即兴表演能显著提高听众的放松感。

Conclusion: 表现力是提升情感沟通与观众参与的关键因素。

Abstract: This study investigates emotional expression and perception in music
performance using computational and neurophysiological methods. The influence
of different performance settings, such as repertoire, diatonic modal etudes,
and improvisation, as well as levels of expressiveness, on performers'
emotional communication and listeners' reactions is explored. Professional
musicians performed various tasks, and emotional annotations were provided by
both performers and the audience. Audio analysis revealed that expressive and
improvisational performances exhibited unique acoustic features, while emotion
analysis showed stronger emotional responses. Neurophysiological measurements
indicated greater relaxation in improvisational performances. This multimodal
study highlights the significance of expressivity in enhancing emotional
communication and audience engagement.

</details>


### [328] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
*Takao Fujii,Katie Seaborn,Madeleine Steeds,Jun Kato*

Key words: 对话代理, 拟人化, 自指词, 语音性别化, 交叉性

TL;DR: 研究探讨了对话代理通过人类身份线索拟人化的伦理问题，并探究了自指词和声音对代理身份感知的影响，发现声音性别化和自指词的交叉性可以模糊性别认知。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 批评者对拟人化机器的伦理问题和中立性假设提出质疑，研究旨在填补非代词自指词和语音表达对社会身份塑造的研究空白。

Method: 通过众包研究，日本参与者评估了三种ChatGPT语音和七种自指词，分析了声音性别化及自指词的交叉性效应。

Result: 研究发现声音显著性别化，同时某些自指词可以模糊性别认知，年龄和正式程度的感知与性别化相互作用。

Conclusion: 研究强调了代理身份认知的复杂性，提倡在语音代理设计中采用交叉性和文化敏感的视角。

Abstract: Conversational agents that mimic people have raised questions about the
ethics of anthropomorphizing machines with human social identity cues. Critics
have also questioned assumptions of identity neutrality in humanlike agents.
Recent work has revealed that intersectional Japanese pronouns can elicit
complex and sometimes evasive impressions of agent identity. Yet, the role of
other "neutral" non-pronominal self-referents (NPSR) and voice as a socially
expressive medium remains unexplored. In a crowdsourcing study, Japanese
participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and
Ember) using seven self-referents. We found strong evidence of voice gendering
alongside the potential of intersectional self-referents to evade gendering,
i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age
and formality intersected with gendering as per sociolinguistic theories,
especially boku and watakushi. This work provides a nuanced take on agent
identity perceptions and champions intersectional and culturally-sensitive work
on voice agents.

</details>


### [329] [Composable Building Blocks for Controllable and Transparent Interactive AI Systems](https://arxiv.org/abs/2506.02262)
*Sebe Vanbrabant,Gustavo Rovelo Ruiz,Davy Vanacken*

Key words: 可解释AI, 交互系统, 模块化设计, 可视化, 黑盒问题

TL;DR: 论文提出了一种通过模块化设计和可视化构建块来提高交互式系统可解释性的方法，以解决AI黑盒问题。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 由于AI技术越来越多地集成到交互系统中，系统的黑盒问题愈发突出。现有的XAI技术虽然能提升单个模型的透明度，但整个系统的可解释性仍不足。

Method: 通过将交互系统分解为结构化的模块（如AI模型和控制机制），并结合可视化构建块（如XAI技术），形成一个清晰的系统概览。

Result: 提出了一种基于流程的架构和原型系统，通过模块化和可视化方法实现了系统级的可解释性。

Conclusion: 该方法为人类和自动化代理（如LLMs）提供了统一的解释基础，提升了AI模型的透明度和可理解性。

Abstract: While the increased integration of AI technologies into interactive systems
enables them to solve an equally increasing number of tasks, the black box
problem of AI models continues to spread throughout the interactive system as a
whole. Explainable AI (XAI) techniques can make AI models more accessible by
employing post-hoc methods or transitioning to inherently interpretable models.
While this makes individual AI models clearer, the overarching system
architecture remains opaque. To this end, we propose an approach to represent
interactive systems as sequences of structural building blocks, such as AI
models and control mechanisms grounded in the literature. These can then be
explained through accompanying visual building blocks, such as XAI techniques.
The flow and APIs of the structural building blocks form an explicit overview
of the system. This serves as a communication basis for both humans and
automated agents like LLMs, aligning human and machine interpretability of AI
models. We discuss a selection of building blocks and concretize our flow-based
approach in an architecture and accompanying prototype interactive system.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [330] [Torsion in Persistent Homology and Neural Networks](https://arxiv.org/abs/2506.03049)
*Maria Walch*

Key words: 扭量, 拓扑数据分析, 自编码器, 整数同调, 深度学习

TL;DR: 论文探讨了扭量在结合拓扑数据分析的混合深度学习模型中的作用，揭示了基于域系数的TDA工具可能隐藏整数同调中的扭量特征，并分析了其在自编码器中的丢失、改变及未重建问题。

<details>
  <summary>Details</summary>

Main category: math.AT

Motivation: 研究旨在揭示基于域系数的拓扑数据分析工具在处理扭量特征时的局限性，并强调在深度学习模型中保留扭量信息的重要性。

Method: 通过合成数据和高维数据，评估扭量对扰动的敏感性，并在多种自编码器架构中分析其可恢复性。

Result: 研究表明，基于域系数的方法在保留扭量信息方面存在关键局限，需要新的架构或损失函数以确保扭量信息的鲁棒表示。

Conclusion: 论文强调了在深度学习模型中保留扭量信息的必要性，为未来的研究提供了方向。

Abstract: We explore the role of torsion in hybrid deep learning models that
incorporate topological data analysis, focusing on autoencoders. While most TDA
tools use field coefficients, this conceals torsional features present in
integer homology. We show that torsion can be lost during encoding, altered in
the latent space, and in many cases, not reconstructed by standard decoders.
Using both synthetic and high-dimensional data, we evaluate torsion sensitivity
to perturbations and assess its recoverability across several autoencoder
architectures. Our findings reveal key limitations of field-based approaches
and underline the need for architectures or loss terms that preserve torsional
information for robust data representation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [331] [SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction](https://arxiv.org/abs/2506.02082)
*Saurabh Agrawal,Raj Gohil,Gopal Kumar Agrawal,Vikram C M,Kushal Verma*

Key words: 语音质量评估，MOS，SALF-MOS，TTS，语音转换

TL;DR: 论文提出了一种名为SALF-MOS的新模型，用于预测语音合成的MOS分数，解决了主观评价的高成本和客观评价不准确的问题。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 语音质量评估在TTS或语音转换模型选择中至关重要，但现有客观指标不可靠，主观评价（如MOS）又耗时耗力。

Method: 使用卷积序列提取语音样本的潜在特征，构建了一个小型、端到端、高泛化性和可扩展的SALF-MOS模型。

Result: 模型在MSE、LCC、SRCC和KTAU等指标上取得了最佳性能。

Conclusion: SALF-MOS是一种高效且可靠的语音质量评估工具。

Abstract: Speech quality assessment is a critical process in selecting text-to-speech
synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can
be done using objective metrics or subjective metrics. Although there are many
objective metrics like the Perceptual Evaluation of Speech Quality (PESQ),
Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time
Objective Intelligibility (STOI) but none of them is feasible in selecting the
best model. On the other hand subjective metric like Mean Opinion Score is
highly reliable but it requires a lot of manual efforts and are time-consuming.
To counter the issues in MOS Evaluation, we have developed a novel model,
Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a
small-sized, end-to-end, highly generalized and scalable model for predicting
MOS score on a scale of 5. We use the sequences of convolutions and stack them
to get the latent features of the audio samples to get the best
state-of-the-art results based on mean squared error (MSE), Linear Concordance
Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and
Kendall Rank Correlation Coefficient (KTAU).

</details>


### [332] [LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention](https://arxiv.org/abs/2506.02083)
*Aditya Srinivas Menon,Raj Prakash Gohil,Kumud Tripathi,Pankaj Wasnik*

Key words: 说话人识别, 多语言, 解缠学习, 交叉注意力

TL;DR: 提出了一种新的解缠学习策略，通过前缀调优的交叉注意力来分离多语言环境中的说话人和语言信息，显著提高了说话人识别的准确性。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 在多语言环境中，说话人嵌入中语言信息的混杂导致说话人识别性能下降。需要解缠语言和说话人信息以提高识别精度。

Method: 采用了一种集成前缀调优交叉注意力的联合学习策略，特别适用于说话人在多种语言间切换的场景。

Result: 模型在单语和多语环境中（包括未见过的语言）表现出良好的泛化能力，显著降低了等错误率。

Conclusion: 该模型有效分离了语言信息与说话人嵌入，在多语言条件下提升了说话人识别性能。

Abstract: Speaker recognition models face challenges in multi-lingual settings due to
the entanglement of linguistic information within speaker embeddings. The
overlap between vocal traits such as accent, vocal anatomy, and a language's
phonetic structure complicates separating linguistic and speaker information.
Disentangling these components can significantly improve speaker recognition
accuracy. To this end, we propose a novel disentanglement learning strategy
that integrates joint learning through prefix-tuned cross-attention. This
approach is particularly effective when speakers switch between languages.
Experimental results show the model generalizes across monolingual and
multi-lingual settings, including unseen languages. Notably, the proposed model
improves the equal error rate across multiple datasets, highlighting its
ability to separate language information from speaker embeddings and enhance
recognition in diverse linguistic conditions.

</details>


### [333] [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/abs/2506.02085)
*Ajinkya Kulkarni,Sandipana Dowerah,Tanel Alumae,Mathew Magimai. -Doss*

Key words: 音频深度伪造,源追踪,Conformer网络,多类别N对损失,Real Emphasis,Fake Dispersion

TL;DR: 论文提出了一种新的音频源追踪系统，结合了深度学习、多类别N对损失、Conformer分类网络和集成评分嵌入融合技术，显著提升了音频源的追踪能力。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 随着AI技术的发展，音频深度伪造的逼真程度日益提高。当前研究主要集中在区分真实与伪造语音，但追踪音频源的系统同样重要。

Method: 提出了一种结合深度度量多类别N对损失、Real Emphasis和Fake Dispersion框架、Conformer分类网络以及集成评分嵌入融合的方法。

Result: 使用Frechet距离和标准指标评估，显示该方法的性能优于基线系统。

Conclusion: 该方法在音频源追踪方面表现出色，尤其是在区分真实和伪造语音模式方面具有更强的鲁棒性。

Abstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced
AI. While current research focuses on discerning real speech from spoofed
speech, tracing the source system is equally crucial. This work proposes a
novel audio source tracing system combining deep metric multi-class N-pair loss
with Real Emphasis and Fake Dispersion framework, a Conformer classification
network, and ensemble score-embedding fusion. The N-pair loss improves
discriminative ability, while Real Emphasis and Fake Dispersion enhance
robustness by focusing on differentiating real and fake speech patterns. The
Conformer network captures both global and local dependencies in the audio
signal, crucial for source tracing. The proposed ensemble score-embedding
fusion shows an optimal trade-off between in-domain and out-of-domain source
tracing scenarios. We evaluate our method using Frechet Distance and standard
metrics, demonstrating superior performance in source tracing over the baseline
system.

</details>


### [334] [Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition](https://arxiv.org/abs/2506.02059)
*Ziwei Gong,Pengyuan Shi,Kaan Donbekci,Lin Ai,Run Chen,David Sasu,Zehui Wu,Julia Hirschberg*

Key words: 语音情感识别, 低资源语言, 无监督学习, 对比学习, BYOL

TL;DR: 该论文探讨了无监督学习在低资源语言（LRLs）的语音情感识别（SER）中的应用，使用对比学习和BYOL方法显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 针对低资源语言中标注数据稀缺的问题，研究旨在通过无监督学习提升SER的跨语言泛化能力。

Method: 采用了对比学习（CL）和Bootstrap Your Own Latent（BYOL）两种自监督学习方法。

Result: 在乌尔都语、德语和孟加拉语中分别实现了10.6%、15.2%和13.9%的F1分数提升。

Conclusion: 该研究为低资源语言的SER提供了更包容、可解释且鲁棒的系统基础。

Abstract: Speech Emotion Recognition (SER) has seen significant progress with deep
learning, yet remains challenging for Low-Resource Languages (LRLs) due to the
scarcity of annotated data. In this work, we explore unsupervised learning to
improve SER in low-resource settings. Specifically, we investigate contrastive
learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised
approaches to enhance cross-lingual generalization. Our methods achieve notable
F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,
demonstrating their effectiveness in LRLs. Additionally, we analyze model
behavior to provide insights on key factors influencing performance across
languages, and also highlighting challenges in low-resource SER. This work
provides a foundation for developing more inclusive, explainable, and robust
emotion recognition systems for underrepresented languages.

</details>


### [335] [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/abs/2506.02088)
*Alef Iury Siqueira Ferreira,Lucas Rafael Gris,Alexandre Ferro Filho,Lucas Ólives,Daniel Ribeiro,Luiz Fernando,Fernanda Lustosa,Rodrigo Tanaka,Frederico Santos de Oliveira,Arlindo Galvão Filho*

Key words: SER, 自然语音, 情感识别, F0量化, Graph Attention Networks

TL;DR: 本文介绍了针对自然语音中情感识别的挑战，提出了一种结合音频和文本特征的SER系统，并在INTERSPEECH 2025挑战中取得了显著成果。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 自然语音中的情感表达微妙且不可预测，训练SER模型具有挑战性。

Method: 结合最先进的音频模型和包含韵律与频谱线索的文本特征，研究了F0量化和预训练音频标记模型的效果，并采用集成模型和Graph Attention Networks。

Result: 在官方测试集上Macro F1-score为39.79%（验证集42.20%）。

Conclusion: 所提方法展示了潜力，特别是Graph Attention Networks在融合技术中的有效性。

Abstract: Training SER models in natural, spontaneous speech is especially challenging
due to the subtle expression of emotions and the unpredictable nature of
real-world audio. In this paper, we present a robust system for the INTERSPEECH
2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing
on categorical emotion recognition. Our method combines state-of-the-art audio
models with text features enriched by prosodic and spectral cues. In
particular, we investigate the effectiveness of Fundamental Frequency (F0)
quantization and the use of a pretrained audio tagging model. We also employ an
ensemble model to improve robustness. On the official test set, our system
achieved a Macro F1-score of 39.79% (42.20% on validation). Our results
underscore the potential of these methods, and analysis of fusion techniques
confirmed the effectiveness of Graph Attention Networks. Our source code is
publicly available.

</details>


### [336] [Cocktail-Party Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.02178)
*Thai-Binh Nguyen,Ngoc-Quan Pham,Alexander Waibel*

Key words: AVSR, 鸡尾酒会问题, 数据集, 噪声环境, WER

TL;DR: 论文提出了一种新型音频-视觉鸡尾酒会数据集，解决了现有AVSR模型在真实噪声环境中的局限性，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有AVSR模型在理想化场景中表现良好，但在真实噪声环境下（如鸡尾酒会）效果不佳，尤其是对说话和静默面部片段的处理不足。

Method: 研究引入了一个包含说话和静默面部片段的1526小时AVSR数据集，并在极端噪声环境中测试。

Result: 新方法在极端噪声环境下将WER从119%降低至39.2%，相对减少了67%的错误率。

Conclusion: 该研究通过真实噪声场景的数据集设计，显著提升了AVSR系统的鲁棒性和性能。

Abstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech
recognition in challenging environments, such as cocktail-party scenarios,
where relying solely on audio proves insufficient. However, current AVSR models
are often optimized for idealized scenarios with consistently active speakers,
overlooking the complexities of real-world settings that include both speaking
and silent facial segments. This study addresses this gap by introducing a
novel audio-visual cocktail-party dataset designed to benchmark current AVSR
systems and highlight the limitations of prior approaches in realistic noisy
conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising
both talking-face and silent-face segments, enabling significant performance
gains in cocktail-party environments. Our approach reduces WER by 67% relative
to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,
without relying on explicit segmentation cues.

</details>


### [337] [Synthetic Speech Source Tracing using Metric Learning](https://arxiv.org/abs/2506.02590)
*Dimitrios Koutsianos,Stavros Zacharopoulos,Yannis Panagakis,Themos Stafylakis*

Key words: 合成语音追踪，说话人识别，ResNet，自监督学习，度量学习

TL;DR: 论文探讨了通过说话人识别技术追溯合成语音来源的方法，比较了基于分类和度量学习的方法。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有研究主要关注伪造检测，缺乏对合成音频来源追踪的鲁棒解决方案。

Method: 采用基于分类和度量学习的两种方法，并在MLAADv5基准上使用ResNet和自监督学习骨干网络进行测试。

Result: 结果显示，ResNet在度量学习方法中表现优异，甚至超越自监督学习系统。

Conclusion: 研究表明ResNet可用于来源追踪，但需优化自监督学习的表征以适配此任务。

Abstract: This paper addresses source tracing in synthetic speech-identifying
generative systems behind manipulated audio via speaker recognition-inspired
pipelines. While prior work focuses on spoofing detection, source tracing lacks
robust solutions. We evaluate two approaches: classification-based and
metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet
and self-supervised learning (SSL) backbones. The results show that ResNet
achieves competitive performance with the metric learning approach, matching
and even exceeding SSL-based systems. Our work demonstrates ResNet's viability
for source tracing while underscoring the need to optimize SSL representations
for this task. Our work bridges speaker recognition methodologies with audio
forensic challenges, offering new directions for combating synthetic media
manipulation.

</details>


### [338] [Comparison of spectrogram scaling in multi-label Music Genre Recognition](https://arxiv.org/abs/2506.02091)
*Bartosz Karpiński,Cyryl Leszczyński*

Key words: 数字音频工作站, 音乐风格分类, 预处理方法, 模型训练, 手动标注数据集

TL;DR: 论文探讨了随着数字音频工作站的普及，音乐风格界限模糊的问题，并比较了多种预处理方法和模型训练方法，使用了18000个手动标注的数据集进行实验。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 随着数字音频工作站的易用性提升，音乐风格界限变得模糊，导致音乐分类困难，需要有效的预处理和模型训练方法。

Method: 使用多种预处理方法和模型训练方法，并利用一个手动标注的超过18000条记录的数据集进行实验。

Result: 通过实验比较了不同方法的性能，为处理多样化的音乐风格提供了参考。

Conclusion: 论文为音乐风格分类提供了一种基于大规模手动标注数据集的解决方案。

Abstract: As the accessibility and ease-of-use of digital audio workstations increases,
so does the quantity of music available to the average listener; additionally,
differences between genres are not always well defined and can be abstract,
with widely varying combinations of genres across individual records. In this
article, multiple preprocessing methods and approaches to model training are
described and compared, accounting for the eclectic nature of today's albums. A
custom, manually labeled dataset of more than 18000 entries has been used to
perform the experiments.

</details>


### [339] [Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm](https://arxiv.org/abs/2506.02610)
*Zhaoyang Li,Jie Wang,XiaoXiao Li,Wangjie Li,Longjie Luo,Lin Li,Qingyang Hong*

Key words: 说话人日志, 图注意力网络, 标签传播算法, 重叠社区检测, DIHARD-III

TL;DR: 该论文提出了一种基于图注意力网络和标签传播算法的重叠社区检测方法（OCDGALP），用于解决说话人日志任务中传统聚类方法难以处理复杂说话人嵌入分布和重叠语音段的局限性。实验结果显示，该方法显著降低了错误率。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 传统说话人日志的聚类方法难以处理复杂说话人嵌入分布和重叠语音段的问题，因此需要一种更高效的方法。

Method: 结合图注意力网络和标签传播算法，通过聚合邻居节点信息优化说话人嵌入和节点连接，并支持多社区标签分配的聚类检测。

Result: 在DIHARD-III数据集上取得了15.94%的错误率（无VAD）和11.07%的错误率（有VAD），表现优异。

Conclusion: OCDGALP方法在说话人日志任务中显著提升了性能，适用于复杂场景。

Abstract: In speaker diarization, traditional clustering-based methods remain widely
used in real-world applications. However, these methods struggle with the
complex distribution of speaker embeddings and overlapping speech segments. To
address these limitations, we propose an Overlapping Community Detection method
based on Graph Attention networks and the Label Propagation Algorithm
(OCDGALP). The proposed framework comprises two key components: (1) a graph
attention network that refines speaker embeddings and node connections by
aggregating information from neighboring nodes, and (2) a label propagation
algorithm that assigns multiple community labels to each node, enabling
simultaneous clustering and overlapping community detection. Experimental
results show that the proposed method significantly reduces the Diarization
Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III
dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%
with oracle VAD.

</details>


### [340] [DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization](https://arxiv.org/abs/2506.02858)
*Geonyoung Lee,Geonhee Han,Paul Hongsuck Seo*

Key words: 零样本学习,音频分离,扩散模型,自然语言查询

TL;DR: 无需额外训练的零样本语言查询音频分离方法，利用生成先验实现高效分离。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 探索预训练扩散模型是否天生具备音频分离能力，避免任务特定训练。

Method: 提出扩散引导掩码优化（DGMO）框架，通过测试时间优化频谱掩码实现精确分离。

Result: 无需监督，表现优异，扩展了扩散模型的应用范围。

Conclusion: 扩散模型可用于零样本音频分离，开辟了新范式。

Abstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound
separation via natural language queries. While existing methods rely on
task-specific training, we explore whether pretrained diffusion models,
originally designed for audio generation, can inherently perform separation
without further training. In this study, we introduce a training-free framework
leveraging generative priors for zero-shot LASS. Analyzing na\"ive adaptations,
we identify key limitations arising from modality-specific challenges.To
address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a
test-time optimization framework that refines spectrogram masks for precise,
input-aligned separation. Our approach effectively repurposes pretrained
diffusion models for source separation, achieving competitive performance
without task-specific supervision. This work expands the application of
diffusion models beyond generation, establishing a new paradigm for zero-shot
audio separation. The code is available at: https://wltschmrz.github.io/DGMO/

</details>


### [341] [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](https://arxiv.org/abs/2506.03099)
*Chetwin Low,Weimin Wang*

Key words: 视频生成,音频驱动,实时动画,大语言模型,知识蒸馏

TL;DR: TalkingMachines是一个高效框架，将预训练视频生成模型转化为实时音频驱动角色动画器，结合音频LLM和视频生成模型，提供自然对话体验。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 旨在实现实时、自然的音频驱动角色动画，提升对话交互的体验。

Method: 1. 将预训练SOTA图像到视频DiT模型适配为音频驱动角色生成模型；2. 通过非对称知识蒸馏实现无限视频流；3. 设计高性能低延迟推理流水线。

Result: 模型参数达180亿，支持无限视频流生成，优化了推理效率。

Conclusion: TalkingMachines展示了高效音频驱动角色动画的潜力，具有实际应用价值。

Abstract: In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a sparse causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [342] [No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction](https://arxiv.org/abs/2506.02039)
*Haoshuai Zhou,Changgeng Mo,Boxuan Cao,Linkai Li,Shan Xiang Wang*

Key words: 个性化语音预测、SSIPNet、深度学习方法、支持样本、听力图

TL;DR: 提出了一种基于支持样本的深度学习方法（SSIPNet），用于个性化语音清晰度预测，通过利用现有用户的语音识别数据，超越了传统的听力图方法。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 传统的听力图方法仅捕捉纯音的听力阈值，准确性有限，本文旨在通过利用用户的语音识别数据来提升预测效果。

Method: 提出支持样本智能预测网络（SSIPNet），利用语音基础模型从多个支持（音频、分数）对中构建高维表示，预测新音频的性能。

Result: 在Clarity Prediction Challenge数据集上，即使支持样本数量较少，SSIPNet的表现也优于基于听力图的方法。

Conclusion: 提出了一种个性化语音清晰度预测的新范式，展示了利用现有数据的优势。

Abstract: Personalized speech intelligibility prediction is challenging. Previous
approaches have mainly relied on audiograms, which are inherently limited in
accuracy as they only capture a listener's hearing threshold for pure tones.
Rather than incorporating additional listener features, we propose a novel
approach that leverages an individual's existing intelligibility data to
predict their performance on new audio. We introduce the Support Sample-Based
Intelligibility Prediction Network (SSIPNet), a deep learning model that
leverages speech foundation models to build a high-dimensional representation
of a listener's speech recognition ability from multiple support (audio, score)
pairs, enabling accurate predictions for unseen audio. Results on the Clarity
Prediction Challenge dataset show that, even with a small number of support
(audio, score) pairs, our method outperforms audiogram-based predictions. Our
work presents a new paradigm for personalized speech intelligibility
prediction.

</details>


### [343] [Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data](https://arxiv.org/abs/2506.02078)
*Emmy Postma,Cristian Tejedor-Garcia*

Key words: 帕金森病, 语音分析, 预训练模型, 性别偏见, 特征提取

TL;DR: 研究探讨了三种预训练音频嵌入模型在帕金森病分类中的效果，发现OpenL3在特定任务中表现最佳，而Wav2Vec2.0存在性别偏见。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 语音障碍是帕金森病的常见生物标志物，但现有研究未充分探讨个体差异对深度学习模型性能的影响。

Method: 使用OpenL3、VGGish和Wav2Vec2.0三种预训练模型，基于NeuroVoz数据集进行分类任务。

Result: OpenL3在DDK和LR任务中表现最佳，Wav2Vec2.0在DDK任务中对男性表现更好。

Conclusion: 研究揭示了模型对非典型语音模式的挑战，强调需改进特征提取和模型鲁棒性。

Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),
motivating the development of diagnostic techniques using speech data for
clinical applications. Although deep acoustic features have shown promise for
PD classification, their effectiveness often varies due to individual speaker
differences, a factor that has not been thoroughly explored in the existing
literature. This study investigates the effectiveness of three pre-trained
audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.
Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)
and listen and repeat (LR) tasks, capturing critical acoustic features for PD
detection. Only Wav2Vec2.0 shows significant gender bias, achieving more
favorable results for male speakers, in DDK tasks. The misclassified cases
reveal challenges with atypical speech patterns, highlighting the need for
improved feature extraction and model robustness in PD detection.

</details>


### [344] [Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge](https://arxiv.org/abs/2506.02080)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Key words: CAPT, GOP, 无对齐方法, 音素替换, L2英语发音

TL;DR: 摘要介绍了一种替代感知的无对齐GOP方法，通过限制音素替换来提高效率，并在两个L2英语语音数据集中优于基线方法。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 传统的GOP依赖于强制对齐，容易因声学变异性导致标记和分割错误，而无对齐方法虽然解决了这些问题，但计算成本高且扩展性差。

Method: 提出了一种基于音素集群和常见学习者错误的替代感知无对齐GOP方法，限制音素替换以提高效率。

Result: 在两个L2英语语音数据集（MPC和SpeechOcean762）中的实验表明，该方法在无对齐方法中优于基线。

Conclusion: 研究结果表明，替代感知无对齐GOP是有效的，并探讨了未来研究方向。

Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic
measures of pronunciation quality, such as the goodness of pronunciation (GOP)
metric. GOP relies on forced alignments, which are prone to labeling and
segmentation errors due to acoustic variability. While alignment-free methods
address these challenges, they are computationally expensive and scale poorly
with phoneme sequence length and inventory size. To enhance efficiency, we
introduce a substitution-aware alignment-free GOP that restricts phoneme
substitutions based on phoneme clusters and common learner errors. We evaluated
our GOP on two L2 English speech datasets, one with child speech, My
Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult
speech. We compared RPS (restricted phoneme substitutions) and UPS
(unrestricted phoneme substitutions) setups within alignment-free methods,
which outperformed the baseline. We discuss our results and outline avenues for
future research.

</details>


### [345] [Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi](https://arxiv.org/abs/2506.02166)
*Arnav Rustagi,Satvik Bajpai,Nimrat Kaur,Siddharth Siddharth*

Key words: CAPT, 印地语, 发音训练, 合成语音, 个性化反馈

TL;DR: 论文提出了一种针对印地语的新CAPT系统Dhvani，填补了印度语言发音训练的空白，包括合成语音生成和个性化反馈方法。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 针对印度语言（尤其是印地语）发音训练工具的缺乏，尽管其使用者众多，亟需有效的CAPT解决方案。

Method: 开发了Dhvani系统，利用印地语的语音特性生成合成语音并提供个性化反馈。

Result: Dhvani系统通过语音分析提供针对性反馈，提升了印地语发音训练的效果。

Conclusion: Dhvani系统为印地语学习者提供了有效的发音训练工具，填补了市场空白。

Abstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied
for English. However, there remains a critical gap in its application to Indian
languages with a base of 1.5 billion speakers. Pronunciation tools tailored to
Indian languages are strikingly lacking despite the fact that millions learn
them every year. With over 600 million speakers and being the fourth
most-spoken language worldwide, improving Hindi pronunciation is a vital first
step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT
system for Hindi, 2) synthetic speech generation for Hindi mispronunciations,
and 3) a novel methodology for providing personalized feedback to learners.
While the system often interacts with learners using Devanagari graphemes, its
core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic
orthography to analyze mispronounced speech and provide targeted feedback.

</details>


### [346] [Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions](https://arxiv.org/abs/2506.02742)
*Xiaoxue Gao,Huayun Zhang,Nancy F. Chen*

Key words: 情感语音合成，TTS，提示学习，零样本学习，LLM

TL;DR: 本文提出了一种新的方法（PUE），通过情感引导提示学习生成未见过的情感语音，填补了现有TTS系统在情感多样性上的不足。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 现有的TTS系统仅能模拟有限的情感类别，而人类对话涉及更复杂的情感，因此需要探索更自然的情感语音生成方法。

Method: 提出PUE方法，基于LLM-TTS架构，利用情感引导提示学习，定量捕获情感权重，并通过灵活调整情感比例生成混合情感语音。

Result: PUE在零样本设置下成功生成了未见情感的语音，实现了情感多样性。

Conclusion: PUE为自然交互中的情感语音合成提供了有效解决方案。

Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited
set of categorical emotions, whereas human conversations extend far beyond
these predefined emotions, making it essential to explore more diverse
emotional speech generation for more natural interactions. To bridge this gap,
this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate
unseen emotional speech via emotion-guided prompt learning. PUE is trained
utilizing an LLM-TTS architecture to ensure emotional consistency between
categorical emotion-relevant prompts and emotional speech, allowing the model
to quantitatively capture different emotion weightings per utterance. During
inference, mixed emotional speech can be generated by flexibly adjusting
emotion proportions and leveraging LLM contextual knowledge, enabling the model
to quantify different emotional styles. Our proposed PUE successfully
facilitates expressive speech synthesis of unseen emotions in a zero-shot
setting.

</details>


### [347] [CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech](https://arxiv.org/abs/2506.02863)
*Helin Wang,Jiarui Hai,Dading Chong,Karan Thakkar,Tiantian Feng,Dongchao Yang,Junhyeok Lee,Laureano Moro Velazquez,Jesus Villalba,Zengyi Qin,Shrikanth Narayanan,Mounya Elhiali,Najim Dehak*

Key words: 生成人工智能, 风格注释文本到语音, CapSpeech, 数据集, 自回归模型

TL;DR: CapSpeech是一个新的基准数据集，用于支持风格注释的文本到语音合成（CapTTS）任务，填补了标准化数据集和下游任务研究的空白。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 由于缺乏标准化的综合数据集和对CapTTS下游任务的有限研究，设计CapSpeech以解决这些挑战。

Method: 引入了CapSpeech数据集，包含机器和人工标注的音频-字幕对，以及针对特定任务的专业录制数据。使用了自回归和非自回归模型进行实验。

Result: 实验结果显示，在多样化的说话风格中实现了高保真和高可理解性的语音合成。CapSpeech是目前最大的CapTTS相关任务数据集。

Conclusion: CapSpeech为CapTTS系统的开发提供了宝贵的数据支持和研究洞察。

Abstract: Recent advancements in generative artificial intelligence have significantly
transformed the field of style-captioned text-to-speech synthesis (CapTTS).
However, adapting CapTTS to real-world applications remains challenging due to
the lack of standardized, comprehensive datasets and limited research on
downstream tasks built upon CapTTS. To address these gaps, we introduce
CapSpeech, a new benchmark designed for a series of CapTTS-related tasks,
including style-captioned text-to-speech synthesis with sound events
(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS
(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech
comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36
million human-annotated audio-caption pairs. In addition, we introduce two new
datasets collected and recorded by a professional voice actor and experienced
audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside
the datasets, we conduct comprehensive experiments using both autoregressive
and non-autoregressive models on CapSpeech. Our results demonstrate
high-fidelity and highly intelligible speech synthesis across a diverse range
of speaking styles. To the best of our knowledge, CapSpeech is the largest
available dataset offering comprehensive annotations for CapTTS-related tasks.
The experiments and findings further provide valuable insights into the
challenges of developing CapTTS systems.

</details>


### [348] [Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency](https://arxiv.org/abs/2506.02908)
*Bunlong Lay,Rostilav Makarov,Timo Gerkmann*

Key words: 扩散模型,语音增强,滑动窗口,实时处理,GPU

TL;DR: 提出了一种基于滑动窗口的扩散模型方法，用于语音增强，平衡了性能和延迟，适用于实时处理。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 扩散模型在语音增强中表现优异，但推理阶段计算成本高，难以实时处理流数据。

Method: 采用滑动窗口扩散框架，随时间逐步对语音信号加噪，对当前帧赋予更多噪声，并通过缓冲区大小权衡延迟与性能。

Result: 该方法优于标准扩散模型，GPU上运行时输入输出延迟为0.3至1秒，首次实现实用的在线语音增强。

Conclusion: 滑动窗口扩散模型是首个适用于实时语音增强的实用解决方案。

Abstract: Diffusion models are a class of generative models that have been recently
used for speech enhancement with remarkable success but are computationally
expensive at inference time. Therefore, these models are impractical for
processing streaming data in real-time. In this work, we adapt a sliding window
diffusion framework to the speech enhancement task. Our approach progressively
corrupts speech signals through time, assigning more noise to frames close to
the present in a buffer. This approach outputs denoised frames with a delay
proportional to the chosen buffer size, enabling a trade-off between
performance and latency. Empirical results demonstrate that our method
outperforms standard diffusion models and runs efficiently on a GPU, achieving
an input-output latency in the order of 0.3 to 1 seconds. This marks the first
practical diffusion-based solution for online speech enhancement.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [349] [Enriching Location Representation with Detailed Semantic Information](https://arxiv.org/abs/2506.02744)
*Junyuan Liu,Xinglei Wang,Tao Cheng*

Key words: 空间表征,城市建模,POI,多模态对比学习

TL;DR: 论文提出了CaLLiPer+模型，通过结合POI名称和类别标签的多模态对比学习框架，提升了城市建模中空间表征的性能。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 传统空间嵌入方法过于依赖空间邻近性，忽略了细粒度的上下文信息，因此需要更有效的表征方法。

Method: 扩展CaLLiPer模型，引入POI名称和类别标签的多模态对比学习框架。

Result: 在两个下游任务中表现优于基线方法4%-11%，且POI名称增强了位置检索能力。

Conclusion: 结合细粒度语义属性和多模态学习技术有助于发展城市基础模型。

Abstract: Spatial representations that capture both structural and semantic
characteristics of urban environments are essential for urban modeling.
Traditional spatial embeddings often prioritize spatial proximity while
underutilizing fine-grained contextual information from places. To address this
limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that
systematically integrates Point-of-Interest (POI) names alongside categorical
labels within a multimodal contrastive learning framework. We evaluate its
effectiveness on two downstream tasks, land use classification and
socioeconomic status distribution mapping, demonstrating consistent performance
gains of 4% to 11% over baseline methods. Additionally, we show that
incorporating POI names enhances location retrieval, enabling models to capture
complex urban concepts with greater precision. Ablation studies further reveal
the complementary role of POI names and the advantages of leveraging pretrained
text encoders for spatial representations. Overall, our findings highlight the
potential of integrating fine-grained semantic attributes and multimodal
learning techniques to advance the development of urban foundation models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [350] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)
*Wang Mengjie,Zhu Huiping,Li Jian,Shi Wenxiu,Zhang Song*

Key words: 自动驾驶,多模态模型,动态提示优化,数据集构建,知识蒸馏

TL;DR: 论文提出了一种综合优化多模态模型在驾驶场景中的方法，涵盖动态提示优化、数据集构建、模型训练和部署，显著提升了任务准确性和资源效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着自动驾驶技术的发展，需要更好地理解复杂驾驶场景，但多模态大模型的应用存在数据收集、训练和部署等难点。

Method: 方法包括动态提示优化以聚焦关键对象，结合真实与合成数据构建多模态数据集，整合知识蒸馏、动态微调和量化等训练技术。

Result: 实验表明该方法显著提高了关键任务的准确性，并实现了高效的资源利用。

Conclusion: 该方法为驾驶场景感知技术的实际应用提供了有力支持。

Abstract: With the advancement of autonomous and assisted driving technologies, higher
demands are placed on the ability to understand complex driving scenarios.
Multimodal general large models have emerged as a solution for this challenge.
However, applying these models in vertical domains involves difficulties such
as data collection, model training, and deployment optimization. This paper
proposes a comprehensive method for optimizing multimodal models in driving
scenarios, including cone detection, traffic light recognition, speed limit
recommendation, and intersection alerts. The method covers key aspects such as
dynamic prompt optimization, dataset construction, model training, and
deployment. Specifically, the dynamic prompt optimization adjusts the prompts
based on the input image content to focus on objects affecting the ego vehicle,
enhancing the model's task-specific focus and judgment capabilities. The
dataset is constructed by combining real and synthetic data to create a
high-quality and diverse multimodal training dataset, improving the model's
generalization in complex driving environments. In model training, advanced
techniques like knowledge distillation, dynamic fine-tuning, and quantization
are integrated to reduce storage and computational costs while boosting
performance. Experimental results show that this systematic optimization method
not only significantly improves the model's accuracy in key tasks but also
achieves efficient resource utilization, providing strong support for the
practical application of driving scenario perception technologies.

</details>


### [351] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/abs/2506.02021)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Yew-Soon Ong,Joey Tianyi Zhou*

Key words: 数据集蒸馏，动态感知，强化学习，视频语义，时间分辨率

TL;DR: 该论文提出了一种名为DAViD的动态感知视频数据集蒸馏方法，通过强化学习预测最优时间分辨率，解决了现有方法在视频数据集上的不足，表现显著优于现有技术。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着视觉任务和数据集的快速发展，数据冗余减少成为研究重点。视频数据集因其独特的时间信息和冗余差异，现有方法未能有效处理。

Method: 使用强化学习（RL）预测合成视频的最优时间分辨率，并提出教师循环奖励函数更新RL策略。

Result: DAViD显著优于现有数据集蒸馏方法，性能大幅提升。

Conclusion: 该研究为未来更高效、语义自适应的视频数据集蒸馏研究铺平了道路。

Abstract: With the rapid development of vision tasks and the scaling on datasets and
models, redundancy reduction in vision datasets has become a key area of
research. To address this issue, dataset distillation (DD) has emerged as a
promising approach to generating highly compact synthetic datasets with
significantly less redundancy while preserving essential information. However,
while DD has been extensively studied for image datasets, DD on video datasets
remains underexplored. Video datasets present unique challenges due to the
presence of temporal information and varying levels of redundancy across
different classes. Existing DD approaches assume a uniform level of temporal
redundancy across all different video semantics, which limits their
effectiveness on video datasets. In this work, we propose Dynamic-Aware Video
Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the
optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop
reward function is proposed to update the RL agent policy. To the best of our
knowledge, this is the first study to introduce adaptive temporal resolution
based on video semantics in video dataset distillation. Our approach
significantly outperforms existing DD methods, demonstrating substantial
improvements in performance. This work paves the way for future research on
more efficient and semantic-adaptive video dataset distillation research.

</details>


### [352] [Implicit Deformable Medical Image Registration with Learnable Kernels](https://arxiv.org/abs/2506.02150)
*Stefano Fogarollo,Gregor Laimer,Reto Bale,Matthias Harders*

Key words: 医学图像配准、隐式配准、稀疏关键点、位移场、肿瘤治疗

TL;DR: 提出了一种新型隐式医学图像配准框架，通过稀疏关键点对应关系预测精确且可靠的变形场，具有临床潜力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 医学图像配准在计算机辅助干预中至关重要，特别是在肿瘤治疗中需要精确的图像对齐。当前AI方法虽然速度快、精度高，但变形不可靠，阻碍了临床应用。

Method: 将图像配准重新定义为信号重建问题，通过学习核函数从稀疏关键点对应关系中恢复密集位移场。采用分层架构进行由粗到细的位移场估计，并允许在测试时高效调整。

Result: 在胸部和腹部的零样本配准任务中表现优越，不仅精度与最先进方法相当，还能更好地保持解剖结构关系，与专业商业系统性能匹配。

Conclusion: 该方法填补了隐式和显式配准技术之间的泛化差距，生成的变形更可靠，具有较高的临床推广潜力。

Abstract: Deformable medical image registration is an essential task in
computer-assisted interventions. This problem is particularly relevant to
oncological treatments, where precise image alignment is necessary for tracking
tumor growth, assessing treatment response, and ensuring accurate delivery of
therapies. Recent AI methods can outperform traditional techniques in accuracy
and speed, yet they often produce unreliable deformations that limit their
clinical adoption. In this work, we address this challenge and introduce a
novel implicit registration framework that can predict accurate and reliable
deformations. Our insight is to reformulate image registration as a signal
reconstruction problem: we learn a kernel function that can recover the dense
displacement field from sparse keypoint correspondences. We integrate our
method in a novel hierarchical architecture, and estimate the displacement
field in a coarse-to-fine manner. Our formulation also allows for efficient
refinement at test time, permitting clinicians to easily adjust registrations
when needed. We validate our method on challenging intra-patient thoracic and
abdominal zero-shot registration tasks, using public and internal datasets from
the local University Hospital. Our method not only shows competitive accuracy
to state-of-the-art approaches, but also bridges the generalization gap between
implicit and explicit registration techniques. In particular, our method
generates deformations that better preserve anatomical relationships and
matches the performance of specialized commercial systems, underscoring its
potential for clinical adoption.

</details>


### [353] [Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos](https://arxiv.org/abs/2506.02167)
*Aditi Tiwari,Farzaneh Masoud,Dac Trong Nguyen,Jill Kraft,Heng Ji,Klara Nahrstedt*

Key words: Fire360, 消防场景, 感知推理, 数据集, 安全关键场景

TL;DR: Fire360是一个用于评估消防场景中感知与推理能力的基准数据集，包含228个360度视频，支持5项任务，旨在提升AI在安全关键场景中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现代AI系统在可靠性要求高的环境中表现不佳，如消防场景中有烟雾、能见度低和结构变形的情况，导致消防员受伤。Fire360旨在提升AI在这些场景中的感知和推理能力。

Method: 数据集包括228个360度视频，来自专业消防训练，涵盖多样条件（如低光、热变形），并标注了动作片段、物体位置和退化元数据。支持视觉问答、时序动作描述等5项任务。

Result: 人类专家在Transformed Object Retrieval任务中达到83.5%准确率，而GPT-4o等模型表现显著落后，显示在退化条件下的推理缺陷。

Conclusion: 通过发布Fire360及其评估工具，旨在推动AI在不确定性条件下的感知、记忆、推理与行动能力。

Abstract: Modern AI systems struggle most in environments where reliability is critical
- scenes with smoke, poor visibility, and structural deformation. Each year,
tens of thousands of firefighters are injured on duty, often due to breakdowns
in situational perception. We introduce Fire360, a benchmark for evaluating
perception and reasoning in safety-critical firefighting scenarios. The dataset
includes 228 360-degree videos from professional training sessions under
diverse conditions (e.g., low light, thermal distortion), annotated with action
segments, object locations, and degradation metadata. Fire360 supports five
tasks: Visual Question Answering, Temporal Action Captioning, Object
Localization, Safety-Critical Reasoning, and Transformed Object Retrieval
(TOR). TOR tests whether models can match pristine exemplars to fire-damaged
counterparts in unpaired scenes, evaluating transformation-invariant
recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag
significantly, exposing failures in reasoning under degradation. By releasing
Fire360 and its evaluation suite, we aim to advance models that not only see,
but also remember, reason, and act under uncertainty. The dataset is available
at: https://uofi.box.com/v/fire360dataset.

</details>


### [354] [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
*Manas Mehta,Yimu Pan,Kelly Gallagher,Alison D. Gernand,Jeffery A. Goldstein,Delia Mwinyelle,Leena Mithal,James Z. Wang*

Key words: 胎盘病理检查、知识蒸馏、视觉-语言对比学习、无监督预蒸馏、AI医疗

TL;DR: 通过改进视觉-语言对比学习框架，提出两种方法提升胎盘病理检测的效率和准确性：基于文本锚定的知识蒸馏和利用自然图像数据集的无监督预蒸馏。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 胎盘病理检查是检测分娩相关健康风险的有效方法，但现有自动化方法计算量大，限制了其实际应用。

Method: 提出文本锚定的视觉-语言对比知识蒸馏（VLCD）和无监督预蒸馏，以提升模型效率和性能。

Result: 实现了模型压缩和加速，性能匹配或超越教师模型，尤其提升了对低质量图像的鲁棒性。

Conclusion: VLCD提高了医疗视觉-语言对比学习方法的效率和可部署性，尤其适用于资源受限环境。

Abstract: Pathological examination of the placenta is an effective method for detecting
and mitigating health risks associated with childbirth. Recent advancements in
AI have enabled the use of photographs of the placenta and pathology reports
for detecting and classifying signs of childbirth-related pathologies. However,
existing automated methods are computationally extensive, which limits their
deployability. We propose two modifications to vision-language contrastive
learning (VLC) frameworks to enhance their accuracy and efficiency: (1)
text-anchored vision-language contrastive knowledge distillation (VLCD)-a new
knowledge distillation strategy for medical VLC pretraining, and (2)
unsupervised predistillation using a large natural images dataset for improved
initialization. Our approach distills efficient neural networks that match or
surpass the teacher model in performance while achieving model compression and
acceleration. Our results showcase the value of unsupervised predistillation in
improving the performance and robustness of our approach, specifically for
lower-quality images. VLCD serves as an effective way to improve the efficiency
and deployability of medical VLC approaches, making AI-based healthcare
solutions more accessible, especially in resource-constrained environments.

</details>


### [355] [Motion aware video generative model](https://arxiv.org/abs/2506.02244)
*Bowen Xue,Giuseppe Claudio Guarnera,Shuang Zhao,Zahra Montazeri*

Key words: 视频生成, 扩散模型, 频域分析, 物理合理性, 深度学习

TL;DR: 本文提出了一种基于物理信息的频域方法，用于提升扩散模型生成视频的物理合理性，通过频域分析和优化运动模式来减少非物理伪影。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前基于扩散模型的视频生成方法依赖大数据统计学习，缺乏对运动物理的显式建模，导致生成的视频存在非物理伪影，影响了真实感。

Method: 提出两个互补组件：1) 一种物理运动损失函数，优化视频运动模式在频域的合理性；2) 频域增强模块，通过零初始化策略调整视频特征以符合物理约束。

Result: 实验表明，该方法显著提升了运动质量和物理合理性，同时保持了视觉质量和语义一致性，适用于多种视频生成架构。

Conclusion: 该工作建立了数据驱动模型与基于物理运动模型的联系，为深度学习中引入物理约束提供了理论支持。

Abstract: Recent advances in diffusion-based video generation have yielded
unprecedented quality in visual content and semantic coherence. However,
current approaches predominantly rely on statistical learning from vast
datasets without explicitly modeling the underlying physics of motion,
resulting in subtle yet perceptible non-physical artifacts that diminish the
realism of generated videos. This paper introduces a physics-informed frequency
domain approach to enhance the physical plausibility of generated videos. We
first conduct a systematic analysis of the frequency-domain characteristics of
diverse physical motions (translation, rotation, scaling), revealing that each
motion type exhibits distinctive and identifiable spectral signatures. Building
on this theoretical foundation, we propose two complementary components: (1) a
physical motion loss function that quantifies and optimizes the conformity of
generated videos to ideal frequency-domain motion patterns, and (2) a frequency
domain enhancement module that progressively learns to adjust video features to
conform to physical motion constraints while preserving original network
functionality through a zero-initialization strategy. Experiments across
multiple video diffusion architectures demonstrate that our approach
significantly enhances motion quality and physical plausibility without
compromising visual quality or semantic alignment. Our frequency-domain
physical motion framework generalizes effectively across different video
generation architectures, offering a principled approach to incorporating
physical constraints into deep learning-based video synthesis pipelines. This
work seeks to establish connections between data-driven models and
physics-based motion models.

</details>


### [356] [QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](https://arxiv.org/abs/2506.02295)
*Ahmed Wasfy,Omer Nacar,Abdelakreem Elkhateb,Mahmoud Reda,Omar Elshehy,Adel Ammar,Wadii Boulila*

Key words: 阿拉伯语 OCR, 视觉语言模型, 迭代微调, 词错误率, 字符错误率

TL;DR: Qari-OCR 是一种针对阿拉伯语优化的视觉语言模型，通过迭代微调在合成数据集上显著提升了光学字符识别（OCR）的准确性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 阿拉伯语书写的复杂性（如连笔、音标符号和多变的字体）给 OCR 带来了持续挑战。研究旨在通过优化模型来解决这些问题。

Method: 基于 Qwen2-VL-2B-Instruct 模型，通过迭代微调在合成数据集上优化，开发了 Qari-OCR 系列模型。

Result: QARI v0.2 模型在开源领域达到最新技术水平，词错误率（WER）为 0.160，字符错误率（CER）为 0.061，BLEU 分数为 0.737。

Conclusion: Qari-OCR 显著提升了阿拉伯语 OCR 的准确性和效率，尤其在处理音标、复杂字体和低分辨率图像方面表现优异。

Abstract: The inherent complexities of Arabic script; its cursive nature, diacritical
marks (tashkeel), and varied typography, pose persistent challenges for Optical
Character Recognition (OCR). We present Qari-OCR, a series of vision-language
models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic
through iterative fine-tuning on specialized synthetic datasets. Our leading
model, QARI v0.2, establishes a new open-source state-of-the-art with a Word
Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score
of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling
of tashkeel, diverse fonts, and document layouts, alongside impressive
performance on low-resolution images. Further explorations (QARI v0.3) showcase
strong potential for structural document understanding and handwritten text.
This work delivers a marked improvement in Arabic OCR accuracy and efficiency,
with all models and datasets released to foster further research.

</details>


### [357] [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/abs/2506.02708)
*Naoto Tanji,Toshihiko Yamasaki*

Key words: 图像评分、视觉语言模型、自训练、可解释性、直接偏好优化

TL;DR: 提出了一种新的视觉语言模型（VLM）训练方法，不仅能生成图像评分，还能提供自然语言解释，通过自训练和迭代优化提升评分准确性与解释一致性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在图像评分任务中，理解模型的决策依据至关重要，但现有方法缺乏对评分的自然语言解释能力。

Method: 利用图像评分数据集和指令调优的VLM进行自训练，结合直接偏好优化（DPO）在两个数据集上迭代训练。

Result: 模型在评分准确性和解释一致性上均得到提升。

Conclusion: 该方法有效提升了VLM在图像评分任务中的可解释性和性能。

Abstract: Image scoring is a crucial task in numerous real-world applications. To trust
a model's judgment, understanding its rationale is essential. This paper
proposes a novel training method for Vision Language Models (VLMs) to generate
not only image scores but also corresponding justifications in natural
language. Leveraging only an image scoring dataset and an instruction-tuned
VLM, our method enables self-training, utilizing the VLM's generated text
without relying on external data or models. In addition, we introduce a simple
method for creating a dataset designed to improve alignment between predicted
scores and their textual justifications. By iteratively training the model with
Direct Preference Optimization on two distinct datasets and merging them, we
can improve both scoring accuracy and the coherence of generated explanations.

</details>


### [358] [Are classical deep neural networks weakly adversarially robust?](https://arxiv.org/abs/2506.02016)
*Nuolin Sun,Linyuan Wang,Dongyang Li,Bin Yan,Lei Li*

Key words: 对抗鲁棒性、特征路径、渐进前馈崩塌、对抗样本检测、DNN

TL;DR: 该论文提出了一种基于层特征的对抗样本检测和图像识别方法，通过构建特征路径并计算其与类中心特征路径的相关性，实现了对抗鲁棒性。结果显示，该方法在干净和对抗样本上达到82.77%和44.17%的准确率，揭示了DNN固有的对抗鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统DNN对抗鲁棒性弱，且对抗训练计算成本高。论文受DNN层特征聚类特性和渐进前馈崩塌现象的启发，寻求更高效的对抗防御方法。

Method: 利用层特征构建特征路径，并计算样本特征路径与类中心特征路径的相关性，用于对抗样本检测和图像识别。

Result: 在ResNet-20上，干净和对抗准确率分别为82.77%和44.17%；在ResNet-18上分别为80.01%和46.1%，优于传统对抗训练。

Conclusion: 该方法展示了DNN固有的对抗鲁棒性，为对抗防御提供了高效的新思路。

Abstract: Adversarial attacks have received increasing attention and it has been widely
recognized that classical DNNs have weak adversarial robustness. The most
commonly used adversarial defense method, adversarial training, improves the
adversarial accuracy of DNNs by generating adversarial examples and retraining
the model. However, adversarial training requires a significant computational
overhead. In this paper, inspired by existing studies focusing on the
clustering properties of DNN output features at each layer and the Progressive
Feedforward Collapse phenomenon, we propose a method for adversarial example
detection and image recognition that uses layer-wise features to construct
feature paths and computes the correlation between the examples feature paths
and the class-centered feature paths. Experimental results show that the
recognition method achieves 82.77% clean accuracy and 44.17% adversarial
accuracy on the ResNet-20 with PFC. Compared to the adversarial training method
with 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits
a trade-off without relying on computationally expensive defense strategies.
Furthermore, on the standard ResNet-18, our method maintains this advantage
with respective metrics of 80.01% and 46.1%. This result reveals inherent
adversarial robustness in DNNs, challenging the conventional understanding of
the weak adversarial robustness in DNNs.

</details>


### [359] [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/abs/2506.02020)
*Youze Xue,Dian Li,Gang Liu*

Key words: 多模态大语言模型, 对比学习, 硬负样本, 梯度分析, MMEB基准

TL;DR: 该论文分析了硬负样本在多模态大语言模型（MLLMs）对比学习中的作用，并提出了一种显式梯度放大方法以提升嵌入区分性，最终在MMEB基准上达到SOTA性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究硬负样本在对比学习中的具体贡献，以提升多模态嵌入的性能。

Method: 通过分析信息NCE损失的梯度，提出显式梯度放大器（Explicit Gradient Amplifier）来放大硬负样本的梯度。

Result: 基于LLaVA-OneVision-7B架构的模型在MMEB基准上取得最优性能，与自研MLLM QQMM结合后登上MMEB排行榜榜首。

Conclusion: 显式梯度放大方法能有效提升多模态嵌入的区分性，证明了硬负样本在对比学习中的重要性。

Abstract: With the rapid advancement of multi-modal large language models (MLLMs) in
recent years, the foundational Contrastive Language-Image Pretraining (CLIP)
framework has been successfully extended to MLLMs, enabling more powerful and
universal multi-modal embeddings for a wide range of retrieval tasks. Despite
these developments, the core contrastive learning paradigm remains largely
unchanged from CLIP-style models to MLLMs. Within this framework, the effective
mining of hard negative samples continues to be a critical factor for enhancing
performance. Prior works have introduced both offline and online strategies for
hard negative mining to improve the efficiency of contrastive learning. While
these approaches have led to improved multi-modal embeddings, the specific
contribution of each hard negative sample to the learning process has not been
thoroughly investigated. In this work, we conduct a detailed analysis of the
gradients of the info-NCE loss with respect to the query, positive, and
negative samples, elucidating the role of hard negatives in updating model
parameters. Building upon this analysis, we propose to explicitly amplify the
gradients associated with hard negative samples, thereby encouraging the model
to learn more discriminative embeddings. Our multi-modal embedding model,
trained with the proposed Explicit Gradient Amplifier and based on the
LLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the
MMEB benchmark compared to previous methods utilizing the same MLLM backbone.
Furthermore, when integrated with our self-developed MLLM, QQMM, our approach
attains the top rank on the MMEB leaderboard. Code and models are released on
https://github.com/QQ-MM/QQMM-embed.

</details>


### [360] [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
*Mengdi Jia,Zekun Qi,Shaochen Zhang,Wenyao Zhang,Xinqiang Yu,Jiawei He,He Wang,Li Yi*

Key words: 空间推理, 视觉语言模型, 认知心理学, 基准测试

TL;DR: 本文提出了一个名为OmniSpatial的全面且具有挑战性的空间推理基准，涵盖动态推理、复杂空间逻辑、空间交互和视角转换四大类别，旨在评估和改进视觉语言模型的空间推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 空间推理是认知心理学的关键部分，但当前视觉语言模型（VLMs）在这一领域仍存在明显瓶颈。现有的研究主要集中在基础空间关系的评估上，缺乏对更复杂空间推理的全面测试。

Method: 通过互联网数据爬取和细致的人工标注，构建了包含50个子类别、超过1.5K问答对的OmniSpatial基准。

Result: 实验表明，现有的开源和闭源VLMs，以及空间理解模型在全面空间理解上存在显著局限性。

Conclusion: OmniSpatial基准揭示了当前模型的不足，并为未来研究方向提供了建议。

Abstract: Spatial reasoning is a key aspect of cognitive psychology and remains a major
bottleneck for current vision-language models (VLMs). While extensive research
has aimed to evaluate or improve VLMs' understanding of basic spatial
relations, such as distinguishing left from right, near from far, and object
counting, these tasks represent only the most fundamental level of spatial
reasoning. In this work, we introduce OmniSpatial, a comprehensive and
challenging benchmark for spatial reasoning, grounded in cognitive psychology.
OmniSpatial covers four major categories: dynamic reasoning, complex spatial
logic, spatial interaction, and perspective-taking, with 50 fine-grained
subcategories. Through Internet data crawling and careful manual annotation, we
construct over 1.5K question-answer pairs. Extensive experiments show that both
open- and closed-source VLMs, as well as existing reasoning and spatial
understanding models, exhibit significant limitations in comprehensive spatial
understanding. We further analyze failure cases and propose potential
directions for future research.

</details>


### [361] [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/abs/2506.03144)
*Wei Chow,Yuan Gao,Linfeng Li,Xian Wang,Qi Xu,Hang Song,Lingdong Kong,Ran Zhou,Yi Zeng,Yidong Cai,Botian Jiang,Shilin Xu,Jiajun Zhang,Minghui Qiu,Xiangtai Li,Tianshu Yang,Siliang Tang,Juncheng Li*

Key words: 语义检索,多条件查询,MERIT数据集,Coral框架,多语言

TL;DR: 论文提出MERIT数据集和Coral框架，解决了多条件语义检索问题，性能提升显著。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有语义检索研究局限于单语言或单图像条件，未能充分利用视觉信息，实际应用需要多条件查询。

Method: 引入MERIT多语言数据集，提出Coral框架，通过嵌入重构和对比学习优化预训练模型。

Result: Coral在MERIT上性能提升45.9%，并在8个基准测试中验证了泛化能力。

Conclusion: 论文为多条件语义检索研究奠定基础，贡献包括数据集、问题发现和创新框架。

Abstract: Semantic retrieval is crucial for modern applications yet remains
underexplored in current research. Existing datasets are limited to single
languages, single images, or singular retrieval conditions, often failing to
fully exploit the expressive capacity of visual information as evidenced by
maintained performance when images are replaced with captions. However,
practical retrieval scenarios frequently involve interleaved multi-condition
queries with multiple images. Hence, this paper introduces MERIT, the first
multilingual dataset for interleaved multi-condition semantic retrieval,
comprising 320,000 queries with 135,000 products in 5 languages, covering 7
distinct product categories. Extensive experiments on MERIT identify existing
models's limitation: focusing solely on global semantic information while
neglecting specific conditional elements in queries. Consequently, we propose
Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by
integrating embedding reconstruction to preserve fine-grained conditional
elements and contrastive learning to extract comprehensive global semantics.
Experiments demonstrate that Coral achieves a 45.9% performance improvement
over conventional approaches on MERIT, with strong generalization capabilities
validated across 8 established retrieval benchmarks. Collectively, our
contributions - a novel dataset, identification of critical limitations in
existing approaches, and an innovative fine-tuning framework - establish a
foundation for future research in interleaved multi-condition semantic
retrieval.

</details>


### [362] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
*Bin Lin,Zongjian Li,Xinhua Cheng,Yuwei Niu,Yang Ye,Xianyi He,Shenghai Yuan,Wangbo Yu,Shaodong Wang,Yunyang Ge,Yatian Pang,Li Yuan*

Key words: UniWorld, 图像感知, 图像操作, GPT-4o-Image, 语义特征

TL;DR: UniWorld是一个基于语义特征的统一生成框架，利用视觉语言模型和对比语义编码器，在少量数据下超越现有模型BAGEL，并在图像理解和生成任务中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有统一模型在图像感知和操作任务上表现不足，而GPT-4o-Image的成功启发了利用语义特征构建新模型的思路。

Method: 使用视觉语言模型和对比语义编码器提供的语义特征，构建统一的生成框架UniWorld。

Result: UniWorld仅用1%的数据就在图像编辑任务上超越BAGEL，同时在图像理解和生成任务中表现优异。

Conclusion: UniWorld展示了语义特征在统一生成模型中的潜力，推动了图像感知和操作任务的进步。

Abstract: Although existing unified models deliver strong performance on
vision-language understanding and text-to-image generation, their models are
limited in exploring image perception and manipulation tasks, which are
urgently desired by users for wide applications. Recently, OpenAI released
their powerful GPT-4o-Image model for comprehensive image perception and
manipulation, achieving expressive capability and attracting community
interests. By observing the performance of GPT-4o-Image in our carefully
constructed experiments, we infer that GPT-4o-Image leverages features
extracted by semantic encoders instead of VAE, while VAEs are considered
essential components in many image manipulation models. Motivated by such
inspiring observations, we present a unified generative framework named
UniWorld based on semantic features provided by powerful visual-language models
and contrastive semantic encoders. As a result, we build a strong unified model
using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on
image editing benchmarks. UniWorld also maintains competitive image
understanding and generation capabilities, achieving strong performance across
multiple image perception tasks. We fully open-source our models, including
model weights, training and evaluation scripts, and datasets.

</details>


### [363] [VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos](https://arxiv.org/abs/2506.02448)
*Baoyu Liang,Qile Su,Shoutai Zhu,Yuchen Liang,Chao Tong*

Key words: 视频事件理解，VidEvent数据集，事件脚本，基准模型

TL;DR: 该论文提出视频事件理解任务，并引入大规模数据集VidEvent，包含23,000多个标注事件，支持事件脚本提取和预测。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于事件复杂的结构、语义层次和动态演化，视频事件理解对AI仍然具有挑战性。

Method: 通过精心标注的大规模数据集VidEvent提供事件结构和逻辑关系，并建立基准模型作为未来研究的基准。

Result: 数据集和基准模型展示了其在视频事件理解中的潜力，推动创新算法的探索。

Conclusion: VidEvent数据集和基准模型为视频事件理解提供了高质量资源，促进未来研究。

Abstract: Despite the significant impact of visual events on human cognition,
understanding events in videos remains a challenging task for AI due to their
complex structures, semantic hierarchies, and dynamic evolution. To address
this, we propose the task of video event understanding that extracts event
scripts and makes predictions with these scripts from videos. To support this
task, we introduce VidEvent, a large-scale dataset containing over 23,000
well-labeled events, featuring detailed event structures, broad hierarchies,
and logical relations extracted from movie recap videos. The dataset was
created through a meticulous annotation process, ensuring high-quality and
reliable event data. We also provide comprehensive baseline models offering
detailed descriptions of their architecture and performance metrics. These
models serve as benchmarks for future research, facilitating comparisons and
improvements. Our analysis of VidEvent and the baseline models highlights the
dataset's potential to advance video event understanding and encourages the
exploration of innovative algorithms and models. The dataset and related
resources are publicly available at www.videvent.top.

</details>


### [364] [Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models](https://arxiv.org/abs/2506.02488)
*Hongtao Huang,Xiaojun Chang,Lina Yao*

Key words: Diffusion Models, Neural Architecture Search, Training-Free, Speedup, FID

TL;DR: Flexiffusion是一种无需训练的NAS框架，通过灵活分段和轻量级评估优化扩散模型，显著加速推理并保持生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有扩散模型因多步推理计算成本高，且传统NAS方法存在重训练需求、搜索复杂度高和评估耗时的问题。

Method: Flexiffusion将生成过程分为灵活分段，动态组合全步、部分步和跳步，并引入轻量级rFID评估指标。

Result: 在多个基准测试中实现2倍至5.1倍加速，FID退化低于5%，Stable Diffusion上CLIP分数几乎不变。

Conclusion: Flexiffusion开创了高效搜索高速扩散模型的范式，显著优化计算资源。

Abstract: Diffusion models (DMs) are powerful generative models capable of producing
high-fidelity images but are constrained by high computational costs due to
iterative multi-step inference. While Neural Architecture Search (NAS) can
optimize DMs, existing methods are hindered by retraining requirements,
exponential search complexity from step-wise optimization, and slow evaluation
relying on massive image generation. To address these challenges, we propose
Flexiffusion, a training-free NAS framework that jointly optimizes generation
schedules and model architectures without modifying pre-trained parameters. Our
key insight is to decompose the generation process into flexible segments of
equal length, where each segment dynamically combines three step types: full
(complete computation), partial (cache-reused computation), and null (skipped
computation). This segment-wise search space reduces the candidate pool
exponentially compared to step-wise NAS while preserving architectural
diversity. Further, we introduce relative FID (rFID), a lightweight evaluation
metric for NAS that measures divergence from a teacher model's outputs instead
of ground truth, slashing evaluation time by over $90\%$. In practice,
Flexiffusion achieves at least $2\times$ acceleration across LDMs, Stable
Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\%$,
outperforming prior NAS and caching methods. Notably, it attains $5.1\times$
speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers
a resource-efficient paradigm for searching high-speed DMs without sacrificing
quality.

</details>


### [365] [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/abs/2506.02537)
*Hao Yan,Handong Zheng,Hao Wang,Liang Yin,Xingchen Liu,Zhenbiao Cao,Xinxing Su,Zihao Chen,Jihao Wu,Minghui Liao,Chao Weng,Wei Chen,Yuliang Liu,Xiang Bai*

Key words: 多模态大语言模型, 抽象视觉推理, 视觉感知, 训练数据合成, 基准评估

TL;DR: 该论文研究了多模态大语言模型（MLLMs）在抽象视觉推理（AVR）任务中的瓶颈问题，提出用于评估的VisuRiddles基准和用于生成训练数据的Perceptual Riddle Synthesizer（PRS）框架，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: MLLMs在抽象视觉推理任务中表现不佳，主要因对抽象图形的感知能力有限，需要提升这方面的能力。

Method: 提出了VisuRiddles基准评估模型能力，并开发PRS框架生成带细粒度感知描述的训练数据。

Result: 实验证明细粒度视觉感知是关键瓶颈，PRS显著提升了MLLMs在AVR任务中的表现。

Conclusion: 细粒度视觉感知和合成训练数据对提升MLLMs的抽象视觉推理能力至关重要。

Abstract: Recent strides in multimodal large language models (MLLMs) have significantly
advanced their performance in many reasoning tasks. However, Abstract Visual
Reasoning (AVR) remains a critical challenge, primarily due to limitations in
perceiving abstract graphics. To tackle this issue, we investigate the
bottlenecks in current MLLMs and synthesize training data to improve their
abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,
featuring tasks meticulously constructed to assess models' reasoning capacities
across five core dimensions and two high-level reasoning categories. Second, we
introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for
generating riddles with fine-grained perceptual descriptions. PRS not only
generates valuable training data for abstract graphics but also provides
fine-grained perceptual description, crucially allowing for supervision over
intermediate reasoning stages and thereby improving both training efficacy and
model interpretability. Our extensive experimental results on VisuRiddles
empirically validate that fine-grained visual perception is the principal
bottleneck and our synthesis framework markedly enhances the performance of
contemporary MLLMs on these challenging tasks. Our code and dataset will be
released at https://github.com/yh-hust/VisuRiddles

</details>


### [366] [Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences](https://arxiv.org/abs/2506.02095)
*Hyojin Bahng,Caroline Chan,Fredo Durand,Phillip Isola*

Key words: 语言-视觉对齐, 循环一致性, 偏好数据集, 奖励模型, 文本-图像生成

TL;DR: 论文提出了一种利用循环一致性作为监督信号的方法，通过文本和图像之间的循环重建来生成偏好数据集，显著提升了语言-视觉对齐任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有方法依赖昂贵且耗时的人类或AI偏好收集的问题，提出一种更高效的替代方案。

Method: 利用文本-图像和图像-文本的循环重建计算相似性，构建偏好数据集，并训练奖励模型。

Result: 该方法在详细字幕任务上优于现有对齐指标，并在多种视觉-语言任务和文本-图像生成中表现卓越。

Conclusion: 循环一致性方法提供了一种高效且可扩展的语言-视觉对齐解决方案。

Abstract: Learning alignment between language and vision is a fundamental challenge,
especially as multimodal data becomes increasingly detailed and complex.
Existing methods often rely on collecting human or AI preferences, which can be
costly and time-intensive. We propose an alternative approach that leverages
cycle consistency as a supervisory signal. Given an image and generated text,
we map the text back to image space using a text-to-image model and compute the
similarity between the original image and its reconstruction. Analogously, for
text-to-image generation, we measure the textual similarity between an input
caption and its reconstruction through the cycle. We use the cycle consistency
score to rank candidates and construct a preference dataset of 866K comparison
pairs. The reward model trained on our dataset outperforms state-of-the-art
alignment metrics on detailed captioning, with superior inference-time
scalability when used as a verifier for Best-of-N sampling. Furthermore,
performing DPO and Diffusion DPO using our dataset enhances performance across
a wide range of vision-language tasks and text-to-image generation. Our
dataset, model, and code are at https://cyclereward.github.io

</details>


### [367] [Quantifying task-relevant representational similarity using decision variable correlation](https://arxiv.org/abs/2506.02164)
*Yu,Qian,Wilson S. Geisler,Xue-Xin Wei*

Key words: 决策变量相关（DVC）、深度神经网络、猴子V4/IT、图像分类、对抗训练

TL;DR: 该论文提出了一种新方法（DVC）来量化模型与猴子大脑在分类任务中决策策略的相似性，发现两者之间存在本质差异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探讨深度神经网络与猴子大脑在图像分类任务中的决策策略是否相似。

Method: 使用决策变量相关（DVC）方法量化不同观察者（模型或大脑）的决策策略相似性。

Result: 模型间相似性与猴子间相似性相当，但模型与猴子相似性较低，且随着模型性能提升反而降低。对抗训练和更大数据集预训练未能提高模型与猴子相似性。

Conclusion: 猴子大脑与模型在任务相关表征上存在根本差异。

Abstract: Previous studies have compared the brain and deep neural networks trained on
image classification. Intriguingly, while some suggest that their
representations are highly similar, others argued the opposite. Here, we
propose a new approach to characterize the similarity of the decision
strategies of two observers (models or brains) using decision variable
correlation (DVC). DVC quantifies the correlation between decoded decisions on
individual samples in a classification task and thus can capture task-relevant
information rather than general representational alignment. We evaluate this
method using monkey V4/IT recordings and models trained on image classification
tasks.
  We find that model--model similarity is comparable to monkey--monkey
similarity, whereas model--monkey similarity is consistently lower and,
surprisingly, decreases with increasing ImageNet-1k performance. While
adversarial training enhances robustness, it does not improve model--monkey
similarity in task-relevant dimensions; however, it markedly increases
model--model similarity. Similarly, pre-training on larger datasets does not
improve model--monkey similarity. These results suggest a fundamental
divergence between the task-relevant representations in monkey V4/IT and those
learned by models trained on image classification tasks.

</details>


### [368] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/abs/2506.02550)
*Qiaohui Chu,Haoyu Zhang,Yisen Feng,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Key words: 长期动作预测，Ego4D，Transformer，大型语言模型，CVPR

TL;DR: 该论文提出了一个新颖的三阶段框架，用于Ego4D长期动作预测任务，结合了视觉特征提取、动作识别和长期动作预测，并在CVPR 2025挑战赛中获得了第一名。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 受基础模型最新进展的启发，旨在提升长期动作预测的准确性。

Method: 采用三阶段方法：1）使用高性能视觉编码器提取特征；2）利用Transformer预测动词和名词，并引入动词-名词共现矩阵提高识别精度；3）将预测结果作为文本提示输入到微调过的大型语言模型（LLM）中，预测未来动作序列。

Result: 在CVPR 2025挑战赛中取得第一名，创造了长期动作预测的最新技术水平。

Conclusion: 该框架有效提升了长期动作预测的准确性，具有创新性和实用性。

Abstract: In this report, we present a novel three-stage framework developed for the
Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in
foundation models, our method consists of three stages: feature extraction,
action recognition, and long-term action anticipation. First, visual features
are extracted using a high-performance visual encoder. The features are then
fed into a Transformer to predict verbs and nouns, with a verb-noun
co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the
predicted verb-noun pairs are formatted as textual prompts and input into a
fine-tuned large language model (LLM) to anticipate future action sequences.
Our framework achieves first place in this challenge at CVPR 2025, establishing
a new state-of-the-art in long-term action prediction. Our code will be
released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

</details>


### [369] [Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment](https://arxiv.org/abs/2506.02221)
*Johannes Schusterbauer,Ming Gui,Frank Fundel,Björn Ommer*

Key words: 扩散模型, 流匹配, 知识迁移, Diff2Flow, 参数高效微调

TL;DR: Diff2Flow框架通过重新缩放时间步长、对齐插值和从扩散预测中导出FM兼容速度场，高效地将预训练扩散模型知识迁移到流匹配模型中。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前基础流匹配模型在微调时计算成本高，而扩散模型如Stable Diffusion则受益于高效架构和生态系统支持。Diff2Flow旨在解决如何高效地从预训练扩散模型迁移知识到流匹配的关键挑战。

Method: Diff2Flow通过三个步骤实现知识迁移：1) 重新缩放时间步长，2) 对齐插值，3) 从扩散预测中导出流匹配兼容的速度场。

Result: 实验表明，Diff2Flow在参数高效的约束条件下优于简单流匹配和扩散微调方法，同时在多样下游任务中表现优于或与最先进方法竞争。

Conclusion: Diff2Flow为扩散模型与流匹配模型的结合提供了一种高效且直接的解决方案，具有显著性能优势。

Abstract: Diffusion models have revolutionized generative tasks through high-fidelity
outputs, yet flow matching (FM) offers faster inference and empirical
performance gains. However, current foundation FM models are computationally
prohibitive for finetuning, while diffusion models like Stable Diffusion
benefit from efficient architectures and ecosystem support. This work addresses
the critical challenge of efficiently transferring knowledge from pre-trained
diffusion models to flow matching. We propose Diff2Flow, a novel framework that
systematically bridges diffusion and FM paradigms by rescaling timesteps,
aligning interpolants, and deriving FM-compatible velocity fields from
diffusion predictions. This alignment enables direct and efficient FM
finetuning of diffusion priors with no extra computation overhead. Our
experiments demonstrate that Diff2Flow outperforms na\"ive FM and diffusion
finetuning particularly under parameter-efficient constraints, while achieving
superior or competitive performance across diverse downstream tasks compared to
state-of-the-art methods. We will release our code at
https://github.com/CompVis/diff2flow.

</details>


### [370] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/abs/2506.02614)
*Guohang Zhuang,Weixi Song,Jinyang Huang,Chenwei Yang,Yan Lu*

Key words: 空间碎片跟踪,深度学习,SDT-Net,SDTD数据集

TL;DR: 本文提出了一种基于深度学习的空间碎片跟踪网络（SDT-Net）和配套的大规模数据集（SDTD），用于解决复杂背景下密集空间碎片的实时准确跟踪问题，并在真实场景中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于空间碎片的潜在威胁日益严重，传统信号处理方法无法有效处理复杂背景和密集碎片，亟需一种更高效的跟踪方法。

Method: 提出SDT-Net，一种基于深度学习的端到端模型，配套生成SDTD数据集，包含18,040个视频序列和250,000个合成碎片。

Result: 模型在真实南极站数据上达到70.6%的MOTA分数，展示了其在实际场景中的强迁移能力。

Conclusion: SDT-Net和SDTD的结合为空间碎片跟踪提供了高效且稳定的解决方案。

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [371] [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/abs/2506.02615)
*Safaa Abdullahi Moallim Mohamud,Minjin Baek,Dong Seog Han*

Key words: 分层问答,视觉语言模型,自动驾驶,场景理解,推理优化

TL;DR: 提出了一种用于自动驾驶场景理解的分层问答方法，通过定制视觉语言模型和动态问题树优化推理效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决自动驾驶场景理解中成本效率与详细视觉解析之间的平衡问题。

Method: 在定制数据集上微调视觉语言模型，采用分层问答策略动态分解问题树，跳过无关问题以优化推理时间。

Result: 在定制数据集上表现优异，推理时间显著低于GPT-4o，实时部署验证了低延迟下的高效场景捕捉能力。

Conclusion: 该方法在保持高准确性的同时显著提升效率，适用于自动驾驶实时场景理解。

Abstract: In this paper, we present a hierarchical question-answering (QA) approach for
scene understanding in autonomous vehicles, balancing cost-efficiency with
detailed visual interpretation. The method fine-tunes a compact vision-language
model (VLM) on a custom dataset specific to the geographical area in which the
vehicle operates to capture key driving-related visual elements. At the
inference stage, the hierarchical QA strategy decomposes the scene
understanding task into high-level and detailed sub-questions. Instead of
generating lengthy descriptions, the VLM navigates a structured question tree,
where answering high-level questions (e.g., "Is it possible for the ego vehicle
to turn left at the intersection?") triggers more detailed sub-questions (e.g.,
"Is there a vehicle approaching the intersection from the opposite
direction?"). To optimize inference time, questions are dynamically skipped
based on previous answers, minimizing computational overhead. The extracted
answers are then synthesized using handcrafted templates to ensure coherent,
contextually accurate scene descriptions. We evaluate the proposed approach on
the custom dataset using GPT reference-free scoring, demonstrating its
competitiveness with state-of-the-art methods like GPT-4o in capturing key
scene details while achieving significantly lower inference time. Moreover,
qualitative results from real-time deployment highlight the proposed approach's
capacity to capture key driving elements with minimal latency.

</details>


### [372] [Approximate Borderline Sampling using Granular-Ball for Classification Tasks](https://arxiv.org/abs/2506.02366)
*Qin Xie,Qinghua Zhang,Shuyin Xia*

Key words: 数据采样,粒度球,边界采样,噪声数据集,分类

TL;DR: 论文提出了一种基于粒度球（GB）的近似边界采样方法GBABS，解决了分类任务中边界模糊或收缩的问题，并在噪声数据集上表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 数据采样通过数据压缩和质量提升提升分类器效率和鲁棒性。现有的GB采样方法在边界采样和噪声数据集处理上存在不足，本文旨在改进这些问题。

Method: 1. 提出受限扩散的GB生成方法RD-GBG，避免重叠并保留几何表示；2. 基于异质最近邻概念，提出GBABS方法，首次实现边界采样和噪声数据集质量提升。

Result: 实验表明，GBABS在边界采样和噪声数据集处理上优于现有方法，无需最优纯度阈值。

Conclusion: GBABS方法在分类任务中表现出色，尤其是在噪声数据集上，为数据采样提供了新思路。

Abstract: Data sampling enhances classifier efficiency and robustness through data
compression and quality improvement. Recently, the sampling method based on
granular-ball (GB) has shown promising performance in generality and noisy
classification tasks. However, some limitations remain, including the absence
of borderline sampling strategies and issues with class boundary blurring or
shrinking due to overlap between GBs. In this paper, an approximate borderline
sampling method using GBs is proposed for classification tasks. First, a
restricted diffusion-based GB generation (RD-GBG) method is proposed, which
prevents GB overlaps by constrained expansion, preserving precise geometric
representation of GBs via redefined ones. Second, based on the concept of
heterogeneous nearest neighbor, a GB-based approximate borderline sampling
(GBABS) method is proposed, which is the first general sampling method capable
of both borderline sampling and improving the quality of class noise datasets.
Additionally, since RD-GBG incorporates noise detection and GBABS focuses on
borderline samples, GBABS performs outstandingly on class noise datasets
without the need for an optimal purity threshold. Experimental results
demonstrate that the proposed methods outperform the GB-based sampling method
and several representative sampling methods. Our source code is publicly
available at https://github.com/CherylTse/GBABS.

</details>


### [373] [Multi-level and Multi-modal Action Anticipation](https://arxiv.org/abs/2506.02382)
*Seulgi Kim,Ghazal Kaviani,Mohit Prabhushankar,Ghassan AlRegib*

Key words: 动作预测、多模态、层次化建模、时间一致性、细粒度标签

TL;DR: 论文提出了一种多模态和多层次的动作预测方法（m&m-Ant），结合视觉和文本线索，并通过细粒度标签生成器和时间一致性损失函数优化预测性能，在多个数据集上实现了3.08%的准确率提升。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 动作预测任务需要处理不完整信息和不确定性，但传统方法仅依赖视觉模态，忽略了多模态信息的潜力。

Method: 提出m&m-Ant方法，结合视觉和文本模态，并利用细粒度标签生成器和时间一致性损失函数进行模型优化。

Result: 在Breakfast、50 Salads和DARai数据集上取得最先进的预测准确率，平均提升3.08%。

Conclusion: 多模态和层次化建模在动作预测任务中具有显著潜力，为未来研究设立了新基准。

Abstract: Action anticipation, the task of predicting future actions from partially
observed videos, is crucial for advancing intelligent systems. Unlike action
recognition, which operates on fully observed videos, action anticipation must
handle incomplete information. Hence, it requires temporal reasoning, and
inherent uncertainty handling. While recent advances have been made,
traditional methods often focus solely on visual modalities, neglecting the
potential of integrating multiple sources of information. Drawing inspiration
from human behavior, we introduce \textit{Multi-level and Multi-modal Action
Anticipation (m\&m-Ant)}, a novel multi-modal action anticipation approach that
combines both visual and textual cues, while explicitly modeling hierarchical
semantic information for more accurate predictions. To address the challenge of
inaccurate coarse action labels, we propose a fine-grained label generator
paired with a specialized temporal consistency loss function to optimize
performance. Extensive experiments on widely used datasets, including
Breakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,
achieving state-of-the-art results with an average anticipation accuracy
improvement of 3.08\% over existing methods. This work underscores the
potential of multi-modal and hierarchical modeling in advancing action
anticipation and establishes a new benchmark for future research in the field.
Our code is available at: https://github.com/olivesgatech/mM-ant.

</details>


### [374] [Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2506.02677)
*Jintao Tong,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Key words: 跨域少样本分割,ViT,特征解耦,距离学习

TL;DR: 本文提出了一种解决跨域少样本分割任务中纠缠问题的方法，通过分解ViT结构并学习权重重组特征，提升泛化和微调能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有跨域少样本分割方法在距离比较中存在纠缠问题，导致源域模式难以迁移。

Method: 分解ViT结构并学习权重以解耦特征，优化距离计算。

Result: 实验表明，新方法在1-shot和5-shot设置下平均准确率分别提升1.92%和1.88%。

Conclusion: 学习权重重组ViT特征能有效解决纠缠问题，提升跨域性能。

Abstract: Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a
source-domain dataset to unseen target-domain datasets with limited
annotations. Current methods typically compare the distance between training
and testing samples for mask prediction. However, we find an entanglement
problem exists in this widely adopted method, which tends to bind sourcedomain
patterns together and make each of them hard to transfer. In this paper, we aim
to address this problem for the CD-FSS task. We first find a natural
decomposition of the ViT structure, based on which we delve into the
entanglement problem for an interpretation. We find the decomposed ViT
components are crossly compared between images in distance calculation, where
the rational comparisons are entangled with those meaningless ones by their
equal importance, leading to the entanglement problem. Based on this
interpretation, we further propose to address the entanglement problem by
learning to weigh for all comparisons of ViT components, which learn
disentangled features and re-compose them for the CD-FSS task, benefiting both
the generalization and finetuning. Experiments show that our model outperforms
the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under
1-shot and 5-shot settings, respectively.

</details>


### [375] [LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering](https://arxiv.org/abs/2506.02733)
*Xiaoyi Feng,Kaifeng Zou,Caichun Cen,Tao Huang,Hui Guo,Zizhou Huang,Yingli Zhao,Mingqing Zhang,Diwei Wang,Yuntao Zou,Dagang Li*

Key words: 光流数据集, 赛璐珞动画, 3D渲染, 视频生成, 基准测试

TL;DR: 本文介绍了首个针对赛璐珞动画角色运动的高质量数据集LinkTo-Anime，填补了现有光流数据集在该领域的空白，并提供了丰富的注释和基准测试。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有光流数据集主要关注现实世界模拟或合成人类运动，缺乏针对赛璐珞动画角色运动的数据集，为此作者构建了LinkTo-Anime以推动光流估计及相关任务的研究。

Method: 通过3D模型渲染生成高质量数据集，包含前向和后向光流、遮挡掩码及Mixamo骨骼标注，总计395个视频序列和大量训练、验证及测试帧。

Result: 构建了一个全面的基准测试，分析了多种光流估计方法在不同数据集上的不足和局限性。

Conclusion: LinkTo-Anime为赛璐珞动画领域的光流研究提供了重要资源，并推动了相关任务的发展。

Abstract: Existing optical flow datasets focus primarily on real-world simulation or
synthetic human motion, but few are tailored to Celluloid(cel) anime character
motion: a domain with unique visual and motion characteristics. To bridge this
gap and facilitate research in optical flow estimation and downstream tasks
such as anime video generation and line drawing colorization, we introduce
LinkTo-Anime, the first high-quality dataset specifically designed for cel
anime character motion generated with 3D model rendering. LinkTo-Anime provides
rich annotations including forward and backward optical flow, occlusion masks,
and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230
training frames, 720 validation frames, and 4,320 test frames. Furthermore, a
comprehensive benchmark is constructed with various optical flow estimation
methods to analyze the shortcomings and limitations across multiple datasets.

</details>


### [376] [Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations](https://arxiv.org/abs/2506.02764)
*Fatma Youssef Mohammed,Kostas Alexis*

Key words: 注意力建模, HAT, 视觉搜索, 迁移学习, 计算效率

TL;DR: 该研究探讨自由观看和任务驱动视觉搜索是否存在共同注意力表示，并提出基于HAT的神经网络架构。结果表明两者可共享表示，模型性能仅下降3.86%，计算成本大幅降低。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究自由观看和任务驱动视觉搜索是否共享注意力表示，以提高模型效率和通用性。

Method: 提出基于Human Attention transformer的神经网络架构，验证自由观看训练的模型能否迁移至任务驱动场景。

Result: 模型迁移性能仅下降3.86%，计算成本减少92.29%（GFLOPs）和31.23%（参数量）。

Conclusion: 自由观看和任务驱动视觉搜索可共享注意力表示，显著提升计算效率。

Abstract: Computational human attention modeling in free-viewing and task-specific
settings is often studied separately, with limited exploration of whether a
common representation exists between them. This work investigates this question
and proposes a neural network architecture that builds upon the Human Attention
transformer (HAT) to test the hypothesis. Our results demonstrate that
free-viewing and visual search can efficiently share a common representation,
allowing a model trained in free-viewing attention to transfer its knowledge to
task-driven visual search with a performance drop of only 3.86% in the
predicted fixation scanpaths, measured by the semantic sequence score (SemSS)
metric which reflects the similarity between predicted and human scanpaths.
This transfer reduces computational costs by 92.29% in terms of GFLOPs and
31.23% in terms of trainable parameters.

</details>


### [377] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/abs/2506.02896)
*Adam Pardyl,Dominik Matuszek,Mateusz Przebieracz,Marek Cygan,Bartosz Zieliński,Maciej Wołczyk*

Key words: 视觉语言模型, 复杂场景, 探索任务, FlySearch, 性能评估

TL;DR: 论文通过FlySearch环境评估了视觉语言模型在复杂场景中的探索能力，发现其表现远不及人类，并提出了一些改进方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 探索视觉语言模型在复杂、非结构化现实环境中的表现，验证其是否能够有效完成主动探索任务。

Method: 引入FlySearch，一个3D、户外、逼真的环境，设计三种难度不同的任务场景，测试并分析模型的性能。

Result: 当前最先进的视觉语言模型即使是最简单任务也无法可靠完成，表现与人类差距明显，且随任务难度增加而恶化。

Conclusion: 视觉语言模型在复杂场景中仍存在显著不足，需进一步改进，尤其需解决视觉幻觉、上下文误解和任务规划失败等问题。

Abstract: The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.

</details>


### [378] [FORLA:Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/abs/2506.02964)
*Guiqiu Liao,Matjaz Jogan,Eric Eaton,Daniel A. Hashimoto*

Key words: 联邦学习, 无监督学习, 对象中心表示, 特征适配, 槽注意力

TL;DR: FORLA是一个联邦学习框架，用于跨客户端的无监督对象中心表示学习和特征适配，通过共享特征适配器和槽注意力模块实现跨领域学习。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决异构无标签数据集中高效视觉表示学习的挑战，特别是在联邦学习中跨客户端联合学习特征的需求。

Method: 采用共享特征适配器和槽注意力模块，设计两分支师生架构，分别重构基础模型和适配后的特征。

Result: 在多个真实数据集上表现优于集中式基线，学习到紧凑且通用的跨领域表示。

Conclusion: 联邦槽注意力是实现分布式概念下跨领域无监督视觉表示学习的有效工具。

Abstract: Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.

</details>


### [379] [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/abs/2506.02975)
*Yicheng Xiao,Lin Song,Rui Yang,Cheng Cheng,Zunnan Xu,Zhaoyang Zhang,Yixiao Ge,Xiu Li,Ying Shan*

Key words: 多模态模型,Transformer,HaploOmni,跨模态兼容

TL;DR: 提出一种高效的多模态训练范式，通过多模态预热策略和跨模态兼容性技术，构建了一个统一的多模态理解和生成的Transformer模型HaploOmni，性能优越。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着语言模型的进步，多模态理解和生成需要更高效的单一模型框架。

Method: 采用多模态预热策略、特征预缩放和多模态AdaLN技术。

Result: HaploOmni在有限训练成本下，在图像和视频理解与生成任务中表现优越。

Conclusion: HaploOmni展示了统一多模态模型的潜力，技术实用且高效。

Abstract: With the advancement of language models, unified multimodal understanding and
generation have made significant strides, with model architectures evolving
from separated components to unified single-model frameworks. This paper
explores an efficient training paradigm to build a single transformer for
unified multimodal understanding and generation. Specifically, we propose a
multimodal warmup strategy utilizing prior knowledge to extend capabilities. To
address cross-modal compatibility challenges, we introduce feature pre-scaling
and multimodal AdaLN techniques. Integrating the proposed technologies, we
present the HaploOmni, a new single multimodal transformer. With limited
training costs, HaploOmni achieves competitive performance across multiple
image and video understanding and generation benchmarks over advanced unified
models. All codes will be made public at https://github.com/Tencent/HaploVLM.

</details>


### [380] [Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge](https://arxiv.org/abs/2506.02976)
*Rachid Zeghlache,Ikram Brahim,Pierre-Henri Conze,Mathieu Lamard,Mohammed El Amine Lazouni,Zineb Aziza Elaouaber,Leila Ryma Lazouni,Christopher Nielsen,Ahmad O. Ahsan,Matthias Wilms,Nils D. Forkert,Lovre Antonio Budimir,Ivana Matovinović,Donik Vršnak,Sven Lončarić,Philippe Zhang,Weili Jiang,Yihao Li,Yiding Hao,Markus Frohmann,Patrick Binder,Marcel Huber,Taha Emre,Teresa Finisterra Araújo,Marzieh Oghbaie,Hrvoje Bogunović,Amerens A. Bekkers,Nina M. van Liebergen,Hugo J. Kuijf,Abdul Qayyum,Moona Mazher,Steven A. Niederer,Alberto J. Beltrán-Carrero,Juan J. Gómez-Valverde,Javier Torresano-Rodríquez,Álvaro Caballero-Sastre,María J. Ledesma Carbayo,Yosuke Yamagishi,Yi Ding,Robin Peretzke,Alexandra Ertl,Maximilian Fischer,Jessica Kächele,Sofiane Zehar,Karim Boukli Hacene,Thomas Monfort,Béatrice Cochener,Mostafa El Habib Daho,Anas-Alexis Benyoussef,Gwenolé Quellec*

Key words: AMD, OCT, AI, MICCAI, 多模态数据

TL;DR: MICCAI 2024的MARIO挑战专注于通过OCT图像自动化检测和监测AMD，评估算法在检测AMD中新血管活动变化的表现。挑战包含多模态数据，主数据集来自法国布雷斯特，辅助数据集来自阿尔及利亚。任务包括分类两个连续OCT B-scans的演变和预测未来三个月的AMD演变。结果显示AI在监测AMD进展上与医生表现相当，但无法预测未来演变。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 推动自动化检测和监测年龄相关性黄斑变性（AMD）的技术发展，特别是通过分析OCT图像评估新血管活动的变化。

Method: 挑战设计了两个任务：分类连续OCT B-scans的演变和预测未来AMD演变。使用多模态数据集（主数据集来自法国布雷斯特，辅助数据集来自阿尔及利亚），35个团队参与并提出方法。

Result: AI在测量AMD进展上与医生表现相当（任务1），但无法预测未来演变（任务2）。

Conclusion: 该挑战为基于OCT和其他临床数据的AMD监测设立了基准，表明AI在进展测量上有效但在预测方面仍有局限。

Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated
detection and monitoring of age-related macular degeneration (AMD) through the
analysis of optical coherence tomography (OCT) images. Designed to evaluate
algorithmic performance in detecting neovascular activity changes within AMD,
the challenge incorporated unique multi-modal datasets. The primary dataset,
sourced from Brest, France, was used by participating teams to train and test
their models. The final ranking was determined based on performance on this
dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate
population and device shifts from submitted solutions. Two tasks were involved
in the MARIO challenge. The first one was the classification of evolution
between two consecutive 2D OCT B-scans. The second one was the prediction of
future AMD evolution over three months for patients undergoing anti-vascular
endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with
the top 12 finalists presenting their methods. This paper outlines the
challenge's structure, tasks, data characteristics, and winning methodologies,
setting a benchmark for AMD monitoring using OCT, infrared imaging, and
clinical data (such as the number of visits, age, gender, etc.). The results of
this challenge indicate that artificial intelligence (AI) performs as well as a
physician in measuring AMD progression (Task 1) but is not yet able of
predicting future evolution (Task 2).

</details>


### [381] [Smartflow: Enabling Scalable Spatiotemporal Geospatial Research](https://arxiv.org/abs/2506.03022)
*David McVicar,Brian Avant,Adrian Gould,Diego Torrejon,Charles Della Porta,Ryan Mukherjee*

Key words: 

TL;DR: BlackSky推出了Smartflow，一个基于云的框架，支持可扩展的时空地理空间研究。该框架利用开源工具和技术，通过STAC兼容目录处理异构地理空间数据为标准化数据立方体，并结合多种工具管理模型实验。其核心为Kubernetes，支持水平与垂直扩展，适用于大范围地理空间分析和模型开发。文章还提出了一种基于Smartflow的新型神经网络架构，用于监测大范围区域的重型建筑。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为地理空间研究提供一种可扩展的解决方案，整合异构数据并支持高效的模型开发和实验。

Method: 使用STAC兼容目录作为输入，处理异构地理空间数据为标准化数据立方体，结合ClearML、Tensorboard等工具管理模型实验，依托Kubernetes实现工作流编排与扩展。

Result: 开发了Smartflow框架，并成功应用于重型建筑监测的神经网络架构，模型能够检测所有主要开发阶段的重型建筑。

Conclusion: Smartflow是一个强大的地理空间研究和模型开发框架，支持大规模分析和扩展。

Abstract: BlackSky introduces Smartflow, a cloud-based framework enabling scalable
spatiotemporal geospatial research built on open-source tools and technologies.
Using STAC-compliant catalogs as a common input, heterogeneous geospatial data
can be processed into standardized datacubes for analysis and model training.
Model experimentation is managed using a combination of tools, including
ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is
Kubernetes, which orchestrates the provisioning and execution of workflows to
support both horizontal and vertical scalability. This combination of features
makes Smartflow well-suited for geospatial model development and analysis over
large geographic areas, time scales, and expansive image archives.
  We also present a novel neural architecture, built using Smartflow, to
monitor large geographic areas for heavy construction. Qualitative results
based on data from the IARPA Space-based Machine Automated Recognition
Technique (SMART) program are presented that show the model is capable of
detecting heavy construction throughout all major phases of development.

</details>


### [382] [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](https://arxiv.org/abs/2506.03065)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Peng Ye,Mingzhu Shen,Wei Cheng,Gang Yu,Tao Chen*

Key words: Diffusion Transformers, 视频生成, 稀疏注意力, 加速框架, FLOP 优化

TL;DR: Sparse-vDiT 是一种针对视频扩散 Transformer (vDiT) 的稀疏加速框架，通过识别和利用注意力图中的稀疏模式，显著降低了计算复杂度和推理延迟，同时保持了高质量的视觉输出。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统 Diffusion Transformers (DiTs) 在生成长视频序列时，因注意力机制的二次复杂度导致推理延迟高。论文通过分析 vDiT 的注意力图，发现可利用的稀疏模式，以优化计算效率。

Method: 1) 设计针对稀疏模式的优化内核，替代密集注意力计算；2) 提出离线稀疏扩散搜索算法，基于硬件感知选择最优稀疏计算策略；3) 头融合技术进一步提升效率。

Result: Sparse-vDiT 在多个先进 vDiT 模型（CogVideoX1.5、HunyuanVideo、Wan2.1）中实现了 1.58-1.85 倍实际推理加速，理论 FLOP 降低 1.67-2.38 倍，PSNR 值达 22.59-27.09。

Conclusion: 研究证明 vDiT 中潜在的稀疏结构可被系统利用以优化长视频合成效率，为未来模型加速提供了新方向。

Abstract: While Diffusion Transformers (DiTs) have achieved breakthroughs in video
generation, this long sequence generation task remains constrained by the
quadratic complexity of attention mechanisms, resulting in significant
inference latency. Through detailed analysis of attention maps in Video
Diffusion Transformer (vDiT), we identify three recurring sparsity patterns:
diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\%
attention heads can be skipped. Crucially, these patterns exhibit strong
layer-depth and head-position correlations but show limited dependence on the
input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity
acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels
that replace dense attention with computationally efficient implementations for
each identified sparsity pattern. 2) An offline sparse diffusion search
algorithm that selects the optimal sparse computation strategy per layer and
head via hardware-aware cost modeling. After determining the optimal
configuration, we fuse heads within the same layer that share the same
attention strategy, enhancing inference efficiency. Integrated into
state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),
Sparse-vDiT achieves 2.09$\times$, 2.38$\times$, and 1.67$\times$ theoretical
FLOP reduction, and actual inference speedups of 1.76$\times$, 1.85$\times$,
and 1.58$\times$, respectively, while maintaining high visual fidelity, with
PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent
structural sparsity in vDiTs can be systematically exploited for long video
synthesis.

</details>


### [383] [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096)
*Christian Schlarmann,Francesco Croce,Nicolas Flammarion,Matthias Hein*

Key words: 多模态嵌入,早期融合,FuseLIP,对比学习,变压器模型

TL;DR: FuseLIP提出了一种新的多模态嵌入架构，通过单一变压器模型处理文本和图像令牌，实现了早期的多模态交互，显著提升了多模态任务的性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的对比学习模型无法直接处理多模态输入，需要使用额外模块合并特征。FuseLIP旨在通过单一模型实现更丰富的多模态表征。

Method: 采用单一变压器模型处理扩展后的文本和图像令牌词汇表，实现早期的多模态交互。

Result: FuseLIP在多模态任务（如VQA和文本引导的图像检索）中表现优于其他方法，同时在单模态任务上保持竞争力。

Conclusion: FuseLIP通过早期融合策略在多模态嵌入任务中取得了显著改进，验证了其有效性。

Abstract: Contrastive language-image pre-training aligns the features of text-image
pairs in a common latent space via distinct encoders for each modality. While
this approach achieves impressive performance in several zero-shot tasks, it
cannot natively handle multimodal inputs, i.e., encoding image and text into a
single feature vector. As a remedy, it is common practice to use additional
modules to merge the features extracted by the unimodal encoders. In this work,
we present FuseLIP, an alternative architecture for multimodal embedding.
Leveraging recent progress in discrete image tokenizers, we propose to use a
single transformer model which operates on an extended vocabulary of text and
image tokens. This early fusion approach allows the different modalities to
interact at each depth of encoding and obtain richer representations compared
to common late fusion. We collect new datasets for multimodal pre-training and
evaluation, designing challenging tasks for multimodal encoder models. We show
that FuseLIP outperforms other approaches in multimodal embedding tasks such as
VQA and text-guided image transformation retrieval, while being comparable to
baselines on unimodal tasks.

</details>


### [384] [Native-Resolution Image Synthesis](https://arxiv.org/abs/2506.03131)
*Zidong Wang,Lei Bai,Xiangyu Yue,Wanli Ouyang,Yiyuan Zhang*

Key words: 图像合成、扩散模型、Transformer、原生分辨率、零样本生成

TL;DR: 论文提出了一种原生分辨率图像合成的新方法，突破了传统固定分辨率方法的限制，能够生成任意分辨率和宽高比的图像。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统图像生成模型局限于固定分辨率和正方形图像，无法灵活处理不同分辨率和宽高比的图像。本文旨在解决这一问题。

Method: 提出了原生分辨率扩散Transformer（NiT），其去噪过程能够显式建模不同分辨率和宽高比，学习广泛的视觉分布。

Result: NiT在ImageNet-256x256和512x512基准测试中表现最佳，且能零样本生成高分辨率（如1536x1536）和多样化宽高比（如16:9、3:1等）的高保真图像。

Conclusion: 原生分辨率建模在视觉生成建模与先进LLM方法之间具有重要潜力。

Abstract: We introduce native-resolution image synthesis, a novel generative modeling
paradigm that enables the synthesis of images at arbitrary resolutions and
aspect ratios. This approach overcomes the limitations of conventional
fixed-resolution, square-image methods by natively handling variable-length
visual tokens, a core challenge for traditional techniques. To this end, we
introduce the Native-resolution diffusion Transformer (NiT), an architecture
designed to explicitly model varying resolutions and aspect ratios within its
denoising process. Free from the constraints of fixed formats, NiT learns
intrinsic visual distributions from images spanning a broad range of
resolutions and aspect ratios. Notably, a single NiT model simultaneously
achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512
benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in
advanced large language models, NiT, trained solely on ImageNet, demonstrates
excellent zero-shot generalization performance. It successfully generates
high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)
and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These
findings indicate the significant potential of native-resolution modeling as a
bridge between visual generative modeling and advanced LLM methodologies.

</details>


### [385] [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097)
*Ashwin Vinod,Shrey Pandit,Aditya Vavre,Linshen Liu*

Key words: EgoVLM, 第一人称视频, 强化学习, GRPO, 时空推理, 关键帧奖励

TL;DR: EgoVLM是一种专为第一人称视频设计的视觉语言模型，通过强化学习方法GRPO提升时空推理能力，在EgoSchema基准上表现优于通用VLMs。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于可穿戴相机和自主代理等新兴应用的需求，需要从第一人称视频流中进行鲁棒的推理。

Method: 使用GRPO强化学习方法微调EgoVLM，直接通过RL训练，无需监督微调，并引入基于关键帧的奖励。

Result: EgoVLM-3B在EgoSchema基准上比Qwen2.5-VL 3B和7B模型分别高出14.33和13.87个准确点。

Conclusion: EgoVLM通过显式生成推理轨迹提升可解释性，适用于下游应用，关键帧奖励为未来研究提供新方向。

Abstract: Emerging embodied AI applications, such as wearable cameras and autonomous
agents, have underscored the need for robust reasoning from first person video
streams. We introduce EgoVLM, a vision-language model specifically designed to
integrate visual comprehension and spatial-temporal reasoning within egocentric
video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization
(GRPO), a reinforcement learning method adapted to align model outputs with
human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly
tune using RL without any supervised fine-tuning phase on chain-of-thought
(CoT) data. We evaluate EgoVLM on egocentric video question answering
benchmarks and show that domain-specific training substantially improves
performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on
non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by
14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By
explicitly generating reasoning traces, EgoVLM enhances interpretability,
making it well-suited for downstream applications. Furthermore, we introduce a
novel keyframe-based reward that incorporates salient frame selection to guide
reinforcement learning optimization. This reward formulation opens a promising
avenue for future exploration in temporally grounded egocentric reasoning.

</details>


### [386] [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](https://arxiv.org/abs/2506.03150)
*Yuanze Lin,Yi-Wen Chen,Yi-Hsuan Tsai,Ronald Clark,Ming-Hsuan Yang*

Key words: IllumiCraft, 扩散模型, 视频生成, 光照控制, 3D几何

TL;DR: IllumiCraft是一个基于扩散模型的端到端框架，通过整合HDR视频、重打光帧和3D点轨迹，生成高质量、光照可控的视频。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有扩散模型在生成视频时缺乏几何线索的光照和外观控制。

Method: 结合HDR视频、重打光帧和3D点轨迹，构建统一扩散架构。

Result: 生成时间一致性高的视频，支持背景和文本条件下的光照调整，优于现有方法。

Conclusion: IllumiCraft在可控视频生成中表现出更高的逼真度和灵活性。

Abstract: Although diffusion-based models can generate high-quality and high-resolution
video sequences from textual or image inputs, they lack explicit integration of
geometric cues when controlling scene lighting and visual appearance across
frames. To address this limitation, we propose IllumiCraft, an end-to-end
diffusion framework accepting three complementary inputs: (1)
high-dynamic-range (HDR) video maps for detailed lighting control; (2)
synthetically relit frames with randomized illumination changes (optionally
paired with a static background reference image) to provide appearance cues;
and (3) 3D point tracks that capture precise 3D geometry information. By
integrating the lighting, appearance, and geometry cues within a unified
diffusion architecture, IllumiCraft generates temporally coherent videos
aligned with user-defined prompts. It supports background-conditioned and
text-conditioned video relighting and provides better fidelity than existing
controllable video generation methods. Project Page:
https://yuanze-lin.me/IllumiCraft_page

</details>


### [387] [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139)
*Siqi Chen,Xinyu Dong,Haolei Xu,Xingyu Wu,Fei Tang,Hang Zhang,Yuchen Yan,Linjuan Wu,Wenqi Zhang,Guiyang Hou,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Key words: SVG处理,基准测试,大语言模型,多模态模型,评估框架

TL;DR: SVGenius是首个全面评估SVG处理的基准测试，覆盖多领域任务，揭示了现有模型的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有SVG处理基准测试覆盖不足且评估碎片化，需要更系统的评估框架。

Method: 构建SVGenius基准，包含2,377个查询和18个指标，评估22种模型。

Result: 专有模型优于开源模型，但所有模型随复杂度增加性能下降；推理增强训练比纯扩展更有效。

Conclusion: SVGenius为SVG处理提供了系统评估框架，推动了向量图形模型的进步。

Abstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising
capabilities for SVG processing, yet existing benchmarks suffer from limited
real-world coverage, lack of complexity stratification, and fragmented
evaluation paradigms. We introduce SVGenius, a comprehensive benchmark
comprising 2,377 queries across three progressive dimensions: understanding,
editing, and generation. Built on real-world data from 24 application domains
with systematic complexity stratification, SVGenius evaluates models through 8
task categories and 18 metrics. We assess 22 mainstream models spanning
different scales, architectures, training paradigms, and accessibility levels.
Our analysis reveals that while proprietary models significantly outperform
open-source counterparts, all models exhibit systematic performance degradation
with increasing complexity, indicating fundamental limitations in current
approaches; however, reasoning-enhanced training proves more effective than
pure scaling for overcoming these limitations, though style transfer remains
the most challenging capability across all model types. SVGenius establishes
the first systematic evaluation framework for SVG processing, providing crucial
insights for developing more capable vector graphics models and advancing
automated graphic design applications. Appendix and supplementary materials
(including all data and code) are available at
https://zju-real.github.io/SVGenius.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [388] [Second-order AAA algorithms for structured data-driven modeling](https://arxiv.org/abs/2506.02241)
*Michael S. Ackermann,Ion Victor Gosea,Serkan Gugercin,Steffen W. R. Werner*

Key words: 数据驱动建模, 动态系统, 二阶微分结构, 频域数据, Adaptive Antoulas-Anderson算法

TL;DR: 本文介绍了三种直接从频域数据构建具有二阶微分结构的动态系统的数据驱动建模方法，并扩展了Adaptive Antoulas-Anderson算法。方法在计算速度和建模精度之间提供权衡，数值实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: math.NA

Motivation: 由于现有数据驱动建模方法常忽略物理现象的微分结构，导致模型难以物理解释，本文旨在解决这一问题。

Method: 基于二阶结构化的重心形式，扩展了Adaptive Antoulas-Anderson算法，提出了兼顾计算速度或建模精度的变体方法。

Result: 通过三个数值例子验证了新方法在建模精度和性能上的优越性。

Conclusion: 所提出的结构化方法比传统非结构化建模更有效，能够更好地捕捉动态系统的物理特性。

Abstract: The data-driven modeling of dynamical systems has become an essential tool
for the construction of accurate computational models from real-world data. In
this process, the inherent differential structures underlying the considered
physical phenomena are often neglected making the reinterpretation of the
learned models in a physically meaningful sense very challenging. In this work,
we present three data-driven modeling approaches for the construction of
dynamical systems with second-order differential structure directly from
frequency domain data. Based on the second-order structured barycentric form,
we extend the well-known Adaptive Antoulas-Anderson algorithm to the case of
second-order systems. Depending on the available computational resources, we
propose variations of the proposed method that prioritize either higher
computation speed or greater modeling accuracy, and we present a theoretical
analysis for the expected accuracy and performance of the proposed methods.
Three numerical examples demonstrate the effectiveness of our new structured
approaches in comparison to classical unstructured data-driven modeling.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [389] [Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability](https://arxiv.org/abs/2506.02073)
*Mengliang He,Jiayi Zeng,Yankai Jiang,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Key words: 流程图、代码生成、多模态大语言模型、基准评测、有监督微调

TL;DR: 提出了Flow2Code基准，用于评估基于流程图的代码生成，覆盖15种语言和多种流程图类型。实验表明当前多模态大语言模型在此任务上表现不佳，但有监督微调能显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有基准忽视基于流程图的代码生成，Flow2Code填补这一空白。

Method: 构建包含5,622代码段和16,866流程图的数据集，评估13种多模态大语言模型，并应用有监督微调。

Result: 当前模型无法完美生成基于流程图的代码，但有监督微调显著改善性能。

Conclusion: Flow2Code为基于流程图的代码生成研究提供新工具，有监督微调是关键改进方向。

Abstract: While large language models (LLMs) show promise in code generation, existing
benchmarks neglect the flowchart-based code generation. To promote further
research on flowchart-based code generation, this work presents Flow2Code, a
novel benchmark for flowchart-based code generation evaluation. The evaluation
dataset spans 15 programming languages and includes 5,622 code segments paired
with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive
experiments with 13 multimodal LLMs reveal that current LLMs can not generate
code based on flowcharts perfectly. Besides, experiment results show that the
supervised fine-tuning technique contributes greatly to the models'
performance. We publicly release our code and datasets at
https://github.com/hml-github/Flow2Code.

</details>


### [390] [The Impact of Software Testing with Quantum Optimization Meets Machine Learning](https://arxiv.org/abs/2506.02090)
*Gopichand Bandarupalli*

Key words: 量子退火, 机器学习, CI/CD, 测试优化, 缺陷检测

TL;DR: 该研究提出了一种结合量子退火与传统机器学习的混合框架，显著提升了CI/CD流水线中的测试用例优先级排序效率。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现代软件系统复杂性高，传统机器学习方法难以应对大规模测试套件，亟需更高效的测试优化方案。

Method: 采用量子退火与机器学习相结合的混合框架，通过量子优化技术优化测试用例优先级。

Result: 在Defects4J数据集上，缺陷检测效率提升25%，测试执行时间减少30%。

Conclusion: 该框架为复杂软件系统的质量保障提供了突破性解决方案，并展望了未来混合量子-经典生态系统的应用前景。

Abstract: Modern software systems complexity challenges efficient testing, as
traditional machine learning (ML) struggles with large test suites. This
research presents a hybrid framework integrating Quantum Annealing with ML to
optimize test case prioritization in CI/CD pipelines. Leveraging quantum
optimization, it achieves a 25 percent increase in defect detection efficiency
and a 30 percent reduction in test execution time versus classical ML,
validated on the Defects4J dataset. A simulated CI/CD environment demonstrates
robustness across evolving codebases. Visualizations, including defect heatmaps
and performance graphs, enhance interpretability. The framework addresses
quantum hardware limits, CI/CD integration, and scalability for 2025s hybrid
quantum-classical ecosystems, offering a transformative approach to software
quality assurance.

</details>


### [391] [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)
*Nguyen-Khang Le,Quan Minh Bui,Minh Ngoc Nguyen,Hiep Nguyen,Trung Vo,Son T. Luu,Shoshin Nomura,Minh Le Nguyen*

Key words: Web应用测试,大型语言模型,图结构,自动化测试

TL;DR: 一种结合图结构和大型语言模型（LLM）的自动化系统，用于生成Web应用程序的测试用例，提升站点导航和表单填写的测试覆盖率和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 确保Web应用程序的可靠性具有挑战性，因界面复杂且动态。传统方法在处理动态导航流和复杂表单交互时存在不足。

Method: 利用屏幕转换图和LLM建模导航流生成测试场景；采用状态图和Selenium脚本自动化处理表单填写测试。

Result: 系统显著提升了测试覆盖率和鲁棒性，推动了Web应用测试的进步。

Conclusion: 结合图结构和LLM的方法有效解决了Web应用测试中的复杂性和动态性问题。

Abstract: Web applications are critical to modern software ecosystems, yet ensuring
their reliability remains challenging due to the complexity and dynamic nature
of web interfaces. Recent advances in large language models (LLMs) have shown
promise in automating complex tasks, but limitations persist in handling
dynamic navigation flows and complex form interactions. This paper presents an
automated system for generating test cases for two key aspects of web
application testing: site navigation and form filling. For site navigation, the
system employs screen transition graphs and LLMs to model navigation flows and
generate test scenarios. For form filling, it uses state graphs to handle
conditional forms and automates Selenium script generation. Key contributions
include: (1) a novel integration of graph structures and LLMs for site
navigation testing, (2) a state graph-based approach for automating
form-filling test cases, and (3) a comprehensive dataset for evaluating
form-interaction testing. Experimental results demonstrate the system's
effectiveness in improving test coverage and robustness, advancing the state of
web application testing.

</details>


### [392] [Rethinking the effects of data contamination in Code Intelligence](https://arxiv.org/abs/2506.02791)
*Zhen Yang,Hongyi Lin,Yifan He,Jie Xu,Zeyu Sun,Shuo Liu,Pengpeng Wang,Zhongxing Yu,Qingyuan Liang*

Key words: 代码智能, 数据污染, 预训练语言模型, 大语言模型, 性能评估

TL;DR: 本文通过实证研究探讨了代码智能任务中的数据污染问题，发现数据污染对预训练语言模型和大语言模型的性能评估影响有限，挑战了传统观念。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 研究数据污染对代码智能任务中模型性能评估的影响，特别是预训练语言模型和大语言模型的表现。

Method: 实验涉及RoBERTa、GPT-2、LLaMA和StarCoder，覆盖代码翻译、生成和摘要任务，对比四种污染场景的影响。

Result: 研究发现，PLMs在配对污染下性能无明显提升，而LLMs受影响显著。其他污染场景对两者均无影响。

Conclusion: 数据污染不必然导致性能高估，为代码智能模型的评估和部署提供了新视角。

Abstract: In recent years, code intelligence has gained increasing importance in the
field of automated software engineering. Meanwhile, the widespread adoption of
Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised
concerns regarding data contamination and its potential impact on model
performance evaluation. This paper presents a systematic empirical study to
investigate the fine-grained data contamination on code intelligence tasks. Our
study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,
namely LLaMA and StarCoder, covering three major tasks: code translation, code
generation, and code summarization. We categorize contamination scenarios into
four types according to the code intelligence practice, namely input-only,
output-only, unpaired, and paired contamination settings, and construct
corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and
inference paradigm adopted by PLMs, even deliberately injecting paired
contamination does not lead to significant performance overestimation. But
direct inference or small-scale fine-tuning uncovers the contamination effects.
In contrast, LLMs with pre-training and inference paradigm are significantly
affected by the paired contamination. Apart from the above, other contamination
scenarios have no impact on both PLMs and LLMs. Our findings challenge the
conventional belief that contamination inevitably leads to performance
overestimation, providing new insights into the evaluation and deployment of
code intelligence models.

</details>


### [393] [How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face](https://arxiv.org/abs/2506.03013)
*Alexandra González,Xavier Franch,David Lo,Silverio Martínez-Fernández*

Key words: 预训练模型（PTMs），软件工程（SE），Hugging Face（HF），代码生成，文本生成

TL;DR: 该论文提出了一种针对软件工程（SE）任务的开源预训练模型（PTMs）分类方法，并通过分析Hugging Face（HF）资源库中的PTMs，识别出2,205个相关模型。研究发现代码生成是SE任务中最常见的类型，而文本生成在ML任务中占主导地位。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 现有的开源预训练模型资源缺乏针对软件工程任务的分类，导致SE领域的研究者和从业者难以选择合适的模型。

Method: 通过系统性地收集HF API中的PTMs数据，并结合模型卡片描述、元数据和相关论文摘要，利用多步过滤法（包括异常值检测、去重和Gemini 2.0 Flash验证）确认SE相关性。

Result: 识别出2,205个SE相关的PTMs，发现代码生成是SE任务中最常见的类型，而需求工程和软件设计相关模型较少；ML任务中以文本生成为主导。2023年Q2以来SE PTMs数量显著增加。

Conclusion: 该分类方法为未来自动化SE场景中的PTMs采样和选择提供了基础，同时指出了SE任务与ML任务在PTMs中的分布差异。

Abstract: Open-Source Pre-Trained Models (PTMs) provide extensive resources for various
Machine Learning (ML) tasks, yet these resources lack a classification tailored
to Software Engineering (SE) needs. To address this gap, we derive a taxonomy
encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a
popular open-source ML repository, Hugging Face (HF). Our repository mining
study began with a systematically gathered database of PTMs from the HF API,
considering their model card descriptions and metadata, and the abstract of the
associated arXiv papers. We confirmed SE relevance through multiple filtering
steps: detecting outliers, identifying near-identical PTMs, and the use of
Gemini 2.0 Flash, which was validated with five pilot studies involving three
human annotators. This approach uncovered 2,205 SE PTMs. We find that code
generation is the most common SE task among PTMs, primarily focusing on
software implementation, while requirements engineering and software design
activities receive limited attention. In terms of ML tasks, text generation
dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly
since 2023 Q2. Our classification provides a solid foundation for future
automated SE scenarios, such as the sampling and selection of suitable PTMs.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [394] [TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation](https://arxiv.org/abs/2506.02267)
*Xue Xia,Saurabh Vishwas Joshi,Kousik Rajesh,Kangnan Li,Yangyi Lu,Nikil Pancha,Dhruvil Deven Badani,Jiajing Xu,Pong Eksombatchai*

Key words: 推荐系统, CTR预测, 长序列, Next Action Loss, 部署优化

TL;DR: 介绍了TransAct V2模型，通过长序列、Next Action Loss和高效部署提升CTR预测性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有工业推荐系统模型在处理长序列用户行为和预测能力方面存在不足。

Method: 使用长序列数据、引入Next Action Loss函数、优化部署方案。

Result: 提升CTR预测准确性并解决大规模序列模型部署问题。

Conclusion: TransAct V2在Pinterest推荐系统中展现出高效和可扩展性。

Abstract: Modeling user action sequences has become a popular focus in industrial
recommendation system research, particularly for Click-Through Rate (CTR)
prediction tasks. However, industry-scale CTR models often rely on short user
sequences, limiting their ability to capture long-term behavior. Additionally,
these models typically lack an integrated action-prediction task within a
point-wise ranking framework, reducing their predictive power. They also rarely
address the infrastructure challenges involved in efficiently serving
large-scale sequential models. In this paper, we introduce TransAct V2, a
production model for Pinterest's Homefeed ranking system, featuring three key
innovations: (1) leveraging very long user sequences to improve CTR
predictions, (2) integrating a Next Action Loss function for enhanced user
action forecasting, and (3) employing scalable, low-latency deployment
solutions tailored to handle the computational demands of extended user action
sequences.

</details>


### [395] [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](https://arxiv.org/abs/2506.02160)
*Madan Krishnamurthy,Daniel Korn,Melissa A Haendel,Christopher J Mungall,Anne E Thessen*

Key words: 生物医学数据, 通用数据元素, 大语言模型, 语义异构, 数据整合

TL;DR: 开发了一个动态可扩展的框架，利用大语言模型和聚类技术，解决了生物医学数据集中通用数据元素的语义异质性和结构变异问题，提高了数据整合效率和互操作性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 生物医学数据集中通用数据元素（CDEs）的语义异质性、结构变异性和上下文依赖性挑战阻碍了数据整合和互操作性，亟需一种动态且可扩展的解决方案。

Method: 方法包括：(1) 大语言模型生成上下文感知的文本嵌入，(2) 使用HDBSCAN对嵌入进行无监督聚类，(3) 利用LLM自动标记，(4) 监督学习训练分类器。

Result: 在包含24,000多个CDEs的NIH NLM CDE库中，系统识别出118个有意义的聚类，分类器准确率达90.46%。外部验证显示强一致性。

Conclusion: 该方法为CDE协调提供了实用的解决方案，提升了选择效率，支持数据互操作性。

Abstract: This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.

</details>


### [396] [Towards Human-like Preference Profiling in Sequential Recommendation](https://arxiv.org/abs/2506.02261)
*Zhongyu Ouyang,Qianlong Wen,Chunhui Zhang,Yanfang Ye,Soroush Vosoughi*

Key words: 顺序推荐系统、偏好优化、人类决策模拟、上下文感知、自适应奖励

TL;DR: RecPO是一个偏好优化框架，旨在通过模拟人类决策中的结构化反馈和上下文延迟，提升顺序推荐系统的性能。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有基于大语言模型的推荐系统未能充分模拟人类灵活、上下文感知的决策策略，忽略了人类行为的结构化、动态和上下文感知机制。

Method: 提出RecPO框架，利用自适应奖励边际，基于推断的偏好层次和时间信号，优先处理相关项目并区分不同程度的偏好和厌恶。

Result: 在五个真实数据集的实验中，RecPO不仅超越了现有最先进基线，还展现了人类决策的关键特征（如及时满足、偏好一致性和上下文辨别能力）。

Conclusion: RecPO通过模拟人类决策机制，显著提升了顺序推荐系统的性能，同时展现出更接近人类行为的推荐策略。

Abstract: Sequential recommendation systems aspire to profile users by interpreting
their interaction histories, echoing how humans make decisions by weighing
experience, relative preference strength, and situational relevance. Yet,
existing large language model (LLM)-based recommenders often fall short of
mimicking the flexible, context-aware decision strategies humans exhibit,
neglecting the structured, dynamic, and context-aware mechanisms fundamental to
human behaviors. To bridge this gap, we propose RecPO, a preference
optimization framework that models structured feedback and contextual delay to
emulate human-like prioritization in sequential recommendation RecPO exploits
adaptive reward margins based on inferred preference hierarchies and temporal
signals, enabling the model to favor immediately relevant items and to
distinguish between varying degrees of preference and aversion. Extensive
experiments across five real-world datasets demonstrate that RecPO not only
yields performance gains over state-of-the-art baselines, but also mirrors key
characteristics of human decision-making: favoring timely satisfaction,
maintaining coherent preferences, and exercising discernment under shifting
contexts.

</details>


### [397] [DeepShop: A Benchmark for Deep Research Shopping Agents](https://arxiv.org/abs/2506.02839)
*Yougang Lyu,Xiaoyu Zhang,Lingyong Yan,Maarten de Rijke,Zhaochun Ren,Xiuying Chen*

Key words: DeepShop, 网络代理, 在线购物, 基准评估, 复杂查询, RAG

TL;DR: DeepShop是一个评估复杂在线购物环境中网络代理性能的基准，通过多样化查询和自动化评估框架，揭示了现有方法在复杂场景中的局限性。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有基准无法反映真实购物场景的复杂性，因此需要开发更全面的评估工具以改进网络代理的性能。

Method: DeepShop包含查询多样化增强、复杂度提升及细粒度与整体评估三个组件，通过自动评估框架测试不同方法的性能。

Result: RAG方法在复杂查询中表现不佳，其他方法在过滤和排序偏好方面面临挑战，整体成功率较低。

Conclusion: DeepShop为未来深度研究购物代理的发展提供了支持，并揭示了当前方法的不足。

Abstract: Web agents for online shopping have shown great promise in automating user
interactions across e-commerce platforms. Benchmarks for assessing such agents
do not reflect the complexity of real-world shopping scenarios, as they often
consist of overly simple queries with deterministic paths, such as "Find iPhone
15." Real shopping scenarios are inherently more layered, involving
multi-dimensional product attributes, search filters, and user-specific sorting
preferences. To address this gap, we introduce DeepShop, a benchmark designed
to evaluate web agents in complex and realistic online shopping environments.
DeepShop comprises three key components. (1) Query diversity evolution:
Starting from real user queries, we generate diverse queries across five
popular online shopping domains. (2) Query complexity evolution: We further
evolve these queries to increase complexity, considering product attributes,
search filters, and sorting preferences, and classify them into three levels:
easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and
holistic evaluation: We propose an automated evaluation framework that assesses
agent performance in terms of fine-grained aspects (product attributes, search
filters, and sorting preferences) and reports the overall success rate through
holistic evaluation. We conduct a systematic evaluation of retrieval-augmented
generation (RAG) methods, web agents, and deep research systems. Results show
that RAG struggles with complex queries due to its lack of web interaction,
while other methods face significant challenges with filters and sorting
preferences, leading to low overall success rates. We also perform
cross-category, complexity-based evaluations and error analyses to support the
advancement of deep research shopping agents.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [398] [FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs](https://arxiv.org/abs/2506.01969)
*Pencuo Zeren,Qiuming Luo,Rui Mao,Chang Kong*

Key words: FlashMLA-ETAP, Efficient Transpose Attention Pipeline, MLA inference, numerical stability, NVIDIA H20 GPUs

TL;DR: FlashMLA-ETAP通过ETAP技术显著提升单实例部署下的Multi-Head Latent Attention推理效率，比现有方法快2.78-5.24倍，并保持数值稳定性。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决在单台多GPU服务器上部署大规模模型（如DeepSeek-R1 671B）时，MLA推理效率低下的问题。

Method: 提出Efficient Transpose Attention Pipeline（ETAP），通过重新配置注意力计算以减少冗余计算。

Result: 在64K序列长度下，速度提升2.78倍（相比FlashMLA），RMSE低15.2倍，数值稳定性高。

Conclusion: ETAP为资源受限的推理提供了可扩展解决方案，适用于中端GPU。

Abstract: Efficient inference of Multi-Head Latent Attention (MLA) is challenged by
deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper
introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the
single-instance deployment scenario on NVIDIA H20 GPUs. We propose the
Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention
computation through transposition to align the KV context length with the
\(M\)-dimension in WGMMA operations, significantly reducing redundant
computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K
sequence length (batch size 16), with 5.24x and 4.94x improvements over
FlashAttention-3 and FlashInfer, respectively, while maintaining numerical
stability with a 15.2x lower RMSE (\(1.25 \times 10^{-5}\)) than
FlashAttention-3. Furthermore, ETAP's design enables seamless integration into
frameworks like FlashAttention-3 and FlashInfer, supported by a detailed
theoretical analysis. Our work addresses a critical gap in resource-constrained
inference, offering a scalable solution for mid-tier GPUs and paving the way
for broader adoption in hardware-aware optimization. Code is available at
https://github.com/pengcuo/FlashMLA-ETAP.

</details>


### [399] [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)
*Yuhao Shen,Junyi Shen,Quan Kong,Tianyu Liu,Yao Lu,Cong Wang*

Key words: 推测解码, 分支并行性, LLM加速, SpecBranch, 动态草稿长度

TL;DR: 本文提出了一种名为SpecBranch的新框架，通过引入分支并行性来优化推测解码（SD），解决了现有方法中串行执行导致的效率问题，显著提升了LLM推理速度。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现有的推测解码（SD）方法因串行执行导致模型间的等待问题，限制了推理速度的提升。本文旨在通过引入分支并行性，优化SD的执行效率。

Method: 提出SpecBranch框架，分析分支并行性的潜力，并通过引入并行推测分支和动态调整草稿长度（结合隐式草稿模型置信度和显式目标模型特征复用）来提升效率。

Result: 实验表明，SpecBranch在多个模型和基准测试中实现了1.8×∼4.5×的加速，并将回退标记减少50%，证明其实际部署的可行性。

Conclusion: SpecBranch通过分支并行性显著提升了推测解码的效率，为解决LLM推理中的瓶颈问题提供了实用方案。

Abstract: Recently, speculative decoding (SD) has emerged as a promising technique to
accelerate LLM inference by employing a small draft model to propose draft
tokens in advance, and validating them in parallel with the large target model.
However, the existing SD methods still remain fundamentally constrained by
their serialized execution, which causes the mutual waiting bubbles between the
draft and target models. To address this challenge, we draw inspiration from
branch prediction in modern processors and propose a novel framework
\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first
take an in-depth analysis of the potential of branch parallelism in SD, and
recognize that the key challenge lies in the trade-offs between parallelization
and token rollback. Based on the analysis, we strategically introduce parallel
speculative branches to preemptively hedge against likely rejections.
Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft
lengths with a hybrid combination of the implicit draft model confidence and
explicit reusing of target model features. Extensive experiments across various
models and benchmarks show that SpecBranch achieves over \textbf{1.8}$\times
\sim$ \textbf{4.5}$\times$ speedups against the auto-regressive decoding and
reduces rollback tokens by $\textbf{50}$\% for poorly aligned models, realizing
its applicability for real-world deployments.

</details>


### [400] [eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems](https://arxiv.org/abs/2506.02007)
*Ruilin Xu,Zongxuan Xie,Pengfei Chen*

Key words: eACGM, eBPF, AI/ML系统监控, GMM, 异常检测

TL;DR: eACGM是一个基于eBPF的全栈AI/ML系统监控框架,实时收集硬件(如GPU、网络)和软件(如CUDA、Python)的性能数据,无需代码修改。通过GMM模型分析多维性能指标,有效识别复杂故障模式。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决AI/ML系统中复杂故障模式的快速诊断和性能优化问题。

Method: 利用eBPF实时收集性能数据,应用GMM进行统计建模和聚类分析。

Result: 在多节点分布式训练场景中验证,低开销且非侵入性,能稳定捕获性能异常。

Conclusion: eACGM适用于生产环境,为大规模AI/ML系统提供强大的性能监控和故障诊断支持。

Abstract: We present eACGM, a full-stack AI/ML system monitoring framework based on
eBPF. eACGM collects real-time performance data from key hardware components,
including the GPU and network communication layer, as well as from key software
stacks such as CUDA, Python, and PyTorch, all without requiring any code
instrumentation or modifications. Additionally, it leverages libnvml to gather
process-level GPU resource usage information. By applying a Gaussian Mixture
Model (GMM) to the collected multidimensional performance metrics for
statistical modeling and clustering analysis, eACGM effectively identifies
complex failure modes, such as latency anomalies, hardware failures, and
communication inefficiencies, enabling rapid diagnosis of system bottlenecks
and abnormal behaviors.
  To evaluate eACGM's effectiveness and practicality, we conducted extensive
empirical studies and case analyses in multi-node distributed training
scenarios. The results demonstrate that eACGM, while maintaining a
non-intrusive and low-overhead profile, successfully captures critical
performance anomalies during model training and inference. Its stable anomaly
detection performance and comprehensive monitoring capabilities validate its
applicability and scalability in real-world production environments, providing
strong support for performance optimization and fault diagnosis in large-scale
AI/ML systems.

</details>


### [401] [Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling](https://arxiv.org/abs/2506.02025)
*Prachi Jadhav,Hongwei Jin,Ewa Deelman,Prasanna Balaprakash*

Key words: 高性能计算, 作业调度, 大型语言模型, 多目标优化, ReAct框架

TL;DR: 提出了一种基于大型语言模型（LLM）的调度器，用于高性能计算（HPC）作业调度，通过自然语言反馈迭代优化决策，平衡多目标优化，并在真实场景中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 传统HPC调度方法在动态负载和异构系统中缺乏适应性，需要更灵活且可解释的解决方案。

Method: 使用ReAct框架（推理+行动）的LLM调度器，结合历史记录和约束模块，通过自然语言反馈优化决策。

Result: 在多种真实HPC负载场景下优于传统方法，能平衡多目标并满足约束，但存在计算开销与实时性的权衡。

Conclusion: LLM在HPC调度中表现出多目标优化潜力，但需解决计算效率问题。

Abstract: High-Performance Computing (HPC) job scheduling involves balancing
conflicting objectives such as minimizing makespan, reducing wait times,
optimizing resource use, and ensuring fairness. Traditional methods, including
heuristic-based (e.g., First-Come-First-Served) or intensive optimization
techniques, often lack adaptability to dynamic workloads and heterogeneous HPC
systems. To address this, we propose a novel Large Language Model (LLM)-based
scheduler using a ReAct-style framework (Reason + Act), enabling iterative,
interpretable decision-making. The system incorporates a scratchpad memory to
track scheduling history and refine decisions via natural language feedback,
while a constraint enforcement module ensures feasibility and safety. We
evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across
seven real-world HPC workload scenarios, including heterogeneous mixes, bursty
patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,
and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling
effectively balances multiple objectives while offering transparent reasoning
through natural language traces. The method excels in constraint satisfaction
and adapts to diverse workloads without domain-specific training. However, a
trade-off between reasoning quality and computational overhead challenges
real-time deployment. This work presents the first comprehensive study of
reasoning-capable LLMs for HPC scheduling, demonstrating their potential to
handle multiobjective optimization while highlighting limitations in
computational efficiency. The findings provide insights into leveraging
advanced language models for complex scheduling problems in dynamic HPC
environments.

</details>


### [402] [EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration](https://arxiv.org/abs/2506.02049)
*Beichen Huang,Ran Cheng,Kay Chen Tan*

Key words: EvoGit, 去中心化, 多智能体, 代码进化, Git

TL;DR: EvoGit是一个去中心化的多智能体框架，通过Git版本控制的系统实现协作式软件开发和自动化代码进化。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 旨在解决传统集中式协作开发中的效率问题，通过智能体自主进化代码，减少人为干预。

Method: 使用多智能体系统，基于Git的版本控制树实现异步协作，无需集中协调或共享内存。

Result: 实验表明，EvoGit能够自主完成功能模块化软件，并在实际任务中表现优异。

Conclusion: EvoGit为去中心化、自动化和持续软件开发提供了新范式。

Abstract: We introduce EvoGit, a decentralized multi-agent framework for collaborative
software development driven by autonomous code evolution. EvoGit deploys a
population of independent coding agents, each proposing edits to a shared
codebase without centralized coordination, explicit message passing, or shared
memory. Instead, all coordination emerges through a Git-based phylogenetic
graph that tracks the full version lineage and enables agents to asynchronously
read from and write to the evolving code repository. This graph-based structure
supports fine-grained branching, implicit concurrency, and scalable agent
interaction while preserving a consistent historical record. Human involvement
is minimal but strategic: users define high-level goals, periodically review
the graph, and provide lightweight feedback to promote promising directions or
prune unproductive ones. Experiments demonstrate EvoGit's ability to
autonomously produce functional and modular software artifacts across two
real-world tasks: (1) building a web application from scratch using modern
frameworks, and (2) constructing a meta-level system that evolves its own
language-model-guided solver for the bin-packing optimization problem. Our
results underscore EvoGit's potential to establish a new paradigm for
decentralized, automated, and continual software development. EvoGit is
open-sourced at https://github.com/BillHuang2001/evogit.

</details>


### [403] [Machine Learning for Consistency Violation Faults Analysis](https://arxiv.org/abs/2506.02002)
*Kamal Giri,Amit Garu*

Key words: 分布式系统, 一致性违反故障, 机器学习, 神经网络, Dijkstra令牌环

TL;DR: 论文提出了一种基于机器学习的分析方法，用于量化分布式系统中一致性违反故障（CVFs）对系统行为的影响，并展示了在Dijkstra令牌环问题中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 分布式系统中一致性违反故障（CVFs）会导致节点基于过时或错误的数据操作，影响系统收敛和性能，因此需要一种有效的方法来量化其影响。

Method: 通过计算程序转换的排名及其效应，量化CVFs的影响。为应对大图的状态空间爆炸问题，采用了前馈神经网络（FNN）和基于TensorFlow分布式API的分布式神经网络模型。

Result: 实验结果显示模型表现良好，测试损失为4.39，平均绝对误差为1.5，但分布式训练在CPU上未显著提升速度。

Conclusion: 结果表明，通过使用GPU或TPU等硬件加速器可以进一步提升模型的扩展性。

Abstract: Distributed systems frequently encounter consistency violation faults (cvfs),
where nodes operate on outdated or inaccurate data, adversely affecting
convergence and overall system performance. This study presents a machine
learning-based approach for analyzing the impact of CVFs, using Dijkstra's
Token Ring problem as a case study. By computing program transition ranks and
their corresponding effects, the proposed method quantifies the influence of
cvfs on system behavior. To address the state space explosion encountered in
larger graphs, two models are implemented: a Feedforward Neural Network (FNN)
and a distributed neural network leveraging TensorFlow's \texttt{tf.distribute}
API. These models are trained on datasets generated from smaller graphs (3 to
10 nodes) to predict parameters essential for determining rank effects.
Experimental results demonstrate promising performance, with a test loss of
4.39 and a mean absolute error of 1.5. Although distributed training on a CPU
did not yield significant speed improvements over a single-device setup, the
findings suggest that scalability could be enhanced through the use of advanced
hardware accelerators such as GPUs or TPUs.

</details>


### [404] [Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing](https://arxiv.org/abs/2506.02006)
*Zhaoyuan Su,Tingfeng Lan,Zirui Wang,Juncheng Yang,Yue Cheng*

Key words: 大型语言模型,动态服务,形态适应,量化,KV缓存

TL;DR: MorphServe是一个动态、工作负载感知的大型语言模型(LLM)服务框架，通过形态适应优化服务效率和资源利用率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现有的服务框架和静态模型压缩技术无法适应工作负载波动，导致服务级目标(SLO)违规或准确性下降。

Method: MorphServe引入了两种异步、token级别的运行时机制：量化层交换和压力感知KV缓存调整。

Result: 实验表明，MorphServe将平均SLO违规减少了92.45%，P95 TTFT延迟提高了2.2x-3.9x，且不影响生成质量。

Conclusion: MorphServe是动态环境中LLM部署的实用弹性解决方案。

Abstract: Efficiently serving large language models (LLMs) under dynamic and bursty
workloads remains a key challenge for real-world deployment. Existing serving
frameworks and static model compression techniques fail to adapt to workload
fluctuations, leading to either service-level objective (SLO) violations under
full-precision serving or persistent accuracy degradation with static
quantization. We present MorphServe, a dynamic, workload-aware LLM serving
framework based on morphological adaptation. MorphServe introduces two
asynchronous, token-level runtime mechanisms: quantized layer swapping, which
selectively replaces less impactful layers with quantized alternatives during
high-load periods, and pressure-aware KV cache resizing, which dynamically
adjusts KV cache capacity in response to memory pressure. These mechanisms
enable state-preserving transitions with minimum runtime overhead and are fully
compatible with modern scheduling and attention techniques. Extensive
experiments on Vicuna and Llama family models with real-world workloads
demonstrate that MorphServe reduces average SLO violations by 92.45 percent and
improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,
without compromising generation quality. These results establish MorphServe as
a practical and elastic solution for LLM deployment in dynamic environments.

</details>


### [405] [DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2506.02023)
*Kevin Han,Bowen Deng,Amir Barati Farimani,Gerbrand Ceder*

Key words: 机器学习原子间势（MLIPs）,分布式计算,图神经网络,分子动力学模拟

TL;DR: 本文提出DistMLIP，一种基于零冗余的图级并行、支持多设备推理的分布式机器学习原子间势（MLIPs）平台。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 为了解决大尺度原子模拟的计算瓶颈，提升MLIPs的并行化效率以满足实际应用需求。

Method: 采用图分区而非传统的空间分区，支持灵活的MLIP模型架构（如多层图神经网络）。

Result: 在8个GPU上实现了近百万原子规模的计算，耗时仅几秒。

Conclusion: DistMLIP为现有MLIPs提供了高效、易用的分布式推理方案，显著提升了计算规模与速度。

Abstract: Large-scale atomistic simulations are essential to bridge computational
materials and chemistry to realistic materials and drug discovery applications.
In the past few years, rapid developments of machine learning interatomic
potentials (MLIPs) have offered a solution to scale up quantum mechanical
calculations. Parallelizing these interatomic potentials across multiple
devices poses a challenging, but promising approach to further extending
simulation scales to real-world applications. In this work, we present
DistMLIP, an efficient distributed inference platform for MLIPs based on
zero-redundancy, graph-level parallelization. In contrast to conventional
space-partitioning parallelization, DistMLIP enables efficient MLIP
parallelization through graph partitioning, allowing multi-device inference on
flexible MLIP model architectures like multi-layer graph neural networks.
DistMLIP presents an easy-to-use, flexible, plug-in interface that enables
distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four
widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We
show that existing foundational potentials can perform near-million-atom
calculations at the scale of a few seconds on 8 GPUs with DistMLIP.

</details>


### [406] [Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM](https://arxiv.org/abs/2506.02490)
*Yong Xiang,Charley Peter Chen,Liyi Zeng,Wei Yin,Xin Liu,Hu Li,Wei Xu*

Key words: Kubernetes, 根因分析, 大型语言模型, StateGraph, MetaGraph

TL;DR: SynergyRCA是一种利用大型语言模型（LLMs）和检索增强工具的创新工具，用于Kubernetes集群的根因分析（RCA），能够高效且精确地识别根因。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 由于Kubernetes集群在动态云环境中面临状态一致性的挑战，导致操作中断和经济损失，因此需要一种稳健的根因分析方法来提升其可靠性。

Method: SynergyRCA结合了LLMs、图数据库的检索增强和专家提示，通过构建StateGraph和MetaGraph来捕获时空关系和实体连接，从而提供上下文相关的RCA见解。

Result: 实验结果表明，SynergyRCA能够在约两分钟内识别根因（包括新发现的问题），并达到约0.90的精确度。

Conclusion: SynergyRCA在提升Kubernetes可靠性的根因分析中表现出高效性和精确性。

Abstract: Kubernetes, a notably complex and distributed system, utilizes an array of
controllers to uphold cluster management logic through state reconciliation.
Nevertheless, maintaining state consistency presents significant challenges due
to unexpected failures, network disruptions, and asynchronous issues,
especially within dynamic cloud environments. These challenges result in
operational disruptions and economic losses, underscoring the necessity for
robust root cause analysis (RCA) to enhance Kubernetes reliability. The
development of large language models (LLMs) presents a promising direction for
RCA. However, existing methodologies encounter several obstacles, including the
diverse and evolving nature of Kubernetes incidents, the intricate context of
incidents, and the polymorphic nature of these incidents. In this paper, we
introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval
augmentation from graph databases and enhancement with expert prompts.
SynergyRCA constructs a StateGraph to capture spatial and temporal
relationships and utilizes a MetaGraph to outline entity connections. Upon the
occurrence of an incident, an LLM predicts the most pertinent resource, and
SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific
insights for RCA. We evaluate SynergyRCA using datasets from two production
Kubernetes clusters, highlighting its capacity to identify numerous root
causes, including novel ones, with high efficiency and precision. SynergyRCA
demonstrates the ability to identify root causes in an average time of about
two minutes and achieves an impressive precision of approximately 0.90.

</details>


### [407] [KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider](https://arxiv.org/abs/2506.02634)
*Jiahao Wang,Jinbo Han,Xingda Wei,Sijie Shen,Dingyan Zhang,Chenguang Fang,Rong Chen,Wenyuan Yu,Haibo Chen*

Key words: 大型语言模型、KV缓存、工作负载分析、缓存淘汰策略、服务性能

TL;DR: 该论文研究了大型语言模型（LLM）服务中KV缓存的工作负载模式，提出了基于真实工作负载的缓存策略优化建议。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: LLM服务中KV缓存的优化效果尚不明确，且缓存策略高度依赖工作负载。研究旨在填补这一空白，提供真实工作负载下的KV缓存特征分析。

Method: 通过分析领先LLM服务提供商的数据，系统性地表征KV缓存的工作负载模式，并提出一种工作负载感知的缓存淘汰策略。

Result: 研究发现KV缓存的复用模式多样且可预测，理想缓存命中率所需缓存容量适中。新策略提升了有限缓存容量下的服务性能。

Conclusion: 真实工作负载下的KV缓存特征分析为LLM服务优化提供了重要见解，尤其是缓存策略的设计。

Abstract: Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.

</details>


### [408] [Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling](https://arxiv.org/abs/2506.02422)
*Xiyu Zhao,Qimei Cui,Ziqiang Du,Weicai Li,Xi Yu,Wei Ni,Ji Zhang,Xiaofeng Tao,Ping Zhang*

Key words: 个性化联邦学习, 隐私保护, 性能公平性, 量子化误差, 高斯差分隐私

TL;DR: 该论文提出了一种基于量化辅助高斯差分隐私机制的无线个性化联邦学习（WPFL）方法，以提高隐私保护和性能公平性。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 解决无线个性化联邦学习中的隐私问题和性能公平性挑战。

Method: 利用量化误差设计高斯差分隐私机制，分析收敛上界，并设计最优传输调度策略。

Result: 实验表明，该方法在准确性、最大测试损失和公平性方面显著优于其他策略。

Conclusion: 提出的方法有效提升了WPFL的隐私保护和性能公平性。

Abstract: Personalized federated learning (PFL) offers a solution to balancing
personalization and generalization by conducting federated learning (FL) to
guide personalized learning (PL). Little attention has been given to wireless
PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is
another challenge resulting from communication bottlenecks in WPFL. This paper
exploits quantization errors to enhance the privacy of WPFL and proposes a
novel quantization-assisted Gaussian differential privacy (DP) mechanism. We
analyze the convergence upper bounds of individual PL models by considering the
impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and
imperfect communication channels on the FL of WPFL. By minimizing the maximum
of the bounds, we design an optimal transmission scheduling strategy that
yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by
revealing the nested structure of this problem to decouple it into subproblems
solved sequentially for the client selection, channel allocation, and power
control, and for the learning rates and PL-FL weighting coefficients.
Experiments validate our analysis and demonstrate that our approach
substantially outperforms alternative scheduling strategies by 87.08%, 16.21%,
and 38.37% in accuracy, the maximum test loss of participating clients, and
fairness (Jain's index), respectively.

</details>


### [409] [Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization](https://arxiv.org/abs/2506.02787)
*Ruilong Wu,Xinjiao Li,Yisu Wang,Xinyu Chen,Dirk Kutscher*

Key words: 混合并行,异构节点,动态网络,仿真优化,大语言模型

TL;DR: 提出了一种考虑节点异构性和动态网络拓扑的混合并行方法，通过仿真优化并行配置，提升大语言模型训练效率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 当前自动并行规划框架忽视节点异构性和动态网络变化，限制了实际应用效果，本文旨在解决这一问题。

Method: 建模异构节点和动态网络环境，利用基于仿真的策略确定最优并行配置，并引入策略剪枝技术加速搜索。

Result: 在异构节点和复杂网络场景下实现与现有方法竞争的性能，并在动态环境中表现更优。

Conclusion: 方法显著提升异构节点训练性能，并增强动态场景的适应性。

Abstract: Hybrid parallelism techniques are essential for efficiently training large
language models (LLMs). Nevertheless, current automatic parallel planning
frameworks often overlook the simultaneous consideration of node heterogeneity
and dynamic network topology changes, limiting their effectiveness in practical
applications. In this paper, we address these limitations by modeling
heterogeneous nodes within dynamically changing network environments and
leveraging simulation-based strategies to determine optimal parallel
configurations. Our approach enables fine-grained workload allocation tailored
for heterogeneous nodes and complex network scenarios, achieving performance
competitive with state-of-the-art methods under regular and stable network
conditions. Additionally, we introduce a strategy pruning technique to rapidly
discard infeasible parallel configurations, substantially reducing the search
space and accelerating the search process through parallel execution within the
simulator. Preliminary evaluations confirm that our method notably enhances
training performance on heterogeneous nodes and demonstrates improved
adaptability in complex, dynamic scenarios such as cloud computing
environments.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [410] [Labelling Data with Unknown References](https://arxiv.org/abs/2506.03083)
*Adrian de Wynter*

Key words: 可信度评估，标注算法，无参考数据，挑战测试

TL;DR: 论文提出了一种无需参考数据即可评估标注者可信度的算法，通过挑战测试验证其可靠性。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 在缺乏标注参考数据时，传统评估方法无法验证标注者的可信度，因此需要一种新的解决方案。

Method: 提出‘No-Data Algorithm’，通过向标注者提出连续挑战来验证其可信度。

Result: 算法能够高概率地验证标注者的可信度，正确接受可靠标注者的输出，并标记不可靠的标注者。

Conclusion: 该算法为无参考数据场景下的评估问题提供了理论保证和实际验证。

Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure
its performance as a labeller. The two ways to establish trustworthiness are
either by testing it, or by assuming the evaluator `knows' somehow the way to
label the corpus. However, if labelled references (e.g., a development set) are
unavailable, neither of these approaches work: the former requires the data,
and the latter is an assumption, not evidence. To address this, we introduce an
algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator
without any existing references. Our algorithm works by successively posing
challenges to said evaluator. We show that this is sufficient to establish
trustworthiness w.h.p., in such a way that when the evaluator actually knows
the way to label the corpus, the No-Data Algorithm accepts its output; and,
conversely, flags untrustworthy evaluators when these are unable to prove it.
We present formal proofs of correctness and limited experiments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [411] [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
*David Sasu,Kweku Andoh Yamoah,Benedict Quartey,Natalie Schluter*

Key words: 语音韵律、意图解析、大语言模型、机器人交互

TL;DR: 提出了一种利用语音韵律直接推断和解析指令意图的新方法，结合大语言模型解决指令歧义，并建立首个机器人歧义语音数据集。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统方法通过语音转文本丢弃了关键的韵律信息，影响了指令意图的解析准确性。

Method: 直接利用语音韵律推断意图，并通过大语言模型的上下文学习选择任务计划。

Result: 方法在检测意图时准确率达95.79%，在解析歧义指令时准确率达71.96%。

Conclusion: 该方法显著提升了人机交互中指令理解的准确性。

Abstract: Enabling robots to accurately interpret and execute spoken language
instructions is essential for effective human-robot collaboration. Traditional
methods rely on speech recognition to transcribe speech into text, often
discarding crucial prosodic cues needed for disambiguating intent. We propose a
novel approach that directly leverages speech prosody to infer and resolve
instruction intent. Predicted intents are integrated into large language models
via in-context learning to disambiguate and select appropriate task plans.
Additionally, we present the first ambiguous speech dataset for robotics,
designed to advance research in speech disambiguation. Our method achieves
95.79% accuracy in detecting referent intents within an utterance and
determines the intended task plan of ambiguous instructions with 71.96%
accuracy, demonstrating its potential to significantly improve human-robot
communication.

</details>


### [412] [HiLO: High-Level Object Fusion for Autonomous Driving using Transformers](https://arxiv.org/abs/2506.02554)
*Timo Osterburg,Franz Albers,Christopher Diehl,Rajesh Pushparaj,Torsten Bertram*

Key words: 自动驾驶, 传感器数据融合, 卡尔曼滤波器, Transformer, 高级别融合

TL;DR: 提出了一种基于改进的卡尔曼滤波器和Transformer的高级别目标融合方法HiLO，显著提升了自动驾驶环境感知的性能，并在跨域场景中验证了其泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为了解决基于学习的传感器数据融合方法在计算复杂度和硬件需求上的限制，同时提升自动驾驶环境感知的鲁棒性。

Method: 改进了Adapted Kalman Filter (AKF)，并提出了基于Transformer的高级别目标融合方法HiLO。

Result: 实验结果表明，F1分数提升了25.9个百分点，平均IoU提升了6.1个百分点。

Conclusion: HiLO方法在真实世界大规模数据集上表现出色，并在跨域场景中验证了其有效性。

Abstract: The fusion of sensor data is essential for a robust perception of the
environment in autonomous driving. Learning-based fusion approaches mainly use
feature-level fusion to achieve high performance, but their complexity and
hardware requirements limit their applicability in near-production vehicles.
High-level fusion methods offer robustness with lower computational
requirements. Traditional methods, such as the Kalman filter, dominate this
area. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel
transformer-based high-level object fusion method called HiLO. Experimental
results demonstrate improvements of $25.9$ percentage points in $\textrm{F}_1$
score and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale
real-world dataset demonstrates the effectiveness of the proposed approaches.
Their generalizability is further validated by cross-domain evaluation between
urban and highway scenarios. Code, data, and models are available at
https://github.com/rst-tu-dortmund/HiLO .

</details>


### [413] [Multi Layered Autonomy and AI Ecologies in Robotic Art Installations](https://arxiv.org/abs/2506.02606)
*Baoyang Chen,Xian Xu,Huamin Qu*

Key words: 机器能动性,艺术创作,AI伦理,沉浸式装置,反馈系统

TL;DR: 《Symbiosis of Agents》是一个探讨机器能动性与艺术创作之间张力的大型装置，通过多层次的信仰系统实现机器与观众的互动。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 探索机器能动性与艺术创作的边界，以及AI未来中责任的归属问题。

Method: 采用三层信仰系统（微观适应行为、中层叙事驱动、宏观首要指令）和沉浸式环境设计，让机器与观众互动。

Result: 作品在国际上展出，展示了如何通过反馈、实验和规则重新定义当代艺术中的能动性、创作权和伦理。

Conclusion: 该作品通过机器与观众的共生关系，重新探讨了艺术创作中的能动性与伦理问题。

Abstract: Symbiosis of Agents is a large-scale installation by Baoyang Chen that embeds
AI-driven robots in an immersive, mirror-lined arena, probing the tension
between machine agency and artistic authorship. Drawing on early cybernetics,
rule-based conceptual art, and seminal robotic works, it orchestrates fluid
exchanges among robotic arms, quadruped machines, their environment, and the
public. A three tier faith system pilots the ecology: micro-level adaptive
tactics, meso-level narrative drives, and a macro-level prime directive. This
hierarchy lets behaviors evolve organically in response to environmental cues
and even a viewer's breath, turning spectators into co-authors of the unfolding
drama.Framed by a speculative terraforming scenario that recalls the historical
exploitation of marginalized labor, the piece asks who bears responsibility in
AI-mediated futures. Choreographed motion, AI-generated scripts, reactive
lighting, and drifting fog cast the robots as collaborators rather than tools,
forging a living, emergent artwork. Exhibited internationally, Symbiosis of
Agents shows how cybernetic feedback, robotic experimentation, and conceptual
rule-making can converge to redefine agency, authorship, and ethics in
contemporary art.

</details>


### [414] [Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent](https://arxiv.org/abs/2506.02373)
*Kordel K. France,Ovidiu Daescu*

Key words: 嗅觉导航, 惯性里程计, 机器人, 气味定位

TL;DR: 论文提出了一种基于嗅觉惯性里程计（OIO）的导航框架，通过结合惯性动力学和快速嗅觉传感器，实现类似视觉惯性里程计的嗅觉导航。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 嗅觉导航是生物探索环境的重要机制，但在机器人中模拟和解决这一任务非常困难。论文旨在通过OIO框架将SLAM和VIO的原理扩展到嗅觉领域，推动实际机器人任务的发展。

Method: 论文定义了嗅觉惯性里程计（OIO）框架，结合惯性动力学和快速嗅觉传感器，并测试了三种不同的气味定位算法，在一个5自由度机器人手臂上进行了气味追踪实验。

Result: 实验结果表明，OIO框架为嗅觉导航研究提供了基础，能够成功应用于农业和食品质检等实际场景。

Conclusion: OIO框架为嗅觉导航研究奠定了基础，未来需进一步优化以应对更复杂的任务。

Abstract: Olfactory navigation is one of the most primitive mechanisms of exploration
used by organisms. Navigation by machine olfaction (artificial smell) is a very
difficult task to both simulate and solve. With this work, we define olfactory
inertial odometry (OIO), a framework for using inertial kinematics, and
fast-sampling olfaction sensors to enable navigation by scent analogous to
visual inertial odometry (VIO). We establish how principles from SLAM and VIO
can be extrapolated to olfaction to enable real-world robotic tasks. We
demonstrate OIO with three different odour localization algorithms on a real
5-DoF robot arm over an odour-tracking scenario that resembles real
applications in agriculture and food quality control. Our results indicate
success in establishing a baseline framework for OIO from which other research
in olfactory navigation can build, and we note performance enhancements that
can be made to address more complex tasks in the future.

</details>


### [415] [Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges](https://arxiv.org/abs/2506.02489)
*Tao Zhong,Jonah Buchanan,Christine Allen-Blanchette*

Key words: 视觉抓取, 抓取转移, Schr\"odinger Bridge, 机器人操纵

TL;DR: 论文提出了一种基于视觉的灵巧抓取转移新方法，通过Schr\"odinger Bridge形式化，实现不同形态机器人手之间的功能等效抓取合成。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决不同形态机器人手之间抓取意图转移的难题，避免配对演示或手特定模拟的需求。

Method: 利用Schr\"odinger Bridge框架进行随机传输，通过分数和流匹配学习源与目标潜在抓取空间之间的映射，结合物理知情成本函数引导转移。

Result: 实验表明，方法能生成稳定且物理基础扎实的抓取，具有强泛化能力。

Conclusion: 该研究实现了异构操纵器的语义抓取转移，连接了基于视觉的抓取与概率生成建模。

Abstract: We propose a new approach to vision-based dexterous grasp translation, which
aims to transfer grasp intent across robotic hands with differing morphologies.
Given a visual observation of a source hand grasping an object, our goal is to
synthesize a functionally equivalent grasp for a target hand without requiring
paired demonstrations or hand-specific simulations. We frame this problem as a
stochastic transport between grasp distributions using the Schr\"odinger Bridge
formalism. Our method learns to map between source and target latent grasp
spaces via score and flow matching, conditioned on visual observations. To
guide this translation, we introduce physics-informed cost functions that
encode alignment in base pose, contact maps, wrench space, and manipulability.
Experiments across diverse hand-object pairs demonstrate our approach generates
stable, physically grounded grasps with strong generalization. This work
enables semantic grasp transfer for heterogeneous manipulators and bridges
vision-based grasping with probabilistic generative modeling.

</details>


### [416] [Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search](https://arxiv.org/abs/2506.02746)
*Lin Xie,Hanyi Li*

Key words: 货架重新定位问题, 机器人移动履行系统, 自适应大邻域搜索, 深度强化学习, 组合优化

TL;DR: 提出了基于深度强化学习（DRL）的自适应大邻域搜索（ALNS）方法，用于优化机器人移动履行系统中的货架重新定位问题，显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决机器人移动履行系统中货架重新定位问题（PRP），通过动态调整ALNS的操作和参数以提高效率。

Method: 结合DRL与ALNS，设计动态选择破坏和修复操作的DRL代理，并根据货架使用频率和移动成本调整参数。

Result: 该方法在解决方案质量上显著优于传统方法（如最便宜位置、固定位置等）。

Conclusion: 证明了学习驱动的控制在仓库系统组合优化中的有效性。

Abstract: The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems
(RMFS) involves selecting optimal storage locations for pods returning from
pick stations. This work presents an improved solution method that integrates
Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning
(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts
key parameters such as destruction degree and acceptance thresholds during the
search. Specialized heuristics for both operators are designed to reflect
PRP-specific characteristics, including pod usage frequency and movement costs.
Computational results show that this DRL-guided ALNS outperforms traditional
approaches such as cheapest-place, fixed-place, binary integer programming, and
static heuristics. The method demonstrates strong solution quality and
illustrating the benefit of learning-driven control within combinatorial
optimization for warehouse systems.

</details>


### [417] [Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games](https://arxiv.org/abs/2506.02849)
*Alejandro Sanchez Roncero,Olov Andersson,Petter Ogren*

Key words: 无人机, 强化学习, 追逃, 对抗训练, 异步多阶段群体算法

TL;DR: 本文提出了一种基于强化学习的1v1四旋翼无人机追逃框架，通过异步多阶段群体算法解决对抗训练中的非稳态和灾难性遗忘问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 随着无人机在民用和军用领域的普及，未经授权或恶意无人机进入禁区的安全与安全问题日益突出，需要高效的追逃策略。

Method: 采用强化学习框架，训练神经网络策略控制机身速率和集体推力，并引入异步多阶段群体算法（AMSPB）优化对抗训练。

Result: 实验表明，基于速率的策略比速度级基线具有更高的捕获率和峰值速度，AMSPB算法在对抗基准对手时表现稳定且持续提升。

Conclusion: 提出的RL框架和AMSPB算法能有效提升无人机追逃性能，为复杂对抗环境提供了新的解决方案。

Abstract: The increasing proliferation of small UAVs in civilian and military airspace
has raised critical safety and security concerns, especially when unauthorized
or malicious drones enter restricted zones. In this work, we present a
reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion.
We train neural network policies to command body rates and collective thrust,
enabling high-speed pursuit and evasive maneuvers that fully exploit the
quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic
forgetting during adversarial co-training, we introduce an Asynchronous
Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the
pursuer or evader learns against a sampled opponent drawn from a growing
population of past and current policies. This continual learning setup ensures
monotonic performance improvement and retention of earlier strategies. Our
results show that (i) rate-based policies achieve significantly higher capture
rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields
stable, monotonic gains against a suite of benchmark opponents.

</details>


### [418] [Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs](https://arxiv.org/abs/2506.02860)
*Wenjing Tang,Xinyu He,Yongxi Huang,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Key words: 任务规划，不确定性，POMDP，大型语言模型，家庭服务机器人

TL;DR: Tru-POMDP是一个结合大型语言模型（LLM）和POMDP规划的规划器，用于处理家庭服务机器人在不确定环境中的任务规划问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 家庭服务机器人在现实世界中执行任务时，需要处理模糊的人类指令、隐藏或未知的物体位置以及开放词汇的物体类型，这导致规划空间巨大且不确定性高。

Method: 提出Tru-POMDP规划器，通过分层假设树（TOH）系统性地查询LLM生成高质量的世界状态和人类目标的粒子信念，并结合开放的POMDP模型进行严格的贝叶斯信念跟踪和规划。

Result: 在多样化的厨房环境中，Tru-POMDP在复杂物体重新排列任务中显著优于现有基于LLM和LLM树搜索混合的规划器，成功率更高，鲁棒性和规划效率更强。

Conclusion: Tru-POMDP通过结合LLM和POMDP规划，有效解决了开放不确定性问题，提升了家庭服务机器人任务规划的效能和鲁棒性。

Abstract: Task planning under uncertainty is essential for home-service robots
operating in the real world. Tasks involve ambiguous human instructions, hidden
or unknown object locations, and open-vocabulary object types, leading to
significant open-ended uncertainty and a boundlessly large planning space. To
address these challenges, we propose Tru-POMDP, a planner that combines
structured belief generation using Large Language Models (LLMs) with principled
POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),
which systematically queries an LLM to construct high-quality particle beliefs
over possible world states and human goals. We further formulate an open-ended
POMDP model that enables rigorous Bayesian belief tracking and efficient
belief-space planning over these LLM-generated hypotheses. Experiments on
complex object rearrangement tasks across diverse kitchen environments show
that Tru-POMDP significantly outperforms state-of-the-art LLM-based and
LLM-tree-search hybrid planners, achieving higher success rates with
significantly better plans, stronger robustness to ambiguity and occlusion, and
greater planning efficiency.

</details>


### [419] [UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models](https://arxiv.org/abs/2506.02955)
*Zewen Yang,Xiaobing Dai,Dian Yu,Qianru Li,Yu Li,Valentin Le Mesle*

Key words: 生成模型,流匹配,轨迹生成,多约束

TL;DR: 本文提出UniConFlow，一种基于流匹配的统一框架，用于轨迹生成，系统性整合了等式与不等式约束。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 现有生成模型虽强大但多无法同时处理多种约束，如避障和动态一致性。

Method: UniConFlow采用流匹配框架，引入预设时间归零函数和二次规划指导输入，以灵活满足任务需求。

Result: 实验显示在移动导航和高维操作任务中，安全性与可行性优于现有方法。

Conclusion: UniConFlow通过统一框架有效解决多约束问题，提升生成轨迹的实用性与适应性。

Abstract: Generative models have become increasingly powerful tools for robot motion
generation, enabling flexible and multimodal trajectory generation across
various tasks. Yet, most existing approaches remain limited in handling
multiple types of constraints, such as collision avoidance and dynamic
consistency, which are often treated separately or only partially considered.
This paper proposes UniConFlow, a unified flow matching (FM) based framework
for trajectory generation that systematically incorporates both equality and
inequality constraints. UniConFlow introduces a novel prescribed-time zeroing
function to enhance flexibility during the inference process, allowing the
model to adapt to varying task requirements. To ensure constraint satisfaction,
particularly with respect to obstacle avoidance, admissible action range, and
kinodynamic consistency, the guidance inputs to the FM model are derived
through a quadratic programming formulation, which enables constraint-aware
generation without requiring retraining or auxiliary controllers. We conduct
mobile navigation and high-dimensional manipulation tasks, demonstrating
improved safety and feasibility compared to state-of-the-art constrained
generative planners. Project page is available at https://uniconflow.github.io.

</details>


### [420] [EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment](https://arxiv.org/abs/2506.03046)
*Mikolaj Walczak,Romina Aalishah,Wyatt Mackey,Brittany Story,David L. Boothe Jr.,Nicholas Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Key words: 强化学习, 网格细胞, 自主导航, 生物启发, PPO

TL;DR: EDEN框架结合生物启发的网格细胞表示和强化学习，实现高效自主导航，优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 深度强化学习代理在适应性上不如人类，EDEN旨在通过生物启发的方法缩小这一差距。

Method: 结合网格细胞编码器与PPO策略，利用视觉和运动传感器数据进行路径整合和向量导航。

Result: EDEN在简单场景中成功率99%，复杂场景中>94%，导航更高效可靠。

Conclusion: EDEN为机器人空间智能提供生物学基础，将神经导航原理与强化学习结合。

Abstract: Deep reinforcement learning agents are often fragile while humans remain
adaptive and flexible to varying scenarios. To bridge this gap, we present
EDEN, a biologically inspired navigation framework that integrates learned
entorhinal-like grid cell representations and reinforcement learning to enable
autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,
EDEN allows agents to perform path integration and vector-based navigation
using visual and motion sensor data. At the core of EDEN is a grid cell encoder
that transforms egocentric motion into periodic spatial codes, producing
low-dimensional, interpretable embeddings of position. To generate these
activations from raw sensory input, we combine fiducial marker detections in
the lightweight MiniWorld simulator and DINO-based visual features in the
high-fidelity Gazebo simulator. These spatial representations serve as input to
a policy trained with Proximal Policy Optimization (PPO), enabling dynamic,
goal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid
prototyping, and Gazebo, which offers realistic physics and perception noise.
Compared to baseline agents using raw state inputs (e.g., position, velocity)
or standard convolutional image encoders, EDEN achieves a 99% success rate,
within the simple scenarios, and >94% within complex floorplans with occluded
paths with more efficient and reliable step-wise navigation. In addition, as a
replacement of ground truth activations, we present a trainable Grid Cell
encoder enabling the development of periodic grid-like patterns from vision and
motion sensor data, emulating the development of such patterns within
biological mammals. This work represents a step toward biologically grounded
spatial intelligence in robotics, bridging neural navigation principles with
reinforcement learning for scalable deployment.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [421] [AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration](https://arxiv.org/abs/2506.02785)
*Charalampos Kalalas,Pavol Mulinka,Guillermo Candela Belmonte,Miguel Fornell,Michail Dalgitsis,Francisco Paredes Vera,Javier Santaella Sánchez,Carmen Vicente Villares,Roshan Sedar,Eftychia Datsika,Angelos Antonopoulos,Antonio Fernández Ojea,Miquel Payaro*

Key words: 人工智能,边缘计算,车辆状态监测,服务迁移,5G

TL;DR: 该论文提出了一种基于边缘计算和人工智能的车辆状态监测服务，通过动态触发服务迁移来应对网络变化，实现在5G环境下的低延迟AI推断。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 目的是利用AI和边缘计算提升车辆状态监测能力，优化维护策略、降低成本并提高安全性。

Method: 提出了一种闭环服务编排框架，动态触发服务迁移以应对网络波动，并在5G赛车场环境中测试。

Result: 实验结果证明框架能有效实现低延迟AI推断和自适应服务部署，适用于智能交通和移动应用。

Conclusion: 该框架在真实环境中表现优异，展示了其在智能交通领域的潜力。

Abstract: Artificial intelligence (AI) has been increasingly applied to the condition
monitoring of vehicular equipment, aiming to enhance maintenance strategies,
reduce costs, and improve safety. Leveraging the edge computing paradigm,
AI-based condition monitoring systems process vast streams of vehicular data to
detect anomalies and optimize operational performance. In this work, we
introduce a novel vehicle condition monitoring service that enables real-time
diagnostics of a diverse set of anomalies while remaining practical for
deployment in real-world edge environments. To address mobility challenges, we
propose a closed-loop service orchestration framework where service migration
across edge nodes is dynamically triggered by network-related metrics. Our
approach has been implemented and tested in a real-world race circuit
environment equipped with 5G network capabilities under diverse operational
conditions. Experimental results demonstrate the effectiveness of our framework
in ensuring low-latency AI inference and adaptive service placement,
highlighting its potential for intelligent transportation and mobility
applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [422] [PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis](https://arxiv.org/abs/2506.02794)
*Mijeong Kim,Gunhee Kim,Jungyoon Choi,Wonjae Roh,Bohyung Han*

Key words: 物理感知、动态新视角合成、数据集、物理模拟、场景建模

TL;DR: PhysGaia是一个专为动态新视角合成（DyNVS）设计的物理感知数据集，涵盖结构化物体和非结构化物理现象，支持复杂的动态场景建模。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 现有数据集多关注于逼真重建，但缺乏对物理现象的建模支持，PhysGaia旨在填补这一空白，推动动态视图合成和物理场景理解的研究。

Method: 通过精心选择的材质特定物理求解器生成严格遵循物理定律的场景，提供3D粒子轨迹和物理参数等真实数据，并与现有DyNVS模型集成。

Result: 数据集包含多样化的物理材料和复杂动态交互场景，为物理建模提供量化评估基础，并展示了与前沿DyNVS模型的结合效果。

Conclusion: PhysGaia填补了物理感知建模数据集的空白，将推动动态视图合成、物理场景理解和深度学习与物理仿真的结合研究。

Abstract: We introduce PhysGaia, a novel physics-aware dataset specifically designed
for Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects
and unstructured physical phenomena. Unlike existing datasets that primarily
focus on photorealistic reconstruction, PhysGaia is created to actively support
physics-aware dynamic scene modeling. Our dataset provides complex dynamic
scenarios with rich interactions among multiple objects, where they
realistically collide with each other and exchange forces. Furthermore, it
contains a diverse range of physical materials, such as liquid, gas,
viscoelastic substance, and textile, which moves beyond the rigid bodies
prevalent in existing datasets. All scenes in PhysGaia are faithfully generated
to strictly adhere to physical laws, leveraging carefully selected
material-specific physics solvers. To enable quantitative evaluation of
physical modeling, our dataset provides essential ground-truth information,
including 3D particle trajectories and physics parameters, e.g., viscosity. To
facilitate research adoption, we also provide essential integration pipelines
for using state-of-the-art DyNVS models with our dataset and report their
results. By addressing the critical lack of datasets for physics-aware
modeling, PhysGaia will significantly advance research in dynamic view
synthesis, physics-based scene understanding, and deep learning models
integrated with physical simulation -- ultimately enabling more faithful
reconstruction and interpretation of complex dynamic scenes. Our datasets and
codes are available in the project website,
http://cvlab.snu.ac.kr/research/PhysGaia.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [423] [Validating remotely sensed biomass estimates with forest inventory data in the western US](https://arxiv.org/abs/2506.03120)
*Xiuyu Cao,Joseph O. Sexton,Panshi Wang,Dimitrios Gounaridis,Neil H. Carter,Kai Zhu*

Key words: 地上生物量, GEDI, 遥感, 森林服务, 碳核算

TL;DR: 该论文通过使用美国森林服务的数据独立验证了terraPulse公司的地上生物量密度(AGBD)数据集，结果显示两者在森林和非森林区域均存在一定差异。

<details>
  <summary>Details</summary>

Main category: stat.AP

Motivation: 高分辨率监测地上生物量(AGB)及其密度(AGBD)对碳核算和生态系统管理至关重要，但目前基于GEDI的商业遥感产品缺乏独立验证。

Method: 研究使用美国森林服务(FIA)的独立数据，对terraPulse的AGBD数据集进行了区域验证，比较了64,000公顷六边形和县级尺度的估计值。

Result: 在六边形尺度上，R²=0.88，RMSE=26.68 Mg/ha；县级尺度上，R²=0.90，RMSE=32.62 Mg/ha。结果显示terraPulse在非森林区域可能高估，在高生物量森林可能低估。

Conclusion: 研究提供了一个可扩展的AGBD验证框架，并为全球生物量监测的商业数据集提供了基准验证。

Abstract: Monitoring aboveground biomass (AGB) and its density (AGBD) at high
resolution is essential for carbon accounting and ecosystem management. While
NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission
provides globally distributed reference measurements for AGBD estimation, the
majority of commercial remote sensing products based on GEDI remain without
rigorous or independent validation. Here, we present an independent regional
validation of an AGBD dataset offered by terraPulse, Inc., based on independent
reference data from the US Forest Service Forest Inventory and Analysis (FIA)
program. Aggregated to 64,000-hectare hexagons and US counties across the US
states of Utah, Nevada, and Washington, we found very strong agreement between
terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =
26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,
agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.
Spatial and statistical analyses indicated that terraPulse AGBD values tended
to exceed FIA estimates in non-forest areas, likely due to FIA's limited
sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited
lower values in high-biomass forests, likely due to saturation effects in its
optical remote-sensing covariates. This study advances operational carbon
monitoring by delivering a scalable framework for comprehensive AGBD validation
using independent FIA data, as well as a benchmark validation of a new
commercial dataset for global biomass monitoring.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [424] [An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models](https://arxiv.org/abs/2506.02730)
*Po-Chieh Yu*

Key words: 语言模型, SETI, 语义诱导, 噪声输入, 潜在结构

TL;DR: 论文提出了一种探索性框架，测试噪声输入是否能引发语言模型的结构化响应。通过评估多种声学输入，发现鲸鱼和鸟类的叫声比白噪声更能触发模型的语义反应。

<details>
  <summary>Details</summary>

Main category: astro-ph.IM

Motivation: 研究旨在探讨语言模型是否能从不具备传统语义的数据中检测出潜在结构，为地外文明搜寻（SETI）提供新的视角。

Method: 使用GPT-2小模型测试四种声学输入（人类语音、鲸鱼叫声、鸟类鸣叫和白噪声），并定义复合评分SIP衡量模型的语义反应。

Result: 鲸鱼和鸟类叫声的SIP评分高于白噪声，人类语音的反应较弱，表明语言模型能检测非常规数据中的潜在结构。

Conclusion: 生成式反应为SETI提供了一种新方法，特别是在未知通信意图的情况下。

Abstract: We present an exploratory framework to test whether noise-like input can
induce structured responses in language models. Instead of assuming that
extraterrestrial signals must be decoded, we evaluate whether inputs can
trigger linguistic behavior in generative systems. This shifts the focus from
decoding to viewing structured output as a sign of underlying regularity in the
input. We tested GPT-2 small, a 117M-parameter model trained on English text,
using four types of acoustic input: human speech, humpback whale vocalizations,
Phylloscopus trochilus birdsong, and algorithmically generated white noise. All
inputs were treated as noise-like, without any assumed symbolic encoding. To
assess reactivity, we defined a composite score called Semantic Induction
Potential (SIP), combining entropy, syntax coherence, compression gain, and
repetition penalty. Results showed that whale and bird vocalizations had higher
SIP scores than white noise, while human speech triggered only moderate
responses. This suggests that language models may detect latent structure even
in data without conventional semantics. We propose that this approach could
complement traditional SETI methods, especially in cases where communicative
intent is unknown. Generative reactivity may offer a different way to identify
data worth closer attention.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [425] [A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads](https://arxiv.org/abs/2506.02802)
*András Strausz,Niels Pardon,Ioana Giurgiu*

Key words: Lakehouse, SQL查询, 跨引擎优化, 成本模型, 多任务学习

TL;DR: 提出一种跨引擎优化器，通过学习的成本模型自动选择适合不同SQL查询的执行引擎，减少人工干预和成本。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: Lakehouse系统中需要为查询选择最佳执行引擎，但目前依赖人工评估，难以应对新引擎和负载的挑战。

Method: 使用多任务学习模型预测查询成本，结合查询计划优化引擎选择。

Result: 优化后的方法比随机选择引擎减少总运行时间高达30.4%。

Conclusion: 跨引擎优化器能高效自动化引擎选择，提升性能并降低人工成本。

Abstract: Lakehouse systems enable the same data to be queried with multiple execution
engines. However, selecting the engine best suited to run a SQL query still
requires a priori knowledge of the query computational requirements and an
engine capability, a complex and manual task that only becomes more difficult
with the emergence of new engines and workloads. In this paper, we address this
limitation by proposing a cross-engine optimizer that can automate engine
selection for diverse SQL queries through a learned cost model. Optimized with
hints, a query plan is used for query cost prediction and routing. Cost
prediction is formulated as a multi-task learning problem, and multiple
predictor heads, corresponding to different engines and provisionings, are used
in the model architecture. This eliminates the need to train engine-specific
models and allows the flexible addition of new engines at a minimal fine-tuning
cost. Results on various databases and engines show that using a query
optimized logical plan for cost estimation decreases the average Q-error by
even 12.6% over using unoptimized plans as input. Moreover, the proposed
cross-engine optimizer reduces the total workload runtime by up to 25.2% in a
zero-shot setting and 30.4% in a few-shot setting when compared to random
routing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [426] [A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder](https://arxiv.org/abs/2506.02044)
*Xinxu Wei,Kanhao Zhao,Yong Jiao,Lifang He,Yu Zhang*

Key words: 大脑基础模型、图对比学习、图掩码自编码器、fMRI、元学习

TL;DR: 本文提出了一个基于图的新型大脑基础模型BrainGFM，利用图对比学习和图掩码自编码器进行大规模fMRI预训练，支持通过图提示和语言提示灵活适应下游任务。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 随着大语言模型在AI研究中的发展，神经科学领域对构建大规模大脑基础模型的兴趣日益增长，现有模型多基于时间序列或ROI特征，本文旨在提出一种基于图的预训练范式以填补这一空白。

Method: 通过图对比学习和图掩码自编码器构建BrainGFM，整合多样大脑图谱进行预训练，结合图提示和语言提示优化模型适应性，并使用元学习优化图提示以实现零样本或少样本学习。

Result: BrainGFM在27个神经影像数据集上预训练，涵盖25种常见神经和精神疾病，支持多种图谱和任务，实验表明其具有强大的泛化能力。

Conclusion: BrainGFM为神经科学研究提供了一个统一的框架，能够通过灵活的提示机制适应不同任务，尤其是在零样本或少样本学习条件下表现出色。

Abstract: As large language models (LLMs) continue to revolutionize AI research, there
is a growing interest in building large-scale brain foundation models to
advance neuroscience. While most existing brain foundation models are
pre-trained on time-series signals or region-of-interest (ROI) features, we
propose a novel graph-based pre-training paradigm for constructing a brain
graph foundation model. In this paper, we introduce the Brain Graph Foundation
Model, termed BrainGFM, a unified framework that leverages graph contrastive
learning and graph masked autoencoders for large-scale fMRI-based pre-training.
BrainGFM is pre-trained on a diverse mixture of brain atlases with varying
parcellations, significantly expanding the pre-training corpus and enhancing
the model's ability to generalize across heterogeneous fMRI-derived brain
representations. To support efficient and versatile downstream transfer, we
integrate both graph prompts and language prompts into the model design,
enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological
and psychiatric disorders, and task settings. Furthermore, we employ
meta-learning to optimize the graph prompts, facilitating strong generalization
to previously unseen disorders under both few-shot and zero-shot learning
conditions via language-guided prompting. BrainGFM is pre-trained on 27
neuroimaging datasets spanning 25 common neurological and psychiatric
disorders, encompassing 2 types of brain atlases (functional and anatomical)
across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000
fMRI scans, and a total of 400,000 graph samples aggregated across all atlases
and parcellations. The code is available at:
https://github.com/weixinxu666/BrainGFM

</details>


### [427] [Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning](https://arxiv.org/abs/2506.03088)
*Lloyd Pellatt,Fotios Drakopoulos,Shievanie Sabesan,Nicholas A. Lesica*

Key words: 听力损失, 变分条件模型, 神经编码, 个性化补偿

TL;DR: 论文提出了一种新的变分条件模型，通过学习从动物听觉中脑神经活动中直接编码听力损失的空间。模型用6个参数即可准确预测正常和听力受损动物的神经响应，为个性化听力补偿提供了新方法。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 解决现有听觉模型无法捕捉听力损失个体差异的问题，希望通过学习神经活动数据直接建模听力损失空间。

Method: 使用变分条件模型，从健康和噪音暴露动物的听觉中脑神经记录中学习听力损失的参数化表示。

Result: 模型预测了62%的正常动物和68%听力受损动物的可解释神经响应方差，并通过贝叶斯优化快速拟合新动物数据。

Conclusion: 该模型为未来开发个性化听力补偿模型奠定了基础，能够快速适配新用户。

Abstract: The mapping from sound to neural activity that underlies hearing is highly
non-linear. The first few stages of this mapping in the cochlea have been
modelled successfully, with biophysical models built by hand and, more
recently, with DNN models trained on datasets simulated by biophysical models.
Modelling the auditory brain has been a challenge because central auditory
processing is too complex for models to be built by hand, and datasets for
training DNN models directly have not been available. Recent work has taken
advantage of large-scale high resolution neural recordings from the auditory
midbrain to build a DNN model of normal hearing with great success. But this
model assumes that auditory processing is the same in all brains, and therefore
it cannot capture the widely varying effects of hearing loss.
  We propose a novel variational-conditional model to learn to encode the space
of hearing loss directly from recordings of neural activity in the auditory
midbrain of healthy and noise exposed animals. With hearing loss parametrised
by only 6 free parameters per animal, our model accurately predicts 62\% of the
explainable variance in neural responses from normal hearing animals and 68%
for hearing impaired animals, within a few percentage points of state of the
art animal specific models. We demonstrate that the model can be used to
simulate realistic activity from out of sample animals by fitting only the
learned conditioning parameters with Bayesian optimisation, achieving
crossentropy loss within 2% of the optimum in 15-30 iterations. Including more
animals in the training data slightly improved the performance on unseen
animals. This model will enable future development of parametrised hearing loss
compensation models trained to directly restore normal neural coding in hearing
impaired brains, which can be quickly fitted for a new user by human in the
loop optimisation.

</details>
