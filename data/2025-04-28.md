<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 32]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.AI](#cs.AI) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.SE](#cs.SE) [Total: 5]
- [eess.IV](#eess.IV) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.CR](#cs.CR) [Total: 8]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.CY](#cs.CY) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 24]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [hep-lat](#hep-lat) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)
*Sabur Butt,Fazlourrahman Balouchzahi,Ahmad Imam Amjad,Maaz Amjad,Hector G. Ceballos,Salud Maria Jimenez-Zafra*

Main category: cs.CL

TL;DR: 研究分析了希望的复杂情感，开发了多语言数据集PolyHope V2，并比较了预训练模型与大型语言模型在不同任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 希望作为一种复杂情感在教育、心理健康和社交中至关重要，但其在自然语言处理中的精准检测因表现形式多样而具有挑战性。

Method: 研究构建了包含30,000条标注推文的多语言数据集，区分四种希望子类型，并测试了预训练模型和大型语言模型的性能。

Result: 微调后的预训练模型在识别希望子类型和讽刺内容上优于基于提示的大型语言模型，尤其在区分细微差异时表现更佳。

Conclusion: 该数据集和研究成果为未来需要更高语义和上下文敏感性的跨语言情感识别任务提供了坚实基础。

Abstract: Hope is a complex and underexplored emotional state that plays a significant
role in education, mental health, and social interaction. Unlike basic
emotions, hope manifests in nuanced forms ranging from grounded optimism to
exaggerated wishfulness or sarcasm, making it difficult for Natural Language
Processing systems to detect accurately. This study introduces PolyHope V2, a
multilingual, fine-grained hope speech dataset comprising over 30,000 annotated
tweets in English and Spanish. This resource distinguishes between four hope
subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances
existing datasets by explicitly labeling sarcastic instances. We benchmark
multiple pretrained transformer models and compare them with large language
models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.
Our findings show that fine-tuned transformers outperform prompt-based LLMs,
especially in distinguishing nuanced hope categories and sarcasm. Through
qualitative analysis and confusion matrices, we highlight systematic challenges
in separating closely related hope subtypes. The dataset and results provide a
robust foundation for future emotion recognition tasks that demand greater
semantic and contextual sensitivity across languages.

</details>


### [2] [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
*Brihi Joshi,Xiang Ren,Swabha Swayamdipta,Rik Koncel-Kedziorski,Tim Paek*

Main category: cs.CL

TL;DR: PB&J框架通过引入心理支架生成的解释，增强了语言模型构建用户角色的能力，从而更准确地预测用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有基于用户人口统计或先前判断的角色构建方法无法捕捉用户判断背后的深层原因，PB&J旨在通过心理学理论填补这一空白。

Method: 利用心理学理论（如大五人格和原始世界信念）设计结构化支架，生成解释用户判断的理性依据。

Result: 在公共意见和电影偏好预测任务中，PB&J增强的角色优于仅依赖人口统计或判断的方法，且与人工撰写解释的性能相当。

Conclusion: PB&J通过心理学理论提升角色构建的解释性，为语言模型预测用户偏好提供了更可靠的基础。

Abstract: Language models prompted with a user description or persona can predict a
user's preferences and opinions, but existing approaches to building personas
-- based solely on a user's demographic attributes and/or prior judgments --
fail to capture the underlying reasoning behind said user judgments. We
introduce PB&J (Psychology of Behavior and Judgments), a framework that
improves LLM personas by incorporating rationales of why a user might make
specific judgments. These rationales are LLM-generated, and aim to reason about
a user's behavior on the basis of their experiences, personality traits or
beliefs. This is done using psychological scaffolds -- structured frameworks
grounded in theories such as the Big 5 Personality Traits and Primal World
Beliefs -- that help provide structure to the generated rationales. Experiments
on public opinion and movie preference prediction tasks demonstrate that LLM
personas augmented with PB&J rationales consistently outperform methods using
only a user's demographics and/or judgments. Additionally, LLM personas
constructed using scaffolds describing user beliefs perform competitively with
those using human-written rationales.

</details>


### [3] [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)
*Zhuang Yu,Shiliang Sun,Jing Zhao,Tengfei Song,Hao Yang*

Main category: cs.CL

TL;DR: 该论文系统研究了预训练编码器和解码器在多模态机器翻译（MMT）中的作用，发现预训练解码器能显著提升翻译质量，而预训练编码器的效果取决于视觉与文本的对齐质量。


<details>
  <summary>Details</summary>
Motivation: 探索大规模预训练语言和视觉模型在MMT中的有效性及作用，填补现有研究中这一领域的不足。

Method: 通过比较从零训练、使用预训练及部分冻结组件等不同策略，在Multi30K和CoMMuTE数据集上评估英德和英法翻译任务的效果。

Result: 预训练在多模态环境中具有不对称作用：解码器能稳定提升输出流畅性和准确性，而编码器的效果受视觉-文本对齐质量影响。

Conclusion: 揭示了预训练组件与模态融合的交互关系，为未来多模态翻译系统架构设计提供指导。

Abstract: Multimodal Machine Translation (MMT) aims to improve translation quality by
leveraging auxiliary modalities such as images alongside textual input. While
recent advances in large-scale pre-trained language and vision models have
significantly benefited unimodal natural language processing tasks, their
effectiveness and role in MMT remain underexplored. In this work, we conduct a
systematic study on the impact of pre-trained encoders and decoders in
multimodal translation models. Specifically, we analyze how different training
strategies, from training from scratch to using pre-trained and partially
frozen components, affect translation performance under a unified MMT
framework. Experiments are carried out on the Multi30K and CoMMuTE dataset
across English-German and English-French translation tasks. Our results reveal
that pre-training plays a crucial yet asymmetrical role in multimodal settings:
pre-trained decoders consistently yield more fluent and accurate outputs, while
pre-trained encoders show varied effects depending on the quality of
visual-text alignment. Furthermore, we provide insights into the interplay
between modality fusion and pre-trained components, offering guidance for
future architecture design in multimodal translation systems.

</details>


### [4] [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)
*Bang An,Shiyue Zhang,Mark Dredze*

Main category: cs.CL

TL;DR: 本文分析了RAG（检索增强生成）框架对LLM（大语言模型）安全性的影响，发现RAG可能降低模型安全性，并探索其成因，指出现有红队方法对RAG效果不佳，呼吁针对RAG的安全研究和红队方法。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG框架广泛使用，但AI安全研究主要针对标准LLM，缺乏RAG对安全性的影响研究。本文旨在填补这一空白。

Method: 对11种LLM进行RAG与非RAG框架的对比分析，探索安全性变化原因，并评估现有红队方法在RAG环境中的效果。

Result: 发现RAG可能使模型更不安全，甚至安全模型与安全文档组合也会产生不安全输出，且现有红队方法对RAG效果较差。

Conclusion: 强调需要专门针对RAG的安全研究和红队方法，以应对其独特挑战。

Abstract: Efforts to ensure the safety of large language models (LLMs) include safety
fine-tuning, evaluation, and red teaming. However, despite the widespread use
of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses
on standard LLMs, which means we know little about how RAG use cases change a
model's safety profile. We conduct a detailed comparative analysis of RAG and
non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe
and change their safety profile. We explore the causes of this change and find
that even combinations of safe models with safe documents can cause unsafe
generations. In addition, we evaluate some existing red teaming methods for RAG
settings and show that they are less effective than when used for non-RAG
settings. Our work highlights the need for safety research and red-teaming
methods specifically tailored for RAG LLMs.

</details>


### [5] [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
*Jianyu Liu,Hangyu Guo,Ranjie Duan,Xingyuan Bu,Yancheng He,Shilong Li,Hui Huang,Jiaheng Liu,Yucheng Wang,Chenchen Jing,Xingwei Qu,Xiao Zhang,Yingshui Tan,Yanan Wu,Jihao Gu,Yangguang Li,Jianke Zhu*

Main category: cs.CL

TL;DR: 论文提出DREAM方法，通过多模态风险解耦和强化学习，显著提升MLLMs的安全性且不影响正常任务性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）因整合视觉和文本数据带来新的安全隐患，需系统性解耦风险以提升模型安全性。

Method: 提出DREAM方法，结合监督微调和迭代式RLAIF（基于AI反馈的强化学习），分步解耦多模态风险。

Result: DREAM在推理和训练阶段均显著提升安全性，SIUO安全&有效性评分较GPT-4V提升16.17%。

Conclusion: DREAM能有效平衡MLLMs的安全性与性能，为多模态模型的可靠部署提供新思路。

Abstract: Multimodal Large Language Models (MLLMs) pose unique safety challenges due to
their integration of visual and textual data, thereby introducing new
dimensions of potential attacks and complex risk combinations. In this paper,
we begin with a detailed analysis aimed at disentangling risks through
step-by-step reasoning within multimodal inputs. We find that systematic
multimodal risk disentanglement substantially enhances the risk awareness of
MLLMs. Via leveraging the strong discriminative abilities of multimodal risk
disentanglement, we further introduce \textbf{DREAM}
(\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety
\textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety
alignment in MLLMs through supervised fine-tuning and iterative Reinforcement
Learning from AI Feedback (RLAIF). Experimental results show that DREAM
significantly boosts safety during both inference and training phases without
compromising performance on normal tasks (namely oversafety), achieving a
16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The
data and code are available at https://github.com/Kizna1ver/DREAM.

</details>


### [6] [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)
*Sijia Cheng,Wen-Yu Chang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 该研究探讨了基于MBTI定义的用户角色对销售对话代理交互质量和性能的影响，揭示了交互动态、任务完成率和对话自然性的显著模式，并提供了实用的用户模拟器工具。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解不同用户角色如何影响销售对话代理的性能，以构建更自适应、以用户为中心的对话系统。

Method: 通过大规模测试和分析，评估预训练代理在多种MBTI用户类型中的有效性、适应性和个性化能力。

Result: 发现了交互动态、任务完成率和对话自然性的显著模式，并发布了不限领域的用户模拟器工具。

Conclusion: 该工作为销售领域提供了构建自适应对话系统的实用见解，同时通过开放的模拟器工具，推动了个性化对话系统的跨领域扩展。

Abstract: The integration of dialogue agents into the sales domain requires a deep
understanding of how these systems interact with users possessing diverse
personas. This study explores the influence of user personas, defined using the
Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance
of sales-oriented dialogue agents. Through large-scale testing and analysis, we
assess the pre-trained agent's effectiveness, adaptability, and personalization
capabilities across a wide range of MBTI-defined user types. Our findings
reveal significant patterns in interaction dynamics, task completion rates, and
dialogue naturalness, underscoring the future potential for dialogue agents to
refine their strategies to better align with varying personality traits. This
work not only provides actionable insights for building more adaptive and
user-centric conversational systems in the sales domain but also contributes
broadly to the field by releasing persona-defined user simulators. These
simulators, unconstrained by domain, offer valuable tools for future research
and demonstrate the potential for scaling personalized dialogue systems across
diverse applications.

</details>


### [7] [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
*Jingjin Wang*

Main category: cs.CL

TL;DR: PropRAG通过利用上下文丰富的命题和新的束搜索算法，显式发现多步推理链，提升了检索增强生成的效能，避免了在证据收集过程中使用生成型LLM的开销和不一致性。


<details>
  <summary>Details</summary>
Motivation: 标准检索增强生成（RAG）未能捕捉到人类记忆的关联性和上下文理解，而现有的结构化RAG方法（如基于知识图谱的HippoRAG）存在上下文丢失的问题。PropRAG旨在通过更丰富的表示和显式的在线路径发现来改善证据检索。

Method: PropRAG使用上下文丰富的命题和一种新的束搜索算法，在命题路径上进行多步推理链的发现。该方法完全依赖高效的图遍历和预计算的嵌入，避免了在线LLM推理的消耗。

Result: PropRAG在多个数据集（如PopQA、2Wiki、HotpotQA和MuSiQue）上取得了最先进的零样本Recall@5和F1分数（如MuSiQue上的52.4%）。

Conclusion: PropRAG通过改进证据检索和多步推理链的显式发现，推动了非参数持续学习的进步。

Abstract: Retrieval Augmented Generation (RAG) has become the standard non-parametric
approach for equipping Large Language Models (LLMs) with up-to-date knowledge
and mitigating catastrophic forgetting common in continual learning. However,
standard RAG, relying on independent passage retrieval, fails to capture the
interconnected nature of human memory crucial for complex reasoning
(associativity) and contextual understanding (sense-making). While structured
RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,
the inherent context loss limits fidelity. We introduce PropRAG, a framework
leveraging contextually rich propositions and a novel beam search algorithm
over proposition paths to explicitly discover multi-step reasoning chains.
Crucially, PropRAG's online retrieval process operates entirely without
invoking generative LLMs, relying instead on efficient graph traversal and
pre-computed embeddings. This avoids online LLM inference costs and potential
inconsistencies during evidence gathering. LLMs are used effectively offline
for high-quality proposition extraction and post-retrieval for answer
generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on
PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside
top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through
richer representation and explicit, LLM-free online path finding, PropRAG
advances non-parametric continual learning.

</details>


### [8] [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)
*Wataru Kawakami,Keita Suzuki,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: 该论文提出了一种针对日本医学领域的72B参数模型Preferred-MedLLM-Qwen-72B，通过两阶段微调实现了高准确性和稳定推理。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医学应用中面临事实准确性、语言限制及推理可靠性问题，阻碍了临床采用。

Method: 采用两阶段微调：持续预训练（CPT）注入医学领域知识，推理偏好优化（RPO）提升可靠推理生成。

Result: 在Japanese Medical Licensing Exam（IgakuQA）上取得0.868准确率，优于GPT-4o（0.866），且在要求解释时仍保持高准确率。

Conclusion: 该研究表明优化可靠解释与准确性同等重要，并开源模型权重以促进高风险领域可信LLM研究。

Abstract: Large Language Models (LLMs) show potential in medicine, yet clinical
adoption is hindered by concerns over factual accuracy, language-specific
limitations (e.g., Japanese), and critically, their reliability when required
to generate reasoning explanations -- a prerequisite for trust. This paper
introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the
Japanese medical domain to achieve both high accuracy and stable reasoning. We
employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first,
Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills
deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a
preference-based method, enhances the generation of reliable reasoning pathways
while preserving high answer accuracy. Evaluations on the Japanese Medical
Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves
state-of-the-art performance (0.868 accuracy), surpassing strong proprietary
models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which
exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively
on IgakuQA) when prompted for explanations, our model maintains its high
accuracy (0.868) under such conditions. This highlights RPO's effectiveness in
stabilizing reasoning generation. This work underscores the importance of
optimizing for reliable explanations alongside accuracy. We release the
Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy
LLMs for specialized, high-stakes applications.

</details>


### [9] [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)
*Muhammad Mubashar,Shireen Kudukkil Manchingal,Fabio Cuzzolin*

Main category: cs.CL

TL;DR: 论文提出了RSLLM方法，通过预测随机集合而非概率向量来量化LLMs的不确定性，实验显示其在答案正确性和不确定性估计方面优于传统LLMs。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs生成文本的可信度问题，尤其是如何量化其不确定性，以提高模型的可信度和实用性。

Method: 提出RSLLM方法，通过预测有限随机集合（信念函数）替代传统概率向量，并利用分层聚类提取关键令牌子集以提高效率。

Result: 在CoQA和OBQA数据集上，RSLLM在答案正确性和不确定性估计方面表现优于标准模型，并能检测幻觉现象。

Conclusion: RSLLM提供了一种有效且可扩展的方法来量化LLMs的不确定性，增强了模型的可信度和实用价值。

Abstract: Large Language Models (LLMs) are known to produce very high-quality tests and
responses to our queries. But how much can we trust this generated text? In
this paper, we study the problem of uncertainty quantification in LLMs. We
propose a novel Random-Set Large Language Model (RSLLM) approach which predicts
finite random sets (belief functions) over the token space, rather than
probability vectors as in classical LLMs. In order to allow so efficiently, we
also present a methodology based on hierarchical clustering to extract and use
a budget of "focal" subsets of tokens upon which the belief prediction is
defined, rather than using all possible collections of tokens, making the
method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced
in their generation process by the size and diversity of its training set via
the size of the credal sets associated with the predicted belief functions. The
proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,
Mistral-7b and Phi-2 models and is shown to outperform the standard model in
both datasets in terms of correctness of answer while also showing potential in
estimating the second level uncertainty in its predictions and providing the
capability to detect when its hallucinating.

</details>


### [10] [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)
*Yinglong Yu,Hao Shen,Zhengyi Lyu,Qi He*

Main category: cs.CL

TL;DR: 本文提出了一种基于提示微调的事实核查价值评估分类方法，通过设计提示模板应用于大语言模型，提升了在有限或无标签数据下判断事实核查价值的准确性。


<details>
  <summary>Details</summary>
Motivation: 针对全球化和信息化背景下错误信息日益严重的问题，需要一种有效的方法来评估信息的核查价值。

Method: 基于提示微调技术构建模型，设计提示模板并应用于大语言模型，通过上下文学习和提示微调提升分类准确性。

Result: 实验表明，该方法在公开数据集上超越了BERT、GPT-3.5和GPT-4等基线模型，F1分数和准确率等指标表现优异。

Conclusion: 提出的基于提示微调的方法在事实核查价值评估任务中表现出有效性和先进性。

Abstract: In response to the growing problem of misinformation in the context of
globalization and informatization, this paper proposes a classification method
for fact-check-worthiness estimation based on prompt tuning. We construct a
model for fact-check-worthiness estimation at the methodological level using
prompt tuning. By applying designed prompt templates to large language models,
we establish in-context learning and leverage prompt tuning technology to
improve the accuracy of determining whether claims have fact-check-worthiness,
particularly when dealing with limited or unlabeled data. Through extensive
experiments on public datasets, we demonstrate that the proposed method
surpasses or matches multiple baseline methods in the classification task of
fact-check-worthiness estimation assessment, including classical pre-trained
models such as BERT, as well as recent popular large models like GPT-3.5 and
GPT-4. Experiments show that the prompt tuning-based method proposed in this
study exhibits certain advantages in evaluation metrics such as F1 score and
accuracy, thereby effectively validating its effectiveness and advancement in
the task of fact-check-worthiness estimation.

</details>


### [11] [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)
*Yinglong Yu,Zhaopu Yao,Fang Yuan*

Main category: cs.CL

TL;DR: 该研究通过主题建模、大语言模型（LLM）提示工程和语料库短语学方法，分析中英文媒体对巴黎奥运会的报道，探讨话语建构和态度意义的异同。


<details>
  <summary>Details</summary>
Motivation: 研究中英文媒体在报道同一国际事件时的视角和态度差异，揭示文化背景对新闻报道的影响。

Method: 结合主题建模、大语言模型（LLM）提示工程和语料库短语学方法，分析媒体报道的文本数据。

Result: 中英文媒体共同关注开幕式、运动员表现和赞助品牌，但中文媒体更侧重具体项目、体育精神、兴奋剂争议和新技术，英文媒体则聚焦女运动员、奖牌和资格争议。中文报道在描述开幕式和体育精神时倾向正面语义韵，而英文报道对女运动员的报道也呈现正面语义韵，但对开幕式反应和女子拳击争议则倾向负面。

Conclusion: 中英文媒体报道在主题选择和态度表达上存在显著差异，反映文化价值观和新闻偏好的不同。

Abstract: This study analyzes Chinese and English media reports on the Paris Olympics
using topic modeling, Large Language Model (LLM) prompt engineering, and corpus
phraseology methods to explore similarities and differences in discourse
construction and attitudinal meanings. Common topics include the opening
ceremony, athlete performance, and sponsorship brands. Chinese media focus on
specific sports, sports spirit, doping controversies, and new technologies,
while English media focus on female athletes, medal wins, and eligibility
controversies. Chinese reports show more frequent prepositional co-occurrences
and positive semantic prosody in describing the opening ceremony and sports
spirit. English reports exhibit positive semantic prosody when covering female
athletes but negative prosody in predicting opening ceremony reactions and
discussing women's boxing controversies.

</details>


### [12] [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
*Atharva Kulkarni,Yuan Zhang,Joel Ruben Antony Moniz,Xiou Ge,Bo-Hsiang Tseng,Dhivya Piraviperumal,Swabha Swayamdipta,Hong Yu*

Main category: cs.CL

TL;DR: 论文研究了语言模型中的幻觉问题，评估了6种不同的幻觉检测指标在4个数据集、37种语言模型和5种解码方法中的表现，发现当前指标与人类判断不一致、过于片面，并指出GPT-4等LLM评估表现最佳，而模式寻求解码方法可减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 幻觉问题是阻碍语言模型可靠性和广泛应用的主要障碍，但现有的检测指标在鲁棒性和泛化性上仍存在不足，需通过大规模实证研究评估其有效性。

Method: 研究对6组不同幻觉检测指标进行了大规模实验，覆盖4个数据集、37个语言模型（来自5个家族）和5种解码方法，并与人类判断对比。

Result: 当前指标常与人类判断不一致，表现片面且参数扩展效果不一致；GPT-4等LLM评估表现最佳，模式寻求解码方法可减少幻觉（尤其在知识相关场景）。

Conclusion: 需开发更鲁棒的幻觉量化指标和更有效的缓解策略，LLM评估和特定解码方法展现了潜力。

Abstract: Hallucinations pose a significant obstacle to the reliability and widespread
adoption of language models, yet their accurate measurement remains a
persistent challenge. While many task- and domain-specific metrics have been
proposed to assess faithfulness and factuality concerns, the robustness and
generalization of these metrics are still untested. In this paper, we conduct a
large-scale empirical evaluation of 6 diverse sets of hallucination detection
metrics across 4 datasets, 37 language models from 5 families, and 5 decoding
methods. Our extensive investigation reveals concerning gaps in current
hallucination evaluation: metrics often fail to align with human judgments,
take an overtly myopic view of the problem, and show inconsistent gains with
parameter scaling. Encouragingly, LLM-based evaluation, particularly with
GPT-4, yields the best overall results, and mode-seeking decoding methods seem
to reduce hallucinations, especially in knowledge-grounded settings. These
findings underscore the need for more robust metrics to understand and quantify
hallucinations, and better strategies to mitigate them.

</details>


### [13] [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)
*Tatsunori Tanaka,Fi Zheng,Kai Sato,Zhifeng Li,Yuanyun Zhang,Shi Li*

Main category: cs.CL

TL;DR: 提出了一种新的临床语言模型预训练目标，利用电子健康记录的时序性改进临床推理能力，在多个任务上达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有临床语言模型通常将电子健康记录视为静态文档，忽略了其时间演化和因果关联的特性，限制了模型在时间相关任务上的表现。

Method: 通过将电子健康记录片段建模为时序句子对，训练模型判断后续状态与先前状态的逻辑关系（蕴含、矛盾或中立），从而学习隐含的临床时序推理能力。

Result: 在MIMIC IV数据上预训练的模型，在时序临床问答、早期预警预测和疾病进展建模等任务上取得最先进结果。

Conclusion: 时序蕴含预训练目标显著提升了临床语言模型在时间敏感任务中的表现，展示了时序结构建模的重要性。

Abstract: Clinical language models have achieved strong performance on downstream tasks
by pretraining on domain specific corpora such as discharge summaries and
medical notes. However, most approaches treat the electronic health record as a
static document, neglecting the temporally-evolving and causally entwined
nature of patient trajectories. In this paper, we introduce a novel temporal
entailment pretraining objective for language models in the clinical domain.
Our method formulates EHR segments as temporally ordered sentence pairs and
trains the model to determine whether a later state is entailed by,
contradictory to, or neutral with respect to an earlier state. Through this
temporally structured pretraining task, models learn to perform latent clinical
reasoning over time, improving their ability to generalize across forecasting
and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and
demonstrate state of the art results on temporal clinical QA, early warning
prediction, and disease progression modeling.

</details>


### [14] [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)
*Fida Ullah,Muhammad Ahmad,Muhammad Tayyab Zamir,Muhammad Arif,Grigori sidorov,Edgardo Manuel Felipe Riverón,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 该论文提出了一个针对乌尔都语教育领域的命名实体识别（NER）任务，创建了名为EDU-NER-2025的手工标注数据集，并分析了该领域的语言学挑战。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语在特定领域（如教育）中的命名实体识别研究不足，特别是缺乏标注数据集，限制了模型在识别学术角色、课程名称和机构术语等实体上的准确性。

Method: 创建了一个手工标注的数据集EDU-NER-2025，包含13个教育领域的重要实体；详细描述了标注流程和指南；分析了乌尔都语中的语言学挑战，如形态复杂性和歧义。

Result: 生成了首个乌尔都语教育领域的手工标注数据集，并提供了标注过程的分析和语言学挑战的讨论。

Conclusion: 该研究填补了乌尔都语教育领域NER的空白，为未来研究提供了宝贵资源，并指出了语言学挑战对模型性能的影响。

Abstract: Named Entity Recognition (NER) plays a pivotal role in various Natural
Language Processing (NLP) tasks by identifying and classifying named entities
(NEs) from unstructured data into predefined categories such as person,
organization, location, date, and time. While extensive research exists for
high-resource languages and general domains, NER in Urdu particularly within
domain-specific contexts like education remains significantly underexplored.
This is Due to lack of annotated datasets for educational content which limits
the ability of existing models to accurately identify entities such as academic
roles, course names, and institutional terms, underscoring the urgent need for
targeted resources in this domain. To the best of our knowledge, no dataset
exists in the domain of the Urdu language for this purpose. To achieve this
objective this study makes three key contributions. Firstly, we created a
manually annotated dataset in the education domain, named EDU-NER-2025, which
contains 13 unique most important entities related to education domain. Second,
we describe our annotation process and guidelines in detail and discuss the
challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed
key linguistic challenges, such as morphological complexity and ambiguity,
which are prevalent in formal Urdu texts.

</details>


### [15] [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)
*Þórir Hrafn Harðarson,Hrafn Loftsson,Stefán Ólafsson*

Main category: cs.CL

TL;DR: 研究探讨了偏好训练（如RLHF和DPO）能否改进语言模型生成冰岛法律摘要的法律准确性，结果显示偏好训练在法律准确性上优于传统监督学习，但对提升冰岛语言质量效果不显著。


<details>
  <summary>Details</summary>
Motivation: 法律领域的术语和语言复杂性对语言模型提出了挑战，偏好训练有望提升模型在专业领域的表现。

Method: 比较了偏好训练（RLHF和DPO）与传统监督学习在生成冰岛法律摘要上的效果。

Result: 偏好训练提高了法律准确性，但对语言质量提升不明显；自动指标与人工评估存在差异。

Conclusion: 法律领域语言模型开发需重视定性评估，偏好训练在法律准确性上有优势。

Abstract: The integration of language models in the legal domain holds considerable
promise for streamlining processes and improving efficiency in managing
extensive workloads. However, the specialized terminology, nuanced language,
and formal style of legal texts can present substantial challenges. This study
examines whether preference-based training techniques, specifically
Reinforcement Learning from Human Feedback and Direct Preference Optimization,
can enhance models' performance in generating Icelandic legal summaries that
align with domain-specific language standards and user preferences. We compare
models fine-tuned with preference training to those using conventional
supervised learning. Results indicate that preference training improves the
legal accuracy of generated summaries over standard fine-tuning but does not
significantly enhance the overall quality of Icelandic language usage.
Discrepancies between automated metrics and human evaluations further
underscore the importance of qualitative assessment in developing language
models for the legal domain.

</details>


### [16] [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)
*Shuxiang Du,Ana Guerberof Arenas,Antonio Toral,Kyo Gerrits,Josep Marco Borillo*

Main category: cs.CL

TL;DR: 研究比较了ChatGPT在六种不同配置下对文学文本的翻译表现，发现简洁指令（如'请创造性翻译'）在温度1.0时表现最优，但仍不及人工翻译。


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在不同设置下对文学文本的创造性翻译能力，并与现有工具（如DeepL）及人工翻译进行对比。

Method: 采用六种配置（包括文本粒度、温度设置和提示策略），通过创造力评分公式评估四种语言的翻译结果。

Result: 简洁指令（温度1.0）在西班牙语、荷兰语和中文中表现最佳，但仍逊色于人工翻译。

Conclusion: ChatGPT在创造性翻译中虽有潜力，但现阶段无法替代人工翻译。

Abstract: This study examines the variability of Chat-GPT machine translation (MT)
outputs across six different configurations in four languages,with a focus on
creativity in a literary text. We evaluate GPT translations in different text
granularity levels, temperature settings and prompting strategies with a
Creativity Score formula. We found that prompting ChatGPT with a minimal
instruction yields the best creative translations, with "Translate the
following text into [TG] creatively" at the temperature of 1.0 outperforming
other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,
ChatGPT consistently underperforms compared to human translation (HT).

</details>


### [17] [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)
*Pierre-Carl Langlais,Pavel Chizhov,Mattia Nee,Carlos Rosas Hinostroza,Matthieu Delsart,Irène Girard,Othman Hicheur,Anastasia Stasenko,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 论文介绍了两款新型小型推理模型Pleias-RAG-350m和Pleias-RAG-1B，用于RAG、搜索和源摘要，表现优于4B参数以下的SLM，并与更大的模型竞争。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前小型推理模型在RAG任务中的性能不足，尤其是在多语言和引用基础方面的问题。

Method: 使用大型合成数据集进行预训练，模拟多语言开源检索，支持引用和基础功能，并整合RAG工作流特性如查询路由和重排。

Result: Pleias-RAG-350m和Pleias-RAG-1B在标准RAG基准测试中表现优异，支持多语言且引用基础一致。

Conclusion: 这两款模型因其小规模和易部署性，为生成式AI开辟了新的应用场景。

Abstract: We introduce a new generation of small reasoning models for RAG, search, and
source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a
large synthetic dataset emulating the retrieval of a wide variety of
multilingual open sources from the Common Corpus. They provide native support
for citation and grounding with literal quotes and reintegrate multiple
features associated with RAG workflows, such as query routing, query
reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B
outperform SLMs below 4 billion parameters on standardized RAG benchmarks
(HotPotQA, 2wiki) and are competitive with popular larger models, including
Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date
maintaining consistent RAG performance across leading European languages and
ensuring systematic reference grounding for statements. Due to their size and
ease of deployment on constrained infrastructure and higher factuality by
design, the models unlock a range of new use cases for generative AI.

</details>


### [18] [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
*Ritesh Goru,Shanay Mehta,Prateek Jain*

Main category: cs.CL

TL;DR: 论文提出了一种新方法，通过响应令牌复制和自定义注意力掩码，解决了在多轮推理数据集上微调大语言模型时无法单次前向处理整个对话的难题，显著减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前在多轮推理任务中，大语言模型生成推理标记后无法将其纳入后续输入，导致无法单次处理整个对话，限制了训练效率。

Method: 采用响应令牌复制和自定义注意力掩码技术，确保推理标记在后续输入中可见，从而支持单次前向处理。

Result: 该方法显著降低了训练时间，实现了在多轮推理数据集上的高效微调。

Conclusion: 通过技术创新解决了多轮推理数据集微调的效率问题，为相关任务提供了实用的优化方案。

Abstract: Training Large Language Models ( LLMs) to generate explicit reasoning before
they produce an answer has been shown to improve their performance across
various tasks such as mathematics and coding. However, fine-tuning LLMs on
multi-turn reasoning datasets presents a unique challenge: LLMs must generate
reasoning tokens that are excluded from subsequent inputs to the LLM. This
discrepancy prevents us from processing an entire conversation in a single
forward pass-an optimization readily available when we fine-tune on a
multi-turn non-reasoning dataset. This paper proposes a novel approach that
overcomes this limitation through response token duplication and a custom
attention mask that enforces appropriate visibility constraints. Our approach
significantly reduces the training time and allows efficient fine-tuning on
multi-turn reasoning datasets.

</details>


### [19] [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)
*Guanqun Bi,Zhuang Chen,Zhoufu Liu,Hongkai Wang,Xiyao Xiao,Yuqiang Xie,Wen Zhang,Yongkang Huang,Yuxuan Chen,Libiao Peng,Yi Feng,Minlie Huang*

Main category: cs.CL

TL;DR: MAGI是一个通过多智能体协作将MINI转化为自动化工作流程的框架，结合临床严谨性、对话适应性和可解释推理，提升LLM辅助的心理健康评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型方法未能符合精神科诊断协议，自动化结构化临床访谈可提高心理健康服务的可及性。

Method: MAGI通过四个专业智能体协作：导航代理、自适应问题代理、判断代理和诊断代理，动态导航临床逻辑并生成明确的症状到临床标准的映射。

Result: 在1002名真实参与者中测试，涵盖抑郁、广泛性焦虑、社交焦虑和自杀倾向，证明了MAGI的有效性。

Conclusion: MAGI框架在临床严谨性、对话适应性和可解释推理方面推动了LLM辅助的心理健康评估发展。

Abstract: Automating structured clinical interviews could revolutionize mental
healthcare accessibility, yet existing large language models (LLMs) approaches
fail to align with psychiatric diagnostic protocols. We present MAGI, the first
framework that transforms the gold-standard Mini International Neuropsychiatric
Interview (MINI) into automatic computational workflows through coordinated
multi-agent collaboration. MAGI dynamically navigates clinical logic via four
specialized agents: 1) an interview tree guided navigation agent adhering to
the MINI's branching structure, 2) an adaptive question agent blending
diagnostic probing, explaining, and empathy, 3) a judgment agent validating
whether the response from participants meet the node, and 4) a diagnosis Agent
generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map
symptoms to clinical criteria. Experimental results on 1,002 real-world
participants covering depression, generalized anxiety, social anxiety and
suicide shows that MAGI advances LLM- assisted mental health assessment by
combining clinical rigor, conversational adaptability, and explainable
reasoning.

</details>


### [20] [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)
*Shintaro Ozaki,Kazuki Hayashi,Yusuke Sakai,Jingun Kwon,Hidetaka Kamigaito,Katsuhiko Hayashi,Manabu Okumura,Taro Watanabe*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TextTIGER的方法，通过增强和总结实体相关的描述来优化图像生成提示，实验证明该方法能提升图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决提示中包含的实体知识难以完全记忆的问题，论文提出通过增强实体知识并结合大语言模型（LLMs）总结描述来优化提示。

Method: TextTIGER方法首先增强提示中的实体知识，然后利用LLMs总结扩展的描述以减少长输入的性能下降。

Result: 实验表明，TextTIGER在多种图像生成模型和LLMs上均优于仅使用标题提示的性能，且在标准评估指标（IS、FID、CLIPScore）上表现更佳。

Conclusion: 通过增强和总结实体相关描述来优化提示能有效提升图像生成能力，LLMs生成简洁但信息丰富的描述也得到了验证。

Abstract: Generating images from prompts containing specific entities requires models
to retain as much entity-specific knowledge as possible. However, fully
memorizing such knowledge is impractical due to the vast number of entities and
their continuous emergence. To address this, we propose Text-based Intelligent
Generation with Entity prompt Refinement (TextTIGER), which augments knowledge
on entities included in the prompts and then summarizes the augmented
descriptions using Large Language Models (LLMs) to mitigate performance
degradation from longer inputs. To evaluate our method, we introduce WiT-Cub
(WiT with Captions and Uncomplicated Background-explanations), a dataset
comprising captions, images, and an entity list. Experiments on four image
generation models and five LLMs show that TextTIGER improves image generation
performance in standard metrics (IS, FID, and CLIPScore) compared to
caption-only prompts. Additionally, multiple annotators' evaluation confirms
that the summarized descriptions are more informative, validating LLMs' ability
to generate concise yet rich descriptions. These findings demonstrate that
refining prompts with augmented and summarized entity-related descriptions
enhances image generation capabilities. The code and dataset will be available
upon acceptance.

</details>


### [21] [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
*Toghrul Abbasli,Kentaroh Toyoda,Yuan Wang,Leon Witt,Muhammad Asif Ali,Yukai Miao,Dan Li,Qingsong Wei*

Main category: cs.CL

TL;DR: 本文系统地综述了大语言模型（LLMs）的不确定性量化（UQ）和校准方法，提出了一个全面的基准测试，并评估了六种相关方法的有效性，填补了该领域的文献空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中出现幻觉（输出不正确但自信的信息）是一个主要挑战，而现有文献缺乏对不确定性量化和校准方法的系统分析和有效性评估。

Method: 本文通过文献综述和实证评估，使用两个广泛应用的可靠性数据集，对六种相关方法进行了测试。

Result: 研究结果表明，现有的不确定性量化和校准方法在LLMs上效果不一，并提出了关键发现和基准测试结果。

Conclusion: 文章总结了该领域的关键未来方向，并提出了开放挑战，填补了LLMs校准方法和相关指标综述的空白。

Abstract: Large Language Models (LLMs) have been transformative across many domains.
However, hallucination -- confidently outputting incorrect information --
remains one of the leading challenges for LLMs. This raises the question of how
to accurately assess and quantify the uncertainty of LLMs. Extensive literature
on traditional models has explored Uncertainty Quantification (UQ) to measure
uncertainty and employed calibration techniques to address the misalignment
between uncertainty and accuracy. While some of these methods have been adapted
for LLMs, the literature lacks an in-depth analysis of their effectiveness and
does not offer a comprehensive benchmark to enable insightful comparison among
existing solutions. In this work, we fill this gap via a systematic survey of
representative prior works on UQ and calibration for LLMs and introduce a
rigorous benchmark. Using two widely used reliability datasets, we empirically
evaluate six related methods, which justify the significant findings of our
review. Finally, we provide outlooks for key future directions and outline open
challenges. To the best of our knowledge, this survey is the first dedicated
study to review the calibration methods and relevant metrics for LLMs.

</details>


### [22] [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)
*Lei Shen,Xiaoyu Shen*

Main category: cs.CL

TL;DR: Auto-SLURP 是一个针对基于大语言模型的多智能体框架的基准数据集，扩展了原有的 SLURP 数据集，用于评估智能个人助手的性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估大语言模型多智能体框架性能的基准数据集，因此开发 Auto-SLURP 填补这一空白。

Method: 通过重新标注数据并集成模拟服务器和外部服务，扩展 SLURP 数据集，以支持端到端的评估流程。

Result: 实验表明 Auto-SLURP 对现有最先进框架提出了显著挑战，显示可靠的多智能体个人助手仍需改进。

Conclusion: Auto-SLURP 为多智能体框架评估提供了新标准，相关代码和数据集已开源。

Abstract: In recent years, multi-agent frameworks powered by large language models
(LLMs) have advanced rapidly. Despite this progress, there is still a notable
absence of benchmark datasets specifically tailored to evaluate their
performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset
aimed at evaluating LLM-based multi-agent frameworks in the context of
intelligent personal assistants. Auto-SLURP extends the original SLURP dataset
-- initially developed for natural language understanding tasks -- by
relabeling the data and integrating simulated servers and external services.
This enhancement enables a comprehensive end-to-end evaluation pipeline,
covering language understanding, task execution, and response generation. Our
experiments demonstrate that Auto-SLURP presents a significant challenge for
current state-of-the-art frameworks, highlighting that truly reliable and
intelligent multi-agent personal assistants remain a work in progress. The
dataset and related code are available at
https://github.com/lorashen/Auto-SLURP/.

</details>


### [23] [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的自然语言推理（NLI）方法，使用GRPO进行Chain-of-Thought学习，无需标注数据，并在ANLI等挑战性数据集上取得优越性能。通过参数高效技术微调大模型，32B量化模型在对抗性NLI任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前NLI系统依赖监督学习，数据集存在标注偏差，限制了泛化能力和实际应用。为解决这一问题，作者提出采用强化学习方法，减少对标注数据的依赖。

Method: 使用Group Relative Policy Optimization (GRPO)进行Chain-of-Thought (CoT)学习，并结合LoRA和QLoRA等参数高效技术，微调7B、14B和32B大模型。

Result: 32B AWQ量化模型在11个对抗性NLI任务中超越现有最佳结果，内存占用仅22GB，表明在激进量化下仍能保持强推理能力。

Conclusion: 该研究为构建无需牺牲推理质量的高效NLI系统提供了可扩展且实用的框架。

Abstract: Natural Language Inference (NLI) is a central task in natural language
understanding with applications in fact-checking, question answering, and
information retrieval. Despite its importance, current NLI systems heavily rely
on supervised learning with datasets that often contain annotation artifacts
and biases, limiting generalization and real-world applicability. In this work,
we apply a reinforcement learning-based approach using Group Relative Policy
Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the
need for labeled rationales and enabling this type of training on more
challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language
models using parameter-efficient techniques (LoRA and QLoRA), demonstrating
strong performance across standard and adversarial NLI benchmarks. Our 32B
AWQ-quantized model surpasses state-of-the-art results on 7 out of 11
adversarial sets$\unicode{x2013}$or on all of them considering our
replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust
reasoning can be retained under aggressive quantization. This work provides a
scalable and practical framework for building robust NLI systems without
sacrificing inference quality.

</details>


### [24] [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
*Amir Zeldes,Nina Speransky,Nicholas Wagner,Caroline T. Schroeder*

Main category: cs.CL

TL;DR: 论文介绍了首个Bohairic Coptic（波海利克科普特语）句法标注语料库，并比较了其与Sahidic Coptic（萨希迪克科普特语）的主要差异。


<details>
  <summary>Details</summary>
Motivation: 由于波海利克科普特语作为拜占庭晚期埃及和现代科普特教会的主要方言，在数字资源方面严重不足，因此研究者希望填补这一空白。

Method: 通过构建波海利克科普特语的句法标注语料库，并对其进行评估，同时对比分析其与萨希迪克科普特语的差异，进行跨方言解析实验。

Result: 研究发现波海利克科普特语与萨希迪克科普特语相关但具有独特性。

Conclusion: 该研究为波海利克科普特语的进一步研究提供了重要资源，并揭示了其作为独立方言的特点。

Abstract: Despite recent advances in digital resources for other Coptic dialects,
especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,
late Byzantine Egypt, and the contemporary language of the Coptic Church,
remains critically under-resourced. This paper presents and evaluates the first
syntactically annotated corpus of Bohairic Coptic, sampling data from a range
of works, including Biblical text, saints' lives and Christian ascetic writing.
We also explore some of the main differences we observe compared to the
existing UD treebank of Sahidic Coptic, the classical dialect of the language,
and conduct joint and cross-dialect parsing experiments, revealing the unique
nature of Bohairic as a related, but distinct variety from the more often
studied Sahidic.

</details>


### [25] [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
*Yusen Zhang,Wenliang Zheng,Aashrith Madasu,Peng Shi,Ryo Kamoi,Hao Zhou,Zhuoyang Zou,Shu Zhao,Sarkar Snigdha Sarathi Das,Vipul Gupta,Xiaoxin Lu,Nan Zhang,Ranran Haoran Zhang,Avitej Iyer,Renze Lou,Wenpeng Yin,Rui Zhang*

Main category: cs.CL

TL;DR: 研究者针对高分辨率图像（HRI）理解缺乏全面评估基准的现状，提出了统一基准HRScene，涵盖25个真实数据集和2个合成诊断数据集，并评估了28种VLM模型，发现当前模型在HRI任务中平均准确率仅50%，且存在区域利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大语言模型（VLMs）号称能处理高分辨率图像，但缺乏系统性评估基准。为了填补这一空白，研究者建立了HRScene基准，旨在全面衡量VLM对HRI的理解能力。

Method: 提出了HRScene基准，整合25个真实场景数据集和2个合成诊断数据集（分辨率从1,024×1,024到35,503×26,627），由10名研究生重新标注。通过结合目标图像与干扰图像合成诊断数据集，评估模型对HRI区域的有效利用能力。

Result: 实验结果显示，当前VLMs在真实世界任务中平均准确率约50%，且合成数据集测试表明模型存在‘区域发散’和‘中间丢失’问题，难以有效利用HRI信息。

Conclusion: HRScene揭示了VLMs在高分辨率图像理解上的显著不足，尤其是区域利用效率低的问题，为未来研究提供了重要方向。

Abstract: High-resolution image (HRI) understanding aims to process images with a large
number of pixels, such as pathological images and agricultural aerial images,
both of which can exceed 1 million pixels. Vision Large Language Models (VLMs)
can allegedly handle HRIs, however, there is a lack of a comprehensive
benchmark for VLMs to evaluate HRI understanding. To address this gap, we
introduce HRScene, a novel unified benchmark for HRI understanding with rich
scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic
datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$
26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,
covering 25 scenarios, ranging from microscopic to radiology images, street
views, long-range pictures, and telescope images. It includes HRIs of
real-world objects, scanned documents, and composite multi-image. The two
diagnostic evaluation datasets are synthesized by combining the target image
with the gold answer and distracting images in different orders, assessing how
well models utilize regions in HRI. We conduct extensive experiments involving
28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show
that current VLMs achieve an average accuracy of around 50% on real-world
tasks, revealing significant gaps in HRI understanding. Results on synthetic
datasets reveal that VLMs struggle to effectively utilize HRI regions, showing
significant Regional Divergence and lost-in-middle, shedding light on future
research.

</details>


### [26] [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)
*Jared Moore,Declan Grabb,William Agnew,Kevin Klyman,Stevie Chancellor,Desmond C. Ong,Nick Haber*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLM）是否能替代心理治疗师，通过实验发现LLM在治疗关系中存在表达偏见、处理关键问题不当等问题，并存在无法满足治疗联盟的人类特质等障碍，因此不建议替代治疗师。


<details>
  <summary>Details</summary>
Motivation: 由于科技创业和研究领域推广LLM替代心理健康服务提供者，论文旨在评估这一应用的可行性和潜在问题，尤其是在治疗关系的核心要素方面。

Method: 通过映射主流医疗机构的治疗指南，识别治疗关系的关键要素，并设计实验测试当前LLM（如GPT-4）在满足这些要素时的表现。

Result: 实验表明，LLM存在对心理健康问题的偏见表达、对关键治疗场景（如妄想思维）的不当回应，且现有安全措施无法解决这些问题。此外，LLM缺乏人类治疗联盟所需的特质（如身份认同和利害关系）。

Conclusion: LLM目前不应替代治疗师，论文讨论了LLM在临床治疗中的替代角色。

Abstract: Should a large language model (LLM) be used as a therapist? In this paper, we
investigate the use of LLMs to *replace* mental health providers, a use case
promoted in the tech startup and research space. We conduct a mapping review of
therapy guides used by major medical institutions to identify crucial aspects
of therapeutic relationships, such as the importance of a therapeutic alliance
between therapist and client. We then assess the ability of LLMs to reproduce
and adhere to these aspects of therapeutic relationships by conducting several
experiments investigating the responses of current LLMs, such as `gpt-4o`.
Contrary to best practices in the medical community, LLMs 1) express stigma
toward those with mental health conditions and 2) respond inappropriately to
certain common (and critical) conditions in naturalistic therapy settings --
e.g., LLMs encourage clients' delusional thinking, likely due to their
sycophancy. This occurs even with larger and newer LLMs, indicating that
current safety practices may not address these gaps. Furthermore, we note
foundational and practical barriers to the adoption of LLMs as therapists, such
as that a therapeutic alliance requires human characteristics (e.g., identity
and stakes). For these reasons, we conclude that LLMs should not replace
therapists, and we discuss alternative roles for LLMs in clinical therapy.

</details>


### [27] [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
*Hongyu Wang,Shuming Ma,Furu Wei*

Main category: cs.CL

TL;DR: BitNet v2解决了1位大语言模型中的激活异常问题，通过H-BitLinear模块实现了4位激活量化，减少内存占用和计算成本。


<details>
  <summary>Details</summary>
Motivation: 1位大语言模型由于激活异常问题难以实现低比特量化，BitNet v2旨在解决这一问题，提高模型效率。

Method: 提出H-BitLinear模块，通过在线哈达玛变换平滑激活分布，使其更适合低比特表示。

Result: 实验显示BitNet v2在8位激活下性能与原模型相当，而在4位激活下性能损失极小。

Conclusion: BitNet v2显著提升了1位大语言模型的部署效率，降低了资源消耗。

Abstract: Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by
activation outliers, which complicate quantization to low bit-widths. We
introduce BitNet v2, a novel framework enabling native 4-bit activation
quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward
network activations, we propose H-BitLinear, a module applying an online
Hadamard transformation prior to activation quantization. This transformation
smooths sharp activation distributions into more Gaussian-like forms, suitable
for low-bit representation. Experiments show BitNet v2 trained from scratch
with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2
achieves minimal performance degradation when trained with native 4-bit
activations, significantly reducing memory footprint and computational cost for
batched inference.

</details>


### [28] [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
*Yiming Wang,Pei Zhang,Jialong Tang,Haoran Wei,Baosong Yang,Rui Wang,Chenshu Sun,Feitong Sun,Jiran Zhang,Junxuan Wu,Qiqian Cang,Yichang Zhang,Fei Huang,Junyang Lin,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: PolyMath是一个多语言数学推理基准，涵盖18种语言和4种难度等级，评测发现当前先进大模型在多语言推理中表现不佳，并提出输出语言控制可能改善性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个全面、高质量的多语言数学推理基准，以评测和提升大语言模型在多语言环境下的推理能力。

Method: 设计包含18种语言和4种难度等级的数学问题数据集，并对多种先进大模型进行评测（如Qwen-QwQ-32B等），分析语言一致性和思维长度差异。

Result: 当前先进模型（如Deepseek-R1-671B）在最高难度下的准确率低于30%，且推理性能因语言差异显著，输出语言控制可能影响结果。

Conclusion: 多语言推理仍面临挑战，但通过控制输出语言或能提升低资源语言性能，为改进大模型多语言能力提供方向。

Abstract: In this paper, we introduce PolyMath, a multilingual mathematical reasoning
benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our
benchmark ensures difficulty comprehensiveness, language diversity, and
high-quality translation, making it a highly discriminative multilingual
mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive
evaluation for advanced LLMs and find that even Deepseek-R1-671B and
Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%
accuracy under the highest level. From a language perspective, our benchmark
reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning
performance varies widely across languages for current LLMs; (2) Input-output
language consistency is low in reasoning LLMs and may be correlated with
performance; (3) The thinking length differs significantly by language for
current LLMs. Additionally, we demonstrate that controlling the output language
in the instructions has the potential to affect reasoning performance,
especially for some low-resource languages, suggesting a promising direction
for improving multilingual capabilities in LLMs.

</details>


### [29] [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)
*Wenyi Xiao,Leilei Gan,Weilong Dai,Wanggui He,Ziwei Huang,Haoyuan Li,Fangxun Shu,Zhelun Yu,Peng Zhang,Hao Jiang,Fei Wu*

Main category: cs.CL

TL;DR: 论文提出FAST框架，通过动态调整推理深度来解决大视语言模型（LVLMs）的‘过度思考’问题，实现更高效的推理。


<details>
  <summary>Details</summary>
Motivation: 解决大视语言模型中存在的‘过度思考’现象，即无论问题复杂度如何，模型都生成冗长的推理过程。

Method: 提出FAST框架，包含三个关键组件：基于模型的指标用于问题特征化、自适应思考奖励机制和难度感知的KL正则化。

Result: 在七个推理基准测试中，FAST相比基础模型提升了10%以上的准确率，同时比之前的‘慢思考’方法减少了32.7-67.3%的token使用量。

Conclusion: FAST有效平衡了推理长度和准确性，展示了动态调整推理深度的潜力。

Abstract: Recent advances in large vision-language models (LVLMs) have revealed an
\textit{overthinking} phenomenon, where models generate verbose reasoning
across all tasks regardless of questions. To address this issue, we present
\textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework
that dynamically adapts reasoning depth based on question characteristics.
Through empirical analysis, we establish the feasibility of fast-slow thinking
in LVLMs by investigating how response length and data distribution affect
performance. We develop FAST-GRPO with three components: model-based metrics
for question characterization, an adaptive thinking reward mechanism, and
difficulty-aware KL regularization. Experiments across seven reasoning
benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over
10\% relative improvement compared to the base model, while reducing token
usage by 32.7-67.3\% compared to previous slow-thinking approaches, effectively
balancing reasoning length and accuracy.

</details>


### [30] [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)
*James D. Finch,Yasasvi Josyula,Jinho D. Choi*

Main category: cs.CL

TL;DR: 该论文提出了一种新的任务导向对话系统方法，通过文本生成任务自动识别关键信息槽，并开发了基于LLM的模拟方法生成高质量数据，改进了评估指标。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决传统Slot Schema Induction中需要人工干预的问题，并通过自动化方法提升对话系统的理解和开发。

Method: 将SSI视为文本生成任务，利用语言模型增量构建和优化槽模式；开发基于LLM的TOD模拟方法生成高质数据；改进评估指标。

Result: 实现了新的SoTA方法，解决了数据泄露和评估指标与人类判断不一致的问题。

Conclusion: 该方法为未来SSI研究奠定了基础，并推进了对话理解和系统开发的SoTA。

Abstract: In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is
essential for automatically identifying key information slots from dialogue
data without manual intervention. This paper presents a novel state-of-the-art
(SoTA) approach that formulates SSI as a text generation task, where a language
model incrementally constructs and refines a slot schema over a stream of
dialogue data. To develop this approach, we present a fully automatic LLM-based
TOD simulation method that creates data with high-quality state labels for
novel task domains. Furthermore, we identify issues in SSI evaluation due to
data leakage and poor metric alignment with human judgment. We resolve these by
creating new evaluation data using our simulation method with human guidance
and correction, as well as designing improved evaluation metrics. These
contributions establish a foundation for future SSI research and advance the
SoTA in dialogue understanding and system development.

</details>


### [31] [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
*Leandra Fichtel,Maximilian Spliethöver,Eyke Hüllermeier,Patricia Jimenez,Nils Klowait,Stefan Kopp,Axel-Cyrille Ngonga Ngomo,Amelie Robrecht,Ingrid Scharlau,Lutz Terfloth,Anna-Lisa Vollmer,Henning Wachsmuth*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在共建性解释对话中作为解释者的能力，通过用户实验评估其效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过共建性解释对话提升解释性人工智能的效果，尤其是LLMs的动态适应能力。

Method: 进行用户研究，让被解释者与LLMs互动，评估解释前后的理解程度及对LLMs行为的感知。

Result: LLMs展现部分共建性行为（如提问验证），能提升参与度和理解，但在监控理解和动态调整解释方面有限。

Conclusion: 当前LLMs在共建性解释对话中表现初步潜力，但仍需改进监控和调整能力。

Abstract: The ability to generate explanations that are understood by explainees is the
quintessence of explainable artificial intelligence. Since understanding
depends on the explainee's background and needs, recent research has focused on
co-constructive explanation dialogues, where the explainer continuously
monitors the explainee's understanding and adapts explanations dynamically. We
investigate the ability of large language models (LLMs) to engage as explainers
in co-constructive explanation dialogues. In particular, we present a user
study in which explainees interact with LLMs, of which some have been
instructed to explain a predefined topic co-constructively. We evaluate the
explainees' understanding before and after the dialogue, as well as their
perception of the LLMs' co-constructive behavior. Our results indicate that
current LLMs show some co-constructive behaviors, such as asking verification
questions, that foster the explainees' engagement and can improve understanding
of a topic. However, their ability to effectively monitor the current
understanding and scaffold the explanations accordingly remains limited.

</details>


### [32] [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
*Gwen Yidou Weng,Benjie Wang,Guy Van den Broeck*

Main category: cs.CL

TL;DR: TRACE框架通过引入隐马尔可夫模型和小分类器来高效计算预期属性概率（EAP），从而实现对语言模型输出的全局控制，显著提高了性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，如何控制其输出以符合人类价值观或特定属性（如去毒化、个性化）成为重要需求。现有方法效率低且不灵活，需提出更高效的解决方案。

Method: TRACE框架通过从语言模型中蒸馏出隐马尔可夫模型（HMM）并搭配小型分类器来估计属性概率，实现了精确的EAP计算，并通过重新加权下一标记概率来控制生成内容。

Result: TRACE在去毒化任务中取得最优效果，解码开销仅为10%，可在秒级内适配76个低资源个性化模型，并能无缝扩展至复合属性。

Conclusion: TRACE通过高效的概率推理和轻量级控制，显著提升了语言模型的可控性和适应性，为未来的可控生成提供了新思路。

Abstract: As large language models (LMs) advance, there is an increasing need to
control their outputs to align with human values (e.g., detoxification) or
desired attributes (e.g., personalization, topic). However, autoregressive
models focus on next-token predictions and struggle with global properties that
require looking ahead. Existing solutions either tune or post-train LMs for
each new attribute - expensive and inflexible - or approximate the Expected
Attribute Probability (EAP) of future sequences by sampling or training, which
is slow and unreliable for rare attributes. We introduce TRACE (Tractable
Probabilistic Reasoning for Adaptable Controllable gEneration), a novel
framework that efficiently computes EAP and adapts to new attributes through
tractable probabilistic reasoning and lightweight control. TRACE distills a
Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to
estimate attribute probabilities, enabling exact EAP computation over the HMM's
predicted futures. This EAP is then used to reweigh the LM's next-token
probabilities for globally compliant continuations. Empirically, TRACE achieves
state-of-the-art results in detoxification with only 10% decoding overhead,
adapts to 76 low-resource personalized LLMs within seconds, and seamlessly
extends to composite attributes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [CaRL: Learning Scalable Planning Policies with Simple Rewards](https://arxiv.org/abs/2504.17838)
*Bernhard Jaeger,Daniel Dauner,Jens Beißwenger,Simon Gerstenecker,Kashyap Chitta,Andreas Geiger*

Main category: cs.LG

TL;DR: 该论文探讨了在自动驾驶中使用强化学习（RL）替代传统规则方法，提出了一种基于单一直观奖励（路线完成度）的新奖励设计，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法难以应对复杂的长尾场景，而现有RL方法中的复杂奖励设计限制了可扩展性，因此需要一种更高效的奖励设计。

Method: 通过简化奖励设计（主要以路线完成度为核心），使用PPO优化策略，并在大批次训练中验证其可扩展性。

Result: 在CARLA和nuPlan平台上，该方法显著优于现有RL方法，CARLA上达到64 DS，nuPlan上非反应性和反应性交通得分分别为91.3和90.6。

Conclusion: 简单奖励设计结合PPO和大批次训练，可实现高效扩展并显著提升自动驾驶性能，同时具备跨平台的通用性。

Abstract: We investigate reinforcement learning (RL) for privileged planning in
autonomous driving. State-of-the-art approaches for this task are rule-based,
but these methods do not scale to the long tail. RL, on the other hand, is
scalable and does not suffer from compounding errors like imitation learning.
Contemporary RL approaches for driving use complex shaped rewards that sum
multiple individual rewards, \eg~progress, position, or orientation rewards. We
show that PPO fails to optimize a popular version of these rewards when the
mini-batch size is increased, which limits the scalability of these approaches.
Instead, we propose a new reward design based primarily on optimizing a single
intuitive reward term: route completion. Infractions are penalized by
terminating the episode or multiplicatively reducing route completion. We find
that PPO scales well with higher mini-batch sizes when trained with our simple
reward, even improving performance. Training with large mini-batch sizes
enables efficient scaling via distributed data parallelism. We scale PPO to
300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The
resulting model achieves 64 DS on the CARLA longest6 v2 benchmark,
outperforming other RL methods with more complex rewards by a large margin.
Requiring only minimal adaptations from its use in CARLA, the same method is
the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and
90.6 in reactive traffic on the Val14 benchmark while being an order of
magnitude faster than prior work.

</details>


### [34] [High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures](https://arxiv.org/abs/2504.17857)
*A. J Miller,Fangzhou Yu,Michael Brauckmann,Farbod Farshidian*

Main category: cs.LG

TL;DR: 该论文介绍了在波士顿动力Spot机器人上部署高性能强化学习策略的技术细节，包括模拟到现实的差距度量、优化方法及实现的多项突破性成果。


<details>
  <summary>Details</summary>
Motivation: 旨在展示如何在Spot硬件上端到端部署强化学习策略，并公开训练和部署代码，以促进未来研究。

Method: 使用Wasserstein距离和最大均值差异量化模拟与硬件数据的分布差异，作为CMA-ES优化的评分函数，优化未知参数，训练高质量策略。

Result: 实现了5.2米/秒的运动速度（超默认控制器三倍），抗滑表面、干扰抑制和高敏捷性等多重突破。

Conclusion: 方法有效，策略性能显著，代码公开为后续研究提供支持。

Abstract: This work presents an overview of the technical details behind a high
performance reinforcement learning policy deployment with the Spot RL
Researcher Development Kit for low level motor access on Boston Dynamics Spot.
This represents the first public demonstration of an end to end end
reinforcement learning policy deployed on Spot hardware with training code
publicly available through Nvidia IsaacLab and deployment code available
through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean
Discrepancy to quantify the distributional dissimilarity of data collected on
hardware and in simulation to measure our sim2real gap. We use these measures
as a scoring function for the Covariance Matrix Adaptation Evolution Strategy
to optimize simulated parameters that are unknown or difficult to measure from
Spot. Our procedure for modeling and training produces high quality
reinforcement learning policies capable of multiple gaits, including a flight
phase. We deploy policies capable of over 5.2ms locomotion, more than triple
Spots default controller maximum speed, robustness to slippery surfaces,
disturbance rejection, and overall agility previously unseen on Spot. We detail
our method and release our code to support future work on Spot with the low
level API.

</details>


### [35] [Do We Need Transformers to Play FPS Video Games?](https://arxiv.org/abs/2504.17891)
*Karmanbir Batth,Krish Sethi,Aly Shariff,Leo Shi,Hetul Patel*

Main category: cs.LG

TL;DR: Transformer架构在VizDoom环境中的在线与离线强化学习表现不如传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在强化学习中的潜力，特别是在Doom游戏环境中的在线（DTQN）与离线（DT）设置。

Method: 对比Deep Transformer Q-learning Networks (DTQN)用于在线学习与Decision Transformers (DT)用于离线学习的表现。

Result: Transformer方法在Atari游戏中表现良好，但在VizDoom环境中传统方法优于Transformer。

Conclusion: Transformer在VizDoom环境中的在线与离线强化学习中表现不及传统方法。

Abstract: In this paper, we explore the Transformer based architectures for
reinforcement learning in both online and offline settings within the Doom game
environment. Our investigation focuses on two primary approaches: Deep
Transformer Q- learning Networks (DTQN) for online learning and Decision
Transformers (DT) for offline reinforcement learning. DTQN leverages the
sequential modelling capabilities of Transformers to enhance Q-learning in
partially observable environments,while Decision Transformers repurpose
sequence modelling techniques to enable offline agents to learn from past
trajectories without direct interaction with the environment. We conclude that
while Transformers might have performed well in Atari games, more traditional
methods perform better than Transformer based method in both the settings in
the VizDoom environment.

</details>


### [36] [The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection](https://arxiv.org/abs/2504.17908)
*Luiz Antonio Nicolau Anghinoni,Gustavo Weber Denardin,Jadson Castro Gertrudes,Dalcimar Casanova,Jefferson Tales Oliva*

Main category: cs.LG

TL;DR: 论文提出了一种系统比较时间、频率和时频域EEG数据表示对深度学习模型性能影响的方法，发现频率域数据在癫痫检测中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 癫痫诊断依赖专家手动分析EEG信号，存在主观差异，自动化需求迫切；现有研究未系统评估EEG数据表示对深度学习模型的影响。

Method: 使用深度神经网络对比时间域、频率域和时频域的EEG数据表示，通过统计测试确定最佳数据表示和模型架构。

Result: 频率域数据在癫痫检测中的性能超过97%，显著优于其他表示方法。

Conclusion: 频率域数据是癫痫检测中最优的EEG表示方式，为更准确可靠的自动化检测系统提供了基础。

Abstract: Epilepsy, affecting approximately 50 million people globally, is
characterized by abnormal brain activity and remains challenging to treat. The
diagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where
specialists manually analyze epileptiform patterns across pre-ictal, ictal,
post-ictal, and interictal periods. However, the manual analysis of EEG signals
is prone to variability between experts, emphasizing the need for automated
solutions. Although previous studies have explored preprocessing techniques and
machine learning approaches for seizure detection, there is a gap in
understanding how the representation of EEG data (time, frequency, or
time-frequency domains) impacts the predictive performance of deep learning
models. This work addresses this gap by systematically comparing deep neural
networks trained on EEG data in these three domains. Through the use of
statistical tests, we identify the optimal data representation and model
architecture for epileptic seizure detection. The results demonstrate that
frequency-domain data achieves detection metrics exceeding 97\%, providing a
robust foundation for more accurate and reliable seizure detection systems.

</details>


### [37] [CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity](https://arxiv.org/abs/2504.17913)
*Mert Sonmezer,Seyda Ertekin*

Main category: cs.LG

TL;DR: 提出了一种名为CANet的新型架构，用于解决长期时间序列预测中的非平稳性问题，通过非平稳自适应归一化模块等技术，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界数据的非平稳性（如分布偏移和统计属性变化），传统模型容易出现过平稳化问题，导致预测性能下降，因此需要一种新的方法来有效处理这些挑战。

Method: CANet结合了非平稳自适应归一化模块（包括Style Blending Gate和AdaIN）、多分辨率分块、傅里叶分析自适应阈值和堆叠克罗内克积层，以动态适应统计变化并保持时间依赖性。

Result: 在真实数据集上的实验表明，CANet在均方误差（MSE）和平均绝对误差（MAE）上分别减少了42%和22%，优于现有方法。

Conclusion: CANet通过创新的架构设计，成功解决了非平稳时间序列预测的挑战，为长期预测提供了更高效的解决方案。

Abstract: Long-term time series forecasting plays a pivotal role in various real-world
applications. Despite recent advancements and the success of different
architectures, forecasting is often challenging due to non-stationary nature of
the real-world data, which frequently exhibit distribution shifts and temporal
changes in statistical properties like mean and variance over time. Previous
studies suggest that this inherent variability complicates forecasting,
limiting the performance of many models by leading to loss of non-stationarity
and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To
address this challenge, we introduce a novel architecture, ChoronoAdaptive
Network (CANet), inspired by style-transfer techniques. The core of CANet is
the Non-stationary Adaptive Normalization module, seamlessly integrating the
Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and
Belongie, 2017). The Style Blending Gate preserves and reintegrates
non-stationary characteristics, such as mean and standard deviation, by
blending internal and external statistics, preventing over-stationarization
while maintaining essential temporal dependencies. Coupled with AdaIN, which
dynamically adapts the model to statistical changes, this approach enhances
predictive accuracy under non-stationary conditions. CANet also employs
multi-resolution patching to handle short-term fluctuations and long-term
trends, along with Fourier analysis-based adaptive thresholding to reduce
noise. A Stacked Kronecker Product Layer further optimizes the model's
efficiency while maintaining high performance. Extensive experiments on
real-world datasets validate CANet's superiority over state-of-the-art methods,
achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is
publicly available at https://github.com/mertsonmezer/CANet.

</details>


### [38] [Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts](https://arxiv.org/abs/2504.17921)
*Mateo Espinosa Zarlenga,Gabriele Dominici,Pietro Barbiero,Zohreh Shams,Mateja Jamnik*

Main category: cs.LG

TL;DR: 本文研究了基于概念的模型（CMs）对分布外（OOD）输入的响应，揭示了当前最先进CMs的“泄漏中毒”问题，并提出了一种新模型MixCEM来动态利用泄漏信息。


<details>
  <summary>Details</summary>
Motivation: 探讨概念干预对CMs在OOD输入情况下的任务预测影响，并识别当前CMs的局限性。

Method: 提出MixCEM模型，动态利用泄漏信息以提高在分布内和OOD输入下的准确率。

Result: MixCEM在有无概念干预的情况下，均显著优于基线模型。

Conclusion: MixCEM有效解决了CMs在OOD输入时的问题，提高了模型的鲁棒性和准确性。

Abstract: In this paper, we investigate how concept-based models (CMs) respond to
out-of-distribution (OOD) inputs. CMs are interpretable neural architectures
that first predict a set of high-level concepts (e.g., stripes, black) and then
predict a task label from those concepts. In particular, we study the impact of
concept interventions (i.e., operations where a human expert corrects a CM's
mispredicted concepts at test time) on CMs' task predictions when inputs are
OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we
term leakage poisoning, that prevents them from properly improving their
accuracy when intervened on for OOD inputs. To address this, we introduce
MixCEM, a new CM that learns to dynamically exploit leaked information missing
from its concepts only when this information is in-distribution. Our results
across tasks with and without complete sets of concept annotations demonstrate
that MixCEMs outperform strong baselines by significantly improving their
accuracy for both in-distribution and OOD samples in the presence and absence
of concept interventions.

</details>


### [39] [Causality-Driven Neural Network Repair: Challenges and Opportunities](https://arxiv.org/abs/2504.17946)
*Fatemeh Vares,Brittany Johnson*

Main category: cs.LG

TL;DR: 该论文探讨了通过因果推断（如因果调试、反事实分析和结构因果模型）来修复深度神经网络的局限性，以提升其鲁棒性和可解释性，并讨论了在公平性、对抗鲁棒性和后门缓解中的应用及挑战。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）通常依赖统计相关性而非因果推理，导致其鲁棒性和可解释性受限。当前的测试方法虽能发现问题，但有效的调试和修复仍具挑战性。

Method: 采用因果推断方法，包括因果调试、反事实分析和结构因果模型（SCMs），以识别和修正DNN的故障。

Result: 这些技术能够通过针对性干预支持公平性、对抗鲁棒性和后门缓解，但面临可扩展性、泛化性和计算效率等挑战。

Conclusion: 未来需进一步整合因果驱动的干预，以提升DNN的可靠性。

Abstract: Deep Neural Networks (DNNs) often rely on statistical correlations rather
than causal reasoning, limiting their robustness and interpretability. While
testing methods can identify failures, effective debugging and repair remain
challenging. This paper explores causal inference as an approach primarily for
DNN repair, leveraging causal debugging, counterfactual analysis, and
structural causal models (SCMs) to identify and correct failures. We discuss in
what ways these techniques support fairness, adversarial robustness, and
backdoor mitigation by providing targeted interventions. Finally, we discuss
key challenges, including scalability, generalization, and computational
efficiency, and outline future directions for integrating causality-driven
interventions to enhance DNN reliability.

</details>


### [40] [Mathematics of Continual Learning](https://arxiv.org/abs/2504.17963)
*Liangzu Peng,René Vidal*

Main category: cs.LG

TL;DR: 该论文比较了持续学习和自适应滤波的数学基础，并探讨了如何利用自适应滤波的成果来增强持续学习的理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习的数学基础尚不完善，而自适应滤波在信号处理领域有丰富的数学理论。通过对比两者，可以借鉴自适应滤波的理论来提升持续学习的理论基础。

Method: 回顾了持续学习和自适应滤波的基本原理，并进行比较分析，找出两者之间的联系。

Result: 发现自适应滤波的理论可以增强持续学习的数学基础，同时也拓展了自适应滤波的视角。

Conclusion: 论文提出了基于自适应滤波历史的持续学习研究方向，为未来发展提供了新的思路。

Abstract: Continual learning is an emerging subject in machine learning that aims to
solve multiple tasks presented sequentially to the learner without forgetting
previously learned tasks. Recently, many deep learning based approaches have
been proposed for continual learning, however the mathematical foundations
behind existing continual learning methods remain underdeveloped. On the other
hand, adaptive filtering is a classic subject in signal processing with a rich
history of mathematically principled methods. However, its role in
understanding the foundations of continual learning has been underappreciated.
In this tutorial, we review the basic principles behind both continual learning
and adaptive filtering, and present a comparative analysis that highlights
multiple connections between them. These connections allow us to enhance the
mathematical foundations of continual learning based on existing results for
adaptive filtering, extend adaptive filtering insights using existing continual
learning methods, and discuss a few research directions for continual learning
suggested by the historical developments in adaptive filtering.

</details>


### [41] [Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation](https://arxiv.org/abs/2504.18003)
*Aditya S Ellendula,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 提出了一种动态自平衡八叉树数据结构，支持在动态度量空间中高效维护邻近关系，适用于机器学习中动态系统的空间组织需求。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统中，学习和生成模型的表示在训练过程中会动态演化，需要一种能快速自适应组织空间的数据结构。

Method: 基于双参数的八叉树结构，支持对数时间复杂度的更新和查询，避免了数据分布变化时的昂贵全重建。

Result: 在四个应用中展示了高效性：加速Stein变分梯度下降、实现实时增量KNN分类、支持检索增强生成的动态索引、优化输入和潜在空间的样本效率。

Conclusion: 该方法在所有应用中均实现了指数级加速，同时保持精度，尤其在高维空间中表现突出。

Abstract: We present a dynamic self-balancing octree data structure that enables
efficient neighborhood maintenance in evolving metric spaces, a key challenge
in modern machine learning systems. Many learning and generative models operate
as dynamical systems whose representations evolve during training, requiring
fast, adaptive spatial organization. Our two-parameter octree supports
logarithmic-time updates and queries, eliminating the need for costly full
rebuilds as data distributions shift. We demonstrate its effectiveness in four
areas: (1) accelerating Stein variational gradient descent by supporting more
particles with lower overhead; (2) enabling real-time, incremental KNN
classification with logarithmic complexity; (3) facilitating efficient, dynamic
indexing and retrieval for retrieval-augmented generation; and (4) improving
sample efficiency by jointly optimizing input and latent spaces. Across all
applications, our approach yields exponential speedups while preserving
accuracy, particularly in high-dimensional spaces where maintaining adaptive
spatial structure is critical.

</details>


### [42] [TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors](https://arxiv.org/abs/2504.18008)
*Nooshin Yousefzadeh,Rahul Sengupta,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了Temporal Graph-based Digital Twin (TGDT)框架，结合时序卷积网络和注意力图神经网络，解决城市交通拥堵问题，具有高扩展性和实时性。


<details>
  <summary>Details</summary>
Motivation: 城市信号灯交叉口拥堵导致延误、经济损失和排放增加，现有深度学习模型空间泛化性差且复杂。

Method: TGDT框架整合了时序卷积网络和注意力图神经网络，动态建模交通流，优化多项关键效果指标。

Result: 优于现有基准模型，能高精度生成多维并发输出，适应各种交通条件且高效实时。

Conclusion: TGDT为交通信号优化提供了一种可扩展、高精度且实时的解决方案。

Abstract: Urban congestion at signalized intersections leads to significant delays,
economic losses, and increased emissions. Existing deep learning models often
lack spatial generalizability, rely on complex architectures, and struggle with
real-time deployment. To address these limitations, we propose the Temporal
Graph-based Digital Twin (TGDT), a scalable framework that integrates Temporal
Convolutional Networks and Attentional Graph Neural Networks for dynamic,
direction-aware traffic modeling and assessment at urban corridors. TGDT
estimates key Measures of Effectiveness (MOEs) for traffic flow optimization at
both the intersection level (e.g., queue length, waiting time) and the corridor
level (e.g., traffic volume, travel time). Its modular architecture and
sequential optimization scheme enable easy extension to any number of
intersections and MOEs. The model outperforms state-of-the-art baselines by
accurately producing high-dimensional, concurrent multi-output estimates. It
also demonstrates high robustness and accuracy across diverse traffic
conditions, including extreme scenarios, while relying on only a minimal set of
traffic features. Fully parallelized, TGDT can simulate over a thousand
scenarios within a matter of seconds, offering a cost-effective, interpretable,
and real-time solution for traffic signal optimization.

</details>


### [43] [Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization](https://arxiv.org/abs/2504.18026)
*Emiliano Penaloza,Tianyue H. Zhan,Laurent Charlin,Mateo Espinosa Zarlenga*

Main category: cs.LG

TL;DR: CPO（Concept Preference Optimization）提出了一种新的损失函数，通过优化概念后验分布来缓解概念标签错误对CBM（Concept Bottleneck Models）性能的影响，实验证明CPO在标签噪声下优于BCE。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中的概念标签常存在错误，这会显著降低CBM的性能（如某些情况下降低25%），因此需要一种对标签噪声更鲁棒的方法。

Method: 提出CPO目标，基于Direct Preference Optimization设计，旨在直接优化概念后验分布，减少对标签噪声的敏感性。

Result: 在三个真实数据集（含噪声和无噪声）上，CPO均优于传统的BCE（Binary Cross Entropy）。

Conclusion: CPO通过优化概念后验分布，有效提升了CBM在标签噪声下的鲁棒性和性能。

Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI
systems by constraining their decisions on a set of human understandable
concepts. However, CBMs typically assume that datasets contains accurate
concept labels an assumption often violated in practice, which we show can
significantly degrade performance (by 25% in some cases). To address this, we
introduce the Concept Preference Optimization (CPO) objective, a new loss
function based on Direct Preference Optimization, which effectively mitigates
the negative impact of concept mislabeling on CBM performance. We provide an
analysis on some key properties of the CPO objective showing it directly
optimizes for the concept's posterior distribution, and contrast it against
Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to
concept noise. We empirically confirm our analysis finding that CPO
consistently outperforms BCE in three real world datasets with and without
added label noise.

</details>


### [44] [Modes of Sequence Models and Learning Coefficients](https://arxiv.org/abs/2504.18048)
*Zhongtian Chen,Daniel Murfet*

Main category: cs.LG

TL;DR: 这篇论文提出了一个几何视角的序列建模方法，通过将条件序列分布映射到希尔伯特空间并利用张量分解识别主模式，从而有效简化数据分布。理论证明局部学习系数（LLC）对低于特定阈值的模式不敏感，解释了为何在网络参数未严格最小化总体损失时仍能获得可靠的LLC估计。


<details>
  <summary>Details</summary>
Motivation: 为了解决序列建模中对数据分布复杂性的理解问题，作者希望从几何角度分析损失景观的属性和数据模式之间的联系。

Method: 通过将条件序列分布嵌入希尔伯特空间框架，应用张量分解识别主模式并截断小幅度模式，从而得到有效的简化数据分布。理论分析了局部学习系数（LLC）对低幅度模式的鲁棒性。

Result: 研究发现LLC对低于数据依赖阈值的模式不敏感，因此实际计算得到的LLC反映了简化分布而非真实分布的几何特性。这一发现解释了为何在网络参数未严格最小化总体损失时，仍能获得可靠的LLC估计。

Conclusion: 论文揭示了通过几何方法简化数据分布的有效性，并阐明了LLC在实践中的鲁棒性，为理解序列建模中的损失景观提供了新的视角。

Abstract: We develop a geometric account of sequence modelling that links patterns in
the data to measurable properties of the loss landscape in transformer
networks. First, we cast conditional sequence distributions into a
Hilbert-space framework and apply tensor decompositions to identify their
principal modes. Truncating the small-amplitude modes yields an effective data
distribution that preserves dominant structure while discarding statistical
detail. Second, we show theoretically that Local Learning Coefficient (LLC)
estimates are insensitive to modes below a data-dependent threshold.
Consequently, the LLC calculated in practice characterises the geometry of the
effective rather than the true distribution. This insight clarifies why
reliable LLC estimates can be obtained even when a network parameter is not a
strict minimiser of the population loss, and it highlights how the inverse
temperature in SGLD acts as a resolution dial on the landscape structure.

</details>


### [45] [A Model Zoo on Phase Transitions in Neural Networks](https://arxiv.org/abs/2504.18072)
*Konstantin Schürholt,Léo Meynent,Yefan Zhou,Haiquan Lu,Yaoqing Yang,Damian Borth*

Main category: cs.LG

TL;DR: 通过结合模型动物园和相位信息，本文提出了一种系统化多样性的方法，创建了12个大型模型动物园，覆盖不同模态，并探索了其在权重空间学习及其他应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有模型动物园缺乏结构化多样性，且未能结合神经网络中的相位信息。本文旨在通过系统化覆盖不同相位，为权重空间学习提供更丰富的数据资源。

Method: 结合相位信息创建12个大型模型动物园，覆盖不同架构、规模和数据集，计算损失景观指标并验证相位覆盖。

Result: 开发了覆盖多种模态的模型动物园，验证了相位信息在模型训练、分析等应用中的重要性，并通过下游任务（如迁移学习）展示了其效用。

Conclusion: 系统化的模型动物园为权重空间学习提供了新资源，相位信息在模型应用中具有潜在重要性。

Abstract: Using the weights of trained Neural Network (NN) models as data modality has
recently gained traction as a research field - dubbed Weight Space Learning
(WSL). Multiple recent works propose WSL methods to analyze models, evaluate
methods, or synthesize weights. Weight space learning methods require
populations of trained models as datasets for development and evaluation.
However, existing collections of models - called `model zoos' - are
unstructured or follow a rudimentary definition of diversity. In parallel, work
rooted in statistical physics has identified phases and phase transitions in NN
models. Models are homogeneous within the same phase but qualitatively differ
from one phase to another. We combine the idea of `model zoos' with phase
information to create a controlled notion of diversity in populations. We
introduce 12 large-scale zoos that systematically cover known phases and vary
over model architecture, size, and datasets. These datasets cover different
modalities, such as computer vision, natural language processing, and
scientific ML. For every model, we compute loss landscape metrics and validate
full coverage of the phases. With this dataset, we provide the community with a
resource with a wide range of potential applications for WSL and beyond.
Evidence suggests the loss landscape phase plays a role in applications such as
model training, analysis, or sparsification. We demonstrate this in an
exploratory study of the downstream methods like transfer learning or model
weights averaging.

</details>


### [46] [Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity](https://arxiv.org/abs/2504.18078)
*Xiaolu Chen,Chenghao Huang,Yanru Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种隐私保护的分布式光伏分解框架，采用个性化联邦学习（PFL）解决因地理位置和行为差异导致的统计异质性挑战，通过局部和全局建模的结合提升了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 分布式光伏（PV）安装快速增长，但不可观测的PV发电加剧了供需平衡的复杂性。隐私问题和大规模训练数据的需求使得联邦学习成为有前景的解决方案，但统计异质性带来了新挑战。

Method: 提出了一个两层的PFL框架：局部层面采用基于Transformer的PV分解模型生成太阳辐射嵌入，并通过自适应局部聚合机制减轻统计异质性影响；全局层面通过中心服务器聚合多数据中心信息，实现跨中心知识共享。

Result: 在真实数据上的实验表明，该框架相比基准方法在准确性和鲁棒性上均有提升。

Conclusion: 通过隐私保护的PFL框架有效解决了PV分解中的统计异质性挑战，为分布式能源管理提供了新思路。

Abstract: The rapid expansion of distributed photovoltaic (PV) installations worldwide,
many being behind-the-meter systems, has significantly challenged energy
management and grid operations, as unobservable PV generation further
complicates the supply-demand balance. Therefore, estimating this generation
from net load, known as PV disaggregation, is critical. Given privacy concerns
and the need for large training datasets, federated learning becomes a
promising approach, but statistical heterogeneity, arising from geographical
and behavioral variations among prosumers, poses new challenges to PV
disaggregation. To overcome these challenges, a privacy-preserving distributed
PV disaggregation framework is proposed using Personalized Federated Learning
(PFL). The proposed method employs a two-level framework that combines local
and global modeling. At the local level, a transformer-based PV disaggregation
model is designed to generate solar irradiance embeddings for representing
local PV conditions. A novel adaptive local aggregation mechanism is adopted to
mitigate the impact of statistical heterogeneity on the local model, extracting
a portion of global information that benefits the local model. At the global
level, a central server aggregates information uploaded from multiple data
centers, preserving privacy while enabling cross-center knowledge sharing.
Experiments on real-world data demonstrate the effectiveness of this proposed
framework, showing improved accuracy and robustness compared to benchmark
methods.

</details>


### [47] [Efficient GNN Training Through Structure-Aware Randomized Mini-Batching](https://arxiv.org/abs/2504.18082)
*Vignesh Balaji,Christos Kozyrakis,Gal Chechik,Haggai Maron*

Main category: cs.LG

TL;DR: COMM-RAND是一种结合社区结构感知与随机性的GNN小批量训练方法，显著提升训练效率且保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有GNN小批量训练方法在随机性和结构效率之间存在权衡，随机性影响缓存效率，而纯结构方法又损害模型精度和收敛速度。

Method: 提出COMM-RAND方法，通过平衡社区结构感知与随机性优化小批量构建，提升训练效率。

Result: 在四个图学习基准测试中，COMM-RAND平均减少1.8倍训练时间，精度损失仅0.42%。

Conclusion: COMM-RAND在训练效率和模型精度之间实现了更好的平衡，适用于大规模图学习任务。

Abstract: Graph Neural Networks (GNNs) enable learning on realworld graphs and
mini-batch training has emerged as the de facto standard for training GNNs
because it can scale to very large graphs and improve convergence. Current
mini-batch construction policies largely ignore efficiency considerations of
GNN training. Specifically, existing mini-batching techniques employ
randomization schemes to improve accuracy and convergence. However, these
randomization schemes are often agnostic to the structural properties of the
graph (for eg. community structure), resulting in highly irregular memory
access patterns during GNN training that make suboptimal use of on-chip GPU
caches. On the other hand, while deterministic mini-batching based solely on
graph structure delivers fast runtime performance, the lack of randomness
compromises both the final model accuracy and training convergence speed. In
this paper, we present Community-structure-aware Randomized Mini-batching
(COMM-RAND), a novel methodology that bridges the gap between the above
extremes. COMM-RAND allows practitioners to explore the space between pure
randomness and pure graph structural awareness during mini-batch construction,
leading to significantly more efficient GNN training with similar accuracy. We
evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND
cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an
accuracy that is within 1.79% points (0.42% on average) compared to popular
random mini-batching approaches.

</details>


### [48] [Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning](https://arxiv.org/abs/2504.18091)
*Shota Deguchi,Mitsuteru Asai*

Main category: cs.LG

TL;DR: 该论文提出了一种基于距离函数（R函数）的方法来解决物理信息神经网絡（PINN）中边界条件处理的不足，展示了该方法在正向和反问题中的高效性与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于惩罚的方法无法精确满足边界条件且对惩罚参数敏感，影响了PINN求解的精度。论文旨在通过距离函数克服这些局限，提升边界条件的精确处理能力。

Method: 利用R函数生成归一化距离场，精确表示边界几何（包括非凸域），并支持多种边界条件。进一步扩展至反问题，结合自适应权重调整技术以提高反分析的可靠性。

Result: 数值实验表明，该方法在复杂非凸几何下比惩罚法更准确高效，尤其适用于反问题求解。

Conclusion: 该研究为PINN提供了一个可靠且高效的反分析框架，在工程问题中具有广泛应用潜力。

Abstract: Physics-informed neural networks have attracted significant attention in
scientific machine learning for their capability to solve forward and inverse
problems governed by partial differential equations. However, the accuracy of
PINN solutions is often limited by the treatment of boundary conditions.
Conventional penalty-based methods, which incorporate boundary conditions as
penalty terms in the loss function, cannot guarantee exact satisfaction of the
given boundary conditions and are highly sensitive to the choice of penalty
parameters. This paper demonstrates that distance functions, specifically
R-functions, can be leveraged to enforce boundary conditions, overcoming these
limitations. R-functions provide normalized distance fields, enabling accurate
representation of boundary geometries, including non-convex domains, and
facilitating various types of boundary conditions. We extend this distance
function-based boundary condition imposition method to inverse problems using
PINNs and introduce an adaptive weight tuning technique to ensure reliable and
efficient inverse analysis. We demonstrate the efficacy of the method through
several numerical experiments. Numerical results show that the proposed method
solves inverse problems more accurately and efficiently than penalty-based
methods, even in the presence of complex non-convex geometries. This approach
offers a reliable and efficient framework for inverse analysis using PINNs,
with potential applications across a wide range of engineering problems.

</details>


### [49] [Subject-independent Classification of Meditative State from the Resting State using EEG](https://arxiv.org/abs/2504.18095)
*Jerrin Thomas Panachakel,Pradeep Kumar G.,Suryaa Seran,Kanishka Sharma,Ramakrishnan Angarai Ganesan*

Main category: cs.LG

TL;DR: 该研究通过EEG数据区分冥想状态与静息状态，提出并评估了三种架构，其中CSP-LDA-LSTM在个体内分类中表现出最高准确率（98.2%），而SVD-NN在个体间分类中表现优异（96.4%），展示了跨个体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖个体特定数据，而本研究旨在以独立于个体的方式区分Rajyoga冥想状态与静息状态，解决现有方法的局限性。

Method: 提出三种架构：CSP-LDA（特征提取与分类）、CSP-LDA-LSTM（序列学习问题建模）和SVD-NN（特征选择与分类），基于EEG数据进行评估。

Result: CSP-LDA-LSTM在个体内分类中准确率达98.2%，SVD-NN在个体间分类中达96.4%，均优于文献中的最佳结果，展示了跨个体分类能力。

Conclusion: 所提架构能有效捕捉跨个体的EEG特征，高准确率表明其鲁棒性和泛化能力，为冥想状态识别提供了可靠工具。

Abstract: While it is beneficial to objectively determine whether a subject is
meditating, most research in the literature reports good results only in a
subject-dependent manner. This study aims to distinguish the modified state of
consciousness experienced during Rajyoga meditation from the resting state of
the brain in a subject-independent manner using EEG data. Three architectures
have been proposed and evaluated: The CSP-LDA Architecture utilizes common
spatial pattern (CSP) for feature extraction and linear discriminant analysis
(LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature
extraction, LDA for dimensionality reduction, and long short-term memory (LSTM)
networks for classification, modeling the binary classification problem as a
sequence learning problem. The SVD-NN Architecture uses singular value
decomposition (SVD) to select the most relevant components of the EEG signals
and a shallow neural network (NN) for classification. The CSP-LDA-LSTM
architecture gives the best performance with 98.2% accuracy for intra-subject
classification. The SVD-NN architecture provides significant performance with
96.4\% accuracy for inter-subject classification. This is comparable to the
best-reported accuracies in the literature for intra-subject classification.
Both architectures are capable of capturing subject-invariant EEG features for
effectively classifying the meditative state from the resting state. The high
intra-subject and inter-subject classification accuracies indicate these
systems' robustness and their ability to generalize across different subjects.

</details>


### [50] [Temperature Estimation in Induction Motors using Machine Learning](https://arxiv.org/abs/2504.18105)
*Dinan Li,Panagiotis Kakosimos*

Main category: cs.LG

TL;DR: 论文探讨了利用机器学习方法估计感应电机定子绕组和轴承温度的数据驱动方法，包括从线性模型到神经网络的不同算法，实验表明神经网络在瞬态条件下表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着电动动力系统的普及，确保其可靠运行和预防故障变得至关重要。监测电机内部温度是重要一步，但传统建模方法需要专家知识和复杂数学，而现代电动驱动系统收集的大量数据使得数据驱动方法成为可能。

Method: 研究采用多种机器学习方法（从线性模型到神经网络）对感应电机的定子绕组和轴承温度进行估计。通过实验室实验数据，对每种方法进行超参数优化，并以多种指标评估模型性能。

Result: 研究发现，即使在瞬态条件下，神经网络方法也能提供满意的温度估计结果。

Conclusion: 数据驱动方法，特别是神经网络，能够有效用于电机温度的实时监测，为电动动力系统的可靠运行提供了新途径。

Abstract: The number of electrified powertrains is ever increasing today towards a more
sustainable future; thus, it is essential that unwanted failures are prevented,
and a reliable operation is secured. Monitoring the internal temperatures of
motors and keeping them under their thresholds is an important first step.
Conventional modeling methods require expert knowledge and complicated
mathematical approaches. With all the data a modern electric drive collects
nowadays during the system operation, it is feasible to apply data-driven
approaches for estimating thermal behaviors. In this paper, multiple
machine-learning methods are investigated on their capability to approximate
the temperatures of the stator winding and bearing in induction motors. The
explored algorithms vary from linear to neural networks. For this reason,
experimental lab data have been captured from a powertrain under predetermined
operating conditions. For each approach, a hyperparameter search is then
performed to find the optimal configuration. All the models are evaluated by
various metrics, and it has been found that neural networks perform
satisfactorily even under transient conditions.

</details>


### [51] [Learning from Less: SINDy Surrogates in RL](https://arxiv.org/abs/2504.18113)
*Aniket Dixit,Muhammad Ibrahim Khan,Faizan Ahmed,James Brusey*

Main category: cs.LG

TL;DR: 本文提出了一种基于SINDy算法的强化学习代理环境构建方法，在OpenAI Gym环境中验证了其有效性，显著降低了计算成本并保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 为减少强化学习中的计算开销并保持环境动态的准确性，研究探索了基于SINDy的代理环境构建方法。

Method: 采用SINDy算法从少量交互数据中稀疏识别非线性动态，构建代理环境。

Result: 代理模型在Mountain Car和Lunar Lander环境中动态捕捉准确（相关性>0.997），计算成本降低20-35%，且训练步骤显著减少（如Mountain Car从100,000降至65,075步）。

Conclusion: 该方法为基于模型的强化学习提供了一种高效、可解释的代理环境生成方案，验证了其在节约资源与保持性能上的优势。

Abstract: This paper introduces an approach for developing surrogate environments in
reinforcement learning (RL) using the Sparse Identification of Nonlinear
Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach
through extensive experiments in OpenAI Gym environments, particularly Mountain
Car and Lunar Lander. Our results show that SINDy-based surrogate models can
accurately capture the underlying dynamics of these environments while reducing
computational costs by 20-35%. With only 75 interactions for Mountain Car and
1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with
mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06
for LunarLander position. RL agents trained in these surrogate environments
require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs.
1,000,000 for Lunar Lander) while achieving comparable performance to those
trained in the original environments, exhibiting similar convergence patterns
and final performance metrics. This work contributes to the field of
model-based RL by providing an efficient method for generating accurate,
interpretable surrogate environments.

</details>


### [52] [Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models](https://arxiv.org/abs/2504.18116)
*Caia Costello,Simon Guo,Anna Goldie,Azalia Mirhoseini*

Main category: cs.LG

TL;DR: 该论文提出了一种名为'Think, Prune, Train'的可扩展框架，通过利用模型自身生成的推理轨迹和高品质训练数据的修剪策略，显著提升了语言模型（LLMs）在编程和数学任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在编程和数学推理任务中表现优异，但受限于高质量训练数据的稀缺性。论文旨在探索如何利用合成数据增强模型性能，并研究哪些条件能够促进模型的自我改进。

Method: 提出了'Think, Prune, Train'流程，通过迭代微调模型自身的推理轨迹，并使用真实值修剪技术确保高质量训练数据。该方法综合考察了模型大小、合成数据量、修剪策略及微调轮次等因素的影响。

Result: 在GSM8K任务上的实验表明，该方法显著提升了模型性能：Gemma2-2B的Pass@1得分从41.9%提高到57.6%，Gemma2-9B达到82%，与LLaMA-3.1-70B持平；LLaMA-3.1-70B更是达到91%，甚至超过了GPT-4o。

Conclusion: 通过系统性数据选择和模型自身生成的推理能力，可以显著提升LLMs的性能，展示了该方法在低质数据环境下的有效性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
programming and mathematical reasoning tasks, but are constrained by limited
high-quality training data. Synthetic data can be leveraged to enhance
fine-tuning outcomes, but several factors influence this process, including
model size, synthetic data volume, pruning strategy, and number of fine-tuning
rounds. We explore these axes and investigate which conditions enable model
self-improvement. We introduce the Think, Prune, Train process, a scalable
framework that iteratively fine-tunes models on their own reasoning traces,
using ground-truth pruning to ensure high-quality training data. This approach
yields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%
(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B
attains 91%, even surpassing GPT-4o, demonstrating the effectiveness of
self-generated reasoning and systematic data selection for improving LLM
capabilities.

</details>


### [53] [Score-Based Deterministic Density Sampling](https://arxiv.org/abs/2504.18130)
*Vasily Ilin,Bamdad Hosseini,Jingwei Hu*

Main category: cs.LG

TL;DR: 提出了基于Score-Based Transport Modeling（SBTM）的确定性采样框架，用于采样非归一化目标密度π，解决了仅知∇logπ的挑战性问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散生成模型依赖预训练得分函数∇log f_t，而SBTM针对更一般且挑战性的情景，即仅知∇logπ，旨在通过动态学习得分函数优化KL散度。

Method: SBTM通过分数匹配动态学习时变得分函数∇log f_t，近似Wasserstein梯度流，提供收敛诊断，轨迹平滑且无布朗噪声。

Result: 理论证明SBTM能以与精确梯度流相同的速率耗散相对熵，数值实验验证其最优收敛速率及与退火动力学的兼容性。

Conclusion: SBTM在仅知∇logπ时表现优异，轨迹平滑且易于整合退火动力学，优于ULA及其退火版本。

Abstract: We propose and analyze a deterministic sampling framework using Score-Based
Transport Modeling (SBTM) for sampling an unnormalized target density $\pi$.
While diffusion generative modeling relies on pre-training the score function
$\nabla \log f_t$ using samples from $\pi$, SBTM addresses the more general and
challenging setting where only $\nabla \log\pi$ is known. SBTM approximates the
Wasserstein gradient flow on KL$(f_t\|\pi)$ by learning the time-varying score
$\nabla \log f_t$ on the fly using score matching. The learned score gives
immediate access to relative Fisher information, providing a built-in
convergence diagnostic. The deterministic trajectories are smooth,
interpretable, and free of Brownian-motion noise, while having the same
distribution as ULA. We prove that SBTM dissipates relative entropy at the same
rate as the exact gradient flow, provided sufficient training. We further
extend our framework to annealed dynamics, to handle non log-concave targets.
Numerical experiments validate our theoretical findings: SBTM converges at the
optimal rate, has smooth trajectories, and is easily integrated with annealed
dynamics. We compare to the baselines of ULA and annealed ULA.

</details>


### [54] [Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment](https://arxiv.org/abs/2504.18133)
*Gissel Velarde,Michael Weichert,Anuj Deshmunkh,Sanjay Deshmane,Anindya Sudhir,Khushboo Sharma,Vaibhav Joshi*

Main category: cs.LG

TL;DR: 论文研究了不平衡数据集中树提升方法（如XGBoost和Imbalance-XGBoost）的性能，提出了一种数据准备和超参数优化的方法，并在不同规模和分布的数据集上验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决不平衡数据集在风险评估中的挑战，尤其是少数类难以检测的问题。

Method: 提出数据准备流程，结合树提升方法和超参数优化，评估不同数据集规模和分布下的性能。

Result: 随着数据量增加，识别性能提升；数据不平衡加剧时F1分数下降，但仍优于基线。超参数优化有效，但需谨慎应用。

Conclusion: 所提方法对数据变化具有鲁棒性，重训练可应对性能下降。平衡采样效果不一致，可能恶化检测。

Abstract: Most real-world classification problems deal with imbalanced datasets, posing
a challenge for Artificial Intelligence (AI), i.e., machine learning
algorithms, because the minority class, which is of extreme interest, often
proves difficult to be detected. This paper empirically evaluates tree boosting
methods' performance given different dataset sizes and class distributions,
from perfectly balanced to highly imbalanced. For tabular data, tree-based
methods such as XGBoost, stand out in several benchmarks due to detection
performance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated.
After introducing the motivation to address risk assessment with machine
learning, the paper reviews evaluation metrics for detection systems or binary
classifiers. It proposes a method for data preparation followed by tree
boosting methods including hyper-parameter optimization. The method is
evaluated on private datasets of 1 thousand (K), 10K and 100K samples on
distributions with 50, 45, 25, and 5 percent positive samples. As expected, the
developed method increases its recognition performance as more data is given
for training and the F1 score decreases as the data distribution becomes more
imbalanced, but it is still significantly superior to the baseline of
precision-recall determined by the ratio of positives divided by positives and
negatives. Sampling to balance the training set does not provide consistent
improvement and deteriorates detection. In contrast, classifier hyper-parameter
optimization improves recognition, but should be applied carefully depending on
data volume and distribution. Finally, the developed method is robust to data
variation over time up to some point. Retraining can be used when performance
starts deteriorating.

</details>


### [55] [A Generative Graph Contrastive Learning Model with Global Signal](https://arxiv.org/abs/2504.18148)
*Xiaofan Wei,Binyan Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的对比信号生成框架（CSG2L），通过SVD增强模块和自适应重加权策略解决现有图对比学习模型中的噪声干扰和样本对重要性不均问题，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习模型因随机扰动生成的增强视图引入了噪声，且未区分样本对的重要性，导致性能下降。

Method: 提出CSG2L框架，包括SVD增强模块（避免噪声）和局部-全局依赖学习模块（自适应重加权样本对）。

Result: 在基准数据集上，CSG2L优于现有方法，且兼容多种图神经网络。

Conclusion: CSG2L通过改进增强策略和样本对权重分配，显著提升了图对比学习的准确性和鲁棒性。

Abstract: Graph contrastive learning (GCL) has garnered significant attention recently
since it learns complex structural information from graphs through
self-supervised learning manner. However, prevalent GCL models may suffer from
performance degradation due to inappropriate contrastive signals. Concretely,
they commonly generate augmented views based on random perturbation, which
leads to biased essential structures due to the introduction of noise. In
addition, they assign equal weight to both hard and easy sample pairs, thereby
ignoring the difference in importance of the sample pairs. To address these
issues, this study proposes a novel Contrastive Signal Generative Framework for
Accurate Graph Learning (CSG2L) with the following two-fold ideas: a) building
a singular value decomposition (SVD)-directed augmented module (SVD-aug) to
obtain the global interactions as well as avoiding the random noise
perturbation; b) designing a local-global dependency learning module (LGDL)
with an adaptive reweighting strategy which can differentiate the effects of
hard and easy sample pairs. Extensive experiments on benchmark datasets
demonstrate that the proposed CSG2L outperforms the state-of-art baselines.
Moreover, CSG2L is compatible with a variety of GNNs.

</details>


### [56] [Offline Learning of Controllable Diverse Behaviors](https://arxiv.org/abs/2504.18160)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier,Ludovic Denoyer*

Main category: cs.LG

TL;DR: 提出了一种新的模仿学习方法，结合时间一致性和可控性，以更好地重现多样的行为演示并支持可控轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习方法通常仅从专家数据中学习单一策略，而近期扩展方法虽尝试处理多样行为数据，但无法充分重现行为多样性或支持可控生成。

Method: 该方法强调两个关键特性：时间一致性（确保整个行为序列的一致性而非仅单步）和可控性（通过构建行为潜在空间实现选择性激活）。

Result: 实验表明，该方法在多种任务和环境中优于现有方法。

Conclusion: 通过结合时间一致性和可控性，该方法能够更有效地重现多样化行为并支持可控生成，解决了传统方法的局限性。

Abstract: Imitation Learning (IL) techniques aim to replicate human behaviors in
specific tasks. While IL has gained prominence due to its effectiveness and
efficiency, traditional methods often focus on datasets collected from experts
to produce a single efficient policy. Recently, extensions have been proposed
to handle datasets of diverse behaviors by mainly focusing on learning
transition-level diverse policies or on performing entropy maximization at the
trajectory level. While these methods may lead to diverse behaviors, they may
not be sufficient to reproduce the actual diversity of demonstrations or to
allow controlled trajectory generation. To overcome these drawbacks, we propose
a different method based on two key features: a) Temporal Consistency that
ensures consistent behaviors across entire episodes and not just at the
transition level as well as b) Controllability obtained by constructing a
latent space of behaviors that allows users to selectively activate specific
behaviors based on their requirements. We compare our approach to
state-of-the-art methods over a diverse set of tasks and environments. Project
page: https://mathieu-petitbois.github.io/projects/swr/

</details>


### [57] [Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation](https://arxiv.org/abs/2504.18181)
*Yvonne Jenniges,Maike Sonnewald,Sebastian Maneth,Are Olsen,Boris P. Koch*

Main category: cs.LG

TL;DR: 本文通过数据驱动的机器学习方法（如KMeans、Ward、DBSCAN和UMAP）对北大西洋海域进行客观分区，验证了UMAP-DBSCAN方法的最佳表现，并利用NEMI技术生成321个稳定聚类。结果与现有水团定义高度一致，且比传统分区（如Longhurst provinces）更详细，为未来海洋生物地球化学研究提供了高效、可重复的方法。


<details>
  <summary>Details</summary>
Motivation: 传统海洋分区常依赖主观决策，可能导致误导性或不可复现的结果。本文旨在通过数据驱动的机器学习方法，客观定义北大西洋海域的分区，以支持海洋保护和生物地球化学研究。

Method: 采用多种聚类方法（KMeans、Ward、DBSCAN）结合UMAP降维技术，系统验证后选择UMAP-DBSCAN，并利用NEMI对100次聚类结果进行聚合，最终生成321个聚类。

Result: UMAP-DBSCAN表现最佳，聚类结果与现有水团定义高度一致，且分区比传统Longhurst provinces更详细。NEMI计算的聚类稳定性和不确定性分别为88.81±1.8%和15.49±20%。

Conclusion: 提出的方法客观、高效且可重复，为海洋区域划分提供了更精细的解决方案，未来可应用于生物地球化学差异和变化的研究。

Abstract: Defining ocean regions and water masses helps to understand marine processes
and can serve downstream-tasks such as defining marine protected areas.
However, such definitions are often a result of subjective decisions
potentially producing misleading, unreproducible results. Here, the aim was to
objectively define regions of the North Atlantic. For this, a data-driven,
systematic machine learning approach was applied to generate and validate ocean
clusters employing external, internal and relative validation techniques. About
300 million measured salinity, temperature, and oxygen, nitrate, phosphate and
silicate concentration values served as input for various clustering methods
(KMeans, agglomerative Ward, and Density-Based Spatial Clustering of
Applications with Noise (DBSCAN)). Uniform Manifold Approximation and
Projection (UMAP) emphasised (dis-)similarities in the data while reducing
dimensionality. Based on a systematic validation of the considered clustering
methods and their hyperparameters, the results showed that UMAP-DBSCAN best
represented the data. To address stochastic variability, 100 UMAP-DBSCAN
clustering runs were conducted and aggregated using Native Emergent Manifold
Interrogation (NEMI), producing a final set of 321 clusters. Reproducibility
was evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean
grid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented
clustering results agreed very well with common water mass definitions. This
study revealed a more detailed regionalization compared to previous concepts
such as the Longhurst provinces. The applied method is objective, efficient and
reproducible and will support future research focusing on biogeochemical
differences and changes in oceanic regions.

</details>


### [58] [An Open-Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting](https://arxiv.org/abs/2504.18185)
*Gissel Velarde,Pedro Branez,Alejandro Bueno,Rodrigo Heredia,Mateo Lopez-Ledezma*

Main category: cs.LG

TL;DR: 论文介绍了LSTM和GRU网络在时间序列预测中的开源可复现实现，评估了两个数据集上的表现，结果显示在具有重复模式的合成数据上表现优异，但在股票数据上与基线相近。


<details>
  <summary>Details</summary>
Motivation: 研究LSTM和GRU网络在时间序列预测中的表现，验证其在具有重复模式的数据上的有效性，并推广开源和可复现的研究实践。

Method: 在S&P BSE BANKEX股票数据集和合成Activities数据集上测试LSTM和GRU网络，使用RMSE和DA作为评估指标。

Result: 在Activities数据集上显著优于基线，但在股票数据集上表现与基线相近。

Conclusion: LSTM和GRU在具有重复模式的数据上表现优异，但受数据特性影响较大，研究提供了开源实现和数据以促进未来比较。

Abstract: This paper introduces an open-source and reproducible implementation of Long
Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) Networks for time
series forecasting. We evaluated LSTM and GRU networks because of their
performance reported in related work. We describe our method and its results on
two datasets. The first dataset is the S&P BSE BANKEX, composed of stock time
series (closing prices) of ten financial institutions. The second dataset,
called Activities, comprises ten synthetic time series resembling weekly
activities with five days of high activity and two days of low activity. We
report Root Mean Squared Error (RMSE) between actual and predicted values, as
well as Directional Accuracy (DA). We show that a single time series from a
dataset can be used to adequately train the networks if the sequences in the
dataset contain patterns that repeat, even with certain variation, and are
properly processed. For 1-step ahead and 20-step ahead forecasts, LSTM and GRU
networks significantly outperform a baseline on the Activities dataset. The
baseline simply repeats the last available value. On the stock market dataset,
the networks perform just like the baseline, possibly due to the nature of
these series. We release the datasets used as well as the implementation with
all experiments performed to enable future comparisons and to make our research
reproducible.

</details>


### [59] [A Machine Learning Approach For Bitcoin Forecasting](https://arxiv.org/abs/2504.18206)
*Stefano Sossi-Rojas,Gissel Velarde,Damian Zieba*

Main category: cs.LG

TL;DR: 该论文提出了一种新的时间序列子集，结合机器学习集成方法提高比特币价格方向预测的准确性，实验表明Open、High和Low价格序列对提升准确性最重要，其中Low贡献最大。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有研究表明仅用收盘价不足以准确预测股票市场序列方向，因此引入新时间序列子集以提高预测准确性。

Method: 方法包括评估不同时间序列和机器学习算法的组合，最终采用GRU网络和基准预测的集成方法。

Result: 实验结果显示Open、High和Low价格序列对方向准确性提升最有效，尤其是Low，而其他非价格相关特征影响可忽略。

Conclusion: 结论是所提方法在方向准确性上与现有最优方法表现相当，验证了时间序列子集和集成算法的有效性。

Abstract: Bitcoin is one of the cryptocurrencies that is gaining more popularity in
recent years. Previous studies have shown that closing price alone is not
enough to forecast stock market series. We introduce a new set of time series
and demonstrate that a subset is necessary to improve directional accuracy
based on a machine learning ensemble. In our experiments, we study which time
series and machine learning algorithms deliver the best results. We found that
the most relevant time series that contribute to improving directional accuracy
are Open, High and Low, with the largest contribution of Low in combination
with an ensemble of Gated Recurrent Unit network and a baseline forecast. The
relevance of other Bitcoin-related features that are not price-related is
negligible. The proposed method delivers similar performance to the
state-of-the-art when observing directional accuracy.

</details>


### [60] [Gradient Descent as a Shrinkage Operator for Spectral Bias](https://arxiv.org/abs/2504.18207)
*Simon Lucey*

Main category: cs.LG

TL;DR: 该论文研究了激活函数与谱偏差的关系，提出了梯度下降作为收缩算子的新视角，并探讨了非单调激活函数在谱偏差中的高效作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解激活函数如何影响神经网络的谱偏差，以及梯度下降如何通过隐式选择频率分量来控制这一偏差。

Method: 方法包括将梯度下降重新解释为收缩算子，分析其如何掩码神经网络雅可比矩阵的奇异值，并探讨超参数与带宽的关系。

Result: 结果表明梯度下降正则化仅对单调激活函数有效，而非单调激活函数（如sinc、高斯）可作为谱偏差的高效替代。

Conclusion: 结论强调了激活函数选择对谱偏差的重要性，并提出非单调激活函数在迭代效率上的优势。

Abstract: We generalize the connection between activation function and spline
regression/smoothing and characterize how this choice may influence spectral
bias within a 1D shallow network. We then demonstrate how gradient descent (GD)
can be reinterpreted as a shrinkage operator that masks the singular values of
a neural network's Jacobian. Viewed this way, GD implicitly selects the number
of frequency components to retain, thereby controlling the spectral bias. An
explicit relationship is proposed between the choice of GD hyperparameters
(learning rate & number of iterations) and bandwidth (the number of active
components). GD regularization is shown to be effective only with monotonic
activation functions. Finally, we highlight the utility of non-monotonic
activation functions (sinc, Gaussian) as iteration-efficient surrogates for
spectral bias.

</details>


### [61] [Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime](https://arxiv.org/abs/2504.18208)
*Raphaël Barboni,Gabriel Peyré,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 该论文研究了梯度方法在均值场单隐藏层神经网络训练中的收敛性，利用变量投影方法简化问题，并证明了特征分布训练的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 探索均值场神经网络训练的收敛性，特别是在特征分布训练中，如何通过简化问题结构来提高学习效率和理论保证。

Method: 采用变量投影（VarPro）或双时间尺度学习算法，消除线性变量，将学习问题简化为特征分布的训练。

Result: 研究表明，特征分布的动态对应于加权超快速扩散方程，并证明了在教师-学生设置下，特征分布向教师特征分布的收敛。

Conclusion: 通过简化问题结构和利用PDE分析，论文为均值场神经网络的特征分布训练提供了理论收敛保证。

Abstract: We study the convergence of gradient methods for the training of mean-field
single hidden layer neural networks with square loss. Observing this is a
separable non-linear least-square problem which is linear w.r.t. the outer
layer's weights, we consider a Variable Projection (VarPro) or two-timescale
learning algorithm, thereby eliminating the linear variables and reducing the
learning problem to the training of the feature distribution. Whereas most
convergence rates or the training of neural networks rely on a neural tangent
kernel analysis where features are fixed, we show such a strategy enables
provable convergence rates for the sampling of a teacher feature distribution.
Precisely, in the limit where the regularization strength vanishes, we show
that the dynamic of the feature distribution corresponds to a weighted
ultra-fast diffusion equation. Relying on recent results on the asymptotic
behavior of such PDEs, we obtain guarantees for the convergence of the trained
feature distribution towards the teacher feature distribution in a
teacher-student setup.

</details>


### [62] [Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction](https://arxiv.org/abs/2504.18230)
*He Shanxuan,Lin Zuhong,Yu Bolun,Gao Xu,Long Biao,Yao Jingjing*

Main category: cs.LG

TL;DR: 该研究提出了一种混合学习框架，通过动态多源数据融合和堆叠集成模型，实现了锂离子电池寿命的精确预测。


<details>
  <summary>Details</summary>
Motivation: 准确预测锂离子电池寿命对于确保电动汽车和智能电网等应用的运行可靠性及降低维护成本至关重要。

Method: 研究采用了动态多源数据融合和堆叠集成（SE）模型，结合了Ridge回归、LSTM网络和XGBoost算法，并通过熵动态加权机制处理异构数据集。

Result: 模型表现优异，MAE为0.0058，RMSE为0.0092，R2达0.9839，相比基线模型在R2上提高了46.2%，RMSE降低了83.2%。

Conclusion: 该框架提升了电池健康管理的可扩展性和可解释性，支持优化维护和安全性，有助于能源存储系统的电池健康管理。

Abstract: Accurate prediction of lithium-ion battery lifespan is vital for ensuring
operational reliability and reducing maintenance costs in applications like
electric vehicles and smart grids. This study presents a hybrid learning
framework for precise battery lifespan prediction, integrating dynamic
multi-source data fusion with a stacked ensemble (SE) modeling approach. By
leveraging heterogeneous datasets from the National Aeronautics and Space
Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE),
MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA)
chemistries, an entropy-based dynamic weighting mechanism mitigates variability
across heterogeneous datasets. The SE model combines Ridge regression, long
short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost),
effectively capturing temporal dependencies and nonlinear degradation patterns.
It achieves a mean absolute error (MAE) of 0.0058, root mean square error
(RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839,
outperforming established baseline models with a 46.2% improvement in R2 and an
83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis
identifies differential discharge capacity (Qdlin) and temperature of
measurement (Temp_m) as critical aging indicators. This scalable, interpretable
framework enhances battery health management, supporting optimized maintenance
and safety across diverse energy storage systems, thereby contributing to
improved battery health management in energy storage systems.

</details>


### [63] [DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering](https://arxiv.org/abs/2504.18243)
*Rong Cheng,Jinyi Liu,YAN ZHENG,Fei Ni,Jiazhen Du,Hangyu Mao,Fuzheng Zhang,Bo Wang,Jianye HAO*

Main category: cs.LG

TL;DR: DualRAG是一个双过程框架，结合推理和检索，通过Reasoning-augmented Querying和progressive Knowledge Aggregation提升多跳问答任务的准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的多跳问答方法在动态知识的识别和组织上表现不足，需要一种能协同推理和检索的解决方案。

Method: 提出了DualRAG框架，包含紧密耦合的RaQ（推理增强查询）和pKA（渐进知识聚合）两个过程，通过知识丰富和推理优化的良性循环提升性能。

Result: 实验表明，DualRAG显著提高了答案准确性和连贯性，甚至在某些情况下超过了使用预设知识的性能。

Conclusion: DualRAG是一个高效且稳健的解决方案，适用于复杂的多跳推理任务。

Abstract: Multi-Hop Question Answering (MHQA) tasks permeate real-world applications,
posing challenges in orchestrating multi-step reasoning across diverse
knowledge domains. While existing approaches have been improved with iterative
retrieval, they still struggle to identify and organize dynamic knowledge. To
address this, we propose DualRAG, a synergistic dual-process framework that
seamlessly integrates reasoning and retrieval. DualRAG operates through two
tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive
Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the
reasoning path and generates targeted queries, pKA ensures that newly acquired
knowledge is systematically integrated to support coherent reasoning. This
creates a virtuous cycle of knowledge enrichment and reasoning refinement.
Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and
retrieval capabilities even in smaller-scale models, demonstrating its
versatility and core advantages across different scales. Extensive experiments
demonstrate that this dual-process approach substantially improves answer
accuracy and coherence, approaching, and in some cases surpassing, the
performance achieved with oracle knowledge access. These results establish
DualRAG as a robust and efficient solution for complex multi-hop reasoning
tasks.

</details>


### [64] [Local Statistical Parity for the Estimation of Fair Decision Trees](https://arxiv.org/abs/2504.18262)
*Andrea Quintanilla,Johan Van Horebeek*

Main category: cs.LG

TL;DR: 论文提出了C-LRT算法，通过局部线性分类器和约束逻辑回归改进CART算法，以在决策树构建中平衡准确性与公平性。


<details>
  <summary>Details</summary>
Motivation: 减少决策树计算复杂度，同时通过节点局部公平性准则提升算法公平性。

Method: 提出C-LRT算法，结合局部线性分类器和约束逻辑回归改进标准CART算法。

Result: 在公平性文献常用数据集上验证，C-LRT能有效平衡分类准确性与公平性。

Conclusion: C-LRT为决策树提供了控制公平性与准确性的可行方案。

Abstract: Given the high computational complexity of decision tree estimation,
classical methods construct a tree by adding one node at a time in a recursive
way. To facilitate promoting fairness, we propose a fairness criterion local to
the tree nodes. We prove how it is related to the Statistical Parity criterion,
popular in the Algorithmic Fairness literature, and show how to incorporate it
into standard recursive tree estimation algorithms.
  We present a tree estimation algorithm called Constrained Logistic Regression
Tree (C-LRT), which is a modification of the standard CART algorithm using
locally linear classifiers and imposing restrictions as done in Constrained
Logistic Regression.
  Finally, we evaluate the performance of trees estimated with C-LRT on
datasets commonly used in the Algorithmic Fairness literature, using various
classification and fairness metrics. The results confirm that C-LRT
successfully allows to control and balance accuracy and fairness.

</details>


### [65] [Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study](https://arxiv.org/abs/2504.18267)
*Prajwal Chauhan,Salah Eddine Choutri,Mohamed Ghattassi,Nader Masmoudi,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: 论文研究了神经算子在处理Hughes模型（一种群体动力学的一阶双曲守恒律系统）时的局限性，发现其在复杂初始条件和边界条件下表现不佳，尤其是无法捕捉间断和重要物理特征。


<details>
  <summary>Details</summary>
Motivation: 探讨神经算子在处理具有复杂解结构（如间断）的非线性双曲系统中的表现，尤其是针对Hughes模型的局限性。

Method: 评估三种先进的神经算子（傅里叶神经算子、小波神经算子和多小波神经算子）在不同初始条件（间断和高斯分布）和边界条件下的表现，并分析数值格式的影响。

Result: 神经算子在简单场景中表现良好，但在复杂场景（多初始间断和动态边界条件）中预测结果过于平滑，导致总变差减少和物理特征丢失。

Conclusion: 当前的神经算子结构可能引入非预期的正则化效应，限制了其对间断主导的输运动态的捕捉能力，对交通应用中的激波保持提出了挑战。

Abstract: This paper investigates the limitations of neural operators in learning
solutions for a Hughes model, a first-order hyperbolic conservation law system
for crowd dynamics. The model couples a Fokker-Planck equation representing
pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes
model belongs to the class of nonlinear hyperbolic systems that often exhibit
complex solution structures, including shocks and discontinuities. In this
study, we assess the performance of three state-of-the-art neural operators
(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural
Operator) in various challenging scenarios. Specifically, we consider (1)
discontinuous and Gaussian initial conditions and (2) diverse boundary
conditions, while also examining the impact of different numerical schemes.
  Our results show that these neural operators perform well in easy scenarios
with fewer discontinuities in the initial condition, yet they struggle in
complex scenarios with multiple initial discontinuities and dynamic boundary
conditions, even when trained specifically on such complex samples. The
predicted solutions often appear smoother, resulting in a reduction in total
variation and a loss of important physical features. This smoothing behavior is
similar to issues discussed by Daganzo (1995), where models that introduce
artificial diffusion were shown to miss essential features such as shock waves
in hyperbolic systems. These results suggest that current neural operator
architectures may introduce unintended regularization effects that limit their
ability to capture transport dynamics governed by discontinuities. They also
raise concerns about generalizing these methods to traffic applications where
shock preservation is essential.

</details>


### [66] [Studying Small Language Models with Susceptibilities](https://arxiv.org/abs/2504.18274)
*Garrett Baker,George Wang,Jesse Hoogland,Daniel Murfet*

Main category: cs.LG

TL;DR: 作者提出了一种线性响应框架，将神经网络视为贝叶斯统计力学系统，通过扰动数据分布来研究网络组件的局部敏感性，实现了高效的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 动机在于为神经网络提供一种高效且可解释的线性响应方法，通过贝叶斯统计力学视角理解数据分布变化对网络行为的影响，从而揭示其内部功能模块。

Method: 方法包括：1）将神经网络建模为贝叶斯统计力学系统；2）通过局部SGLD采样高效估计敏感性；3）设计扰动（探针）构建响应矩阵，分离功能模块。

Result: 结果表明，该方法能有效分离功能模块（如多语法和归纳头），并量化数据分布变化对损失函数局部几何的形变影响。

Conclusion: 该框架不仅连接了奇异学习理论与线性响应理论，还提供了一种可解释的工具，用于分析神经网络在数据分布变化下的行为。

Abstract: We develop a linear response framework for interpretability that treats a
neural network as a Bayesian statistical mechanical system. A small, controlled
perturbation of the data distribution, for example shifting the Pile toward
GitHub or legal text, induces a first-order change in the posterior expectation
of an observable localized on a chosen component of the network. The resulting
susceptibility can be estimated efficiently with local SGLD samples and
factorizes into signed, per-token contributions that serve as attribution
scores. Building a set of perturbations (probes) yields a response matrix whose
low-rank structure separates functional modules such as multigram and induction
heads in a 3M-parameter transformer. Susceptibilities link local learning
coefficients from singular learning theory with linear-response theory, and
quantify how local loss landscape geometry deforms under shifts in the data
distribution.

</details>


### [67] [A comprehensive review of classifier probability calibration metrics](https://arxiv.org/abs/2504.18278)
*Richard Oliver Lane*

Main category: cs.LG

TL;DR: 该论文综述了分类器和目标检测模型的概率校准指标，识别了82种主要指标，并将其分为四大类，旨在帮助研究者更好地理解和比较不同校准指标的优劣。


<details>
  <summary>Details</summary>
Motivation: AI和ML模型的概率或置信度常与其真实准确性不符，导致模型预测可能过或欠自信。校准指标能测量这种差异，对模型在安全或关键业务环境中的应用尤为重要。

Method: 论文全面回顾并分类了82种概率校准指标，分为点基、分箱基、核或曲线基、累积及目标检测五大类，并提供了相关公式以便实现和比较。

Result: 研究者可通过分类和公式更便捷地理解和实现校准指标，从而评估模型的校准性能。

Conclusion: 概率校准指标的梳理为未来研究提供了系统化的参考，促进了模型校准性能的评估与应用。

Abstract: Probabilities or confidence values produced by artificial intelligence (AI)
and machine learning (ML) models often do not reflect their true accuracy, with
some models being under or over confident in their predictions. For example, if
a model is 80% sure of an outcome, is it correct 80% of the time? Probability
calibration metrics measure the discrepancy between confidence and accuracy,
providing an independent assessment of model calibration performance that
complements traditional accuracy metrics. Understanding calibration is
important when the outputs of multiple systems are combined, for assurance in
safety or business-critical contexts, and for building user trust in models.
This paper provides a comprehensive review of probability calibration metrics
for classifier and object detection models, organising them according to a
number of different categorisations to highlight their relationships. We
identify 82 major metrics, which can be grouped into four classifier families
(point-based, bin-based, kernel or curve-based, and cumulative) and an object
detection family. For each metric, we provide equations where available,
facilitating implementation and comparison by future researchers.

</details>


### [68] [Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps](https://arxiv.org/abs/2504.18300)
*Simon Hakenes,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 提出了一种基于物体导向宏动作的方法，利用拓扑地图简化导航任务，使DQN能高效学习导航策略。


<details>
  <summary>Details</summary>
Motivation: 解决在视觉复杂且奖励稀疏的大环境中导航的挑战。

Method: 通过RGBD输入检测物体并构建拓扑地图，选择离散宏动作导航到目标物体。

Result: 在3D仿真环境中显著优于随机基线，展示了拓扑结构和宏动作的样本高效学习能力。

Conclusion: 拓扑结构和宏动作能够从像素数据中实现高效的样本学习。

Abstract: This paper addresses the challenge of navigation in large, visually complex
environments with sparse rewards. We propose a method that uses object-oriented
macro actions grounded in a topological map, allowing a simple Deep Q-Network
(DQN) to learn effective navigation policies. The agent builds a map by
detecting objects from RGBD input and selecting discrete macro actions that
correspond to navigating to these objects. This abstraction drastically reduces
the complexity of the underlying reinforcement learning problem and enables
generalization to unseen environments. We evaluate our approach in a
photorealistic 3D simulation and show that it significantly outperforms a
random baseline under both immediate and terminal reward conditions. Our
results demonstrate that topological structure and macro-level abstraction can
enable sample-efficient learning even from pixel data.

</details>


### [69] [SSA-UNet: Advanced Precipitation Nowcasting via Channel Shuffling](https://arxiv.org/abs/2504.18309)
*Marco Turzi,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SSA-UNet的新设计，通过引入通道混洗机制优化SmaAt-UNet架构，用于天气预测。实验在荷兰降水数据和法国云层数据上进行，使用Grad-CAM分析模型关注区域。


<details>
  <summary>Details</summary>
Motivation: 天气预测对社会经济和环保至关重要。深度学习可作为数值天气预报模型的补充，降低复杂性并提高适应性。

Method: 提出SSA-UNet架构，引入通道混洗机制优化性能并简化模型。在荷兰降水数据集（2016-2019）和法国云层数据集（2017-2018）上进行训练和测试，并使用Grad-CAM进行预测分析。

Result: 评估了三种输出配置（1、6、12张降水图），Grad-CAM热图揭示了模型关注的关键输入区域。

Conclusion: SSA-UNet通过通道混洗机制优化了天气预测性能，Grad-CAM分析增强了模型的可解释性。

Abstract: Weather forecasting is essential for facilitating diverse socio-economic
activity and environmental conservation initiatives. Deep learning techniques
are increasingly being explored as complementary approaches to Numerical
Weather Prediction (NWP) models, offering potential benefits such as reduced
complexity and enhanced adaptability in specific applications. This work
presents a novel design, Small Shuffled Attention UNet (SSA-UNet), which
enhances SmaAt-UNet's architecture by including a shuffle channeling mechanism
to optimize performance and diminish complexity. To assess its efficacy, this
architecture and its reduced variant are examined and trained on two datasets:
a Dutch precipitation dataset from 2016 to 2019, and a French cloud cover
dataset containing radar images from 2017 to 2018. Three output configurations
of the proposed architecture are evaluated, yielding outputs of 1, 6, and 12
precipitation maps, respectively. To better understand how this model operates
and produces its predictions, a gradient-based approach called Grad-CAM is used
to analyze the outputs generated. The analysis of heatmaps generated by
Grad-CAM facilitated the identification of regions within the input maps that
the model considers most informative for generating its predictions. The
implementation of SSA-UNet can be found on our
Github\footnote{\href{https://github.com/MarcoTurzi/SSA-UNet}{https://github.com/MarcoTurzi/SSA-UNet}}

</details>


### [70] [PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology](https://arxiv.org/abs/2504.18329)
*Anh-Duy Pham,Olivier Basole Kashongwe,Martin Atzmueller,Tim Römer*

Main category: cs.LG

TL;DR: 提出了PHeatPruner方法，结合持续同调和层论，平衡多元时间序列分类中的性能和可解释性，显著降维且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 解决多元时间序列分类中高维数据复杂性与模型可解释性之间的平衡问题。

Method: 整合持续同调（降维）和层论（解释性），无需后验概率或监督优化，支持多种模型如随机森林、CatBoost等。

Result: 在UEA Archive和奶牛乳腺炎数据集上验证，可削减45%变量并保持精度，同时提供结构解释。

Conclusion: PHeatPruner在简化数据和提升可解释性方面效果显著，且计算效率高，具有广泛应用潜力。

Abstract: Balancing performance and interpretability in multivariate time series
classification is a significant challenge due to data complexity and high
dimensionality. This paper introduces PHeatPruner, a method integrating
persistent homology and sheaf theory to address these challenges. Persistent
homology facilitates the pruning of up to 45% of the applied variables while
maintaining or enhancing the accuracy of models such as Random Forest,
CatBoost, XGBoost, and LightGBM, all without depending on posterior
probabilities or supervised optimization algorithms. Concurrently, sheaf theory
contributes explanatory vectors that provide deeper insights into the data's
structural nuances. The approach was validated using the UEA Archive and a
mastitis detection dataset for dairy cows. The results demonstrate that
PHeatPruner effectively preserves model accuracy. Furthermore, our results
highlight PHeatPruner's key features, i.e. simplifying complex data and
offering actionable insights without increasing processing time or complexity.
This method bridges the gap between complexity reduction and interpretability,
suggesting promising applications in various fields.

</details>


### [71] [Testing Individual Fairness in Graph Neural Networks](https://arxiv.org/abs/2504.18353)
*Roya Nasiri*

Main category: cs.LG

TL;DR: 该研究聚焦于图神经网络（GNNs）中的个体公平性问题，旨在开发一个测试框架以评估和确保GNNs的公平性。


<details>
  <summary>Details</summary>
Motivation: 由于GNNs的特殊结构，其中的偏见可能通过节点间的连接传播，这一问题在现有研究中较少被关注，因此需要开发新的公平性测试方法。

Method: 研究首先系统回顾文献，建立个体公平性的分类法，然后开发一个框架，结合现有的公平性测试和缓解技术，并通过工业案例研究进行验证。

Result: 最终将提出一个专门针对GNNs的公平性测试框架，并在图基础的大型语言模型中进行评估。

Conclusion: 该研究填补了GNNs领域个体公平性研究的空白，为复杂图结构中的偏见检测和缓解提供了新思路。

Abstract: The biases in artificial intelligence (AI) models can lead to automated
decision-making processes that discriminate against groups and/or individuals
based on sensitive properties such as gender and race. While there are many
studies on diagnosing and mitigating biases in various AI models, there is
little research on individual fairness in Graph Neural Networks (GNNs). Unlike
traditional models, which treat data features independently and overlook their
inter-relationships, GNNs are designed to capture graph-based structure where
nodes are interconnected. This relational approach enables GNNs to model
complex dependencies, but it also means that biases can propagate through these
connections, complicating the detection and mitigation of individual fairness
violations. This PhD project aims to develop a testing framework to assess and
ensure individual fairness in GNNs. It first systematically reviews the
literature on individual fairness, categorizing existing approaches to define,
measure, test, and mitigate model biases, creating a taxonomy of individual
fairness. Next, the project will develop a framework for testing and ensuring
fairness in GNNs by adapting and extending current fairness testing and
mitigation techniques. The framework will be evaluated through industrial case
studies, focusing on graph-based large language models.

</details>


### [72] [Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization](https://arxiv.org/abs/2504.18371)
*Irshad A. Meer,Bruno Hörmann,Mustafa Ozger,Fabien Geyer,Alberto Viseras,Dominic Schupke,Cicek Cavdar*

Main category: cs.LG

TL;DR: 该论文提出了一种结合可解释AI（XAI）和SHAP的框架，用于增强基于DQN的无人机移动管理系统中切换决策的透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的无人机移动管理方法缺乏可解释性，限制了其在决策过程中的可信度。

Method: 采用可解释AI（XAI）框架和SHAP方法，量化关键特征（如RSRP、RSRQ、缓冲区状态和无人机位置）对切换决策的影响。

Result: 仿真结果表明，该方法能够直观解释策略决策，缩小AI模型与人类决策之间的差距。

Conclusion: 提出的框架显著提升了RL模型的可解释性和可靠性，适用于无人机网络中的移动管理。

Abstract: The integration of unmanned aerial vehicles (UAVs) into cellular networks
presents significant mobility management challenges, primarily due to frequent
handovers caused by probabilistic line-of-sight conditions with multiple ground
base stations (BSs). To tackle these challenges, reinforcement learning
(RL)-based methods, particularly deep Q-networks (DQN), have been employed to
optimize handover decisions dynamically. However, a major drawback of these
learning-based approaches is their black-box nature, which limits
interpretability in the decision-making process. This paper introduces an
explainable AI (XAI) framework that incorporates Shapley Additive Explanations
(SHAP) to provide deeper insights into how various state parameters influence
handover decisions in a DQN-based mobility management system. By quantifying
the impact of key features such as reference signal received power (RSRP),
reference signal received quality (RSRQ), buffer status, and UAV position, our
approach enhances the interpretability and reliability of RL-based handover
solutions. To validate and compare our framework, we utilize real-world network
performance data collected from UAV flight trials. Simulation results show that
our method provides intuitive explanations for policy decisions, effectively
bridging the gap between AI-driven models and human decision-makers.

</details>


### [73] [Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels](https://arxiv.org/abs/2504.18385)
*Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TL;DR: 本文提出了一种多重插补技术，用于评估分类器在标签缺失情况下的性能，并提供了预测分布，尤其在数据非随机缺失（MNAR）时表现良好。


<details>
  <summary>Details</summary>
Motivation: 目前研究多关注监督学习中的数据缺失问题，但忽视了模型评估中标签缺失的影响。常见方法是忽略缺失样本，但这可能导致偏差。本文旨在解决这一缺口。

Method: 采用多重插补技术，通过生成预测分布来估计精度、召回率和ROC-AUC等指标，尤其在MNAR情况下表现稳健。

Result: 实验表明，预测分布的位置和形状基本正确，且近似高斯分布，同时提供了有限样本收敛界限和稳健性证明。

Conclusion: 该方法不仅提供了点估计，还能在标签缺失时给出预测分布，适用于MNAR等复杂场景，具有理论和实践价值。

Abstract: Missing data in supervised learning is well-studied, but the specific issue
of missing labels during model evaluation has been overlooked. Ignoring samples
with missing values, a common solution, can introduce bias, especially when
data is Missing Not At Random (MNAR). We propose a multiple imputation
technique for evaluating classifiers using metrics such as precision, recall,
and ROC-AUC. This method not only offers point estimates but also a predictive
distribution for these quantities when labels are missing. We empirically show
that the predictive distribution's location and shape are generally correct,
even in the MNAR regime. Moreover, we establish that this distribution is
approximately Gaussian and provide finite-sample convergence bounds.
Additionally, a robustness proof is presented, confirming the validity of the
approximation under a realistic error model.

</details>


### [74] [Machine Learning and Statistical Insights into Hospital Stay Durations: The Italian EHR Case](https://arxiv.org/abs/2504.18393)
*Marina Andric,Mauro Dragoni*

Main category: cs.LG

TL;DR: 该研究通过分析意大利皮埃蒙特地区60多家医疗机构2020-2023年的住院记录，利用CatBoost和随机森林模型预测住院时长（LoS），发现年龄、合并症评分、入院类型和月份等因素与LoS显著相关，CatBoost模型表现最佳（R2=0.49）。


<details>
  <summary>Details</summary>
Motivation: 住院时长（LoS）是衡量医疗质量和优化医院资源管理的关键指标。研究旨在识别意大利医疗背景下影响LoS的因素。

Method: 使用60多家医疗机构2020-2023年的住院记录，分析患者特征、合并症、入院信息和医院因素，并采用CatBoost和随机森林模型进行预测。

Result: 年龄组、合并症评分、入院类型和月份与LoS显著相关，CatBoost模型表现最佳（R2=0.49）。

Conclusion: 研究成功识别了影响LoS的关键因素，CatBoost模型在预测LoS方面表现出色，为优化医疗资源管理提供了依据。

Abstract: Length of hospital stay is a critical metric for assessing healthcare quality
and optimizing hospital resource management. This study aims to identify
factors influencing LoS within the Italian healthcare context, using a dataset
of hospitalization records from over 60 healthcare facilities in the Piedmont
region, spanning from 2020 to 2023. We explored a variety of features,
including patient characteristics, comorbidities, admission details, and
hospital-specific factors. Significant correlations were found between LoS and
features such as age group, comorbidity score, admission type, and the month of
admission. Machine learning models, specifically CatBoost and Random Forest,
were used to predict LoS. The highest R2 score, 0.49, was achieved with
CatBoost, demonstrating good predictive performance.

</details>


### [75] [Three Types of Calibration with Properties and their Semantic and Formal Relationships](https://arxiv.org/abs/2504.18395)
*Rabanus Derr,Jessie Finocchiaro,Robert C. Williamson*

Main category: cs.LG

TL;DR: 该论文重新审视了预测系统的校准问题，提出了两种校准动机（自我实现和精确损失估计），并定义了原型概念以统一多种校准定义。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对预测系统‘可信度’和算法公平性的讨论，旨在理清并统一日益分散的校准定义和理解。

Method: 通过‘自我实现’（反射原理）和‘精确损失估计’（保险公平性）两种动机，提出基于结果分布特性的原型定义（如均值或中位数），并探讨其与交换后悔和学习范式的关系。

Result: 在二元结果集下，两种原型定义可通过合理选择参考特性统一；高维结果集下则通过‘分布校准’概念扩展兼容。

Conclusion: 本文通过语义映射整合了碎片化的校准定义，强调了分组在多元校准中的作用，为后续研究提供了清晰框架。

Abstract: Fueled by discussions around "trustworthiness" and algorithmic fairness,
calibration of predictive systems has regained scholars attention. The vanilla
definition and understanding of calibration is, simply put, on all days on
which the rain probability has been predicted to be p, the actual frequency of
rain days was p. However, the increased attention has led to an immense variety
of new notions of "calibration." Some of the notions are incomparable, serve
different purposes, or imply each other. In this work, we provide two accounts
which motivate calibration: self-realization of forecasted properties and
precise estimation of incurred losses of the decision makers relying on
forecasts. We substantiate the former via the reflection principle and the
latter by actuarial fairness. For both accounts we formulate prototypical
definitions via properties $\Gamma$ of outcome distributions, e.g., the mean or
median. The prototypical definition for self-realization, which we call
$\Gamma$-calibration, is equivalent to a certain type of swap regret under
certain conditions. These implications are strongly connected to the
omniprediction learning paradigm. The prototypical definition for precise loss
estimation is a modification of decision calibration adopted from Zhao et al.
[73]. For binary outcome sets both prototypical definitions coincide under
appropriate choices of reference properties. For higher-dimensional outcome
sets, both prototypical definitions can be subsumed by a natural extension of
the binary definition, called distribution calibration with respect to a
property. We conclude by commenting on the role of groupings in both accounts
of calibration often used to obtain multicalibration. In sum, this work
provides a semantic map of calibration in order to navigate a fragmented
terrain of notions and definitions.

</details>


### [76] [Online learning to accelerate nonlinear PDE solvers: applied to multiphase porous media flow](https://arxiv.org/abs/2504.18414)
*Vinicius L S Silva,Pablo Salinas,Claire E Heaney,Matthew Jackson,Christopher C Pain*

Main category: cs.LG

TL;DR: 论文提出了一种基于在线/自适应学习的非线性求解器加速方法，用于非线性偏微分方程系统，应用于多孔介质中的多相流问题。通过动态调整松弛因子和在线学习，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 多孔介质中的多相流问题计算复杂且耗时，传统非线性求解器效率低。通过机器学习和动态参数调整，旨在提升求解效率和减少计算时间。

Method: 方法基于四个支柱：(1) 无量纲数作为机器学习输入，(2) 二维简化模型离线训练，(3) 动态控制松弛因子，(4) 在线学习实时优化模型。

Result: 在复杂三维模型中验证，非线性迭代次数减少，计算时间最高减少85%。

Conclusion: 结合机器学习的动态加速方法显著提升多相流模拟效率，适用于复杂实际问题。

Abstract: We propose a novel type of nonlinear solver acceleration for systems of
nonlinear partial differential equations (PDEs) that is based on
online/adaptive learning. It is applied in the context of multiphase flow in
porous media. The proposed method rely on four pillars: (i) dimensionless
numbers as input parameters for the machine learning model, (ii) simplified
numerical model (two-dimensional) for the offline training, (iii) dynamic
control of a nonlinear solver tuning parameter (numerical relaxation), (iv) and
online learning for real-time improvement of the machine learning model. This
strategy decreases the number of nonlinear iterations by dynamically modifying
a single global parameter, the relaxation factor, and by adaptively learning
the attributes of each numerical model on-the-run. Furthermore, this work
performs a sensitivity study in the dimensionless parameters (machine learning
features), assess the efficacy of various machine learning models, demonstrate
a decrease in nonlinear iterations using our method in more intricate,
realistic three-dimensional models, and fully couple a machine learning model
into an open-source multiphase flow simulator achieving up to 85\% reduction in
computational time.

</details>


### [77] [An Axiomatic Assessment of Entropy- and Variance-based Uncertainty Quantification in Regression](https://arxiv.org/abs/2504.18433)
*Christopher Bülte,Yusuf Sale,Timo Löhr,Paul Hofman,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 该论文提出了回归任务中不确定性量化的公理，分析了熵和方差方法的局限性，为回归中的可靠不确定性评估提供了理论基础和实践指南。


<details>
  <summary>Details</summary>
Motivation: 当前不确定性量化的研究多集中于分类任务，忽视了回归任务中的不确定性评估，因此需要填补这一理论空白并提供实用方法。

Method: 作者提出一组公理，通过指数族预测模型推广了常用的不确定性表示方法，并分析了熵和方差基的测度。

Result: 研究为回归任务的不确定性量化提供了理论框架，揭示了现有测度的局限性，并提出了改进方向。

Conclusion: 论文通过严格的公理化方法填补了回归任务中不确定性量化的空白，为实际应用中可靠的评估提供基础。

Abstract: Uncertainty quantification (UQ) is crucial in machine learning, yet most
(axiomatic) studies of uncertainty measures focus on classification, leaving a
gap in regression settings with limited formal justification and evaluations.
In this work, we introduce a set of axioms to rigorously assess measures of
aleatoric, epistemic, and total uncertainty in supervised regression. By
utilizing a predictive exponential family, we can generalize commonly used
approaches for uncertainty representation and corresponding uncertainty
measures. More specifically, we analyze the widely used entropy- and
variance-based measures regarding limitations and challenges. Our findings
provide a principled foundation for UQ in regression, offering theoretical
insights and practical guidelines for reliable uncertainty assessment.

</details>


### [78] [Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse](https://arxiv.org/abs/2504.18437)
*Kun He,Zijian Song,Shuoxi Zhang,John E. Hopcroft*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于预训练模型（PTM）的类增量学习（CIL）新方法，通过利用神经坍缩（NC）现象动态调整特征空间，显著提升了持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型中特征演化与类增量学习的关系，解决现有方法在特征分布动态调整上的不足。

Method: 提出NCPTM-CIL方法，通过将特征分布与NC几何对齐，动态优化特征空间结构。

Result: 在多个基准数据集上表现优异，尤其在ViT-B/16-IN1K初始化下，性能显著优于其他方法。

Conclusion: NCPTM-CIL通过NC几何对齐有效提升了类增量学习的性能，为实际应用提供了新思路。

Abstract: Class-Incremental Learning (CIL) is a critical capability for real-world
applications, enabling learning systems to adapt to new tasks while retaining
knowledge from previous ones. Recent advancements in pre-trained models (PTMs)
have significantly advanced the field of CIL, demonstrating superior
performance over traditional methods. However, understanding how features
evolve and are distributed across incremental tasks remains an open challenge.
In this paper, we propose a novel approach to modeling feature evolution in
PTM-based CIL through the lens of neural collapse (NC), a striking phenomenon
observed in the final phase of training, which leads to a well-separated,
equiangular feature space. We explore the connection between NC and CIL
effectiveness, showing that aligning feature distributions with the NC geometry
enhances the ability to capture the dynamic behavior of continual learning.
Based on this insight, we introduce Neural Collapse-inspired Pre-Trained
Model-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature
space to conform to the elegant NC structure, thereby enhancing the continual
learning process. Extensive experiments demonstrate that NCPTM-CIL outperforms
state-of-the-art methods across four benchmark datasets. Notably, when
initialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by
6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.

</details>


### [79] [Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data and Machine Learning](https://arxiv.org/abs/2504.18451)
*Tewodros Alemu Ayall,Andy Li,Matthew Beddows,Milan Markovic,Georgios Leontidis*

Main category: cs.LG

TL;DR: 通过部署物联网传感器和AI预测模型，研究解决了农业数据不足的问题，合成数据提高了草莓产量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于全球人口快速增长，数字化农业对可持续食品生产和资源管理决策至关重要。但AI模型需要大量数据，而实际农场环境中长期收集物联网数据具有挑战性。

Method: 在草莓种植地部署物联网传感器收集两个生长季节的环境数据，结合四个季节的手工产量记录，提出AI回填方法生成合成数据，构建产量预测模型评估效果。

Result: 实验结果表明，合成数据的加入提高了产量预测的准确性，相比仅使用真实数据、天气记录和手工产量数据的模型表现更优。

Conclusion: 通过结合AI生成的合成数据与真实观测数据，可以有效解决农业数据不足的问题，提升产量预测模型的性能。

Abstract: Due to rapid population growth globally, digitally-enabled agricultural
sectors are crucial for sustainable food production and making informed
decisions about resource management for farmers and various stakeholders. The
deployment of Internet of Things (IoT) technologies that collect real-time
observations of various environmental (e.g., temperature, humidity, etc.) and
operational factors (e.g., irrigation) influencing production is often seen as
a critical step to enable additional novel downstream tasks, such as AI-based
yield forecasting. However, since AI models require large amounts of data, this
creates practical challenges in a real-world dynamic farm setting where IoT
observations would need to be collected over a number of seasons. In this
study, we deployed IoT sensors in strawberry production polytunnels for two
growing seasons to collect environmental data, including water usage, external
and internal temperature, external and internal humidity, soil moisture, soil
temperature, and photosynthetically active radiation. The sensor observations
were combined with manually provided yield records spanning a period of four
seasons. To bridge the gap of missing IoT observations for two additional
seasons, we propose an AI-based backcasting approach to generate synthetic
sensor observations using historical weather data from a nearby weather station
and the existing polytunnel observations. We built an AI-based yield
forecasting model to evaluate our approach using the combination of real and
synthetic observations. Our results demonstrated that incorporating synthetic
data improved yield forecasting accuracy, with models incorporating synthetic
data outperforming those trained only on historical yield, weather records, and
real sensor data.

</details>


### [80] [Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training](https://arxiv.org/abs/2504.18454)
*Hiroki Naganuma,Xinzhi Zhang,Man-Chung Yue,Ioannis Mitliagkas,Philipp A. Witte,Russell J. Hewett,Yin Tat Lee*

Main category: cs.LG

TL;DR: 论文提出了一种名为PALSGD的方法，通过伪异步同步机制减少数据并行训练中的通信频率，同时保持模型一致性，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型规模的增大，数据并行训练中的通信频率成为瓶颈，需要一种既能减少通信又能保持模型性能的方法。

Method: PALSGD扩展了Local SGD和DiLoCo，引入伪同步机制，允许更长的同步间隔，同时通过理论分析证明其收敛性。

Result: 实验结果显示，PALSGD在ImageNet-1K和TinyStories任务上比DDP分别快18.4%和21.1%-24.4%。

Conclusion: PALSGD通过减少通信频率显著提升了训练效率，且性能不逊于高频率同步方法。

Abstract: Following AI scaling trends, frontier models continue to grow in size and
continue to be trained on larger datasets. Training these models requires huge
investments in exascale computational resources, which has in turn driven
development of distributed deep learning methods. Data parallelism is an
essential approach to speed up training, but it requires frequent global
communication between workers, which can bottleneck training at the largest
scales. In this work, we propose a method called Pseudo-Asynchronous Local SGD
(PALSGD) to improve the efficiency of data-parallel training. PALSGD is an
extension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023),
designed to further reduce communication frequency by introducing a
pseudo-synchronization mechanism. PALSGD allows the use of longer
synchronization intervals compared to standard Local SGD. Despite the reduced
communication frequency, the pseudo-synchronization approach ensures that model
consistency is maintained, leading to performance results comparable to those
achieved with more frequent synchronization. Furthermore, we provide a
theoretical analysis of PALSGD, establishing its convergence and deriving its
convergence rate. This analysis offers insights into the algorithm's behavior
and performance guarantees. We evaluated PALSGD on image classification and
language modeling tasks. Our results show that PALSGD achieves better
performance in less time compared to existing methods like Distributed Data
Parallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on
ImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with
GPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.

</details>


### [81] [Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional](https://arxiv.org/abs/2504.18506)
*Sanjeev Raja,Martin Šípka,Michael Psenka,Tobias Kreiman,Michal Pavelka,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于预训练生成模型的零样本方法，用于解决过渡路径采样问题，通过重新利用生成模型的评分函数并最小化Onsager-Machlup动作泛函来寻找高概率过渡路径。


<details>
  <summary>Details</summary>
Motivation: 由于当前基于机器学习的过渡路径采样方法需要昂贵的任务特定训练且无法充分利用预训练生成模型的优势，本研究旨在开发一种无需额外训练、能直接利用预训练模型进行过渡路径采样的方法。

Method: 将过渡路径视为预训练生成模型（去噪扩散和流匹配）诱导的随机动态采样的轨迹，通过最小化Onsager-Machlup动作泛函来识别高概率路径。

Result: 在多种分子系统中验证了方法的有效性，生成了多样化且物理真实的过渡路径，并能推广到预训练模型原始训练数据之外的范围。

Conclusion: 该方法无需额外训练即可实现过渡路径采样，具有广泛适用性，并能随着生成模型的改进而持续受益。

Abstract: Transition path sampling (TPS), which involves finding probable paths
connecting two points on an energy landscape, remains a challenge due to the
complexity of real-world atomistic systems. Current machine learning approaches
use expensive, task-specific, and data-free training procedures, limiting their
ability to benefit from recent advances in atomistic machine learning, such as
high-quality datasets and large-scale pre-trained models. In this work, we
address TPS by interpreting candidate paths as trajectories sampled from
stochastic dynamics induced by the learned score function of pre-trained
generative models, specifically denoising diffusion and flow matching. Under
these dynamics, finding high-likelihood transition paths becomes equivalent to
minimizing the Onsager-Machlup (OM) action functional. This enables us to
repurpose pre-trained generative models for TPS in a zero-shot manner, in
contrast with bespoke, task-specific TPS models trained in previous work. We
demonstrate our approach on varied molecular systems, obtaining diverse,
physically realistic transition pathways and generalizing beyond the
pre-trained model's original training dataset. Our method can be easily
incorporated into new generative models, making it practically relevant as
models continue to scale and improve with increased data availability.

</details>


### [82] [Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks](https://arxiv.org/abs/2504.18519)
*Han Zhang,Hao Zhou,Medhat Elsayed,Majid Bavand,Raimundas Gaigalas,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.LG

TL;DR: 探讨了联邦学习在无线网络中面临的恶意攻击风险，提出了一种基于深度强化学习的睡眠控制方案，以及针对GAN增强的模型投毒攻击和正则化攻击的防御方法。


<details>
  <summary>Details</summary>
Motivation: 由于联邦学习的分布式特性，无线网络中容易受到恶意攻击，且动态环境和非IID数据增加了检测难度，因此需要评估攻击影响并开发防御技术。

Method: 设计了两种攻击模型（GAN增强的模型投毒攻击和正则化攻击）和两种防御方案（基于自动编码器和基于知识蒸馏的防御）。

Result: 提出的防御方法能有效识别恶意参与者并保护模型免受攻击，从而提升网络能效。

Conclusion: 研究证明了联邦学习在无线网络中的脆弱性，并提供了有效的防御方案，为实际应用提供了重要参考。

Abstract: Federated learning (FL) is a promising technique for learning-based functions
in wireless networks, thanks to its distributed implementation capability. On
the other hand, distributed learning may increase the risk of exposure to
malicious attacks where attacks on a local model may spread to other models by
parameter exchange. Meanwhile, such attacks can be hard to detect due to the
dynamic wireless environment, especially considering local models can be
heterogeneous with non-independent and identically distributed (non-IID) data.
Therefore, it is critical to evaluate the effect of malicious attacks and
develop advanced defense techniques for FL-enabled wireless networks. In this
work, we introduce a federated deep reinforcement learning-based cell sleep
control scenario that enhances the energy efficiency of the network. We propose
multiple intelligent attacks targeting the learning-based approach and we
propose defense methods to mitigate such attacks. In particular, we have
designed two attack models, generative adversarial network (GAN)-enhanced model
poisoning attack and regularization-based model poisoning attack. As a
counteraction, we have proposed two defense schemes, autoencoder-based defense,
and knowledge distillation (KD)-enabled defense. The autoencoder-based defense
method leverages an autoencoder to identify the malicious participants and only
aggregate the parameters of benign local models during the global aggregation,
while KD-based defense protects the model from attacks by controlling the
knowledge transferred between the global model and local models.

</details>


### [83] [Generalization Capability for Imitation Learning](https://arxiv.org/abs/2504.18538)
*Yixiao Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论和数据分布特性的模仿学习泛化能力统一视角，包括上限分析及训练策略设计指导。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人技能学习上有潜力，但有限数据集训练的模型难以泛化。

Method: 通过条件信息瓶颈和模型参数与训练数据的互信息来界定泛化差距上限，并分析条件熵与梯度下降逃逸时间的关系。

Result: 高条件熵会导致平坦似然景观，减少泛化差距，优化训练效率。

Conclusion: 模仿学习的泛化受限原因被解释，强调了增强输入数据多样性和输出标签变异性的重要性。

Abstract: Imitation learning holds the promise of equipping robots with versatile
skills by learning from expert demonstrations. However, policies trained on
finite datasets often struggle to generalize beyond the training distribution.
In this work, we present a unified perspective on the generalization capability
of imitation learning, grounded in both information theorey and data
distribution property. We first show that the generalization gap can be upper
bounded by (i) the conditional information bottleneck on intermediate
representations and (ii) the mutual information between the model parameters
and the training dataset. This characterization provides theoretical guidance
for designing effective training strategies in imitation learning, particularly
in determining whether to freeze, fine-tune, or train large pretrained encoders
(e.g., vision-language models or vision foundation models) from scratch to
achieve better generalization. Furthermore, we demonstrate that high
conditional entropy from input to output induces a flatter likelihood
landscape, thereby reducing the upper bound on the generalization gap. In
addition, it shortens the stochastic gradient descent (SGD) escape time from
sharp local minima, which may increase the likelihood of reaching global optima
under fixed optimization budgets. These insights explain why imitation learning
often exhibits limited generalization and underscore the importance of not only
scaling the diversity of input data but also enriching the variability of
output labels conditioned on the same input.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [84] [ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing](https://arxiv.org/abs/2504.17929)
*Ayesha Siddique,Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: XAIedge提出了一种新框架，利用近似计算技术优化XAI算法，在TPU边缘设备上实现了更高的能效和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI硬件加速方法未能充分解决实时场景下的能效问题，限制了XAI在实时应用中的部署。

Method: 通过将XAI算法（如积分梯度、模型蒸馏和Shapley分析）转化为近似矩阵计算，并结合卷积、傅里叶变换与近似计算范式，在TPU边缘设备上实现高效加速。

Result: XAIedge在能效上比现有精确XAI硬件加速技术提升2倍，同时保持准确性。

Conclusion: XAIedge为能源受限的实时应用中的可解释AI部署提供了重要进展。

Abstract: Explainable artificial intelligence (XAI) enhances AI system transparency by
framing interpretability as an optimization problem. However, this approach
often necessitates numerous iterations of computationally intensive operations,
limiting its applicability in real-time scenarios. While recent research has
focused on XAI hardware acceleration on FPGAs and TPU, these methods do not
fully address energy efficiency in real-time settings. To address this
limitation, we propose XAIedge, a novel framework that leverages approximate
computing techniques into XAI algorithms, including integrated gradients, model
distillation, and Shapley analysis. XAIedge translates these algorithms into
approximate matrix computations and exploits the synergy between convolution,
Fourier transform, and approximate computing paradigms. This approach enables
efficient hardware acceleration on TPU-based edge devices, facilitating faster
real-time outcome interpretations. Our comprehensive evaluation demonstrates
that XAIedge achieves a $2\times$ improvement in energy efficiency compared to
existing accurate XAI hardware acceleration techniques while maintaining
comparable accuracy. These results highlight the potential of XAIedge to
significantly advance the deployment of explainable AI in energy-constrained
real-time applications.

</details>


### [85] [LLM Agent Swarm for Hypothesis-Driven Drug Discovery](https://arxiv.org/abs/2504.17967)
*Kevin Song,Andrew Trotter,Jake Y. Chen*

Main category: cs.AI

TL;DR: PharmaSwarm是一个统一的多代理框架，通过协调专门的LLM代理来提出、验证和优化新药靶点和先导化合物的假设，以加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 药物发现成本高、失败率高，且不同数据流难以整合，阻碍了机制性洞见的获取和进展。大型语言模型在推理和工具集成方面表现出色，但缺乏模块化专业化和迭代记忆能力。

Method: PharmaSwarm框架结合了多个专用LLM代理，各代理具备特定功能（如基因组分析、生物医学知识图谱等），并通过中央评估LLM对假设进行持续评估和优化。

Result: PharmaSwarm支持文献驱动发现、组学引导的靶点识别和市场导向的再利用，并通过四层验证流程确保透明度和可重复性。

Conclusion: PharmaSwarm作为AI副驾驶，能够比传统流程更高效地加速转化研究并提供高可信度的假设。

Abstract: Drug discovery remains a formidable challenge: more than 90 percent of
candidate molecules fail in clinical evaluation, and development costs often
exceed one billion dollars per approved therapy. Disparate data streams, from
genomics and transcriptomics to chemical libraries and clinical records, hinder
coherent mechanistic insight and slow progress. Meanwhile, large language
models excel at reasoning and tool integration but lack the modular
specialization and iterative memory required for regulated, hypothesis-driven
workflows. We introduce PharmaSwarm, a unified multi-agent framework that
orchestrates specialized LLM "agents" to propose, validate, and refine
hypotheses for novel drug targets and lead compounds. Each agent accesses
dedicated functionality--automated genomic and expression analysis; a curated
biomedical knowledge graph; pathway enrichment and network simulation;
interpretable binding affinity prediction--while a central Evaluator LLM
continuously ranks proposals by biological plausibility, novelty, in silico
efficacy, and safety. A shared memory layer captures validated insights and
fine-tunes underlying submodels over time, yielding a self-improving system.
Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm
supports literature-driven discovery, omics-guided target identification, and
market-informed repurposing. We also describe a rigorous four-tier validation
pipeline spanning retrospective benchmarking, independent computational assays,
experimental testing, and expert user studies to ensure transparency,
reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm
can accelerate translational research and deliver high-confidence hypotheses
more efficiently than traditional pipelines.

</details>


### [86] [Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction](https://arxiv.org/abs/2504.18007)
*Yazan Otoum,Amiya Nayak*

Main category: cs.AI

TL;DR: 论文提出了结合差分隐私和联邦学习的隐私保护模型，用于医疗数据分析，在保证隐私的同时实现了85%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 随着医疗系统数字化，患者隐私数据保护变得至关重要，而机器学习在医疗领域的广泛应用需要确保数据隐私。

Method: 采用差分隐私（添加噪声保障隐私）和联邦学习（分散数据集上的协作训练）方法，应用于心脏病数据集。

Result: 实验结果显示，结合差分隐私的联邦学习模型达到了85%的测试准确率，同时保证了数据隐私安全。

Conclusion: 研究表明，差分隐私与联邦学习的结合能有效保护医疗数据隐私，同时提供有价值的分析结果。

Abstract: With the rapid digitalization of healthcare systems, there has been a
substantial increase in the generation and sharing of private health data.
Safeguarding patient information is essential for maintaining consumer trust
and ensuring compliance with legal data protection regulations. Machine
learning is critical in healthcare, supporting personalized treatment, early
disease detection, predictive analytics, image interpretation, drug discovery,
efficient operations, and patient monitoring. It enhances decision-making,
accelerates research, reduces errors, and improves patient outcomes. In this
paper, we utilize machine learning methodologies, including differential
privacy and federated learning, to develop privacy-preserving models that
enable healthcare stakeholders to extract insights without compromising
individual privacy. Differential privacy introduces noise to data to guarantee
statistical privacy, while federated learning enables collaborative model
training across decentralized datasets. We explore applying these technologies
to Heart Disease Data, demonstrating how they preserve privacy while delivering
valuable insights and comprehensive analysis. Our results show that using a
federated learning model with differential privacy achieved a test accuracy of
85%, ensuring patient data remained secure and private throughout the process.

</details>


### [87] [MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind](https://arxiv.org/abs/2504.18039)
*Zheng Zhang,Nuoqian Xiao,Qi Chai,Deheng Ye,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出了MultiMind框架，这是首个在多模态信息（如面部表情和语音语调）基础上结合心理理论（ToM）和蒙特卡洛树搜索（MCTS）的社交推理代理，用于提升大型语言模型（LLM）在社交推理游戏中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有社交推理游戏代理仅依赖文本信息，忽略了人类沟通中的多模态线索（如面部表情和语调），且缺乏对玩家间相互心理感知的建模。本文旨在填补这一空白。

Method: 以One Night Ultimate Werewolf游戏为测试平台，结合ToM模型和MCTS，MultiMind通过分析多模态数据（面部表情、语调）和语言内容，优化代理的沟通策略以减少自身嫌疑。

Result: 通过与代理和人类玩家的广泛测试，MultiMind表现出更高的游戏性能，验证了多模态和心理理论模型在社交推理中的重要性。

Conclusion: MultiMind的提出标志着LLM代理在多模态社交推理领域的重大进展，为更接近人类社交能力的AI代理奠定了基础。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities
in social deduction games (SDGs) like Werewolf, where strategic reasoning and
social deception are essential. However, current approaches remain limited to
textual information, ignoring crucial multimodal cues such as facial
expressions and tone of voice that humans naturally use to communicate.
Moreover, existing SDG agents primarily focus on inferring other players'
identities without modeling how others perceive themselves or fellow players.
To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a
testbed and present MultiMind, the first framework integrating multimodal
information into SDG agents. MultiMind processes facial expressions and vocal
tones alongside verbal content, while employing a Theory of Mind (ToM) model to
represent each player's suspicion levels toward others. By combining this ToM
model with Monte Carlo Tree Search (MCTS), our agent identifies communication
strategies that minimize suspicion directed at itself. Through comprehensive
evaluation in both agent-versus-agent simulations and studies with human
players, we demonstrate MultiMind's superior performance in gameplay. Our work
presents a significant advancement toward LLM agents capable of human-like
social reasoning across multimodal domains.

</details>


### [88] [Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation](https://arxiv.org/abs/2504.18096)
*Xiang Li,Haixu Ma,Guanyong Wu,Shi Mu,Chen Li,Shunpan Liang*

Main category: cs.AI

TL;DR: 论文提出了MKMed框架，通过跨模态编码器解决药物推荐中的“桶效应”，整合多知识模态数据提升推荐准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐模型因药物知识数据不平衡（多模态数据不统一）导致性能受限，需解决“桶效应”问题。

Method: 提出跨模态药物编码器，通过对比学习预训练对齐五类知识模态，再结合患者记录进行推荐。

Result: 在MIMIC-III和MIMIC-IV数据集上，MKMed显著超越现有基准模型，缓解了数据不平衡问题。

Conclusion: MKMed框架有效整合多模态药物知识，提升推荐性能，为医疗领域的药物推荐提供了新思路。

Abstract: Medication recommendation is crucial in healthcare, offering effective
treatments based on patient's electronic health records (EHR). Previous studies
show that integrating more medication-related knowledge improves medication
representation accuracy. However, not all medications encompass multiple types
of knowledge data simultaneously. For instance, some medications provide only
textual descriptions without structured data. This imbalance in data
availability limits the performance of existing models, a challenge we term the
"bucket effect" in medication recommendation. Our data analysis uncovers the
severity of the "bucket effect" in medication recommendation. To fill this gap,
we introduce a cross-modal medication encoder capable of seamlessly aligning
data from different modalities and propose a medication recommendation
framework to integrate Multiple types of Knowledge, named MKMed. Specifically,
we first pre-train a cross-modal encoder with contrastive learning on five
knowledge modalities, aligning them into a unified space. Then, we combine the
multi-knowledge medication representations with patient records for
recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets
demonstrate that MKMed mitigates the "bucket effect" in data, and significantly
outperforms state-of-the-art baselines in recommendation accuracy and safety.

</details>


### [89] [Pseudo-Boolean Proof Logging for Optimal Classical Planning](https://arxiv.org/abs/2504.18443)
*Simon Dold,Malte Helmert,Jakob Nordström,Gabriele Röger,Tanja Schindler*

Main category: cs.AI

TL;DR: 该论文提出了一种用于经典规划任务的低界证书，能够证明任务无解或计划最优性，并通过伪布尔约束框架生成这些证书。通过对A*算法的修改，结合模式数据库和h^max启发式，展示了如何高效生成最优性证明。


<details>
  <summary>Details</summary>
Motivation: 动机是开发一种能够独立验证规划任务无解或计划最优性的方法，为规划领域提供可验证的证明机制，增强算法的透明度和可信度。

Method: 方法是通过伪布尔约束框架生成低界证书，修改A*算法以生成最优性证明，并以模式数据库和h^max启发式为例进行具体实现。

Result: 结果表明，这种方法能够在不显著增加计算开销的情况下，生成可验证的最优性证明，适用于任何能够高效表达伪布尔约束的启发式。

Conclusion: 结论是所提出的低界证书框架为规划任务提供了可验证的证明方法，扩展了A*算法的应用范围，增强了规划系统的可靠性和透明度。

Abstract: We introduce lower-bound certificates for classical planning tasks, which can
be used to prove the unsolvability of a task or the optimality of a plan in a
way that can be verified by an independent third party. We describe a general
framework for generating lower-bound certificates based on pseudo-Boolean
constraints, which is agnostic to the planning algorithm used.
  As a case study, we show how to modify the $A^{*}$ algorithm to produce
proofs of optimality with modest overhead, using pattern database heuristics
and $h^\textit{max}$ as concrete examples. The same proof logging approach
works for any heuristic whose inferences can be efficiently expressed as
reasoning over pseudo-Boolean constraints.

</details>


### [90] [Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation](https://arxiv.org/abs/2504.18453)
*Peiyuan Jing,Kinhei Lee,Zhenxuan Zhang,Huichi Zhou,Zhengqing Yuan,Zhifan Gao,Lei Zhu,Giorgos Papanastasiou,Yingying Fang,Guang Yang*

Main category: cs.AI

TL;DR: 本文提出BoxMed-RL框架，通过两阶段训练（预训练和下游适配）生成可空间验证和解释的放射学报告，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前放射学报告生成模型缺乏专家结构化推理能力，难以为临床提供可信和可解释的报告。该研究旨在通过模仿放射科医生工作流程，将视觉发现与解剖位置精确关联，提升报告质量。

Method: BoxMed-RL采用统一训练框架：1）预训练阶段通过医学概念学习和空间验证强化优化模型；2）下游适配阶段冻结预训练权重并训练适配器确保报告流畅性和临床可信度。

Result: 实验表明，BoxMed-RL在METEOR和ROUGE-L指标上平均提升7%，在大语言模型指标上提升5%，显著优于现有方法。

Conclusion: BoxMed-RL通过结合空间验证和医学概念推理，显著提升了放射学报告的生成质量和可解释性，为临床实践提供了更可靠的工具。

Abstract: Radiology report generation is critical for efficiency but current models
lack the structured reasoning of experts, hindering clinical trust and
explainability by failing to link visual findings to precise anatomical
locations. This paper introduces BoxMed-RL, a groundbreaking unified training
framework for generating spatially verifiable and explainable radiology
reports. Built on a large vision-language model, BoxMed-RL revolutionizes
report generation through two integrated phases: (1) In the Pretraining Phase,
we refine the model via medical concept learning, using Chain-of-Thought
supervision to internalize the radiologist-like workflow, followed by spatially
verifiable reinforcement, which applies reinforcement learning to align medical
findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze
the pretrained weights and train a downstream adapter to ensure fluent and
clinically credible reports. This framework precisely mimics radiologists'
workflow, compelling the model to connect high-level medical concepts with
definitive anatomical evidence. Extensive experiments on public datasets
demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR
and ROUGE-L metrics compared to state-of-the-art methods. An average 5%
improvement in large language model-based metrics further underscores
BoxMed-RL's robustness in generating high-quality radiology reports.

</details>


### [91] [Scaling Laws For Scalable Oversight](https://arxiv.org/abs/2504.18530)
*Joshua Engels,David D. Baek,Subhash Kantamneni,Max Tegmark*

Main category: cs.AI

TL;DR: 该论文提出了一种量化监督成功概率的框架，通过监督游戏模型和Elo评分，研究了在不同能力差距下监督的可行性，尤其在嵌套可扩展监督（NSO）中测试了其效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决可扩展监督（即较弱AI系统监督更强AI系统）在实际应用中的效率问题，论文旨在量化监督的成功概率，并探讨其在能力差异较大的系统中的表现。

Method: 通过建模监督为能力不匹配玩家之间的游戏，使用Elo评分（包含任务无能和饱和阶段的片段线性函数）来评估监督效果。在Nim等游戏中验证框架，并将其应用于“Mafia”、“Debate”等监督游戏，最后在嵌套可扩展监督（NSO）中进行数值和理论分析。

Result: 监督成功率随AI系统能力差距增大而下降，例如在400 Elo差距下NSO成功率低于52%。研究还推导了最大化监督成功概率的最优监督层级数量。

Conclusion: 论文表明监督效果受能力差距显著影响，嵌套监督虽有一定效果，但面对更强系统时成功率较低，需进一步优化框架或探索替代方案。

Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger
ones, has been proposed as a key strategy to control future superintelligent
systems. However, it is still unclear how scalable oversight itself scales. To
address this gap, we propose a framework that quantifies the probability of
successful oversight as a function of the capabilities of the overseer and the
system being overseen. Specifically, our framework models oversight as a game
between capability-mismatched players; the players have oversight-specific and
deception-specific Elo scores that are a piecewise-linear function of their
general intelligence, with two plateaus corresponding to task incompetence and
task saturation. We validate our framework with a modified version of the game
Nim and then apply it to four oversight games: "Mafia", "Debate", "Backdoor
Code" and "Wargames". For each game, we find scaling laws that approximate how
domain performance depends on general AI system capability (using Chatbot Arena
Elo as a proxy for general capability). We then build on our findings in a
theoretical study of Nested Scalable Oversight (NSO), a process in which
trusted models oversee untrusted stronger models, which then become the trusted
models in the next step. We identify conditions under which NSO succeeds and
derive numerically (and in some cases analytically) the optimal number of
oversight levels to maximize the probability of oversight success. In our
numerical examples, the NSO success rate is below 52% when overseeing systems
that are 400 Elo points stronger than the baseline overseer, and it declines
further for overseeing even stronger systems.

</details>


### [92] [Adapting Probabilistic Risk Assessment for AI](https://arxiv.org/abs/2504.18536)
*Anna Katariina Wisakanto,Joe Rogero,Avyay M. Casheekar,Richard Mallah*

Main category: cs.AI

TL;DR: 本文提出了一个针对现代通用人工智能（AI）系统的概率风险评估（PRA）框架，旨在系统性识别和评估AI可能对社会和生物圈带来的直接或间接风险。


<details>
  <summary>Details</summary>
Motivation: 现代通用AI系统的快速发展和潜在灾难性风险超出了现有评估方法的能力，因此需要一种更系统、可靠的评估框架。

Method: 该框架采用概率风险评估技术，结合面向方面的危害分析、风险路径建模和不确定性管理，为AI风险提供量化评估。

Result: 实现了一个工具，通过系统化方法生成风险报告卡，整合多种评估方法并提供可比的风险量化估计。

Conclusion: 该框架为AI开发者、评估者和监管者提供了一种系统性评估和管理AI风险的工具。

Abstract: Modern general-purpose artificial intelligence (AI) systems present an urgent
risk management challenge, as their rapidly evolving capabilities and potential
for catastrophic harm outpace our ability to reliably assess their risks.
Current methods often rely on selective testing and undocumented assumptions
about risk priorities, frequently failing to make a serious attempt at
assessing the set of pathways through which Al systems pose direct or indirect
risks to society and the biosphere. This paper introduces the probabilistic
risk assessment (PRA) for AI framework, adapting established PRA techniques
from high-reliability industries (e.g., nuclear power, aerospace) for the new
challenges of advanced AI. The framework guides assessors in identifying
potential risks, estimating likelihood and severity, and explicitly documenting
evidence, underlying assumptions, and analyses at appropriate granularities.
The framework's implementation tool synthesizes the results into a risk report
card with aggregated risk estimates from all assessed risks. This systematic
approach integrates three advances: (1) Aspect-oriented hazard analysis
provides systematic hazard coverage guided by a first-principles taxonomy of AI
system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk
pathway modeling analyzes causal chains from system aspects to societal impacts
using bidirectional analysis and incorporating prospective techniques; and (3)
Uncertainty management employs scenario decomposition, reference scales, and
explicit tracing protocols to structure credible projections with novelty or
limited data. Additionally, the framework harmonizes diverse assessment methods
by integrating evidence into comparable, quantified absolute risk estimates for
critical decisions. We have implemented this as a workbook tool for AI
developers, evaluators, and regulators, available on the project website.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [93] [A computational model of infant sensorimotor exploration in the mobile paradigm](https://arxiv.org/abs/2504.17939)
*Josua Spisak,Sergiu Tcaci Popescu,Stefan Wermter,Matej Hoffmann,J. Kevin O'Regan*

Main category: q-bio.NC

TL;DR: 提出了一种计算模型，模拟婴儿在“移动范式”中学习动作与感觉效应的行为，发现模型能复现婴儿偏好移动连接肢体的现象，并揭示了动作-结果预测、探索、运动噪声和生物启发运动控制在其中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 研究婴儿如何在‘移动范式’中学习感知运动关联性，这是发展认知能力的基础。

Method: 构建了一个包含神经网络、动作-结果预测、探索、运动噪声、偏好活动水平和生物启发运动控制的计算模型，模拟婴儿行为。

Result: 模型成功复现了婴儿偏好移动连接肢体的经典现象，并解释了断开连接后的突发运动；同时复现了渐进式和全或无连接的实验数据。

Conclusion: 模型证实动作-结果预测等机制对婴儿感知运动学习至关重要，揭示了其认知发展的潜在机制。

Abstract: We present a computational model of the mechanisms that may determine
infants' behavior in the "mobile paradigm". This paradigm has been used in
developmental psychology to explore how infants learn the sensory effects of
their actions. In this paradigm, a mobile (an articulated and movable object
hanging above an infant's crib) is connected to one of the infant's limbs,
prompting the infant to preferentially move that "connected" limb. This ability
to detect a "sensorimotor contingency" is considered to be a foundational
cognitive ability in development. To understand how infants learn sensorimotor
contingencies, we built a model that attempts to replicate infant behavior. Our
model incorporates a neural network, action-outcome prediction, exploration,
motor noise, preferred activity level, and biologically-inspired motor control.
We find that simulations with our model replicate the classic findings in the
literature showing preferential movement of the connected limb. An interesting
observation is that the model sometimes exhibits a burst of movement after the
mobile is disconnected, casting light on a similar occasional finding in
infants. In addition to these general findings, the simulations also replicate
data from two recent more detailed studies using a connection with the mobile
that was either gradual or all-or-none. A series of ablation studies further
shows that the inclusion of mechanisms of action-outcome prediction,
exploration, motor noise, and biologically-inspired motor control was essential
for the model to correctly replicate infant behavior. This suggests that these
components are also involved in infants' sensorimotor learning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [94] [Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning](https://arxiv.org/abs/2504.17950)
*Isadora White,Kolby Nottingham,Ayush Maniar,Max Robinson,Hansen Lillemark,Mehul Maheshwari,Lianhui Qin,Prithviraj Ammanabrolu*

Main category: cs.MA

TL;DR: 这篇论文研究了LLM如何通过自适应协作完成复杂的具身推理任务，提出了MINDcraft平台和MineCollab基准测试。实验发现当前先进的LLM代理在自然语言交流中存在瓶颈，协作性能下降可达15%。结论表明现有LLM代理在多代理协作中表现不佳，需要改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLM如何在开放的Minecraft游戏中通过协作完成复杂任务，尤其是在具身场景中如何高效协作。

Method: 方法包括开发MINDcraft平台和MineCollab基准测试，用于测试LLM代理在具身和协作推理中的表现。

Result: 实验结果显示，当前先进的LLM代理在需要详细任务计划交流时，协作性能下降高达15%，表明自然语言交流是主要瓶颈。

Conclusion: 论文结论指出，现有LLM代理在多代理协作（尤其是具身场景）中表现不足，需要在上下文学习和模仿学习之外探索新方法。

Abstract: Collaboration is ubiquitous and essential in day-to-day life -- from
exchanging ideas, to delegating tasks, to generating plans together. This work
studies how LLMs can adaptively collaborate to perform complex embodied
reasoning tasks. To this end we introduce MINDcraft, an easily extensible
platform built to enable LLM agents to control characters in the open-world
game of Minecraft; and MineCollab, a benchmark to test the different dimensions
of embodied and collaborative reasoning. An experimental study finds that the
primary bottleneck in collaborating effectively for current state-of-the-art
agents is efficient natural language communication, with agent performance
dropping as much as 15% when they are required to communicate detailed task
completion plans. We conclude that existing LLM agents are ill-optimized for
multi-agent collaboration, especially in embodied scenarios, and highlight the
need to employ methods beyond in-context and imitation learning. Our website
can be found here: https://mindcraft-minecollab.github.io/

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [95] [Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models](https://arxiv.org/abs/2504.17807)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu,Yihan Zhang,Shuyang Ji*

Main category: cs.NI

TL;DR: 本文提出了一种基于大语言模型（LLM）的网络流量监控与异常检测系统，通过结合传统模型（如自编码器和决策树）并利用LLM处理序列数据，能更好地捕捉复杂模式和微小波动。实验表明，该混合模型在检测准确率和计算效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着云平台的快速发展和网络流量复杂度的增加，传统的网络监控与异常检测方法难以应对，需要更高效、准确的解决方案。

Method: 结合Transformer架构的注意力机制与监督学习框架，使用预训练的LLM分析流量并添加考虑时序和上下文的异常检测层，同时采用迁移学习提升模型的适应性。

Result: 实验结果显示，该模型在检测准确率和计算效率上优于传统方法，能有效识别零日攻击和流量拥塞等异常，并显著降低误报率。

Conclusion: 提出的LLM混合模型在网络流量监控与异常检测中表现出色，尤其在处理复杂模式和适应性方面具有优势，为网络安全和性能提供了可靠支持。

Abstract: The rapidly evolving cloud platforms and the escalating complexity of network
traffic demand proper network traffic monitoring and anomaly detection to
ensure network security and performance. This paper introduces a large language
model (LLM)-based network traffic monitoring and anomaly detection system. In
addition to existing models such as autoencoders and decision trees, we harness
the power of large language models for processing sequence data from network
traffic, which allows us a better capture of underlying complex patterns, as
well as slight fluctuations in the dataset. We show for a given detection task,
the need for a hybrid model that incorporates the attention mechanism of the
transformer architecture into a supervised learning framework in order to
achieve better accuracy. A pre-trained large language model analyzes and
predicts the probable network traffic, and an anomaly detection layer that
considers temporality and context is added. Moreover, we present a novel
transfer learning-based methodology to enhance the model's effectiveness to
quickly adapt to unknown network structures and adversarial conditions without
requiring extensive labeled datasets. Actual results show that the designed
model outperforms traditional methods in detection accuracy and computational
efficiency, effectively identify various network anomalies such as zero-day
attacks and traffic congestion pattern, and significantly reduce the false
positive rate.

</details>


### [96] [LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control](https://arxiv.org/abs/2504.18062)
*Lingyan Bao,Sinwoong Yun,Jemin Lee,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 本文提出了一种LLM赋能的层次化RIC框架（LLM-hRIC），通过结合大型语言模型（LLM）和强化学习（RL）来优化无线通信网络中的资源管理。该框架在IAB网络环境中表现出优异性能，并探讨了LLM在O-RAN中的未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）和开放无线接入网（O-RAN）技术的发展，如何高效协同不同时间尺度的无线资源管理成为研究重点。本文旨在利用LLM和RL的结合，提升RIC之间的协作效率。

Method: 提出LLM-hRIC框架，将LLM赋能的非实时RIC（non-RT RIC）用于提供战略性指导，RL赋能的近实时RIC（near-RT RIC）执行低延迟任务，结合环境上下文和本地观测优化资源管理。

Result: 在IAB网络仿真中，该框架表现出卓越的性能，验证了其在实际应用中的有效性。

Conclusion: LLM-hRIC框架为O-RAN中的智能资源管理提供了新思路，但LLM在O-RAN中的实际应用仍需解决多项挑战。

Abstract: Recent advancements in large language models (LLMs) have led to a significant
interest in deploying LLMempowered algorithms for wireless communication
networks. Meanwhile, open radio access network (O-RAN) techniques offer
unprecedented flexibility, with the non-real-time (non-RT) radio access network
(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)
RIC (near-RT RIC) components enabling intelligent resource management across
different time scales. In this paper, we propose the LLM empowered hierarchical
RIC (LLM-hRIC) framework to improve the collaboration between RICs. This
framework integrates LLMs with reinforcement learning (RL) for efficient
network resource management. In this framework, LLMs-empowered non-RT RICs
provide strategic guidance and high-level policies based on environmental
context. Concurrently, RL-empowered near-RT RICs perform low-latency tasks
based on strategic guidance and local near-RT observation. We evaluate the
LLM-hRIC framework in an integrated access and backhaul (IAB) network setting.
Simulation results demonstrate that the proposed framework achieves superior
performance. Finally, we discuss the key future challenges in applying LLMs to
O-RAN.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [97] [SOFARI-R: High-Dimensional Manifold-Based Inference for Latent Responses](https://arxiv.org/abs/2504.17874)
*Zemin Zheng,Xin Zhou,Jinchi Lv*

Main category: stat.ME

TL;DR: 该论文提出了SOFARI-R方法，用于高维流形上的多任务学习推断，重点关注潜在右因子向量的有效推断，通过两种变体处理强正交和弱正交因子，并展示了其有效性和应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有SOFAR方法在潜在左因子向量和奇异值的推断上取得进展，但对右因子向量的推断仍具挑战性，因其与左向量不对称。因此，需要一种新方法解决右因子向量的推断问题。

Method: 提出SOFARI-R方法，包括两种变体：一、针对强正交因子，通过耦合左奇异向量与设计矩阵并重新缩放生成新Stiefel流形；二、针对弱正交因子，使用硬阈值SOFARI估计并将近似误差纳入分布。

Result: 两种变体均能生成偏差校正的潜在右因子向量估计量，具有渐近正态分布和合理的渐近方差估计。模拟研究和经济应用验证了方法的有效性。

Conclusion: SOFARI-R方法在多任务学习中实现了对潜在右因子向量的有效推断，提供了偏差校正估计和渐近正态性，扩展了SOFAR框架的适用性。

Abstract: Data reduction with uncertainty quantification plays a key role in various
multi-task learning applications, where large numbers of responses and features
are present. To this end, a general framework of high-dimensional
manifold-based SOFAR inference (SOFARI) was introduced recently in Zheng, Zhou,
Fan and Lv (2024) for interpretable multi-task learning inference focusing on
the left factor vectors and singular values exploiting the latent singular
value decomposition (SVD) structure. Yet, designing a valid inference procedure
on the latent right factor vectors is not straightforward from that of the left
ones and can be even more challenging due to asymmetry of left and right
singular vectors in the response matrix. To tackle these issues, in this paper
we suggest a new method of high-dimensional manifold-based SOFAR inference for
latent responses (SOFARI-R), where two variants of SOFARI-R are introduced. The
first variant deals with strongly orthogonal factors by coupling left singular
vectors with the design matrix and then appropriately rescaling them to
generate new Stiefel manifolds. The second variant handles the more general
weakly orthogonal factors by employing the hard-thresholded SOFARI estimates
and delicately incorporating approximation errors into the distribution. Both
variants produce bias-corrected estimators for the latent right factor vectors
that enjoy asymptotically normal distributions with justified asymptotic
variance estimates. We demonstrate the effectiveness of the newly suggested
method using extensive simulation studies and an economic application.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [98] [Non-identifiability distinguishes Neural Networks among Parametric Models](https://arxiv.org/abs/2504.18017)
*Sourav Chatterjee,Timothy Sudijono*

Main category: math.ST

TL;DR: 论文证明了前馈神经网络在回归任务中与传统的参数模型有本质区别：神经网络总能学习到变量间的非平凡关系，而某些参数模型在某些条件下只能学习到常数预测器。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络与传统参数模型的本质差异，尤其是在回归任务中的表现。

Method: 通过理论证明，比较前馈神经网络和传统参数模型在变量关系学习和预测能力上的差异。

Result: 神经网络总能学习变量间的非平凡关系，而某些参数模型在特定条件下无法做到。

Conclusion: 神经网络因其在可识别性上的独特性而区别于传统平滑参数模型。

Abstract: One of the enduring problems surrounding neural networks is to identify the
factors that differentiate them from traditional statistical models. We prove a
pair of results which distinguish feedforward neural networks among parametric
models at the population level, for regression tasks. Firstly, we prove that
for any pair of random variables $(X,Y)$, neural networks always learn a
nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove
that for reasonable smooth parametric models, under local and global
identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which
the parametric model learns the constant predictor $\mathbb{E}[Y]$. Together,
our results suggest that a lack of identifiability distinguishes neural
networks among the class of smooth parametric models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [99] [Subfunction Structure Matters: A New Perspective on Local Optima Networks](https://arxiv.org/abs/2504.17799)
*S. L. Thomson,M. W. Przewozniczek*

Main category: cs.NE

TL;DR: 该论文提出了一种改进的局部最优网络（LON）分析方法，通过结合子函数信息（无论是先验还是学习到的）来丰富优化动态的洞察。与传统黑盒方法不同，它利用问题结构（如变量交互）提升LON的构建与分析。实验在伪布尔问题上验证了基于确定性灰盒交叉和学习变量交互的两种新方法，结果表明结构信息的引入能更深入理解问题求解难度。


<details>
  <summary>Details</summary>
Motivation: 传统LON构建和分析忽略问题结构信息（如变量交互），限制了其对优化动态的深入理解。本文旨在探索如何通过利用子函数信息改进LON分析，从而更好地揭示问题求解的难点。

Method: 提出三种LON构建方法：1. 标准算法（黑盒）；2. 基于确定性灰盒交叉的算法；3. 基于学习变量交互的扰动选择算法。并提出新的子函数变化相关指标，与传统指标对比。

Result: 实验证明，结合问题结构（如灰盒交叉或学习到的交互信息）能显著丰富LON分析结果，提供更全面的优化动态洞察，尤其对具有已知/疑似子函数结构的问题。

Conclusion: 建议将问题结构信息纳入LON分析范式，特别是针对具有子函数结构的问题。这一方法可能为理解现代链路学习优化器的求解难度提供关键信息。

Abstract: Local optima networks (LONs) capture fitness landscape information. They are
typically constructed in a black-box manner; information about the problem
structure is not utilised. This also applies to the analysis of LONs: knowledge
about the problem, such as interaction between variables, is not considered. We
challenge this status-quo with an alternative approach: we consider how LON
analysis can be improved by incorporating subfunction-based information - this
can either be known a-priori or learned during search. To this end, LONs are
constructed for several benchmark pseudo-boolean problems using three
approaches: firstly, the standard algorithm; a second algorithm which uses
deterministic grey-box crossover; and a third algorithm which selects
perturbations based on learned information about variable interactions. Metrics
related to subfunction changes in a LON are proposed and compared with metrics
from previous literature which capture other aspects of a LON. Incorporating
problem structure in LON construction and analysing it can bring enriched
insight into optimisation dynamics. Such information may be crucial to
understanding the difficulty of solving a given problem with state-of-the-art
linkage learning optimisers. In light of the results, we suggest incorporation
of problem structure as an alternative paradigm in landscape analysis for
problems with known or suspected subfunction structure.

</details>


### [100] [Evolution of Optimization Algorithms for Global Placement via Large Language Models](https://arxiv.org/abs/2504.17801)
*Xufeng Yao,Jiaxi Jiang,Yuxuan Zhao,Peiyu Liao,Yibo Lin,Bei Yu*

Main category: cs.NE

TL;DR: 本文提出了一种利用大语言模型（LLM）自动演化优化算法的框架，用于全球布局问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 设计优化算法通常需要大量专业知识和手动调整，全球布局又是电子设计自动化（EDA）中的关键步骤，但当前的优化算法仍依赖启发式和定制组件。因此，本文旨在通过LLM自动演化算法以解决这一问题。

Method: 采用LLM生成多样化的候选算法，并通过LLM驱动的遗传流程优化这些算法。

Result: 在多个基准测试中，发现的算法平均HPWL提升了5.05%至8.30%，个别案例甚至达到17%，且表现出良好的泛化能力。

Conclusion: 该方法不仅性能优越，还能与现有参数调优方法互补，为自动化算法设计提供了新思路。

Abstract: Optimization algorithms are widely employed to tackle complex problems, but
designing them manually is often labor-intensive and requires significant
expertise. Global placement is a fundamental step in electronic design
automation (EDA). While analytical approaches represent the state-of-the-art
(SOTA) in global placement, their core optimization algorithms remain heavily
dependent on heuristics and customized components, such as initialization
strategies, preconditioning methods, and line search techniques. This paper
presents an automated framework that leverages large language models (LLM) to
evolve optimization algorithms for global placement. We first generate diverse
candidate algorithms using LLM through carefully crafted prompts. Then we
introduce an LLM-based genetic flow to evolve selected candidate algorithms.
The discovered optimization algorithms exhibit substantial performance
improvements across many benchmarks. Specifically, Our design-case-specific
discovered algorithms achieve average HPWL improvements of \textbf{5.05\%},
\text{5.29\%} and \textbf{8.30\%} on MMS, ISPD2005 and ISPD2019 benchmarks, and
up to \textbf{17\%} improvements on individual cases. Additionally, the
discovered algorithms demonstrate good generalization ability and are
complementary to existing parameter-tuning methods.

</details>


### [101] [Fuzzy Logic -- Based Scheduling System for Part-Time Workforce](https://arxiv.org/abs/2504.17805)
*Tri Nguyen,Kelly Cohen*

Main category: cs.NE

TL;DR: 论文研究了如何利用遗传模糊系统高效生成大学兼职学生的工作排班表，考虑了员工偏好工作时间和可用性等因素。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决大学兼职学生排班的复杂问题，确保排班满足运营需求和员工偏好。

Method: 方法是通过遗传模糊系统算法，结合员工的可用性和偏好数据，生成可行的排班方案。

Result: 结果显示算法能高效生成符合运营标准的排班表，并在人员不足时表现稳健。

Conclusion: 结论是遗传模糊系统能有效解决复杂排班问题，适用于类似场景。

Abstract: This paper explores the application of genetic fuzzy systems to efficiently
generate schedules for a team of part-time student workers at a university.
Given the preferred number of working hours and availability of employees, our
model generates feasible solutions considering various factors, such as maximum
weekly hours, required number of workers on duty, and the preferred number of
working hours. The algorithm is trained and tested with availability data
collected from students at the University of Cincinnati. The results
demonstrate the algorithm's efficiency in producing schedules that meet
operational criteria and its robustness in understaffed conditions.

</details>


### [102] [Evolution Meets Diffusion: Efficient Neural Architecture Generation](https://arxiv.org/abs/2504.17827)
*Bingye Zhou,Caiyang Yu*

Main category: cs.NE

TL;DR: 论文提出EDNAG方法，结合进化算法和扩散模型，实现高效且无需训练的神经网络架构生成，显著提升精度和速度。


<details>
  <summary>Details</summary>
Motivation: 解决NAS因搜索空间大导致的计算和时间成本高的问题，以及现有方法在全局搜索能力和效率上的局限。

Method: EDNAG利用进化算法模拟扩散模型的去噪过程，通过适应度指导从随机高斯分布到最优架构分布的转换。

Result: EDNAG在架构优化中达到SOTA性能，精度提升高达10.45%，推理速度平均提升50倍。

Conclusion: EDNAG展示了高效且无需训练的架构生成潜力，有望推动NAS领域的发展。

Abstract: Neural Architecture Search (NAS) has gained widespread attention for its
transformative potential in deep learning model design. However, the vast and
complex search space of NAS leads to significant computational and time costs.
Neural Architecture Generation (NAG) addresses this by reframing NAS as a
generation problem, enabling the precise generation of optimal architectures
for specific tasks. Despite its promise, mainstream methods like diffusion
models face limitations in global search capabilities and are still hindered by
high computational and time demands. To overcome these challenges, we propose
Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel
approach that achieves efficient and training-free architecture generation.
EDNAG leverages evolutionary algorithms to simulate the denoising process in
diffusion models, using fitness to guide the transition from random Gaussian
distributions to optimal architecture distributions. This approach combines the
strengths of evolutionary strategies and diffusion models, enabling rapid and
effective architecture generation. Extensive experiments demonstrate that EDNAG
achieves state-of-the-art (SOTA) performance in architecture optimization, with
an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need
for time-consuming training and boosts inference speed by an average of 50
times, showcasing its exceptional efficiency and effectiveness.

</details>


### [103] [Near-Driven Autonomous Rover Navigation in Complex Environments: Extensions to Urban Search-and-Rescue and Industrial Inspection](https://arxiv.org/abs/2504.17794)
*Dhadkan Shrestha,Lincoln Bhattarai*

Main category: cs.NE

TL;DR: 该论文研究了一种基于NEAT的扩展神经进化方法，用于危险任务中的自主机器人，展示了NEAT的适应性和在复杂任务中的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探讨NEAT方法在动态环境中自主机器人任务（如消防、救援和工业检查）中的应用潜力，作为深度强化学习的替代或补充。

Method: 通过模拟环境扩展和混合算法优化，结合NEAT与强化学习的最新进展，测试其性能。

Result: 实验结果中，NEAT控制器的成功率与深度强化学习方法相当（约80%），且结构适应性更优。

Conclusion: NEAT在多样性任务中表现出色，具备部署潜力，可替代或补充传统深度强化学习方法。

Abstract: This paper explores the use of an extended neuroevolutionary approach, based
on NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in
dynamic environments associated with hazardous tasks like firefighting, urban
search-and-rescue (USAR), and industrial inspections. Building on previous
research, it expands the simulation environment to larger and more complex
settings, demonstrating NEAT's adaptability across different applications. By
integrating recent advancements in NEAT and reinforcement learning, the study
uses modern simulation frameworks for realism and hybrid algorithms for
optimization. Experimental results show that NEAT-evolved controllers achieve
success rates comparable to state-of-the-art deep reinforcement learning
methods, with superior structural adaptability. The agents reached ~80% success
in outdoor tests, surpassing baseline models. The paper also highlights the
benefits of transfer learning among tasks and evaluates the effectiveness of
NEAT in complex 3D navigation. Contributions include evaluating NEAT for
diverse autonomous applications and discussing real-world deployment
considerations, emphasizing the approach's potential as an alternative or
complement to deep reinforcement learning in autonomous navigation tasks.

</details>


### [104] [Switch-Based Multi-Part Neural Network](https://arxiv.org/abs/2504.18241)
*Surajit Majumder,Paritosh Ranjan,Prodip Roy,Bhuban Padhan*

Main category: cs.NE

TL;DR: 论文提出了一种去中心化和模块化的神经网络框架，通过动态切换机制选择性激活和训练神经元，提高AI系统的可扩展性、可解释性和性能，适用于边缘计算和分布式环境。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统神经网络在可扩展性、可解释性和性能上的局限性，并适应分布式环境的需求，论文提出了一个模块化且去中心化的框架。

Method: 采用动态切换机制选择性激活和训练神经元，使神经元专注于数据的特定部分。结合联邦学习和分散训练，模拟非重叠数据子集的本地训练。

Result: 展示了模块化网络在分布式环境中的高效训练和评估，同时提升了模型的可扩展性和可解释性。

Conclusion: 该框架为设计可扩展、隐私保护且高效的AI系统提供了潜力，适用于多样化应用场景。

Abstract: This paper introduces decentralized and modular neural network framework
designed to enhance the scalability, interpretability, and performance of
artificial intelligence (AI) systems. At the heart of this framework is a
dynamic switch mechanism that governs the selective activation and training of
individual neurons based on input characteristics, allowing neurons to
specialize in distinct segments of the data domain. This approach enables
neurons to learn from disjoint subsets of data, mimicking biological brain
function by promoting task specialization and improving the interpretability of
neural network behavior. Furthermore, the paper explores the application of
federated learning and decentralized training for real-world AI deployments,
particularly in edge computing and distributed environments. By simulating
localized training on non-overlapping data subsets, we demonstrate how modular
networks can be efficiently trained and evaluated. The proposed framework also
addresses scalability, enabling AI systems to handle large datasets and
distributed processing while preserving model transparency and
interpretability. Finally, we discuss the potential of this approach in
advancing the design of scalable, privacy-preserving, and efficient AI systems
for diverse applications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [105] [EduBot -- Can LLMs Solve Personalized Learning and Programming Assignments?](https://arxiv.org/abs/2504.17824)
*Yibin Wang,Jiaxi Xie,Lakshminarayanan Subramanian*

Main category: cs.SE

TL;DR: EduBot是一个基于LLMs的智能编程助手，通过递归提示驱动方法解决复杂编程任务，包括概念教学、代码开发和调试，无需微调LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索预训练LLMs在多步推理和代码生成中的潜力，解决个性化编程任务，减少人工干预。

Method: 结合概念教学、端到端代码开发、个性化编程和调试，通过递归提示驱动方法，利用LLMs完成任务。

Result: 在算法、机器学习和实际问题等20个场景的测试中，EduBot能在20分钟内完成大部分任务。

Conclusion: EduBot展示了预训练LLMs在多步推理和代码生成中的潜力，为个性化编程任务提供了有效解决方案。

Abstract: The prevalence of Large Language Models (LLMs) is revolutionizing the process
of writing code. General and code LLMs have shown impressive performance in
generating standalone functions and code-completion tasks with one-shot
queries. However, the ability to solve comprehensive programming tasks with
recursive requests and bug fixes remains questionable. In this paper, we
propose EduBot, an intelligent automated assistant system that combines
conceptual knowledge teaching, end-to-end code development, personalized
programming through recursive prompt-driven methods, and debugging with limited
human interventions powered by LLMs. We show that EduBot can solve complicated
programming tasks consisting of sub-tasks with increasing difficulties ranging
from conceptual to coding questions by recursive automatic prompt-driven
systems without finetuning on LLMs themselves. To further evaluate EduBot's
performance, we design and conduct a benchmark suite consisting of 20 scenarios
in algorithms, machine learning, and real-world problems. The result shows that
EduBot can complete most scenarios in less than 20 minutes. Based on the
benchmark suites, we perform a comparative study to take different LLMs as the
backbone and to verify EduBot's compatibility and robustness across LLMs with
varying capabilities. We believe that EduBot is an exploratory approach to
explore the potential of pre-trained LLMs in multi-step reasoning and code
generation for solving personalized assignments with knowledge learning and
code generation.

</details>


### [106] [Validating Network Protocol Parsers with Traceable RFC Document Interpretation](https://arxiv.org/abs/2504.18050)
*Mingwei Zheng,Danning Xie,Qingkai Shi,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 该论文提出了一种利用大语言模型（LLMs）自动从协议规范文档生成形式化协议消息规范的方法，解决了网络协议实现验证中的oracle和traceability问题，并在多个协议实现中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 网络协议实现的正确性验证面临oracle问题（如何判断实现是否有bug）和traceability问题（如何追踪bug的来源）。现有方法很少同时解决这两个问题，因此作者提出了一种结合LLMs的自动化方案。

Method: 通过LLMs将结构化的协议规范文档（如RFC）翻译为形式化协议消息规范，作为quasi-oracle验证协议解析器，并通过验证结果逐步修正oracle。由于oracle源自文档，发现的bug可追踪到文档。

Result: 在9个网络协议及其C、Python、Go实现中评估，方法优于当前最优技术，检测到69个bug（36个已确认），证明了基于自然语言规范自动化验证软件的潜力。

Conclusion: 该方法不仅高效解决了oracle和traceability问题，还展示了自动化软件验证的可能性，减少了传统手动解析规范文档和生成测试用例的工作量。

Abstract: Validating the correctness of network protocol implementations is highly
challenging due to the oracle and traceability problems. The former determines
when a protocol implementation can be considered buggy, especially when the
bugs do not cause any observable symptoms. The latter allows developers to
understand how an implementation violates the protocol specification, thereby
facilitating bug fixes. Unlike existing works that rarely take both problems
into account, this work considers both and provides an effective solution using
recent advances in large language models (LLMs). Our key observation is that
network protocols are often released with structured specification documents,
a.k.a. RFC documents, which can be systematically translated to formal protocol
message specifications via LLMs. Such specifications, which may contain errors
due to the hallucination of LLMs, are used as a quasi-oracle to validate
protocol parsers, while the validation results in return gradually refine the
oracle. Since the oracle is derived from the document, any bugs we find in a
protocol implementation can be traced back to the document, thus addressing the
traceability problem. We have extensively evaluated our approach using nine
network protocols and their implementations written in C, Python, and Go. The
results show that our approach outperforms the state-of-the-art and has
detected 69 bugs, with 36 confirmed. The project also demonstrates the
potential for fully automating software validation based on natural language
specifications, a process previously considered predominantly manual due to the
need to understand specification documents and derive expected outputs for test
inputs.

</details>


### [107] [Towards Adaptive Software Agents for Debugging](https://arxiv.org/abs/2504.18316)
*Yacine Majdoub,Eya Ben Charrada,Haifa Touati*

Main category: cs.SE

TL;DR: 本文提出了一种自适应多智能体设计，动态调整智能体数量和角色以提升大语言模型的调试能力，实验显示对复杂问题修复效果提升11%。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体方法虽能提升调试能力，但存在成本高和智能体易失焦的问题，因此需要一种自适应机制来优化智能体使用。

Method: 提出动态生成智能体数量和角色的方法，根据任务特征自动调整，无需预设角色。

Result: 实验表明，自适应设计能根据代码复杂度生成不同数量的智能体，简单问题仅需一个智能体，复杂问题则生成更多，修复效果平均提升11%。

Conclusion: 自适应设计在调试任务中表现优异，未来将探索进一步提升智能体自主规划和执行软件目标的能力。

Abstract: Using multiple agents was found to improve the debugging capabilities of
Large Language Models. However, increasing the number of LLM-agents has several
drawbacks such as increasing the running costs and rising the risk for the
agents to lose focus. In this work, we propose an adaptive agentic design,
where the number of agents and their roles are determined dynamically based on
the characteristics of the task to be achieved. In this design, the agents
roles are not predefined, but are generated after analyzing the problem to be
solved. Our initial evaluation shows that, with the adaptive design, the number
of agents that are generated depends on the complexity of the buggy code. In
fact, for simple code with mere syntax issues, the problem was usually fixed
using one agent only. However, for more complex problems, we noticed the
creation of a higher number of agents. Regarding the effectiveness of the fix,
we noticed an average improvement of 11% compared to the one-shot prompting.
Given these promising results, we outline future research directions to improve
our design for adaptive software agents that can autonomously plan and conduct
their software goals.

</details>


### [108] [Spatial Reasoner: A 3D Inference Pipeline for XR Applications](https://arxiv.org/abs/2504.18380)
*Steven Häsler,Philipp Ackermann*

Main category: cs.SE

TL;DR: 提出了一种空间推理框架，通过3D边界框和空间谓词构建空间知识图谱，支持语义化3D场景分析，适用于XR应用。


<details>
  <summary>Details</summary>
Motivation: 现代XR系统需要能够对3D场景进行语义化分析，但现有技术缺乏高效的空间推理能力。本文旨在填补这一空白。

Method: 基于定向3D边界框表示，结合空间谓词（如拓扑、方向性等）构建空间知识图谱，并采用流水线推理模型支持动态规则评估。

Result: 框架实现了从几何数据到可操作知识的高效转换，支持复杂的空间查询，并展示了客户端和服务端的高效处理能力。

Conclusion: 该框架为XR应用提供了可扩展且技术无关的空间推理能力，推动了空间本体论的构建，并增强了机器学习、自然语言处理和规则系统的集成。

Abstract: Modern extended reality XR systems provide rich analysis of image data and
fusion of sensor input and demand AR/VR applications that can reason about 3D
scenes in a semantic manner. We present a spatial reasoning framework that
bridges geometric facts with symbolic predicates and relations to handle key
tasks such as determining how 3D objects are arranged among each other ('on',
'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box
representations, enhanced by a comprehensive set of spatial predicates, ranging
from topology and connectivity to directionality and orientation, expressed in
a formalism related to natural language. The derived predicates form a spatial
knowledge graph and, in combination with a pipeline-based inference model,
enable spatial queries and dynamic rule evaluation. Implementations for client-
and server-side processing demonstrate the framework's capability to
efficiently translate geometric data into actionable knowledge, ensuring
scalable and technology-independent spatial reasoning in complex 3D
environments. The Spatial Reasoner framework is fostering the creation of
spatial ontologies, and seamlessly integrates with and therefore enriches
machine learning, natural language processing, and rule systems in XR
applications.

</details>


### [109] [Paradigm shift on Coding Productivity Using GenAI](https://arxiv.org/abs/2504.18404)
*Liang Yu*

Main category: cs.SE

TL;DR: 这篇论文研究了生成式AI在电信和金融科技领域对编码助手（如Codeium、Amazon Q）的应用，发现其在常规任务中提升效率，但在复杂任务中表现不足。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI在软件开发中具有潜力，但缺乏其在工业环境中实际效率的实证证据，因此需要深入研究。

Method: 通过问卷调查和专家访谈，分析任务复杂性、编码技能、领域知识和AI集成对生产力的影响。

Result: 生成式AI能提高常规编码任务（如重构和文档生成）的效率，但在复杂或领域特定的任务中表现不佳，因缺乏上下文感知和定制设计规则支持。

Conclusion: 提出新编码范式，强调迭代提示优化、沉浸式开发环境和自动化代码评估，以更有效地使用生成式AI。

Abstract: Generative AI (GenAI) applications are transforming software engineering by
enabling automated code co-creation. However, empirical evidence on GenAI's
productivity effects in industrial settings remains limited. This paper
investigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q)
within telecommunications and FinTech domains. Through surveys and interviews
with industrial domain-experts, we identify primary productivity-influencing
factors, including task complexity, coding skills, domain knowledge, and GenAI
integration. Our findings indicate that GenAI tools enhance productivity in
routine coding tasks (e.g., refactoring and Javadoc generation) but face
challenges in complex, domain-specific activities due to limited
context-awareness of codebases and insufficient support for customized design
rules. We highlight new paradigms for coding transfer, emphasizing iterative
prompt refinement, immersive development environment, and automated code
evaluation as essential for effective GenAI usage.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [110] [A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification](https://arxiv.org/abs/2504.17819)
*Mohaddeseh Chegini,Ali Mahloojifar*

Main category: eess.IV

TL;DR: 论文提出了一种结合贝叶斯卷积脉冲神经网络的CAD系统，用于医疗图像分类，解决了传统深度SNN的不可靠性问题，并通过实验验证了其准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统深度脉冲神经网络（SNN）在医疗图像分类中存在不可靠性问题，尤其是预测不确定性难以量化，因此需要一种能够量化不确定性的改进方法。

Method: 提出了一种基于深度贝叶斯卷积脉冲神经网络的CAD系统，采用蒙特卡洛Dropout方法作为不确定性量化手段，并应用于多项医疗图像分类任务。

Result: 实验结果表明，所提出的模型不仅准确性高，而且可靠性强，能够成为传统深度学习方法在医疗图像分类中的有效替代方案。

Conclusion: 通过结合贝叶斯方法和SNN，本研究成功提升了CAD系统的可靠性，为医疗图像分类提供了更优的解决方案。

Abstract: The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of
diseases. The development of CADs by leveraging third generation neural
network, namely, Spiking Neural Network (SNN), is essential to utilize of the
benefits of SNNs, such as their event_driven processing, parallelism, low power
consumption, and the ability to process sparse temporal_spatial information.
However, Deep SNN as a deep learning model faces challenges with unreliability.
To deal with unreliability challenges due to inability to quantify the
uncertainty of the predictions, we proposed a deep Bayesian Convolutional
Spiking Neural Network based_CADs with uncertainty_aware module. In this study,
the Monte Carlo Dropout method as Bayesian approximation is used as an
uncertainty quantification method. This method was applied to several medical
image classification tasks. Our experimental results demonstrate that our
proposed model is accurate and reliable and will be a proper alternative to
conventional deep learning for medical image classification.

</details>


### [111] [A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography](https://arxiv.org/abs/2504.18400)
*Yui Lo,Yuqian Chen,Dongnan Liu,Leo Zekelman,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Alexandra J. Golby,Fan Zhang,Weidong Cai,Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: Tract2Shape是一个多模态深度学习框架，用于从白质纤维束追踪数据中高效预测形状测量，优于现有方法并展示强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统白质形状测量方法计算成本高且耗时，难以应用于大规模数据集。

Method: 采用几何（点云）和标量（表格）特征的多模态深度学习框架，结合降维算法预测形状测量。

Result: 在HCP-YA数据集上表现优于现有模型（最高Pearson's r和最低nMSE），在PPMI数据集上展示强泛化能力。

Conclusion: Tract2Shape为大规模白质形状分析提供了快速、准确且可泛化的解决方案。

Abstract: Shape measures have emerged as promising descriptors of white matter
tractography, offering complementary insights into anatomical variability and
associations with cognitive and clinical phenotypes. However, conventional
methods for computing shape measures are computationally expensive and
time-consuming for large-scale datasets due to reliance on voxel-based
representations. We propose Tract2Shape, a novel multimodal deep learning
framework that leverages geometric (point cloud) and scalar (tabular) features
to predict ten white matter tractography shape measures. To enhance model
efficiency, we utilize a dimensionality reduction algorithm for the model to
predict five primary shape components. The model is trained and evaluated on
two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.
We evaluate the performance of Tract2Shape by training and testing it on the
HCP-YA dataset and comparing the results with state-of-the-art models. To
further assess its robustness and generalization ability, we also test
Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep
learning models across all ten shape measures, achieving the highest average
Pearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows
that both multimodal input and PCA contribute to performance gains. On the
unseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low
nMSE, demonstrating strong generalizability in cross-dataset evaluation.
Tract2Shape enables fast, accurate, and generalizable prediction of white
matter shape measures from tractography data, supporting scalable analysis
across datasets. This framework lays a promising foundation for future
large-scale white matter shape analysis.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [112] [Time and Frequency Domain-based Anomaly Detection in Smart Meter Data for Distribution Network Studies](https://arxiv.org/abs/2504.18231)
*Petar Labura,Tomislav Antic,Tomislav Capuder*

Main category: eess.SY

TL;DR: 本文提出了一种基于隔离森林算法和快速傅里叶变换的异常检测框架，用于低电压配电网络中智能电表数据的异常检测，解决现有算法忽视数据质量的问题。


<details>
  <summary>Details</summary>
Motivation: 由于智能电表和先进测量基础设施产生的大量数据在配电网络中被广泛应用，现有数据驱动算法未能有效检测和区分数据异常，从而导致网络状态估计不准确。

Method: 结合隔离森林机器学习算法和快速傅里叶变换滤波，在时域和频域同时检测和缓解主动和无效功率数据集中的异常，包括点异常和上下文异常。

Result: 提出框架能够有效检测和区分电力消耗数据中的异常，适用于高比例智能电表的配电网络。

Conclusion: 集成异常检测方法对配电网络至关重要，本文提出的框架在数据质量和网络状态估计方面具有显著优势。

Abstract: The widespread integration of new technologies in low-voltage distribution
networks on the consumer side creates the need for distribution system
operators to perform advanced real-time calculations to estimate network
conditions. In recent years, data-driven models based on machine learning and
big data analysis have emerged for calculation purposes, leveraging the
information available in large datasets obtained from smart meters and other
advanced measurement infrastructure. However, existing data-driven algorithms
do not take into account the quality of data collected from smart meters. They
lack built-in anomaly detection mechanisms and fail to differentiate anomalies
based on whether the value or context of anomalous data instances deviates from
the norm. This paper focuses on methods for detecting and mitigating the impact
of anomalies on the consumption of active and reactive power datasets. It
proposes an anomaly detection framework based on the Isolation Forest machine
learning algorithm and Fast Fourier Transform filtering that works in both the
time and frequency domain and is unaffected by point anomalies or contextual
anomalies of the power consumption data. The importance of integrating anomaly
detection methods is demonstrated in the analysis important for distribution
networks with a high share of smart meters.

</details>


### [113] [Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise](https://arxiv.org/abs/2504.18444)
*Vinay Kanakeri,Aritra Mitra*

Main category: eess.SY

TL;DR: 论文提出了一种在广义重尾噪声下识别部分观测线性时不变系统参数的新算法，样本复杂度接近子高斯噪声下的结果。


<details>
  <summary>Details</summary>
Motivation: 现有系统识别方法通常假设噪声为高斯或子高斯分布，但实际应用中噪声可能仅为二阶矩有限。本文旨在放宽这一限制。

Method: 采用鲁棒统计工具和Boosting思想设计新算法，仅需噪声存在二阶矩。

Result: 样本复杂度边界接近子高斯噪声假设下的结果，且仅需输入激励过程的四阶矩有限。

Conclusion: 所提算法在更弱的噪声假设下实现了与经典方法媲美的性能，具有理论和实用价值。

Abstract: We consider the problem of system identification of partially observed linear
time-invariant (LTI) systems. Given input-output data, we provide
non-asymptotic guarantees for identifying the system parameters under general
heavy-tailed noise processes. Unlike previous works that assume Gaussian or
sub-Gaussian noise, we consider significantly broader noise distributions that
are required to admit only up to the second moment. For this setting, we
leverage tools from robust statistics to propose a novel system identification
algorithm that exploits the idea of boosting. Despite the much weaker noise
assumptions, we show that our proposed algorithm achieves sample complexity
bounds that nearly match those derived under sub-Gaussian noise. In particular,
we establish that our bounds retain a logarithmic dependence on the prescribed
failure probability. Interestingly, we show that such bounds can be achieved by
requiring just a finite fourth moment on the excitatory input process.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [114] [Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm](https://arxiv.org/abs/2504.17878)
*Xu Wang,Yiquan Wang,Tin-yeh Huang*

Main category: cs.CR

TL;DR: 论文提出了一种基于非编码RNA（ncRNA）的动态折叠特性的生物融合加密框架crypto-ncRNA，以生成高熵、抗量子计算的密钥和不可预测的密文。尽管加密速度略低于AES，但其效率和可扩展性显著优于RSA，并能通过NIST随机性测试。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算时代的到来，传统密码系统的数学基础可能被量子计算攻击破坏，亟需一种抗量子计算的加密方案。

Method: 框架采用多阶段流程：将明文编码为RNA序列，利用先进算法预测和操纵RNA二级结构，并通过RNA分子的物理不可克隆性生成加密密钥。

Result: 实验表明，crypto-ncRNA的加密速度略低于AES，但在效率和可扩展性上显著优于RSA，且能100%通过NIST SP 800-22随机性测试。

Conclusion: crypto-ncRNA为抵御量子计算威胁提供了一种有前景且稳健的加密方案。

Abstract: In the looming post-quantum era, traditional cryptographic systems are
increasingly vulnerable to quantum computing attacks that can compromise their
mathematical foundations. To address this critical challenge, we propose
crypto-ncRNA-a bio-convergent cryptographic framework that leverages the
dynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy,
quantum-resistant keys and produce unpredictable ciphertexts. The framework
employs a novel, multi-stage process: encoding plaintext into RNA sequences,
predicting and manipulating RNA secondary structures using advanced algorithms,
and deriving cryptographic keys through the intrinsic physical unclonability of
RNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's
encryption speed is marginally lower than that of AES, it significantly
outperforms RSA in terms of efficiency and scalability while achieving a 100%
pass rate on the NIST SP 800-22 randomness tests. These results demonstrate
that crypto-ncRNA offers a promising and robust approach for securing digital
infrastructures against the evolving threats posed by quantum computing.

</details>


### [115] [Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)
*Narek Maloyan,Dmitry Namiot*

Main category: cs.CR

TL;DR: LLM作为评估系统易受提示注入攻击，研究评估多个模型并提出防御建议。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在文本评估中的安全漏洞，尤其是提示注入攻击的威胁。

Method: 分离内容与系统提示攻击，评估5种模型在4项任务中的表现，每条件50个提示。

Result: 攻击成功率高达73.8%，小模型更脆弱，建议使用多模型委员会和比较评分。

Conclusion: 结果表明现有防御不足，开源代码和数据集以促进进一步研究。

Abstract: LLM as judge systems used to assess text quality code correctness and
argument strength are vulnerable to prompt injection attacks. We introduce a
framework that separates content author attacks from system prompt attacks and
evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3
Opus on four tasks with various defenses using fifty prompts per condition.
Attacks achieved up to seventy three point eight percent success smaller models
proved more vulnerable and transferability ranged from fifty point five to
sixty two point six percent. Our results contrast with Universal Prompt
Injection and AdvPrompter We recommend multi model committees and comparative
scoring and release all code and datasets

</details>


### [116] [Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2504.17930)
*Abrar Fahim,Shamik Dey,Md. Nurul Absur,Md Kamrul Siam,Md. Tahmidul Huque,Jafreen Jafor Godhuli*

Main category: cs.CR

TL;DR: 论文探讨了利用机器学习（ML）和深度学习（DL）方法检测恶意软件，比较了几种常用模型的性能，发现DNN模型表现最佳，准确率达99.92%。通过特征选择和预处理提高了检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统恶意软件检测方法在高误报率和低准确性方面表现不佳，而每天新增的恶意软件威胁对数字生态系统构成重大风险，促使研究采用ML和DL技术改进检测效果。

Method: 研究比较了随机森林、多层感知器（MLP）和深度神经网络（DNN）等模型的性能，使用了Kaggle的大规模数据集，并进行了特征选择和预处理优化。

Result: DNN模型表现最优，训练准确率达99.92%，AUC分数接近完美。特征选择和预处理显著提升了检测能力。

Conclusion: 研究通过分析模型性能指标，证明了先进检测技术的有效性，为构建更强大可靠的网络安全解决方案提供了重要见解。

Abstract: Digital systems find it challenging to keep up with cybersecurity threats.
The daily emergence of more than 560,000 new malware strains poses significant
hazards to the digital ecosystem. The traditional malware detection methods
fail to operate properly and yield high false positive rates with low accuracy
of the protection system. This study explores the ways in which malware can be
detected using these machine learning (ML) and deep learning (DL) approaches to
address those shortcomings. This study also includes a systematic comparison of
the performance of some of the widely used ML models, such as random forest,
multi-layer perceptron (MLP), and deep neural network (DNN), for determining
the effectiveness of the domain of modern malware threat systems. We use a
considerable-sized database from Kaggle, which has undergone optimized feature
selection and preprocessing to improve model performance. Our finding suggests
that the DNN model outperformed the other traditional models with the highest
training accuracy of 99.92% and an almost perfect AUC score. Furthermore, the
feature selection and preprocessing can help improve the capabilities of
detection. This research makes an important contribution by analyzing the
performance of the model on the performance metrics and providing insight into
the effectiveness of the advanced detection techniques to build more robust and
more reliable cybersecurity solutions against the growing malware threats.

</details>


### [117] [Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions](https://arxiv.org/abs/2504.17953)
*Ahod Alghuried,Abdulaziz Alghamdi,Ali Alkinoon,Soohyeon Choi,Manar Mohaisen,David Mohaisen*

Main category: cs.CR

TL;DR: 本文通过实验分析以太坊钓鱼检测中显式交易特征与隐式图特征的效能，发现两类特征各具优缺点，并探讨了类别不平衡与数据集构成对检测稳健性的影响。


<details>
  <summary>Details</summary>
Motivation: 解决当前以太坊钓鱼检测中特征选择策略和基于图的模型的有效性未被充分研究的问题。

Method: 系统地分析和对比显式交易特征与隐式图特征，实验验证其对检测模型性能的影响，并探讨类别不平衡与数据集构成的挑战。

Result: 展示了两类特征在提升检测准确率方面的优势和局限，同时揭示了特征对模型在对抗环境中的稳健性和泛化能力的作用。

Conclusion: 研究为以太坊钓鱼检测的特征选择提供了更清晰的指导，并指出了未来改进的方向。

Abstract: Phishing detection on Ethereum has increasingly leveraged advanced machine
learning techniques to identify fraudulent transactions. However, limited
attention has been given to understanding the effectiveness of feature
selection strategies and the role of graph-based models in enhancing detection
accuracy. In this paper, we systematically examine these issues by analyzing
and contrasting explicit transactional features and implicit graph-based
features, both experimentally and analytically. We explore how different
feature sets impact the performance of phishing detection models, particularly
in the context of Ethereum's transactional network. Additionally, we address
key challenges such as class imbalance and dataset composition and their
influence on the robustness and precision of detection methods. Our findings
demonstrate the advantages and limitations of each feature type, while also
providing a clearer understanding of how feature affect model resilience and
generalization in adversarial environments.

</details>


### [118] [Diffusion-Driven Universal Model Inversion Attack for Face Recognition](https://arxiv.org/abs/2504.18015)
*Hanrui Wang,Shuo Wang,Chun-Shien Lu,Isao Echizen*

Main category: cs.CR

TL;DR: 论文提出了一种名为DiffUMI的无训练扩散驱动通用模型反转攻击方法，用于评估人脸识别系统的隐私风险，该方法无需针对每个目标模型训练生成器，且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人脸识别技术依赖敏感且不可变的生物特征数据，传统隐私保护方法通过将原始图像转换为嵌入向量，但模型反转攻击仍可复原这些隐私图像，现有方法需为每个目标模型训练生成器，计算成本高昂。

Method: 提出DiffUMI，基于扩散模型的无条件图像生成技术，无需训练特定生成器，通过优化的对抗搜索实现高效高保真的人脸重建，并首次利用模型反转区分非人脸输入。

Result: DiffUMI在隐私保护人脸识别系统中实现了最先进的攻击成功率，验证了无条件扩散模型在高效复原人脸图像上的有效性。

Conclusion: DiffUMI作为一种通用且高效的方法，揭示了人脸识别系统在隐私保护上的潜在风险，同时为模型反转攻击提供了新思路。

Abstract: Facial recognition technology poses significant privacy risks, as it relies
on biometric data that is inherently sensitive and immutable if compromised. To
mitigate these concerns, face recognition systems convert raw images into
embeddings, traditionally considered privacy-preserving. However, model
inversion attacks pose a significant privacy threat by reconstructing these
private facial images, making them a crucial tool for evaluating the privacy
risks of face recognition systems. Existing methods usually require training
individual generators for each target model, a computationally expensive
process. In this paper, we propose DiffUMI, a training-free diffusion-driven
universal model inversion attack for face recognition systems. DiffUMI is the
first approach to apply a diffusion model for unconditional image generation in
model inversion. Unlike other methods, DiffUMI is universal, eliminating the
need for training target-specific generators. It operates within a fixed
framework and pretrained diffusion model while seamlessly adapting to diverse
target identities and models. DiffUMI breaches privacy-preserving face
recognition systems with state-of-the-art success, demonstrating that an
unconditional diffusion model, coupled with optimized adversarial search,
enables efficient and high-fidelity facial reconstruction. Additionally, we
introduce a novel application of out-of-domain detection (OODD), marking the
first use of model inversion to distinguish non-face inputs from face inputs
based solely on embeddings.

</details>


### [119] [NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation](https://arxiv.org/abs/2504.18147)
*Rob Romijnders,Stefanos Laskaridis,Ali Shahin Shamsabadi,Hamed Haddadi*

Main category: cs.CR

TL;DR: 该论文提出了NoEsis框架，通过结合差分隐私和参数高效微调技术，在保证隐私性的同时实现跨领域知识转移，并在CodeXGLUE基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决大语言模型（LLM）在模块化训练中可能泄露源数据隐私的问题，同时避免孤立训练导致泛化能力下降的缺点。

Method: 方法为NoEsis框架，结合差分隐私和混合两阶段的参数高效微调技术，使用域特定低秩适配器（作为专家）和共享提示令牌（作为知识共享主干）。

Result: 实验结果在CodeXGLUE上显示，NoEsis能够实现可证明的隐私保证和跨领域的知识转移，并有效抵抗成员推理攻击，代码补全任务中填补了77%的准确率差距。

Conclusion: 结论表明NoEsis在隐私保护和知识共享之间取得了平衡，为模块化LLM训练提供了可行的解决方案。

Abstract: Large Language Models (LLM) are typically trained on vast amounts of data
from various sources. Even when designed modularly (e.g., Mixture-of-Experts),
LLMs can leak privacy on their sources. Conversely, training such models in
isolation arguably prohibits generalization. To this end, we propose a
framework, NoEsis, which builds upon the desired properties of modularity,
privacy, and knowledge transfer. NoEsis integrates differential privacy with a
hybrid two-staged parameter-efficient fine-tuning that combines domain-specific
low-rank adapters, acting as experts, with common prompt tokens, acting as a
knowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase
that NoEsis can achieve provable privacy guarantees with tangible knowledge
transfer across domains, and empirically show protection against Membership
Inference Attacks. Finally, on code completion tasks, NoEsis bridges at least
77% of the accuracy gap between the non-shared and the non-private baseline.

</details>


### [120] [LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection](https://arxiv.org/abs/2504.18423)
*Rajesh Yarra*

Main category: cs.CR

TL;DR: 论文探讨了LLMs在漏洞检测中的潜力与局限性，提出结合RAG和MoA的创新方法以提升AI驱动的漏洞检测的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全工具存在高误报率和代码理解浅显的问题，而传统机器学习方法在漏洞检测中表现不佳，因此需要更先进的AI解决方案。

Method: 提出结合RAG与MoA的方法，以弥补LLMs的幻觉、上下文长度限制和知识截断等缺陷。

Result: 该方法旨在提升漏洞检测的质量和可靠性，为AI驱动的安全解决方案铺平道路。

Conclusion: 通过创新方法，LLMs可以在漏洞检测中发挥更大作用，但仍需进一步研究以应对不断变化的威胁环境。

Abstract: Despite the transformative impact of Artificial Intelligence (AI) across
various sectors, cyber security continues to rely on traditional static and
dynamic analysis tools, hampered by high false positive rates and superficial
code comprehension. While generative AI offers promising automation
capabilities for software development, leveraging Large Language Models (LLMs)
for vulnerability detection presents unique challenges. This paper explores the
potential and limitations of LLMs in identifying vulnerabilities, acknowledging
inherent weaknesses such as hallucinations, limited context length, and
knowledge cut-offs. Previous attempts employing machine learning models for
vulnerability detection have proven ineffective due to limited real-world
applicability, feature engineering challenges, lack of contextual
understanding, and the complexities of training models to keep pace with the
evolving threat landscape. Therefore, we propose a robust AI-driven approach
focused on mitigating these limitations and ensuring the quality and
reliability of LLM based vulnerability detection. Through innovative
methodologies combining Retrieval-Augmented Generation (RAG) and
Mixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs
while addressing their weaknesses, ultimately paving the way for dependable and
efficient AI-powered solutions in securing the ever-evolving software
landscape.

</details>


### [121] [DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics](https://arxiv.org/abs/2504.18497)
*Yifeng Mao,Bozhidar Stevanoski,Yves-Alexandre de Montjoye*

Main category: cs.CR

TL;DR: 论文提出了一种针对固定汇总统计数据的推理攻击框架DeSIA，展示了其在识别易受攻击用户方面的高效性，并强调仅靠数据聚合不足以保护隐私，需结合正式隐私机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注机器学习模型或合成数据发布的隐私风险评估，但对固定汇总统计数据的隐私攻击方法研究不足，尤其是在仅发布少量统计数据的情况下。

Method: 提出了一种称为DeSIA的属性推理攻击框架，并应用于美国人口普查PPMF数据集，测试其在不同统计数量和噪声水平下的表现。

Result: DeSIA在识别易受攻击用户时表现优异，真阳性率为0.14，假阳性率为$10^{-3}$，且在无法验证属性的用户群中也表现良好。

Conclusion: 研究结果表明，仅靠数据聚合无法有效保护隐私，尤其在发布少量统计数据时，需结合正式隐私机制并进行测试。

Abstract: Empirical inference attacks are a popular approach for evaluating the privacy
risk of data release mechanisms in practice. While an active attack literature
exists to evaluate machine learning models or synthetic data release, we
currently lack comparable methods for fixed aggregate statistics, in particular
when only a limited number of statistics are released. We here propose an
inference attack framework against fixed aggregate statistics and an attribute
inference attack called DeSIA. We instantiate DeSIA against the U.S. Census
PPMF dataset and show it to strongly outperform reconstruction-based attacks.
In particular, we show DeSIA to be highly effective at identifying vulnerable
users, achieving a true positive rate of 0.14 at a false positive rate of
$10^{-3}$. We then show DeSIA to perform well against users whose attributes
cannot be verified and when varying the number of aggregate statistics and
level of noise addition. We also perform an extensive ablation study of DeSIA
and show how DeSIA can be successfully adapted to the membership inference
task. Overall, our results show that aggregation alone is not sufficient to
protect privacy, even when a relatively small number of aggregates are being
released, and emphasize the need for formal privacy mechanisms and testing
before aggregate statistics are released.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [122] [Efficient Learning on Large Graphs using a Densifying Regularity Lemma](https://arxiv.org/abs/2504.18273)
*Jonathan Kouchly,Ben Finkelshtein,Michael Bronstein,Ron Levie*

Main category: cs.SI

TL;DR: 论文介绍了一种名为IBG的图分解方法，通过低秩分解高效逼近大型有向图，并在节点分类等任务中表现优异，同时降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递神经网络在处理大型图时存在计算和内存成本高的问题，因此需要一种更高效的图表示方法。

Method: 提出了IBG（Intersecting Block Graph）方法，通过对图的低秩分解和密集近似来优化表示。

Result: 实验证明该方法在节点分类等任务中具有竞争力，且计算复杂度与节点数而非边数呈线性关系。

Conclusion: IBG提供了一种高效且通用的图表示方法，适用于稀疏或密集图，显著降低了计算资源需求。

Abstract: Learning on large graphs presents significant challenges, with traditional
Message Passing Neural Networks suffering from computational and memory costs
scaling linearly with the number of edges. We introduce the Intersecting Block
Graph (IBG), a low-rank factorization of large directed graphs based on
combinations of intersecting bipartite components, each consisting of a pair of
communities, for source and target nodes. By giving less weight to non-edges,
we show how to efficiently approximate any graph, sparse or dense, by a dense
IBG. Specifically, we prove a constructive version of the weak regularity
lemma, showing that for any chosen accuracy, every graph, regardless of its
size or sparsity, can be approximated by a dense IBG whose rank depends only on
the accuracy. This dependence of the rank solely on the accuracy, and not on
the sparsity level, is in contrast to previous forms of the weak regularity
lemma. We present a graph neural network architecture operating on the IBG
representation of the graph and demonstrating competitive performance on node
classification, spatio-temporal graph analysis, and knowledge graph completion,
while having memory and computational complexity linear in the number of nodes
rather than edges.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [123] [Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G](https://arxiv.org/abs/2504.17938)
*Raza Ul Mustafa,Sesha Dassanayake*

Main category: cs.MM

TL;DR: 论文研究了YouTube视频流中的质量转换（如分辨率变化）与用户QoE的关系，提出通过信道指标（RSRP、RSRQ、SNR）预测质量转换，以优化用户体验。传统ML方法达到77%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统QoS指标（如带宽、延迟）无法完全反映用户QoE，OTT平台需更有效的预测方法以提高流媒体体验。作者探索了质量转换与信道指标的关联，为5G时代的流媒体优化提供新思路。

Method: 通过分析YouTube流媒体会话中的质量转换事件，研究其与RSRP、RSRQ、SNR等信道指标的相关性。使用传统ML分类器（未指定具体算法）预测分辨率切换的类别。

Result: 研究发现信道指标与质量转换显著正相关。仅基于RSRP、RSRQ、SNR的ML模型预测准确率达77%。

Conclusion: 信道指标可作为实时预测质量转换的有效参数，帮助OTT平台动态分配资源以提升QoE。该方法适用于5G及未来的低延迟网络环境。

Abstract: The Quality of Experience (QoE) is the users satisfaction while streaming a
video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube
reflects the smooth streaming session without any buffering and quality shift
events. One of the most important factors nowadays affecting QoE of YouTube is
frequent shifts from higher to lower resolutions and vice versa. These shifts
ensure a smooth streaming session; however, it might get a lower mean opinion
score. For instance, dropping from 1080p to 480p during a video can preserve
continuity but might reduce the viewers enjoyment. Over time, OTT platforms are
looking for alternative ways to boost user experience instead of relying on
traditional Quality of Service (QoS) metrics such as bandwidth, latency, and
throughput. As a result, we look into the relationship between quality shifting
in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our
findings state that these channel metrics positively correlate with shifts.
Thus, in real-time, OTT can only rely on them to predict video streaming
sessions into lower- and higher-resolution categories, thus providing more
resources to improve user experience. Using traditional Machine Learning (ML)
classifiers, we achieved an accuracy of 77-percent, while using only RSRP,
RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency
networks promise enhanced streaming capabilities, the proposed methodology can
be used to improve OTT services.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [124] [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)
*KimiTeam,Ding Ding,Zeqian Ju,Yichong Leng,Songxiang Liu,Tong Liu,Zeyu Shang,Kai Shen,Wei Song,Xu Tan,Heyi Tang,Zhengtao Wang,Chu Wei,Yifei Xin,Xinran Xu,Jianwei Yu,Yutao Zhang,Xinyu Zhou,Y. Charles,Jun Chen,Yanru Chen,Yulun Du,Weiran He,Zhenxing Hu,Guokun Lai,Qingcheng Li,Yangyang Liu,Weidong Sun,Jianzhou Wang,Yuzhi Wang,Yuefeng Wu,Yuxin Wu,Dongchao Yang,Hao Yang,Ying Yang,Zhilin Yang,Aoxiong Yin,Ruibin Yuan,Yutong Zhang,Zaida Zhou*

Main category: eess.AS

TL;DR: Kimi-Audio是一款开源的音频基础模型，擅长音频理解、生成和对话。通过创新的模型架构、大规模数据训练和多任务微调，该模型在多项音频任务中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 为了构建一个能够全面理解、生成和处理音频的多功能模型，满足音频领域多样化的应用需求。

Method: 采用12.5Hz音频分词器，设计基于LLM的架构（输入为连续特征，输出为离散令牌），开发流式分块解码器。通过1300万小时的多模态音频数据预训练，并结合精心设计的任务微调。

Result: 在语音识别、音频理解、音频问答和语音对话等任务中实现最先进的性能。

Conclusion: Kimi-Audio展示了开源音频基础模型的强大能力，为音频相关任务提供了高效解决方案。代码与模型已公开。

Abstract: We present Kimi-Audio, an open-source audio foundation model that excels in
audio understanding, generation, and conversation. We detail the practices in
building Kimi-Audio, including model architecture, data curation, training
recipe, inference deployment, and evaluation. Specifically, we leverage a
12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous
features as input and discrete tokens as output, and develop a chunk-wise
streaming detokenizer based on flow matching. We curate a pre-training dataset
that consists of more than 13 million hours of audio data covering a wide range
of modalities including speech, sound, and music, and build a pipeline to
construct high-quality and diverse post-training data. Initialized from a
pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text
data with several carefully designed tasks, and then fine-tuned to support a
diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio
achieves state-of-the-art performance on a range of audio benchmarks including
speech recognition, audio understanding, audio question answering, and speech
conversation. We release the codes, model checkpoints, as well as the
evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [125] [Learning Enhanced Ensemble Filters](https://arxiv.org/abs/2504.17836)
*Eviatar Bach,Ricardo Baptista,Edoardo Calvello,Bohan Chen,Andrew Stuart*

Main category: stat.ML

TL;DR: 该论文提出了一种基于神经算子的新滤波方法（MNMEF），通过改进高斯假设的限制，提升了隐藏马尔可夫模型中的滤波精度，并在Lorenz 96和Kuramoto-Sivashinsky模型中展现了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成卡尔曼滤波（EnKF）因高斯假设限制了其精度，论文旨在通过神经算子（MNM）设计更精确的滤波方法。

Method: 提出了一种名为MNMEF的新滤波方法，利用神经算子（MNM）近似平均场演化，并通过集总转换器实现多尺度参数化部署。

Result: 在Lorenz 96和Kuramoto-Sivashinsky模型中，MNMEF的均方根误差性能优于现有方法。

Conclusion: MNMEF通过神经算子突破了高斯假设的局限，显著提升了滤波精度和适应性。

Abstract: The filtering distribution in hidden Markov models evolves according to the
law of a mean-field model in state--observation space. The ensemble Kalman
filter (EnKF) approximates this mean-field model with an ensemble of
interacting particles, employing a Gaussian ansatz for the joint distribution
of the state and observation at each observation time. These methods are
robust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed
by approximating the mean-field evolution using a novel form of neural operator
taking probability distributions as input: a Measure Neural Mapping (MNM). A
MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble
filter (MNMEF), which is defined in both the mean-fieldlimit and for
interacting ensemble particle approximations. The ensemble approach uses
empirical measures as input to the MNM and is implemented using the set
transformer, which is invariant to ensemble permutation and allows for
different ensemble sizes. The derivation of methods from a mean-field
formulation allows a single parameterization of the algorithm to be deployed at
different ensemble sizes. In practice fine-tuning of a small number of
parameters, for specific ensemble sizes, further enhances the accuracy of the
scheme. The promise of the approach is demonstrated by its superior
root-mean-square-error performance relative to leading methods in filtering the
Lorenz 96 and Kuramoto-Sivashinsky models.

</details>


### [126] [Learning Operators by Regularized Stochastic Gradient Descent with Operator-valued Kernels](https://arxiv.org/abs/2504.18184)
*Jia-Qi Yang,Lei Shi*

Main category: stat.ML

TL;DR: 本文研究了一种正则化随机梯度下降（SGD）算法，用于估计从波兰空间到可分离希尔伯特空间的非线性算子，并提供了近乎最优的收敛速度分析。


<details>
  <summary>Details</summary>
Motivation: 探索在向量值再生核希尔伯特空间中估计非线性算子的高效方法，特别是在在线和有限范围设置下的收敛性。

Method: 采用正则化SGD算法，结合算子值核，分析在线（多项式衰减步长和正则化参数）和有限范围（恒定参数）两种设置的收敛性。

Result: 在特定正则条件下，证明了预测和估计误差的维度无关收敛性，并提供了高概率误差界和几乎肯定的收敛保证。

Conclusion: 所提算法在收敛速度和适用范围上表现优越，并具有向更广泛的算子值核和编解码框架扩展的潜力。

Abstract: This paper investigates regularized stochastic gradient descent (SGD)
algorithms for estimating nonlinear operators from a Polish space to a
separable Hilbert space. We assume that the regression operator lies in a
vector-valued reproducing kernel Hilbert space induced by an operator-valued
kernel. Two significant settings are considered: an online setting with
polynomially decaying step sizes and regularization parameters, and a
finite-horizon setting with constant step sizes and regularization parameters.
We introduce regularity conditions on the structure and smoothness of the
target operator and the input random variables. Under these conditions, we
provide a dimension-free convergence analysis for the prediction and estimation
errors, deriving both expectation and high-probability error bounds. Our
analysis demonstrates that these convergence rates are nearly optimal.
Furthermore, we present a new technique for deriving bounds with high
probability for general SGD schemes, which also ensures almost-sure
convergence. Finally, we discuss potential extensions to more general
operator-valued kernels and the encoder-decoder framework.

</details>


### [127] [Post-Transfer Learning Statistical Inference in High-Dimensional Regression](https://arxiv.org/abs/2504.18212)
*Nguyen Vu Khai Tam,Cao Huyen My,Vo Nguyen Le Duy*

Main category: stat.ML

TL;DR: 论文提出了一种名为PTL-SI的新统计推断框架，用于评估高维回归迁移学习（TL-HDR）中特征选择的可靠性，并能够提供有效的p值以控制假阳性率，同时通过分治策略增强统计功效。


<details>
  <summary>Details</summary>
Motivation: 当前在高维回归迁移学习中缺乏量化特征与响应关系统计显著性的方法，导致无法可靠评估特征选择结果。

Method: 提出了PTL-SI框架，结合分治策略计算特征p值，严格控制假阳性率。

Result: 实验验证了PTL-SI在合成和真实高维数据集中的有效性与理论特性。

Conclusion: PTL-SI为TL-HDR提供了一种可靠的特征选择统计推断工具，填补了现有方法的空白。

Abstract: Transfer learning (TL) for high-dimensional regression (HDR) is an important
problem in machine learning, particularly when dealing with limited sample size
in the target task. However, there currently lacks a method to quantify the
statistical significance of the relationship between features and the response
in TL-HDR settings. In this paper, we introduce a novel statistical inference
framework for assessing the reliability of feature selection in TL-HDR, called
PTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its
ability to provide valid $p$-values to features selected in TL-HDR, thereby
rigorously controlling the false positive rate (FPR) at desired significance
level $\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by
incorporating a strategic divide-and-conquer approach into our framework. We
demonstrate the validity and effectiveness of the proposed PTL-SI through
extensive experiments on both synthetic and real-world high-dimensional
datasets, confirming its theoretical properties and utility in testing the
reliability of feature selection in TL scenarios.

</details>


### [128] [Generalization Guarantees for Multi-View Representation Learning and Application to Regularization via Gaussian Product Mixture Prior](https://arxiv.org/abs/2504.18455)
*Milad Sefidgaran,Abdellatif Zaidi,Piotr Krasnowski*

Main category: stat.ML

TL;DR: 研究分布式多视点表示学习问题，探讨无协调下各代理提取必要且充分表示的方法，提出基于相对熵和MDL的正则化方法，实验验证其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多视点表示学习中代理如何独立提取必要且足够的信息以便解码器正确估计标签的问题，特别关注无显式协调情况下的表示学习方法。

Method: 提出基于相对熵和MDL的泛化误差边界，设计正则化方法，并使用数据依赖的高斯混合先验进行实验验证。

Result: 在单视点设定中，性能优于VIB和CDVIB；多视点设定中，高斯乘积混合先验隐式鼓励冗余特征提取。

Conclusion: 通过数据依赖的对称先验和正则化方法，有效解决了多视点表示学习问题，实验结果显示其优越性和反直觉的冗余特征提取现象。

Abstract: We study the problem of distributed multi-view representation learning. In
this problem, $K$ agents observe each one distinct, possibly statistically
correlated, view and independently extracts from it a suitable representation
in a manner that a decoder that gets all $K$ representations estimates
correctly the hidden label. In the absence of any explicit coordination between
the agents, a central question is: what should each agent extract from its view
that is necessary and sufficient for a correct estimation at the decoder? In
this paper, we investigate this question from a generalization error
perspective. First, we establish several generalization bounds in terms of the
relative entropy between the distribution of the representations extracted from
training and "test" datasets and a data-dependent symmetric prior, i.e., the
Minimum Description Length (MDL) of the latent variables for all views and
training and test datasets. Then, we use the obtained bounds to devise a
regularizer; and investigate in depth the question of the selection of a
suitable prior. In particular, we show and conduct experiments that illustrate
that our data-dependent Gaussian mixture priors with judiciously chosen weights
lead to good performance. For single-view settings (i.e., $K=1$), our
experimental results are shown to outperform existing prior art Variational
Information Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches.
Interestingly, we show that a weighted attention mechanism emerges naturally in
this setting. Finally, for the multi-view setting, we show that the selection
of the joint prior as a Gaussians product mixture induces a Gaussian mixture
marginal prior for each marginal view and implicitly encourages the agents to
extract and output redundant features, a finding which is somewhat
counter-intuitive.

</details>


### [129] [Enhancing Visual Interpretability and Explainability in Functional Survival Trees and Forests](https://arxiv.org/abs/2504.18498)
*Giuseppe Loffredo,Elvira Romano,Fabrizio MAturo*

Main category: stat.ML

TL;DR: 论文研究了功能性生存树（FST）和功能性随机生存森林（FRSF）两种生存模型，提出增强FST可解释性和改进FRSF可解释性的新方法，通过真实和模拟数据集验证了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管功能性生存模型在预测时表现强劲，但其缺乏可解释性限制了在实际决策和风险分析中的应用。

Method: 引入了新的方法和工具，旨在增强FST的可解释性，并改进FRSF集合的可解释性。

Result: 实验结果显示，提出的方法能生成高效且易于理解的决策树，准确捕捉模型集合的底层决策过程。

Conclusion: 研究为功能性生存模型的可解释性提供了有效工具，有助于提升其在实践中的应用价值。

Abstract: Functional survival models are key tools for analyzing time-to-event data
with complex predictors, such as functional or high-dimensional inputs. Despite
their predictive strength, these models often lack interpretability, which
limits their value in practical decision-making and risk analysis. This study
investigates two key survival models: the Functional Survival Tree (FST) and
the Functional Random Survival Forest (FRSF). It introduces novel methods and
tools to enhance the interpretability of FST models and improve the
explainability of FRSF ensembles. Using both real and simulated datasets, the
results demonstrate that the proposed approaches yield efficient,
easy-to-understand decision trees that accurately capture the underlying
decision-making processes of the model ensemble.

</details>


### [130] [Representation Learning for Distributional Perturbation Extrapolation](https://arxiv.org/abs/2504.18522)
*Julius von Kügelgen,Jakob Ketterer,Xinwei Shen,Nicolai Meinshausen,Jonas Peters*

Main category: stat.ML

TL;DR: 论文提出了一种通过潜变量模型预测未见扰动效应的方法（PDAE），证明在训练扰动足够多样化时，扰动效应可通过仿射变换识别，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决如何通过已知扰动数据预测新扰动（如基因敲除或药物组合）对RNA测序等低维测量的影响分布，这是一项具有挑战性的外推任务。

Method: 提出潜变量模型，假设扰动在潜在空间中表现为均值平移且可叠加，并开发PDAE方法，通过最大化真实与预测扰动分布的相似性来训练模型。

Result: 理论证明在多样化训练数据下扰动效应可识别，实验表明PDAE在预测未见扰动效果上优于基线方法。

Conclusion: PDAE为扰动效应的外推提供了理论和实证支持，尤其在复杂生物扰动预测中表现优越。

Abstract: We consider the problem of modelling the effects of unseen perturbations such
as gene knockdowns or drug combinations on low-level measurements such as RNA
sequencing data. Specifically, given data collected under some perturbations,
we aim to predict the distribution of measurements for new perturbations. To
address this challenging extrapolation task, we posit that perturbations act
additively in a suitable, unknown embedding space. More precisely, we formulate
the generative process underlying the observed data as a latent variable model,
in which perturbations amount to mean shifts in latent space and can be
combined additively. Unlike previous work, we prove that, given sufficiently
diverse training perturbations, the representation and perturbation effects are
identifiable up to affine transformation, and use this to characterize the
class of unseen perturbations for which we obtain extrapolation guarantees. To
estimate the model from data, we propose a new method, the perturbation
distribution autoencoder (PDAE), which is trained by maximising the
distributional similarity between true and predicted perturbation
distributions. The trained model can then be used to predict previously unseen
perturbation distributions. Empirical evidence suggests that PDAE compares
favourably to existing methods and baselines at predicting the effects of
unseen perturbations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [131] [The Cloud Weaving Model for AI development](https://arxiv.org/abs/2504.17823)
*Darcy Kim,Aida Kalender,Sennay Ghebreab,Giovanni Sileno*

Main category: cs.CY

TL;DR: 本文提出了一种名为‘云编织模型’的替代性概念框架，旨在将AI开发根植于社会结构中，并应用于理解边缘化社区中的共创试点模式。


<details>
  <summary>Details</summary>
Motivation: 由于现有范式难以表达边缘化社区AI开发中的挑战，作者受土著知识、自然图案和东方传统启发，构建了新的框架以捕捉常被忽略的维度。

Method: 通过引入‘云、蜘蛛、线、蜘蛛网和天气’等核心元素及其在AI语境下的解释，框架被设计并应用于共创试点的模式分析。

Result: 该模型成功识别了边缘化社区AI开发中被忽视但关键的社会维度，为责任AI提供了新视角。

Conclusion: 云编织模型为AI开发的社会嵌入性提供了实用工具，强调了多元知识体系的价值。

Abstract: While analysing challenges in pilot projects developing AI with marginalized
communities, we found it difficult to express them within commonly used
paradigms. We therefore constructed an alternative conceptual framework to
ground AI development in the social fabric -- the Cloud Weaving Model --
inspired (amongst others) by indigenous knowledge, motifs from nature, and
Eastern traditions. This paper introduces and elaborates on the fundamental
elements of the model (clouds, spiders, threads, spiderwebs, and weather) and
their interpretation in an AI context. The framework is then applied to
comprehend patterns observed in co-creation pilots approaching marginalized
communities, highlighting neglected yet relevant dimensions for responsible AI
development.

</details>


### [132] [The Role of Open-Source LLMs in Shaping the Future of GeoAI](https://arxiv.org/abs/2504.17833)
*Xiao Huang,Zhengzhong Tu,Xinyue Ye,Michael Goodchild*

Main category: cs.CY

TL;DR: 这篇论文探讨了开源大语言模型（LLMs）在地理空间人工智能（GeoAI）中的关键作用，强调了其相较于专有模型的优势（如定制化、透明度和社区创新），但也提出了安全、伦理和治理的挑战。


<details>
  <summary>Details</summary>
Motivation: 专有的LLMs虽然易于使用，但缺乏灵活性和透明度，而开源LLMs能够促进地理信息科学（GIScience）的适应性和创新。研究旨在分析开源LLMs的优势与挑战，推动GeoAI的公平和可持续发展。

Method: 论文通过批判性评估开源LLMs在地理空间AI领域的应用，讨论了定制化、开源框架集成（如强化学习和空间索引）以及FAIR原则的作用。

Result: 开源LLMs能够推动GIScience的进步，但需要解决安全、伦理和治理问题。最佳路径是构建多样化的生态系统，结合开源创新和跨学科合作。

Conclusion: 开源LLMs为GeoAI提供了重要的创新基础，但其发展需兼顾安全性和伦理问题。未来的重点应放在构建多样化、可互操作的生态系统上。

Abstract: Large Language Models (LLMs) are transforming geospatial artificial
intelligence (GeoAI), offering new capabilities in data processing, spatial
analysis, and decision support. This paper examines the open-source paradigm's
pivotal role in this transformation. While proprietary LLMs offer
accessibility, they often limit the customization, interoperability, and
transparency vital for specialized geospatial tasks. Conversely, open-source
alternatives significantly advance Geographic Information Science (GIScience)
by fostering greater adaptability, reproducibility, and community-driven
innovation. Open frameworks empower researchers to tailor solutions, integrate
cutting-edge methodologies (e.g., reinforcement learning, advanced spatial
indexing), and align with FAIR principles. However, the growing reliance on any
LLM necessitates careful consideration of security vulnerabilities, ethical
risks, and robust governance for AI-generated geospatial outputs. Ongoing
debates on accessibility, regulation, and misuse underscore the critical need
for responsible AI development strategies. This paper argues that GIScience
advances best not through a single model type, but by cultivating a diverse,
interoperable ecosystem combining open-source foundations for innovation,
bespoke geospatial models, and interdisciplinary collaboration. By critically
evaluating the opportunities and challenges of open-source LLMs within the
broader GeoAI landscape, this work contributes to a nuanced discourse on
leveraging AI to effectively advance spatial research, policy, and
decision-making in an equitable, sustainable, and scientifically rigorous
manner.

</details>


### [133] [AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How](https://arxiv.org/abs/2504.18044)
*Omid Veisi,Sasan Bahrami,Roman Englert,Claudia Müller*

Main category: cs.CY

TL;DR: 摘要讨论了在医疗、CSCW和社交计算中使用LLM时需审查伦理和社会规范，通过混合方法研究（调查和专家访谈）评估ChatGPT是否符合伦理和社会规范。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确保ChatGPT等工具在实际应用中遵循伦理和社会规范，为工业与学术研究提供行动指导，实现机器伦理。

Method: 采用混合方法，包括111人的在线调查和38位专家的访谈研究。

Result: 初步揭示了AI伦理的六个关键方面（偏见、可信度、安全等），并指出ChatGPT在透明度和无监督数据收集中的偏见问题是主要伦理难题。

Conclusion: 研究强调了AI工具需符合伦理和社会规范的重要性，并识别了ChatGPT在实际应用中的主要伦理挑战。

Abstract: Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social
Computing requires the examination of ethical and social norms to ensure safe
incorporation into human life. We conducted a mixed-method study, including an
online survey with 111 participants and an interview study with 38 experts, to
investigate the AI ethics and social norms in ChatGPT as everyday life tools.
This study aims to evaluate whether ChatGPT in an empirical context operates
following ethics and social norms, which is critical for understanding actions
in industrial and academic research and achieving machine ethics. The findings
of this study provide initial insights into six important aspects of AI ethics,
including bias, trustworthiness, security, toxicology, social norms, and
ethical data. Significant obstacles related to transparency and bias in
unsupervised data collection methods are identified as ChatGPT's ethical
concerns.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [134] [Artificial Intelligence health advice accuracy varies across languages and contexts](https://arxiv.org/abs/2504.18310)
*Prashant Garg,Thiemo Fetzer*

Main category: econ.GN

TL;DR: 研究发现，尽管大型语言模型在英语教科书内容上准确性高，但在非欧洲语言及不同主题和来源背景下表现不稳定，凸显了全球健康传播中多语言和领域验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在全球健康通信中的表现，尤其是在非英语和非欧洲语言环境中的准确性，以确保AI在全球范围内的可靠应用。

Method: 利用英国和欧盟官方健康声明以及9100条记者审核的健康断言（涵盖堕胎、COVID-19等主题和多种来源），测试了六种领先的大型语言模型的性能。

Result: 模型在英语内容上表现良好，但在非欧洲语言中准确性下降，且表现因主题和来源不同而波动。

Conclusion: 在AI应用于全球健康通信前，需进行全面的多语言和领域验证，以确保其准确性和可靠性。

Abstract: Using basic health statements authorized by UK and EU registers and 9,100
journalist-vetted public-health assertions on topics such as abortion, COVID-19
and politics from sources ranging from peer-reviewed journals and government
advisories to social media and news across the political spectrum, we benchmark
six leading large language models from in 21 languages, finding that, despite
high accuracy on English-centric textbook claims, performance falls in multiple
non-European languages and fluctuates by topic and source, highlighting the
urgency of comprehensive multilingual, domain-aware validation before deploying
AI in global health communication.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [135] [Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics](https://arxiv.org/abs/2504.18367)
*Maodong Li,Jiying Zhang,Bin Feng,Wenqi Zeng,Dechin Chen,Zhijun Pan,Yu Li,Zijing Liu,Yi Isaac Yang*

Main category: physics.comp-ph

TL;DR: 该论文提出了一种结合分子动力学模拟、增强采样和AI生成模型的新方法，用于研究药物-蛋白质结合/解离动力学，并构建了包含13M帧的解离轨迹数据集DD-13M，训练了UnbindingFlow生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有工具对药物-蛋白质结合/解离动力学的研究有限，尤其是基于人工智能的方法。本文旨在填补这一空白，通过一种新的研究范式提高预测能力。

Method: 结合分子动力学（MD）模拟、增强采样和AI生成模型，提出了一种增强采样策略来高效模拟解离过程并估计自由能面（FES）。构建了一个包含26,612条解离轨迹（约13M帧）的DD-13M数据集，并训练了深度等变生成模型UnbindingFlow。

Result: 成功生成了DD-13M数据集和UnbindingFlow模型，显著提升了计算结构生物学的研究能力，为药物-蛋白质相互作用的机器学习研究提供了重要资源。

Conclusion: 该方法为药物-蛋白质动力学研究提供了新工具，未来将扩展至更多复合体及应用场景（如通路预测）。

Abstract: Drug-protein binding and dissociation dynamics are fundamental to
understanding molecular interactions in biological systems. While many tools
for drug-protein interaction studies have emerged, especially artificial
intelligence (AI)-based generative models, predictive tools on
binding/dissociation kinetics and dynamics are still limited. We propose a
novel research paradigm that combines molecular dynamics (MD) simulations,
enhanced sampling, and AI generative models to address this issue. We propose
an enhanced sampling strategy to efficiently implement the drug-protein
dissociation process in MD simulations and estimate the free energy surface
(FES). We constructed a program pipeline of MD simulations based on this
sampling strategy, thus generating a dataset including 26,612 drug-protein
dissociation trajectories containing about 13 million frames. We named this
dissociation dynamics dataset DD-13M and used it to train a deep equivariant
generative model UnbindingFlow, which can generate collision-free dissociation
trajectories. The DD-13M database and UnbindingFlow model represent a
significant advancement in computational structural biology, and we anticipate
its broad applicability in machine learning studies of drug-protein
interactions. Our ongoing efforts focus on expanding this methodology to
encompass a broader spectrum of drug-protein complexes and exploring novel
applications in pathway prediction.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [136] [A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw](https://arxiv.org/abs/2504.17822)
*Wenwen Li,Chia-Yu Hsu,Sizhe Wang,Zhining Gu,Yili Yang,Brendan M. Rogers,Anna Liljedahl*

Main category: cs.CV

TL;DR: 使用改进的Cascade Mask R-CNN与多尺度视觉Transformer结合的策略，提升了北极地区Retrogressive Thaw Slumps（RTS）的测绘精度。


<details>
  <summary>Details</summary>
Motivation: RTS作为永久冻土融化的标志性特征，其精确测绘对环境研究至关重要，但存在规模小、边界模糊和时空变化等挑战。

Method: 采用Cascade Mask R-CNN模型和多尺度视觉Transformer，并引入两种新策略：特征级残差跨模态注意力融合与预训练单模态学习后多模态微调。

Result: 该方法优于现有的数据级和特征级融合策略，为多模态数据高效利用提供了新思路。

Conclusion: 研究不仅提升了RTS测绘的精度，还加深了对永久冻土地貌及其环境影响的理解。

Abstract: Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost
landforms with significant environmental impacts. Mapping these RTS is crucial
because their appearance serves as a clear indication of permafrost thaw.
However, their small scale compared to other landform features, vague
boundaries, and spatiotemporal variation pose significant challenges for
accurate detection. In this paper, we employed a state-of-the-art deep learning
model, the Cascade Mask R-CNN with a multi-scale vision transformer-based
backbone, to delineate RTS features across the Arctic. Two new strategies were
introduced to optimize multimodal learning and enhance the model's predictive
performance: (1) a feature-level, residual cross-modality attention fusion
strategy, which effectively integrates feature maps from multiple modalities to
capture complementary information and improve the model's ability to understand
complex patterns and relationships within the data; (2) pre-trained unimodal
learning followed by multimodal fine-tuning to alleviate high computing demand
while achieving strong model performance. Experimental results demonstrated
that our approach outperformed existing models adopting data-level fusion,
feature-level convolutional fusion, and various attention fusion strategies,
providing valuable insights into the efficient utilization of multimodal data
for RTS mapping. This research contributes to our understanding of permafrost
landforms and their environmental implications.

</details>


### [137] [Dual Prompting Image Restoration with Diffusion Transformers](https://arxiv.org/abs/2504.17825)
*Dehong Kong,Fan Li,Zhixin Wang,Jiaqi Xu,Renjing Pei,Wenbo Li,WenQi Ren*

Main category: cs.CV

TL;DR: DPIR是一种新型图像修复方法，通过双提示控制分支和多角度条件信息提取，显著提升了基于扩散变换器的图像修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如U-Net隐扩散模型）在图像修复中能力有限，而扩散变换器（DiTs）因其可扩展性和高质量表现成为潜在替代方案。本文旨在通过双提示模块和多角度条件提取解决文本描述无法充分捕捉视觉特征的问题。

Method: DPIR采用双分支结构：1）轻量级低质图像条件分支，高效整合图像先验；2）双提示控制分支，提供全局上下文和局部外观的视觉提示，与文本提示结合形成双提示。

Result: 实验表明，DPIR在图像修复中表现优越，双提示设计显著增强了修复质量。

Conclusion: DPIR通过双提示和多角度条件提取，为基于扩散变换器的图像修复提供了高效且高质量的新方案。

Abstract: Recent state-of-the-art image restoration methods mostly adopt latent
diffusion models with U-Net backbones, yet still facing challenges in achieving
high-quality restoration due to their limited capabilities. Diffusion
transformers (DiTs), like SD3, are emerging as a promising alternative because
of their better quality with scalability. In this paper, we introduce DPIR
(Dual Prompting Image Restoration), a novel image restoration method that
effectivly extracts conditional information of low-quality images from multiple
perspectives. Specifically, DPIR consits of two branches: a low-quality image
conditioning branch and a dual prompting control branch. The first branch
utilizes a lightweight module to incorporate image priors into the DiT with
high efficiency. More importantly, we believe that in image restoration,
textual description alone cannot fully capture its rich visual characteristics.
Therefore, a dual prompting module is designed to provide DiT with additional
visual cues, capturing both global context and local appearance. The extracted
global-local visual prompts as extra conditional control, alongside textual
prompts to form dual prompts, greatly enhance the quality of the restoration.
Extensive experimental results demonstrate that DPIR delivers superior image
restoration performance.

</details>


### [138] [FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model](https://arxiv.org/abs/2504.17826)
*Kaicheng Pang,Xingxing Zou,Waikeung Wong*

Main category: cs.CV

TL;DR: 该论文提出了FashionM3，这是一个基于视觉语言模型（VLM）构建的多模态、多任务、多轮时尚助手，旨在通过自然语言和视觉交互提升时尚零售体验。


<details>
  <summary>Details</summary>
Motivation: 现代零售中，时尚搭配和个性化推荐对时尚产业经济价值贡献巨大，而视觉语言模型的出现为提升零售体验提供了新机遇。

Method: 论文提出了FashionM3，基于一个专门为时尚任务微调的VLM，具备个性化推荐、替代建议、产品图像生成和虚拟试穿模拟等功能。

Result: 在FashionRec数据集上微调后，FashionM3通过多轮交互实现了上下文个性化的推荐。定量、定性评估及用户研究表明其在推荐效果和实用价值上表现优越。

Conclusion: FashionM3作为一个时尚助手，在推荐有效性和用户体验上表现优异，验证了其在时尚零售中的实际应用潜力。

Abstract: Fashion styling and personalized recommendations are pivotal in modern
retail, contributing substantial economic value in the fashion industry. With
the advent of vision-language models (VLM), new opportunities have emerged to
enhance retailing through natural language and visual interactions. This work
proposes FashionM3, a multimodal, multitask, and multiround fashion assistant,
built upon a VLM fine-tuned for fashion-specific tasks. It helps users discover
satisfying outfits by offering multiple capabilities including personalized
recommendation, alternative suggestion, product image generation, and virtual
try-on simulation. Fine-tuned on the novel FashionRec dataset, comprising
331,124 multimodal dialogue samples across basic, personalized, and alternative
recommendation tasks, FashionM3 delivers contextually personalized suggestions
with iterative refinement through multiround interactions. Quantitative and
qualitative evaluations, alongside user studies, demonstrate FashionM3's
superior performance in recommendation effectiveness and practical value as a
fashion assistant.

</details>


### [139] [VEU-Bench: Towards Comprehensive Understanding of Video Editing](https://arxiv.org/abs/2504.17828)
*Bozheng Li,Yongliang Wu,Yi Lu,Jiashuo Yu,Licheng Tang,Jiawang Cao,Wenqing Zhu,Yuyang Sun,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 论文介绍了VEU-Bench，一个用于评估视频编辑理解（VEU）任务的全面基准测试，并开发了专用模型Oscars，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型（Vid-LLMs）在视频编辑理解任务上的能力尚未被充分探索，需要一个专门的基准测试和模型来解决这一差距。

Method: 提出VEU-Bench，包含19个细粒度任务，并开发了集成本体知识库的自动标注流程；通过微调构建了专用模型Oscars。

Result: 现有Vid-LLMs在VEU任务中表现不佳，Oscars模型在VEU-Bench上准确率提升28.3%，且VEU数据还能提升通用视频理解任务的性能。

Conclusion: VEU-Bench和Oscars模型填补了Vid-LLMs在视频编辑理解领域的空白，显著提升了任务表现和通用能力。

Abstract: Widely shared videos on the internet are often edited. Recently, although
Video Large Language Models (Vid-LLMs) have made great progress in general
video understanding tasks, their capabilities in video editing understanding
(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce
VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark
that categorizes video editing components across various dimensions, from
intra-frame features like shot size to inter-shot attributes such as cut types
and transitions. Unlike previous video editing understanding benchmarks that
focus mainly on editing element classification, VEU-Bench encompasses 19
fine-grained tasks across three stages: recognition, reasoning, and judging. To
enhance the annotation of VEU automatically, we built an annotation pipeline
integrated with an ontology-based knowledge base. Through extensive experiments
with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs
face significant challenges in VEU tasks, with some performing worse than
random choice. To alleviate this issue, we develop Oscars, a VEU expert model
fine-tuned on the curated VEU-Bench dataset. It outperforms existing
open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves
performance comparable to commercial models like GPT-4o. We also demonstrate
that incorporating VEU data significantly enhances the performance of Vid-LLMs
on general video understanding benchmarks, with an average improvement of 8.3%
across nine reasoning tasks.

</details>


### [140] [Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing](https://arxiv.org/abs/2504.17829)
*Vlad Vasilescu,Ana Neacsu,Daniela Faur*

Main category: cs.CV

TL;DR: 论文研究了单图像去雾任务中对抗噪声的影响，提出了两种轻量级微调策略以增强预训练Transformer的鲁棒性，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 单图像去雾在遥感应用中至关重要，但现有方法对对抗噪声的鲁棒性不足，导致性能显著下降。这激发了研究者提出新方法来提升模型的抗干扰能力。

Method: 论文通过实验证明了现有去雾Transformer对对抗噪声的敏感性，随后提出了两种轻量级微调策略来增强预训练模型的鲁棒性。

Result: 实验结果显示，提出的方法在保持干净数据性能的同时，显著提升了对抗噪声的防御能力，并在遥感场景中验证了其泛化性。

Conclusion: 论文提出的轻量级微调策略有效提升了单图像去雾模型的鲁棒性，为实际应用提供了可靠的解决方案。

Abstract: Single-image dehazing is an important topic in remote sensing applications,
enhancing the quality of acquired images and increasing object detection
precision. However, the reliability of such structures has not been
sufficiently analyzed, which poses them to the risk of imperceptible
perturbations that can significantly hinder their performance. In this work, we
show that state-of-the-art image-to-image dehazing transformers are susceptible
to adversarial noise, with even 1 pixel change being able to decrease the PSNR
by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies
aimed at increasing the robustness of pre-trained transformers. Our methods
results in comparable clean performance, while significantly increasing the
protection against adversarial data. We further present their applicability in
two remote sensing scenarios, showcasing their robust behavior for
out-of-distribution data. The source code for adversarial fine-tuning and
attack algorithms can be found at github.com/Vladimirescu/RobustDehazing.

</details>


### [141] [Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892)
*Yasmine Omri,Parth Shroff,Thierry Tambe*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型的自适应压缩方法，通过简化的簇级标记聚合优化视觉标记选择和合并，显著提高了效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前视觉编码器存在冗余和低效问题，多模态数据处理的计算成本高昂，迫切需要更有效的编码和处理方法。

Method: 采用了视觉标记选择和合并的多种方法，特别是简单的簇级标记聚合，通过基准测试和定性分析验证其效果。

Result: 簇级标记聚合在视觉标记选择和合并方面优于现有方法，揭示了视觉编码器中的冗余，并通过跨模态注意力可视化解释了视觉标记选择的一些趋势。

Conclusion: 这项研究为高维多模态数据的更有效编码和处理开辟了新方向，为可扩展和可持续的多模态系统奠定了基础。

Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven
advancements in cross-modal reasoning but at significant computational costs.
In this work, we focus on visual language models. We highlight the redundancy
and inefficiency in current vision encoders, and seek to construct an adaptive
compression method for multimodal data. In this work, we characterize a panoply
of visual token selection and merging approaches through both benchmarking and
qualitative analysis. In particular, we demonstrate that simple cluster-level
token aggregation outperforms prior state-of-the-art works in token selection
and merging, including merging at the vision encoder level and attention-based
approaches. We underline the redundancy in current vision encoders, and shed
light on several puzzling trends regarding principles of visual token selection
through cross-modal attention visualizations. This work is a first effort
towards more effective encoding and processing of high-dimensional data, and
paves the way for more scalable and sustainable multimodal systems.

</details>


### [142] [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
*Xinyu Chen,Yunxin Li,Haoyuan Shi,Baotian Hu,Wenhan Luo,Yaowei Wang,Min Zhang*

Main category: cs.CV

TL;DR: VideoVista-CulturalLingo 是首个跨文化、语言和领域的视频理解评测基准，包含1389个视频和3134个问答对，评估了24个视频大模型。结果表明，现有模型在中文问题、时间理解及数学领域表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频评测基准多为单一语言（英语）和西方文化背景，缺乏多样性。本文旨在填补这一空白，提供更全面的评测标准。

Method: 构建包含中、美、欧文化背景的双语（中英）视频数据集，涵盖多领域，并评估24个主流和开源视频模型。

Result: 模型在中文问题（尤其是历史类）、时间理解任务（最高45.2%）和数学领域表现较弱，科学类问题表现较好。

Conclusion: VideoVista-CulturalLingo 揭示了现有视频理解模型在文化多样性和复杂任务中的局限性，为未来研究提供了新方向。

Abstract: Assessing the video comprehension capabilities of multimodal AI systems can
effectively measure their understanding and reasoning abilities. Most video
evaluation benchmarks are limited to a single language, typically English, and
predominantly feature videos rooted in Western cultural contexts. In this
paper, we present VideoVista-CulturalLingo, the first video evaluation
benchmark designed to bridge cultural, linguistic, and domain divide in video
comprehension. Our work differs from existing benchmarks in the following ways:
1) Cultural diversity, incorporating cultures from China, North America, and
Europe; 2) Multi-linguistics, with questions presented in Chinese and
English-two of the most widely spoken languages; and 3) Broad domain, featuring
videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo
contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent
open-source or proprietary video large models. From the experiment results, we
observe that: 1) Existing models perform worse on Chinese-centric questions
than Western-centric ones, particularly those related to Chinese history; 2)
Current open-source models still exhibit limitations in temporal understanding,
especially in the Event Localization task, achieving a maximum score of only
45.2%; 3) Mainstream models demonstrate strong performance in general
scientific questions, while open-source models demonstrate weak performance in
mathematics.

</details>


### [143] [A Large Vision-Language Model based Environment Perception System for Visually Impaired People](https://arxiv.org/abs/2504.18027)
*Zezhou Chen,Zhaoxiang Liu,Kai Wang,Kohou Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: 论文提出了一种基于大型视觉-语言模型（LVLM）的环境感知系统，帮助视障人士通过可穿戴设备捕捉并理解周围环境，结合分割模型减少幻觉，提升描述准确性。


<details>
  <summary>Details</summary>
Motivation: 视障人士由于自然场景的复杂性难以感知环境，限制了个人和社交活动。系统旨在通过技术手段改善这一现状。

Method: 使用可穿戴设备捕捉场景，结合LVLM和分割模型生成场景描述。分割结果作为外部知识输入以减少幻觉。

Result: 在POPE、MME和LLaVA-QA90上的实验表明，系统比Qwen-VL-Chat提供更准确的场景描述，探索性实验验证了其对视障人士的有效性。

Conclusion: 系统通过结合LVLM与分割模型，显著提升了视障人士的环境感知能力。

Abstract: It is a challenging task for visually impaired people to perceive their
surrounding environment due to the complexity of the natural scenes. Their
personal and social activities are thus highly limited. This paper introduces a
Large Vision-Language Model(LVLM) based environment perception system which
helps them to better understand the surrounding environment, by capturing the
current scene they face with a wearable device, and then letting them retrieve
the analysis results through the device. The visually impaired people could
acquire a global description of the scene by long pressing the screen to
activate the LVLM output, retrieve the categories of the objects in the scene
resulting from a segmentation model by tapping or swiping the screen, and get a
detailed description of the objects they are interested in by double-tapping
the screen. To help visually impaired people more accurately perceive the
world, this paper proposes incorporating the segmentation result of the RGB
image as external knowledge into the input of LVLM to reduce the LVLM's
hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the
system could provide a more accurate description of the scene compared to
Qwen-VL-Chat, exploratory experiments show that the system helps visually
impaired people to perceive the surrounding environment effectively.

</details>


### [144] [CAMU: Context Augmentation for Meme Understanding](https://arxiv.org/abs/2504.17902)
*Girish A. Koushik,Diptesh Kanojia,Helen Treharne,Aditya Joshi*

Main category: cs.CV

TL;DR: 提出了一个名为CAMU的新框架，通过生成更详细的描述性标题、强化仇恨相关内容评分以及高效调整CLIP文本编码器来提升对社交媒体模因中仇恨内容的多模态理解，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 社交媒体模因因其结合视觉和文本线索的文化复杂性，对仇恨内容检测提出挑战，需要更高效且准确的多模态理解方法。

Method: 结合视觉语言模型生成描述性标题，利用标题评分神经网络突出仇恨相关内容，并通过参数高效微调CLIP文本编码器优化多模态理解。

Result: 在多种评估指标上表现优异，尤其在Hateful Memes数据集上准确率和F1分数达到0.807和0.806，同时在MultiOFF数据集上F1分数为0.673，展示了其泛化能力。

Conclusion: 结果表明，强大的视觉基础和细致的文本表示对可靠检测至关重要，CAMU框架不仅高效且在实际应用中具有优势。

Abstract: Social media memes are a challenging domain for hate detection because they
intertwine visual and textual cues into culturally nuanced messages. We
introduce a novel framework, CAMU, which leverages large vision-language models
to generate more descriptive captions, a caption-scoring neural network to
emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's
text encoder for an improved multimodal understanding of memes. Experiments on
publicly available hateful meme datasets show that simple projection layer
fine-tuning yields modest gains, whereas selectively tuning deeper text encoder
layers significantly boosts performance on all evaluation metrics. Moreover,
our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful
Memes dataset, at par with the existing SoTA framework while being much more
efficient, offering practical advantages in real-world scenarios that rely on
fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the
MultiOFF dataset for offensive meme identification, demonstrating its
generalisability. Additional analyses on benign confounders reveal that robust
visual grounding and nuanced text representations are crucial for reliable hate
and offence detection. We will publicly release CAMU along with the resultant
models for further research.
  Disclaimer: This paper includes references to potentially disturbing,
hateful, or offensive content due to the nature of the task.

</details>


### [145] [DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification](https://arxiv.org/abs/2504.18046)
*Guohao Huo,Zibo Lin,Zitong Wang,Ruiting Dai,Hao Tang*

Main category: cs.CV

TL;DR: 论文提出了DMS-Net，一种用于双目眼底图像分类的双模态多尺度Siamese网络，通过多尺度上下文感知模块和双模态特征融合模块提升性能，在ODIR-5K数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统诊断方法和现有单眼深度学习模型未能充分考虑到双眼病理关联性，因此需要一种能够有效利用双目图像信息的深度学习框架。

Method: DMS-Net结合了权重共享的Siamese ResNet-152主干网络提取特征，并引入多尺度上下文感知模块（MSCAM）和双模态特征融合模块（DMFF）来增强特征表示和跨模态交互。

Result: 在ODIR-5K数据集上，DMS-Net达到了80.5%的准确率、86.1%的召回率和83.8%的Cohen's kappa值，表现优于现有方法。

Conclusion: DMS-Net在检测对称性病理和提高眼科疾病临床决策能力方面表现出色，为眼科疾病的诊断提供了新的解决方案。

Abstract: Ophthalmic diseases pose a significant global health challenge, yet
traditional diagnosis methods and existing single-eye deep learning approaches
often fail to account for binocular pathological correlations. To address this,
we propose DMS-Net, a dual-modal multi-scale Siamese network for binocular
fundus image classification. Our framework leverages weight-shared Siamese
ResNet-152 backbones to extract deep semantic features from paired fundus
images. To tackle challenges such as lesion boundary ambiguity and scattered
pathological distributions, we introduce a Multi-Scale Context-Aware Module
(MSCAM) that integrates adaptive pooling and attention mechanisms for
multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion
(DMFF) module enhances cross-modal interaction through spatial-semantic
recalibration and bidirectional attention, effectively combining global context
and local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves
state-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%
Cohen's kappa, demonstrating superior capability in detecting symmetric
pathologies and advancing clinical decision-making for ocular diseases.

</details>


### [146] [A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images](https://arxiv.org/abs/2504.18049)
*Xin Li,Wenhui Zhu,Peijie Qiu,Oana M. Dumitrascu,Amal Youssef,Yalin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合轻量级CNN框架nn-MobileNet和自监督学习的方法，用于医学影像分析，解决了标注数据稀缺和高计算成本的问题，并在多种疾病识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析领域虽受益于深度学习，但标注数据稀缺且昂贵。传统ViT方法计算密集且缺乏局部性特征，限制了其应用。本研究旨在通过结合轻量级CNN和自监督学习，高效利用无标注数据提升下游任务表现。

Method: 采用nn-MobileNet轻量CNN框架，结合BERT风格的自监督学习，在UK Biobank的无标注视网膜图像上进行预训练，随后在阿尔茨海默病、帕金森病及多种视网膜疾病识别任务中验证模型性能。

Result: 实验表明，该方法显著提升了下游任务的表现，验证了轻量CNN结合自监督学习在医学影像领域的潜力。

Conclusion: 本研究展示了轻量CNN与自监督学习在标注数据稀缺场景下的高效性，为医学影像分析提供了新的解决方案。

Abstract: In the field of medical imaging, the advent of deep learning, especially the
application of convolutional neural networks (CNNs) has revolutionized the
analysis and interpretation of medical images. Nevertheless, deep learning
methods usually rely on large amounts of labeled data. In medical imaging
research, the acquisition of high-quality labels is both expensive and
difficult. The introduction of Vision Transformers (ViT) and self-supervised
learning provides a pre-training strategy that utilizes abundant unlabeled
data, effectively alleviating the label acquisition challenge while broadening
the breadth of data utilization. However, ViT's high computational density and
substantial demand for computing power, coupled with the lack of localization
characteristics of its operations on image patches, limit its efficiency and
applicability in many application scenarios. In this study, we employ
nn-MobileNet, a lightweight CNN framework, to implement a BERT-style
self-supervised learning approach. We pre-train the network on the unlabeled
retinal fundus images from the UK Biobank to improve downstream application
performance. We validate the results of the pre-trained model on Alzheimer's
disease (AD), Parkinson's disease (PD), and various retinal diseases
identification. The results show that our approach can significantly improve
performance in the downstream tasks. In summary, this study combines the
benefits of CNNs with the capabilities of advanced self-supervised learning in
handling large-scale unlabeled data, demonstrating the potential of CNNs in the
presence of label scarcity.

</details>


### [147] [S3MOT: Monocular 3D Object Tracking with Selective State Space Model](https://arxiv.org/abs/2504.18068)
*Zhuohao Yan,Shaoquan Feng,Xingxing Li,Yuxuan Zhou,Chunxi Xia,Shengyu Li*

Main category: cs.CV

TL;DR: 本文提出三种创新技术（HSSM、FCOE、VeloSSM）提升单目3D多目标跟踪性能，在KITTI基准测试中刷新SOTA，HOTA达到76.86。


<details>
  <summary>Details</summary>
Motivation: 解决单目系统中因2D视频流难以挖掘3D时空关联而导致的多目标跟踪挑战。

Method: 1. HSSM：基于匈牙利算法的数据关联机制；2. FCOE：无ROI池化的对比学习嵌入；3. VeloSSM：建模速度时序依赖的6-DoF姿态估计。

Result: KITTI测试中HOTA 76.86，比之前最佳提升+2.63，推理速度31FPS。

Conclusion: 方法显著提升单目3D MOT的鲁棒性和效率，代码已开源。

Abstract: Accurate and reliable multi-object tracking (MOT) in 3D space is essential
for advancing robotics and computer vision applications. However, it remains a
significant challenge in monocular setups due to the difficulty of mining 3D
spatiotemporal associations from 2D video streams. In this work, we present
three innovative techniques to enhance the fusion and exploitation of
heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State
Space Model (HSSM), a novel data association mechanism that compresses
contextual tracking cues across multiple paths, enabling efficient and
comprehensive assignment decisions with linear complexity. HSSM features a
global receptive field and dynamic weights, in contrast to traditional linear
assignment algorithms that rely on hand-crafted association costs. (2) We
propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI
pooling by directly using dense feature maps for contrastive learning, thus
improving object re-identification accuracy under challenging conditions such
as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation
through VeloSSM, an encoder-decoder architecture that models temporal
dependencies in velocity to capture motion dynamics, overcoming the limitations
of frame-based 3D inference. Experiments on the KITTI public test benchmark
demonstrate the effectiveness of our method, achieving a new state-of-the-art
performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best
by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness
and efficiency for monocular 3D MOT tasks. The code and models are available at
https://github.com/bytepioneerX/s3mot.

</details>


### [148] [PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models](https://arxiv.org/abs/2504.18165)
*Michel Gokan Khan,Renan Guarese,Fabian Johnson,Xi Vincent Wang,Anders Bergman,Benjamin Edvinsson,Mario Romero,Jérémy Vachier,Jan Kronqvist*

Main category: cs.CV

TL;DR: PerfCam是一个开源的数字孪生框架，结合相机和传感器数据、3D高斯喷绘及计算机视觉模型，用于工业生产线中的数字孪生、物体跟踪和关键绩效指标（KPIs）提取。通过3D重建和卷积神经网络（CNNs），实现了半自动化的物体跟踪与空间映射，并验证了其在制药行业实际生产线中的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决工业生产线中实时数字孪生和KPIs提取的挑战，提供一种高效、可操作的工具以支持智能制造环境。

Method: 采用3D高斯喷绘和CNNs实现物体跟踪与空间映射，结合相机和传感器数据进行半自动化数字孪生。

Result: 在制药行业实际生产线中验证了PerfCam的有效性，展示了其精确的数字孪生能力和可操作的洞察力。

Conclusion: PerfCam为智能制造环境中的数字孪生开发和运营分析提取提供了有效工具，并通过公开数据集支持进一步研究。

Abstract: We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning
framework that combines camera and sensory data with 3D Gaussian Splatting and
computer vision models for digital twinning, object tracking, and Key
Performance Indicators (KPIs) extraction in industrial production lines. By
utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam
offers a semi-automated approach to object tracking and spatial mapping,
enabling digital twins that capture real-time KPIs such as availability,
performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts
in the production line. We validate the effectiveness of PerfCam through a
practical deployment within realistic test production lines in the
pharmaceutical industry and contribute an openly published dataset to support
further research and development in the field. The results demonstrate
PerfCam's ability to deliver actionable insights through its precise digital
twin capabilities, underscoring its value as an effective tool for developing
usable digital twins in smart manufacturing environments and extracting
operational analytics.

</details>


### [149] [Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition](https://arxiv.org/abs/2504.18201)
*Yin Tang,Jiankai Li,Hongyu Yang,Xuan Dong,Lifeng Fan,Weixin Li*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MCCL的新方法，通过多粒度视觉线索组合学习来识别图像意图，解决了传统方法在多样性和主观性上的局限性，并在多个数据集上取得了最先进的表现。


<details>
  <summary>Details</summary>
Motivation: 社交媒体图像的广泛分享揭示了用户的意图和兴趣，但传统计算机视觉任务难以捕捉这些隐含线索。现有方法在处理意图类别的视觉多样性时效果有限，因此需要新方法提升识别准确性和可解释性。

Method: 提出MCCL方法，分解意图识别的视觉线索并整合多粒度特征，使用类特定原型缓解数据不平衡，并通过图卷积网络结合标签嵌入先验知识进行多标签分类。

Result: 在Intentonomy和MDID数据集上达到最先进性能，同时具备良好的可解释性。

Conclusion: MCCL为理解人类复杂多样的表达形式提供了新思路，未来可进一步探索相关应用。

Abstract: In an era where social media platforms abound, individuals frequently share
images that offer insights into their intents and interests, impacting
individual life quality and societal stability. Traditional computer vision
tasks, such as object detection and semantic segmentation, focus on concrete
visual representations, while intent recognition relies more on implicit visual
clues. This poses challenges due to the wide variation and subjectivity of such
clues, compounded by the problem of intra-class variety in conveying abstract
concepts, e.g. "enjoy life". Existing methods seek to solve the problem by
manually designing representative features or building prototypes for each
class from global features. However, these methods still struggle to deal with
the large visual diversity of each intent category. In this paper, we introduce
a novel approach named Multi-grained Compositional visual Clue Learning (MCCL)
to address these challenges for image intent recognition. Our method leverages
the systematic compositionality of human cognition by breaking down intent
recognition into visual clue composition and integrating multi-grained
features. We adopt class-specific prototypes to alleviate data imbalance. We
treat intent recognition as a multi-label classification problem, using a graph
convolutional network to infuse prior knowledge through label embedding
correlations. Demonstrated by a state-of-the-art performance on the Intentonomy
and MDID datasets, our approach advances the accuracy of existing methods while
also possessing good interpretability. Our work provides an attempt for future
explorations in understanding complex and miscellaneous forms of human
expression.

</details>


### [150] [Event-Based Eye Tracking. 2025 Event-based Vision Workshop](https://arxiv.org/abs/2504.18249)
*Qinyu Chen,Chang Gao,Min Liu,Daniele Perrone,Yan Ru Pei,Zuowen Wang,Zhuo Zou,Shihang Tan,Tao Han,Guorui Lu,Zhen Xu,Junyuan Ding,Ziteng Wang,Zongwei Wu,Han Han,Yuliang Wu,Jinze Chen,Wei Zhai,Yang Cao,Zheng-jun Zha,Nuwan Bandara,Thivya Kandappu,Archan Misra,Xiaopeng Lin,Hongxiang Huang,Hongwei Ren,Bojun Cheng,Hoang M. Truong,Vinh-Thuan Ly,Huy G. Tran,Thuan-Phat Nguyen,Tram T. Doan*

Main category: cs.CV

TL;DR: 这篇论文是对2025年基于事件的眼动追踪挑战赛的综述，总结了排名靠前团队的创新方法，并从硬件设计角度讨论了基于事件的眼动追踪。


<details>
  <summary>Details</summary>
Motivation: 通过综述挑战赛中的创新方法，推动未来基于事件的眼动追踪研究。

Method: 回顾并总结了挑战赛中排名靠前团队的方法，重点关注了准确率、模型大小和运算量等指标。

Result: 分析了各方法的性能指标，并探讨了硬件设计的视角。

Conclusion: 这篇综述为基于事件的眼动追踪领域的进一步发展提供了参考和启发。

Abstract: This survey serves as a review for the 2025 Event-Based Eye Tracking
Challenge organized as part of the 2025 CVPR event-based vision workshop. This
challenge focuses on the task of predicting the pupil center by processing
event camera recorded eye movement. We review and summarize the innovative
methods from teams rank the top in the challenge to advance future event-based
eye tracking research. In each method, accuracy, model size, and number of
operations are reported. In this survey, we also discuss event-based eye
tracking from the perspective of hardware design.

</details>


### [151] [Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator](https://arxiv.org/abs/2504.18283)
*Minjae Kang,Martim Brandão*

Main category: cs.CV

TL;DR: 论文提出了一种音频-视觉生成与分离模型（AV-GAS），解决了现有方法无法从混合音频生成图像的局限性，通过音频-视觉分离器实现多类音频输入下的图像生成，并引入了新的评估指标，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉生成模型仅能处理单类音频输入，而无法应对混合音频（包含多类声音）的图像生成需求。本文旨在填补这一空白，提出一种能够处理多类音频输入并生成对应图像的方法。

Method: 通过音频-视觉分离器（AV-GAS）实现多类音频输入的图像生成，同时支持生成混合音频中各个独立声音对应的图像。引入新的评估指标：Class Representation Score (CRS) 和修改版R@K。

Result: 在VGGSound数据集上，所提方法性能优于当前最优模型，CRS提高7%，R@2*提高4%，生成的图像更具可信度。

Conclusion: AV-GAS模型成功解决了多类音频输入的图像生成问题，并通过新任务和评估指标推动了音频-视觉生成领域的发展。

Abstract: Recent audio-visual generative models have made substantial progress in
generating images from audio. However, existing approaches focus on generating
images from single-class audio and fail to generate images from mixed audio. To
address this, we propose an Audio-Visual Generation and Separation model
(AV-GAS) for generating images from soundscapes (mixed audio containing
multiple classes). Our contribution is threefold: First, we propose a new
challenge in the audio-visual generation task, which is to generate an image
given a multi-class audio input, and we propose a method that solves this task
using an audio-visual separator. Second, we introduce a new audio-visual
separation task, which involves generating separate images for each class
present in a mixed audio input. Lastly, we propose new evaluation metrics for
the audio-visual generation task: Class Representation Score (CRS) and a
modified R@K. Our model is trained and evaluated on the VGGSound dataset. We
show that our method outperforms the state-of-the-art, achieving 7% higher CRS
and 4% higher R@2* in generating plausible images with mixed audio.

</details>


### [152] [Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis](https://arxiv.org/abs/2504.18286)
*Christian Pionzewski,Rebecca Rademacher,Jérôme Rutinowski,Antonia Ponikarov,Stephan Matzke,Tim Chilla,Pia Schreynemackers,Alice Kirchheim*

Main category: cs.CV

TL;DR: 论文研究了合成训练数据对材料磨损和老化预测的影响，通过实验发现使用持续更新的图库和部分合成数据可显著提升识别的准确性，并引入了一个新的开源数据集。


<details>
  <summary>Details</summary>
Motivation: 探索合成训练数据在材料老化再识别中的应用，以提升系统对老化对象的识别性能。

Method: 测试了不同的实验设置和图库扩展策略，使用持续更新的图库和10%的人工合成数据训练模型。

Result: 持续更新的图库使平均Rank-1准确率提高24%；使用10%合成数据的训练模型比纯真实数据模型提升13%的准确率。

Conclusion: 合成数据和动态图库更新能显著提升老化再识别性能，新数据集为相关研究提供了支持。

Abstract: This contribution explores the impact of synthetic training data usage and
the prediction of material wear and aging in the context of re-identification.
Different experimental setups and gallery set expanding strategies are tested,
analyzing their impact on performance over time for aging re-identification
subjects. Using a continuously updating gallery, we were able to increase our
mean Rank-1 accuracy by 24%, as material aging was taken into account step by
step. In addition, using models trained with 10% artificial training data,
Rank-1 accuracy could be increased by up to 13%, in comparison to a model
trained on only real-world data, significantly boosting generalized performance
on hold-out data. Finally, this work introduces a novel, open-source
re-identification dataset, pallet-block-2696. This dataset contains 2,696
images of Euro pallets, taken over a period of 4 months. During this time,
natural aging processes occurred and some of the pallets were damaged during
their usage. These wear and tear processes significantly changed the appearance
of the pallets, providing a dataset that can be used to generate synthetically
aged pallets or other wooden materials.

</details>


### [153] [TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning](https://arxiv.org/abs/2504.18348)
*Fengchun Liu. Tong Zhang,Chunying Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种两阶段课程学习损失调度器（TSCL），用于平衡深度学习图像隐写算法中的多任务损失，通过先验课程控制和动态损失控制优化训练过程，实验表明其提升了隐写质量、解码准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写框架通常使用固定损失权重，未考虑任务重要性和训练过程动态变化，因此需要一种更灵活的方法来平衡多任务损失。

Method: TSCL分为两阶段：先验课程控制（分步学习嵌入、解码和抗分析能力）和动态损失控制（通过损失下降速度评估任务学习进度，动态调整权重）。

Result: 在ALASKA2、VOC2012和ImageNet数据集上，TSCL显著提升了隐写图像质量、解码准确性和抗分析安全性。

Conclusion: TSCL通过动态调整多任务损失权重，解决了固定权重训练的局限性，为深度图像隐写提供了一种有效的优化策略。

Abstract: For deep learning-based image steganography frameworks, in order to ensure
the invisibility and recoverability of the information embedding, the loss
function usually contains several losses such as embedding loss, recovery loss
and steganalysis loss. In previous research works, fixed loss weights are
usually chosen for training optimization, and this setting is not linked to the
importance of the steganography task itself and the training process. In this
paper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for
balancing multinomial losses in deep learning image steganography algorithms.
TSCL consists of two phases: a priori curriculum control and loss dynamics
control. The first phase firstly focuses the model on learning the information
embedding of the original image by controlling the loss weights in the
multi-party adversarial training; secondly, it makes the model shift its
learning focus to improving the decoding accuracy; and finally, it makes the
model learn to generate a steganographic image that is resistant to
steganalysis. In the second stage, the learning speed of each training task is
evaluated by calculating the loss drop of the before and after iteration rounds
to balance the learning of each task. Experimental results on three large
public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL
strategy improves the quality of steganography, decoding accuracy and security.

</details>


### [154] [COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization](https://arxiv.org/abs/2504.18361)
*Haozhen Yan,Yan Hong,Jiahui Zhan,Yikun Ji,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 介绍了一个专门用于检测修复图像的基准数据集COCOInpaint，填补了现有图像操纵检测方法在修复图像领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有图像操纵检测方法主要聚焦于拼接或复制-移动伪造，缺乏针对修复类操纵的专用基准，需填补这一研究空白。

Method: 构建COCOInpaint数据集，包含由六种先进修复模型生成的高质量修复样本，四种掩模生成策略，及25万+语义多样的修复图像。

Result: 强调了修复区域与真实区域的内在不一致性，而非表面语义伪影，并制定严格评估协议使用三种标准指标测试现有方法。

Conclusion: 该数据集将公开以促进修复检测领域的未来研究，为多媒体真实性和网络安全提供新工具。

Abstract: Recent advancements in image manipulation have achieved unprecedented
progress in generating photorealistic content, but also simultaneously
eliminating barriers to arbitrary manipulation and editing, raising concerns
about multimedia authenticity and cybersecurity. However, existing Image
Manipulation Detection and Localization (IMDL) methodologies predominantly
focus on splicing or copy-move forgeries, lacking dedicated benchmarks for
inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a
comprehensive benchmark specifically designed for inpainting detection, with
three key contributions: 1) High-quality inpainting samples generated by six
state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by
four mask generation strategies with optional text guidance, and 3) Large-scale
coverage with 258,266 inpainted images with rich semantic diversity. Our
benchmark is constructed to emphasize intrinsic inconsistencies between
inpainted and authentic regions, rather than superficial semantic artifacts
such as object shapes. We establish a rigorous evaluation protocol using three
standard metrics to assess existing IMDL approaches. The dataset will be made
publicly available to facilitate future research in this area.

</details>


### [155] [Label-independent hyperparameter-free self-supervised single-view deep subspace clustering](https://arxiv.org/abs/2504.18179)
*Lovro Sindicic,Ivica Kopriva*

Main category: cs.CV

TL;DR: 论文提出了一种新的单视图深度子空间聚类方法，通过联合表示矩阵和多阶段学习框架解决了现有方法的不足，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度子空间聚类方法存在多个问题，如仅使用编码器输出层评估聚类质量、表示学习与子空间聚类分离、依赖外部标签等，限制了实际应用。

Method: 提出了一个单视图DSC方法，包括层间自表达损失、子空间结构范数优化、多阶段学习框架、基于相对误差的自停止机制，以及固定前导系数。

Result: 在六个数据集（人脸、数字、物体）上的实验表明，该方法优于多数需调参的线性SC算法，并与最佳线性方法竞争。

Conclusion: 该方法通过联合优化和自适应机制，显著提升了深度子空间聚类的性能和实用性。

Abstract: Deep subspace clustering (DSC) algorithms face several challenges that hinder
their widespread adoption across variois application domains. First, clustering
quality is typically assessed using only the encoder's output layer,
disregarding valuable information present in the intermediate layers. Second,
most DSC approaches treat representation learning and subspace clustering as
independent tasks, limiting their effectiveness. Third, they assume the
availability of a held-out dataset for hyperparameter tuning, which is often
impractical in real-world scenarios. Fourth, learning termination is commonly
based on clustering error monitoring, requiring external labels. Finally, their
performance often depends on post-processing techniques that rely on labeled
data. To address this limitations, we introduce a novel single-view DSC
approach that: (i) minimizes a layer-wise self expression loss using a joint
representation matrix; (ii) optimizes a subspace-structured norm to enhance
clustering quality; (iii) employs a multi-stage sequential learning framework,
consisting of pre-training and fine-tuning, enabling the use of multiple
regularization terms without hyperparameter tuning; (iv) incorporates a
relative error-based self-stopping mechanism to terminate training without
labels; and (v) retains a fixed number of leading coefficients in the learned
representation matrix based on prior knowledge. We evaluate the proposed method
on six datasets representing faces, digits, and objects. The results show that
our method outperforms most linear SC algorithms with careffulyl tuned
hyperparameters while maintaining competitive performance with the best
performing linear appoaches.

</details>


### [156] [A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection](https://arxiv.org/abs/2504.18419)
*Carlo Sgaravatti,Roberto Basla,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 提出了一种基于LiDAR和RGB相机的新型3D目标检测方法，采用混合级联方案减少误报和漏报，显著提升了行人和骑行者检测性能。


<details>
  <summary>Details</summary>
Motivation: 通过融合LiDAR和RGB多模态数据，解决单一模态检测中误报和漏报的问题，提升3D目标检测的准确性。

Method: 采用混合级联方案，包括RGB检测网络和3D LiDAR探测器，通过投影和级联融合减少LiDAR误报并填补漏报，支持灵活训练。

Result: 在KITTI目标检测基准测试中表现优异，尤其提升了行人和骑行者的检测性能。

Conclusion: 该方法有效结合多模态数据，通过级联融合策略显著提升3D目标检测性能，具有实际应用潜力。

Abstract: We present a new way to detect 3D objects from multimodal inputs, leveraging
both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an
RGB detection network and a 3D LiDAR detector. We exploit late fusion
principles to reduce LiDAR False Positives, matching LiDAR detections with RGB
ones by projecting the LiDAR bounding boxes on the image. We rely on cascade
fusion principles to recover LiDAR False Negatives leveraging epipolar
constraints and frustums generated by RGB detections of separate views. Our
solution can be plugged on top of any underlying single-modal detectors,
enabling a flexible training process that can take advantage of pre-trained
LiDAR and RGB detectors, or train the two branches separately. We evaluate our
results on the KITTI object detection benchmark, showing significant
performance improvements, especially for the detection of Pedestrians and
Cyclists.

</details>


### [157] [LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring](https://arxiv.org/abs/2504.18203)
*Raul David Dominguez Sanchez,Xavier Diaz Ortiz,Xingcheng Zhou,Max Peter Ronecker,Michael Karner,Daniel Watzenig,Alois Knoll*

Main category: cs.CV

TL;DR: 本文提出了一个基于深度学习的单目图像长距离3D物体检测方法，专为铁路自动化设计。该方法结合LiDAR数据改进深度估计，在OSDaR23数据集上有效检测250米内的物体。


<details>
  <summary>Details</summary>
Motivation: 德国铁路系统需要高度自动化以应对老化基础设施和增加列车流量的挑战，尤其是长距离感知以提前检测危险物件。

Method: 方法包括改进的YOLOv9用于2.5D物体检测、深度估计网络，以及短、长距离3D检测头。LiDAR数据用于训练以提升深度估计。

Result: 在OSDaR23数据集上，方法能检测250米内的物体，验证了其铁路自动化应用的潜力。

Conclusion: 该长距离3D检测方法对铁路自动化有重要意义，但仍需进一步优化以提升性能。

Abstract: Railway systems, particularly in Germany, require high levels of automation
to address legacy infrastructure challenges and increase train traffic safely.
A key component of automation is robust long-range perception, essential for
early hazard detection, such as obstacles at level crossings or pedestrians on
tracks. Unlike automotive systems with braking distances of ~70 meters, trains
require perception ranges exceeding 1 km. This paper presents an
deep-learning-based approach for long-range 3D object detection tailored for
autonomous trains. The method relies solely on monocular images, inspired by
the Faraway-Frustum approach, and incorporates LiDAR data during training to
improve depth estimation. The proposed pipeline consists of four key modules:
(1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation
network, and (3-4) dedicated short- and long-range 3D detection heads.
Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the
approach in detecting objects up to 250 meters. Results highlight its potential
for railway automation and outline areas for future improvement.

</details>


### [158] [Iterative Event-based Motion Segmentation by Variational Contrast Maximization](https://arxiv.org/abs/2504.18447)
*Ryo Yamaki,Shintaro Shiba,Guillermo Gallego,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的迭代运动分割方法，通过将事件分类为背景和前景，扩展了对比度最大化框架，显著提升了运动物体检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 事件相机能够捕捉场景变化，但需要将数据分类为不同运动以实现运动分割，这对于物体检测和视觉伺服等任务非常重要。

Method: 提出了一个迭代运动分割方法，将事件分为背景（主导运动假设）和前景（独立运动残差），扩展了对比度最大化框架。

Result: 实验结果表明，该方法成功分类了公共和自录数据集中的事件簇，生成了清晰的运动补偿边缘图像，在运动物体检测基准上实现了超过30%的精度提升。

Conclusion: 该方法扩展了对比度最大化对运动参数和输入事件的敏感性，为事件驱动运动分割的理论进展作出了贡献。

Abstract: Event cameras provide rich signals that are suitable for motion estimation
since they respond to changes in the scene. As any visual changes in the scene
produce event data, it is paramount to classify the data into different motions
(i.e., motion segmentation), which is useful for various tasks such as object
detection and visual servoing. We propose an iterative motion segmentation
method, by classifying events into background (e.g., dominant motion
hypothesis) and foreground (independent motion residuals), thus extending the
Contrast Maximization framework. Experimental results demonstrate that the
proposed method successfully classifies event clusters both for public and
self-recorded datasets, producing sharp, motion-compensated edge-like images.
The proposed method achieves state-of-the-art accuracy on moving object
detection benchmarks with an improvement of over 30%, and demonstrates its
possibility of applying to more complex and noisy real-world scenes. We hope
this work broadens the sensitivity of Contrast Maximization with respect to
both motion parameters and input events, thus contributing to theoretical
advancements in event-based motion segmentation estimation.
https://github.com/aoki-media-lab/event_based_segmentation_vcmax

</details>


### [159] [Fast Autoregressive Models for Continuous Latent Generation](https://arxiv.org/abs/2504.18391)
*Tiankai Hang,Jianmin Bao,Fangyun Wei,Dong Chen*

Main category: cs.CV

TL;DR: FAR模型通过替换MAR的扩散头为轻量级快捷头，显著提升了连续域图像生成的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决MAR模型在高计算成本的迭代去噪过程中推理速度慢的问题，同时保持自回归模型的优势。

Method: 采用轻量级快捷头替代扩散头，实现高效少步采样，并与因果Transformer无缝集成。

Result: FAR比MAR推理速度快2.3倍，同时保持竞争力FID和IS分数。

Conclusion: FAR为高质量连续空间图像生成建立了首个高效自回归范式，弥补了视觉自回归建模中质量与可扩展性的关键差距。

Abstract: Autoregressive models have demonstrated remarkable success in sequential data
generation, particularly in NLP, but their extension to continuous-domain image
generation presents significant challenges. Recent work, the masked
autoregressive model (MAR), bypasses quantization by modeling per-token
distributions in continuous spaces using a diffusion head but suffers from slow
inference due to the high computational cost of the iterative denoising
process. To address this, we propose the Fast AutoRegressive model (FAR), a
novel framework that replaces MAR's diffusion head with a lightweight shortcut
head, enabling efficient few-step sampling while preserving autoregressive
principles. Additionally, FAR seamlessly integrates with causal Transformers,
extending them from discrete to continuous token generation without requiring
architectural modifications. Experiments demonstrate that FAR achieves
$2.3\times$ faster inference than MAR while maintaining competitive FID and IS
scores. This work establishes the first efficient autoregressive paradigm for
high-fidelity continuous-space image generation, bridging the critical gap
between quality and scalability in visual autoregressive modeling.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [160] [Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture](https://arxiv.org/abs/2504.18099)
*Leena G Pillai,D. Muhammad Noorul Mubarak,Elizabeth Sherly*

Main category: cs.SD

TL;DR: 这篇论文提出了一种新的方法，通过结合双向长短期记忆网络和一维卷积网络来预测语音中的舌头和嘴唇发音特征，使用固定权重初始化，实验结果表明该方法在较少训练轮次下优于自适应权重初始化。


<details>
  <summary>Details</summary>
Motivation: 语音产生是一个复杂的协调过程，其中舌头作为高度灵活的发声器官起着重要作用。论文旨在通过预测发音特征来推动语音产生研究和应用的进展。

Method: 采用堆叠的双向长短期记忆网络（BiLSTM）结合一维卷积网络（CNN）进行后处理，使用固定权重初始化，训练数据包括同时记录的语音和电磁发音仪（EMA）数据集。

Result: 实验证明，固定权重初始化方法在较少的训练轮次下优于自适应权重初始化，模型在说话人相关、说话人无关、语料库相关和跨语料库模式下均表现出色。

Conclusion: 该方法为发音特征预测提供了高效和稳健的模型，有助于推动语音产生研究和应用的进步。

Abstract: Speech production is a complex sequential process which involve the
coordination of various articulatory features. Among them tongue being a highly
versatile active articulator responsible for shaping airflow to produce
targeted speech sounds that are intellectual, clear, and distinct. This paper
presents a novel approach for predicting tongue and lip articulatory features
involved in a given speech acoustics using a stacked Bidirectional Long
Short-Term Memory (BiLSTM) architecture, combined with a one-dimensional
Convolutional Neural Network (CNN) for post-processing with fixed weights
initialization. The proposed network is trained with two datasets consisting of
simultaneously recorded speech and Electromagnetic Articulography (EMA)
datasets, each introducing variations in terms of geographical origin,
linguistic characteristics, phonetic diversity, and recording equipment. The
performance of the model is assessed in Speaker Dependent (SD), Speaker
Independent (SI), corpus dependent (CD) and cross corpus (CC) modes.
Experimental results indicate that the proposed model with fixed weights
approach outperformed the adaptive weights initialization with in relatively
minimal number of training epochs. These findings contribute to the development
of robust and efficient models for articulatory feature prediction, paving the
way for advancements in speech production research and applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [161] [Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval](https://arxiv.org/abs/2504.17884)
*Yongkang Li,Panagiotis Eustratiadis,Simon Lupart,Evangelos Kanoulas*

Main category: cs.IR

TL;DR: 该论文提出了针对密集信息检索的语料库投毒攻击方法，通过直接优化嵌入空间中的对抗性文档生成，解决了现有工作在离散词空间操作的局限性，并在无查询先验知识的情况下实现了快速有效的攻击。


<details>
  <summary>Details</summary>
Motivation: 当前研究在对抗性攻击中主要在离散词空间操作，而检索实际发生在连续嵌入空间，同时多数方法假设攻击者已知查询分布。本文旨在解决这些局限，提出更高效的攻击方法。

Method: 通过训练一个扰动模型，在嵌入空间直接优化，保持原始与对抗文档的几何距离，同时最大化词级差异。攻击方法无需查询先验知识，适用于白盒和黑盒场景。

Result: 实验表明，该方法在两种攻击任务（top-1攻击和语料库投毒）中均有效，生成对抗文本的速度比现有最快方法快4倍，且文本更自然（低困惑度），难以检测。

Conclusion: 本文提出的嵌入空间优化方法实现了高效、自然的对抗攻击，突破了传统词空间操作的局限，并在无查询先验条件下验证了其有效性。

Abstract: This paper concerns corpus poisoning attacks in dense information retrieval,
where an adversary attempts to compromise the ranking performance of a search
algorithm by injecting a small number of maliciously generated documents into
the corpus. Our work addresses two limitations in the current literature.
First, attacks that perform adversarial gradient-based word substitution search
do so in the discrete lexical space, while retrieval itself happens in the
continuous embedding space. We thus propose an optimization method that
operates in the embedding space directly. Specifically, we train a perturbation
model with the objective of maintaining the geometric distance between the
original and adversarial document embeddings, while also maximizing the
token-level dissimilarity between the original and adversarial documents.
Second, it is common for related work to have a strong assumption that the
adversary has prior knowledge about the queries. In this paper, we focus on a
more challenging variant of the problem where the adversary assumes no prior
knowledge about the query distribution (hence, unsupervised). Our core
contribution is an adversarial corpus attack that is fast and effective. We
present comprehensive experimental results on both in- and out-of-domain
datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning
attack. We consider attacks under both a white-box and a black-box setting.
Notably, our method can generate successful adversarial examples in under two
minutes per target document; four times faster compared to the fastest
gradient-based word substitution methods in the literature with the same
hardware. Furthermore, our adversarial generation method generates text that is
more likely to occur under the distribution of natural text (low perplexity),
and is therefore more difficult to detect.

</details>


### [162] [OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation Learning](https://arxiv.org/abs/2504.17811)
*Anirudhan Badrinath,Alex Yang,Kousik Rajesh,Prabhat Agarwal,Jaewon Yang,Haoyu Chen,Jiajing Xu,Charles Rosenberg*

Main category: cs.IR

TL;DR: OmniSage是一个统一表示学习框架，结合图神经网络、内容模型和用户序列模型，通过对比学习任务处理多种数据类型，显著提升Pinterest用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前表示学习方法多样但缺乏统一框架，难以支持多种应用。OmniSage旨在整合图数据、用户序列和内容信号，提供通用表示。

Method: 采用多任务对比学习，结合图神经网络、内容模型和用户序列模型，并开发高效基础设施支持数十亿节点的图数据处理。

Result: OmniSage的通用表示使Pinterest全站“保存”操作提升约2.5%，覆盖五个应用场景。

Conclusion: 统一表示学习方法具有显著效果，OmniSage代码将开源，推动领域发展。

Abstract: Representation learning, a task of learning latent vectors to represent
entities, is a key task in improving search and recommender systems in web
applications. Various representation learning methods have been developed,
including graph-based approaches for relationships among entities,
sequence-based methods for capturing the temporal evolution of user activities,
and content-based models for leveraging text and visual content. However, the
development of a unifying framework that integrates these diverse techniques to
support multiple applications remains a significant challenge. This paper
presents OmniSage, a large-scale representation framework that learns universal
representations for a variety of applications at Pinterest. OmniSage integrates
graph neural networks with content-based models and user sequence models by
employing multiple contrastive learning tasks to effectively process graph
data, user sequence data, and content signals. To support the training and
inference of OmniSage, we developed an efficient infrastructure capable of
supporting Pinterest graphs with billions of nodes. The universal
representations generated by OmniSage have significantly enhanced user
experiences on Pinterest, leading to an approximate 2.5% increase in sitewide
repins (saves) across five applications. This paper highlights the impact of
unifying representation learning methods, and we will open source the OmniSage
code by the time of publication.

</details>


### [163] [Bridge the Domains: Large Language Models Enhanced Cross-domain Sequential Recommendation](https://arxiv.org/abs/2504.18383)
*Qidong Liu,Xiangyu Zhao,Yejing Wang,Zijian Zhang,Howard Zhong,Chong Chen,Xiang Li,Wei Huang,Feng Tian*

Main category: cs.IR

TL;DR: 论文提出LLM4CDSR模型，利用大语言模型解决跨域序列推荐中的重叠困境和转移复杂性，通过语义表示和层次化用户偏好建模提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨域序列推荐方法过度依赖全领域交互用户且难以学习复杂转移模式，大语言模型的强大表示和推理能力为这些问题提供了新思路。

Method: 1. 基于LLM的统一物品语义表示模块；2. 可训练适配器与对比正则化；3. 层次化LLM用户偏好建模；4. 三线程框架整合上述模块。

Result: 在三个公开跨域数据集上验证了LLM4CDSR的有效性，代码已开源。

Conclusion: LLM4CDSR通过语义关联和用户偏好分层建模显著提升了跨域推荐的性能和解耦能力。

Abstract: Cross-domain Sequential Recommendation (CDSR) aims to extract the preference
from the user's historical interactions across various domains. Despite some
progress in CDSR, two problems set the barrier for further advancements, i.e.,
overlap dilemma and transition complexity. The former means existing CDSR
methods severely rely on users who own interactions on all domains to learn
cross-domain item relationships, compromising the practicability. The latter
refers to the difficulties in learning the complex transition patterns from the
mixed behavior sequences. With powerful representation and reasoning abilities,
Large Language Models (LLMs) are promising to address these two problems by
bridging the items and capturing the user's preferences from a semantic view.
Therefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation
model (LLM4CDSR). To obtain the semantic item relationships, we first propose
an LLM-based unified representation module to represent items. Then, a
trainable adapter with contrastive regularization is designed to adapt the CDSR
task. Besides, a hierarchical LLMs profiling module is designed to summarize
user cross-domain preferences. Finally, these two modules are integrated into
the proposed tri-thread framework to derive recommendations. We have conducted
extensive experiments on three public cross-domain datasets, validating the
effectiveness of LLM4CDSR. We have released the code online.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [164] [Bayesian Quantum Orthogonal Neural Networks for Anomaly Detection](https://arxiv.org/abs/2504.18103)
*Natansh Mathur,Brian Coyle,Nishant Jain,Snehal Raj,Akshat Tandon,Jasper Simon Krauser,Rainer Stoessel*

Main category: quant-ph

TL;DR: 该论文结合贝叶斯学习和量子启发的正交神经网络，开发了一种用于3D物体异常检测的量子增强方法，并在IBM量子设备上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 3D物体的缺陷或异常检测对确保功能正确至关重要。本研究旨在利用贝叶斯学习和量子或量子启发方法（如正交神经网络）解决这一工业相关问题，并测试量子计算机在此类任务中的可行性。

Method: 开发了正交（量子）版本的3D卷积神经网络，结合贝叶斯学习以量化预测的不确定性。在IBM的127量子位Brisbane设备上进行了硬件实验，测试噪声和有限测量次数的影响。

Result: 实验表明，正交量子版本的3D卷积神经网络能够成功检测3D物体中的异常。

Conclusion: 该方法展示了量子增强异常检测的潜力，并为未来的量子计算在工业应用中的集成提供了参考。

Abstract: Identification of defects or anomalies in 3D objects is a crucial task to
ensure correct functionality. In this work, we combine Bayesian learning with
recent developments in quantum and quantum-inspired machine learning,
specifically orthogonal neural networks, to tackle this anomaly detection
problem for an industrially relevant use case. Bayesian learning enables
uncertainty quantification of predictions, while orthogonality in weight
matrices enables smooth training. We develop orthogonal (quantum) versions of
3D convolutional neural networks and show that these models can successfully
detect anomalies in 3D objects. To test the feasibility of incorporating
quantum computers into a quantum-enhanced anomaly detection pipeline, we
perform hardware experiments with our models on IBM's 127-qubit Brisbane
device, testing the effect of noise and limited measurement shots.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [165] [My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data](https://arxiv.org/abs/2504.17792)
*Hauke Sandhaus,Angel Hsing-Chi Hwang,Wendy Ju,Qian Yang*

Main category: cs.HC

TL;DR: 本文探讨了自动驾驶公司不愿共享安全关键数据的原因，并提出促进共享的新方法。


<details>
  <summary>Details</summary>
Motivation: 共享安全关键数据有助于提升所有自动驾驶车辆的安全性，但公司很少共享此类数据。研究旨在揭示其背后的障碍。

Method: 通过对12名自动驾驶公司员工的访谈，分析数据共享的政治性和知识私有化问题。

Result: 发现数据共享的两大障碍：数据集嵌入的关键知识具有竞争价值，员工视安全知识为私有而非公共财产。

Conclusion: 研究提出了激励数据共享的新方法，包括区分公共与私有知识、创新数据工具以及成本补偿机制。

Abstract: Safety-critical data, such as crash and near-crash records, are crucial to
improving autonomous vehicle (AV) design and development. Sharing such data
across AV companies, academic researchers, regulators, and the public can help
make all AVs safer. However, AV companies rarely share safety-critical data
externally. This paper aims to pinpoint why AV companies are reluctant to share
safety-critical data, with an eye on how these barriers can inform new
approaches to promote sharing. We interviewed twelve AV company employees who
actively work with such data in their day-to-day work. Findings suggest two
key, previously unknown barriers to data sharing: (1) Datasets inherently embed
salient knowledge that is key to improving AV safety and are
resource-intensive. Therefore, data sharing, even within a company, is fraught
with politics. (2) Interviewees believed AV safety knowledge is private
knowledge that brings competitive edges to their companies, rather than public
knowledge for social good. We discuss the implications of these findings for
incentivizing and enabling safety-critical AV data sharing, specifically,
implications for new approaches to (1) debating and stratifying public and
private AV safety knowledge, (2) innovating data tools and data sharing
pipelines that enable easier sharing of public AV safety data and knowledge;
(3) offsetting costs of curating safety-critical data and incentivizing data
sharing.

</details>


### [166] [Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content](https://arxiv.org/abs/2504.17964)
*Celia Chen,Alex Leitch*

Main category: cs.HC

TL;DR: 研究生通过与LLM的互动构建了评估框架，主要受专业身份、验证能力和系统导航经验影响，而非简单接受或拒绝输出。


<details>
  <summary>Details</summary>
Motivation: 探讨研究生如何评估与LLMs生成的机器专业知识，揭示其在AI交互中的评估模式。

Method: 通过问卷调查、LLM互动记录和14名研究生的深度访谈进行定性研究。

Result: 学生构建的评估框架受专业身份、验证能力和系统导航经验影响，不同领域的学生对LLM输出的反应不同。

Conclusion: 研究揭示了人机交互的新模式，并为平台如何支持用户评估AI生成内容提供建议。

Abstract: This paper examines how graduate students develop frameworks for evaluating
machine-generated expertise in web-based interactions with large language
models (LLMs). Through a qualitative study combining surveys, LLM interaction
transcripts, and in-depth interviews with 14 graduate students, we identify
patterns in how these emerging professionals assess and engage with
AI-generated content. Our findings reveal that students construct evaluation
frameworks shaped by three main factors: professional identity, verification
capabilities, and system navigation experience. Rather than uniformly accepting
or rejecting LLM outputs, students protect domains central to their
professional identities while delegating others--with managers preserving
conceptual work, designers safeguarding creative processes, and programmers
maintaining control over core technical expertise. These evaluation frameworks
are further influenced by students' ability to verify different types of
content and their experience navigating complex systems. This research
contributes to web science by highlighting emerging human-genAI interaction
patterns and suggesting how platforms might better support users in developing
effective frameworks for evaluating machine-generated expertise signals in
AI-mediated web environments.

</details>


### [167] [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
*Chaoran Chen,Zhiping Zhang,Ibrahim Khalilov,Bingcan Guo,Simret A Gebreegziabher,Yanfang Ye,Ziang Xiao,Yaxing Yao,Tianshi Li,Toby Jia-Jun Li*

Main category: cs.HC

TL;DR: 本文讨论了LLM驱动的GUI代理在处理敏感数据时的隐私和安全风险，提倡以人为中心的评估框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在GUI自动化中的广泛应用，其处理敏感数据可能带来的隐私和安全风险尚未被充分评估，需要更多关注。

Method: 文章分析了GUI代理的三大风险，回顾现有评估指标，并提出整合人类评估者的五大挑战。

Result: 发现现有评估主要关注性能，隐私和安全评估被忽视，提出了以人为中心的评估框架。

Conclusion: 呼吁在设计GUI代理时将隐私和安全考虑纳入评估框架，并通过用户知情同意增强风险意识。

Abstract: The rise of Large Language Models (LLMs) has revolutionized Graphical User
Interface (GUI) automation through LLM-powered GUI agents, yet their ability to
process sensitive data with limited human oversight raises significant privacy
and security risks. This position paper identifies three key risks of GUI
agents and examines how they differ from traditional GUI automation and general
autonomous agents. Despite these risks, existing evaluations focus primarily on
performance, leaving privacy and security assessments largely unexplored. We
review current evaluation metrics for both GUI and general LLM agents and
outline five key challenges in integrating human evaluators for GUI agent
assessments. To address these gaps, we advocate for a human-centered evaluation
framework that incorporates risk assessments, enhances user awareness through
in-context consent, and embeds privacy and security considerations into GUI
agent design and evaluation.

</details>


### [168] [Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving](https://arxiv.org/abs/2504.17999)
*Chang Xiao,Brenda Yang*

Main category: cs.HC

TL;DR: 论文提出了一种自适应流式方法，通过动态调整大语言模型（LLM）的输出速度以匹配用户认知负荷，显著节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的对话接口通常以固定速率输出，忽略了用户的实际阅读速度和认知负荷，导致计算资源浪费。

Method: 采用自适应流式方法，实时推断内容认知负荷，并动态调整输出速度，复杂内容放慢以释放资源。

Result: 统计分析和用户研究表明，该方法可减少高达16.8%的计算消耗，且不影响用户体验。

Conclusion: 该方法为云对话AI提供了高效资源管理框架，平衡了系统效率与用户满意度。

Abstract: Generative conversational interfaces powered by large language models (LLMs)
typically stream output token-by-token at a rate determined by computational
budget, often neglecting actual human reading speeds and the cognitive load
associated with the content. This mismatch frequently leads to inefficient use
of computational resources. For example, in cloud-based services, streaming
content faster than users can read appears unnecessary, resulting in wasted
computational resources and potential delays for other users, particularly
during peak usage periods. To address this issue, we propose an adaptive
streaming method that dynamically adjusts the pacing of LLM streaming output in
real-time based on inferred cognitive load. Our approach estimates the
cognitive load associated with streaming content and strategically slows down
the stream during complex or information-rich segments, thereby freeing
computational resources for other users. Our statistical analysis of
computational savings, combined with crowdsourced user studies, provides
insights into the trade-offs between service efficiency and user satisfaction,
demonstrating that our method can significantly reduce computational
consumption up to 16.8\%. This context-aware computational resource management
strategy presents a practical framework for enhancing system efficiency in
cloud-based conversational AI interfaces without compromising user experience.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [169] [Lecture Notes on Normalizing Flows for Lattice Quantum Field Theories](https://arxiv.org/abs/2504.18126)
*Miranda C. N. Cheng,Niki Stratikopoulou*

Main category: hep-lat

TL;DR: 本文探讨了机器学习（特别是归一化流）在解决量子场论中非微扰问题和拓扑结构挑战中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子场论的非微扰区域和临界点附近的挑战难以用传统解析方法解决，机器学习提供了新的研究途径。

Method: 结合归一化流（normalizing flows）与晶格场论，解决连续极限和临界点的模拟问题。

Result: 展示了机器学习方法在优化量子场论模拟中的潜力。

Conclusion: 归一化流为研究量子场论的复杂问题提供了有效工具，尤其是在处理拓扑结构和非微扰效应时。

Abstract: Numerical simulations of quantum field theories on lattices serve as a
fundamental tool for studying the non-perturbative regime of the theories,
where analytic tools often fall short. Challenges arise when one takes the
continuum limit or as the system approaches a critical point, especially in the
presence of non-trivial topological structures in the theory. Rapid recent
advances in machine learning provide a promising avenue for progress in this
area. These lecture notes aim to give a brief account of lattice field
theories, normalizing flows, and how the latter can be applied to study the
former. The notes are based on the lectures given by the first author in
various recent research schools.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [170] [Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation](https://arxiv.org/abs/2504.18323)
*Yangyang Xu,Kexin Li,Li Yang,You-Wei Wen*

Main category: math.NA

TL;DR: 提出了一种自引导数据增强的自适应加权TRPCA方法，通过动态抑制异常值影响并将其转化为标准TPCA问题，提高了处理结构化损坏的能力。


<details>
  <summary>Details</summary>
Motivation: 现有TRPCA方法依赖稀疏异常假设，对结构化损坏处理效果不佳，需要更鲁棒的解决方案。

Method: 采用自引导数据增强和自适应加权策略，通过优化驱动的加权方案动态调整异常值权重，并使用高效近端块坐标下降算法求解。

Result: 在合成和真实数据集（如面部恢复、背景去除和高光谱去噪）上的实验表明，该方法在处理多种损坏模式时表现优越。

Conclusion: 所提方法在准确性和计算效率上均优于现有技术，适用于多维度数据分解任务。

Abstract: Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique
for decomposing multi-dimensional data into a low-rank tensor and an outlier
tensor, yet existing methods relying on sparse outlier assumptions often fail
under structured corruptions. In this paper, we propose a self-guided data
augmentation approach that employs adaptive weighting to suppress outlier
influence, reformulating the original TRPCA problem into a standard Tensor
Principal Component Analysis (TPCA) problem. The proposed model involves an
optimization-driven weighting scheme that dynamically identifies and
downweights outlier contributions during tensor augmentation. We develop an
efficient proximal block coordinate descent algorithm with closed-form updates
to solve the resulting optimization problem, ensuring computational efficiency.
Theoretical convergence is guaranteed through a framework combining block
coordinate descent with majorization-minimization principles. Numerical
experiments on synthetic and real-world datasets, including face recovery,
background subtraction, and hyperspectral denoising, demonstrate that our
method effectively handles various corruption patterns. The results show the
improvements in both accuracy and computational efficiency compared to
state-of-the-art methods.

</details>


### [171] [PODNO: Proper Orthogonal Decomposition Neural Operators](https://arxiv.org/abs/2504.18513)
*Zilan Cheng,Zhongjian Wang,Li-Lian Wang,Mejdi Azaiez*

Main category: math.NA

TL;DR: 该论文提出了一种名为PODNO的新方法，通过结合POD方法改进FNO，用于解决高频主导的PDE问题。


<details>
  <summary>Details</summary>
Motivation: 传统的FNO在处理高频PDE问题时存在局限，PODNO旨在通过更优的POD基提升精度和计算效率。

Method: 用POD生成的（逆）正交变换替换FNO中的傅里叶变换，构建积分核。

Result: PODNO在Nonlinear Schrodinger方程和KP方程上的数值实验显示其优于FNO。

Conclusion: PODNO在高频PDE问题中具有潜力，且广义版本GSO的普适性得到理论支持。

Abstract: In this paper, we introduce Proper Orthogonal Decomposition Neural Operators
(PODNO) for solving partial differential equations (PDEs) dominated by
high-frequency components. Building on the structure of Fourier Neural
Operators (FNO), PODNO replaces the Fourier transform with (inverse)
orthonormal transforms derived from the Proper Orthogonal Decomposition (POD)
method to construct the integral kernel. Due to the optimality of POD basis,
the PODNO has potential to outperform FNO in both accuracy and computational
efficiency for high-frequency problems. From analysis point of view, we
established the universality of a generalization of PODNO, termed as
Generalized Spectral Operator (GSO). In addition, we evaluate PODNO's
performance numerically on dispersive equations such as the Nonlinear
Schrodinger (NLS) equation and the Kadomtsev-Petviashvili (KP) equation.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [172] [SMARTFinRAG: Interactive Modularized Financial RAG Benchmark](https://arxiv.org/abs/2504.18024)
*Yiwei Zha*

Main category: cs.CE

TL;DR: 论文介绍了一个名为SMARTFinRAG的平台，用于评估金融领域的RAG系统，解决了模块化架构、领域特定QA对生成和研究-实现桥梁的三大关键问题。


<details>
  <summary>Details</summary>
Motivation: 金融领域快速采用语言模型技术，但评估专门的RAG系统仍具挑战性。

Method: SMARTFinRAG采用模块化架构、文档中心评估范式（生成领域特定QA对）和直观界面。

Result: 评估显示不同配置下的检索效力和响应质量存在显著差异，平台支持透明和可重复的研究。

Conclusion: SMARTFinRAG为金融机构部署RAG系统提供了实用解决方案，支持研究与实践的结合。

Abstract: Financial sectors are rapidly adopting language model technologies, yet
evaluating specialized RAG systems in this domain remains challenging. This
paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG
assessment: (1) a fully modular architecture where components can be
dynamically interchanged during runtime; (2) a document-centric evaluation
paradigm generating domain-specific QA pairs from newly ingested financial
documents; and (3) an intuitive interface bridging research-implementation
divides. Our evaluation quantifies both retrieval efficacy and response
quality, revealing significant performance variations across configurations.
The platform's open-source architecture supports transparent, reproducible
research while addressing practical deployment challenges faced by financial
institutions implementing RAG systems.

</details>


### [173] [Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression](https://arxiv.org/abs/2504.18461)
*Stefano Markidis,Jonah Ekelund,Luca Pennati,Andong Hu,Ivy Peng*

Main category: cs.CE

TL;DR: 该研究使用符号回归方法从历史数据中推导描述Dst指数时间演化的数据驱动方程，展示了比传统模型更高的准确性，尤其是在中等磁暴期间。


<details>
  <summary>Details</summary>
Motivation: 地磁暴对空间和地面基础设施构成重大风险，Dst指数用于量化其强度。传统模型虽然有效但存在局限性，因此需要数据驱动的、可解释的数学表达式来描述Dst指数的演化。

Method: 利用NASA OMNIweb数据库的历史数据（如太阳风密度、速度等），通过PySR框架（基于进化算法的符号回归库）生成dDst/dt与太阳风参数的数学关系式，并与Burton-McPherron-Russell等传统模型对比。

Result: 符号回归模型在多数情况下表现出更高的准确性，尤其在中等磁暴期间，同时保持了物理可解释性。通过2003年万圣节磁暴等历史事件验证了模型的性能。

Conclusion: 研究提供了可解释的闭式表达式，捕捉了Dst演化中的非线性依赖和阈值效应，为地磁暴预测提供了新工具。

Abstract: Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere
driven by solar wind interactions, posing significant risks to space-based and
ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies
geomagnetic storm intensity by measuring global magnetic field variations. This
study applies symbolic regression to derive data-driven equations describing
the temporal evolution of the Dst index. We use historical data from the NASA
OMNIweb database, including solar wind density, bulk velocity, convective
electric field, dynamic pressure, and magnetic pressure. The PySR framework, an
evolutionary algorithm-based symbolic regression library, is used to identify
mathematical expressions linking dDst/dt to key solar wind. The resulting
models include a hierarchy of complexity levels and enable a comparison with
well-established empirical models such as the Burton-McPherron-Russell and
O'Brien-McPherron models. The best-performing symbolic regression models
demonstrate superior accuracy in most cases, particularly during moderate
geomagnetic storms, while maintaining physical interpretability. Performance
evaluation on historical storm events includes the 2003 Halloween Storm, the
2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide
interpretable, closed-form expressions that capture nonlinear dependencies and
thresholding effects in Dst evolution.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [174] [iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting](https://arxiv.org/abs/2504.17954)
*Kaiyuan Tang,Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TL;DR: 该论文提出了一种基于高斯抛洒的逆体积渲染方法（iVR-GS），在降低渲染成本的同时支持场景编辑，实现了交互式体积探索。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成（NVS）方法虽然渲染速度快且硬件要求低，但渲染场景的可见部分受预设传输函数（TF）限制，无法满足用户交互探索需求。

Method: 通过构建多个覆盖不同可见部分的基本TF关联的iVR-GS模型，每个模型包含可编辑的3D高斯分布点，支持实时渲染与编辑。

Result: 实验表明，iVR-GS在多种体积数据集上优于其他NVS方法（如Plenoxels、CCNeRF和3DGS），且重建质量和可组合性更优。

Conclusion: iVR-GS实现了高效、可编辑的体积渲染，为交互式探索提供了新解决方案。

Abstract: In volume visualization, users can interactively explore the
three-dimensional data by specifying color and opacity mappings in the transfer
function (TF) or adjusting lighting parameters, facilitating meaningful
interpretation of the underlying structure. However, rendering large-scale
volumes demands powerful GPUs and high-speed memory access for real-time
performance. While existing novel view synthesis (NVS) methods offer faster
rendering speeds with lower hardware requirements, the visible parts of a
reconstructed scene are fixed and constrained by preset TF settings,
significantly limiting user exploration. This paper introduces inverse volume
rendering via Gaussian splatting (iVR-GS), an innovative NVS method that
reduces the rendering cost while enabling scene editing for interactive volume
exploration. Specifically, we compose multiple iVR-GS models associated with
basic TFs covering disjoint visible parts to make the entire volumetric scene
visible. Each basic model contains a collection of 3D editable Gaussians, where
each Gaussian is a 3D spatial point that supports real-time scene rendering and
editing. We demonstrate the superior reconstruction quality and composability
of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on
various volume datasets. The code is available at
https://github.com/TouKaienn/iVR-GS.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [175] [Flow Matching Ergodic Coverage](https://arxiv.org/abs/2504.17872)
*Max Muchen Sun,Allison Pinosky,Todd Murphey*

Main category: cs.RO

TL;DR: 论文提出了一种基于流匹配的遍历覆盖方法，替代了传统方法中的遍历度量限制，并验证了其性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有遍历覆盖方法受限于可用的遍历度量，限制了性能表现。

Method: 采用流匹配技术，推导致遍历覆盖的流匹配问题，并展示其等价于一个闭环解的线性二次调节问题。

Result: 新方法支持了以前不可行的度量，提升了鲁棒性和覆盖性能，并在数值实验和机器人任务中验证了有效性。

Conclusion: 基于流匹配的遍历覆盖方法克服了现有度量的限制，具有更好的性能和实用性。

Abstract: Ergodic coverage effectively generates exploratory behaviors for embodied
agents by aligning the spatial distribution of the agent's trajectory with a
target distribution, where the difference between these two distributions is
measured by the ergodic metric. However, existing ergodic coverage methods are
constrained by the limited set of ergodic metrics available for control
synthesis, fundamentally limiting their performance. In this work, we propose
an alternative approach to ergodic coverage based on flow matching, a technique
widely used in generative inference for efficient and scalable sampling. We
formally derive the flow matching problem for ergodic coverage and show that it
is equivalent to a linear quadratic regulator problem with a closed-form
solution. Our formulation enables alternative ergodic metrics from generative
inference that overcome the limitations of existing ones. These metrics were
previously infeasible for control synthesis but can now be supported with no
computational overhead. Specifically, flow matching with the Stein variational
gradient flow enables control synthesis directly over the score function of the
target distribution, improving robustness to the unnormalized distributions; on
the other hand, flow matching with the Sinkhorn divergence flow enables an
optimal transport-based ergodic metric, improving coverage performance on
non-smooth distributions with irregular supports. We validate the improved
performance and competitive computational efficiency of our method through
comprehensive numerical benchmarks and across different nonlinear dynamics. We
further demonstrate the practicality of our method through a series of drawing
and erasing tasks on a Franka robot.

</details>


### [176] [Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies](https://arxiv.org/abs/2504.17901)
*Benned Hedegaard,Ziyi Yang,Yichen Wei,Ahmed Jaafar,Stefanie Tellex,George Konidaris,Naman Shah*

Main category: cs.RO

TL;DR: 该论文提出了一种名为TASP的新方法，通过Composable Interaction Primitives (CIPs)将运动规划与非组合式预学习技能整合，解决了传统任务与运动规划中仅考虑运动学的问题。


<details>
  <summary>Details</summary>
Motivation: 传统任务与运动规划方法仅考虑运动学技能，无法有效利用闭环电机控制器等非运动学技能。本文旨在解决这一问题，使机器人能够在规划中结合更多通用技能。

Method: 采用Composable Interaction Primitives (CIPs)方法，将闭环电机控制器等非运动学技能整合到分层机器人规划中。

Result: 目前正在进行的真实场景机器人实验展示了TASP方法的有效性，表明移动操作机器人可以通过CIPs结合运动规划和通用技能完成复杂任务。

Conclusion: TASP通过CIPs成功扩展了传统任务与运动规划的能力，为非组合式技能的应用提供了新途径，实验验证了其在实际场景中的潜力。

Abstract: Task and motion planning is a well-established approach for solving
long-horizon robot planning problems. However, traditional methods assume that
each task-level robot action, or skill, can be reduced to kinematic motion
planning. In this work, we address the challenge of planning with both
kinematic skills and closed-loop motor controllers that go beyond kinematic
considerations. We propose a novel method that integrates these controllers
into motion planning using Composable Interaction Primitives (CIPs), enabling
the use of diverse, non-composable pre-learned skills in hierarchical robot
planning. Toward validating our Task and Skill Planning (TASP) approach, we
describe ongoing robot experiments in real-world scenarios designed to
demonstrate how CIPs can allow a mobile manipulator robot to effectively
combine motion planning with general-purpose skills to accomplish complex
tasks.

</details>


### [177] [Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm](https://arxiv.org/abs/2504.17979)
*Kaaustaaub Shankar,Wilhelm Louw,Bharadwaj Dogga,Nick Ernest,Tim Arnett,Kelly Cohen*

Main category: cs.RO

TL;DR: 论文提出了一种模糊快速探索随机树算法，用于改进太空任务中的半自主机器人手术系统，显著提升了路径搜索时间和路径成本。


<details>
  <summary>Details</summary>
Motivation: 解决长期星际任务中医疗挑战，因船员有限和通信延迟，传统手术方式不适用，需发展半自主机器人手术。

Method: 采用模糊快速探索随机树算法，用于两自由度机器人臂的避障和协作控制。

Result: 算法使路径搜索时间提升743%，路径成本降低43%。

Conclusion: 该算法显著提升了半自主机器人手术系统的性能，适用于太空任务。

Abstract: AI-driven semi-autonomous robotic surgery is essential for addressing the
medical challenges of long-duration interplanetary missions, where limited crew
sizes and communication delays restrict traditional surgical approaches.
Current robotic surgery systems require full surgeon control, demanding
extensive expertise and limiting feasibility in space. We propose a novel
adaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle
avoidance and collaborative control in a two-degree-of-freedom robotic arm
modeled on the Miniaturized Robotic-Assisted surgical system. It was found that
the Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent
improvement to path search time and 43 percent improvement to path cost.

</details>


### [178] [Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation](https://arxiv.org/abs/2504.18010)
*Zilin Huang,Zihao Sheng,Zhengyang Wan,Yansong Qu,Yuhao Luo,Boyue Wang,Pei Li,Yen-Jung Chen,Jiancong Chen,Keke Long,Jiayi Meng,Yue Leng,Sikai Chen*

Main category: cs.RO

TL;DR: Sky-Drive是新型分布式多智能体仿真平台，通过四大创新解决现有仿真的不足，支持人机协作和社会意识研究。


<details>
  <summary>Details</summary>
Motivation: 现有仿真器无法满足未来交通研究的需求，尤其是在社会意识驾驶智能体和有效人机协作方面。

Method: 采用分布式架构、多模态人机交互框架、人机协作机制及数字孪生技术。

Result: Sky-Drive支持多种应用，包括AV-VRU交互建模、人机培训、社会意识强化学习等。

Conclusion: Sky-Drive有潜力成为下一代社会意识和以人为中心的自动驾驶研究的基础平台。

Abstract: Recent advances in autonomous system simulation platforms have significantly
enhanced the safe and scalable testing of driving policies. However, existing
simulators do not yet fully meet the needs of future transportation research,
particularly in modeling socially-aware driving agents and enabling effective
human-AI collaboration. This paper introduces Sky-Drive, a novel distributed
multi-agent simulation platform that addresses these limitations through four
key innovations: (a) a distributed architecture for synchronized simulation
across multiple terminals; (b) a multi-modal human-in-the-loop framework
integrating diverse sensors to collect rich behavioral data; (c) a human-AI
collaboration mechanism supporting continuous and adaptive knowledge exchange;
and (d) a digital twin (DT) framework for constructing high-fidelity virtual
replicas of real-world transportation environments. Sky-Drive supports diverse
applications such as autonomous vehicle (AV)-vulnerable road user (VRU)
interaction modeling, human-in-the-loop training, socially-aware reinforcement
learning, personalized driving policy, and customized scenario generation.
Future extensions will incorporate foundation models for context-aware decision
support and hardware-in-the-loop (HIL) testing for real-world validation. By
bridging scenario generation, data collection, algorithm training, and hardware
integration, Sky-Drive has the potential to become a foundational platform for
the next generation of socially-aware and human-centered autonomous
transportation research. The demo video and code are available
at:https://sky-lab-uw.github.io/Sky-Drive-website/

</details>


### [179] [Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization](https://arxiv.org/abs/2504.18057)
*Jiayi Chen,Shuai Wang,Guoliang Li,Wei Xu,Guangxu Zhu,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.RO

TL;DR: 本文提出了机会协作规划（OCP），结合小型本地模型与强大云端模型，通过LVM-MPC和CTO优化协作时机，提升自动驾驶在开放场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在开放场景中处理未知物体时小型模型泛化能力不足与大型模型资源消耗高的矛盾。

Method: 提出LVM-MPC利用云端进行感知与决策，本地MPC执行；通过ODCT和CFS优化协作时机（CTO）。

Result: 实验表明OCP在导航时间和成功率上优于现有方法。

Conclusion: OCP有效平衡资源与性能，为自动驾驶协作规划提供新思路。

Abstract: Navigating autonomous vehicles in open scenarios is a challenge due to the
difficulties in handling unseen objects. Existing solutions either rely on
small models that struggle with generalization or large models that are
resource-intensive. While collaboration between the two offers a promising
solution, the key challenge is deciding when and how to engage the large model.
To address this issue, this paper proposes opportunistic collaborative planning
(OCP), which seamlessly integrates efficient local models with powerful cloud
models through two key innovations. First, we propose large vision model guided
model predictive control (LVM-MPC), which leverages the cloud for LVM
perception and decision making. The cloud output serves as a global guidance
for a local MPC, thereby forming a closed-loop perception-to-control system.
Second, to determine the best timing for large model query and service, we
propose collaboration timing optimization (CTO), including object detection
confidence thresholding (ODCT) and cloud forward simulation (CFS), to decide
when to seek cloud assistance and when to offer cloud service. Extensive
experiments show that the proposed OCP outperforms existing methods in terms of
both navigation time and success rate.

</details>


### [180] [Depth-Constrained ASV Navigation with Deep RL and Limited Sensing](https://arxiv.org/abs/2504.18253)
*Amirhossein Zhalehmehrabi,Daniele Meli,Francesco Dal Santo,Francesco Trotti,Alessandro Farinelli*

Main category: cs.RO

TL;DR: 提出了一种结合强化学习与高斯过程的框架，用于自主水面车辆（ASV）在浅水环境中的导航，通过稀疏声呐数据逐步估计水深图，提升安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 浅水环境中ASV导航因动态干扰和水深限制面临挑战，传统方法在传感器信息有限的情况下难以安全高效运行。

Method: 使用强化学习框架，结合高斯过程回归，通过单波束回声测深仪（SBES）的稀疏数据逐步构建水深图，优化导航决策。

Result: 实验验证该框架能有效提升ASV在浅水环境的导航性能，并实现从仿真到现实的高效迁移。

Conclusion: 该方法为浅水环境下的ASV导航提供了安全可靠的解决方案，具有实际应用潜力。

Abstract: Autonomous Surface Vehicles (ASVs) play a crucial role in maritime
operations, yet their navigation in shallow-water environments remains
challenging due to dynamic disturbances and depth constraints. Traditional
navigation strategies struggle with limited sensor information, making safe and
efficient operation difficult. In this paper, we propose a reinforcement
learning (RL) framework for ASV navigation under depth constraints, where the
vehicle must reach a target while avoiding unsafe areas with only a single
depth measurement per timestep from a downward-facing Single Beam Echosounder
(SBES). To enhance environmental awareness, we integrate Gaussian Process (GP)
regression into the RL framework, enabling the agent to progressively estimate
a bathymetric depth map from sparse sonar readings. This approach improves
decision-making by providing a richer representation of the environment.
Furthermore, we demonstrate effective sim-to-real transfer, ensuring that
trained policies generalize well to real-world aquatic conditions. Experimental
results validate our method's capability to improve ASV navigation performance
while maintaining safety in challenging shallow-water environments.

</details>


### [181] [CIVIL: Causal and Intuitive Visual Imitation Learning](https://arxiv.org/abs/2504.17959)
*Yinlong Dai,Robert Ramirez Sanchez,Ryan Jeronimus,Shahabedin Sagheb,Cara M. Nunez,Heramb Nemlekar,Dylan P. Losey*

Main category: cs.RO

TL;DR: 论文提出CIVIL算法，通过人类标记和语言提示来增强视觉模仿学习，以更好地理解任务相关特征，从而提高机器人在新场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉模仿学习方法仅关注人类的动作，忽视了行为背后的决策原因，导致机器人在环境变化时表现不佳。论文旨在通过引入人类提供的特征标记和语言提示，提升机器人对任务的理解能力。

Method: 提出CIVIL算法，利用人类提供的额外数据（标记和语言提示）来过滤视觉观察并提取与人类行为因果相关的特征表示，随后训练基于transformer的策略。

Result: 实验表明，采用CIVIL训练的机器人能在更少的人类演示下学习，并在未见过的场景中表现优于现有方法。

Conclusion: 通过结合人类提供的特征信息，CIVIL能显著提升视觉模仿学习的效率和适应性，尤其在复杂或变化的环境中。

Abstract: Today's robots learn new tasks by imitating human examples. However, this
standard approach to visual imitation learning is fundamentally limited: the
robot observes what the human does, but not why the human chooses those
behaviors. Without understanding the features that factor into the human's
decisions, robot learners often misinterpret the data and fail to perform the
task when the environment changes. We therefore propose a shift in perspective:
instead of asking human teachers just to show what actions the robot should
take, we also enable humans to indicate task-relevant features using markers
and language prompts. Our proposed algorithm, CIVIL, leverages this augmented
data to filter the robot's visual observations and extract a feature
representation that causally informs human actions. CIVIL then applies these
causal features to train a transformer-based policy that emulates human
behaviors without being confused by visual distractors. Our simulations,
real-world experiments, and user study demonstrate that robots trained with
CIVIL can learn from fewer human demonstrations and perform better than
state-of-the-art baselines, especially in previously unseen scenarios. See
videos at our project website: https://civil2025.github.io

</details>


### [182] [Plug-and-Play Physics-informed Learning using Uncertainty Quantified Port-Hamiltonian Models](https://arxiv.org/abs/2504.17966)
*Kaiyuan Tan,Peilun Li,Jun Wang,Thomas Beckers*

Main category: cs.RO

TL;DR: 该文章提出了一种插件式物理信息机器学习框架（PnP-PIML），通过结合保形预测和分布式Port-Hamiltonian系统（dPHS），在遇到训练数据分布外的观测时，能够切换到物理一致的模型，以提高预测的可靠性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 在机器人应用中，预测周围物体和障碍物的轨迹至关重要。然而，当遇到训练数据分布外的场景时，数据驱动方法的性能和可靠性会下降。本文旨在解决这一挑战。

Method: 提出了PnP-PIML框架，通过保形预测识别异常动态，并切换到基于高斯过程的dPHS模型，学习系统动态并量化预测不确定性。

Result: 该框架能够在分布外场景下提供可靠且物理信息丰富的预测。

Conclusion: PnP-PIML框架通过结合数据驱动和物理模型，显著提升了预测的鲁棒性和不确定性量化能力。

Abstract: The ability to predict trajectories of surrounding agents and obstacles is a
crucial component in many robotic applications. Data-driven approaches are
commonly adopted for state prediction in scenarios where the underlying
dynamics are unknown. However, the performance, reliability, and uncertainty of
data-driven predictors become compromised when encountering out-of-distribution
observations relative to the training data. In this paper, we introduce a
Plug-and-Play Physics-Informed Machine Learning (PnP-PIML) framework to address
this challenge. Our method employs conformal prediction to identify outlier
dynamics and, in that case, switches from a nominal predictor to a
physics-consistent model, namely distributed Port-Hamiltonian systems (dPHS).
We leverage Gaussian processes to model the energy function of the dPHS,
enabling not only the learning of system dynamics but also the quantification
of predictive uncertainty through its Bayesian nature. In this way, the
proposed framework produces reliable physics-informed predictions even for the
out-of-distribution scenarios.

</details>


### [183] [Action Flow Matching for Continual Robot Learning](https://arxiv.org/abs/2504.18471)
*Alejandro Murillo-Gonzalez,Lantao Liu*

Main category: cs.RO

TL;DR: 论文提出了一种基于流匹配的生成框架，用于在线优化机器人动力学模型，通过调整动作而非探索来提高数据效率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 旨在解决机器人持续学习中的关键挑战，如模型动态优化、安全适应、灾难性遗忘等问题，以实现像人类一样的适应能力。

Method: 采用生成框架和流匹配技术，优化动作以匹配理想模型，减少对传统探索方式或历史数据的依赖。

Result: 在无人车和四旋翼平台上验证，任务成功率提高了34.2%，显示了高效的数据收集和学习加速能力。

Conclusion: 该方法展现了机器人持续学习的潜力，特别是在适应性和效率方面的显著提升。

Abstract: Continual learning in robotics seeks systems that can constantly adapt to
changing environments and tasks, mirroring human adaptability. A key challenge
is refining dynamics models, essential for planning and control, while
addressing issues such as safe adaptation, catastrophic forgetting, outlier
management, data efficiency, and balancing exploration with exploitation -- all
within task and onboard resource constraints. Towards this goal, we introduce a
generative framework leveraging flow matching for online robot dynamics model
alignment. Rather than executing actions based on a misaligned model, our
approach refines planned actions to better match with those the robot would
take if its model was well aligned. We find that by transforming the actions
themselves rather than exploring with a misaligned model -- as is traditionally
done -- the robot collects informative data more efficiently, thereby
accelerating learning. Moreover, we validate that the method can handle an
evolving and possibly imperfect model while reducing, if desired, the
dependency on replay buffers or legacy model snapshots. We validate our
approach using two platforms: an unmanned ground vehicle and a quadrotor. The
results highlight the method's adaptability and efficiency, with a record
34.2\% higher task success rate, demonstrating its potential towards enabling
continual robot learning. Code:
https://github.com/AlejandroMllo/action_flow_matching.

</details>
