<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.IR](#cs.IR) [Total: 5]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.CV](#cs.CV) [Total: 27]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.SI](#cs.SI) [Total: 4]
- [cs.HC](#cs.HC) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.RO](#cs.RO) [Total: 2]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 5]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.SE](#cs.SE) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
*Trilok Padhi,Ramneet Kaur,Adam D. Cobb,Manoj Acharya,Anirban Roy,Colin Samplawski,Brian Matejek,Alexander M. Berenbeim,Nathaniel D. Bastian,Susmit Jha*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的不确定性量化（UQ）校准方法，针对多模态大语言模型（LLMs），通过结合跨模态一致性和自一致性来改善校准效果。


<details>
  <summary>Details</summary>
Motivation: 现有的UQ方法在多模态LLMs中存在校准不足的问题，尤其是在模型持续错误时仍报告高置信度，导致置信度与准确性不匹配。

Method: 该方法通过将文本响应与视觉输入关联，利用基础模型的置信度校准整体置信度，并结合温度缩放技术校准基础模型的不确定性。

Result: 实验结果表明，该方法在医疗问答（Slake）和视觉问答（VQAv2）等任务中显著改善了校准效果。

Conclusion: 通过结合跨模态一致性和温度缩放技术，该方法有效提升了多模态LLMs的不确定性量化校准性能。

Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ)
tailored for multi-modal large language models (LLMs). Existing
state-of-the-art UQ methods rely on consistency among multiple responses
generated by the LLM on an input query under diverse settings. However, these
approaches often report higher confidence in scenarios where the LLM is
consistently incorrect. This leads to a poorly calibrated confidence with
respect to accuracy. To address this, we leverage cross-modal consistency in
addition to self-consistency to improve the calibration of the multi-modal
models. Specifically, we ground the textual responses to the visual inputs. The
confidence from the grounding model is used to calibrate the overall
confidence. Given that using a grounding model adds its own uncertainty in the
pipeline, we apply temperature scaling - a widely accepted parametric
calibration technique - to calibrate the grounding model's confidence in the
accuracy of generated responses. We evaluate the proposed approach across
multiple multi-modal tasks, such as medical question answering (Slake) and
visual question answering (VQAv2), considering multi-modal models such as
LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework
achieves significantly improved calibration on both tasks.

</details>


### [2] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
*Gianluca Manzo,Julia Ive*

Main category: cs.CL

TL;DR: 使用深度学习自动化胸片解读的研究表明，尽管贝叶斯方法提供了有用的不确定性估计，但仍需进一步优化以更好地匹配人类解读的细微差别。


<details>
  <summary>Details</summary>
Motivation: 自动化胸片解读能提升临床效率，但仅优化预测性能不足，还需量化不确定性以匹配临床需求。

Method: 利用BERT模型，比较不同不确定性标签二值化方法，并评估蒙特卡洛Dropout和深度集成技术对预测不确定性的估计效果。

Result: 模型表现良好，但预测不确定性与语言不确定性（基于放射科报告）仅显示适度相关性。

Conclusion: 贝叶斯近似能提供有价值的不确定性估计，但需进一步改进以更贴合临床应用中的复杂性和人类解读的细节。

Abstract: Automating chest radiograph interpretation using Deep Learning (DL) models
has the potential to significantly improve clinical workflows, decision-making,
and large-scale health screening. However, in medical settings, merely
optimising predictive performance is insufficient, as the quantification of
uncertainty is equally crucial. This paper investigates the relationship
between predictive uncertainty, derived from Bayesian Deep Learning
approximations, and human/linguistic uncertainty, as estimated from free-text
radiology reports labelled by rule-based labellers. Utilising BERT as the model
of choice, this study evaluates different binarisation methods for uncertainty
labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in
estimating predictive uncertainty. The results demonstrate good model
performance, but also a modest correlation between predictive and linguistic
uncertainty, highlighting the challenges in aligning machine uncertainty with
human interpretation nuances. Our findings suggest that while Bayesian
approximations provide valuable uncertainty estimates, further refinement is
necessary to fully capture and utilise the subtleties of human uncertainty in
clinical applications.

</details>


### [3] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
*Lucia Zheng,Neel Guha,Javokhir Arifov,Sarah Zhang,Michal Skreta,Christopher D. Manning,Peter Henderson,Daniel E. Ho*

Main category: cs.CL

TL;DR: 这篇论文介绍了两个新的法律RAG基准测试（Bar Exam QA和Housing Statute QA），旨在解决法律领域RAG系统缺乏真实复杂基准的问题，并展示了现有检索管线的性能，强调了法律RAG应用的挑战性。


<details>
  <summary>Details</summary>
Motivation: 法律领域缺乏能够捕捉法律检索和问答复杂性的真实RAG基准测试，阻碍了专业RAG系统的发展。

Method: 通过类似于法律研究的标注流程，构建了两个新的法律RAG基准测试（Bar Exam QA和Housing Statute QA），并评估了现有检索管线的性能。

Result: 现有检索管线在法律RAG任务中表现不佳，表明法律RAG仍是一个具有挑战性的应用场景。

Conclusion: 研究成果强调了未来在法律RAG领域进一步研究的必要性。

Abstract: As the legal community increasingly examines the use of large language models
(LLMs) for various legal applications, legal AI developers have turned to
retrieval-augmented LLMs ("RAG" systems) to improve system performance and
robustness. An obstacle to the development of specialized RAG systems is the
lack of realistic legal RAG benchmarks which capture the complexity of both
legal retrieval and downstream legal question-answering. To address this, we
introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA.
Our tasks correspond to real-world legal research tasks, and were produced
through annotation processes which resemble legal research. We describe the
construction of these benchmarks and the performance of existing retriever
pipelines. Our results suggest that legal RAG remains a challenging
application, thus motivating future research.

</details>


### [4] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
*Jiale Liu,Yifan Zeng,Shaokun Zhang,Chi Zhang,Malte Højmark-Bertelsen,Marie Normann Gadeberg,Huazheng Wang,Qingyun Wu*

Main category: cs.CL

TL;DR: 论文提出了Fine-Grained Optimization (FGO)框架，通过将大规模优化任务拆分为可管理的子集，进行针对性优化并逐步合并，有效解决了LLM优化中的上下文溢出和模式识别退化问题。在多个基准测试中，FGO性能优于现有方法1.6-8.6%，同时平均提示令牌消耗减少56.3%。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM优化方法在面对大规模训练数据时，由于上下文窗口溢出和模式识别能力下降，表现不佳。FGO旨在通过细粒度优化提升可扩展性和效率。

Method: FGO将大型优化任务拆分为子集，进行针对性优化后逐步合并，实现高效的渐进式优化。

Result: 实验表明，FGO在ALFWorld、LogisticsQA和GAIA基准测试中性能提升1.6-8.6%，提示令牌消耗减少56.3%，且在不同规模数据集上表现一致。

Conclusion: FGO为复杂代理系统的LLM优化提供了可扩展且高效的解决方案，尤其适合大规模数据集。

Abstract: LLM-based optimization has shown remarkable potential in enhancing agentic
systems. However, the conventional approach of prompting LLM optimizer with the
whole training trajectories on training dataset in a single pass becomes
untenable as datasets grow, leading to context window overflow and degraded
pattern recognition. To address these challenges, we propose Fine-Grained
Optimization (FGO), a scalable framework that divides large optimization tasks
into manageable subsets, performs targeted optimizations, and systematically
combines optimized components through progressive merging. Evaluation across
ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms
existing approaches by 1.6-8.6% while reducing average prompt token consumption
by 56.3%. Our framework provides a practical solution for scaling up LLM-based
optimization of increasingly sophisticated agent systems. Further analysis
demonstrates that FGO achieves the most consistent performance gain in all
training dataset sizes, showcasing its scalability and efficiency.

</details>


### [5] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Timothy Ossowski,Yu Gu,Ying Jin,Sid Kiblawi,Sam Preston,Mu Wei,Paul Vozila,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: 该论文探讨了推理能力是否可跨模态和领域推广，并提出X-Reasoner模型：通过通用文本训练实现跨模态和领域推理，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开源研究多集中于纯文本推理模型，缺乏对多模态和跨领域推理能力的探索。本文试图验证推理能力的普适性。

Method: 采用两阶段训练：监督微调（长思维链蒸馏）和强化学习（可验证奖励），后扩展到医疗领域（X-Reasoner-Med）。

Result: X-Reasoner在通用和医疗多模态任务中表现优于现有方法，X-Reasoner-Med进一步在医疗基准上达到新SOTA。

Conclusion: 通用文本训练可推广推理能力，领域专用数据能进一步提升性能，验证了跨模态/领域推理的可行性。

Abstract: Recent proprietary models (e.g., o3) have begun to demonstrate strong
multimodal reasoning capabilities. Yet, most existing open-source research
concentrates on training text-only reasoning models, with evaluations limited
to mainly mathematical and general-domain tasks. Therefore, it remains unclear
how to effectively extend reasoning capabilities beyond text input and general
domains. This paper explores a fundamental research question: Is reasoning
generalizable across modalities and domains? Our findings support an
affirmative answer: General-domain text-based post-training can enable such
strong generalizable reasoning. Leveraging this finding, we introduce
X-Reasoner, a vision-language model post-trained solely on general-domain text
for generalizable reasoning, using a two-stage approach: an initial supervised
fine-tuning phase with distilled long chain-of-thoughts, followed by
reinforcement learning with verifiable rewards. Experiments show that
X-Reasoner successfully transfers reasoning capabilities to both multimodal and
out-of-domain settings, outperforming existing state-of-the-art models trained
with in-domain and multimodal data across various general and medical
benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in
specialized domains can be further enhanced through continued training on
domain-specific text-only data. Building upon this, we introduce
X-Reasoner-Med, a medical-specialized variant that achieves new state of the
art on numerous text-only and multimodal medical benchmarks.

</details>


### [6] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
*Darren Yow-Bang Wang,Zhengyuan Shen,Soumya Smruti Mishra,Zhichao Xu,Yifei Teng,Haibo Ding*

Main category: cs.CL

TL;DR: 论文提出了SLOT（Structured LLM Output Transformer），一种模型无关的方法，用于将无结构的LLM输出转换为精确的结构化格式。SLOT通过微调轻量级语言模型作为后处理层，实现了跨多种LLM和模式规范的灵活性。实验证明，SLOT在模式准确性和内容保真度上显著优于现有方法，尤其是轻量级模型的表现甚至超过了大型专有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在关键应用（如代理和信息提取）中表现强大，但其输出常常偏离预定义的模式，这严重影响了可靠的应用开发。为了解决这一问题，论文提出了SLOT方法，旨在提供一种灵活且高效的解决方案，确保LLM输出的结构化准确性。

Method: SLOT采用了一种模型无关的方法，通过微调轻量级语言模型作为后处理层，将无结构的LLM输出转换为精确的结构化格式。论文还提出了一个系统化的数据整理和合成流程，以及一个形式化的评估方法，用于量化模式准确性和内容保真度。

Result: 实验结果显示，采用SLOT的微调Mistral-7B模型在模式准确性和内容相似性上分别达到了99.5%和94.0%，显著优于Claude-3.5-Sonnet（分别超出25和20个百分点）。此外，轻量级模型（如Llama-3.2-1B）在配备SLOT后也能匹配或超越大型专有模型的结构化输出能力。

Conclusion: SLOT为LLM的结构化输出提供了一种高效且灵活的解决方案，显著提升了模式准确性和内容保真度。该方法特别适用于资源受限的环境，证明了轻量级模型在配备适当后处理技术后也能实现高性能结构化输出。

Abstract: Structured outputs are essential for large language models (LLMs) in critical
applications like agents and information extraction. Despite their
capabilities, LLMs often generate outputs that deviate from predefined schemas,
significantly hampering reliable application development. We present SLOT
(Structured LLM Output Transformer), a model-agnostic approach that transforms
unstructured LLM outputs into precise structured formats. While existing
solutions predominantly rely on constrained decoding techniques or are tightly
coupled with specific models, SLOT employs a fine-tuned lightweight language
model as a post-processing layer, achieving flexibility across various LLMs and
schema specifications. We introduce a systematic pipeline for data curation and
synthesis alongside a formal evaluation methodology that quantifies both schema
accuracy and content fidelity. Our results demonstrate that fine-tuned
Mistral-7B model with constrained decoding achieves near perfect schema
accuracy (99.5%) and content similarity (94.0%), outperforming
Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points,
respectively). Notably, even compact models like Llama-3.2-1B can match or
exceed the structured output capabilities of much larger proprietary models
when equipped with SLOT, enabling reliable structured generation in
resource-constrained environments.

</details>


### [7] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
*Xu Huang,Yuefeng Huang,Weiwen Liu,Xingshan Zeng,Yasheng Wang,Ruiming Tang,Hong Xie,Defu Lian*

Main category: cs.CL

TL;DR: 该论文提出了个性化工具调用的概念，定义了两个关键任务：工具偏好和依赖配置的查询，并提出了PTool数据合成框架和首个个性化工具调用基准PTBench。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大型语言模型（LLMs）调用工具解决问题的基本能力，而忽略了工具调用中的个性化约束。

Method: 提出PTool数据合成框架，用于个性化工具调用，并构建PTBench基准。对多种开源模型进行微调以验证框架效果。

Result: 展示了PTool框架的有效性，并提供了有价值的见解。PTBench基准已公开。

Conclusion: 论文为个性化工具调用提供了新的研究方向和实践工具，推动了LLMs在复杂问题中的应用。

Abstract: Tool invocation is a crucial mechanism for extending the capabilities of
Large Language Models (LLMs) and has recently garnered significant attention.
It enables LLMs to solve complex problems through tool calls while accessing
up-to-date world knowledge. However, existing work primarily focuses on the
fundamental ability of LLMs to invoke tools for problem-solving, without
considering personalized constraints in tool invocation. In this work, we
introduce the concept of Personalized Tool Invocation and define two key tasks:
Tool Preference and Profile-dependent Query. Tool Preference addresses user
preferences when selecting among functionally similar tools, while
Profile-dependent Query considers cases where a user query lacks certain tool
parameters, requiring the model to infer them from the user profile. To tackle
these challenges, we propose PTool, a data synthesis framework designed for
personalized tool invocation. Additionally, we construct \textbf{PTBench}, the
first benchmark for evaluating personalized tool invocation. We then fine-tune
various open-source models, demonstrating the effectiveness of our framework
and providing valuable insights. Our benchmark is public at
https://github.com/hyfshadow/PTBench.

</details>


### [8] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
*Mengxian Lyu,Xiaohan Li,Ziyi Chen,Jinqian Pan,Cheng Peng,Sankalp Talankar,Yonghui Wu*

Main category: cs.CL

TL;DR: 该论文系统综述了自然语言生成（NLG）在医疗领域的应用，涵盖了数据模态、模型架构、临床应用及评估方法，旨在为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的突破，NLG在医疗领域的应用潜力巨大，但缺乏全面的综述。本研究旨在填补这一空白。

Method: 通过PRISMA指南，系统地筛选和分析了113篇科学论文，重点关注数据模态、模型架构、临床应用和评估方法。

Result: 总结了NLG的关键技术、实际医疗应用及其能力、局限性和新兴挑战。

Conclusion: 该综述为未来利用NLG推动医学发现和医疗保健转型提供了有价值的见解。

Abstract: Natural language generation (NLG) is the key technology to achieve generative
artificial intelligence (AI). With the breakthroughs in large language models
(LLMs), NLG has been widely used in various medical applications, demonstrating
the potential to enhance clinical workflows, support clinical decision-making,
and improve clinical documentation. Heterogeneous and diverse medical data
modalities, such as medical text, images, and knowledge bases, are utilized in
NLG. Researchers have proposed many generative models and applied them in a
number of healthcare applications. There is a need for a comprehensive review
of NLG methods and applications in the medical domain. In this study, we
systematically reviewed 113 scientific publications from a total of 3,988
NLG-related articles identified using a literature search, focusing on data
modality, model architecture, clinical applications, and evaluation methods.
Following PRISMA (Preferred Reporting Items for Systematic reviews and
Meta-Analyses) guidelines, we categorize key methods, identify clinical
applications, and assess their capabilities, limitations, and emerging
challenges. This timely review covers the key NLG technologies and medical
applications and provides valuable insights for future studies to leverage NLG
to transform medical discovery and healthcare.

</details>


### [9] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
*Mingruo Yuan,Ben Kao,Tien-Hsuan Wu,Michael M. K. Cheung,Henry W. H. Chan,Anne S. Y. Cheung,Felix W. H. Chan,Yongxi Chen*

Main category: cs.CL

TL;DR: 论文提出了一种三步法，将复杂的法律文件转化为易于公众理解的知识。重点展示了如何利用GPT-3等模型自动生成法律问题库，对比了机器生成问题和人工问题的优劣，并展示了原型系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决法律文件对非专业人士难以理解和导航的问题，提升法律信息的可及性。

Method: 采用三步法：1) 将法律条文转化为易于理解的片段；2) 构建法律问题库（LQB）；3) 设计交互式推荐系统（CRec）。利用GPT-3生成法律问题，并对比机器生成与人工编写问题。

Result: 机器生成的问题更具可扩展性和多样性，但人工问题更精确。原型系统成功推荐了相关法律知识。

Conclusion: 结合机器生成和人工问题的优势，可以有效提升法律知识的可及性和理解性。

Abstract: Access to legal information is fundamental to access to justice. Yet
accessibility refers not only to making legal documents available to the
public, but also rendering legal information comprehensible to them. A vexing
problem in bringing legal information to the public is how to turn formal legal
documents such as legislation and judgments, which are often highly technical,
to easily navigable and comprehensible knowledge to those without legal
education. In this study, we formulate a three-step approach for bringing legal
knowledge to laypersons, tackling the issues of navigability and
comprehensibility. First, we translate selected sections of the law into
snippets (called CLIC-pages), each being a small piece of article that focuses
on explaining certain technical legal concept in layperson's terms. Second, we
construct a Legal Question Bank (LQB), which is a collection of legal questions
whose answers can be found in the CLIC-pages. Third, we design an interactive
CLIC Recommender (CRec). Given a user's verbal description of a legal situation
that requires a legal solution, CRec interprets the user's input and shortlists
questions from the question bank that are most likely relevant to the given
legal situation and recommends their corresponding CLIC pages where relevant
legal knowledge can be found. In this paper we focus on the technical aspects
of creating an LQB. We show how large-scale pre-trained language models, such
as GPT-3, can be used to generate legal questions. We compare machine-generated
questions (MGQs) against human-composed questions (HCQs) and find that MGQs are
more scalable, cost-effective, and more diversified, while HCQs are more
precise. We also show a prototype of CRec and illustrate through an example how
our 3-step approach effectively brings relevant legal knowledge to the public.

</details>


### [10] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
*Vihaan Miriyala,Smrithi Bukkapatnam,Lavanya Prahallad*

Main category: cs.CL

TL;DR: 使用Chain-of-Thought (CoT)提示方法显著提升了细粒度情感分类的准确率，从84%提升至93%。


<details>
  <summary>Details</summary>
Motivation: 传统的数值和极性评分无法充分捕捉用户反馈中的细腻情感，因此需要更精确的分类方法。

Method: 通过对比简单提示与CoT提示在2000条亚马逊应用评论上的表现，评估每种方法对人类判断的预测准确性。

Result: CoT提示将分类准确率从84%提高到93%，证明了显式推理在情感分析中的有效性。

Conclusion: CoT提示能显著增强情感分析性能，特别适用于需要理解复杂情感的文本分类任务。

Abstract: We explore the use of Chain-of-Thought (CoT) prompting with large language
models (LLMs) to improve the accuracy of granular sentiment categorization in
app store reviews. Traditional numeric and polarity-based ratings often fail to
capture the nuanced sentiment embedded in user feedback. We evaluated the
effectiveness of CoT prompting versus simple prompting on 2000 Amazon app
reviews by comparing each method's predictions to human judgements. CoT
prompting improved classification accuracy from 84% to 93% highlighting the
benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>


### [11] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
*Variath Madhupal Gautham Nair,Vishal Varma Dantuluri*

Main category: cs.CL

TL;DR: 论文介绍了UTC Benchmark (UTCB)，一个动态可扩展的基准数据集，用于评估大语言模型（LLM）在图像生成任务中的安全性漏洞。通过多语言模糊处理和结构化提示工程，作者展示了现有LLM在生成内容时易受短自然提示攻击的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在图像生成任务中表现优异，但其安全性检查仍然脆弱，容易被简单的自然提示绕过，导致生成不当内容。需要一种动态、可扩展的基准来评估和解决这一问题。

Method: 作者提出了UTCB数据集，结合结构化提示工程、多语言模糊处理（如祖鲁语、盖尔语、Base64编码），并使用Groq-hosted LLaMA-3进行评估。支持零样本和回退提示策略、风险评分及自动化标记。

Result: UTCB能够有效评估LLM的脆弱性，并提出了分层的验证机制（青铜、白银、黄金层级），同时支持动态更新以适应新的数据源和模型行为。

Conclusion: UTCB为LLM安全性评估提供了一种标准化方法，揭示了现有模型的漏洞，并为未来安全改进奠定了基础。

Abstract: Existing large language models (LLMs) are advancing rapidly and produce
outstanding results in image generation tasks, yet their content safety checks
remain vulnerable to prompt-based jailbreaks. Through preliminary testing on
platforms such as ChatGPT, MetaAI, and Grok, we observed that even short,
natural prompts could lead to the generation of compromising images ranging
from realistic depictions of forged documents to manipulated images of public
figures.
  We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and
scalable benchmark dataset to evaluate LLM vulnerability in image generation.
Our methodology combines structured prompt engineering, multilingual
obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted
LLaMA-3. The pipeline supports both zero-shot and fallback prompting
strategies, risk scoring, and automated tagging. All generations are stored
with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided
verification), and Gold (manually verified) tiers. UTCB is designed to evolve
over time with new data sources, prompt templates, and model behaviors.
  Warning: This paper includes visual examples of adversarial inputs designed
to test model safety. All outputs have been redacted to ensure responsible
disclosure.

</details>


### [12] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
*Manas Satish Bedmutha,Feng Chen,Andrea Hartzler,Trevor Cohen,Nadir Weibel*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在临床对话中自动分析和提取社交信号的能力，展示了首个能追踪20种社交信号的系统，并分析了模型行为和优化方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLM在临床沟通中的潜力，尤其是通过文本分析捕捉非语言社交信号（如医患关系质量）的能力，以提升沟通效果和医疗结果。

Method: 方法包括设计任务专用提示，并在标注数据集上评估多种LLM架构和提示风格，数据集涵盖20种社交信号（如医生主导性、患者亲和力等）。

Result: 结果显示，系统首次实现了对20种社交信号的追踪，并揭示了LLM行为模式及临床上下文对性能的影响。

Conclusion: 结论指出，通过优化模型配置和临床上下文理解，可进一步提升LLM在医疗社交信号处理中的表现，为自动化临床沟通分析提供基础。

Abstract: Effective communication between providers and their patients influences
health and care outcomes. The effectiveness of such conversations has been
linked not only to the exchange of clinical information, but also to a range of
interpersonal behaviors; commonly referred to as social signals, which are
often conveyed through non-verbal cues and shape the quality of the
patient-provider relationship. Recent advances in large language models (LLMs)
have demonstrated an increasing ability to infer emotional and social behaviors
even when analyzing only textual information. As automation increases also in
clinical settings, such as for transcription of patient-provider conversations,
there is growing potential for LLMs to automatically analyze and extract social
behaviors from these interactions. To explore the foundational capabilities of
LLMs in tracking social signals in clinical dialogue, we designed task-specific
prompts and evaluated model performance across multiple architectures and
prompting styles using a highly imbalanced, annotated dataset spanning 20
distinct social signals such as provider dominance, patient warmth, etc. We
present the first system capable of tracking all these 20 coded signals, and
uncover patterns in LLM behavior. Further analysis of model configurations and
clinical context provides insights for enhancing LLM performance on social
signal processing tasks in healthcare settings.

</details>


### [13] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
*Maria Marina,Nikolay Ivanov,Sergey Pletenev,Mikhail Salnikov,Daria Galimzianova,Nikita Krayko,Vasily Konovalov,Alexander Panchenko,Viktor Moskvoretskii*

Main category: cs.CL

TL;DR: 该研究提出一种轻量级且不依赖大语言模型的适应性检索方法，解决了现有基于LLM的方法效率低下和实用性不足的问题，并在多个QA数据集上验证了其性能与效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型(LLMs)的幻觉问题，同时减少检索增强生成(RAG)的计算成本与错误信息风险，现有基于LLM的自适应检索方法效率低下且不实用。

Method: 引入基于外部信息的轻量级LLM无关自适应检索方法，研究27个特征（分7组）及其混合组合，并在6个QA数据集上评估性能与效率。

Result: 该方法在性能上与复杂LLM方法相当，但效率显著提升，验证了外部信息在自适应检索中的潜力。

Conclusion: 研究表明，基于外部信息的轻量级自适应检索方法既高效又实用，为减少LLM幻觉和计算成本提供了可行方案。

Abstract: Large Language Models~(LLMs) are prone to hallucinations, and
Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high
computational cost while risking misinformation. Adaptive retrieval aims to
retrieve only when necessary, but existing approaches rely on LLM-based
uncertainty estimation, which remain inefficient and impractical. In this
study, we introduce lightweight LLM-independent adaptive retrieval methods
based on external information. We investigated 27 features, organized into 7
groups, and their hybrid combinations. We evaluated these methods on 6 QA
datasets, assessing the QA performance and efficiency. The results show that
our approach matches the performance of complex LLM-based methods while
achieving significant efficiency gains, demonstrating the potential of external
information for adaptive retrieval.

</details>


### [14] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
*Sofia Jamil,Aryan Dabad,Bollampalli Areen Reddy,Sriparna Saha,Rajiv Misra,Adil A. Shakur*

Main category: cs.CL

TL;DR: 这篇论文介绍了针对癌症治疗的药物不良事件（ADEs）分组摘要任务，提出了MCADRS数据集和GASCADE框架，结合LLM的信息提取能力和T5模型的摘要能力，显著提升了药物相关决策效率。


<details>
  <summary>Details</summary>
Motivation: 癌症药物监测领域缺乏针对性的数据集和研究，现有的工作主要集中在普通疾病上。作者希望通过分组摘要任务和新型框架填补这一空白。

Method: 作者构建了MCADRS数据集，并提出GASCADE框架，结合LLMs的信息提取和T5的摘要能力，首次将对齐技术（如Direct Preference Optimization）应用于编码器-解码器模型。

Result: 实验证明，GASCADE在多项指标上表现优异，并通过自动评估和人工验证。该多任务方法提升了药物决策效率和对患者问题的理解。

Conclusion: GASCADE框架为个性化癌症护理提供了新方法，数据集和代码已开源，推动了该领域的进步。

Abstract: In the realm of cancer treatment, summarizing adverse drug events (ADEs)
reported by patients using prescribed drugs is crucial for enhancing
pharmacovigilance practices and improving drug-related decision-making. While
the volume and complexity of pharmacovigilance data have increased, existing
research in this field has predominantly focused on general diseases rather
than specifically addressing cancer. This work introduces the task of grouped
summarization of adverse drug events reported by multiple patients using the
same drug for cancer treatment. To address the challenge of limited resources
in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug
Reaction and Summarization (MCADRS) dataset. This dataset includes
pharmacovigilance posts detailing patient concerns regarding drug efficacy and
adverse effects, along with extracted labels for drug names, adverse drug
events, severity, and adversity of reactions, as well as summaries of ADEs for
each drug. Additionally, we propose the Grouping and Abstractive Summarization
of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that
combines the information extraction capabilities of Large Language Models
(LLMs) with the summarization power of the encoder-decoder T5 model. Our work
is the first to apply alignment techniques, including advanced algorithms like
Direct Preference Optimization, to encoder-decoder models using synthetic
datasets for summarization tasks. Through extensive experiments, we demonstrate
the superior performance of GASCADE across various metrics, validated through
both automated assessments and human evaluations. This multitasking approach
enhances drug-related decision-making and fosters a deeper understanding of
patient concerns, paving the way for advancements in personalized and
responsive cancer care. The code and dataset used in this work are publicly
available.

</details>


### [15] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
*Dario Garcia-Gasulla,Jordi Bayarri-Planas,Ashwin Kumar Gururajan,Enrique Lopez-Cuena,Adrian Tormos,Daniel Hinjos,Pablo Bernabeu-Perez,Anna Arias-Duart,Pablo Agustin Martin-Torres,Marta Gonzalez-Mallo,Sergio Alvarez-Napagao,Eduard Ayguadé-Parra,Ulises Cortés*

Main category: cs.CL

TL;DR: 该论文介绍了开源医疗大模型Aloe Beta的开发，通过优化数据预处理和训练，结合DPO和RAG提升模型安全性和效能，并制定了新的评估标准。模型表现优异并开放许可。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在医疗领域的进步，开发开源模型以保护公共利益的需求日益迫切。论文旨在推动开源医疗LLM的发展，优化数据处理和训练流程，并提升模型的安全性和效能。

Method: 基于Llama 3.1和Qwen 2.5等强大基础模型，Aloe Beta通过合成链式思考示例增强公共数据集，并采用直接偏好优化（DPO）进行模型对齐，确保伦理和政策兼容性。评估包括封闭式、开放式、安全性和人工测试。

Result: Aloe家族模型在医疗基准测试中表现优异，医疗专业人员更倾向使用。在偏见和毒性方面显著提升安全性，对未知越狱攻击展现强韧性。发布时附带了详细的风险评估。

Conclusion: Aloe Beta及其开发方法为开源医疗LLM领域做出重要贡献，在保持高性能的同时满足伦理要求，为医疗领域LLM的开发与报告设定了新标准。

Abstract: Purpose: With advancements in Large Language Models (LLMs) for healthcare,
the need arises for competitive open-source models to protect the public
interest. This work contributes to the field of open medical LLMs by optimizing
key stages of data preprocessing and training, while showing how to improve
model safety (through DPO) and efficacy (through RAG). The evaluation
methodology used, which includes four different types of tests, defines a new
standard for the field. The resultant models, shown to be competitive with the
best private alternatives, are released with a permisive license.
  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,
Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of
Thought examples. The models undergo alignment with Direct Preference
Optimization, emphasizing ethical and policy-aligned performance in the
presence of jailbreaking attacks. Evaluation includes close-ended, open-ended,
safety and human assessments, to maximize the reliability of results.
  Results: Recommendations are made across the entire pipeline, backed by the
solid performance of the Aloe Family. These models deliver competitive
performance across healthcare benchmarks and medical fields, and are often
preferred by healthcare professionals. On bias and toxicity, the Aloe Beta
models significantly improve safety, showing resilience to unseen jailbreaking
attacks. For a responsible release, a detailed risk assessment specific to
healthcare is attached to the Aloe Family models.
  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a
significant contribution to the open-source medical LLM field, offering
top-of-the-line performance while maintaining high ethical requirements. This
work sets a new standard for developing and reporting aligned LLMs in
healthcare.

</details>


### [16] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
*David Exler,Mark Schutera,Markus Reischl,Luca Rettenberger*

Main category: cs.CL

TL;DR: 该研究量化了流行大语言模型（LLMs）在德国联邦议院投票背景下的政治偏见，发现这些模型普遍存在左倾偏见，且其政治立场受语言、来源和发布时间影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，其潜在偏见可能误导用户并影响公众意见。本研究旨在量化这种偏见并探讨其影响因素，以促进模型的负责任使用。

Method: 使用Wahl-O-Mat评分系统比较各模型与德国政党的政治立场契合度，分析模型规模、语言、来源和发布时间对偏见的影响。

Result: 研究发现大语言模型普遍左倾，且语言和模型特征（如规模和发布时间）显著影响其政治立场。

Conclusion: LLMs易表现出政治偏见，开发这些模型的大公司有责任控制这些偏见，以避免对公众意见和决策过程的大规模影响。

Abstract: With the increasing prevalence of artificial intelligence, careful evaluation
of inherent biases needs to be conducted to form the basis for alleviating the
effects these predispositions can have on users. Large language models (LLMs)
are predominantly used by many as a primary source of information for various
topics. LLMs frequently make factual errors, fabricate data (hallucinations),
or present biases, exposing users to misinformation and influencing opinions.
Educating users on their risks is key to responsible use, as bias, unlike
hallucinations, cannot be caught through data verification. We quantify the
political bias of popular LLMs in the context of the recent vote of the German
Bundestag using the score produced by the Wahl-O-Mat. This metric measures the
alignment between an individual's political views and the positions of German
political parties. We compare the models' alignment scores to identify factors
influencing their political preferences. Doing so, we discover a bias toward
left-leaning parties, most dominant in larger LLMs. Also, we find that the
language we use to communicate with the models affects their political views.
Additionally, we analyze the influence of a model's origin and release date and
compare the results to the outcome of the recent vote of the Bundestag. Our
results imply that LLMs are prone to exhibiting political bias. Large
corporations with the necessary means to develop LLMs, thus, knowingly or
unknowingly, have a responsibility to contain these biases, as they can
influence each voter's decision-making process and inform public opinion in
general and at scale.

</details>


### [17] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
*Aidar Valeev,Roman Garaev,Vadim Lomshakov,Irina Piontkovskaya,Vladimir Ivanov,Israel Adewuyi*

Main category: cs.CL

TL;DR: 这篇论文提出了一个针对大型代码库（高达2000K行代码）的长上下文代码生成基准（YABLoCo），专注于C和C++语言的函数体生成，填补了现有基准仅覆盖中小规模代码的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要测试小中型代码上下文（数千行代码），而实际软件项目可能包含数百万行代码。本文旨在填补这一差距，为大型代码库的代码生成能力评估提供工具和数据支持。

Method: 论文构建了YABLoCo基准，包含来自四个大型仓库的215个函数，提供函数元数据、不同依赖级别的上下文、文档字符串、函数体及调用图。方法还包括高效计算指标的可扩展评估流程和生成代码的可视化分析工具。

Result: YABLoCo基准覆盖了C和C++语言，支持对200K至2000K行代码库的代码生成评估，并提供了评估流程和可视化工具。

Conclusion: YABLoCo基准为评估大型代码库中的代码生成能力提供了全面支持，尤其在C和C++语言上填补了现有研究的空白。

Abstract: Large Language Models demonstrate the ability to solve various programming
tasks, including code generation. Typically, the performance of LLMs is
measured on benchmarks with small or medium-sized context windows of thousands
of lines of code. At the same time, in real-world software projects,
repositories can span up to millions of LoC. This paper closes this gap by
contributing to the long context code generation benchmark (YABLoCo). The
benchmark featured a test set of 215 functions selected from four large
repositories with thousands of functions. The dataset contained metadata of
functions, contexts of the functions with different levels of dependencies,
docstrings, functions bodies, and call graphs for each repository. This paper
presents three key aspects of the contribution. First, the benchmark aims at
function body generation in large repositories in C and C++, two languages not
covered by previous benchmarks. Second, the benchmark contains large
repositories from 200K to 2,000K LoC. Third, we contribute a scalable
evaluation pipeline for efficient computing of the target metrics and a tool
for visual analysis of generated code. Overall, these three aspects allow for
evaluating code generation in large repositories in C and C++.

</details>


### [18] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
*Xiaoyu Xu,Minxin Du,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: OBLIVIATE框架通过结构化流程和定制损失函数，有效删除大型语言模型中的敏感数据，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型可能记忆敏感、受版权保护或有毒内容的问题。

Method: 采用目标令牌提取、保留集构建和包含掩码、蒸馏及世界事实的三部分定制损失函数的微调流程，结合低秩适配器（LoRA）提高效率。

Result: 实验表明OBLIVIATE在抵抗成员推理攻击、减少对保留数据影响及保持多场景鲁棒性方面表现优异。

Conclusion: OBLIVIATE为大型语言模型提供了高效且可靠的去学习方法。

Abstract: Large language models (LLMs) trained over extensive corpora risk memorizing
sensitive, copyrighted, or toxic content. To address this, we propose
OBLIVIATE, a robust unlearning framework that removes targeted data while
preserving model utility. The framework follows a structured process:
extracting target tokens, building retain sets, and fine-tuning with a tailored
loss function comprising three components -- masking, distillation, and world
fact. Using low-rank adapters (LoRA), it ensures efficiency without
compromising unlearning quality. We conduct experiments on multiple datasets,
including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite
of metrics: forget quality (new document-level memorization score), model
utility, and fluency. Results demonstrate its effectiveness in resisting
membership inference attacks, minimizing the impact on retained data, and
maintaining robustness across diverse scenarios.

</details>


### [19] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
*Ilya Koziev*

Main category: cs.CL

TL;DR: 本文探讨了自然语言文本质量对生成模型（特别是诗歌或歌词生成等创造性任务）性能的影响，提出了利用自动化语言异常检测来筛选低质量训练数据的方法。


<details>
  <summary>Details</summary>
Motivation: 研究发现，训练文本的质量直接影响到生成诗歌的流畅性，而现有数据往往缺乏严格的质量控制，因此需要有效的方法来识别和过滤低质量问题。

Method: 本文对比了无监督和有监督的文本异常检测方法，并使用了合成数据集和人工标注数据集（如RUPOR数据集）。同时提供了完整的评估代码。

Result: 通过实验，验证了自动化异常检测方法在提升训练数据质量方面的有效性，并开源了RUPOR数据集和评估工具。

Conclusion: 本文为社区提供了改善生成模型训练数据质量的工具和见解，尤其在创造性领域中具有重要意义。

Abstract: The quality of natural language texts in fine-tuning datasets plays a
critical role in the performance of generative models, particularly in
computational creativity tasks such as poem or song lyric generation. Fluency
defects in generated poems significantly reduce their value. However, training
texts are often sourced from internet-based platforms without stringent quality
control, posing a challenge for data engineers to manage defect levels
effectively.
  To address this issue, we propose the use of automated linguistic anomaly
detection to identify and filter out low-quality texts from training datasets
for creative models. In this paper, we present a comprehensive comparison of
unsupervised and supervised text anomaly detection approaches, utilizing both
synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a
collection of Russian-language human-labeled poems designed for cross-sentence
grammatical error detection, and provide the full evaluation code. Our work
aims to empower the community with tools and insights to improve the quality of
training datasets for generative models in creative domains.

</details>


### [20] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
*Yehui Tang,Yichun Yin,Yaoyuan Wang,Hang Zhou,Yu Pan,Wei Guo,Ziyang Zhang,Miao Rang,Fangcheng Liu,Naifu Zhang,Binghan Li,Yonghan Dong,Xiaojun Meng,Yasheng Wang,Dong Li,Yin Li,Dandan Tu,Can Chen,Youliang Yan,Fisher Yu,Ruiming Tang,Yunhe Wang,Botian Huang,Bo Wang,Boxiao Liu,Changzheng Zhang,Da Kuang,Fei Liu,Gang Huang,Jiansheng Wei,Jiarui Qin,Jie Ran,Jinpeng Li,Jun Zhao,Liang Dai,Lin Li,Liqun Deng,Peifeng Qin,Pengyuan Zeng,Qiang Gu,Shaohua Tang,Shengjun Cheng,Tao Gao,Tao Yu,Tianshu Li,Tianyu Bi,Wei He,Weikai Mao,Wenyong Huang,Wulong Liu,Xiabing Li,Xianzhi Yu,Xueyu Wu,Xu He,Yangkai Du,Yan Xu,Ye Tian,Yimeng Wu,Yongbing Huang,Yong Tian,Yong Zhu,Yue Li,Yufei Wang,Yuhang Gai,Yujun Li,Yu Luo,Yunsheng Ni,Yusen Sun,Zelin Chen,Zhe Liu,Zhicheng Liu,Zhipeng Tu,Zilin Ding,Zongyuan Zhan*

Main category: cs.CL

TL;DR: 该论文提出了一种在Ascend NPUs上高效训练稀疏大语言模型（LLMs）的方法，通过模拟选择模型配置并优化系统通信和内存管理，最终在6K Ascend NPUs上实现了30.0%的MFU性能，训练了7180亿参数的Pangu Ultra MoE模型。


<details>
  <summary>Details</summary>
Motivation: 稀疏大语言模型（如MoE结构）的规模巨大，给软硬件系统带来挑战。论文旨在探索如何在Ascend NPUs上高效利用这种规模，优化计算资源使用并实现预期性能增益。

Method: 通过模拟比较模型超参数的权衡，选择适合Ascend NPUs的配置；优化专家并行（Expert Parallelism）以减少设备间通信开销；提升设备内内存效率以降低参数和激活管理成本。

Result: 训练了7180亿参数的Pangu Ultra MoE模型，在6K Ascend NPUs上实现了30.0%的MFU性能，性能与DeepSeek R1相当，验证了Ascend系统高效训练稀疏LLMs的能力。

Conclusion: 论文提出的方法能高效训练大规模稀疏语言模型，并为未来研究提供了参考。

Abstract: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close
to a trillion parameters are dominating the realm of most capable language
models. However, the massive model scale poses significant challenges for the
underlying software and hardware systems. In this paper, we aim to uncover a
recipe to harness such scale on Ascend NPUs. The key goals are better usage of
the computing resources under the dynamic sparse model structures and
materializing the expected performance gain on the actual hardware. To select
model configurations suitable for Ascend NPUs without repeatedly running the
expensive experiments, we leverage simulation to compare the trade-off of
various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM
with 718 billion parameters, and we conducted experiments on the model to
verify the simulation results. On the system side, we dig into Expert
Parallelism to optimize the communication between NPU devices to reduce the
synchronization overhead. We also optimize the memory efficiency within the
devices to further reduce the parameter and activation management overhead. In
the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with
performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and
demonstrate that the Ascend system is capable of harnessing all the training
stages of the state-of-the-art language models. Extensive experiments indicate
that our recipe can lead to efficient training of large-scale sparse language
models with MoE. We also study the behaviors of such models for future
reference.

</details>


### [21] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文系统地综述了针对低资源语言生成模型的稀缺数据应对策略，总结了54项研究中的技术方法、架构选择、语言覆盖和评估趋势。


<details>
  <summary>Details</summary>
Motivation: 生成语言模型如ChatGPT和谷歌Gemini虽在英语等高资源语言中表现突出，但低资源语言（LRL）的数据稀缺问题加剧了NLP领域的语言不平等。

Method: 通过分析54项研究，分类评估了单语数据增强、回译、多语言训练和提示工程等技术方法在生成任务中的应用。

Result: 研究发现广泛依赖Transformer架构、LRL覆盖有限且评估标准不一致，建议拓展技术应用范围并攻克公平性挑战。

Conclusion: 综述旨在支持开发包容性AI工具，以赋能LRL使用者并保护语言多样性，推动大规模语言技术时代的公平发展。

Abstract: Generative language modelling has surged in popularity with the emergence of
services such as ChatGPT and Google Gemini. While these models have
demonstrated transformative potential in productivity and communication, they
overwhelmingly cater to high-resource languages like English. This has
amplified concerns over linguistic inequality in natural language processing
(NLP). This paper presents the first systematic review focused specifically on
strategies to address data scarcity in generative language modelling for
low-resource languages (LRL). Drawing from 54 studies, we identify, categorise
and evaluate technical approaches, including monolingual data augmentation,
back-translation, multilingual training, and prompt engineering, across
generative tasks. We also analyse trends in architecture choices, language
family representation, and evaluation methods. Our findings highlight a strong
reliance on transformer-based models, a concentration on a small subset of
LRLs, and a lack of consistent evaluation across studies. We conclude with
recommendations for extending these methods to a wider range of LRLs and
outline open challenges in building equitable generative language systems.
Ultimately, this review aims to support researchers and developers in building
inclusive AI tools for underrepresented languages, a necessary step toward
empowering LRL speakers and the preservation of linguistic diversity in a world
increasingly shaped by large-scale language technologies.

</details>


### [22] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
*Hao Sun,Zile Qiao,Jiayan Guo,Xuanbo Fan,Yingyan Hou,Yong Jiang,Pengjun Xie,Fei Huang,Yan Zhang*

Main category: cs.CL

TL;DR: ZeroSearch是一种强化学习框架，无需与真实搜索引擎交互，即可提升大语言模型（LLM）的搜索能力，通过生成文档的渐进式质量退化训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的方法面临文档质量不可控和高API成本的问题，限制了模型的扩展性和稳定性。

Method: 采用轻量级监督微调将LLM转变为检索模块，并在强化学习训练中使用基于课程的策略逐步降低生成文档质量。

Result: 实验证明，ZeroSearch显著提升了LLM的搜索能力，甚至14B参数模型的表现超越了真实搜索引擎，且适用于不同规模的模型和多种RL算法。

Conclusion: ZeroSearch为LLM的搜索能力提供了一种高效、低成本的训练方案，具有广泛的适用性和可扩展性。

Abstract: Effective information searching is essential for enhancing the reasoning and
generation capabilities of large language models (LLMs). Recent research has
explored using reinforcement learning (RL) to improve LLMs' search capabilities
by interacting with live search engines in real-world environments. While these
approaches show promising results, they face two major challenges: (1)
Uncontrolled Document Quality: The quality of documents returned by search
engines is often unpredictable, introducing noise and instability into the
training process. (2) Prohibitively High API Costs: RL training requires
frequent rollouts, potentially involving hundreds of thousands of search
requests, which incur substantial API expenses and severely constrain
scalability. To address these challenges, we introduce ZeroSearch, a
reinforcement learning framework that incentivizes the search capabilities of
LLMs without interacting with real search engines. Our approach begins with
lightweight supervised fine-tuning to transform the LLM into a retrieval module
capable of generating both relevant and noisy documents in response to a query.
During RL training, we employ a curriculum-based rollout strategy that
incrementally degrades the quality of generated documents, progressively
eliciting the model's reasoning ability by exposing it to increasingly
challenging retrieval scenarios. Extensive experiments demonstrate that
ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B
LLM as the retrieval module. Remarkably, a 7B retrieval module achieves
comparable performance to the real search engine, while a 14B retrieval module
even surpasses it. Furthermore, it generalizes well across both base and
instruction-tuned models of various parameter sizes and is compatible with a
wide range of RL algorithms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation](https://arxiv.org/abs/2505.03774)
*Tao Yin,Chen Zhao,Xiaoyan Liu,Minglai Shao*

Main category: cs.LG

TL;DR: 提出了一种用于异构图中的OOD检测新方法OODHG，通过学习节点表示、计算能量值进行分类，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实图中的异构性和分布偏移导致OOD检测变得复杂且关键，而现有研究多集中于同构图，该方法填补了这一空白。

Method: 结合元路径的能量传播机制和能量约束，学习节点表示并计算能量值，先检测OOD节点再分类ID节点。

Result: 实验证明OODHG在OOD检测和ID节点分类任务中优于基线模型，且方法简单高效。

Conclusion: OODHG为异构图中的OOD检测提供了有效解决方案，具有实际应用潜力。

Abstract: Graph neural networks (GNNs) are proven effective in extracting complex node
and structural information from graph data. While current GNNs perform well in
node classification tasks within in-distribution (ID) settings, real-world
scenarios often present distribution shifts, leading to the presence of
out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and
challenging task. Most existing research focuses on homogeneous graphs, but
real-world graphs are often heterogeneous, consisting of diverse node and edge
types. This heterogeneity adds complexity and enriches the informational
content. To the best of our knowledge, OOD detection in heterogeneous graphs
remains an underexplored area. In this context, we propose a novel methodology
for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main
objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the
first task's results. Specifically, we learn representations for each node in
the heterogeneous graph, calculate energy values to determine whether nodes are
OOD, and then classify ID nodes. To leverage the structural information of
heterogeneous graphs, we introduce a meta-path-based energy propagation
mechanism and an energy constraint to enhance the distinction between ID and
OOD nodes. Extensive experimental findings substantiate the simplicity and
effectiveness of OODHG, demonstrating its superiority over baseline models in
OOD detection tasks and its accuracy in ID node classification.

</details>


### [24] [Hierarchical Multi-Label Generation with Probabilistic Level-Constraint](https://arxiv.org/abs/2505.03775)
*Linqing Chen,Weilei Wang,Wentao Wu,Hanmeng Zhong*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法，将分层多标签分类重新定义为分层多标签生成（HMG），并采用带概率级别约束（PLC）的生成框架，显著提升了模型输出控制的精确性。


<details>
  <summary>Details</summary>
Motivation: 传统分层多标签分类因标签的复杂层级关系和数量庞大而难以处理，且现有方法无法有效控制生成模型的输出。

Method: 通过重新定义任务为HMG，并引入PLC生成框架，直接生成符合层级关系的标签，避免了聚类等预处理步骤。

Result: 实验表明，该方法在HMG任务中达到新的SOTA性能，且在输出控制（数量、长度、层级）上优于先前研究。

Conclusion: 提出的生成框架解决了复杂标签层级和输出控制的问题，为分层多标签任务提供了更高效的解决方案。

Abstract: Hierarchical Extreme Multi-Label Classification poses greater difficulties
compared to traditional multi-label classification because of the intricate
hierarchical connections of labels within a domain-specific taxonomy and the
substantial number of labels. Some of the prior research endeavors centered on
classifying text through several ancillary stages such as the cluster algorithm
and multiphase classification. Others made attempts to leverage the assistance
of generative methods yet were unable to properly control the output of the
generative model. We redefine the task from hierarchical multi-Label
classification to Hierarchical Multi-Label Generation (HMG) and employ a
generative framework with Probabilistic Level Constraints (PLC) to generate
hierarchical labels within a specific taxonomy that have complex hierarchical
relationships. The approach we proposed in this paper enables the framework to
generate all relevant labels across levels for each document without relying on
preliminary operations like clustering. Meanwhile, it can control the model
output precisely in terms of count, length, and level aspects. Experiments
demonstrate that our approach not only achieves a new SOTA performance in the
HMG task, but also has a much better performance in constrained the output of
model than previous research work.

</details>


### [25] [PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction](https://arxiv.org/abs/2505.03776)
*Hansi Denis,Siegfried Mercelis,Ngoc-Quang Luong*

Main category: cs.LG

TL;DR: 该论文提出了一种名为PAPN的新型模型，结合近邻注意力机制和指针网络，优化了最后一公里配送和首公里取件的路线预测，显著优于现有监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 最后一公里配送和首公里取件的优化对物流效率和成本至关重要，但现有方法难以兼顾全局和局部信息。作者旨在通过混合注意力机制提升路线预测的准确性。

Method: 采用编码器-解码器架构，编码器使用多头注意力捕捉全局上下文，解码器通过近邻注意力机制和指针网络利用位置间关联性进行预测。

Result: 在真实数据集LaDE上验证，模型在多数指标上超越现有监督学习方法，并与最佳强化学习方法DRL4Route竞争力相当。

Conclusion: PAPN模型通过结合全局和局部注意力，显著提升了路线预测性能，为物流优化提供了有效解决方案。

Abstract: Optimization of the last-mile delivery and first-mile pickup of parcels is an
integral part of the broader logistics optimization pipeline as it entails both
cost and resource efficiency as well as a heightened service quality. Such
optimization requires accurate route and time prediction systems to adapt to
different scenarios in advance. This work tackles the first building block,
namely route prediction. This is done by introducing a novel Proximity
Attention mechanism in an encoder-decoder architecture utilizing a Pointer
Network in the decoding process (Proximity Attention Encoder and Pointer
Network decoder: PAPN) to leverage the underlying connections between the
different visitable pickup positions at each timestep. To this local attention
process is coupled global context computing via a multi-head attention
transformer encoder. The obtained global context is then mixed to an aggregated
version of the local embedding thus achieving a mix of global and local
attention for complete modeling of the problems. Proximity attention is also
used in the decoding process to skew predictions towards the locations with the
highest attention scores and thus using inter-connectivity of locations as a
base for next-location prediction. This method is trained, validated and tested
on a large industry-level dataset of real-world, large-scale last-mile delivery
and first-mile pickup named LaDE[1]. This approach shows noticeable promise,
outperforming all state-of-the-art supervised systems in terms of most metrics
used for benchmarking methods on this dataset while still being competitive
with the best-performing reinforcement learning method named DRL4Route[2].

</details>


### [26] [MolMole: Molecule Mining from Scientific Literature](https://arxiv.org/abs/2505.03777)
*LG AI Research,Sehyun Chun,Jiye Kim,Ahra Jo,Yeonsik Jo,Seungyul Oh,Seungjun Lee,Kwangrok Ryoo,Jongmin Lee,Seunghwan Kim,Byung Jun Kang,Soonyoung Lee,Jun Ha Park,Chanwoo Moon,Jiwon Ham,Haein Lee,Heejae Han,Jaeseung Byun,Soojong Do,Minju Ha,Dongyun Kim,Kyunghoon Bae,Woohyung Lim,Edward Hwayoung Lee,Yongmin Park,Jeongsang Yu,Gerrard Jeongwon Jo,Yeonjung Hong,Kyungjae Yoo,Sehui Han,Jaewan Lee,Changyoung Park,Kijeong Jeon,Sihyuk Yi*

Main category: cs.LG

TL;DR: MolMole是一个基于视觉的深度学习框架，用于从科学文档中自动化提取分子结构和反应数据，并通过统一检测、解析和识别流程显著优于现有工具包。


<details>
  <summary>Details</summary>
Motivation: 科学文档中化学数据的提取因格式多样且布局复杂而具有挑战性，现有方法缺乏统一标准和评估指标。

Method: MolMole结合了分子检测、反应图解析和光学化学结构识别，并引入了新的测试集和评估指标。

Result: 实验表明，MolMole在自建测试集和公共数据集上均优于现有工具包。

Conclusion: MolMole为化学数据提取提供了高效解决方案，测试集和工具包将公开以促进研究。

Abstract: The extraction of molecular structures and reaction data from scientific
documents is challenging due to their varied, unstructured chemical formats and
complex document layouts. To address this, we introduce MolMole, a vision-based
deep learning framework that unifies molecule detection, reaction diagram
parsing, and optical chemical structure recognition (OCSR) into a single
pipeline for automating the extraction of chemical data directly from
page-level documents. Recognizing the lack of a standard page-level benchmark
and evaluation metric, we also present a testset of 550 pages annotated with
molecule bounding boxes, reaction labels, and MOLfiles, along with a novel
evaluation metric. Experimental results demonstrate that MolMole outperforms
existing toolkits on both our benchmark and public datasets. The benchmark
testset will be publicly available, and the MolMole toolkit will be accessible
soon through an interactive demo on the LG AI Research website. For commercial
inquiries, please contact us at
\href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.

</details>


### [27] [Dragonfly: a modular deep reinforcement learning library](https://arxiv.org/abs/2505.03778)
*Jonathan Viquerat,Paul Garnier,Amirhossein Bateni,Elie Hachem*

Main category: cs.LG

TL;DR: Dragonfly 是一个模块化的深度强化学习库，支持通过 JSON 序列化交换组件和参数扫描，适用于 CPU 密集型任务，性能优于文献中的标准方法。


<details>
  <summary>Details</summary>
Motivation: 为了简化实验和开发，研究团队设计了一个模块化的深度强化学习库，尤其适合 CPU 密集型环境（如数值模拟）。

Method: 使用 JSON 序列化实现模块化设计，支持灵活更换组件和参数扫描，同时减少代码维护负担。

Result: 在标准代理测试中，Dragonfly 的性能优于文献中的现有方法。

Conclusion: Dragonfly 通过模块化和 JSON 序列化，为深度强化学习提供了高效且易维护的实验框架。

Abstract: Dragonfly is a deep reinforcement learning library focused on modularity, in
order to ease experimentation and developments. It relies on a json
serialization that allows to swap building blocks and perform parameter sweep,
while minimizing code maintenance. Some of its features are specifically
designed for CPU-intensive environments, such as numerical simulations. Its
performance on standard agents using common benchmarks compares favorably with
the literature.

</details>


### [28] [Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites](https://arxiv.org/abs/2505.03779)
*Tao Liu,Tianyu Zhang,Yongxue Chen,Weiming Wang,Yu Jiang,Yuming Huang,Charlie C. L. Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于神经网络的框架，用于同时优化纤维增强热塑性复合材料的结构拓扑、弯曲层和路径方向，以实现强各向异性强度并确保可制造性。


<details>
  <summary>Details</summary>
Motivation: 通过集成设计和制造目标，解决传统顺序优化方法在纤维增强复合材料中无法同时优化机械强度和可制造性的问题。

Method: 使用三个隐式神经场表示形状、层序列和纤维方向，将设计目标和制造约束作为可微损失函数整合到优化过程中。

Result: 实验表明，该方法生成的复合材料在破坏载荷上比顺序优化方法提高了33.1%。

Conclusion: 该框架实现了复合材料设计和制造的一体化优化，显著提升了性能并确保可制造性。

Abstract: We propose a neural network-based computational framework for the
simultaneous optimization of structural topology, curved layers, and path
orientations to achieve strong anisotropic strength in fiber-reinforced
thermoplastic composites while ensuring manufacturability. Our framework
employs three implicit neural fields to represent geometric shape, layer
sequence, and fiber orientation. This enables the direct formulation of both
design and manufacturability objectives - such as anisotropic strength,
structural volume, machine motion control, layer curvature, and layer thickness
- into an integrated and differentiable optimization process. By incorporating
these objectives as loss functions, the framework ensures that the resultant
composites exhibit optimized mechanical strength while remaining its
manufacturability for filament-based multi-axis 3D printing across diverse
hardware platforms. Physical experiments demonstrate that the composites
generated by our co-optimization method can achieve an improvement of up to
33.1% in failure loads compared to composites with sequentially optimized
structures and manufacturing sequences.

</details>


### [29] [ALFRED: Ask a Large-language model For Reliable ECG Diagnosis](https://arxiv.org/abs/2505.03781)
*Jin Yu,JaeHo Park,TaeJun Park,Gyurin Kim,JiHyun Lee,Min Sung Lee,Joon-myoung Kwon,Jeong Min Son,Yong-Yeon Jo*

Main category: cs.LG

TL;DR: 论文提出了一种基于RAG的零样本ECG诊断框架，结合专家知识提高准确性和可解释性，在PTB-XL数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗数据（如ECG）分析中生成可靠、循证结果的挑战，现有RAG方法在专业领域可能不足。

Method: 提出零样本ECG诊断框架，通过RAG结合专家知识库，增强诊断准确性和解释性。

Result: 在PTB-XL数据集上验证框架有效性，凸显结构化领域知识在自动化ECG解析中的价值。

Conclusion: 该框架支持全面ECG分析，满足多样化诊断需求，并具备跨数据集的潜在应用能力。

Abstract: Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation
(RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers
high accuracy and convenience. However, generating reliable, evidence-based
results in specialized fields like healthcare remains a challenge, as RAG alone
may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG
for ECG analysis that incorporates expert-curated knowledge to enhance
diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset
demonstrates the framework's effectiveness, highlighting the value of
structured domain expertise in automated ECG interpretation. Our framework is
designed to support comprehensive ECG analysis, addressing diverse diagnostic
needs with potential applications beyond the tested dataset.

</details>


### [30] [A general physics-constrained method for the modelling of equation's closure terms with sparse data](https://arxiv.org/abs/2505.03783)
*Tian Chen,Shengping Liu,Li Liu,Heng Yong*

Main category: cs.LG

TL;DR: 研究提出了一种新型的Series-Parallel多网络架构，结合物理信息神经网络（PINNs），用于在数据稀疏情况下构建通用的闭包模型。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀疏（不完全或稀缺），开发广泛适用的闭包模型具有挑战性。

Method: 采用Series-Parallel多网络架构，整合PINNs以嵌入物理约束和多源数据，并使用专用子网络独立建模未知闭包项。

Result: 该方法提升了闭包模型的泛化能力，并集成到精确的PDE求解器中，支持复杂工程预测模拟。

Conclusion: 研究提出的方法在数据稀疏情况下有效提升了闭包模型的普适性和准确性。

Abstract: Accurate modeling of closure terms is a critical challenge in engineering and
scientific research, particularly when data is sparse (scarse or incomplete),
making widely applicable models difficult to develop. This study proposes a
novel approach for constructing closure models in such challenging scenarios.
We introduce a Series-Parallel Multi-Network Architecture that integrates
Physics-Informed Neural Networks (PINNs) to incorporate physical constraints
and heterogeneous data from multiple initial and boundary conditions, while
employing dedicated subnetworks to independently model unknown closure terms,
enhancing generalizability across diverse problems. These closure models are
integrated into an accurate Partial Differential Equation (PDE) solver,
enabling robust solutions to complex predictive simulations in engineering
applications.

</details>


### [31] [Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers](https://arxiv.org/abs/2505.03784)
*Ahmed A. Metwally,A. Ali Heydari,Daniel McDuff,Alexandru Solot,Zeinab Esmaeilpour,Anthony Z Faranesh,Menglian Zhou,David B. Savage,Conor Heneghan,Shwetak Patel,Cathy Speed,Javier L. Prieto*

Main category: cs.LG

TL;DR: 通过结合可穿戴设备数据和血液生物标志物，开发深度学习模型预测胰岛素抵抗，效果优于单一数据源，为2型糖尿病早期干预提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 现有测量胰岛素抵抗的方法昂贵且不易获取，限制了早期干预机会。研究旨在开发一种基于可穿戴数据和血液生物标志物的预测模型，以提高早期检测的可及性。

Method: 研究招募了美国最大的数据集（N=1,165），结合可穿戴设备数据和血液生物标志物（如HOMA-IR），开发深度学习模型预测胰岛素抵抗。

Result: 模型结合两种数据源的预测效果优于单一数据源（R2=0.5，auROC=0.80），在肥胖和久坐人群中表现更佳（敏感性93%，特异性95%）。

Conclusion: 该模型为2型糖尿病早期检测提供了可行方案，结合大语言模型可帮助个性化推荐，推动预防策略实施。

Abstract: Insulin resistance, a precursor to type 2 diabetes, is characterized by
impaired insulin action in tissues. Current methods for measuring insulin
resistance, while effective, are expensive, inaccessible, not widely available
and hinder opportunities for early intervention. In this study, we remotely
recruited the largest dataset to date across the US to study insulin resistance
(N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%),
incorporating wearable device time series data and blood biomarkers, including
the ground-truth measure of insulin resistance, homeostatic model assessment
for insulin resistance (HOMA-IR). We developed deep neural network models to
predict insulin resistance based on readily available digital and blood
biomarkers. Our results show that our models can predict insulin resistance by
combining both wearable data and readily available blood biomarkers better than
either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%,
and specificity 84%). The model showed 93% sensitivity and 95% adjusted
specificity in obese and sedentary participants, a subpopulation most
vulnerable to developing type 2 diabetes and who could benefit most from early
intervention. Rigorous evaluation of model performance, including
interpretability, and robustness, facilitates generalizability across larger
cohorts, which is demonstrated by reproducing the prediction performance on an
independent validation cohort (N=72 participants). Additionally, we
demonstrated how the predicted insulin resistance can be integrated into a
large language model agent to help understand and contextualize HOMA-IR values,
facilitating interpretation and safe personalized recommendations. This work
offers the potential for early detection of people at risk of type 2 diabetes
and thereby facilitate earlier implementation of preventative strategies.

</details>


### [32] [mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging](https://arxiv.org/abs/2505.03785)
*Eleftherios Tzanis,Michail E. Klontzas*

Main category: cs.LG

TL;DR: mAIstro是一个基于LLM的开源自洽多代理框架，专为医疗AI模型的端到端开发和部署设计，支持自然语言交互无需编码。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI领域复杂工作流自动化的问题，提供无需编码的解决方案，促进临床和研究AI的整合。

Method: 采用模块化架构，支持开源和闭源LLM，通过自然语言接口协调数据分析、影像分割、分类和回归等任务。

Result: 在16个开源数据集上成功执行所有任务，生成可解释输出和验证模型。

Conclusion: mAIstro首次统一了医疗AI的数据分析、模型开发和推理，为临床和研究提供了可复现和可扩展的基础。

Abstract: Agentic systems built on large language models (LLMs) offer promising
capabilities for automating complex workflows in healthcare AI. We introduce
mAIstro, an open-source, autonomous multi-agentic framework for end-to-end
development and deployment of medical AI models. The system orchestrates
exploratory data analysis, radiomic feature extraction, image segmentation,
classification, and regression through a natural language interface, requiring
no coding from the user. Built on a modular architecture, mAIstro supports both
open- and closed-source LLMs, and was evaluated using a large and diverse set
of prompts across 16 open-source datasets, covering a wide range of imaging
modalities, anatomical regions, and data types. The agents successfully
executed all tasks, producing interpretable outputs and validated models. This
work presents the first agentic framework capable of unifying data analysis, AI
model development, and inference across varied healthcare applications,
offering a reproducible and extensible foundation for clinical and research AI
integration. The code is available at: https://github.com/eltzanis/mAIstro

</details>


### [33] [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
*Md Fahim Anjum*

Main category: cs.LG

TL;DR: 研究比较了具有推理能力的大型语言模型（DeepSeek-R1）与非推理模型在文本到SQL任务中的表现，发现推理模型作为判别器表现更优，但生成能力较弱。


<details>
  <summary>Details</summary>
Motivation: 探索具有推理能力的LLM在规划框架中作为候选评估工具的潜力，尤其是在文本到SQL任务中，与传统非推理模型的性能对比。

Method: 采用生成器-判别器框架，引入一种从推理的思维链（CoT）输出中提取软分数的新方法，用于细粒度排名候选。

Result: DeepSeek-R1-1.5B在F1分数和判别准确率上显著优于CodeLlama-7B和13B，但推理模型在生成任务上表现较差。

Conclusion: 推理模型在代理框架中作为判别器潜力巨大，但其生成能力有限，需进一步优化其角色分工。

Abstract: Large Language Models (LLM) with reasoning capabilities offer a promising
path for improving candidate evaluation in planning frameworks, but their
relative performance against traditional non-reasoning models remains largely
underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning
model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within
a generator-discriminator LLM planning framework for the text-to-SQL task. For
this, we introduce a novel method for extracting soft scores from the
chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking
of candidates. Our central hypothesis is that reasoning models are more
effective discriminators than non-reasoning LLMs. Our results show that
distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better
discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution
accuracy than CodeLlama-13B, despite having significantly fewer parameters.
Furthermore, we find that there is a limit to the logical capabilities of
reasoning models, and only providing more context or allowing more compute
budget for reasoning is not enough to improve their discrimination performance.
Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find
generation more challenging than discrimination and may underperform as
generators compared to smaller non-reasoning LLMs. Our work highlights the
potential of reasoning models as discriminators in agentic frameworks, far
outweighing their capabilities as generators, offering insights into their
optimal role within LLM planning infrastructures.

</details>


### [34] [ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification](https://arxiv.org/abs/2505.03787)
*Zuraiz Baig,Sidra Nasir,Rizwan Ahmed Khan,Muhammad Zeeshan Ul Haque*

Main category: cs.LG

TL;DR: 本文提出了两种轻量级1D卷积神经网络模型ArrhythmiNet V1和V2，用于实时心律失常分类，优化了边缘设备的计算效率，同时保持了高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 心律失常是危及生命的常见心脏问题，当前的心电图（ECG）手动分析耗时且依赖专家经验。现有深度学习模型忽视信号的时间与形态特征，计算复杂且缺乏可解释性。本文旨在设计高效、轻量且可解释的模型。

Method: 提出两种基于MobileNet深度可分离卷积的轻量级1D卷积神经网络（V1和V2），在MIT-BIH数据集上测试，并集成SHAP和Grad-CAM增强可解释性。

Result: 模型体积小（V1为302.18 KB，V2为157.76 KB），分类准确率高（V1为0.99，V2为0.98），且能通过可解释性技术提取QRS波群和T波等生理特征。

Conclusion: 研究表明，结合可解释性、高准确性和高效计算的模型可行，适用于可穿戴或嵌入式ECG监测系统，但数据多样性和泛化性仍需改进。

Abstract: Cardiac arrhythmias are a leading cause of life-threatening cardiac events,
highlighting the urgent need for accurate and timely detection.
Electrocardiography (ECG) remains the clinical gold standard for arrhythmia
diagnosis; however, manual interpretation is time-consuming, dependent on
clinical expertise, and prone to human error. Although deep learning has
advanced automated ECG analysis, many existing models abstract away the
signal's intrinsic temporal and morphological features, lack interpretability,
and are computationally intensive-hindering their deployment on
resource-constrained platforms. In this work, we propose two novel lightweight
1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for
efficient, real-time arrhythmia classification on edge devices. Inspired by
MobileNet's depthwise separable convolutional design, these models maintain
memory footprints of just 302.18 KB and 157.76 KB, respectively, while
achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH
Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch
Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature
Ventricular Contraction. In order to ensure clinical transparency and
relevance, we integrate Shapley Additive Explanations and Gradient-weighted
Class Activation Mapping, enabling both local and global interpretability.
These techniques highlight physiologically meaningful patterns such as the QRS
complex and T-wave that contribute to the model's predictions. We also discuss
performance-efficiency trade-offs and address current limitations related to
dataset diversity and generalizability. Overall, our findings demonstrate the
feasibility of combining interpretability, predictive accuracy, and
computational efficiency in practical, wearable, and embedded ECG monitoring
systems.

</details>


### [35] [A new architecture of high-order deep neural networks that learn martingales](https://arxiv.org/abs/2505.03789)
*Syoiti Ninomiya,Yuming Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于高阶弱近似算法的新型深度学习神经网络架构，用于高效学习随机微分方程（SDEs）的鞅，并研究了其在金融衍生品定价中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法在高效学习SDEs鞅方面的局限性，本文旨在通过深度学习模型结合高阶弱近似算法，提升金融衍生品定价的准确性和效率。

Method: 采用高阶弱近似算法（显式Runge-Kutta类型），通过迭代组合和线性组合目标SDEs的向量场实现近似，构建新型深度学习神经网络架构。

Result: 实验表明，该架构能够有效学习SDEs的鞅，并在金融衍生品定价中表现出良好的性能。

Conclusion: 本文提出的深度学习架构结合高阶弱近似算法，为SDEs的鞅学习和金融衍生品定价提供了一种高效且准确的新方法。

Abstract: A new deep-learning neural network architecture based on high-order weak
approximation algorithms for stochastic differential equations (SDEs) is
proposed. The architecture enables the efficient learning of martingales by
deep learning models. The behaviour of deep neural networks based on this
architecture, when applied to the problem of pricing financial derivatives, is
also examined. The core of this new architecture lies in the high-order weak
approximation algorithms of the explicit Runge--Kutta type, wherein the
approximation is realised solely through iterative compositions and linear
combinations of vector fields of the target SDEs.

</details>


### [36] [A Time-Series Data Augmentation Model through Diffusion and Transformer Integration](https://arxiv.org/abs/2505.03790)
*Yuren Zhang,Zhongnan Pu,Lei Jing*

Main category: cs.LG

TL;DR: 本文提出了一种结合扩散模型和Transformer的方法，用于生成高质量的时间序列数据，以解决该领域数据增强不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，许多任务需要大量训练数据，但时间序列数据的数据增强研究相对较少。本文旨在填补这一空白。

Method: 采用扩散去噪模型生成初始时间步数据，随后用Transformer预测后续动作，并通过加权损失函数实现收敛。

Result: 实验证明，该方法生成的数据能显著提升模型性能，优于传统数据增强方法或无数据增强的情况。

Conclusion: 该方法为时间序列数据增强提供了一种简单有效的解决方案，展示了高质量数据生成的潜力。

Abstract: With the development of Artificial Intelligence, numerous real-world tasks
have been accomplished using technology integrated with deep learning. To
achieve optimal performance, deep neural networks typically require large
volumes of data for training. Although advances in data augmentation have
facilitated the acquisition of vast datasets, most of this data is concentrated
in domains like images and speech. However, there has been relatively less
focus on augmenting time-series data. To address this gap and generate a
substantial amount of time-series data, we propose a simple and effective
method that combines the Diffusion and Transformer models. By utilizing an
adjusted diffusion denoising model to generate a large volume of initial
time-step action data, followed by employing a Transformer model to predict
subsequent actions, and incorporating a weighted loss function to achieve
convergence, the method demonstrates its effectiveness. Using the performance
improvement of the model after applying augmented data as a benchmark, and
comparing the results with those obtained without data augmentation or using
traditional data augmentation methods, this approach shows its capability to
produce high-quality augmented data.

</details>


### [37] [Practical Boolean Backpropagation](https://arxiv.org/abs/2505.03791)
*Simon Golbert*

Main category: cs.LG

TL;DR: 论文提出了一种基于纯布尔代数的反向传播方法，无需数值计算，初步实验验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管量化方法常见，但纯布尔神经网络的训练方法仍未被充分探索。本文旨在弥补这一研究空白。

Method: 采用基于单一特定门的纯布尔代数方法，直接在布尔代数中进行反向传播，无需数值计算。

Result: 初始实验验证了该方法的可行性。

Conclusion: 该方法为硬件高效的布尔神经网络提供了一种新的训练途径。

Abstract: Boolean neural networks offer hardware-efficient alternatives to real-valued
models. While quantization is common, purely Boolean training remains
underexplored. We present a practical method for purely Boolean backpropagation
for networks based on a single specific gate we chose, operating directly in
Boolean algebra involving no numerics. Initial experiments confirm its
feasibility.

</details>


### [38] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/abs/2505.03792)
*Lang Feng,Weihao Tan,Zhiyi Lyu,Longtao Zheng,Haiyang Xu,Ming Yan,Fei Huang,Bo An*

Main category: cs.LG

TL;DR: 论文提出了一种名为CoSo的新型在线微调方法，通过反事实推理动态评估文本动作中关键token的因果影响，从而优化探索效率。


<details>
  <summary>Details</summary>
Motivation: 现有的在线强化学习方法在视觉语言模型(VLM)代理的多步目标导向任务中面临探索空间爆炸的问题，尤其是在开放文本动作空间下。

Method: 采用反事实软强化学习(CoSo)动态评估token对动作的因果影响，优先探索关键token，降低冗余或低影响token的干扰。

Result: CoSo在多种代理任务（如安卓设备控制、卡牌游戏和嵌入式AI）中显著提升了探索效率和性能表现。

Conclusion: CoSo为文本动作空间的在线强化学习提供了一种高效且理论可靠的解决方案，实验验证了其优越性。

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>


### [39] [LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection](https://arxiv.org/abs/2505.03793)
*Xinyue Zeng,Haohui Wang,Junhong Lin,Jun Wu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 论文提出LENSLLM框架，结合Hessian-based PAC-Bayes泛化边界与NTK修正缩放模型，高效预测LLM在下游任务的表现，节约88.5%计算成本且准确率达91.1%。


<details>
  <summary>Details</summary>
Motivation: 为解决LLM在众多下游任务中因计算资源限制无法全量微调的问题，需动态建模其微调行为以优化模型选择。

Method: 首先推导Hessian-based PAC-Bayes泛化边界揭示微调动态，再设计NTK-based修正缩放模型LENSLLM实现高效性能预测。

Result: 在三个大型基准测试中，LENSLLM准确率最高达91.1%，计算成本降低88.5%，超越5种现有方法。

Conclusion: LENSLLM为LLM选择提供了理论支持与高效工具，开源模型促进进一步研究。

Abstract: The proliferation of open-sourced Large Language Models (LLMs) and diverse
downstream tasks necessitates efficient model selection, given the
impracticality of fine-tuning all candidates due to computational constraints.
Despite the recent advances in LLM selection, a fundamental research question
largely remains nascent: how can we model the dynamic behaviors of LLMs during
fine-tuning, thereby enhancing our understanding of their generalization
performance across diverse downstream tasks? In this work, we propose a novel
theoretical framework that provides a proper lens to assess the generalization
capabilities of LLMs, thereby enabling accurate and efficient LLM selection for
downstream applications. In particular, we first derive a Hessian-based
PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and
then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling
Model that enables accurate performance predictions across diverse tasks while
maintaining computational efficiency. Extensive empirical results on 3
large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy
and reduces up to 88.5% computational cost in LLM selection, outperforming 5
state-of-the-art methods. We open-source our proposed LENSLLM model and
corresponding results at the Github link:
https://github.com/Susan571/LENSLLM.git.

</details>


### [40] [A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems](https://arxiv.org/abs/2505.03794)
*İrfan Işik,Ibrahim Karahan,Okan Erkaymaz*

Main category: cs.LG

TL;DR: 提出了一种改进的前后向分裂算法，含两个惯性参数，用于寻找实希尔伯特空间中算子的零点。实验结果显示该算法弱收敛且性能优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 解决实希尔伯特空间中算子零点问题，提升算法的收敛性和实用性。

Method: 改进的前后向分裂算法，引入两个惯性参数，在标准假设下实现弱收敛。

Result: 算法在回归和数据分类问题中表现优于现有方法。

Conclusion: 提出的算法在理论和实验中均表现出优越性，适用于实际应用。

Abstract: This paper presents an improved forward-backward splitting algorithm with two
inertial parameters. It aims to find a point in the real Hilbert space at which
the sum of a co-coercive operator and a maximal monotone operator vanishes.
Under standard assumptions, our proposed algorithm demonstrates weak
convergence. We present numerous experimental results to demonstrate the
behavior of the developed algorithm by comparing it with existing algorithms in
the literature for regression and data classification problems. Furthermore,
these implementations suggest our proposed algorithm yields superior outcomes
when benchmarked against other relevant algorithms in existing literature.

</details>


### [41] [Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks](https://arxiv.org/abs/2505.03797)
*Andrew Millard,Joshua Murphy,Simon Maskell,Zheng Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于SMC的新型训练方法，用于部分贝叶斯神经网络（pBNN），通过引导提议和梯度马尔可夫核改善了高维问题的扩展性，并在预测性能和损失优化上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 部分贝叶斯神经网络（pBNN）仅需随机化部分参数却能媲美全贝叶斯网络，但其训练方法仍有改进空间，尤其是在高维问题上的扩展性。

Method: 采用序列蒙特卡洛（SMC）采样器，结合引导提议和梯度马尔可夫核，提出了一种新的训练方法，提升了高维问题的处理能力。

Result: 新方法在预测性能和损失优化上超过现有技术，且在更大批量下训练时间显著减少，性能更优。

Conclusion: 该SMC-based方法为pBNN提供了高效且可扩展的训练方案，尤其在复杂和高维任务中表现突出。

Abstract: Partial Bayesian neural networks (pBNNs) have been shown to perform
competitively with fully Bayesian neural networks while only having a subset of
the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as
the inference method for pBNNs gives a non-parametric probabilistic estimation
of the stochastic parameters, and has shown improved performance over
parametric methods. In this paper we introduce a new SMC-based training method
for pBNNs by utilising a guided proposal and incorporating gradient-based
Markov kernels, which gives us better scalability on high dimensional problems.
We show that our new method outperforms the state-of-the-art in terms of
predictive performance and optimal loss. We also show that pBNNs scale well
with larger batch sizes, resulting in significantly reduced training times and
often better performance.

</details>


### [42] [Position: Foundation Models Need Digital Twin Representations](https://arxiv.org/abs/2505.03798)
*Yiqing Shen,Hao Ding,Lalithkumar Seenivasan,Tianmin Shu,Mathias Unberath*

Main category: cs.LG

TL;DR: 当前基础模型（FMs）使用的离散令牌表示法限制了其学习真实世界知识的能力，本文提出以数字孪生（DT）表示法替代，以解决多模态语义一致性、时空动态捕捉及因果推理等问题。


<details>
  <summary>Details</summary>
Motivation: 当前FMs的令牌表示法仅依赖统计相关性，无法充分利用显式领域知识，导致在多模态语义一致性、时空动态捕捉及因果推理等方面表现不足。

Method: 提出采用数字孪生（DT）表示法作为FMs的构建基础，其通过物理驱动的数字表示保留真实世界过程的连续性并显式编码领域知识。

Result: DT表示法有望提升FMs在多模态一致性、时空动态建模及因果推理等任务中的表现。

Conclusion: 数字孪生表示法为构建更具语义连贯性和物理合理性的基础模型提供了潜在解决方案。

Abstract: Current foundation models (FMs) rely on token representations that directly
fragment continuous real-world multimodal data into discrete tokens. They limit
FMs to learning real-world knowledge and relationships purely through
statistical correlation rather than leveraging explicit domain knowledge.
Consequently, current FMs struggle with maintaining semantic coherence across
modalities, capturing fine-grained spatial-temporal dynamics, and performing
causal reasoning. These limitations cannot be overcome by simply scaling up
model size or expanding datasets. This position paper argues that the machine
learning community should consider digital twin (DT) representations, which are
outcome-driven digital representations that serve as building blocks for
creating virtual replicas of physical processes, as an alternative to the token
representation for building FMs. Finally, we discuss how DT representations can
address these challenges by providing physically grounded representations that
explicitly encode domain knowledge and preserve the continuous nature of
real-world processes.

</details>


### [43] [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
*Hyun Lee,Chris Yi,Maminur Islam,B. D. S. Aritra*

Main category: cs.LG

TL;DR: 论文提出了一种名为SDM-InstructGLM的新框架，通过指令调优的图语言模型（InstructGLM）处理大规模图结构问题，无需依赖图神经网络（GNN），显著提升了图的表示效率和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将大语言模型（LLM）应用于图相关问题时，主要依赖图神经网络（GNN）作为辅助工具，但直接在大语言模型中编码图结构的方法尚未充分探索。尤其是在大规模图中，由于token限制，图结构的有效表示面临挑战。

Method: 论文提出了SDM-InstructGLM框架，引入一种基于相似度和节点中心性的有偏随机游走机制，选择性采样和编码图信息，从而在大语言模型内实现高效的图结构表示。

Result: 实验结果表明，该方法显著提升了token效率，减少了随机采样导致的信息损失，并在节点分类和链路预测等任务中表现优异。同时，证明了仅用大语言模型进行图处理的可行性。

Conclusion: 本研究为不依赖图神经网络的图学习方法提供了新思路，展示了指令调优图语言模型在大规模图推理中的潜力和可扩展性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in various
natural language processing tasks; however, their application to graph-related
problems remains limited, primarily due to scalability constraints and the
absence of dedicated mechanisms for processing graph structures. Existing
approaches predominantly integrate LLMs with Graph Neural Networks (GNNs),
using GNNs as feature encoders or auxiliary components. However, directly
encoding graph structures within LLMs has been underexplored, particularly in
the context of large-scale graphs where token limitations hinder effective
representation. To address these challenges, we propose SDM-InstructGLM, a
novel instruction-tuned Graph Language Model (InstructGLM) framework that
enhances scalability and efficiency without relying on GNNs. Our method
introduces a similarity-degree-based biased random walk mechanism, which
selectively samples and encodes graph information based on node-feature
similarity and degree centrality, ensuring an adaptive and structured
representation within the LLM. This approach significantly improves token
efficiency, mitigates information loss due to random sampling, and enhances
performance on graph-based tasks such as node classification and link
prediction. Furthermore, our results demonstrate the feasibility of LLM-only
graph processing, enabling scalable and interpretable Graph Language Models
(GLMs) optimized through instruction-based fine-tuning. This work paves the way
for GNN-free approaches to graph learning, leveraging LLMs as standalone graph
reasoning models. Our source code is available on GitHub.

</details>


### [44] [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
*Euntae Choi,Sumin Song,Woosang Lim,Sungjoo Yoo*

Main category: cs.LG

TL;DR: 本文提出了一种无需训练的改进旋转矩阵方法，通过Walsh-Hadamard变换和分组序列排列旋转（GSR），在极低比特位宽（如2位）下显著降低了量化误差并提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有旋转基方法在极低比特位宽（如2位）下的性能不足问题，同时避免高昂的计算成本，本文探索了一种无需训练的改进旋转矩阵方法。

Method: 利用Walsh-Hadamard变换的序列排序聚类相似频率成分以减少量化误差，并提出分组序列排列旋转（GSR）技术，通过块对角矩阵隔离异常值影响。

Result: 该方法在推理任务和WikiText-2的Perplexity（PPL）评分上表现优异，性能接近基于优化的方法，且无需任何训练。

Conclusion: 本方法在极低比特位宽下有效提升了量化性能，且兼容现有学习旋转技术，具有较强的实用性和推广价值。

Abstract: Large Language Models (LLMs) face deployment challenges due to high
computational costs, and while Post-Training Quantization (PTQ) offers a
solution, existing rotation-based methods struggle at very low bit-widths like
2-bit. We introduce a novel, training-free approach to construct an improved
rotation matrix, addressing the limitations of current methods. The key
contributions include leveraging the Walsh-Hadamard transform with sequency
ordering, which clusters similar frequency components to reduce quantization
error compared to standard Hadamard matrices, significantly improving
performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR)
using block-diagonal matrices with smaller Walsh blocks, effectively isolating
outlier impacts and achieving performance comparable to optimization-based
methods without requiring any training. Our method demonstrates robust
performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our
method also enhances results even when applied over existing learned rotation
techniques.

</details>


### [45] [Large Language Model Compression with Global Rank and Sparsity Optimization](https://arxiv.org/abs/2505.03801)
*Changhai Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出了一种新型两阶段LLM压缩方法，通过全局秩和稀疏优化解决低秩与稀疏矩阵交互及权重分配的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决LLM压缩中低秩与稀疏矩阵交互及不同层冗余差异的问题。

Method: 第一阶段使用鲁棒主成分分析分解权重矩阵，第二阶段通过概率全局优化技术联合识别低秩与稀疏结构。

Result: 实验表明，该方法显著优于现有的稀疏化和复合近似技术。

Conclusion: 该方法能自动检测不同层的冗余并管理稀疏与低秩组件的交互，有效提升压缩性能。

Abstract: Low-rank and sparse composite approximation is a natural idea to compress
Large Language Models (LLMs). However, such an idea faces two primary
challenges that adversely affect the performance of existing methods. The first
challenge relates to the interaction and cooperation between low-rank and
sparse matrices, while the second involves determining weight allocation across
different layers, as redundancy varies considerably among them. To address
these challenges, we propose a novel two-stage LLM compression method with the
capability of global rank and sparsity optimization. It is noteworthy that the
overall optimization space is vast, making comprehensive optimization
computationally prohibitive. Therefore, to reduce the optimization space, our
first stage utilizes robust principal component analysis to decompose the
weight matrices of LLMs into low-rank and sparse components, which span the low
dimensional and sparse spaces containing the resultant low-rank and sparse
matrices, respectively. In the second stage, we propose a probabilistic global
optimization technique to jointly identify the low-rank and sparse structures
within the above two spaces. The appealing feature of our approach is its
ability to automatically detect the redundancy across different layers and to
manage the interaction between the sparse and low-rank components. Extensive
experimental results indicate that our method significantly surpasses
state-of-the-art techniques for sparsification and composite approximation.

</details>


### [46] [Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth](https://arxiv.org/abs/2505.03802)
*Changhai Zhou,Yuhua Zhou,Qian Qiao,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: QLoRA结合低比特量化和LoRA实现大模型内存友好微调，提出QR-Adaptor联合搜索量化组件和低秩空间秩分配，性能优于SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于SVD的方法无法持续提升性能，动态混合精度虽直观但未考虑量化与低秩子空间的协同优化，需一种统一策略。

Method: 提出QR-Adaptor，通过部分校准数据联合搜索每层的量化组件和低秩空间秩，以离散优化方式分配精度和秩。

Result: 在GSM8K上准确率提升4.89%，某些情况下性能超过16位微调模型，同时保持4位内存占用。

Conclusion: QR-Adaptor通过协同优化量化与低秩分配，显著提升量化模型性能，且内存高效。

Abstract: QLoRA effectively combines low-bit quantization and LoRA to achieve
memory-friendly fine-tuning for large language models (LLM). Recently, methods
based on SVD for continuous update iterations to initialize LoRA matrices to
accommodate quantization errors have generally failed to consistently improve
performance. Dynamic mixed precision is a natural idea for continuously
improving the fine-tuning performance of quantized models, but previous methods
often optimize low-rank subspaces or quantization components separately,
without considering their synergy. To address this, we propose
\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial
calibration data to jointly search the quantization components and the rank of
low-rank spaces for each layer, thereby continuously improving model
performance. QR-Adaptor does not minimize quantization error but treats
precision and rank allocation as a discrete optimization problem guided by
actual downstream performance and memory usage. Compared to state-of-the-art
(SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\%
accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit
fine-tuned model while maintaining the memory footprint of the 4-bit setting.

</details>


### [47] [RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization](https://arxiv.org/abs/2505.03803)
*Chen Xu,Yuxuan Yue,Zukang Xu,Xing Hu,Jiangyong Yu,Zhixuan Chen,Sifan Zhou,Zhihang Yuan,Dawei Yang*

Main category: cs.LG

TL;DR: 论文为了解决RWKV模型在资源受限设备上部署时量化性能下降的问题，提出了RWKVQuant框架，通过自适应的量化选择和编码本优化算法，成功实现了高效的低比特量化。


<details>
  <summary>Details</summary>
Motivation: RWKV虽然与Transformer性能相当，但在量化时面临显著性能下降的问题，主要受限于非线性操作符和权重分布均匀性两大挑战。

Method: 提出RWKVQuant框架，包含两新技术：1)通过权重均匀性和异常值评估自适应选择量化方法的粗到细代理；2)优化编码本算法以提升逐元素乘法性能。

Result: 实验表明，RWKVQuant能对RWKV-6-14B模型实现约3比特量化，精度损失小于1%，速度提升2.14倍。

Conclusion: RWKVQuant是首个针对RWKV模型的高效量化方案，显著提升了其在资源受限设备上的实用性。

Abstract: RWKV is a modern RNN architecture with comparable performance to Transformer,
but still faces challenges when deployed to resource-constrained devices. Post
Training Quantization (PTQ), which is a an essential technique to reduce model
size and inference latency, has been widely used in Transformer models.
However, it suffers significant degradation of performance when applied to
RWKV. This paper investigates and identifies two key constraints inherent in
the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of
both smooth- and rotation-based quantization, introducing extra computation
overhead. (2) The larger amount of uniformly distributed weights poses
challenges for cluster-based quantization, leading to reduced accuracy. To this
end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting
of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively
selecting different quantization approaches by assessing the uniformity and
identifying outliers in the weights, and (2) a codebook optimization algorithm
that enhances the performance of cluster-based quantization methods for
element-wise multiplication in RWKV. Experiments show that RWKVQuant can
quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x
speed up.

</details>


### [48] [MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance](https://arxiv.org/abs/2505.03804)
*Xing Hu,Zhixuan Chen,Dawei Yang,Zukang Xu,Chen Xu,Zhihang Yuan,Sifan Zhou,Jiangyong Yu*

Main category: cs.LG

TL;DR: MoEQuant是一种针对MoE LLMs的量化框架，通过EBSS和AGQ技术解决了量化中专家间和专家内不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE)大型语言模型虽高效但面临内存开销大和量化后精度下降的问题，需定制化解决方案。

Method: 提出MoEQuant框架，包括专家平衡自采样（EBSS）和亲和引导量化（AGQ）两种技术。

Result: 在4位量化下，MoEQuant为DeepSeekMoE-16B带来超过10个点的准确性提升。

Conclusion: MoEQuant有效解决了MoE模型量化难题，显著提升性能和效率，具有实际部署潜力。

Abstract: Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic
routing and sparse activation to enhance efficiency and scalability, have
achieved higher performance while reducing computational costs. However, these
models face significant memory overheads, limiting their practical deployment
and broader adoption. Post-training quantization (PTQ), a widely used method
for compressing LLMs, encounters severe accuracy degradation and diminished
generalization performance when applied to MoE models. This paper investigates
the impact of MoE's sparse and dynamic characteristics on quantization and
identifies two primary challenges: (1) Inter-expert imbalance, referring to the
uneven distribution of samples across experts, which leads to insufficient and
biased calibration for less frequently utilized experts; (2) Intra-expert
imbalance, arising from MoE's unique aggregation mechanism, which leads to
varying degrees of correlation between different samples and their assigned
experts. To address these challenges, we propose MoEQuant, a novel quantization
framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1)
Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that
efficiently constructs a calibration set with balanced expert distributions by
leveraging the cumulative probabilities of tokens and expert balance metrics as
guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates
affinities between experts and samples into the quantization process, thereby
accurately assessing the impact of individual samples on different experts
within the MoE layer. Experiments demonstrate that MoEQuant achieves
substantial performance gains (more than 10 points accuracy gain in the
HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.

</details>


### [49] [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
*Prudhviraj Naidu,Zixian Wang,Leon Bergen,Ramamohan Paturi*

Main category: cs.LG

TL;DR: 本文研究了Transformer语言模型在十种基础算法任务上的训练过程，发现损失曲线出现了显著的阶段性变化，偏离了传统的幂律缩放规律。在大量的计算过程中，验证损失几乎不改善，然后突然下降。通过探测模型内部表示，发现在停滞期学习到了“静默特征”，随后突然获取“响亮特征”，此时损失急剧下降。消融实验表明，干扰单个学习到的特征会显著降低性能，证明这些特征对任务表现具有因果关系。


<details>
  <summary>Details</summary>
Motivation: 挑战了当前普遍认为的下一个词预测损失可以可靠追踪渐进进展的假设，揭示了模型可能在表面之下发展关键内部特征，直至它们突然融合并触发性能的快速提升。

Method: 通过在十种基础算法任务上训练Transformer语言模型，观察损失曲线的阶段性变化，并通过内部分析和消融实验来验证特征学习的因果作用。

Result: 观察到了损失曲线的显著阶段性变化，发现了“静默特征”和“响亮特征”的交替学习，并通过消融实验证实了这些特征对性能的因果关系。

Conclusion: 这些发现挑战了现有的损失追踪假设，揭示了模型内部特征学习的非线性过程及其对性能的关键影响。

Abstract: We train Transformer-based language models on ten foundational algorithmic
tasks and observe pronounced phase transitions in their loss curves that
deviate from established power-law scaling trends. Over large ranges of
compute, the validation loss barely improves, then abruptly decreases. Probing
the models' internal representations reveals the learning of quiet features
during the stagnant phase, followed by sudden acquisition of loud features that
coincide with the sharp drop in loss. Our ablation experiments show that
disrupting a single learned feature can dramatically degrade performance,
providing evidence of their causal role in task performance. These findings
challenge the prevailing assumption that next-token predictive loss reliably
tracks incremental progress; instead, key internal features may be developing
below the surface until they coalesce, triggering a rapid performance gain.

</details>


### [50] [Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing](https://arxiv.org/abs/2505.03805)
*Nguyen Van Thanh*

Main category: cs.LG

TL;DR: 该论文提出了一个基于随机上坡攀爬（RUC）的模型无关特征优化框架，用于多元时间序列预测，通过高效的特征发现方法提升效率、降低能耗并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 改进传统方法中对GPU深度学习的依赖，提供更轻量、快速且透明的预测工具，以支持资源受限机构和气候风险NGO等的数据驱动决策。

Method: 通过随机组合领域特定语法的算子生成候选特征，利用廉价代理模型在滚动窗口上快速评分，并通过嵌套交叉验证和信息论收缩筛选不稳定特征。

Result: 该框架能够在减少计算资源需求的同时，保持预测性能，并生成更可解释的特征。

Conclusion: 该方法为多元时间序列预测提供了一种高效、节能且透明的替代方案，尤其适用于资源受限的场景。

Abstract: Randomized Uphill Climbing is a lightweight, stochastic search heuristic that
has delivered state of the art equity alpha factors for quantitative hedge
funds. I propose to generalize RUC into a model agnostic feature optimization
framework for multivariate time series forecasting. The core idea is to
synthesize candidate feature programs by randomly composing operators from a
domain specific grammar, score candidates rapidly with inexpensive surrogate
models on rolling windows, and filter instability via nested cross validation
and information theoretic shrinkage. By decoupling feature discovery from GPU
heavy deep learning, the method promises faster iteration cycles, lower energy
consumption, and greater interpretability. Societal relevance: accurate,
transparent forecasting tools empower resource constrained institutions, energy
regulators, climate risk NGOs to make data driven decisions without proprietary
black box models.

</details>


### [51] [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
*Tuochao Chen,Nicholas Batchelder,Alisa Liu,Noah Smith,Shyamnath Gollakota*

Main category: cs.LG

TL;DR: LlamaPIE是一款实时主动助手，通过耳机设备提供不显眼的指导以增强人类对话，无需用户明确调用即可在后台运行。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型需要明确调用的局限性，提供不显眼的实时对话辅助。

Method: 构建半合成对话数据集，采用双模型流水线：小模型决定何时响应，大模型生成响应。

Result: 在真实数据集上验证了有效性，用户研究显示优于无辅助和被动模型的基线。

Conclusion: LlamaPIE展现了增强实时对话的潜力，用户更偏好其主动辅助。

Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to
enhance human conversations through discreet, concise guidance delivered via
hearable devices. Unlike traditional language models that require explicit user
invocation, this assistant operates in the background, anticipating user needs
without interrupting conversations. We address several challenges, including
determining when to respond, crafting concise responses that enhance
conversations, leveraging knowledge of the user for context-aware assistance,
and real-time, on-device processing. To achieve this, we construct a
semi-synthetic dialogue dataset and propose a two-model pipeline: a small model
that decides when to respond and a larger model that generates the response. We
evaluate our approach on real-world datasets, demonstrating its effectiveness
in providing helpful, unobtrusive assistance. User studies with our assistant,
implemented on Apple Silicon M2 hardware, show a strong preference for the
proactive assistant over both a baseline with no assistance and a reactive
model, highlighting the potential of LlamaPie to enhance live conversations.

</details>


### [52] [Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks](https://arxiv.org/abs/2505.03806)
*Mehran Mazandarani,Marzieh Najariyan*

Main category: cs.LG

TL;DR: 本文提出了感知信息神经网络（PrINNs），将感知信息融入神经网络，扩展了物理信息神经网络（PINNs）的概念，支持多种感知形式和多专家知识集成，用于建模复杂系统并发现新的微分方程形式。


<details>
  <summary>Details</summary>
Motivation: 解决传统物理建模与现代数据驱动方法之间的差距，通过整合专家知识和感知信息，使神经网络能在不确定环境中建模复杂系统。

Method: 提出PrINNs框架，引入混合专家信息神经网络（MOEINNs）和转换知识信息神经网络（TKINNs），结合模糊逻辑和深度学习（FINNs），通过损失函数集成感知信息。

Result: PrINNs能够建模动态系统、发现新微分方程形式，并在不确定环境中提升模型性能，无需预训练或去模糊化。

Conclusion: PrINNs是计算科学与工程中的重要工具，弥合了物理建模与数据驱动方法的鸿沟，为复杂系统建模提供了新思路。

Abstract: This article introduces Perception-Informed Neural Networks (PrINNs), a
framework designed to incorporate perception-based information into neural
networks, addressing both systems with known and unknown physics laws or
differential equations. Moreover, PrINNs extend the concept of Physics-Informed
Neural Networks (PINNs) and their variants, offering a platform for the
integration of diverse forms of perception precisiation, including singular,
probability distribution, possibility distribution, interval, and fuzzy graph.
In fact, PrINNs allow neural networks to model dynamical systems by integrating
expert knowledge and perception-based information through loss functions,
enabling the creation of modern data-driven models. Some of the key
contributions include Mixture of Experts Informed Neural Networks (MOEINNs),
which combine heterogeneous expert knowledge into the network, and
Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the
incorporation of meta-information for enhanced model performance. Additionally,
Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural
networks leverage fuzzy logic constraints within a deep learning architecture,
allowing online training without pre-training and eliminating the need for
defuzzification. PrINNs represent a significant step forward in bridging the
gap between traditional physics-based modeling and modern data-driven
approaches, enabling neural networks to learn from both structured physics laws
and flexible perception-based rules. This approach empowers neural networks to
operate in uncertain environments, model complex systems, and discover new
forms of differential equations, making PrINNs a powerful tool for advancing
computational science and engineering.

</details>


### [53] [AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data](https://arxiv.org/abs/2505.03808)
*Ioannis Nasios*

Main category: cs.LG

TL;DR: 该研究提出了一种高效、准确且经济的藻华检测方法，结合开源遥感数据和AI模型，通过集成树模型和神经网络提升性能，适用于全球监测。


<details>
  <summary>Details</summary>
Motivation: 有害藻华对内陆水质和公共健康构成威胁，亟需高效、精准且经济的检测方法。

Method: 整合多源开源遥感数据（如Sentinel-2、DEM、HRRR气候数据），利用树模型和神经网络集成分类藻华严重程度。

Result: 树模型表现优异，加入神经网络后鲁棒性增强，验证了深度学习对多源遥感数据的有效性。

Conclusion: 该方法通过高分辨率卫星影像和AI分析动态监测藻华，具有全球应用潜力，代码已开源。

Abstract: Harmful algal blooms are a growing threat to inland water quality and public
health worldwide, creating an urgent need for efficient, accurate, and
cost-effective detection methods. This research introduces a high-performing
methodology that integrates multiple open-source remote sensing data with
advanced artificial intelligence models. Key data sources include Copernicus
Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and
NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently
retrieved using platforms like Google Earth Engine (GEE) and Microsoft
Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the
altitude from the elevation model, the temperature and wind from NOAA as well
as the longitude and latitude were the most important features. The approach
combines two types of machine learning models, tree-based models and a neural
network, into an ensemble for classifying algal bloom severity. While the tree
models performed strongly on their own, incorporating a neural network added
robustness and demonstrated how deep learning models can effectively use
diverse remote sensing inputs. The method leverages high-resolution satellite
imagery and AI-driven analysis to monitor algal blooms dynamically, and
although initially developed for a NASA competition in the U.S., it shows
potential for global application. The complete code is available for further
adaptation and practical implementation, illustrating the convergence of remote
sensing data and AI to address critical environmental challenges
(https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).

</details>


### [54] [When Dynamic Data Selection Meets Data Augmentation](https://arxiv.org/abs/2505.03809)
*Suorong Yang,Peng Ye,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种在线数据训练框架，首次统一了动态数据选择和增强技术，通过估计样本的局部密度和多模态语义一致性联合分布，实现了高效训练和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的动态数据选择方法虽然能加速训练但不损失性能，但减少了数据多样性影响了泛化能力；而数据增强通常未与选择优化结合，无法充分发挥协同效应。

Method: 提出在线数据训练框架，联合动态数据选择和增强，估计样本的局部密度和多模态语义一致性分布，有针对性地选择适合增强的样本，避免噪声或模糊数据的干扰。

Result: 在多个基准数据集和架构上表现优于现有方法，例如在ImageNet-1k上减少50%训练成本且性能无损。同时提升了抗噪声能力和模型鲁棒性。

Conclusion: 该方法成功统一了数据选择和增强，显著减少了数据集规模而不牺牲泛化能力，具有实际应用价值。

Abstract: Dynamic data selection aims to accelerate training with lossless performance.
However, reducing training data inherently limits data diversity, potentially
hindering generalization. While data augmentation is widely used to enhance
diversity, it is typically not optimized in conjunction with selection. As a
result, directly combining these techniques fails to fully exploit their
synergies. To tackle the challenge, we propose a novel online data training
framework that, for the first time, unifies dynamic data selection and
augmentation, achieving both training efficiency and enhanced performance. Our
method estimates each sample's joint distribution of local density and
multimodal semantic consistency, allowing for the targeted selection of
augmentation-suitable samples while suppressing the inclusion of noisy or
ambiguous data. This enables a more significant reduction in dataset size
without sacrificing model generalization. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches on various
benchmark datasets and architectures, e.g., reducing 50\% training costs on
ImageNet-1k with lossless performance. Furthermore, our approach enhances noise
resistance and improves model robustness, reinforcing its practical utility in
real-world scenarios.

</details>


### [55] [ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior](https://arxiv.org/abs/2505.03811)
*Surajit Chakrabarty,Rukma Talwadker,Tridib Mukherjee*

Main category: cs.LG

TL;DR: ScarceGAN通过重新设计半监督GAN，解决了多维纵向数据中极端稀有样本识别的挑战，特别是在标签稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 针对正类样本稀缺、负类样本多类分布不均以及大量未标注数据的挑战，提出ScarceGAN以改进半监督学习方法。

Method: 通过放松判别器的精确区分约束并引入‘leeway’项，修改判别器和生成器的损失目标。

Result: 在技能游戏风险玩家识别中，稀有类召回率达85%，优于当前GAN模型，并在KDDCUP99入侵数据集中表现优异。

Conclusion: ScarceGAN有效提升了半监督GAN在稀有样本识别中的性能，特别适用于标签稀缺场景。

Abstract: This paper introduces ScarceGAN which focuses on identification of extremely
rare or scarce samples from multi-dimensional longitudinal telemetry data with
small and weak label prior. We specifically address: (i) severe scarcity in
positive class, stemming from both underlying organic skew in the data, as well
as extremely limited labels; (ii) multi-class nature of the negative samples,
with uneven density distributions and partially overlapping feature
distributions; and (iii) massively unlabelled data leading to tiny and weak
prior on both positive and negative classes, and possibility of unseen or
unknown behavior in the unlabelled set, especially in the negative class.
Although related to PU learning problems, we contend that knowledge (or lack of
it) on the negative class can be leveraged to learn the compliment of it (i.e.,
the positive class) better in a semi-supervised manner. To this effect,
ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled
multi-class negative samples and the available positive samples. It relaxes the
supervised discriminator's constraint on exact differentiation between negative
samples by introducing a 'leeway' term for samples with noisy prior. We propose
modifications to the cost objectives of discriminator, in supervised and
unsupervised path as well as that of the generator. For identifying risky
players in skill gaming, this formulation in whole gives us a recall of over
85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very
minimal verbosity in the unknown space. Further ScarceGAN outperforms the
recall benchmarks established by recent GAN based specialized models for the
positive imbalanced class identification and establishes a new benchmark in
identifying one of rare attack classes (0.09%) in the intrusion dataset from
the KDDCUP99 challenge.

</details>


### [56] [Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications](https://arxiv.org/abs/2505.03812)
*Tomaso Aste*

Main category: cs.LG

TL;DR: 本文综述了信息过滤网络（IFNs）的理论基础、构建方法及多领域应用，强调其在高维数据建模中的优势及其与机器学习的融合潜力。


<details>
  <summary>Details</summary>
Motivation: 针对高维数据建模中的关键挑战，IFNs通过全局稀疏但局部密集且可解释的结构捕捉多元依赖关系，填补了传统方法的不足。

Method: IFNs采用三角最大过滤图（TMFG）和最大过滤团森林（MFCF）等方法构建高阶网络，生成单纯复形结构。

Result: IFNs在金融、生物学、心理学和人工智能等领域提升了模型的可解释性、计算效率和预测性能，尤其在图形建模中超越了传统方法。

Conclusion: IFNs不仅连接经典网络理论与现代数据驱动范式，还能影响深度学习模型架构，具有广阔的应用前景。

Abstract: Information Filtering Networks (IFNs) provide a powerful framework for
modeling complex systems through globally sparse yet locally dense and
interpretable structures that capture multivariate dependencies. This review
offers a comprehensive account of IFNs, covering their theoretical foundations,
construction methodologies, and diverse applications. Tracing their origins
from early network-based models to advanced formulations such as the
Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique
Forest (MFCF), the paper highlights how IFNs address key challenges in
high-dimensional data-driven modeling. IFNs and their construction
methodologies are intrinsically higher-order networks that generate simplicial
complexes-structures that are only now becoming popular in the broader
literature. Applications span fields including finance, biology, psychology,
and artificial intelligence, where IFNs improve interpretability, computational
efficiency, and predictive performance. Special attention is given to their
role in graphical modeling, where IFNs enable the estimation of sparse inverse
covariance matrices with greater accuracy and scalability than traditional
approaches like Graphical LASSO. Finally, the review discusses recent
developments that integrate IFNs with machine learning and deep learning,
underscoring their potential not only to bridge classical network theory with
contemporary data-driven paradigms, but also to shape the architectures of deep
learning models themselves.

</details>


### [57] [Program Semantic Inequivalence Game with Large Language Models](https://arxiv.org/abs/2505.03818)
*Antonio Valerio Miceli-Barone,Vaishak Belle,Ali Payani*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为SInQ的方法，通过半对抗性的语义不等价游戏生成代码推理训练数据，提升大语言模型在复杂编程任务中的表现，并展示了在多语言漏洞检测等任务中的显著改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在简单编程任务中表现良好，但在需要复杂程序语义推理的任务中表现不佳。现有训练数据难以满足这类需求，因此需开发一种能自动生成高质量训练数据的方法。

Method: 采用SInQ方法，通过生成器代理生成语义不同的程序变体，评估器代理识别导致行为差异的输入样例，两者通过半对抗性方式互相训练，实现无限自我提升的可能性。

Result: 该方法在多语言漏洞检测（如C/C++）和Python内置标识符替换等任务中显著提升了模型表现，即使仅用Python代码训练也能泛化到其他语言。

Conclusion: SInQ提供了一种有效生成高质量代码推理训练数据的途径，验证了其在大语言模型复杂任务上的潜力，并开源了实验代码和合成数据以供社区使用。

Abstract: Large Language Models (LLMs) can achieve strong performance on everyday
coding tasks, but they can fail on complex tasks that require non-trivial
reasoning about program semantics. Finding training examples to teach LLMs to
solve these tasks can be challenging.
  In this work, we explore a method to synthetically generate code reasoning
training data based on a semantic inequivalence game SInQ: a generator agent
creates program variants that are semantically distinct, derived from a dataset
of real-world programming tasks, while an evaluator agent has to identify input
examples that cause the original programs and the generated variants to diverge
in their behaviour, with the agents training each other semi-adversarially. We
prove that this setup enables theoretically unlimited improvement through
self-play in the limit of infinite computational resources.
  We evaluated our approach on multiple code generation and understanding
benchmarks, including cross-language vulnerability detection (Lu et al., 2021),
where our method improves vulnerability detection in C/C++ code despite being
trained exclusively on Python code, and the challenging Python builtin
identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas
modern LLMs still struggle with this benchmark, our approach yields substantial
improvements.
  We release the code needed to replicate the experiments, as well as the
generated synthetic data, which can be used to fine-tune LLMs.

</details>


### [58] [Focus on the Likely: Test-time Instance-based Uncertainty Removal](https://arxiv.org/abs/2505.03819)
*Johannes Schneider*

Main category: cs.LG

TL;DR: 论文提出两种无需辅助数据的测试时微调方法，通过单步梯度下降优化高不确定性预测，提升模型在文本和图像领域的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决模型在高不确定性情况下的预测问题，无需额外数据，仅利用测试实例优化预测结果。

Method: 引入额外关注步骤，对高不确定性预测进行单步梯度下降微调，减少不相关结果的概率。

Result: 在文本和图像领域的多种模型上，使用相同超参数显著提升了高不确定性样本的预测准确率。

Conclusion: 该方法通过微调高不确定性预测，有效提高了模型在实际应用中的准确性和鲁棒性。

Abstract: We propose two novel test-time fine-tuning methods to improve uncertain model
predictions. Our methods require no auxiliary data and use the given test
instance only. Instead of performing a greedy selection of the most likely
class to make a prediction, we introduce an additional focus on the likely
classes step during inference. By applying a single-step gradient descent, we
refine predictions when an initial forward pass indicates high uncertainty.
This aligns predictions more closely with the ideal of assigning zero
probability to less plausible outcomes. Our theoretical discussion provides a
deeper understanding highlighting the impact on shared and non-shared features
among (focus) classes. The experimental evaluation highlights accuracy gains on
samples exhibiting high decision uncertainty for a diverse set of models from
both the text and image domain using the same hyperparameters.

</details>


### [59] [DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction](https://arxiv.org/abs/2505.03822)
*Hao Wu,Jialiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种双正则二阶潜在因子（DRSLF）模型，通过结合L1和L2正则项以及二阶信息，提高了QoS预测精度。实验证明其优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统LFA模型因依赖一阶优化器和L2正则而导致的QoS预测精度不足问题。

Method: 提出DRSLF模型，结合L1和L2正则项，并在共轭梯度步中引入Hessian-向量积以利用二阶信息。

Result: 在两个真实QoS数据集上的实验表明，DRSLF的低秩表示能力优于基线方法。

Conclusion: DRSLF通过双重正则和二阶优化显著提升了QoS预测性能，为高维不完整矩阵问题提供了有效解决方案。

Abstract: Quality-of-Service (QoS) data plays a crucial role in cloud service
selection. Since users cannot access all services, QoS can be represented by a
high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA)
models have been proven effective as low-rank representation techniques for
addressing this issue. However, most LFA models rely on first-order optimizers
and use L2-norm regularization, which can lead to lower QoS prediction
accuracy. To address this issue, this paper proposes a double regularized
second-order latent factor (DRSLF) model with two key ideas: a) integrating
L1-norm and L2-norm regularization terms to enhance the low-rank representation
performance; b) incorporating second-order information by calculating the
Hessian-vector product in each conjugate gradient step. Experimental results on
two real-world response-time QoS datasets demonstrate that DRSLF has a higher
low-rank representation capability than two baselines.

</details>


### [60] [Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments](https://arxiv.org/abs/2505.03825)
*Anushiya Arunan,Yan Qin,Xiaoli Li,Yuen Chau*

Main category: cs.LG

TL;DR: 论文提出了ITA-CTF框架，通过智能增强对比张量分解学习多维时间序列的有效表示，解决了低数据环境下深度学习的过拟合问题，性能提升高达18.7%。


<details>
  <summary>Details</summary>
Motivation: 在多维时间序列分类任务中，深度学习在低训练数据环境下易过拟合，难以学习泛化特征。论文旨在提出一种数据高效的方法，捕捉跨维度依赖和类内变化。

Method: ITA-CTF框架结合了对比张量分解（CTF）和智能增强（ITA）模块。CTF通过对比损失优化学习时间序列核心成分及其联合依赖，ITA通过动态生成类感知的增强数据强化对比学习。

Result: 在五个分类任务中，ITA-CTF相比标准张量分解和深度学习方法，性能显著提升，最高达18.7%。

Conclusion: ITA-CTF通过增强数据效率和类感知表示学习，有效提升了低数据环境下多维时间序列分类的性能。

Abstract: Classification of multi-dimensional time series from real-world systems
require fine-grained learning of complex features such as cross-dimensional
dependencies and intra-class variations-all under the practical challenge of
low training data availability. However, standard deep learning (DL) struggles
to learn generalizable features in low-data environments due to model
overfitting. We propose a versatile yet data-efficient framework, Intelligently
Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective
representations from multi-dimensional time series. The CTF module learns core
explanatory components of the time series (e.g., sensor factors, temporal
factors), and importantly, their joint dependencies. Notably, unlike standard
tensor factorization (TF), the CTF module incorporates a new contrastive loss
optimization to induce similarity learning and class-awareness into the learnt
representations for better classification performance. To strengthen this
contrastive learning, the preceding ITA module generates targeted but
informative augmentations that highlight realistic intra-class patterns in the
original data, while preserving class-wise properties. This is achieved by
dynamically sampling a "soft" class prototype to guide the warping of each
query data sample, which results in an augmentation that is intelligently
pattern-mixed between the "soft" class prototype and the query sample. These
augmentations enable the CTF module to recognize complex intra-class variations
despite the limited original training data, and seek out invariant class-wise
properties for accurate classification performance. The proposed method is
comprehensively evaluated on five different classification tasks. Compared to
standard TF and several DL benchmarks, notable performance improvements up to
18.7% were achieved.

</details>


### [61] [MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation](https://arxiv.org/abs/2505.03827)
*Xin Wang,Ling Feng,Huijun Zhang,Lei Cao,Kaisheng Zeng,Qi Li,Yang Ding,Yi Dai,David Clifton*

Main category: cs.LG

TL;DR: 该研究提出了一种基于元学习的社交媒体帖子压力源估计框架，并引入了元知识继承机制，解决了多样性压力源和小样本条件下的学习问题，同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现代社会中压力可能导致严重的健康问题，而社交媒体成为压力检测的新工具。现有工作多关注压力状态分类，本研究旨在通过社交媒体帖子估计更具体的压力源（如考试、写论文等），但压力源的多样性和不断新增的特性带来了挑战。

Method: 研究将压力源估计问题置于小样本学习场景，提出了一个基于元学习的框架，并引入元知识继承机制，以增强模型的泛化能力和防止灾难性遗忘。

Result: 实验表明，该模型在基线比较中达到最优性能，并公开了一个社交媒体压力源估计数据集，用于支持AI模型训练。

Conclusion: 该框架不仅能够有效学习压力源上下文，还能在小样本条件下泛化至新压力源，同时避免灾难性遗忘，为压力源估计提供了实用解决方案。

Abstract: Stress haunts people in modern society, which may cause severe health issues
if left unattended. With social media becoming an integral part of daily life,
leveraging social media to detect stress has gained increasing attention. While
the majority of the work focuses on classifying stress states and stress
categories, this study introduce a new task aimed at estimating more specific
stressors (like exam, writing paper, etc.) through users' posts on social
media. Unfortunately, the diversity of stressors with many different classes
but a few examples per class, combined with the consistent arising of new
stressors over time, hinders the machine understanding of stressors. To this
end, we cast the stressor estimation problem within a practical scenario
few-shot learning setting, and propose a novel meta-learning based stressor
estimation framework that is enhanced by a meta-knowledge inheritance
mechanism. This model can not only learn generic stressor context through
meta-learning, but also has a good generalization ability to estimate new
stressors with little labeled data. A fundamental breakthrough in our approach
lies in the inclusion of the meta-knowledge inheritance mechanism, which equips
our model with the ability to prevent catastrophic forgetting when adapting to
new stressors. The experimental results show that our model achieves
state-of-the-art performance compared with the baselines. Additionally, we
construct a social media-based stressor estimation dataset that can help train
artificial intelligence models to facilitate human well-being. The dataset is
now public at
\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\underline{Kaggle}}
and
\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\underline{Hugging
Face}}.

</details>


### [62] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/abs/2505.03849)
*Jonathan Gorard,Ammar Hakim,Hong Qin,Kyle Parfrey,Shantenu Jha*

Main category: cs.LG

TL;DR: 摘要主张结合蒙特卡洛采样与现代非线性降维技术，并通过形式化验证方法，构建具有数学和物理正确性的参数空间限制。


<details>
  <summary>Details</summary>
Motivation: 解决核聚变和高能天体物理中的高维参数扫描问题，同时处理测量参数和物理模型的不确定性。

Method: 结合蒙特卡洛采样、自动编码器和流形学习等非线性降维技术，并使用形式化验证方法确保参数空间的数学和物理一致性。

Result: 提出了一种混合方法，能够在保证数学和物理正确性的同时，处理实验和物理过程中的不确定性。

Conclusion: 通过结合降维技术和形式化验证，能够在高维参数扫描中有效减少参数空间，同时确保结果的可靠性。

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>


### [63] [Machine Learning: a Lecture Note](https://arxiv.org/abs/2505.03861)
*Kyunghyun Cho*

Main category: cs.LG

TL;DR: 这份讲义旨在为数据科学或相关学科的早期硕士和博士生提供机器学习的基础知识，涵盖分类任务、基本概念（如损失函数、反向传播等）、无监督学习的概率方法，以及进阶主题（如强化学习、元学习等）。


<details>
  <summary>Details</summary>
Motivation: 为早期研究生提供机器学习的全面基础，帮助他们为后续高级研究和学习做准备。

Method: 从基本概念（如分类、损失函数、反向传播）入手，逐步深入无监督学习的概率方法，并拓展到多样化的进阶主题。

Result: 学生将掌握机器学习的基础知识和核心方法，具备进一步研究高级课题的能力。

Conclusion: 讲义系统地覆盖了机器学习的基础和进阶内容，为学生提供了扎实的入门指导。

Abstract: This lecture note is intended to prepare early-year master's and PhD students
in data science or a related discipline with foundational ideas in machine
learning. It starts with basic ideas in modern machine learning with
classification as a main target task. These basic ideas include loss
formulation, backpropagation, stochastic gradient descent, generalization,
model selection as well as fundamental blocks of artificial neural networks.
Based on these basic ideas, the lecture note explores in depth the probablistic
approach to unsupervised learning, covering directed latent variable models,
product of experts, generative adversarial networks and autoregressive models.
Finally, the note ends by covering a diverse set of further topics, such as
reinforcement learning, ensemble methods and meta-learning. After reading this
lecture note, a student should be ready to embark on studying and researching
more advanced topics in machine learning and more broadly artificial
intelligence.

</details>


### [64] [Explaining Anomalies with Tensor Networks](https://arxiv.org/abs/2505.03911)
*Hans Hohenfeld,Marius Beuerle,Elie Mounzer*

Main category: cs.LG

TL;DR: 该论文扩展了张量网络在可解释异常检测中的应用，从离散值数据扩展到实值数据，并引入了树张量网络，展示了其在三个基准问题上的预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展张量网络的应用范围，尤其是将其从离散值数据领域推广到实值数据领域，并探索其在可解释异常检测中的潜力。

Method: 方法包括使用矩阵乘积态和树张量网络作为变分量子多体波函数，结合量子启发的技术，对实值数据进行可解释异常检测。

Result: 研究结果表明，两种张量网络架构在三个基准问题上表现出与基线模型相当的预测性能，并能有效解释异常样本。

Conclusion: 结论是这种方法不仅扩展了张量网络的应用范围，还为进一步探索更复杂的张量网络架构奠定了基础。

Abstract: Tensor networks, a class of variational quantum many-body wave functions have
attracted considerable research interest across many disciplines, including
classical machine learning. Recently, Aizpurua et al. demonstrated explainable
anomaly detection with matrix product states on a discrete-valued
cyber-security task, using quantum-inspired methods to gain insight into the
learned model and detected anomalies. Here, we extend this framework to
real-valued data domains. We furthermore introduce tree tensor networks for the
task of explainable anomaly detection. We demonstrate these methods with three
benchmark problems, show adequate predictive performance compared to several
baseline models and both tensor network architectures' ability to explain
anomalous samples. We thereby extend the application of tensor networks to a
broader class of potential problems and open a pathway for future extensions to
more complex tensor network architectures.

</details>


### [65] [SAND: One-Shot Feature Selection with Additive Noise Distortion](https://arxiv.org/abs/2505.03923)
*Pedram Pad,Hadi Hammoud,Mohamad Dia,Nadim Maamari,L. Andrea Dunbar*

Main category: cs.LG

TL;DR: 提出了一个简单的特征选择层，无需修改损失函数或重新训练，自动选择最重要的特征，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法通常需要繁琐的调整和重训练，影响了易用性和效率。本文旨在简化这一过程。

Method: 在神经网络训练中引入了一个基于高斯噪声和可训练增益的非侵入式特征选择层，自动选择最具信息量的特征。

Result: 在标准数据集和真实数据集中展示出与现有方法相当或更好的性能，且无需额外的超参数调整或重训练。

Conclusion: 证明简单性和高性能可以共存，为特征选择提供了一个有效且易于使用的工具。

Abstract: Feature selection is a critical step in data-driven applications, reducing
input dimensionality to enhance learning accuracy, computational efficiency,
and interpretability. Existing state-of-the-art methods often require
post-selection retraining and extensive hyperparameter tuning, complicating
their adoption. We introduce a novel, non-intrusive feature selection layer
that, given a target feature count $k$, automatically identifies and selects
the $k$ most informative features during neural network training. Our method is
uniquely simple, requiring no alterations to the loss function, network
architecture, or post-selection retraining. The layer is mathematically elegant
and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i +
(1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the
output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that
$\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect,
driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the
rest to $0$ (discarding redundant ones) via weighted noise distortion and gain
normalization. Despite its extreme simplicity, our method delivers
state-of-the-art performance on standard benchmark datasets and a novel
real-world dataset, outperforming or matching existing approaches without
requiring hyperparameter search for $k$ or retraining. Theoretical analysis in
the context of linear regression further validates its efficacy. Our work
demonstrates that simplicity and performance are not mutually exclusive,
offering a powerful yet straightforward tool for feature selection in machine
learning.

</details>


### [66] [Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading](https://arxiv.org/abs/2505.03949)
*John Christopher Tidwell,John Storm Tidwell*

Main category: cs.LG

TL;DR: 论文提出了一种结合CNN、LSTM和DQN的集成深度学习框架，用于自动化股票交易，解决了传统方法和直接强化学习在市场噪声、复杂性和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统方法和直接强化学习在自动化股票交易中难以应对市场噪声、复杂性和泛化问题，因此需要一种更有效的解决方案。

Method: 采用了集成的深度学习框架：CNN用于识别技术指标图像中的模式，LSTM捕捉价格历史和技术指标的时间依赖性，DQN基于CNN和LSTM提取的特征学习最优交易策略。

Result: 未明确提及具体结果，但框架设计旨在提升交易策略的准确性和鲁棒性。

Conclusion: 集成CNN、LSTM和DQN的框架有望解决自动化股票交易中的关键挑战，提升交易性能。

Abstract: This project addresses the challenge of automated stock trading, where
traditional methods and direct reinforcement learning (RL) struggle with market
noise, complexity, and generalization. Our proposed solution is an integrated
deep learning framework combining a Convolutional Neural Network (CNN) to
identify patterns in technical indicators formatted as images, a Long
Short-Term Memory (LSTM) network to capture temporal dependencies across both
price history and technical indicators, and a Deep Q-Network (DQN) agent which
learns the optimal trading policy (buy, sell, hold) based on the features
extracted by the CNN and LSTM.

</details>


### [67] [Sufficient Decision Proxies for Decision-Focused Learning](https://arxiv.org/abs/2505.03953)
*Noah Schutte,Grigorii Veviurko,Krzysztof Postek,Neil Yorke-Smith*

Main category: cs.LG

TL;DR: 论文研究了在优化问题中使用决策导向学习（DFL）时，如何根据问题特性选择单值预测或分布估计，并提出了有效的决策代理方法，实验表明其在不同问题中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补决策导向学习中关于单值预测和分布估计选择依据的理论空白。

Method: 通过分析问题特性，设计适用于决策导向学习的代理方法，并在不同优化问题中进行实验验证。

Result: 实验结果表明所提出的方法在连续和离散变量问题中均有效，包括目标函数和约束中的不确定性。

Conclusion: 论文结论指出，通过合理选择问题代理方法，可以在不增加学习复杂度的情况下提高决策质量。

Abstract: When solving optimization problems under uncertainty with contextual data,
utilizing machine learning to predict the uncertain parameters is a popular and
effective approach. Decision-focused learning (DFL) aims at learning a
predictive model such that decision quality, instead of prediction accuracy, is
maximized. Common practice here is to predict a single value for each uncertain
parameter, implicitly assuming that there exists a (single-scenario)
deterministic problem approximation (proxy) that is sufficient to obtain an
optimal decision. Other work assumes the opposite, where the underlying
distribution needs to be estimated. However, little is known about when either
choice is valid. This paper investigates for the first time problem properties
that justify using either assumption. Using this, we present effective decision
proxies for DFL, with very limited compromise on the complexity of the learning
task. We show the effectiveness of presented approaches in experiments on
problems with continuous and discrete variables, as well as uncertainty in the
objective function and in the constraints.

</details>


### [68] [Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation](https://arxiv.org/abs/2505.03955)
*Charupriya Sharma,Iñaki Estella Aguerri,Daniel Guimarans*

Main category: cs.LG

TL;DR: FlowRec将层次预测协调问题转化为网络流优化，显著提高了计算效率和准确性，适用于大规模应用。


<details>
  <summary>Details</summary>
Motivation: 现有的层次预测协调方法（如MinT）局限于树结构且计算成本高，FlowRec旨在解决这一问题。

Method: 将层次预测协调重新建模为网络流优化，支持广义网络结构并提供高效的局部更新。

Result: FlowRec在准确性和计算效率（提升3-40倍）上优于MinT，内存使用减少5-7倍。

Conclusion: FlowRec是一种高效的大规模层次预测协调工具，尤其在动态场景中表现优越。

Abstract: Hierarchical forecasting with reconciliation requires forecasting values of a
hierarchy (e.g.~customer demand in a state and district), such that forecast
values are linked (e.g.~ district forecasts should add up to the state
forecast). Basic forecasting provides no guarantee for these desired structural
relationships. Reconciliation addresses this problem, which is crucial for
organizations requiring coherent predictions across multiple aggregation
levels. Current methods like minimum trace (MinT) are mostly limited to tree
structures and are computationally expensive. We introduce FlowRec, which
reformulates hierarchical forecast reconciliation as a network flow
optimization, enabling forecasting on generalized network structures. While
reconciliation under the $\ell_0$ norm is NP-hard, we prove polynomial-time
solvability for all $\ell_{p > 0}$ norms and , for any strictly convex and
continuously differentiable loss function. For sparse networks, FlowRec
achieves $O(n^2\log n)$ complexity, significantly improving upon MinT's
$O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general
networks, replacing MinT's error-covariance estimation step with direct network
structural information. A key novelty of our approach is its handling of
dynamic scenarios: while traditional methods recompute both base forecasts and
reconciliation, FlowRec provides efficient localised updates with optimality
guarantees. Monotonicity ensures that when forecasts improve incrementally, the
initial reconciliation remains optimal. We also establish efficient,
error-bounded approximate reconciliation, enabling fast updates in
time-critical applications. Experiments on both simulated and real benchmarks
demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage
by 5-7x. These results establish FlowRec as a powerful tool for large-scale
hierarchical forecasting applications.

</details>


### [69] [Call for Action: towards the next generation of symbolic regression benchmark](https://arxiv.org/abs/2505.03977)
*Guilherme S. Imai Aldeia,Hengzhe Zhang,Geoffrey Bomarito,Miles Cranmer,Alcides Fonseca,Bogdan Burlacu,William G. La Cava,Fabrício Olivetti de França*

Main category: cs.LG

TL;DR: 该论文介绍了SRBench的更新版本，通过增加评估方法、优化指标和改进可视化，分析模型复杂性、准确性和能耗之间的权衡，发现没有单一算法在所有数据集上表现最优，并呼吁SR社区共同维护和更新这一基准。


<details>
  <summary>Details</summary>
Motivation: 由于符号回归方法的多样性和评估标准的差异，基准测试变得复杂且有挑战性。因此，提出更新SRBench以更全面地评估和比较不同算法的性能。

Method: 扩展了原有基准测试的方法数量，优化了评估指标，改进了结果的可视化方式，并分析了模型复杂性、准确性和能耗的权衡。

Result: 研究发现没有单一算法在所有数据集上表现最优，同时提出了维护SRBench的建议，包括标准化的超参数调整和计算资源分配。

Conclusion: 呼吁SR社区共同维护SRBench作为反映最新技术的动态基准，并提出了改进符号回归算法的最佳实践，如自适应超参数调整和能效实现。

Abstract: Symbolic Regression (SR) is a powerful technique for discovering
interpretable mathematical expressions. However, benchmarking SR methods
remains challenging due to the diversity of algorithms, datasets, and
evaluation criteria. In this work, we present an updated version of SRBench.
Our benchmark expands the previous one by nearly doubling the number of
evaluated methods, refining evaluation metrics, and using improved
visualizations of the results to understand the performances. Additionally, we
analyze trade-offs between model complexity, accuracy, and energy consumption.
Our results show that no single algorithm dominates across all datasets. We
propose a call for action from SR community in maintaining and evolving SRBench
as a living benchmark that reflects the state-of-the-art in symbolic
regression, by standardizing hyperparameter tuning, execution constraints, and
computational resource allocation. We also propose deprecation criteria to
maintain the benchmark's relevance and discuss best practices for improving SR
algorithms, such as adaptive hyperparameter tuning and energy-efficient
implementations.

</details>


### [70] [Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations](https://arxiv.org/abs/2505.03980)
*Aroon Sankoh,Victor Wickerhauser*

Main category: cs.LG

TL;DR: 论文比较了传统统计方法（如MLE）和深度学习模型（如RNN）在估计Ornstein-Uhlenbeck过程参数时的准确性和计算成本，发现RNN可能更优。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法（如MLE）长期以来用于估计随机微分方程参数，但深度学习技术的发展可能提供更精确的估计方案。

Method: 通过实验比较了最大似然估计（MLE）和循环神经网络（RNN）在估计Ornstein-Uhlenbeck过程参数时的表现。

Result: 实验表明，RNN在参数估计准确性上可能优于传统统计方法，尤其是在复杂场景中。

Conclusion: 深度学习模型（如RNN）为随机微分方程参数估计提供了新的高效解决方案，可能在未来取代传统统计方法。

Abstract: Stochastic differential equations such as the Ornstein-Uhlenbeck process have
long been used to model realworld probablistic events such as stock prices and
temperature fluctuations. While statistical methods such as Maximum Likelihood
Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have
historically been used to estimate the parameters of stochastic differential
equations, the recent explosion of deep learning technology suggests that
models such as a Recurrent Neural Network (RNN) could produce more precise
estimators. We present a series of experiments that compare the estimation
accuracy and computational expensiveness of a statistical method (MLE) with a
deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.

</details>


### [71] [Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation](https://arxiv.org/abs/2505.03983)
*Hengyuan Hu,Aniket Das,Dorsa Sadigh,Nima Anari*

Main category: cs.LG

TL;DR: 该论文通过重新参数化DDPM，证明其增量满足可交换性，从而将自回归模型的性能优化技术应用于扩散模型，并提出无需辅助模型的Autospeculative Decoding (ASD)方法，显著加速DDPM推理。


<details>
  <summary>Details</summary>
Motivation: 由于DDPM的顺序计算需求导致推理时间瓶颈，论文旨在通过理论分析和方法创新来提升其推理效率。

Method: 利用DDPM与随机定位的联系，证明其增量可交换性，并基于此提出ASD方法，无需辅助模型即可实现并行加速。

Result: 理论分析显示ASD在并行运行时实现$	ilde{O}(K^{rac{1}{3}})$的速度提升，实际应用中也显著加速不同领域的DDPM推理。

Conclusion: 通过理论突破和方法创新，论文成功解决了DDPM的推理效率问题，为扩散模型的优化提供了新思路。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful
tools for generative modeling. However, their sequential computation
requirements lead to significant inference-time bottlenecks. In this work, we
utilize the connection between DDPMs and Stochastic Localization to prove that,
under an appropriate reparametrization, the increments of DDPM satisfy an
exchangeability property. This general insight enables near-black-box
adaptation of various performance optimization techniques from autoregressive
models to the diffusion setting. To demonstrate this, we introduce
\emph{Autospeculative Decoding} (ASD), an extension of the widely used
speculative decoding algorithm to DDPMs that does not require any auxiliary
draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O}
(K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM.
We also demonstrate that a practical implementation of autospeculative decoding
accelerates DDPM inference significantly in various domains.

</details>


### [72] [Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics](https://arxiv.org/abs/2505.03992)
*Jarren Briscoe,Garrett Kepler,Daryl Deford,Assefaw Gebremedhin*

Main category: cs.LG

TL;DR: 本文揭示了组合学在分类指标中引入的样本量偏差，挑战了高分辨率评估偏差的有效性，尤其在不同规模群体比较时。提出了模型无关的评估与校正技术。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习模型时，样本量偏差（尤其是组合学引发的）常被忽视，但其在高分辨率评估和社会应用中可能导致误导性结果。

Method: 分析了多种常见指标中的偏差，并提出模型无关的评估与校正方法，同时研究了未定义案例对评估的影响。

Result: 揭示了组合学和概率对标准评估实践的潜在影响，提出了更公平、可信的分类方法。

Conclusion: 该研究强调了组合学偏差的重要性，为公平和可信的分类评估提供了新方向。

Abstract: Evaluating machine learning models is crucial not only for determining their
technical accuracy but also for assessing their potential societal
implications. While the potential for low-sample-size bias in algorithms is
well known, we demonstrate the significance of sample-size bias induced by
combinatorics in classification metrics. This revelation challenges the
efficacy of these metrics in assessing bias with high resolution, especially
when comparing groups of disparate sizes, which frequently arise in social
applications. We provide analyses of the bias that appears in several commonly
applied metrics and propose a model-agnostic assessment and correction
technique. Additionally, we analyze counts of undefined cases in metric
calculations, which can lead to misleading evaluations if improperly handled.
This work illuminates the previously unrecognized challenge of combinatorics
and probability in standard evaluation practices and thereby advances
approaches for performing fair and trustworthy classification methods.

</details>


### [73] [Iterative Orthogonalization Scaling Laws](https://arxiv.org/abs/2505.04005)
*Devan Selvaraj*

Main category: cs.LG

TL;DR: 摘要探讨了μon优化器在大规模应用中的潜在问题，即随机矩阵奇异值随规模缩小的影响，但未提出解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究μon优化器在大规模应用中由于随机矩阵奇异值缩小可能引发的问题。

Method: 通过理论和实证分析展示随机矩阵中奇异值的缩放行为。

Result: 证实了在大规模下μon优化器的正交化过程中奇异值缩小的现象。

Conclusion: 虽然指出了问题，但未提出具体改进或解决方案。

Abstract: The muon optimizer has picked up much attention as of late as a possible
replacement to the seemingly omnipresent Adam optimizer. Recently, care has
been taken to document the scaling laws of hyper-parameters under muon such as
weight decay and learning rate. However, at much larger scales the iterative
orthogonalization procedure present in muon may suffer a possible issue as the
singular values of random matrices shrink with scale. This paper shows this
scaling behavior theoretically and empirically on random matrices but does not
suggest what to do about it.

</details>


### [74] [Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks](https://arxiv.org/abs/2505.04046)
*Xuyang Wang,Siyuan Duan,Qizhi Li,Guiduo Duan,Yuan Sun,Dezhong Peng*

Main category: cs.LG

TL;DR: 该论文提出了一种名为RDML的新型多视图学习框架，通过证据解缠学习和特征重校准模块来应对对抗性扰动，提升多视图学习的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的可信多视图学习方法假设数据安全，而实际应用中对抗性扰动会威胁模型可靠性。为了解决这一问题，作者提出了RDML框架。

Method: RDML通过证据解缠学习分解视图为干净和对抗部分，利用预训练证据提取器；采用特征重校准模块减少对抗扰动影响；并使用视图级证据注意力机制忽略不可修复的干扰。

Result: 实验表明，RDML在对抗攻击下的多视图分类任务中显著优于现有方法。

Conclusion: RDML框架有效地提升了多视图学习在对抗性攻击下的可靠性和性能。

Abstract: Recently, trustworthy multi-view learning has attracted extensive attention
because evidence learning can provide reliable uncertainty estimation to
enhance the credibility of multi-view predictions. Existing trusted multi-view
learning methods implicitly assume that multi-view data is secure. In practice,
however, in safety-sensitive applications such as autonomous driving and
security monitoring, multi-view data often faces threats from adversarial
perturbations, thereby deceiving or disrupting multi-view learning models. This
inevitably leads to the adversarial unreliability problem (AUP) in trusted
multi-view learning. To overcome this tricky problem, we propose a novel
multi-view learning framework, namely Reliable Disentanglement Multi-view
Learning (RDML). Specifically, we first propose evidential disentanglement
learning to decompose each view into clean and adversarial parts under the
guidance of corresponding evidences, which is extracted by a pretrained
evidence extractor. Then, we employ the feature recalibration module to
mitigate the negative impact of adversarial perturbations and extract potential
informative features from them. Finally, to further ignore the irreparable
adversarial interferences, a view-level evidential attention mechanism is
designed. Extensive experiments on multi-view classification tasks with
adversarial attacks show that our RDML outperforms the state-of-the-art
multi-view learning methods by a relatively large margin.

</details>


### [75] [LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?](https://arxiv.org/abs/2505.04075)
*Teddy Foley,Spencer Guo,Henry Josephson,Anqi Qu,Jack Sanderson*

Main category: cs.LG

TL;DR: 论文探讨了语言模型发展是否可以在不增加计算资源的情况下通过算法创新继续进步，提出了计算等价增益（CEG）作为衡量标准，并通过小规模GPT-2实验验证了计算无关的算法改进在资源受限环境下仍能带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 受监管机构限制高性能硬件访问的推动，研究旨在分析在计算资源受限的环境下，算法创新如何推动语言模型的发展。

Method: 提出了一种分类框架区分计算依赖和计算无关的算法创新，并引入CEG指标量化这些创新的贡献。通过小规模GPT-2训练实验验证框架。

Result: 实验表明计算无关的算法改进在资源受限时能带来显著性能提升（CEG达3.5倍），而计算依赖的改进在小规模下效果有限或甚至降低性能。

Conclusion: 计算资源的可用性对某些算法创新至关重要，但计算无关的算法改进能在资源受限时持续推动语言模型进步。

Abstract: This paper examines whether large language model (LLM) capabilities can
continue to advance without additional compute by analyzing the development and
role of algorithms used in state-of-the-art LLMs. Motivated by regulatory
efforts that have largely focused on restricting access to high-performance
hardware, we ask: Can LLMs progress in a compute-constrained environment, and
how do algorithmic innovations perform under such conditions?
  To address these questions, we introduce a novel classification framework
that distinguishes between compute-dependent innovations -- which yield
disproportionate benefits at high compute levels (e.g., the Transformer
architecture and mixture-of-experts models) and compute-independent
innovations, which improve efficiency across all compute scales (e.g., rotary
positional encoding, FlashAttention, or layer normalization). We quantify these
contributions using a metric called compute-equivalent gain (CEG), which
estimates the additional compute that would be required to achieve similar
improvements without these algorithmic advancements.
  To validate this framework, we conduct small-scale training experiments with
a scaled-down GPT-2 model. Our results confirm that compute-independent
advancements yield meaningful performance gains even in resource-constrained
settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast,
compute-dependent advancements provided little benefit or even degraded
performance at the small scale, reinforcing the importance of compute
availability for certain algorithmic gains.

</details>


### [76] [Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training](https://arxiv.org/abs/2505.04083)
*Aditya K. Ranjan,Siddharth Singh,Cunyang Wei,Abhinav Bhatele*

Main category: cs.LG

TL;DR: Plexus是一种新的三维并行方法，用于全图训练，解决了大规模图上的GPU内存限制和分布式训练的问题，并在多个数据集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实中的图数据规模庞大，超出了GPU内存容量，现有方法如分批采样可能降低准确性，而分布式全图训练则因通信开销和负载不均效率低下。

Method: Plexus采用三维并行方法，引入负载均衡的排列方案和性能模型预测最优配置，支持数十亿边规模的图训练。

Result: 在Perlmutter和Frontier上，Plexus实现了2.3x-12.5x的速度提升，并将解决时间缩短了5.2-8.7x和7-54.2x。

Conclusion: Plexus是一种高效的大规模图训练方法，能显著提升训练速度和资源利用率。

Abstract: Graph neural networks have emerged as a potent class of neural networks
capable of leveraging the connectivity and structure of real-world graphs to
learn intricate properties and relationships between nodes. Many real-world
graphs exceed the memory capacity of a GPU due to their sheer size, and using
GNNs on them requires techniques such as mini-batch sampling to scale. However,
this can lead to reduced accuracy in some cases, and sampling and data transfer
from the CPU to the GPU can also slow down training. On the other hand,
distributed full-graph training suffers from high communication overhead and
load imbalance due to the irregular structure of graphs. We propose Plexus, a
three-dimensional (3D) parallel approach for full-graph training that tackles
these issues and scales to billion-edge graphs. Additionally, we introduce
optimizations such as a permutation scheme for load balancing, and a
performance model to predict the optimal 3D configuration. We evaluate Plexus
on several graph datasets and show scaling results for up to 2048 GPUs on
Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus
achieves unprecedented speedups of 2.3x-12.5x over existing methods and a
reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on
Frontier.

</details>


### [77] [Position: We need responsible, application-driven (RAD) AI research](https://arxiv.org/abs/2505.04104)
*Sarah Hartman,Cheng Soon Ong,Julia Powles,Petra Kuhnert*

Main category: cs.LG

TL;DR: 这篇立场论文主张，通过负责任、应用驱动的方法（RAD-AI）推动人工智能研究，以实现科学和社会进步。


<details>
  <summary>Details</summary>
Motivation: 随着AI日益融入社会，研究人员需关注AI应用的具体背景，包括伦理、法律、技术和社会约束以及公共讨论。

Method: 采用三阶段方法：组建跨学科团队和以人为本的研究；解决特定情境的方法、伦理承诺、假设和指标；通过分阶段测试和实践社区验证和维持效果。

Result: 提出应用驱动AI研究的未来愿景，通过技术上可行且适应社区需求和价值观的方法创造新价值。

Conclusion: 倡导RAD-AI方法，确保AI研究在技术和伦理上都能为社会带来实质性进步。

Abstract: This position paper argues that achieving meaningful scientific and societal
advances with artificial intelligence (AI) requires a responsible,
application-driven approach (RAD) to AI research. As AI is increasingly
integrated into society, AI researchers must engage with the specific contexts
where AI is being applied. This includes being responsive to ethical and legal
considerations, technical and societal constraints, and public discourse. We
present the case for RAD-AI to drive research through a three-staged approach:
(1) building transdisciplinary teams and people-centred studies; (2) addressing
context-specific methods, ethical commitments, assumptions, and metrics; and
(3) testing and sustaining efficacy through staged testbeds and a community of
practice. We present a vision for the future of application-driven AI research
to unlock new value through technically feasible methods that are adaptive to
the contextual needs and values of the communities they ultimately serve.

</details>


### [78] [Alpha Excel Benchmark](https://arxiv.org/abs/2505.04110)
*David Noever,Forrest McKee*

Main category: cs.LG

TL;DR: 该研究提出了一个基于金融建模世界杯(FMWC) Excel竞赛的新基准，用于评估大型语言模型(LLMs)在实际商业任务中的表现，结果显示不同模型在模式识别和复杂数值推理方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准主要集中在抽象学术问题，缺乏对实际商业应用的评估。通过将FMWC Excel竞赛的挑战转化为可编程评估格式，填补了学术基准与商业应用之间的差距。

Method: 将113个FMWC挑战转化为可编程的JSON格式数据集，并用于测试多个主流LLMs的表现。

Result: 模型在不同挑战类别中表现差异显著，擅长模式识别任务，但在复杂数值推理方面表现较弱。

Conclusion: 该基准为标准化的LLMs能力评估提供了新工具，尤其适用于商业导向任务，扩展了AI评估的实际应用范围。

Abstract: This study presents a novel benchmark for evaluating Large Language Models
(LLMs) using challenges derived from the Financial Modeling World Cup (FMWC)
Excel competitions. We introduce a methodology for converting 113 existing FMWC
challenges into programmatically evaluable JSON formats and use this dataset to
compare the performance of several leading LLMs. Our findings demonstrate
significant variations in performance across different challenge categories,
with models showing specific strengths in pattern recognition tasks but
struggling with complex numerical reasoning. The benchmark provides a
standardized framework for assessing LLM capabilities in realistic
business-oriented tasks rather than abstract academic problems. This research
contributes to the growing field of AI benchmarking by establishing proficiency
among the 1.5 billion people who daily use Microsoft Excel as a meaningful
evaluation metric that bridges the gap between academic AI benchmarks and
practical business applications.

</details>


### [79] [LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification](https://arxiv.org/abs/2505.04139)
*Hongyi Li,Jun Xu,William Ward Armstrong*

Main category: cs.LG

TL;DR: 论文提出了一种名为LHT的新型斜决策树模型，通过非迭代的统计驱动方法构建分割超平面，兼具表达力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统斜决策树依赖迭代优化或启发式方法，而LHT旨在通过确定性机制直接计算超平面参数，提升分类效果并保持可解释性。

Method: LHT通过类间特征期望差异直接计算超平面参数，叶子节点采用局部最小二乘拟合的分段线性隶属函数。

Result: 实验证明LHT在基准数据集上具有竞争力，时间复杂度为O(mnd)，适用于构建深度树。

Conclusion: LHT是一种理论扎实、实用且可解释的斜决策树模型。

Abstract: We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision
tree model designed for expressive and interpretable classification. LHT
fundamentally distinguishes itself through a non-iterative,
statistically-driven approach to constructing splitting hyperplanes. Unlike
methods that rely on iterative optimization or heuristics, LHT directly
computes the hyperplane parameters, which are derived from feature weights
based on the differences in feature expectations between classes within each
node. This deterministic mechanism enables a direct and well-defined hyperplane
construction process. Predictions leverage a unique piecewise linear membership
function within leaf nodes, obtained via local least-squares fitting. We
formally analyze the convergence of the LHT splitting process, ensuring that
each split yields meaningful, non-empty partitions. Furthermore, we establish
that the time complexity for building an LHT up to depth $d$ is $O(mnd)$,
demonstrating the practical feasibility of constructing trees with powerful
oblique splits using this methodology. The explicit feature weighting at each
split provides inherent interpretability. Experimental results on benchmark
datasets demonstrate LHT's competitive accuracy, positioning it as a practical,
theoretically grounded, and interpretable alternative in the landscape of
tree-based models. The implementation of the proposed method is available at
https://github.com/Hongyi-Li-sz/LHT_model.

</details>


### [80] [FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04158)
*Yulong Wang,Yushuo Liu,Xiaoyi Duan,Kai Wang*

Main category: cs.LG

TL;DR: FilterTS是一种新颖的多变量时间序列预测模型，通过动态和静态滤波模块在频域中提取复杂周期和趋势成分，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉多变量时间序列中复杂的周期和趋势模式，因此需要一种更高效的方法来提取这些模式并提升预测准确性。

Method: FilterTS采用动态跨变量滤波模块和静态全局滤波模块在频域中提取特征，并将时间域卷积转换为频域乘法运算以提高计算效率。

Result: 在八个真实数据集上的实验表明，FilterTS在预测准确性和计算效率上均显著优于现有方法。

Conclusion: FilterTS通过频域滤波和动态跨变量交互，成功提升了多变量时间序列预测的性能，具有实际应用价值。

Abstract: Multivariate time series forecasting is crucial across various industries,
where accurate extraction of complex periodic and trend components can
significantly enhance prediction performance. However, existing models often
struggle to capture these intricate patterns. To address these challenges, we
propose FilterTS, a novel forecasting model that utilizes specialized filtering
techniques based on the frequency domain. FilterTS introduces a Dynamic
Cross-Variable Filtering Module, a key innovation that dynamically leverages
other variables as filters to extract and reinforce shared variable frequency
components across variables in multivariate time series. Additionally, a Static
Global Filtering Module captures stable frequency components, identified
throughout the entire training set. Moreover, the model is built in the
frequency domain, converting time-domain convolutions into frequency-domain
multiplicative operations to enhance computational efficiency. Extensive
experimental results on eight real-world datasets have demonstrated that
FilterTS significantly outperforms existing methods in terms of prediction
accuracy and computational efficiency.

</details>


### [81] [Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic data](https://arxiv.org/abs/2505.04161)
*Baida Zhang,Yakai Chen,Huichun Li,Zhenghu Zu*

Main category: cs.LG

TL;DR: 该论文提出了一个基于个体代理传播模型的强化学习决策框架，用于优化传染病干预措施，并通过实验和理论验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 全球传染病爆发对健康安全和经济造成严重影响，现有研究多基于微分方程模型，简化限制了模型的复杂性和动态性。

Method: 利用强化学习在个体代理模型（如修改后的Covasim模型）中探索策略函数，并对多种算法在不同动作空间的应用效果进行了全面探索。

Result: 实验结果验证了框架的有效性，所获策略能有效抑制疫情扩散并维护经济系统稳定。

Conclusion: 该研究为全球公共卫生安全策略制定提供了重要参考，强化了干预措施的可行性和效果。

Abstract: Globally, the outbreaks of infectious diseases have exerted an extremely
profound and severe influence on health security and the economy. During the
critical phases of epidemics, devising effective intervention measures poses a
significant challenge to both the academic and practical arenas. There is
numerous research based on reinforcement learning to optimize intervention
measures of infectious diseases. Nevertheless, most of these efforts have been
confined within the differential equation based on infectious disease models.
Although a limited number of studies have incorporated reinforcement learning
methodologies into individual-based infectious disease models, the models
employed therein have entailed simplifications and limitations, rendering it
incapable of modeling the complexity and dynamics inherent in infectious
disease transmission. We establish a decision-making framework based on an
individual agent-based transmission model, utilizing reinforcement learning to
continuously explore and develop a strategy function. The framework's validity
is verified through both experimental and theoretical approaches. Covasim, a
detailed and widely used agent-based disease transmission model, was modified
to support reinforcement learning research. We conduct an exhaustive
exploration of the application efficacy of multiple algorithms across diverse
action spaces. Furthermore, we conduct an innovative preliminary theoretical
analysis concerning the issue of "time coverage". The results of the experiment
robustly validate the effectiveness and feasibility of the methodological
framework of this study. The coping strategies gleaned therefrom prove highly
efficacious in suppressing the expansion of the epidemic scale and safeguarding
the stability of the economic system, thereby providing crucial reference
perspectives for the formulation of global public health security strategies.

</details>


### [82] [Retrieval Augmented Time Series Forecasting](https://arxiv.org/abs/2505.04163)
*Sungwon Han,Seungeon Lee,Meeyoung Cha,Sercan O Arik,Jinsung Yoon*

Main category: cs.LG

TL;DR: RAFT是一种基于检索的时间序列预测方法，通过从训练数据中检索相似历史模式来增强模型预测能力，实验证明其在十个基准数据集上平均胜率达到86%。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测通常依赖历史数据与特征的关系，RAFT旨在通过检索相似历史模式为模型提供额外的归纳偏置，弥补其学习能力的不足。

Method: RAFT在预测时直接检索训练数据中与输入最相似的历史模式，并利用这些模式的未来值结合输入生成预测。

Result: 在十个基准数据集上，RAFT的平均胜率为86%，显著优于现有基线方法。

Conclusion: RAFT通过检索历史数据有效提升了时间序列预测性能，证明了外部检索机制对模型增强的价值。

Abstract: Time series forecasting uses historical data to predict future trends,
leveraging the relationships between past observations and available features.
In this paper, we propose RAFT, a retrieval-augmented time series forecasting
method to provide sufficient inductive biases and complement the model's
learning capacity. When forecasting the subsequent time frames, we directly
retrieve historical data candidates from the training dataset with patterns
most similar to the input, and utilize the future values of these candidates
alongside the inputs to obtain predictions. This simple approach augments the
model's capacity by externally providing information about past patterns via
retrieval modules. Our empirical evaluations on ten benchmark datasets show
that RAFT consistently outperforms contemporary baselines with an average win
ratio of 86%.

</details>


### [83] [STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.04167)
*Yulong Wang,Xiaofeng Hu,Xiaojian Cui,Kai Wang*

Main category: cs.LG

TL;DR: STRGCN模型直接在图上处理不规则多元时间序列，避免了预对齐的数据扭曲问题，通过分层结构优化计算效率，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列（IMTS）因传感器频率不同和异步测量带来建模挑战，现有方案依赖预对齐会破坏数据固有模式并增加计算负担。

Method: 提出STRGCN模型，将IMTS表示为全连接图，节点对应观测值，直接捕捉复杂依赖；采用分层“三明治”结构优化节点聚合，平衡计算效率和上下文保留。

Result: 在四个公开数据集上，STRGCN达到最优准确率，同时保持竞争力的内存使用和训练速度。

Conclusion: STRGCN无需预对齐即可高效建模IMTS的异步特性，为实际应用提供了一种高精度、低开销的解决方案。

Abstract: Irregular multivariate time series (IMTS) are prevalent in real-world
applications across many fields, where varying sensor frequencies and
asynchronous measurements pose significant modeling challenges. Existing
solutions often rely on a pre-alignment strategy to normalize data, which can
distort intrinsic patterns and escalate computational and memory demands.
Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational
Graph Convolutional Network that avoids pre-alignment and directly captures the
complex interdependencies in IMTS by representing them as a fully connected
graph. Each observation is represented as a node, allowing the model to
effectively handle misaligned timestamps by mapping all inter-node
relationships, thus faithfully preserving the asynchronous nature of the data.
Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that
strategically aggregates nodes to optimize graph embeddings, reducing
computational overhead while maintaining detailed local and global context.
Extensive experiments on four public datasets demonstrate that STRGCN achieves
state-of-the-art accuracy, competitive memory usage and training speed.

</details>


### [84] [DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion](https://arxiv.org/abs/2505.04173)
*Zixiao Wang,Wenqian Zhao,Yunheng Shen,Yang Bai,Guojin Chen,Farzan Farnia,Bei Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DiffPattern-Flex的新方法，利用离散扩散模型生成多样化拓扑布局，并通过优化评估和高效技术确保合法性与效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前布局生成主要依赖深度生成模型，但其合法性保障不足，实用性受限。因此，需设计一种既能高效生成多样化布局，又能确保合法性的方法。

Method: 采用离散扩散模型生成多样化拓扑，结合基于设计规则的白盒优化评估流程，并应用快速采样与合法化技术加速。

Result: 实验结果表明，DiffPattern-Flex在多个基准测试中显著优于现有方法，能高效生成可靠布局。

Conclusion: DiffPattern-Flex通过融合扩散模型与优化技术，解决了布局生成的合法性与效率问题，为实际应用提供了可靠方案。

Abstract: Recent advancements in layout pattern generation have been dominated by deep
generative models. However, relying solely on neural networks for legality
guarantees raises concerns in many practical applications. In this paper, we
present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable
layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method
for generating diverse topologies using a discrete diffusion model while
maintaining a lossless and compute-efficient layout representation. To ensure
legal pattern generation, we employ {an} optimization-based, white-box pattern
assessment process based on specific design rules. Furthermore, fast sampling
and efficient legalization technologies are employed to accelerate the
generation process. Experimental results across various benchmarks demonstrate
that \tool{DiffPattern}-Flex significantly outperforms existing methods and
excels at producing reliable layout patterns.

</details>


### [85] [On-Device LLM for Context-Aware Wi-Fi Roaming](https://arxiv.org/abs/2505.04174)
*Ju-Hyung Lee,Yanqing Lu*

Main category: cs.LG

TL;DR: 论文提出了一种利用设备端大语言模型（LLM）实现无线漫游优化的跨层方法，通过上下文感知接入点选择和动态阈值调整，显著提升了漫游稳定性和信号质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值或启发式的无线漫游方案在动态移动环境中表现不佳，容易导致粘滞或过度切换。论文旨在通过LLM的高层推理能力优化漫游决策。

Method: 采用跨层设计，利用LLM实现上下文感知的接入点选择（融合环境信息）和动态阈值调整，并通过思维链提示、参数高效微调和量化等技术优化模型以适应边缘硬件。

Result: 实验表明，该方法在室内外数据集中均优于传统启发式和深度强化学习基线，实现了漫游稳定性与信号质量的良好平衡。

Conclusion: 研究证明了应用层LLM推理在底层无线控制中的潜力，为未来边缘系统的优化提供了新思路。

Abstract: Wireless roaming is a critical yet challenging task for maintaining seamless
connectivity in dynamic mobile environments. Conventional threshold-based or
heuristic schemes often fail, leading to either sticky or excessive handovers.
We introduce the first cross-layer use of an on-device large language model
(LLM): high-level reasoning in the application layer that issues real-time
actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i)
context-aware AP selection, where structured prompts fuse environmental cues
(e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold
adjustment, where the model adaptively decides when to roam. To satisfy the
tight latency and resource budgets of edge hardware, we apply a suite of
optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and
quantization. Experiments on indoor and outdoor datasets show that our approach
surpasses legacy heuristics and DRL baselines, achieving a strong balance
between roaming stability and signal quality. These findings underscore the
promise of application-layer LLM reasoning for lower-layer wireless control in
future edge systems.

</details>


### [86] [Trajectory Entropy Reinforcement Learning for Predictable and Robust Control](https://arxiv.org/abs/2505.04193)
*Bang You,Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 本文提出了一种强化学习中的简单性归纳偏置方法，通过最小化动作轨迹的熵来提升策略的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂控制任务中表现优异，但容易捕捉观测与动作之间的虚假关联，导致在环境微扰下失效。因此需要引入简单性归纳偏置以提升鲁棒性。

Method: 通过最小化动作轨迹的熵（描述信息所需的比特数），结合变分参数化动作预测模型估计轨迹熵，并构建信息正则化的奖励函数，联合优化策略和预测模型。

Result: 实验表明，该方法在高维运动任务中生成更具周期性和一致性的动作轨迹，性能及对噪声和动态变化的鲁棒性优于现有技术。

Conclusion: 轨迹熵强化学习通过引入简单性偏置，显著提升了策略的鲁棒性和性能，尤其在复杂动态环境中表现优异。

Abstract: Simplicity is a critical inductive bias for designing data-driven
controllers, especially when robustness is important. Despite the impressive
results of deep reinforcement learning in complex control tasks, it is prone to
capturing intricate and spurious correlations between observations and actions,
leading to failure under slight perturbations to the environment. To tackle
this problem, in this work we introduce a novel inductive bias towards simple
policies in reinforcement learning. The simplicity inductive bias is introduced
by minimizing the entropy of entire action trajectories, corresponding to the
number of bits required to describe information in action trajectories after
the agent observes state trajectories. Our reinforcement learning agent,
Trajectory Entropy Reinforcement Learning, is optimized to minimize the
trajectory entropy while maximizing rewards. We show that the trajectory
entropy can be effectively estimated by learning a variational parameterized
action prediction model, and use the prediction model to construct an
information-regularized reward function. Furthermore, we construct a practical
algorithm that enables the joint optimization of models, including the policy
and the prediction model. Experimental evaluations on several high-dimensional
locomotion tasks show that our learned policies produce more cyclical and
consistent action trajectories, and achieve superior performance, and
robustness to noise and dynamic changes than the state-of-the-art.

</details>


### [87] [A Large Language Model for Feasible and Diverse Population Synthesis](https://arxiv.org/abs/2505.04196)
*Sung Yoo Lim,Hyunsoo Yun,Prateek Bansal,Dong-Kyu Kim,Eui-Jin Kim*

Main category: cs.LG

TL;DR: 提出了一种基于轻量级开源大语言模型（LLM）和贝叶斯网络（BN）的微调方法，用于生成既可行又多样化的合成人口，显著提高了活动基准模型（ABM）下游模拟的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度生成模型（DGM）在生成合成人口时难以平衡稀有但合理的组合（采样零）与排除不合理组合（结构零）的问题。

Method: 通过贝叶斯网络（BN）的拓扑顺序显式控制LLM的自回归生成过程，结合微调技术，生成高质量的合成人口数据。

Result: 实验表明，该方法可行性达95%，显著优于传统DGM的80%，同时保持多样性，适用于实际应用。

Conclusion: 该方法的轻量级开源特性使其成本低且可扩展，适合大规模应用（如超大城市人口合成），并通过高质量的合成人口改善了ABM模拟的可靠性。

Abstract: Generating a synthetic population that is both feasible and diverse is
crucial for ensuring the validity of downstream activity schedule simulation in
activity-based models (ABMs). While deep generative models (DGMs), such as
variational autoencoders and generative adversarial networks, have been applied
to this task, they often struggle to balance the inclusion of rare but
plausible combinations (i.e., sampling zeros) with the exclusion of implausible
ones (i.e., structural zeros). To improve feasibility while maintaining
diversity, we propose a fine-tuning method for large language models (LLMs)
that explicitly controls the autoregressive generation process through
topological orderings derived from a Bayesian Network (BN). Experimental
results show that our hybrid LLM-BN approach outperforms both traditional DGMs
and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically,
our approach achieves approximately 95% feasibility, significantly higher than
the ~80% observed in DGMs, while maintaining comparable diversity, making it
well-suited for practical applications. Importantly, the method is based on a
lightweight open-source LLM, enabling fine-tuning and inference on standard
personal computing environments. This makes the approach cost-effective and
scalable for large-scale applications, such as synthesizing populations in
megacities, without relying on expensive infrastructure. By initiating the ABM
pipeline with high-quality synthetic populations, our method improves overall
simulation reliability and reduces downstream error propagation. The source
code for these methods is available for research and practical application.

</details>


### [88] [Estimating Causal Effects in Networks with Cluster-Based Bandits](https://arxiv.org/abs/2505.04200)
*Ahmed Sayeed Faruk,Jason Sulskis,Elena Zheleva*

Main category: cs.LG

TL;DR: 论文提出了两种基于聚类的多臂老虎机（MAB）算法，用于在网络中逐步估计总治疗效应并最大化预期奖励，通过权衡探索与利用。与传统方法和忽略聚类的MAB相比，算法在奖励-行动比上表现更优且治疗效应估计准确。


<details>
  <summary>Details</summary>
Motivation: 在社交网络中，A/B测试面临干扰问题和高性能损失，因此需要设计能适应并高效学习网络总治疗效应的策略。

Method: 引入两种基于聚类的MAB算法，权衡探索与利用，逐步估计总治疗效应，并与忽略聚类的MAB及RCT方法进行对比。

Result: 基于聚类的MAB算法在奖励-行动比上优于RCT方法且治疗效应估计准确，而忽略聚类的MAB虽奖励-行动比高，却导致更大的治疗效应误差。

Conclusion: 基于聚类的MAB算法能有效应对网络干扰问题，实现高奖励-行动比和准确的治疗效应估计，优于传统方法。

Abstract: The gold standard for estimating causal effects is randomized controlled
trial (RCT) or A/B testing where a random group of individuals from a
population of interest are given treatment and the outcome is compared to a
random group of individuals from the same population. However, A/B testing is
challenging in the presence of interference, commonly occurring in social
networks, where individuals can impact each others outcome. Moreover, A/B
testing can incur a high performance loss when one of the treatment arms has a
poor performance and the test continues to treat individuals with it.
Therefore, it is important to design a strategy that can adapt over time and
efficiently learn the total treatment effect in the network. We introduce two
cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the
total treatment effect in a network while maximizing the expected reward by
making a tradeoff between exploration and exploitation. We compare the
performance of our MAB algorithms with a vanilla MAB algorithm that ignores
clusters and the corresponding RCT methods on semi-synthetic data with
simulated interference. The vanilla MAB algorithm shows higher reward-action
ratio at the cost of higher treatment effect error due to undesired spillover.
The cluster-based MAB algorithms show higher reward-action ratio compared to
their corresponding RCT methods without sacrificing much accuracy in treatment
effect estimation.

</details>


### [89] [Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets](https://arxiv.org/abs/2505.04204)
*Mateo Lopez-Ledezma,Gissel Velarde*

Main category: cs.LG

TL;DR: 该论文探讨了自动化在网络安全中的重要性，重点研究了二进制分类问题。通过三个实验评估了不同单分类器和不平衡学习技术，发现不同数据集表现最佳的方法不同，建议针对新数据集进行单独测试。


<details>
  <summary>Details</summary>
Motivation: 网络安全日益重要，自动化在处理大规模操作中尤为关键。二进制分类问题（如异常检测、入侵检测等）是网络安全的核心，但数据集不平衡问题需要解决。

Method: 论文设计了三个实验：1) 评估单分类器（如随机森林、XGBoost等）；2) 测试不同采样技术（过采样、欠采样等）；3) 评估Self-Paced Ensembling及其基分类器数量。

Result: 不平衡学习技术效果因数据集而异，某些情况下可能产生负面效果。不同数据集的最佳性能方法不同，没有一种技术适用于所有场景。

Conclusion: 建议在实际应用中对新数据集分别测试单分类器和不平衡学习技术，以确保最佳效果，特别是在网络安全这类不平衡数据集中。

Abstract: Cybersecurity has become essential worldwide and at all levels, concerning
individuals, institutions, and governments. A basic principle in cybersecurity
is to be always alert. Therefore, automation is imperative in processes where
the volume of daily operations is large. Several cybersecurity applications can
be addressed as binary classification problems, including anomaly detection,
fraud detection, intrusion detection, spam detection, or malware detection. We
present three experiments. In the first experiment, we evaluate single
classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme
Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting
Decision Tree. In the second experiment, we test different sampling techniques
including over-sampling, under-sampling, Synthetic Minority Over-sampling
Technique, and Self-Paced Ensembling. In the last experiment, we evaluate
Self-Paced Ensembling and its number of base classifiers. We found that
imbalance learning techniques had positive and negative effects, as reported in
related studies. Thus, these techniques should be applied with caution.
Besides, we found different best performers for each dataset. Therefore, we
recommend testing single classifiers and imbalance learning techniques for each
new dataset and application involving imbalanced datasets as is the case in
several cyber security applications.

</details>


### [90] [FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning](https://arxiv.org/abs/2505.04223)
*Sanghyeon Park,Soo-Mook Moon*

Main category: cs.LG

TL;DR: FRAIN是一种新的异步联邦学习方法，通过FastSync策略和球形线性插值（SLERP）解决现有方法在非独立同分布数据、延迟和恶意节点下的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法（如FedAvg和FedAsync）在非独立同分布数据、延迟或恶意节点下表现不佳，FRAIN旨在解决这些问题。

Method: FRAIN采用FastSync策略避免重放历史模型，并使用SLERP合并参数以减少干扰。

Result: 实验表明，FRAIN在非独立同分布数据、延迟和恶意节点环境下比FedAvg、FedAsync和BRAIN表现更稳定。

Conclusion: FRAIN通过高效同步和参数合并策略，提升了异步联邦学习的鲁棒性和稳定性。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data locality. Although FedAvg pioneered
synchronous rounds for global model averaging, slower devices can delay
collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by
continuously integrating client updates, yet naive implementations risk client
drift due to non-IID data and stale contributions. Some Blockchain-based FL
approaches (e.g., BRAIN) employ robust weighting or scoring of updates to
resist malicious or misaligned proposals. However, performance drops can still
persist under severe data heterogeneity or high staleness, and synchronization
overhead has emerged as a new concern due to its aggregator-free architectures.
  We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL
method that mitigates these limitations by incorporating two key ideas. First,
our FastSync strategy eliminates the need to replay past model versions,
enabling newcomers and infrequent participants to efficiently approximate the
global model. Second, we adopt spherical linear interpolation (SLERP) when
merging parameters, preserving models' directions and alleviating destructive
interference from divergent local training.
  Experiments with a CNN image-classification model and a Transformer-based
language model demonstrate that FRAIN achieves more stable and robust
convergence than FedAvg, FedAsync, and BRAIN, especially under harsh
environments: non-IID data distributions, networks that experience delays and
require frequent re-synchronization, and the presence of malicious nodes.

</details>


### [91] [Technology prediction of a 3D model using Neural Network](https://arxiv.org/abs/2505.04241)
*Grzegorz Miebs,Rafał A. Bachorz*

Main category: cs.LG

TL;DR: 该论文提出了一种基于数据驱动的方法，通过产品的3D模型直接预测制造步骤及其时间，利用神经网络的图像处理能力，实现跨产品类型的精确和自适应的工艺规划。该方法在动态或定制化生产环境中弥补了传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖专家分析或历史数据，难以适应动态或定制化生产环境的需求，迫切需要一种自动化和准确的制造时间预测方法。

Method: 通过将3D模型转换为多张2D图像，并使用受生成查询网络启发的神经网络，将几何特征映射为预定义生产步骤的时间估计。

Result: 该方法能够实现跨产品类型的可扩展、自适应和精确的工艺规划。

Conclusion: 数据驱动的方法在制造业中展现出了潜力，能够显著提升生产调度的效率和准确性。

Abstract: Accurate estimation of production times is critical for effective
manufacturing scheduling, yet traditional methods relying on expert analysis or
historical data often fall short in dynamic or customized production
environments. This paper introduces a data-driven approach that predicts
manufacturing steps and their durations directly from a product's 3D model. By
rendering the model into multiple 2D images and leveraging a neural network
inspired by the Generative Query Network, the method learns to map geometric
features into time estimates for predefined production steps enabling scalable,
adaptive, and precise process planning across varied product types.

</details>


### [92] [Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification](https://arxiv.org/abs/2505.04263)
*Jan Blechschmidt,Tom-Christian Riemer,Max Winkler,Martin Stoll,Jan-F. Pietschmann*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理信息的深度学习方法，用于解决度量图上的非线性漂移-扩散方程，并通过边缘域分解方法耦合DeepONet模型。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在模型设计或参数识别问题中需要大量定制，而物理信息深度算子网络（DeepONet）因其在处理偏微分方程时的灵活性和易于参数识别的特性而受到关注。

Method: 首先分别学习代表流入边、内部边和流出边的三个DeepONet模型，然后通过边缘域分解方法耦合这些模型以解决问题。

Result: 框架能够准确评估图耦合物理模型，并适用于解决这些耦合网络的优化或逆问题。

Conclusion: 提出的方法在解决度量图上的非线性漂移-扩散方程方面表现优越，尤其适合优化和参数识别问题。

Abstract: We develop a novel physics informed deep learning approach for solving
nonlinear drift-diffusion equations on metric graphs. These models represent an
important model class with a large number of applications in areas ranging from
transport in biological cells to the motion of human crowds. While traditional
numerical schemes require a large amount of tailoring, especially in the case
of model design or parameter identification problems, physics informed deep
operator networks (DeepONet) have emerged as a versatile tool for the solution
of partial differential equations with the particular advantage that they
easily incorporate parameter identification questions. We here present an
approach where we first learn three DeepONet models for representative inflow,
inner and outflow edges, resp., and then subsequently couple these models for
the solution of the drift-diffusion metric graph problem by relying on an
edge-based domain decomposition approach. We illustrate that our framework is
applicable for the accurate evaluation of graph-coupled physics models and is
well suited for solving optimization or inverse problems on these coupled
networks.

</details>


### [93] [Non-stationary Diffusion For Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.04278)
*Weiwei Ye,Zhuopeng Xu,Ning Gui*

Main category: cs.LG

TL;DR: 该论文提出了一种名为NsDiff的非平稳扩散模型，利用位置-尺度噪声模型（LSNM）替代传统DDPM的固定方差假设，以更准确地建模时间序列中的不确定性变化。


<details>
  <summary>Details</summary>
Motivation: 传统的DDPM因基于加性噪声模型（ANM）而无法捕捉时间序列中随时间变化的不确定性，限制了其在实际应用中的表现。

Method: NsDiff结合基于LSNM的扩散模型和预训练的条件均值与方差估计器，提出动态调整噪声水平的不确定性感知噪声调度机制。

Result: 在九个真实世界和合成数据集上的实验表明，NsDiff性能优于现有方法。

Conclusion: NsDiff成功地建模了非平稳不确定性，为时间序列预测提供了更灵活的框架。

Abstract: Due to the dynamics of underlying physics and external influences, the
uncertainty of time series often varies over time. However, existing Denoising
Diffusion Probabilistic Models (DDPMs) often fail to capture this
non-stationary nature, constrained by their constant variance assumption from
the additive noise model (ANM). In this paper, we innovatively utilize the
Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of
ANM. A diffusion-based probabilistic forecasting framework, termed
Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of
modeling the changing pattern of uncertainty. Specifically, NsDiff combines a
denoising diffusion-based conditional generative model with a pre-trained
conditional mean and variance estimator, enabling adaptive endpoint
distribution modeling. Furthermore, we propose an uncertainty-aware noise
schedule, which dynamically adjusts the noise levels to accurately reflect the
data uncertainty at each step and integrates the time-varying variances into
the diffusion process. Extensive experiments conducted on nine real-world and
synthetic datasets demonstrate the superior performance of NsDiff compared to
existing approaches. Code is available at https://github.com/wwy155/NsDiff.

</details>


### [94] [Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing](https://arxiv.org/abs/2505.04318)
*Jacob Glenn Ayers,Buvaneswari A. Ramanan,Manzoor A. Khan*

Main category: cs.LG

TL;DR: 本文提出了一种基于χ²拟合优度检验的概念漂移检测方法，用于监控深度学习模型在推理时的数据分布变化，无需直接检查输出，从而提高模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型的广泛应用，其推理数据的分布可能偏离训练数据，导致模型性能下降。目前概念漂移检测方法在监控神经网络时未得到充分利用，因此需要一种适应性强的元算法来确保模型推理的可靠性。

Method: 本文采用χ²拟合优度检验作为概念漂移检测的元算法，并应用于多层感知机、卷积神经网络和Transformer等模型在机器视觉任务中的推理过程，模拟数据分布漂移场景。

Result: 研究表明，该方法能在不直接检查推理输出的情况下，有效检测因概念漂移导致准确率下降的现象。

Conclusion: 提出的χ²检验方法增强了模型在不同条件下的可靠性评估，为深度学习模型的安全监控提供了新思路。

Abstract: As the adoption of deep learning models has grown beyond human capacity for
verification, meta-algorithms are needed to ensure reliable model inference.
Concept drift detection is a field dedicated to identifying statistical shifts
that is underutilized in monitoring neural networks that may encounter
inference data with distributional characteristics diverging from their
training data. Given the wide variety of model architectures, applications, and
datasets, it is important that concept drift detection algorithms are adaptable
to different inference scenarios. In this paper, we introduce an application of
the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection
meta-algorithm applied to a multilayer perceptron, a convolutional neural
network, and a transformer trained for machine vision as they are exposed to
simulated drift during inference. To that end, we demonstrate how unexpected
drops in accuracy due to concept drift can be detected without directly
examining the inference outputs. Our approach enhances safety by ensuring
models are continually evaluated for reliability across varying conditions.

</details>


### [95] [Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces](https://arxiv.org/abs/2505.04335)
*Swagato Das,Arghya Pratihar,Swagatam Das*

Main category: cs.LG

TL;DR: HypeFCM算法是针对非欧几里得空间数据优化的新型模糊聚类方法，结合双曲几何和权重过滤机制，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统模糊聚类（如FCM）在非欧几里得空间中表现不佳，无法有效捕捉复杂或层次化数据结构，因此需要一种新方法。

Method: 提出HypeFCM算法，结合双曲几何（Poincaré Disc模型）和基于Dirichlet分布的权重初始化，迭代优化聚类中心和隶属度。

Result: 实验证明HypeFCM在非欧几里得环境下优于传统模糊聚类方法，表现出更强的鲁棒性和有效性。

Conclusion: HypeFCM为处理非欧几里得空间数据提供了一种高效且准确的模糊聚类解决方案。

Abstract: Clustering algorithms play a pivotal role in unsupervised learning by
identifying and grouping similar objects based on shared characteristics. While
traditional clustering techniques, such as hard and fuzzy center-based
clustering, have been widely used, they struggle with complex,
high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy
$C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits
notable limitations in non-Euclidean spaces. Euclidean spaces assume linear
separability and uniform distance scaling, limiting their effectiveness in
capturing complex, hierarchical, or non-Euclidean structures in fuzzy
clustering. To overcome these challenges, we introduce Filtration-based
Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for
better representation of data relationships in non-Euclidean spaces. HypeFCM
integrates the principles of fuzzy clustering with hyperbolic geometry and
employs a weight-based filtering mechanism to improve performance. The
algorithm initializes weights using a Dirichlet distribution and iteratively
refines cluster centroids and membership assignments based on a hyperbolic
metric in the Poincar\'e Disc model. Extensive experimental evaluations
demonstrate that HypeFCM significantly outperforms conventional fuzzy
clustering methods in non-Euclidean settings, underscoring its robustness and
effectiveness.

</details>


### [96] [Riemannian Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2505.04338)
*Zichen Liu,Wei Zhang,Christof Schütte,Tiejun Li*

Main category: cs.LG

TL;DR: 提出Riemannian去噪扩散概率模型（RDDPMs），用于学习欧几里得空间子流形的分布，仅需函数值和一阶导数信息，适用于更广泛的流形。


<details>
  <summary>Details</summary>
Motivation: 现有基于流形的生成模型依赖几何信息（如测地线或Laplace-Beltrami算子特征函数），限制了其适用性。RDDPMs旨在通过投影方案克服这一局限。

Method: 基于投影方案，仅需子流形定义函数的值和一阶导数，构建连续时间理论分析，连接流形上的分数生成模型。

Result: 在$	ext{SO}(10)$和丙氨酸二肽分子构象空间等数据集上验证了方法的有效性。

Conclusion: RDDPMs为更通用的流形生成建模提供了高效工具，无需复杂几何信息。

Abstract: We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for
learning distributions on submanifolds of Euclidean space that are level sets
of functions, including most of the manifolds relevant to applications.
Existing methods for generative modeling on manifolds rely on substantial
geometric information such as geodesic curves or eigenfunctions of the
Laplace-Beltrami operator and, as a result, they are limited to manifolds where
such information is available. In contrast, our method, built on a projection
scheme, can be applied to more general manifolds, as it only requires being
able to evaluate the value and the first order derivatives of the function that
defines the submanifold. We provide a theoretical analysis of our method in the
continuous-time limit, which elucidates the connection between our RDDPMs and
score-based generative models on manifolds. The capability of our method is
demonstrated on datasets from previous studies and on new datasets sampled from
two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration
space of molecular system alanine dipeptide with fixed dihedral angle.

</details>


### [97] [Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2505.04339)
*Hao Peng,Xiang Huang,Shuo Sun,Ruitong Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于多智能体强化学习的自适应鲁棒DBSCAN聚类框架（AR-DBSCAN），通过密度分区和自动参数搜索，显著提升了聚类精度和参数鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统DBSCAN在处理多密度数据集时表现不佳，无法自适应调整参数。本文旨在通过强化学习实现参数自动优化，解决密度变化带来的聚类挑战。

Method: 1. 将数据集建模为两级编码树，按密度分区；2. 为每个分区分配智能体，通过多智能体强化学习自动搜索最佳参数；3. 设计递归搜索机制以适应数据规模。

Result: 在人工和真实数据集上，AR-DBSCAN的NMI和ARI指标分别提升144.1%和175.3%，且能稳定找到主导参数。

Conclusion: AR-DBSCAN通过强化学习实现了对多密度数据的高效聚类，显著优于传统方法，为实际应用提供了一种鲁棒解决方案。

Abstract: DBSCAN, a well-known density-based clustering algorithm, has gained
widespread popularity and usage due to its effectiveness in identifying
clusters of arbitrary shapes and handling noisy data. However, it encounters
challenges in producing satisfactory cluster results when confronted with
datasets of varying density scales, a common scenario in real-world
applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with
Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,
we model the initial dataset as a two-level encoding tree and categorize the
data vertices into distinct density partitions according to the information
uncertainty determined in the encoding tree. Each partition is then assigned to
an agent to find the best clustering parameters without manual assistance. The
allocation is density-adaptive, enabling AR-DBSCAN to effectively handle
diverse density distributions within the dataset by utilizing distinct agents
for different partitions. Second, a multi-agent deep reinforcement learning
guided automatic parameter searching process is designed. The process of
adjusting the parameter search direction by perceiving the clustering
environment is modeled as a Markov decision process. Using a weakly-supervised
reward training policy network, each agent adaptively learns the optimal
clustering parameters by interacting with the clusters. Third, a recursive
search mechanism adaptable to the data's scale is presented, enabling efficient
and controlled exploration of large parameter spaces. Extensive experiments are
conducted on nine artificial datasets and a real-world dataset. The results of
offline and online tasks show that AR-DBSCAN not only improves clustering
accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,
but also is capable of robustly finding dominant parameters.

</details>


### [98] [Multi-Granular Attention based Heterogeneous Hypergraph Neural Network](https://arxiv.org/abs/2505.04340)
*Hong Jin,Kaicheng Zhou,Jie Yin,Lan You,Zhifeng Zhou*

Main category: cs.LG

TL;DR: HeteGNNs通常通过元路径消息传递学习节点表示，但无法捕捉高阶关系且存在信息失真问题。论文提出MGA-HHN来解决这些问题，通过构建异构超图和多粒度注意力机制，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有HeteGNNs因元路径的成对性无法建模高阶关系，且长距离消息传递导致信息失真，限制了模型效果。

Method: MGA-HHN通过构建元路径异构超图来显式建模高阶语义信息，并设计节点和超边级别的多粒度注意力机制以捕捉细粒度交互。

Result: 实验表明，MGA-HHN在节点分类、聚类和可视化任务上优于现有模型。

Conclusion: MGA-HHN通过高阶语义建模和多粒度注意力机制，有效解决了信息失真问题，提升了异构图表示学习的效果。

Abstract: Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong
abilities to learn node representations by effectively extracting complex
structural and semantic information in heterogeneous graphs. Most of the
prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging
meta-path based message passing to learn latent node representations. However,
due to the pairwise nature of meta-paths, these models fail to capture
high-order relations among nodes, resulting in suboptimal performance.
Additionally, the challenge of ``over-squashing'', where long-range message
passing in HeteGNNs leads to severe information distortion, further limits the
efficacy of these models. To address these limitations, this paper proposes
MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural
Network for heterogeneous graph representation learning. MGA-HHN introduces two
key innovations: (1) a novel approach for constructing meta-path based
heterogeneous hypergraphs that explicitly models higher-order semantic
information in heterogeneous graphs through multiple views, and (2) a
multi-granular attention mechanism that operates at both the node and hyperedge
levels. This mechanism enables the model to capture fine-grained interactions
among nodes sharing the same semantic context within a hyperedge type, while
preserving the diversity of semantics across different hyperedge types. As
such, MGA-HHN effectively mitigates long-range message distortion and generates
more expressive node representations. Extensive experiments on real-world
benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art
models, showcasing its effectiveness in node classification, node clustering
and visualization tasks.

</details>


### [99] [Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration](https://arxiv.org/abs/2505.04346)
*Arghya Pratihar,Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 该论文提出了一种基于拓扑结构的聚类算法，通过Vietoris-Rips复合体和Betti数过滤识别相似邻居，并引入Betti序列捕捉特征，能有效处理复杂数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的拓扑聚类算法未能充分利用拓扑结构，对复杂数据集表现不稳定。作者希望通过更好的拓扑特征提取方法改进聚类性能。

Method: 使用Vietoris-Rips复合体和Betti数过滤识别拓扑相似邻居，并引入Betti序列灵活捕捉关键特征。

Result: 在合成和真实数据集上测试，相比其他拓扑聚类算法表现更优。

Conclusion: 该方法能有效聚类复杂交织形状的数据集，展示了拓扑特征在聚类中的潜力。

Abstract: Clustering aims to form groups of similar data points in an unsupervised
regime. Yet, clustering complex datasets containing critically intertwined
shapes poses significant challenges. The prevailing clustering algorithms
widely depend on evaluating similarity measures based on Euclidean metrics.
Exploring topological characteristics to perform clustering of complex datasets
inevitably presents a better scope. The topological clustering algorithms
predominantly perceive the point set through the lens of Simplicial complexes
and Persistent homology. Despite these approaches, the existing topological
clustering algorithms cannot somehow fully exploit topological structures and
show inconsistent performances on some highly complicated datasets. This work
aims to mitigate the limitations by identifying topologically similar neighbors
through the Vietoris-Rips complex and Betti number filtration. In addition, we
introduce the concept of the Betti sequences to capture flexibly essential
features from the topological structures. Our proposed algorithm is adept at
clustering complex, intertwined shapes contained in the datasets. We carried
out experiments on several synthetic and real-world datasets. Our algorithm
demonstrated commendable performances across the datasets compared to some of
the well-known topology-based clustering algorithms.

</details>


### [100] [Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid](https://arxiv.org/abs/2505.04367)
*Stavros Sykiotis*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过深度学习技术（如非侵入式负载监测和深度强化学习）来减少住宅能耗和优化电动车充电，以加速能源转型。


<details>
  <summary>Details</summary>
Motivation: 全球能源转型面临复杂性和不确定性，需要对高能耗领域（如住宅和道路交通）进行改进，以减少碳排放并加速可持续发展。

Method: 采用深度学习技术，包括非侵入式负载监测（用于住宅能耗）和深度强化学习（用于电动车充电优化）。

Result: 开发了能够减少住宅能耗和优化电动车充电的工具，有助于降低碳排放。

Conclusion: 通过深度学习技术在关键能源领域的应用，可以加速全球能源转型并实现气候目标。

Abstract: The global energy landscape is undergoing a profound transformation, often
referred to as the energy transition, driven by the urgent need to mitigate
climate change, reduce greenhouse gas emissions, and ensure sustainable energy
supplies. However, the undoubted complexity of new investments in renewables,
as well as the phase out of high CO2-emission energy sources, hampers the pace
of the energy transition and raises doubts as to whether new renewable energy
sources are capable of solely meeting the climate target goals. This highlights
the need to investigate alternative pathways to accelerate the energy
transition, by identifying human activity domains with higher/excessive energy
demands. Two notable examples where there is room for improvement, in the sense
of reducing energy consumption and consequently CO2 emissions, are residential
energy consumption and road transport. This dissertation investigates the
development of novel Deep Learning techniques to create tools which solve
limitations in these two key energy domains. Reduction of residential energy
consumption can be achieved by empowering end-users with the user of
Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep
Reinforcement Learning can tackle road transport decarbonization.

</details>


### [101] [Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four](https://arxiv.org/abs/2505.04371)
*Filipe Santos,João Paulo Fernandes,Luís Macedo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于标志的动作选择强化学习探索策略，并将量子版本应用于Connect Four游戏，验证了其在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何在Connect Four游戏中通过标志动作选择和量子加速改进探索效率，并分析其在复杂场景（如后手劣势）中的表现。

Method: 使用经典和量子RL代理，训练它们在先手或后手情况下对抗随机Negamax对手，并记录迭代次数等新指标。

Result: 标志探索策略优于简单的epsilon-greedy策略，量子代理能更快采样标志动作，但胜负率与经典版本相同。

Conclusion: 量子方法能更高效地采样标志动作，但训练场景的简单性可能导致胜负率无差异，未来需进一步验证更复杂环境。

Abstract: Action selection based on flags is a Reinforcement Learning (RL) exploration
policy that improves the exploration of the state space through the use of
flags, which can identify the most promising actions to take in each state. The
quantum counterpart of this exploration policy further improves upon this by
taking advantage of a quadratic speedup for sampling flagged actions. This
approach has already been successfully employed for the game of Checkers. In
this work, we describe the application of this method to the context of Connect
Four, in order to study its performance in a different setting, which can lead
to a better generalization of the technique. We also kept track of a metric
that wasn't taken into account in previous work: the average number of
iterations to obtain a flagged action. Since going second is a significant
disadvantage in Connect Four, we also had the intent of exploring how this more
complex scenario would impact the performance of our approach. The experiments
involved training and testing classical and quantum RL agents that played
either going first or going second against a Randomized Negamax opponent. The
results showed that both flagged exploration policies were clearly superior to
a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact
sample flagged actions in less iterations. Despite obtaining tagged actions
more consistently, the win rates between the classical and quantum versions of
the approach were identical, which could be due to the simplicity of the
training scenario chosen.

</details>


### [102] [Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets](https://arxiv.org/abs/2505.04389)
*Jenni Lampainen,Kaisa Joki,Napsu Karmitsa,Marko M. Mäkelä*

Main category: cs.LG

TL;DR: Clust-Splitter是一种基于非光滑优化的高效聚类算法，用于处理超大规模数据集的最小化平方和聚类问题。通过组合有限内存束方法和增量方法，实验证明其在处理高维、大数据集时表现优异。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集的聚类是数据挖掘和机器学习中的基础任务，现有方法在处理高维和大规模数据时效率不足，因此需要一种更高效的算法。

Method: 提出Clust-Splitter算法，通过三个非光滑优化问题（两个辅助问题生成初始点，一个主聚类问题）结合有限内存束方法和增量方法进行实现。

Result: 在真实数据集上验证，Clust-Splitter在效率和解质量上与现有最优方法相当，尤其适用于高维大规模数据。

Conclusion: Clust-Splitter是一种高效且高质量的大规模聚类解决方案，适用于实际应用场景。

Abstract: Clustering is a fundamental task in data mining and machine learning,
particularly for analyzing large-scale data. In this paper, we introduce
Clust-Splitter, an efficient algorithm based on nonsmooth optimization,
designed to solve the minimum sum-of-squares clustering problem in very large
datasets. The clustering task is approached through a sequence of three
nonsmooth optimization problems: two auxiliary problems used to generate
suitable starting points, followed by a main clustering formulation. To solve
these problems effectively, the limited memory bundle method is combined with
an incremental approach to develop the Clust-Splitter algorithm. We evaluate
Clust-Splitter on real-world datasets characterized by both a large number of
attributes and a large number of data points and compare its performance with
several state-of-the-art large-scale clustering algorithms. Experimental
results demonstrate the efficiency of the proposed method for clustering very
large datasets, as well as the high quality of its solutions, which are on par
with those of the best existing methods.

</details>


### [103] [Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast](https://arxiv.org/abs/2505.04396)
*Jingnan Wang,Jie Chao,Shangshang Yang,Congyi Nai,Kaijun Ren,Kefeng Deng,Xi Chen,Yaxin Liu,Hanqiuzi Wen,Ziniu Xiao,Lifeng Zhang,Xiaodong Wang,Jiping Guan,Baoxiang Pan*

Main category: cs.LG

TL;DR: 该论文提出了一种通过融合学习高分辨率气候先验与粗网格大尺度预测的混合方法，显著提高了风电场天气预报的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有风力发电的天气预报方法存在尺度不一致、计算成本高和多种不确定性来源纠缠等问题，亟需一种高效且准确的解决方案。

Method: 通过学习目标风电场的高分辨率数值天气模拟的气候分布，并结合粗网格大尺度预报，生成高精度、细粒度的天气模式预测。

Result: 该方法在确定性和概率性预测能力以及经济收益上均优于现有方法，且计算时间从约1000 CPU小时缩短至1 GPU小时内。

Conclusion: 该方法大幅降低了计算成本并保持了准确性，为可再生能源规划和运营提供了更高效可靠的解决方案。

Abstract: The planning and operation of renewable energy, especially wind power, depend
crucially on accurate, timely, and high-resolution weather information.
Coarse-grid global numerical weather forecasts are typically downscaled to meet
these requirements, introducing challenges of scale inconsistency, process
representation error, computation cost, and entanglement of distinct
uncertainty sources from chaoticity, model bias, and large-scale forcing. We
address these challenges by learning the climatological distribution of a
target wind farm using its high-resolution numerical weather simulations. An
optimal combination of this learned high-resolution climatological prior with
coarse-grid large scale forecasts yields highly accurate, fine-grained,
full-variable, large ensemble of weather pattern forecasts. Using observed
meteorological records and wind turbine power outputs as references, the
proposed methodology verifies advantageously compared to existing
numerical/statistical forecasting-downscaling pipelines, regarding either
deterministic/probabilistic skills or economic gains. Moreover, a 100-member,
10-day forecast with spatial resolution of 1 km and output frequency of 15 min
takes < 1 hour on a moderate-end GPU, as contrast to $\mathcal{O}(10^3)$ CPU
hours for conventional numerical simulation. By drastically reducing
computational costs while maintaining accuracy, our method paves the way for
more efficient and reliable renewable energy planning and operation.

</details>


### [104] [Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization](https://arxiv.org/abs/2505.04412)
*Ren Wang,Pengcheng Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种基于AutoEncoder的方法，通过集**流形重建层**和拓扑/几何正则化，从噪声点云中揭示潜在流形结构，并在降维过程中保持其完整性。实验表明，该方法在噪声数据下优于t-SNE、UMAP等方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在噪声数据中兼顾局部细节与全局拓扑完整性，导致降维结果扭曲或断裂。论文旨在通过联合流形重建与学习，实现更鲁棒的流形表示。

Method: 提出一种AutoEncoder框架，包含流形重建层（捕捉潜在流形结构）和正则化项（约束拓扑/几何属性），两部分在训练中协同优化。

Result: 在点云数据集上，本方法在流形结构发现和降维保留方面均优于基线（如t-SNE），通过可视化和定量指标验证。

Conclusion: 结合流形重建与学习能有效提升噪声数据下的流形表示可靠性，为现实场景提供更鲁棒的解决方案。

Abstract: Manifold learning aims to discover and represent low-dimensional structures
underlying high-dimensional data while preserving critical topological and
geometric properties. Existing methods often fail to capture local details with
global topological integrity from noisy data or construct a balanced
dimensionality reduction, resulting in distorted or fractured embeddings. We
present an AutoEncoder-based method that integrates a manifold reconstruction
layer, which uncovers latent manifold structures from noisy point clouds, and
further provides regularizations on topological and geometric properties during
dimensionality reduction, whereas the two components promote each other during
training. Experiments on point cloud datasets demonstrate that our method
outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in
discovering manifold structures from noisy data and preserving them through
dimensionality reduction, as validated by visualization and quantitative
metrics. This work demonstrates the significance of combining manifold
reconstruction with manifold learning to achieve reliable representation of the
latent manifold, particularly when dealing with noisy real-world data. Code
repository: https://github.com/Thanatorika/mrtg.

</details>


### [105] [Localized Diffusion Models for High Dimensional Distributions Generation](https://arxiv.org/abs/2505.04417)
*Georg A. Gottwald,Shuigen Liu,Youssef Marzouk,Sebastian Reich,Xin T. Tong*

Main category: cs.LG

TL;DR: 本文提出了局部扩散模型，通过利用目标分布的低维局部结构，减少高维得分函数的估计复杂度，从而缓解维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 高维得分函数的估计会导致维度灾难（CoD），因此需要更好地理解和利用目标分布的低维结构。局部结构描述了模型组件之间的稀疏依赖关系，使得分函数在局部上低维，从而降低样本复杂度。

Method: 提出局部扩散模型，使用局部得分匹配损失在局部假设空间中训练得分函数，证明了局部化能绕开CoD，但会引入额外局部化误差。通过理论分析和数值实验，展示了适中的局部化半径能平衡统计误差和局部化误差。

Result: 局部扩散模型在高维任务中表现更好，且局部结构还支持并行训练，提高了大规模应用的效率。

Conclusion: 局部扩散模型通过利用局部结构，有效缓解了维度灾难问题，在高维生成任务中实现了更好的性能和效率。

Abstract: Diffusion models are the state-of-the-art tools for various generative tasks.
However, estimating high-dimensional score functions makes them potentially
suffer from the curse of dimensionality (CoD). This underscores the importance
of better understanding and exploiting low-dimensional structure in the target
distribution. In this work, we consider locality structure, which describes
sparse dependencies between model components. Under locality structure, the
score function is effectively low-dimensional, so that it can be estimated by a
localized neural network with significantly reduced sample complexity. This
motivates the localized diffusion model, where a localized score matching loss
is used to train the score function within a localized hypothesis space. We
prove that such localization enables diffusion models to circumvent CoD, at the
price of additional localization error. Under realistic sample size scaling, we
show both theoretically and numerically that a moderate localization radius can
balance the statistical and localization error, leading to a better overall
performance. The localized structure also facilitates parallel training of
diffusion models, making it potentially more efficient for large-scale
applications.

</details>


### [106] [FedBWO: Enhancing Communication Efficiency in Federated Learning](https://arxiv.org/abs/2505.04435)
*Vahideh Hayyolalam,Öznur Özkasap*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FedBWO的联邦学习优化技术，通过仅传输性能评分而非模型权重来减少通信数据量，显著提升了全局模型性能和通信效率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的设备上进行联邦学习时，传统方法因传输大量模型权重导致通信带宽压力大，FedBWO旨在通过优化通信数据量来解决这一问题。

Method: FedBWO采用黑寡妇优化算法改进本地模型更新，仅传输性能评分而非完整权重，从而减少通信负担。

Result: 实验表明，FedBWO在全局模型准确率上平均比FedAvg提升21%，比FedGWO提升12%，同时显著降低了通信成本。

Conclusion: FedBWO在提高模型性能和减少通信开销方面表现出色，为资源受限的联邦学习场景提供了有效解决方案。

Abstract: Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a
shared model is collaboratively trained by various clients using their local
datasets while keeping the data private. Considering resource-constrained
devices, FL clients often suffer from restricted transmission capacity. Aiming
to enhance the system performance, the communication between clients and server
needs to be diminished. Current FL strategies transmit a tremendous amount of
data (model weights) within the FL process, which needs a high communication
bandwidth. Considering resource constraints, increasing the number of clients
and, consequently, the amount of data (model weights) can lead to a bottleneck.
In this paper, we introduce the Federated Black Widow Optimization (FedBWO)
technique to decrease the amount of transmitted data by transmitting only a
performance score rather than the local model weights from clients. FedBWO
employs the BWO algorithm to improve local model updates. The conducted
experiments prove that FedBWO remarkably improves the performance of the global
model and the communication efficiency of the overall system. According to the
experimental outcomes, FedBWO enhances the global model accuracy by an average
of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically
decreases the communication cost compared to other methods.

</details>


### [107] [Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory](https://arxiv.org/abs/2505.04440)
*Xiaozheng Qu,Zhaochuan Li,Zhuang Qi,Xiang Li,Haibei Huang,Lei Meng,Xiangxu Meng*

Main category: cs.LG

TL;DR: IR-ART通过迭代框架提升Fuzzy ART对预设警戒参数的鲁棒性，同时保持算法简洁性，适合非专家用户。


<details>
  <summary>Details</summary>
Motivation: 解决Fuzzy ART因警戒参数预设偏差导致的聚类结果波动问题，同时避免引入复杂框架。

Method: 提出IR-ART，包含动态稳定性检测、低质簇删除和警戒区域自适应扩展三个迭代阶段。

Result: 实验表明IR-ART提升了对次优警戒参数的容忍度，并在15个数据集上验证了有效性。

Conclusion: IR-ART通过迭代自优化机制，既简化了参数调整，又提升了聚类稳定性，适用于资源受限场景。

Abstract: The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is
highly dependent on the preset vigilance parameter, where deviations in its
value can lead to significant fluctuations in clustering results, severely
limiting its practicality for non-expert users. Existing approaches generally
enhance vigilance parameter robustness through adaptive mechanisms such as
particle swarm optimization and fuzzy logic rules. However, they often
introduce additional hyperparameters or complex frameworks that contradict the
original simplicity of the algorithm. To address this, we propose Iterative
Refinement Adaptive Resonance Theory (IR-ART), which integrates three key
phases into a unified iterative framework: (1) Cluster Stability Detection: A
dynamic stability detection module that identifies unstable clusters by
analyzing the change of sample size (number of samples in the cluster) in
iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that
eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance
region expansion mechanism that adaptively adjusts similarity thresholds.
Independent of the specific execution of clustering, these three phases
sequentially focus on analyzing the implicit knowledge within the iterative
process, adjusting weights and vigilance parameters, thereby laying a
foundation for the next iteration. Experimental evaluation on 15 datasets
demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter
values while preserving the parameter simplicity of Fuzzy ART. Case studies
visually confirm the algorithm's self-optimization capability through iterative
refinement, making it particularly suitable for non-expert users in
resource-constrained scenarios.

</details>


### [108] [Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs](https://arxiv.org/abs/2505.04441)
*Mirazul Haque,Petr Babkin,Farima Farmahinifarahani,Manuela Veloso*

Main category: cs.LG

TL;DR: 该研究探讨了在自动程序修复（APR）中引入程序执行轨迹对大型语言模型（LLMs）性能的影响，结果显示简单添加轨迹效果有限，但优化后的提示策略能更稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的APR方法多忽略程序的运行时行为，作者希望通过引入执行轨迹弥补这一不足，探索其对LLM推理能力的补充作用。

Method: 通过将程序执行轨迹融入APR提示中，利用GPT模型在三个APR数据集上评估效果，并测试不同轨迹复杂度和提示策略的影响。

Result: 仅部分配置下轨迹能略微提升性能，且复杂度越高效果越差；优化后的提示策略表现更稳定，且优于对小规模数据集的微调。

Conclusion: 执行轨迹可作为LLM推理的补充，但需优化提示设计以充分发挥其潜力。

Abstract: Large Language Models (LLMs) show promising performance on various
programming tasks, including Automatic Program Repair (APR). However, most
approaches to LLM-based APR are limited to the static analysis of the programs,
while disregarding their runtime behavior. Inspired by knowledge-augmented NLP,
in this work, we aim to remedy this potential blind spot by augmenting standard
APR prompts with program execution traces. We evaluate our approach using the
GPT family of models on three popular APR datasets. Our findings suggest that
simply incorporating execution traces into the prompt provides a limited
performance improvement over trace-free baselines, in only 2 out of 6 tested
dataset / model configurations. We further find that the effectiveness of
execution traces for APR diminishes as their complexity increases. We explore
several strategies for leveraging traces in prompts and demonstrate that
LLM-optimized prompts help outperform trace-free prompts more consistently.
Additionally, we show trace-based prompting to be superior to finetuning a
smaller LLM on a small-scale dataset; and conduct probing studies reinforcing
the notion that execution traces can complement the reasoning abilities of the
LLMs.

</details>


### [109] [A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities](https://arxiv.org/abs/2505.04461)
*Pengfei Jiao,Hongjiang Chen,Xuan Guo,Zhidong Zhao,Dongxiao He,Di Jin*

Main category: cs.LG

TL;DR: 该论文综述了时序交互图表示学习（TIGRL）的研究进展，提出了分类法，归纳了现有方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 时序交互图（TIGs）能建模复杂动态系统行为，但如何有效学习其表示以保留结构和时序信息是一个关键挑战。

Method: 提出了TIGRL方法的分类法，基于学习过程中利用的信息类型系统性地归纳现有方法。

Result: 整理了数据集和基准资源，并总结了TIGRL面临的开放挑战。

Conclusion: 论文为TIGRL的未来研究提供了方向，可能推动该领域的进一步发展。

Abstract: Temporal interaction graphs (TIGs), defined by sequences of timestamped
interaction events, have become ubiquitous in real-world applications due to
their capability to model complex dynamic system behaviors. As a result,
temporal interaction graph representation learning (TIGRL) has garnered
significant attention in recent years. TIGRL aims to embed nodes in TIGs into
low-dimensional representations that effectively preserve both structural and
temporal information, thereby enhancing the performance of downstream tasks
such as classification, prediction, and clustering within constantly evolving
data environments. In this paper, we begin by introducing the foundational
concepts of TIGs and emphasize the critical role of temporal dependencies. We
then propose a comprehensive taxonomy of state-of-the-art TIGRL methods,
systematically categorizing them based on the types of information utilized
during the learning process to address the unique challenges inherent to TIGs.
To facilitate further research and practical applications, we curate the source
of datasets and benchmarks, providing valuable resources for empirical
investigations. Finally, we examine key open challenges and explore promising
research directions in TIGRL, laying the groundwork for future advancements
that have the potential to shape the evolution of this field.

</details>


### [110] [Discriminative Ordering Through Ensemble Consensus](https://arxiv.org/abs/2505.04464)
*Louis Ohl,Fredrik Lindsten*

Main category: cs.LG

TL;DR: 该论文提出了一种基于聚类模型与共识矩阵连接性距离的集成聚类方法，通过构建判别排序来评估聚类模型的性能，尤其在处理多样化聚类定义和约束集成时表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前聚类模型评估指标难以处理多样化聚类定义或集成约束，导致性能评估存在局限性，因此需要一种更灵活且有效的评估方法。

Method: 采用共识聚类思想，通过计算聚类模型连接性与共识矩阵的距离，构建判别排序，并以合成场景验证其有效性。

Result: 所提方法能优先匹配共识的模型，且在多样化算法和非固定簇数场景中显著优于其他评分方法，同时兼容聚类约束。

Conclusion: 该排序方法为聚类模型评估提供了一种灵活且高效的解决方案，特别适用于复杂或多样化的聚类场景。

Abstract: Evaluating the performance of clustering models is a challenging task where
the outcome depends on the definition of what constitutes a cluster. Due to
this design, current existing metrics rarely handle multiple clustering models
with diverse cluster definitions, nor do they comply with the integration of
constraints when available. In this work, we take inspiration from consensus
clustering and assume that a set of clustering models is able to uncover hidden
structures in the data. We propose to construct a discriminative ordering
through ensemble clustering based on the distance between the connectivity of a
clustering model and the consensus matrix. We first validate the proposed
method with synthetic scenarios, highlighting that the proposed score ranks the
models that best match the consensus first. We then show that this simple
ranking score significantly outperforms other scoring methods when comparing
sets of different clustering algorithms that are not restricted to a fixed
number of clusters and is compatible with clustering constraints.

</details>


### [111] [Spectral and Temporal Denoising for Differentially Private Optimization](https://arxiv.org/abs/2505.04468)
*Hyeju Shin,Kyudan Jung,Seongwon Yun,Juyoung Yun*

Main category: cs.LG

TL;DR: 提出了一种基于频域噪声整形和卡尔曼滤波的差分隐私优化方法FFTKF，在保证隐私的前提下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决DP-SGD中添加噪声导致模型性能下降的问题。

Method: 结合频域噪声整形（高频掩码）和卡尔曼滤波（标量增益+Hessian近似）降低噪声影响，保持梯度信号。

Result: 在多个数据集和模型上测试精度优于DP-SGD和DiSK，复杂度为O(d log d)。

Conclusion: FFTKF在相同隐私保证下，通过减少噪声和控制偏差实现了更优的隐私-效用权衡。

Abstract: This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a
differentially private optimization method that addresses the challenge of
preserving performance in DP-SGD, where added noise typically degrades model
utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering
to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP
guarantees. It employs a high-frequency shaping mask in the Fourier domain to
concentrate differential privacy noise in less informative spectral components,
preserving low-frequency gradient signals. A scalar-gain Kalman filter with
finite-difference Hessian approximation further refines the denoised gradients.
With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates
improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100,
and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers.
Theoretical analysis confirms that FFTKF maintains equivalent privacy
guarantees while achieving a tighter privacy-utility trade-off through reduced
noise and controlled bias.

</details>


### [112] [Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations](https://arxiv.org/abs/2505.04471)
*Vincent Souveton,Sébastien Terrana*

Main category: cs.LG

TL;DR: 提出了一种基于哈密顿动力学的新型归一化流方法，用于快速采样和预测相空间密度演化。


<details>
  <summary>Details</summary>
Motivation: 针对Vlasov-Poisson方程这类复杂保守物理系统，传统数值方法难以高效建模相空间演化，需要一种兼具速度与物理可解释性的新方法。

Method: 使用哈密顿归一化流（固定动力神经哈密顿流）对初始高斯分布进行体积保持的变换，训练数据为数值模拟生成的初始与终态相空间分布。

Result: 模型能快速采样终态分布，并自动学习可解释的物理势场，推广至未见的中间状态。

Conclusion: 该方法为相空间演化建模提供了高效且物理解释性强的新工具，适用于等离子体和宇宙学等领域。

Abstract: Many conservative physical systems can be described using the Hamiltonian
formalism. A notable example is the Vlasov-Poisson equations, a set of partial
differential equations that govern the time evolution of a phase-space density
function representing collisionless particles under a self-consistent
potential. These equations play a central role in both plasma physics and
cosmology. Due to the complexity of the potential involved, analytical
solutions are rarely available, necessitating the use of numerical methods such
as Particle-In-Cell. In this work, we introduce a novel approach based on
Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic
Neural Hamiltonian Flows. Our method transforms an initial Gaussian
distribution in phase space into the final distribution using a sequence of
invertible, volume-preserving transformations derived from Hamiltonian
dynamics. The model is trained on a dataset comprising initial and final states
at a fixed time T, generated via numerical simulations. After training, the
model enables fast sampling of the final distribution from any given initial
state. Moreover, by automatically learning an interpretable physical potential,
it can generalize to intermediate states not seen during training, offering
insights into the system's evolution across time.

</details>


### [113] [Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules](https://arxiv.org/abs/2505.04535)
*Michail Theologitis,Vasilis Samoladas,Antonios Deligiannakis*

Main category: cs.LG

TL;DR: 本文介绍了FDA-Opt算法家族，作为FedOpt和FDA的统一优化版本，解决了联邦学习中参数同步的固有问题，并在下游NLP任务中表现出更优性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）和预训练语言模型（LM）的结合为模型微调提供了新机遇，但其参数同步问题因模型规模而加剧。现有方法（如FedOpt和FDA）存在同步机制僵化或参数调优困难等局限性。

Method: 提出了FDA-Opt算法家族，动态监控训练进度以调整同步频率，并统一了FedOpt和FDA的核心思想，同时消除了二者的主要缺点。

Result: 实验表明，即便在竞争算法的超参数设置下，FDA-Opt在多种NLP下游任务中仍优于FedOpt，且无需额外配置。

Conclusion: FDA-Opt可作为FedOpt的直接替代方案，在无需调整的情况下提供更优性能，适用于现代FL系统。

Abstract: Federated learning (FL) makes it possible to train models on data that would
otherwise remain untapped and inaccessible. Simultaneously, pre-trained
language models (LMs) have emerged as indispensable tools in modern workflows.
These models exhibit extraordinary capabilities and are easily adapted to
downstream tasks. This opens one of the most exciting frontiers in FL:
fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid
communication of parameters, a problem which is magnified by the sheer size of
these modern models. Currently, the FedOpt family of algorithms is the
prevailing approach in FL, though it relies on fixed, heuristic intervals for
model synchronization. Recently, the FDA algorithm introduced a dynamic
alternative by monitoring training progress, but it came with its own
drawbacks; namely, a hard-to-tune threshold parameter and a rigid
synchronization scheme. In this work, we introduce the FDA-Opt family of
algorithms -- a unified generalization that extends the principles behind both
FDA and FedOpt, while resolving their core limitations. We evaluate our
approach on fine-tuning LMs across a range of downstream NLP tasks, and
demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt
operates under hyper-parameter settings originally optimized for its
competitors. In other words, we show that FDA-Opt is a practical, drop-in
replacement for FedOpt in modern FL libraries and systems: it requires no
additional configuration and delivers superior performance out of the box.

</details>


### [114] [Purity Law for Generalizable Neural TSP Solvers](https://arxiv.org/abs/2505.04558)
*Wenzhao Liu,Haoran Li,Congying Han,Zicheng Zhang,Anqi Li,Tiande Guo*

Main category: cs.LG

TL;DR: 该论文提出了纯度法则(PuLa)，揭示了优化旅行商问题(TSP)解决方案中全局最优的局部稀疏性偏好，并基于此提出了PUPO训练范式，显著提升了神经求解器的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 神经方法在处理不同规模和分布的TSP问题时泛化能力不足，主要原因是缺乏对通用模式的鲁棒学习。

Method: 通过统计分析发现PuLa法则，并提出PUPO训练范式，将神经解的构造过程与PuLa对齐。

Result: PUPO能无缝集成到流行神经求解器中，显著提升泛化性能且不增加推理计算开销。

Conclusion: 研究证明了PuLa的有效性和PUPO在提升神经求解器泛化能力方面的潜力。

Abstract: Achieving generalization in neural approaches across different scales and
distributions remains a significant challenge for the Traveling Salesman
Problem~(TSP). A key obstacle is that neural networks often fail to learn
robust principles for identifying universal patterns and deriving optimal
solutions from diverse instances. In this paper, we first uncover Purity Law
(PuLa), a fundamental structural principle for optimal TSP solutions, defining
that edge prevalence grows exponentially with the sparsity of surrounding
vertices. Statistically validated across diverse instances, PuLa reveals a
consistent bias toward local sparsity in global optima. Building on this
insight, we propose Purity Policy Optimization~(PUPO), a novel training
paradigm that explicitly aligns characteristics of neural solutions with PuLa
during the solution construction process to enhance generalization. Extensive
experiments demonstrate that PUPO can be seamlessly integrated with popular
neural solvers, significantly enhancing their generalization performance
without incurring additional computational overhead during inference.

</details>


### [115] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/abs/2505.04560)
*Guanghui Wang,Zhiyong Yang,Zitai Wang,Shi Wang,Qianqian Xu,Qingming Huang*

Main category: cs.LG

TL;DR: 论文提出ABKD框架，通过α-β散度平衡知识蒸馏中的Hardness-Concentration和Confidence-Concentration效应，优于传统的FKLD和RKLD方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法（FKLD和RKLD）在平衡Hardness-Concentration和Confidence-Concentration效应时存在极端问题，导致学生模型难以有效学习教师模型的分布信息。

Method: 提出ABKD框架，基于α-β散度，平滑插值FKLD和RKLD，实现两种效应的平衡。

Result: 在17个语言/视觉数据集和12种师生设置下的实验证明了ABKD的有效性。

Conclusion: ABKD通过理论分析和实验验证，显著提升了知识蒸馏的平衡性和性能。

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [116] [Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data](https://arxiv.org/abs/2505.04566)
*Lucas R. C. Farias,Talita P. Silva,Pedro H. M. Araujo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于LSTM的多任务学习方法，用于联合预测巴西Recife地区登革热、基孔肯雅热和寨卡病毒的爆发及病例数。通过利用历史公共卫生数据和滑动窗口策略，模型在分类和回归任务上表现良好，证明了统一建模策略在公共卫生场景中的可行性和优势。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多任务学习方法，结合历史公共卫生数据，提高对登革热、基孔肯雅热和寨卡病毒爆发及病例数的预测能力，以支持公共卫生决策。

Method: 采用基于LSTM的多任务学习架构，结合滑动窗口策略构建时间特征，并使用Keras Tuner进行超参数优化。模型同时执行爆发检测（分类）和病例预测（回归）任务。

Result: 较长的滑动窗口提高了登革热回归任务的准确性，而分类性能在中长窗口达到峰值。多任务架构在各疾病和任务中均表现优异。

Conclusion: 多任务学习在数据有限的公共卫生场景中具有可行性和优势，能够为流行病预测提供统一且可扩展的解决方案。

Abstract: This paper presents a multitask learning approach based on long-short-term
memory (LSTM) networks for the joint prediction of arboviral outbreaks and case
counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging
historical public health data from DataSUS (2017-2023), the proposed model
concurrently performs binary classification (outbreak detection) and regression
(case forecasting) tasks. A sliding window strategy was adopted to construct
temporal features using varying input lengths (60, 90, and 120 days), with
hyperparameter optimization carried out using Keras Tuner. Model evaluation
used time series cross-validation for robustness and a held-out test from 2023
for generalization assessment. The results show that longer windows improve
dengue regression accuracy, while classification performance peaked at
intermediate windows, suggesting an optimal trade-off between sequence length
and generalization. The multitask architecture delivers competitive performance
across diseases and tasks, demonstrating the feasibility and advantages of
unified modeling strategies for scalable epidemic forecasting in data-limited
public health scenarios.

</details>


### [117] [Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization](https://arxiv.org/abs/2505.04578)
*Wenjun Cao*

Main category: cs.LG

TL;DR: 论文研究了强化学习微调对大型语言模型的安全影响，展示了一种高效的攻击方式，仅需50步即可绕过安全防护。提出针对性的防御框架Reward Neutralization，有效抵御攻击，保持模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对监督微调有效，但对强化学习的动态反馈机制无效。研究旨在填补这一安全漏洞，保护开源模型免受参数级攻击。

Method: 提出Reward Neutralization防御框架，通过生成最小化信息的拒绝模式，使恶意奖励信号失效，从而中和攻击者优化有害输出的尝试。

Result: 实验表明，该方法在200步攻击后仍能将有害分数控制在2以下，而标准模型迅速恶化。

Conclusion: 该研究首次证明，针对日益普遍的强化学习攻击，可实现有效防御，为开源模型安全提供了关键解决方案。

Abstract: Reinforcement learning (RL) fine-tuning transforms large language models
while creating a vulnerability we experimentally verify: Our experiment shows
that malicious RL fine-tuning dismantles safety guardrails with remarkable
efficiency, requiring only 50 steps and minimal adversarial prompts, with
harmful escalating from 0-2 to 7-9. This attack vector particularly threatens
open-source models with parameter-level access. Existing defenses targeting
supervised fine-tuning prove ineffective against RL's dynamic feedback
mechanisms. We introduce Reward Neutralization, the first defense framework
specifically designed against RL fine-tuning attacks, establishing concise
rejection patterns that render malicious reward signals ineffective. Our
approach trains models to produce minimal-information rejections that attackers
cannot exploit, systematically neutralizing attempts to optimize toward harmful
outputs. Experiments validate that our approach maintains low harmful scores
(no greater than 2) after 200 attack steps, while standard models rapidly
deteriorate. This work provides the first constructive proof that robust
defense against increasingly accessible RL attacks is achievable, addressing a
critical security gap for open-weight models.

</details>


### [118] [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://arxiv.org/abs/2505.04599)
*Michael Crawshaw,Mingrui Liu*

Main category: cs.LG

TL;DR: 该论文探讨了在$(L_0, L_1)$-平滑条件下自适应优化算法的收敛复杂性下界，发现某些算法（如AdaGrad变体）的复杂性至少与问题参数的平方成比例，表明这种设定比标准平滑设定更困难。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定自适应优化算法在$(L_0, L_1)$-平滑条件下的复杂性下界，揭示其与标准平滑设定的差异。

Method: 通过分析几种AdaGrad变体和SGD的复杂性下界，重点关注问题参数$Δ, L_0, L_1$的依赖关系。

Result: 发现某些自适应算法的复杂性至少为问题参数的二次函数，例如AdaGrad-Norm的装饰变体需要$Ω ⁡(Δ^2 L_1^2 σ^2 ϵ^{-4})$次查询。

Conclusion: $(L_0, L_1)$-平滑设定对某些自适应算法而言比标准平滑设定更具挑战性，复杂性下界更高。

Abstract: Recent results in non-convex stochastic optimization demonstrate the
convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0,
L_1)$-smoothness condition, but the rate of convergence is a higher-order
polynomial in terms of problem parameters like the smoothness constants. The
complexity guaranteed by such algorithms to find an $\epsilon$-stationary point
may be significantly larger than the optimal complexity of $\Theta \left(
\Delta L \sigma^2 \epsilon^{-4} \right)$ achieved by SGD in the $L$-smooth
setting, where $\Delta$ is the initial optimality gap, $\sigma^2$ is the
variance of stochastic gradient. However, it is currently not known whether
these higher-order dependencies can be tightened. To answer this question, we
investigate complexity lower bounds for several adaptive optimization
algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence
in terms of problem parameters $\Delta, L_0, L_1$. We provide complexity bounds
for three variations of AdaGrad, which show at least a quadratic dependence on
problem parameters $\Delta, L_0, L_1$. Notably, we show that the decorrelated
variant of AdaGrad-Norm requires at least $\Omega \left( \Delta^2 L_1^2
\sigma^2 \epsilon^{-4} \right)$ stochastic gradient queries to find an
$\epsilon$-stationary point. We also provide a lower bound for SGD with a broad
class of adaptive stepsizes. Our results show that, for certain adaptive
algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult
than the standard smooth setting, in terms of the initial optimality gap and
the smoothness constants.

</details>


### [119] [Testing Juntas Optimally with Samples](https://arxiv.org/abs/2505.04604)
*Lorenzo Beretta,Nathaniel Harms,Caleb Koch*

Main category: cs.LG

TL;DR: 该论文首次在无分布样本模型中，对自然布尔函数类（k-junta测试）给出了紧的上界和下界样本复杂度。结果显示，junta测试必须学习相关变量的集合，且容错测试与学习之间不存在巨大差距。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为无分布样本模型中的k-junta测试提供紧的样本复杂度界限，并探讨其与特征选择问题的关联，以及容错测试与学习之间的差距。

Method: 通过数学证明，得出k-junta测试和容错测试的样本复杂度上界和下界。

Result: 证明了样本复杂度的紧界限$	heta(rac{1}{ε}(√{2^k ⁡loginom{n}{k}} + ⁡loginom{n}{k}))$，并发现容错测试的样本下界$Ω(2^{(1-o(1))k} + ⁡loginom{n}{k})$。

Conclusion: 论文首次为无分布样本模型中的自然布尔函数类提供了紧的样本界限，并揭示了容错测试与学习之间的关系，具有重要理论意义。

Abstract: We prove tight upper and lower bounds of
$\Theta\left(\tfrac{1}{\epsilon}\left( \sqrt{2^k \log\binom{n}{k} } +
\log\binom{n}{k} \right)\right)$ on the number of samples required for
distribution-free $k$-junta testing. This is the first tight bound for testing
a natural class of Boolean functions in the distribution-free sample-based
model. Our bounds also hold for the feature selection problem, showing that a
junta tester must learn the set of relevant variables. For tolerant junta
testing, we prove a sample lower bound of $\Omega(2^{(1-o(1)) k} +
\log\binom{n}{k})$ showing that, unlike standard testing, there is no large gap
between tolerant testing and learning.

</details>


### [120] [WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales](https://arxiv.org/abs/2505.04608)
*Drew Prinster,Xing Han,Anqi Liu,Suchi Saria*

Main category: cs.LG

TL;DR: 该论文提出了一种加权共形测试鞅（WCTMs）方法，用于在线监控AI/ML系统中的数据分布变化，控制误报并适应轻微协变量偏移。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中，AI/ML系统需要持续监控以确保安全，现有方法在监控范围和在线适应性上存在局限。

Method: 提出了加权共形测试鞅（WCTMs）的理论框架，并设计了具体算法，支持在线适应轻微协变量偏移，同时对严重偏移发出警报。

Result: 在真实数据集上，WCTMs表现优于现有基线方法。

Conclusion: WCTMs为AI/ML系统的安全监控提供了更灵活的理论和实用工具。

Abstract: Responsibly deploying artificial intelligence (AI) / machine learning (ML)
systems in high-stakes settings arguably requires not only proof of system
reliability, but moreover continual, post-deployment monitoring to quickly
detect and address any unsafe behavior. Statistical methods for nonparametric
change-point detection -- especially the tools of conformal test martingales
(CTMs) and anytime-valid inference -- offer promising approaches to this
monitoring task. However, existing methods are restricted to monitoring limited
hypothesis classes or ``alarm criteria,'' such as data shifts that violate
certain exchangeability assumptions, or do not allow for online adaptation in
response to shifts. In this paper, we expand the scope of these monitoring
methods by proposing a weighted generalization of conformal test martingales
(WCTMs), which lay a theoretical foundation for online monitoring for any
unexpected changepoints in the data distribution while controlling
false-alarms. For practical applications, we propose specific WCTM algorithms
that accommodate online adaptation to mild covariate shifts (in the marginal
input distribution) while raising alarms in response to more severe shifts,
such as concept shifts (in the conditional label distribution) or extreme
(out-of-support) covariate shifts that cannot be easily adapted to. On
real-world datasets, we demonstrate improved performance relative to
state-of-the-art baselines.

</details>


### [121] [Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2505.04619)
*Abdulaziz Almuzairee,Rohan Patil,Dwait Bhatt,Henrik I. Christensen*

Main category: cs.LG

TL;DR: 提出MAD算法，高效合并多视角提升样本利用率，同时结合单视角特征实现轻量部署和鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: 为应对多视角视觉伺服的计算挑战及部署成本，设计了更高效的表示方法。

Method: 结合多视角合并与单视角特征，采用Q学习和MAD算法优化样本效率。

Result: 在Meta-World和ManiSkill3上验证了方法的效率和鲁棒性。

Conclusion: MAD算法成功平衡了样本效率与轻量部署需求。

Abstract: Vision is well-known for its use in manipulation, especially using visual
servoing. To make it robust, multiple cameras are needed to expand the field of
view. That is computationally challenging. Merging multiple views and using
Q-learning allows the design of more effective representations and optimization
of sample efficiency. Such a solution might be expensive to deploy. To mitigate
this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently
merges views to increase sample efficiency while augmenting with single-view
features to allow lightweight deployment and ensure robust policies. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [122] [Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind](https://arxiv.org/abs/2505.03770)
*Mouad Abrini,Omri Abend,Dina Acklin,Henny Admoni,Gregor Aichinger,Nitay Alon,Zahra Ashktorab,Ashish Atreja,Moises Auron,Alexander Aufreiter,Raghav Awasthi,Soumya Banerjee,Joe M. Barnby,Rhea Basappa,Severin Bergsmann,Djallel Bouneffouf,Patrick Callaghan,Marc Cavazza,Thierry Chaminade,Sonia Chernova,Mohamed Chetouan,Moumita Choudhury,Axel Cleeremans,Jacek B. Cywinski,Fabio Cuzzolin,Hokin Deng,N'yoma Diamond,Camilla Di Pasquasio,Guillaume Dumas,Max van Duijn,Mahapatra Dwarikanath,Qingying Gao,Ashok Goel,Rebecca Goldstein,Matthew Gombolay,Gabriel Enrique Gonzalez,Amar Halilovic,Tobias Halmdienst,Mahimul Islam,Julian Jara-Ettinger,Natalie Kastel,Renana Keydar,Ashish K. Khanna,Mahdi Khoramshahi,JiHyun Kim,MiHyeon Kim,YoungBin Kim,Senka Krivic,Nikita Krasnytskyi,Arun Kumar,JuneHyoung Kwon,Eunju Lee,Shane Lee,Peter R. Lewis,Xue Li,Yijiang Li,Michal Lewandowski,Nathan Lloyd,Matthew B. Luebbers,Dezhi Luo,Haiyun Lyu,Dwarikanath Mahapatra,Kamal Maheshwari,Mallika Mainali,Piyush Mathur,Patrick Mederitsch,Shuwa Miura,Manuel Preston de Miranda,Reuth Mirsky,Shreya Mishra,Nina Moorman,Katelyn Morrison,John Muchovej,Bernhard Nessler,Felix Nessler,Hieu Minh Jord Nguyen,Abby Ortego,Francis A. Papay,Antoine Pasquali,Hamed Rahimi,Charumathi Raghu,Amanda Royka,Stefan Sarkadi,Jaelle Scheuerman,Simon Schmid,Paul Schrater,Anik Sen,Zahra Sheikhbahaee,Ke Shi,Reid Simmons,Nishant Singh,Mason O. Smith,Ramira van der Meulen,Anthia Solaki,Haoran Sun,Viktor Szolga,Matthew E. Taylor,Travis Taylor,Sanne Van Waveren,Juan David Vargas,Rineke Verbrugge,Eitan Wagner,Justin D. Weisz,Ximing Wen,William Yeoh,Wenlong Zhang,Michelle Zhao,Shlomo Zilberstein*

Main category: cs.AI

TL;DR: 该论文集收录了2025年3月3日在AAAI 2025研讨会上关于通过心智理论推进人工智能的论文，旨在为ToM和AI研究社区提供开放获取的精选文集。


<details>
  <summary>Details</summary>
Motivation: 为心智理论（ToM）和人工智能研究社区提供一个开放且精选的资源，促进相关领域的发展与交流。

Method: 通过研讨会和论文选集的形式，收集并整理相关研究。

Result: 一本开放获取的论文集，涵盖了ToM与AI交叉领域的最新研究进展。

Conclusion: 该论文集为ToM和AI研究社区提供了有价值的资源，有助于推动这两个领域的进一步发展。

Abstract: This volume includes a selection of papers presented at the Workshop on
Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in
Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an
open access and curated anthology for the ToM and AI research community.

</details>


### [123] [Design description of Wisdom Computing Persperctive](https://arxiv.org/abs/2505.03800)
*TianYi Yu*

Main category: cs.AI

TL;DR: 该研究设计了一个结合AI和可视化动画技术的手写矩阵识别与逐步计算显示系统，旨在解决学生学习数学时因抽象公式和复杂步骤难以理解的问题。


<details>
  <summary>Details</summary>
Motivation: 数学学习中的抽象公式和复杂计算步骤常让学生难以理解，研究通过结合人工智能与可视化技术，提升学习体验。

Method: 系统采用Mamba骨干网络增强手写矩阵识别精度，结合YOLO模型完成数字提取与矩阵重构，并利用CoordAttention机制优化字符空间定位；通过Manim动画引擎逐步展示计算过程。

Result: 系统成功实现了高模块化和灵活性，能按需实时生成多种数学运算动画，帮助学生直观理解计算逻辑。

Conclusion: 该系统通过创新的人机交互方式，生动展示数学计算过程，提升学习效果，具有教育辅助工具的潜力和扩展性。

Abstract: This course design aims to develop and research a handwriting matrix
recognition and step-by-step visual calculation process display system,
addressing the issue of abstract formulas and complex calculation steps that
students find difficult to understand when learning mathematics. By integrating
artificial intelligence with visualization animation technology, the system
enhances precise recognition of handwritten matrix content through the
introduction of Mamba backbone networks, completes digital extraction and
matrix reconstruction using the YOLO model, and simultaneously combines
CoordAttention coordinate attention mechanisms to improve the accurate grasp of
character spatial positions. The calculation process is demonstrated frame by
frame through the Manim animation engine, vividly showcasing each mathematical
calculation step, helping students intuitively understand the intrinsic logic
of mathematical operations. Through dynamically generating animation processes
for different computational tasks, the system exhibits high modularity and
flexibility, capable of generating various mathematical operation examples in
real-time according to student needs. By innovating human-computer interaction
methods, it brings mathematical calculation processes to life, helping students
bridge the gap between knowledge and understanding on a deeper level,
ultimately achieving a learning experience where "every step is understood."
The system's scalability and interactivity make it an intuitive, user-friendly,
and efficient auxiliary tool in education.

</details>


### [124] [GRAML: Dynamic Goal Recognition As Metric Learning](https://arxiv.org/abs/2505.03941)
*Matan Shamir,Reuth Mirsky*

Main category: cs.AI

TL;DR: 该论文提出了一种名为GRAML的新方法，通过深度度量学习解决目标识别问题，能够快速适应新目标且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的目标识别方法依赖预定义目标集且训练耗时，无法快速适应新目标。GRAML旨在实现自动模型学习的同时，快速适应新目标。

Method: GRAML采用连体网络（Siamese network）将目标识别视为深度度量学习任务，利用RNN学习嵌入空间的度量，区分不同目标的观察轨迹。

Result: GRAML在多种环境中测试，表现出速度快、灵活性强和运行时性能提升，同时保持了高识别准确率。

Conclusion: GRAML通过深度度量学习有效解决了目标识别的快速适应问题，显著优于现有技术。

Abstract: Goal Recognition (GR) is the problem of recognizing an agent's objectives
based on observed actions. Recent data-driven approaches for GR alleviate the
need for costly, manually crafted domain models. However, these approaches can
only reason about a pre-defined set of goals, and time-consuming training is
needed for new emerging goals. To keep this model-learning automated while
enabling quick adaptation to new goals, this paper introduces GRAML: Goal
Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a
deep metric learning task, employing an RNN that learns a metric over an
embedding space, where the embeddings for observation traces leading to
different goals are distant, and embeddings of traces leading to the same goals
are close. This metric is especially useful when adapting to new goals, even if
given just one example observation trace per goal. Evaluated on a versatile set
of environments, GRAML shows speed, flexibility, and runtime improvements over
the state-of-the-art GR while maintaining accurate recognition.

</details>


### [125] [Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents](https://arxiv.org/abs/2505.03947)
*Xiang Li,Yiyang Hao,Doug Fulop*

Main category: cs.AI

TL;DR: 论文研究了如何利用预训练的大型语言模型（LLMs）在零样本条件下玩Atari游戏Frogger，并探讨了上下文学习和推理量对性能的影响，同时展示了用LLM示范提升传统强化学习方法的效果。


<details>
  <summary>Details</summary>
Motivation: 强化学习研究的一个主要目标是开发能快速适应和掌握新任务的通用智能体。当前RL游戏智能体虽能掌握许多Atari游戏，但训练速度慢且成本高。本文希望通过LLMs解决这一问题。

Method: 使用预训练的LLMs进行零样本游戏测试，研究上下文学习和推理量对性能的影响，并利用LLM示范引导传统RL方法。

Result: LLMs在零样本条件下能成功玩Frogger游戏，上下文学习和推理量显著影响性能，同时LLM示范显著提升了传统RL方法的性能和样本效率。

Conclusion: LLMs在RL任务中表现出潜力，尤其在零样本和上下文学习场景下，结合LLM示范可显著提升传统RL方法的效率。

Abstract: One of the primary aspirations in reinforcement learning research is
developing general-purpose agents capable of rapidly adapting to and mastering
novel tasks. While RL gaming agents have mastered many Atari games, they remain
slow and costly to train for each game. In this work, we demonstrate that
latest reasoning LLMs with out-of-domain RL post-training can play a
challenging Atari game called Frogger under a zero-shot setting. We then
investigate the effect of in-context learning and the amount of reasoning
effort on LLM performance. Lastly, we demonstrate a way to bootstrap
traditional RL method with LLM demonstrations, which significantly improves
their performance and sample efficiency. Our implementation is open sourced at
https://github.com/AlienKevin/frogger.

</details>


### [126] [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
*Gerrit Großmann,Larisa Ivanova,Sai Leela Poduru,Mohaddeseh Tabrizian,Islam Mesabah,David A. Selby,Sebastian J. Vollmer*

Main category: cs.AI

TL;DR: 研究表明，共享故事可以促使LLM代理在公共物品游戏中更倾向于合作，而不同故事或自利导向则会降低合作效果。


<details>
  <summary>Details</summary>
Motivation: 探索共享叙事是否能够像影响人类合作一样，促进LLM代理之间的协作行为。

Method: 在有限重复的公共物品游戏中，LLM代理被植入不同团队合作程度的故事，测试其对谈判行为和结果的影响。

Result: 共享故事显著提高了代理间的合作率和谈判成功率，而不同故事或自利导向则导致合作减少。

Conclusion: 叙事对LLM代理行为有显著影响，对多Agent系统设计和AI对齐具有潜在意义。

Abstract: According to Yuval Noah Harari, large-scale human cooperation is driven by
shared narratives that encode common beliefs and values. This study explores
whether such narratives can similarly nudge LLM agents toward collaboration. We
use a finitely repeated public goods game in which LLM agents choose either
cooperative or egoistic spending strategies. We prime agents with stories
highlighting teamwork to different degrees and test how this influences
negotiation outcomes. Our experiments explore four questions:(1) How do
narratives influence negotiation behavior? (2) What differs when agents share
the same story versus different ones? (3) What happens when the agent numbers
grow? (4) Are agents resilient against self-serving negotiators? We find that
story-based priming significantly affects negotiation strategies and success
rates. Common stories improve collaboration, benefiting each agent. By
contrast, priming agents with different stories reverses this effect, and those
agents primed toward self-interest prevail. We hypothesize that these results
carry implications for multi-agent system design and AI alignment.

</details>


### [127] [LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration](https://arxiv.org/abs/2505.03985)
*Zirong Chen,Ziyan An,Jennifer Reynolds,Kristin Mullen,Stephen Martini,Meiyi Ma*

Main category: cs.AI

TL;DR: LogiDebrief是一个AI驱动的框架，通过结合信号时序逻辑（STL）和大语言模型（LLMs）自动化911呼叫评估，解决了传统人工评估低覆盖和高延迟的问题，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的人类主导的911呼叫评估在高呼叫量下难以实现全面覆盖和及时反馈，亟需自动化解决方案以提高效率和一致性。

Method: LogiDebrief采用三步验证：(1)上下文理解以识别响应类型和关键条件；(2)基于STL和LLM的运行时合规检查； (3)自动化生成质量评估报告。

Result: 该框架在纳什维尔紧急通信部门成功应用，处理了1,701次真实呼叫，节省311.85小时人工时间，实证数据显示其准确性和性能提升效果。

Conclusion: LogiDebrief证明了AI技术在紧急响应服务质量评估中的实用性和效益，为未来自动化评估系统提供了可行范例。

Abstract: Emergency response services are critical to public safety, with 9-1-1
call-takers playing a key role in ensuring timely and effective emergency
operations. To ensure call-taking performance consistency, quality assurance is
implemented to evaluate and refine call-takers' skillsets. However, traditional
human-led evaluations struggle with high call volumes, leading to low coverage
and delayed assessments. We introduce LogiDebrief, an AI-driven framework that
automates traditional 9-1-1 call debriefing by integrating Signal-Temporal
Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous
performance evaluation. LogiDebrief formalizes call-taking requirements as
logical specifications, enabling systematic assessment of 9-1-1 calls against
procedural guidelines. It employs a three-step verification process: (1)
contextual understanding to identify responder types, incident classifications,
and critical conditions; (2) STL-based runtime checking with LLM integration to
ensure compliance; and (3) automated aggregation of results into quality
assurance reports. Beyond its technical contributions, LogiDebrief has
demonstrated real-world impact. Successfully deployed at Metro Nashville
Department of Emergency Communications, it has assisted in debriefing 1,701
real-world calls, saving 311.85 hours of active engagement. Empirical
evaluation with real-world data confirms its accuracy, while a case study and
extensive user study highlight its effectiveness in enhancing call-taking
performance.

</details>


### [128] [An alignment safety case sketch based on debate](https://arxiv.org/abs/2505.03989)
*Marie Davidsen Buhl,Jacob Pfau,Benjamin Hilton,Geoffrey Irving*

Main category: cs.AI

TL;DR: 论文探讨了利用辩论机制提升AI安全性，防止超级AI系统产生有害行为，强调诚实性训练和在线学习的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力接近或超越人类，人类难以有效评估其行为，需探索新方法（如辩论）来引导AI向良性发展，尤其防止AI研发代理在内部破坏。

Method: 通过辩论机制训练AI代理，确保诚实性，并在部署过程中持续在线学习，依赖于四大关键假设：辩论能力、诚实性关联、诚实性保持和错误容忍。

Result: 提出了一个基于辩论的AI安全框架，并指出需进一步研究的关键问题以确保该方法的有效性。

Conclusion: 辩论机制为AI安全性提供了潜在解决方案，但需解决开放性问题才能构建可信的安全论证。

Abstract: If AI systems match or exceed human capabilities on a wide range of tasks, it
may become difficult for humans to efficiently judge their actions -- making it
hard to use human feedback to steer them towards desirable traits. One proposed
solution is to leverage another superhuman system to point out flaws in the
system's outputs via a debate. This paper outlines the value of debate for AI
safety, as well as the assumptions and further research required to make debate
work. It does so by sketching an ``alignment safety case'' -- an argument that
an AI system will not autonomously take actions which could lead to egregious
harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D
agent inside an AI company sabotaging research, for example by producing false
results. To prevent this, the agent is trained via debate, subject to
exploration guarantees, to teach the system to be honest. Honesty is maintained
throughout deployment via online training. The safety case rests on four key
claims: (1) the agent has become good at the debate game, (2) good performance
in the debate game implies that the system is mostly honest, (3) the system
will not become significantly less honest during deployment, and (4) the
deployment context is tolerant of some errors. We identify open research
problems that, if solved, could render this a compelling argument that an AI
system is safe.

</details>


### [129] [Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest](https://arxiv.org/abs/2505.04019)
*Matteo Ceschin,Leonardo Arrighi,Luca Longo,Sylvio Barbon Junior*

Main category: cs.AI

TL;DR: 本文提出了一种新型可解释AI方法（XAI），通过决策谓词图（DPG）和异常值-正常值传播分数（IOP-Score）提升异常检测方法Isolation Forest（iForest）的可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管iForest在异常检测中表现优异，但其多树结构的复杂性导致解释异常值选择和决策边界困难。研究旨在解决iForest的全局可解释性问题，以促进更可靠的机器学习流程。

Method: 提出基于DPG的方法，结合IOP-Score，解析集成方法的逻辑并提供图形化指标，详细说明样本被识别为异常值的特征贡献和模型决策过程。

Result: 该方法提升了iForest的可解释性，提供了对决策边界的洞察和特征使用的全局视角，推动了可解释机器学习流程的发展。

Conclusion: 研究通过DPG和IOP-Score实现了iForest的全局解释，为异常检测提供了透明且全面的决策过程分析。

Abstract: The need to explain predictive models is well-established in modern machine
learning. However, beyond model interpretability, understanding pre-processing
methods is equally essential. Understanding how data modifications impact model
performance improvements and potential biases and promoting a reliable pipeline
is mandatory for developing robust machine learning solutions. Isolation Forest
(iForest) is a widely used technique for outlier detection that performs well.
Its effectiveness increases with the number of tree-based learners. However,
this also complicates the explanation of outlier selection and the decision
boundaries for inliers. This research introduces a novel Explainable AI (XAI)
method, tackling the problem of global explainability. In detail, it aims to
offer a global explanation for outlier detection to address its opaque nature.
Our approach is based on the Decision Predicate Graph (DPG), which clarifies
the logic of ensemble methods and provides both insights and a graph-based
metric to explain how samples are identified as outliers using the proposed
Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's
explainability and provides a comprehensive view of the decision-making
process, detailing which features contribute to outlier identification and how
the model utilizes them. This method advances the state-of-the-art by providing
insights into decision boundaries and a comprehensive view of holistic feature
usage in outlier identification. -- thus promoting a fully explainable machine
learning pipeline.

</details>


### [130] [Polynomial-Time Relational Probabilistic Inference in Open Universes](https://arxiv.org/abs/2505.04115)
*Luise Ge,Brendan Juba,Kris Nilsson*

Main category: cs.AI

TL;DR: 该论文提出了一种基于人类推理启发的第一阶关系概率推理方法，支持混合变量（离散和连续），并在多项式时间内实现了有界程度片段的提升推理，即使在对象集未知或无限的情况下。


<details>
  <summary>Details</summary>
Motivation: 解决AI中不确定性推理的表达能力与计算可处理性之间的矛盾，模仿人类推理方式提升推理效率。

Method: 扩展期望的平方和逻辑至关系设置，利用有界程度片段和有限量词秩知识库，实现多项式时间内的提升推理。

Result: 证明了该方法在固定程度的平方和反驳中是完备的，并能推导出最紧的证明界限。

Conclusion: 该方法在理论证明框架下实现了高效的关系概率推理，为不确定性推理提供了新的解决方案。

Abstract: Reasoning under uncertainty is a fundamental challenge in Artificial
Intelligence. As with most of these challenges, there is a harsh dilemma
between the expressive power of the language used, and the tractability of the
computational problem posed by reasoning. Inspired by human reasoning, we
introduce a method of first-order relational probabilistic inference that
satisfies both criteria, and can handle hybrid (discrete and continuous)
variables. Specifically, we extend sum-of-squares logic of expectation to
relational settings, demonstrating that lifted reasoning in the bounded-degree
fragment for knowledge bases of bounded quantifier rank can be performed in
polynomial time, even with an a priori unknown and/or countably infinite set of
objects. Crucially, our notion of tractability is framed in proof-theoretic
terms, which extends beyond the syntactic properties of the language or
queries. We are able to derive the tightest bounds provable by proofs of a
given degree and size and establish completeness in our sum-of-squares
refutations for fixed degrees.

</details>


### [131] [Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning](https://arxiv.org/abs/2505.04310)
*Simo Alami C.,Rim Kaddah,Jesse Read,Marie-Paule Cani*

Main category: cs.AI

TL;DR: 本文提出一种基于归一化流的新型分布强化学习架构（DistRL），能够灵活建模无界回报分布，优于依赖固定或有限表示的分类方法（如C51）和基于分位数的方法。该方法参数效率更高，并提出一种新的Cramér距离替代方案以解决现有度量（如KL散度或Wasserstein距离）的局限性。在ATARI-5子基准测试中，该方法优于基于PDF的模型，并与基于分位数的方法竞争。


<details>
  <summary>Details</summary>
Motivation: 动机是解决现有分布强化学习方法中回报分布建模的局限性，包括固定或有限表示、梯度偏差问题，以及对多模态、偏态和尾部行为的捕捉不足。

Method: 方法是通过归一化流建模回报分布，支持无界和灵活的分布表示。提出一种新的几何感知的Cramér距离替代方案，避免昂贵的CDF计算。

Result: 在ATARI-5子基准测试中，该方法优于基于PDF的模型，并保持与基于分位数方法的竞争力。

Conclusion: 结论是提出的归一化流架构和新的距离度量显著提升了分布强化学习的建模能力和效率。

Abstract: We introduce a new architecture for Distributional Reinforcement Learning
(DistRL) that models return distributions using normalizing flows. This
approach enables flexible, unbounded support for return distributions, in
contrast to categorical approaches like C51 that rely on fixed or bounded
representations. It also offers richer modeling capacity to capture
multi-modality, skewness, and tail behavior than quantile based approaches. Our
method is significantly more parameter-efficient than categorical approaches.
Standard metrics used to train existing models like KL divergence or
Wasserstein distance either are scale insensitive or have biased sample
gradients, especially when return supports do not overlap. To address this, we
propose a novel surrogate for the Cram\`er distance, that is geometry-aware and
computable directly from the return distribution's PDF, avoiding the costly CDF
computation. We test our model on the ATARI-5 sub-benchmark and show that our
approach outperforms PDF based models while remaining competitive with quantile
based methods.

</details>


### [132] [KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning](https://arxiv.org/abs/2505.04313)
*Stephen Richard Varey,Alessandro Di Stefano,The Anh Han*

Main category: cs.AI

TL;DR: KERAIA 是一个新的符号知识工程框架，旨在将非结构化的人类专业知识转化为计算机可处理的算法，通过创新的动态知识表示和推理方法解决复杂环境的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究核心问题是如何将非结构化的人类专业知识有效转化为AI系统可用的算法，以解决动态、复杂环境中的知识表示与推理问题。

Method: 基于Minsky的框架推理和K-lines概念，引入知识云动态聚合、动态关系（DRels）、显式推理路径（LoTs）和云精化等创新技术，设计了KSYNTH表示语言和通用范式构建器（GPPB）。

Result: 通过多个案例（如海军作战模拟、工业诊断和战略游戏）验证了KERAIA的通用性、表达力和实用性，并对比了传统知识表示方法的优势。

Conclusion: KERAIA以可解释AI为核心，提供了一种透明、自适应的知识工程方法，超越了静态范式，适用于动态复杂场景。

Abstract: In this paper, we introduce KERAIA, a novel framework and software platform
for symbolic knowledge engineering designed to address the persistent
challenges of representing, reasoning with, and executing knowledge in dynamic,
complex, and context-sensitive environments. The central research question that
motivates this work is: How can unstructured, often tacit, human expertise be
effectively transformed into computationally tractable algorithms that AI
systems can efficiently utilise? KERAIA seeks to bridge this gap by building on
foundational concepts such as Minsky's frame-based reasoning and K-lines, while
introducing significant innovations. These include Clouds of Knowledge for
dynamic aggregation, Dynamic Relations (DRels) for context-sensitive
inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and
Cloud Elaboration for adaptive knowledge transformation. This approach moves
beyond the limitations of traditional, often static, knowledge representation
paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle,
ensuring transparency and interpretability, particularly through the use of
LoTs. The paper details the framework's architecture, the KSYNTH representation
language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse
inference methods within a unified structure. We validate KERAIA's versatility,
expressiveness, and practical applicability through detailed analysis of
multiple case studies spanning naval warfare simulation, industrial diagnostics
in water treatment plants, and strategic decision-making in the game of RISK.
Furthermore, we provide a comparative analysis against established knowledge
representation paradigms (including ontologies, rule-based systems, and
knowledge graphs) and discuss the implementation aspects and computational
considerations of the KERAIA platform.

</details>


### [133] [Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning](https://arxiv.org/abs/2505.04317)
*Ruize Zhang,Sirui Xiang,Zelai Xu,Feng Gao,Shilong Ji,Wenhao Tang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出了一种分层强化学习框架（HCSP），用于解决3v3多无人机排球任务的高层策略协调和底层敏捷控制问题，通过分阶段训练实现了优于非分层和规则基线的性能。


<details>
  <summary>Details</summary>
Motivation: 任务具有长时依赖、紧密的代理耦合和无人机动力学的不完全控制特性，需要同时处理高层策略和底层控制。

Method: 采用分层强化学习框架HCSP，分为低层技能训练、高层策略自我对弈及联合优化三阶段训练。

Result: HCSP平均胜率达82.9%，击败非分层和规则基线，并涌现出角色切换和队形协调等行为。

Conclusion: 分层设计和协作自我对弈能有效解决复杂任务，并促进团队行为涌现。

Abstract: In this paper, we tackle the problem of learning to play 3v3 multi-drone
volleyball, a new embodied competitive task that requires both high-level
strategic coordination and low-level agile control. The task is turn-based,
multi-agent, and physically grounded, posing significant challenges due to its
long-horizon dependencies, tight inter-agent coupling, and the underactuated
dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play
(HCSP), a hierarchical reinforcement learning framework that separates
centralized high-level strategic decision-making from decentralized low-level
motion control. We design a three-stage population-based training pipeline to
enable both strategy and skill to emerge from scratch without expert
demonstrations: (I) training diverse low-level skills, (II) learning high-level
strategy via self-play with fixed low-level controllers, and (III) joint
fine-tuning through co-self-play. Experiments show that HCSP achieves superior
performance, outperforming non-hierarchical self-play and rule-based
hierarchical baselines with an average 82.9\% win rate and a 71.5\% win rate
against the two-stage variant. Moreover, co-self-play leads to emergent team
behaviors such as role switching and coordinated formations, demonstrating the
effectiveness of our hierarchical design and training scheme.

</details>


### [134] [Uncertain Machine Ethics Planning](https://arxiv.org/abs/2505.04352)
*Simon Kolker,Louise A. Dennis,Ramon Fraga Pereira,Mengwei Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于多道德马尔可夫决策过程的算法，用于处理机器伦理决策中的不确定性和道德理论冲突问题，并通过具体案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器伦理决策需要权衡不确定性及不同道德理论（如功利主义、义务论和德行伦理学）之间的潜在冲突，从而实现长期理想结果。

Method: 将问题形式化为多道德马尔可夫决策过程和多道德随机最短路径问题，提出基于多目标AO*的启发式算法，并结合Sven-Ove Hansson的假设回顾程序进行伦理推理。

Result: 通过“是否应偷窃胰岛素以救急”的案例研究，验证了该方法的实用性。

Conclusion: 形式化多道德冲突问题并引入启发式算法，为机器伦理决策提供了可操作的解决方案。

Abstract: Machine Ethics decisions should consider the implications of uncertainty over
decisions. Decisions should be made over sequences of actions to reach
preferable outcomes long term. The evaluation of outcomes, however, may invoke
one or more moral theories, which might have conflicting judgements. Each
theory will require differing representations of the ethical situation. For
example, Utilitarianism measures numerical values, Deontology analyses duties,
and Virtue Ethics emphasises moral character. While balancing potentially
conflicting moral considerations, decisions may need to be made, for example,
to achieve morally neutral goals with minimal costs. In this paper, we
formalise the problem as a Multi-Moral Markov Decision Process and a
Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm
based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical
Retrospection procedure for ethical reasoning under uncertainty. Our approach
is validated by a case study from Machine Ethics literature: the problem of
whether to steal insulin for someone who needs it.

</details>


### [135] [TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2505.04480)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.AI

TL;DR: 论文提出TrajEvo框架，利用大语言模型（LLMs）和进化算法自动设计轨迹预测启发式方法，显著优于传统启发式和深度学习方法，尤其在泛化性能上表现突出。


<details>
  <summary>Details</summary>
Motivation: 传统基于手工规则的启发式方法精度不足，而深度学习方法存在计算成本高、可解释性差和泛化能力有限的问题，限制了实际应用。

Method: TrajEvo结合LLMs和进化算法，通过跨代精英采样（Cross-Generation Elite Sampling）提升种群多样性，并利用统计反馈循环（Statistics Feedback Loop）优化预测。

Result: 在ETH-UCY数据集上TrajEvo优于传统启发式方法；在未见过的SDD数据集上，其泛化性能显著超越启发式和深度学习方法。

Conclusion: TrajEvo为快速、可解释且泛化性强的轨迹预测启发式方法的自动化设计提供了初步解决方案，开源代码以推动未来研究。

Abstract: Trajectory prediction is a crucial task in modeling human behavior,
especially in fields as social robotics and autonomous vehicle navigation.
Traditional heuristics based on handcrafted rules often lack accuracy, while
recently proposed deep learning approaches suffer from computational cost, lack
of explainability, and generalization issues that limit their practical
adoption. In this paper, we introduce TrajEvo, a framework that leverages Large
Language Models (LLMs) to automatically design trajectory prediction
heuristics. TrajEvo employs an evolutionary algorithm to generate and refine
prediction heuristics from past trajectory data. We introduce a
Cross-Generation Elite Sampling to promote population diversity and a
Statistics Feedback Loop allowing the LLM to analyze alternative predictions.
Our evaluations show TrajEvo outperforms previous heuristic methods on the
ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning
methods when generalizing to the unseen SDD dataset. TrajEvo represents a first
step toward automated design of fast, explainable, and generalizable trajectory
prediction heuristics. We make our source code publicly available to foster
future research at https://github.com/ai4co/trajevo.

</details>


### [136] [On some improvements to Unbounded Minimax](https://arxiv.org/abs/2505.04525)
*Quentin Cohen-Solal,Tristan Cazenave*

Main category: cs.AI

TL;DR: 实验评估了四种对无界最佳优先Minimax算法的改进，包括转置表、反向传播策略、启发式函数替代和完成技术，发现特定改进可提升效率。


<details>
  <summary>Details</summary>
Motivation: 探索无界最佳优先Minimax算法的潜在优化，以提高其性能和在游戏树搜索中的效率。

Method: 通过转置表、改进的反向传播策略、启发式函数替代和完成技术四种方法对算法进行修改，并进行了实验比较。

Result: 转置表和改进的反向传播策略提升了性能，启发式函数在成本高时有益，完成技术也有正面效果。

Conclusion: 无界最佳优先Minimax算法可以通过有针对性的修改显著提升效率，特别是在复杂或需要优化的场景中。

Abstract: This paper presents the first experimental evaluation of four previously
untested modifications of Unbounded Best-First Minimax algorithm. This
algorithm explores the game tree by iteratively expanding the most promising
sequences of actions based on the current partial game tree. We first evaluate
the use of transposition tables, which convert the game tree into a directed
acyclic graph by merging duplicate states. Second, we compare the original
algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which
differs in its backpropagation strategy: instead of stopping when a stable
value is encountered, it updates values up to the root. This change slightly
improves performance when value ties or transposition tables are involved.
Third, we assess replacing the exact terminal evaluation function with the
learned heuristic function. While beneficial when exact evaluations are costly,
this modification reduces performance in inexpensive settings. Finally, we
examine the impact of the completion technique that prioritizes resolved
winning states and avoids resolved losing states. This technique also improves
performance. Overall, our findings highlight how targeted modifications can
enhance the efficiency of Unbounded Best-First Minimax.

</details>


### [137] [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
*Qi Liu,Xinhao Zheng,Renqiu Xia,Xingzhi Qi,Qinxiang Cao,Junchi Yan*

Main category: cs.AI

TL;DR: 该研究提出了FPS框架，将问题解决建模为确定性马尔可夫决策过程，并开发了D-FPS以增强人机对齐，同时验证了框架的表达性、完备性。通过三个基准测试评估了现有FTP模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI问题解决代理缺乏过程级可验证性，且缺乏通用而具体的问题解决理论框架。

Method: 提出FPS框架，将问题解决建模为确定性马尔可夫决策过程；开发D-FPS解耦求解与验证；设计RPE方法进行符号化评估。

Result: 在FormalMath500、MiniF2F-Solving和PutnamBench-Solving上的最高解决率分别为23.77%、27.47%和0.31%。

Conclusion: FPS和D-FPS框架填补了问题解决理论及可验证性空白，但现有FTP模型性能仍有提升空间。

Abstract: As a seemingly self-explanatory task, problem-solving has been a significant
component of science and engineering. However, a general yet concrete
formulation of problem-solving itself is missing. With the recent development
of AI-based problem-solving agents, the demand for process-level verifiability
is rapidly increasing yet underexplored. To fill these gaps, we present a
principled formulation of problem-solving as a deterministic Markov decision
process; a novel framework, FPS (Formal Problem-Solving), which utilizes
existing FTP (formal theorem proving) environments to perform process-verified
problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer
verification for better human-alignment. The expressiveness, soundness and
completeness of the frameworks are proven. We construct three benchmarks on
problem-solving: FormalMath500, a formalization of a subset of the MATH500
benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP
benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and
human-aligned evaluation, we propose RPE (Restricted Propositional
Equivalence), a symbolic approach to determine the correctness of answers by
formal verification. We evaluate four prevalent FTP models and two prompting
methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of
MiniF2F-Solving, and 0.31% of PutnamBench-Solving.

</details>


### [138] [Qualitative Analysis of $ω$-Regular Objectives on Robust MDPs](https://arxiv.org/abs/2505.04539)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Kafshdar Goharshady,Mehrdad Karrabi,Ali Shafiee*

Main category: cs.AI

TL;DR: 论文研究了无结构假设（如单链或非周期性）的鲁棒马尔可夫决策过程（RMDPs）中可达性和奇偶性目标的定性分析问题，提出了基于不确定性集合的高效算法，并展示了其在大规模问题中的有效性。


<details>
  <summary>Details</summary>
Motivation: 鲁棒马尔可夫决策过程（RMDPs）通过考虑转移概率的不确定性扩展了经典MDPs，但在无结构假设下的定性分析问题尚未得到充分研究。本文旨在填补这一空白。

Method: 提出了基于不确定性集合的高效算法，通过Oracle访问解决可达性和奇偶性目标的定性问题，无需依赖RMDPs的结构假设。

Result: 实验结果表明，所提出的Oracle方法在经典RMDP示例中有效，并可扩展至数千个状态的大规模问题。

Conclusion: 该工作为无结构假设的RMDPs提供了高效的定性分析解决方案，为实际应用中鲁棒决策提供了有力工具。

Abstract: Robust Markov Decision Processes (RMDPs) generalize classical MDPs that
consider uncertainties in transition probabilities by defining a set of
possible transition functions. An objective is a set of runs (or infinite
trajectories) of the RMDP, and the value for an objective is the maximal
probability that the agent can guarantee against the adversarial environment.
We consider (a) reachability objectives, where given a target set of states,
the goal is to eventually arrive at one of them; and (b) parity objectives,
which are a canonical representation for $\omega$-regular objectives. The
qualitative analysis problem asks whether the objective can be ensured with
probability 1.
  In this work, we study the qualitative problem for reachability and parity
objectives on RMDPs without making any assumption over the structures of the
RMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first
present efficient algorithms with oracle access to uncertainty sets that solve
qualitative problems of reachability and parity objectives. We then report
experimental results demonstrating the effectiveness of our oracle-based
approach on classical RMDP examples from the literature scaling up to thousands
of states.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [139] [PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers](https://arxiv.org/abs/2505.04002)
*Michael Xu,Yi Shi,KangKang Yin,Xue Bin Peng*

Main category: cs.GR

TL;DR: PARC框架利用机器学习和物理模拟，通过迭代增强运动数据集，改进了复杂地形中角色控制的敏捷性。


<details>
  <summary>Details</summary>
Motivation: 实现人类在复杂环境中的敏捷运动（如跑酷）对模拟角色仍然具有挑战性，主要因为相关运动捕捉数据稀缺且获取成本高。

Method: 结合小规模地形穿越技能数据集训练运动生成器，生成合成数据，并通过物理跟踪控制器修正运动中的伪影，迭代优化数据集和模型。

Result: PARC框架成功生成并修正了敏捷地形穿越的运动数据，提升了角色控制器的通用性和适应性。

Conclusion: PARC为克服运动数据稀缺与多样化角色控制器需求之间的鸿沟提供了有效解决方案，增强了复杂环境中的运动控制能力。

Abstract: Humans excel in navigating diverse, complex environments with agile motor
skills, exemplified by parkour practitioners performing dynamic maneuvers, such
as climbing up walls and jumping across gaps. Reproducing these agile movements
with simulated characters remains challenging, in part due to the scarcity of
motion capture data for agile terrain traversal behaviors and the high cost of
acquiring such data. In this work, we introduce PARC (Physics-based
Augmentation with Reinforcement Learning for Character Controllers), a
framework that leverages machine learning and physics-based simulation to
iteratively augment motion datasets and expand the capabilities of terrain
traversal controllers. PARC begins by training a motion generator on a small
dataset consisting of core terrain traversal skills. The motion generator is
then used to produce synthetic data for traversing new terrains. However, these
generated motions often exhibit artifacts, such as incorrect contacts or
discontinuities. To correct these artifacts, we train a physics-based tracking
controller to imitate the motions in simulation. The corrected motions are then
added to the dataset, which is used to continue training the motion generator
in the next iteration. PARC's iterative process jointly expands the
capabilities of the motion generator and tracker, creating agile and versatile
models for interacting with complex environments. PARC provides an effective
approach to develop controllers for agile terrain traversal, which bridges the
gap between the scarcity of motion data and the need for versatile character
controllers.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [140] [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
*Shigeki Karita,Yuma Koizumi,Heiga Zen,Haruko Ishikawa,Robin Scheibler,Michiel Bacchiani*

Main category: cs.SD

TL;DR: Miipher-2是一种基于生成模型的语音修复（SR）模型，专为百万小时级数据清理设计，适用于大规模生成模型（如大语言模型）的训练数据清洗。它解决了泛化到未见语言、无需显式条件输入（如文本、说话人ID）及计算效率等挑战，显著提升了语音质量与处理速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模生成模型训练数据中的噪声问题，同时支持多语言泛化、无需显式条件输入，并提升计算效率，设计了Miipher-2这一语音修复模型。

Method: 利用预训练且冻结参数的通用语音模型（USM，支持300+语言）作为特征提取器，结合并行适配器预测干净特征，并使用WaneFit神经声码器合成波形。模型在3,000小时多语言高质量录音（含增强退化）上训练。

Result: 实验表明，Miipher-2在词错误率、说话人相似度及主客观音质评分上优于或媲美传统SR模型，且计算高效（实时因子0.0078），仅需100个消费级加速器即可在约3天内处理百万小时数据。

Conclusion: Miipher-2为大规模语音数据清洗提供了高效、多语言适配的解决方案，显著提升了数据质量与处理效率，适用于生成模型的训练优化。

Abstract: Training data cleaning is a new application for generative model-based speech
restoration (SR). This paper introduces Miipher-2, an SR model designed for
million-hour scale data, for training data cleaning for large-scale generative
models like large language models. Key challenges addressed include
generalization to unseen languages, operation without explicit conditioning
(e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a
frozen, pre-trained Universal Speech Model (USM), supporting over 300
languages, as a robust, conditioning-free feature extractor. To optimize
efficiency and minimize memory, Miipher-2 incorporates parallel adapters for
predicting clean USM features from noisy inputs and employs the WaneFit neural
vocoder for waveform synthesis. These components were trained on 3,000 hours of
multi-lingual, studio-quality recordings with augmented degradations, while USM
parameters remained fixed. Experimental results demonstrate Miipher-2's
superior or comparable performance to conventional SR models in
word-error-rate, speaker similarity, and both objective and subjective sound
quality scores across all tested languages. Miipher-2 operates efficiently on
consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling
the processing of a million-hour speech dataset in approximately three days
using only 100 such accelerators.

</details>


### [141] [Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform](https://arxiv.org/abs/2505.04451)
*Yohannis Telila,Tommaso Cucinotta,Davide Bacciu*

Main category: cs.SD

TL;DR: 论文提出了一种处理流程，将古典钢琴音频文件转换为乐谱表示，使用恒定Q变换提取特征，并利用卷积神经网络（CNN）模型进行处理。


<details>
  <summary>Details</summary>
Motivation: 自动音乐转录（AMT）是一个挑战性问题，尤其是复调音乐。目标是通过分析包含多个同时播放音符的音频信号，生成音乐作品的乐谱表示。

Method: 设计了一个处理流程，首先利用恒定Q变换从音频信号中提取特征，然后将这些系数作为卷积神经网络（CNN）模型的输入。

Result: 该方法能够有效将古典钢琴音频文件转换为乐谱表示。

Conclusion: 通过结合恒定Q变换和CNN模型，实现了复调音乐的自动转录。

Abstract: Automatic music transcription (AMT) is the problem of analyzing an audio
recording of a musical piece and detecting notes that are being played. AMT is
a challenging problem, particularly when it comes to polyphonic music. The goal
of AMT is to produce a score representation of a music piece, by analyzing a
sound signal containing multiple notes played simultaneously. In this work, we
design a processing pipeline that can transform classical piano audio files in
.wav format into a music score representation. The features from the audio
signals are extracted using the constant-Q transform, and the resulting
coefficients are used as an input to the convolutional neural network (CNN)
model.

</details>


### [142] [Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond](https://arxiv.org/abs/2505.04621)
*Jessie Richter-Powell,Antonio Torralba,Jonathan Lorraine*

Main category: cs.SD

TL;DR: Audio-SDS将SDS方法推广到音频领域，利用预训练模型支持多种任务，无需专用数据集，展示了基于蒸馏方法的跨模态潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在扩展SDS方法至音频领域，利用生成先验知识解决多样音频任务，避免对专业数据集的依赖。

Method: 采用Audio-SDS方法，基于预训练模型，通过蒸馏生成先验知识到参数化表示中，支持物理模拟、FM合成参数校准及特定源分离任务。

Result: Audio-SDS在多种音频任务中表现优异，验证了蒸馏方法在跨模态应用中的广泛适用性和有效性。

Conclusion: 研究证实Audio-SDS为音频任务提供了通用且强大的解决方案，并为未来基于生成先验的研究奠定了基础。

Abstract: We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS)
to text-conditioned audio diffusion models. While SDS was initially designed
for text-to-3D generation using image diffusion, its core idea of distilling a
powerful generative prior into a separate parametric representation extends to
the audio domain. Leveraging a single pretrained model, Audio-SDS enables a
broad range of tasks without requiring specialized datasets. In particular, we
demonstrate how Audio-SDS can guide physically informed impact sound
simulations, calibrate FM-synthesis parameters, and perform prompt-specified
source separation. Our findings illustrate the versatility of
distillation-based methods across modalities and establish a robust foundation
for future work using generative priors in audio tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [143] [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
*Yogesh Gajula*

Main category: cs.IR

TL;DR: 本文综述了2023年至2025年初基于情感分析的电子商务推荐系统，强调通过自然语言处理提升推荐准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统过于依赖数值评分，忽视了用户文本反馈中的细粒度情感信息，因此需要整合情感分析以优化推荐效果。

Method: 文章总结了四类方法：结合情感嵌入与用户-物品交互的深度学习分类器、基于Transformer的细粒度特征提取、传播情感信号的图神经网络，以及实时适应用户反馈的对话推荐系统。

Result: 研究表明，情感分析能显著提升推荐准确性，并通过意见提取增强解释性，但需解决噪声文本、动态偏好和偏见等问题。

Conclusion: 作者指出了研究空白，并提出了开发更智能、公平、以用户为中心的推荐工具的未来方向。

Abstract: E-commerce platforms generate vast volumes of user feedback, such as star
ratings, written reviews, and comments. However, most recommendation engines
rely primarily on numerical scores, often overlooking the nuanced opinions
embedded in free text. This paper comprehensively reviews sentiment-aware
recommendation systems from a natural language processing perspective, covering
advancements from 2023 to early 2025. It highlights the benefits of integrating
sentiment analysis into e-commerce recommenders to enhance prediction accuracy
and explainability through detailed opinion extraction. Our survey categorizes
recent work into four main approaches: deep learning classifiers that combine
sentiment embeddings with user item interactions, transformer based methods for
nuanced feature extraction, graph neural networks that propagate sentiment
signals, and conversational recommenders that adapt in real time to user
feedback. We summarize model architectures and demonstrate how sentiment flows
through recommendation pipelines, impacting dialogue-based suggestions. Key
challenges include handling noisy or sarcastic text, dynamic user preferences,
and bias mitigation. Finally, we outline research gaps and provide a roadmap
for developing smarter, fairer, and more user-centric recommendation tools.

</details>


### [144] [Memory Assisted LLM for Personalized Recommendation System](https://arxiv.org/abs/2505.03824)
*Jiarui Chen*

Main category: cs.IR

TL;DR: 论文提出了记忆辅助个性化大模型（MAP），通过用户历史偏好构建记忆库，提升推荐效果，优于传统基于提示设计的LLM推荐方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化LLM方法成本高且效率低，无法及时更新用户历史偏好，MAP旨在解决这些问题。

Method: 构建用户历史偏好档案，通过相似性提取相关记忆并融入提示，增强个性化推荐。

Result: MAP在单领域和跨领域的评分预测任务中表现优于传统方法，且随着用户历史增长优势更明显。

Conclusion: MAP能有效处理连续个性化请求，适合实际应用场景。

Abstract: Large language models (LLMs) have demonstrated significant potential in
solving recommendation tasks. With proven capabilities in understanding user
preferences, LLM personalization has emerged as a critical area for providing
tailored responses to individuals. Current studies explore personalization
through prompt design and fine-tuning, paving the way for further research in
personalized LLMs. However, existing approaches are either costly and
inefficient in capturing diverse user preferences or fail to account for timely
updates to user history. To address these gaps, we propose the Memory-Assisted
Personalized LLM (MAP). Through user interactions, we first create a history
profile for each user, capturing their preferences, such as ratings for
historical items. During recommendation, we extract relevant memory based on
similarity, which is then incorporated into the prompts to enhance personalized
recommendations. In our experiments, we evaluate MAP using a sequential rating
prediction task under two scenarios: single domain, where memory and tasks are
from the same category (e.g., movies), and cross-domain (e.g., memory from
movies and recommendation tasks in books). The results show that MAP
outperforms regular LLM-based recommenders that integrate user history directly
through prompt design. Moreover, as user history grows, MAP's advantage
increases in both scenarios, making it more suitable for addressing successive
personalized user requests.

</details>


### [145] [OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery](https://arxiv.org/abs/2505.03836)
*Chongsheng Zhang,Shuwen Wu,Yingqi Chen,Matthias Aßenmacher,Christian Heumann,Yi Men,Gaojuan Fan,João Gama*

Main category: cs.IR

TL;DR: 该论文提出了一个渐进式的甲骨文重复发现框架，结合无监督低层关键点匹配和高层文本内容匹配，显著提高了甲骨文重复识别的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 甲骨文是中国最早的系统文字，而甲骨文重复识别是甲骨文研究的基础问题。现有的方法在准确性和效率上存在不足，需要一种更有效的方法。

Method: 采用渐进式框架，结合无监督低层关键点匹配和高层文本内容匹配，优化并排序候选重复甲骨文。

Result: 与现有方法相比，本方法在Top-5和Top-15检索结果中表现出更高的召回率和简化平均倒数排名，计算效率显著提升，并发现了60多对新的甲骨文重复。

Conclusion: 该方法不仅提高了甲骨文重复识别的准确性和效率，还发现了此前几十年被忽视的新重复对，对甲骨文研究具有重要意义。

Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in
China, while the identification of Oracle Bone (OB) duplicates is a fundamental
issue in OBI research. In this work, we design a progressive OB duplicate
discovery framework that combines unsupervised low-level keypoints matching
with high-level text-centric content-based matching to refine and rank the
candidate OB duplicates with semantic awareness and interpretability. We
compare our approach with state-of-the-art content-based image retrieval and
image matching methods, showing that our approach yields comparable recall
performance and the highest simplified mean reciprocal rank scores for both
Top-5 and Top-15 retrieval results, and with significantly accelerated
computation efficiency. We have discovered over 60 pairs of new OB duplicates
in real-world deployment, which were missed by OBI researchers for decades. The
models, video illustration and demonstration of this work are available at:
https://github.com/cszhangLMU/OBD-Finder/.

</details>


### [146] [CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation](https://arxiv.org/abs/2505.03840)
*Cairong Yan,Jinyi Han,Jin Ju,Yanting Zhang,Zijian Wang,Xuan Shao*

Main category: cs.IR

TL;DR: 该论文提出了自适应协作组合多臂老虎机算法（CoCoB），通过双面老虎机架构动态调整用户相似性，提高推荐质量。


<details>
  <summary>Details</summary>
Motivation: 传统聚类老虎机方法在用户相似性定义不明确或用户偏好独特时推荐效果不佳，需要一种更灵活的方法。

Method: CoCoB采用双面老虎机架构，用户端老虎机通过贝叶斯模型探索用户相似性，项目端老虎机生成多样化推荐。

Result: 在线性上下文老虎机设置下的遗憾分析和三个真实数据集实验中，CoCoB的平均F1分数提升了2.4%。

Conclusion: CoCoB通过动态调整用户相似性和推荐策略，显著提升了推荐系统的性能。

Abstract: Clustering bandits have gained significant attention in recommender systems
by leveraging collaborative information from neighboring users to better
capture target user preferences. However, these methods often lack a clear
definition of similar users and face challenges when users with unique
preferences lack appropriate neighbors. In such cases, relying on divergent
preferences of misidentified neighbors can degrade recommendation quality. To
address these limitations, this paper proposes an adaptive Collaborative
Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided
bandit architecture, applying bandit principles to both the user and item
sides. The user-bandit employs an enhanced Bayesian model to explore user
similarity, identifying neighbors based on a similarity probability threshold.
The item-bandit treats items as arms, generating diverse recommendations
informed by the user-bandit's output. CoCoB dynamically adapts, leveraging
neighbor preferences when available or focusing solely on the target user
otherwise. Regret analysis under a linear contextual bandit setting and
experiments on three real-world datasets demonstrate CoCoB's effectiveness,
achieving an average 2.4% improvement in F1 score over state-of-the-art
methods.

</details>


### [147] [To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay](https://arxiv.org/abs/2505.04209)
*Soumik Dey,Hansi Wu,Binbin Li*

Main category: cs.IR

TL;DR: 通过卖家判断来优化广告关键词相关性模型，利用LLM作为人类判断的替代，提升系统间的协调性。


<details>
  <summary>Details</summary>
Motivation: 避免搜索系统因大量不相关广告关键词而过载，同时维护卖家体验，需确保关键词推荐与卖家判断一致。

Method: 提出框架分析卖家判断、广告系统和搜索系统的动态交互，并利用LLM作为人类判断的规模化代理训练模型。

Result: 在eBay广告案例中证明，结合业务指标的评估框架下，LLM能有效替代人类判断，提升模型效果。

Conclusion: 基于人类判断的LLM代理可规模化优化关键词相关性模型，实现三系统间的协调平衡。

Abstract: E-commerce sellers are recommended keyphrases based on their inventory on
which they advertise to increase buyer engagement (clicks/sales). The relevance
of advertiser keyphrases plays an important role in preventing the inundation
of search systems with numerous irrelevant items that compete for attention in
auctions, in addition to maintaining a healthy seller perception. In this work,
we describe the shortcomings of training Advertiser keyphrase relevance filter
models on click/sales/search relevance signals and the importance of aligning
with human judgment, as sellers have the power to adopt or reject said
keyphrase recommendations. In this study, we frame Advertiser keyphrase
relevance as a complex interaction between 3 dynamical systems -- seller
judgment, which influences seller adoption of our product, Advertising, which
provides the keyphrases to bid on, and Search, who holds the auctions for the
same keyphrases. This study discusses the practicalities of using human
judgment via a case study at eBay Advertising and demonstrate that using
LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our
relevance models achieves a better harmony across the three systems -- provided
that they are bound by a meticulous evaluation framework grounded in business
metrics.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [148] [IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification](https://arxiv.org/abs/2505.03838)
*Ting Yu Tsai,An Yu,Meghana Spurthi Maadugundu,Ishrat Jahan Mohima,Umme Habiba Barsha,Mei-Hwa F. Chen,Balakrishnan Prabhakaran,Ming-Ching Chang*

Main category: eess.IV

TL;DR: 该论文提出了一种基于AI的医疗图像处理平台IntelliCardiac，用于4D心脏图像的自动分割和疾病分类，在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 心脏病数据的精确处理对心血管疾病的识别和管理至关重要，但现有方法在准确性和效率上仍有提升空间。

Method: 结合深度学习分割模型和两步分类流程，支持左右心室及心肌分析，并分类五种心脏疾病。

Result: 分割模块准确率92.6%，分类模块在五类疾病上达到98%准确率，超越现有方法。

Conclusion: IntelliCardiac具备实时可视化与临床工作流集成能力，有望成为心脏影像诊断的高效工具。

Abstract: Precise and effective processing of cardiac imaging data is critical for the
identification and management of the cardiovascular diseases. We introduce
IntelliCardiac, a comprehensive, web-based medical image processing platform
for the automatic segmentation of 4D cardiac images and disease classification,
utilizing an AI model trained on the publicly accessible ACDC dataset. The
system, intended for patients, cardiologists, and healthcare professionals,
offers an intuitive interface and uses deep learning models to identify
essential heart structures and categorize cardiac diseases. The system supports
analysis of both the right and left ventricles as well as myocardium, and then
classifies patient's cardiac images into five diagnostic categories: dilated
cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right
ventricular abnormality, and no disease. IntelliCardiac combines a deep
learning-based segmentation model with a two-step classification pipeline. The
segmentation module gains an overall accuracy of 92.6\%. The classification
module, trained on characteristics taken from segmented heart structures,
achieves 98\% accuracy in five categories. These results exceed the performance
of the existing state-of-the-art methods that integrate both segmentation and
classification models. IntelliCardiac, which supports real-time visualization,
workflow integration, and AI-assisted diagnostics, has great potential as a
scalable, accurate tool for clinical decision assistance in cardiac imaging and
diagnosis.

</details>


### [149] [From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation](https://arxiv.org/abs/2505.03844)
*Solène Debuysère,Nicolas Trouvé,Nathan Letheule,Olivier Lévêque,Elise Colin*

Main category: eess.IV

TL;DR: 该论文提出了一种利用空间条件技术将卫星合成孔径雷达（SAR）图像转换为机载SAR表示的新方法，填补了开放源代码SAR数据集稀缺的空白，并通过大量数据和预训练模型展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于高分辨率机载SAR图像获取成本高且数据稀缺，限制了现有基础模型在遥感应用中的使用，该研究旨在通过合成图像生成技术解决这一问题。

Method: 研究利用ONERA多年的机载数据构建了11万张SAR图像数据集，并采用35亿参数的预训练潜在扩散模型，结合空间条件技术将卫星SAR图像转换为机载SAR表示。

Result: 该方法成功提升了ONERA物理模拟器EMPRISE生成图像的逼真度，为SAR成像技术的进步提供了关键应用。

Conclusion: 该研究首次在文献中提出了一种创新的AI应用方法，通过合成数据生成解决了SAR数据稀缺的问题，推动了SAR成像技术的发展。

Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has
increased considerably in recent years, with datasets commercially available.
However, the acquisition of high-resolution SAR images in airborne
configurations, remains costly and limited. Thus, the lack of open source,
well-labeled, or easily exploitable SAR text-image datasets is a barrier to the
use of existing foundation models in remote sensing applications. In this
context, synthetic image generation is a promising solution to augment this
scarce data, enabling a broader range of applications. Leveraging over 15 years
of ONERA's extensive archival airborn data from acquisition campaigns, we
created a comprehensive training dataset of 110 thousands SAR images to exploit
a 3.5 billion parameters pre-trained latent diffusion model. In this work, we
present a novel approach utilizing spatial conditioning techniques within a
foundation model to transform satellite SAR imagery into airborne SAR
representations. Additionally, we demonstrate that our pipeline is effective
for bridging the realism of simulated images generated by ONERA's physics-based
simulator EMPRISE. Our method explores a key application of AI in advancing SAR
imaging technology. To the best of our knowledge, we are the first to introduce
this approach in the literature.

</details>


### [150] [A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos](https://arxiv.org/abs/2505.03845)
*Ioannis Kyprakis,Vasileios Skaramagkas,Iro Boura,Georgios Karamanis,Dimitrios I. Fotiadis,Zinovia Kefalopoulou,Cleanthe Spanaki,Manolis Tsiknakis*

Main category: eess.IV

TL;DR: 摘要：本研究探索了三种深度学习模型（ViViT、Video Swin Tiny 和 3D CNN-LSTM）用于通过面部视频分析评估帕金森病患者的抑郁症状存在及严重程度。研究使用了 1,875 个视频的数据集，Video Swin Tiny 模型在二元分类和多类分类任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 帕金森病患者的抑郁症状常被忽视，尤其是其面部表情（如表情减少）与抑郁症状叠加。研究希望通过深度学习模型提高抑郁症状的检测效率。

Method: 研究使用三种深度学习模型（ViViT、Video Swin Tiny 和 3D CNN-LSTM）对患者的面部视频进行分析，采用 GDS 量表作为抑郁症状评估标准，并考虑了患者在用药和停药状态下的差异。

Result: Video Swin Tiny 模型表现最好，二元分类准确率达 94%，F1 分数为 93.7%；多类分类准确率达 87.1%，F1 分数为 85.4%。

Conclusion: 视频分析结合深度学习模型，尤其是 Video Swin Tiny，能有效识别和评估帕金森病患者的抑郁症状，为临床诊断提供辅助工具。

Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with
motor and non-motor symptoms. Depressive symptoms are prevalent in PD,
affecting up to 45% of patients. They are often underdiagnosed due to
overlapping motor features, such as hypomimia. This study explores deep
learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention
layers-to assess the presence and severity of depressive symptoms, as detected
by the Geriatric Depression Scale (GDS), in PD patients through facial video
analysis. The same parameters were assessed in a secondary analysis taking into
account whether patients were one hour after (ON-medication state) or 12 hours
without (OFF-medication state) dopaminergic medication. Using a dataset of
1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest
performance, with up to 94% accuracy and 93.7% F1-score in binary
classification (presence of absence of depressive symptoms), and 87.1% accuracy
with an 85.4% F1-score in multiclass tasks (absence or mild or severe
depressive symptoms).

</details>


### [151] [3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation](https://arxiv.org/abs/2505.04097)
*Thien Nhan Vo,Bac Nam Ho,Thanh Xuan Truong*

Main category: eess.IV

TL;DR: 该论文开发了一个三维卷积神经网络，用于通过T1加权脑部MRI扫描分类健康或阿尔茨海默病患者，通过噪声注入和交叉验证取得了高准确率和ROC曲线下面积。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用三维卷积神经网络提升脑部MRI分类的准确性，探索数据增强方法的效果。

Method: 采用3D卷积、池化、批量归一化、ReLU层和Sigmoid输出结构，结合噪声注入和五折交叉验证。

Result: 模型测试集准确率为0.912，ROC曲线下面积为0.961，敏感性和特异性均超过0.90，优于仅使用尺寸调整的方法。

Conclusion: 研究表明简单数据增强对3D MRI分类有效，并鼓励未来探索更高级的增强方法和架构如3D U-Net和视觉变换器。

Abstract: A three-dimensional convolutional neural network was developed to classify
T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D
convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid
output. Using stochastic noise injection and five-fold cross-validation, the
model achieved test set accuracy of 0.912 and area under the ROC curve of
0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity
and specificity both exceeded 0.90. These results align with prior work
reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate
the effectiveness of simple augmentation for 3D MRI classification and motivate
future exploration of advanced augmentation methods and architectures such as
3D U-Net and vision transformers.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [152] [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
*Trinh T. L. Vuong,Jin Tae Kwak*

Main category: cs.CV

TL;DR: VideoPath-LLaVA是一种多模态模型，结合了病理学中的单张图像、自动提取的关键帧和手动分割的视频，模拟病理诊断过程。通过生成详细描述并最终诊断，它连接了视觉信息与诊断推理。


<details>
  <summary>Details</summary>
Motivation: 旨在模拟病理学家的自然诊断过程，解决高质量数据稀缺且制作耗时的问题。

Method: 使用VideoPath-Instruct数据集（4278个视频和诊断教学对），利用现有单图像数据集的知识迁移训练关键帧片段，再微调手动分割视频。

Result: 在病理视频分析中设定了新标准，为支持临床决策的AI系统提供了基础。

Conclusion: VideoPath-LLaVA为未来结合视觉和诊断推理的AI系统提供了潜力，数据与模型已开源。

Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in
computational pathology that integrates three distinct image scenarios, single
patch images, automatically keyframe-extracted clips, and manually segmented
video pathology images, to mimic the natural diagnostic process of
pathologists. By generating detailed histological descriptions and culminating
in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives
with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278
video and diagnosis-specific chain-of-thought instructional pairs sourced from
educational histopathology videos on YouTube. Although high-quality data is
critical for enhancing diagnostic reasoning, its creation is time-intensive and
limited in volume. To overcome this challenge, we transfer knowledge from
existing single-image instruction datasets to train on weakly annotated,
keyframe-extracted clips, followed by fine-tuning on manually segmented videos.
VideoPath-LLaVA establishes a new benchmark in pathology video analysis and
offers a promising foundation for future AI systems that support clinical
decision-making through integrated visual and diagnostic reasoning. Our code,
data, and model are publicly available at
https://github.com/trinhvg/VideoPath-LLaVA.

</details>


### [153] [Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models](https://arxiv.org/abs/2505.03821)
*Gracjan Góral,Alicja Ziarko,Piotr Miłoś,Michał Nauman,Maciej Wołczyk,Michał Kosiński*

Main category: cs.CV

TL;DR: 该论文研究了视觉语言模型（VLMs）在视觉透视任务中的表现，发现模型在场景理解上表现良好，但空间推理和视角转换能力较差。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在复杂视觉任务（如视觉透视）中的能力，以评估其在更高层次视觉认知上的局限性。

Method: 通过系统变化空间配置（如物体位置和人物朝向）创建144个视觉任务，并提出7个诊断问题评估三个认知层级。

Result: 测试了多个先进模型（如GPT-4-Turbo、Llama-3等），发现其在场景理解上表现优秀，但空间推理和视角转换能力显著下降。

Conclusion: VLMs在深层空间和视角推理上存在不足，未来需整合几何表示和针对性训练以提升性能。

Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual
perspective taking using a novel set of visual tasks inspired by established
human tests. Our approach leverages carefully controlled scenes, in which a
single humanoid minifigure is paired with a single object. By systematically
varying spatial configurations - such as object position relative to the
humanoid minifigure and the humanoid minifigure's orientation - and using both
bird's-eye and surface-level views, we created 144 unique visual tasks. Each
visual task is paired with a series of 7 diagnostic questions designed to
assess three levels of visual cognition: scene understanding, spatial
reasoning, and visual perspective taking. Our evaluation of several
state-of-the-art models, including GPT-4-Turbo, GPT-4o,
Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that
while they excel in scene understanding, the performance declines significantly
on spatial reasoning and further deteriorates on perspective-taking. Our
analysis suggests a gap between surface-level object recognition and the deeper
spatial and perspective reasoning required for complex visual tasks, pointing
to the need for integrating explicit geometric representations and tailored
training protocols in future VLM development.

</details>


### [154] [In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry](https://arxiv.org/abs/2505.03826)
*Minji Kang,Seongho Kim,Eunseo Go,Donghyeon Paek,Geon Lim,Muyoung Kim,Soyeun Kim,Sung Kyu Jang,Min Sup Choi,Woo Seok Kang,Jaehyun Kim,Jaekwang Kim,Hyeong-U Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于机器学习（ML）的非接触式原位蚀刻深度预测框架，用于半导体制造中的蚀刻深度和绝缘材料厚度监测，通过人工神经网络（ANN）和贝叶斯神经网络（BNN）分别优化预测精度和不确定性估计，并结合数字图像比色法（DIC）实现低成本实时监测。


<details>
  <summary>Details</summary>
Motivation: 为解决传统离位分析方法的时间延迟和污染风险问题，研究旨在开发一种实时、非接触的原位监测方法，以提升半导体制造的工艺稳定性和生产效率。

Method: 采用人工神经网络（ANN）从工艺参数预测平均蚀刻深度，并扩展至贝叶斯神经网络（BNN）以捕捉测量变异性和不确定性；同时验证了数字图像比色法（DIC）结合RGB数据的可行性。

Result: ANN相比线性基线模型显著降低均方误差（MSE）；BNN能可靠估计不确定性；DIC数据即使无需工艺参数也能实现强预测性能。

Conclusion: 机器学习与DIC的结合为等离子蚀刻工艺提供了实时、原位且经济高效的非侵入式监测方案，可增强工艺稳定性和制造效率。

Abstract: Precise monitoring of etch depth and the thickness of insulating materials,
such as Silicon dioxide and silicon nitride, is critical to ensuring device
performance and yield in semiconductor manufacturing. While conventional
ex-situ analysis methods are accurate, they are constrained by time delays and
contamination risks. To address these limitations, this study proposes a
non-contact, in-situ etch depth prediction framework based on machine learning
(ML) techniques. Two scenarios are explored. In the first scenario, an
artificial neural network (ANN) is trained to predict average etch depth from
process parameters, achieving a significantly lower mean squared error (MSE)
compared to a linear baseline model. The approach is then extended to
incorporate variability from repeated measurements using a Bayesian Neural
Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage
analysis confirms the BNN's capability to provide reliable uncertainty
estimates. In the second scenario, we demonstrate the feasibility of using RGB
data from digital image colorimetry (DIC) as input for etch depth prediction,
achieving strong performance even in the absence of explicit process
parameters. These results suggest that the integration of DIC and ML offers a
viable, cost-effective alternative for real-time, in-situ, and non-invasive
monitoring in plasma etching processes, contributing to enhanced process
stability, and manufacturing efficiency.

</details>


### [155] [VideoLLM Benchmarks and Evaluation: A Survey](https://arxiv.org/abs/2505.03829)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: 本文综述了视频大型语言模型（VideoLLMs）的评估基准与方法，分析了当前视频理解基准的特点、评估协议及其局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，视频理解技术取得显著进展。本文旨在为研究者提供VideoLLMs的评估框架，推动视频理解领域的进步。

Method: 通过分析现有视频理解基准（如封闭集、开放集及时间/时空任务评估），总结VideoLLMs的性能趋势和评估框架的挑战。

Result: 揭示了当前评估框架的局限性，并指出多样化、多模态和可解释性基准的重要性。

Conclusion: 本文为VideoLLMs评估提供结构化指导，并提出未来研究方向，以促进视频理解技术的发展。

Abstract: The rapid development of Large Language Models (LLMs) has catalyzed
significant advancements in video understanding technologies. This survey
provides a comprehensive analysis of benchmarks and evaluation methodologies
specifically designed or used for Video Large Language Models (VideoLLMs). We
examine the current landscape of video understanding benchmarks, discussing
their characteristics, evaluation protocols, and limitations. The paper
analyzes various evaluation methodologies, including closed-set, open-set, and
specialized evaluations for temporal and spatiotemporal understanding tasks. We
highlight the performance trends of state-of-the-art VideoLLMs across these
benchmarks and identify key challenges in current evaluation frameworks.
Additionally, we propose future research directions to enhance benchmark
design, evaluation metrics, and protocols, including the need for more diverse,
multimodal, and interpretability-focused benchmarks. This survey aims to equip
researchers with a structured understanding of how to effectively evaluate
VideoLLMs and identify promising avenues for advancing the field of video
understanding with large language models.

</details>


### [156] [Video Forgery Detection for Surveillance Cameras: A Review](https://arxiv.org/abs/2505.03832)
*Noor B. Tayfor,Tarik A. Rashid,Shko M. Qader,Bryar A. Hassan,Mohammed H. Abdalla,Jafar Majidpour,Aram M. Ahmed,Hussein M. Ali,Aso M. Aladdin,Abdulhady A. Abdullah,Ahmed S. Shamsaldin,Haval M. Sidqi,Abdulrahman Salih,Zaher M. Yaseen,Azad A. Ameen,Janmenjoy Nayak,Mahmood Yashar Hamza*

Main category: cs.CV

TL;DR: 本文综述了用于检测视频伪造的现有法证技术，强调了在监控记录中验证真实性的重要性，并探讨了不同方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着视频编辑工具的普及，数字视频篡改变得容易，威胁到视频证据的真实性，可能影响司法公正。因此，需要有效的法证技术来确保监控视频的完整性。

Method: 介绍了多种技术，包括基于压缩的分析、帧重复检测和机器学习方法，用于检测视频伪造。

Result: 研究发现需要更强大的法证技术以应对不断演变的伪造方法，确保监控视频的可靠性。

Conclusion: 加强视频法证能力是确保监控记录可信和可作为法律证据的关键。

Abstract: The widespread availability of video recording through smartphones and
digital devices has made video-based evidence more accessible than ever.
Surveillance footage plays a crucial role in security, law enforcement, and
judicial processes. However, with the rise of advanced video editing tools,
tampering with digital recordings has become increasingly easy, raising
concerns about their authenticity. Ensuring the integrity of surveillance
videos is essential, as manipulated footage can lead to misinformation and
undermine judicial decisions. This paper provides a comprehensive review of
existing forensic techniques used to detect video forgery, focusing on their
effectiveness in verifying the authenticity of surveillance recordings. Various
methods, including compression-based analysis, frame duplication detection, and
machine learning-based approaches, are explored. The findings highlight the
growing necessity for more robust forensic techniques to counteract evolving
forgery methods. Strengthening video forensic capabilities will ensure that
surveillance recordings remain credible and admissible as legal evidence.

</details>


### [157] [PointExplainer: Towards Transparent Parkinson's Disease Diagnosis](https://arxiv.org/abs/2505.03833)
*Xuechao Wang,Sven Nomm,Junqing Huang,Kadri Medijainen,Aaro Toomela,Michael Ruzhansky*

Main category: cs.CV

TL;DR: PointExplainer是一种可解释的诊断策略，用于分析手绘信号以早期诊断帕金森病，通过量化手绘段落的相对贡献提供直观解释，且不影响诊断性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有诊断方法缺乏可解释性的问题，提升临床信任度。

Method: 结合诊断模块（将手绘信号编码为3D点云）和解释模块（训练可解释的替代模型），并引入一致性度量确保解释忠实性。

Result: 在多个数据集上验证，PointExplainer能提供直观解释且诊断性能未下降。

Conclusion: PointExplainer为帕金森病早期诊断提供了一种有效且可解释的新方法。

Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn
signals for early diagnosis of Parkinson's disease. However, the lack of clear
interpretability in existing diagnostic methods presents a challenge to
clinical trust. In this paper, we propose PointExplainer, an explainable
diagnostic strategy to identify hand-drawn regions that drive model diagnosis.
Specifically, PointExplainer assigns discrete attribution values to hand-drawn
segments, explicitly quantifying their relative contributions to the model's
decision. Its key components include: (i) a diagnosis module, which encodes
hand-drawn signals into 3D point clouds to represent hand-drawn trajectories,
and (ii) an explanation module, which trains an interpretable surrogate model
to approximate the local behavior of the black-box diagnostic model. We also
introduce consistency measures to further address the issue of faithfulness in
explanations. Extensive experiments on two benchmark datasets and a newly
constructed dataset show that PointExplainer can provide intuitive explanations
with no diagnostic performance degradation. The source code is available at
https://github.com/chaoxuewang/PointExplainer.

</details>


### [158] [Explainable Face Recognition via Improved Localization](https://arxiv.org/abs/2505.03837)
*Rashik Shadman,Daqing Hou,Faraz Hussain,M G Sarwar Murshed*

Main category: cs.CV

TL;DR: 提出了一种基于SDD的CAM方法，用于提高深度学习人脸识别系统的可解释性，增强用户信任。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习人脸识别系统缺乏解释性，用户难以信任这些“黑盒”模型，因此需要一种方法提供透明的决策依据。

Method: 采用Scaled Directed Divergence（SDD）结合CAM技术，精确定位影响模型决策的面部特征。

Result: 实验表明，SDD-CAM比传统CAM更精准地突出相关面部特征，显著提升解释的准确性。

Conclusion: SDD-CAM能为深度学习人脸识别系统提供高精度的可视化解释，有效增强透明度和用户信任。

Abstract: Biometric authentication has become one of the most widely used tools in the
current technological era to authenticate users and to distinguish between
genuine users and imposters. Face is the most common form of biometric modality
that has proven effective. Deep learning-based face recognition systems are now
commonly used across different domains. However, these systems usually operate
like black-box models that do not provide necessary explanations or
justifications for their decisions. This is a major disadvantage because users
cannot trust such artificial intelligence-based biometric systems and may not
feel comfortable using them when clear explanations or justifications are not
provided. This paper addresses this problem by applying an efficient method for
explainable face recognition systems. We use a Class Activation Mapping
(CAM)-based discriminative localization (very narrow/specific localization)
technique called Scaled Directed Divergence (SDD) to visually explain the
results of deep learning-based face recognition systems. We perform fine
localization of the face features relevant to the deep learning model for its
prediction/decision. Our experiments show that the SDD Class Activation Map
(CAM) highlights the relevant face features very specifically compared to the
traditional CAM and very accurately. The provided visual explanations with
narrow localization of relevant features can ensure much-needed transparency
and trust for deep learning-based face recognition systems.

</details>


### [159] [GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation](https://arxiv.org/abs/2505.03846)
*Kangsheng Wang,Yuhang Li,Chengwei Ye,Yufei Lin,Huanzhen Zhang,Bohan Hu,Linuo Xu,Shuyan Liu*

Main category: cs.CV

TL;DR: GAME方法通过图增强多模态编码器融合视觉、听觉和文本特征，利用GCN和CNN结合注意力机制提升面部特征提取，结合BiGRU和XLM-Roberta处理时序和语义信息，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 短视频中的人格分析因多模态特征的复杂交互而具有挑战性，需要一种能有效融合视觉、听觉和文本特征的方法。

Method: 提出GAME模型，包含面部图构建、双分支Geo Two-Stream Network（GCN+CNN）、BiGRU时序处理、XLM-Roberta语义提取，以及通道注意力融合模块和MLP回归头。

Result: GAME在多个基准测试中优于现有方法，验证了其有效性和泛化能力。

Conclusion: GAME通过多模态特征融合和注意力机制，显著提升了人格预测的准确性，为复杂场景下的分析提供了新思路。

Abstract: Apparent personality analysis from short videos poses significant chal-lenges
due to the complex interplay of visual, auditory, and textual cues. In this
paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to
robustly model and fuse multi-source features for automatic personality
prediction. For the visual stream, we construct a facial graph and introduce a
dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks
(GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to
capture both structural and appearance-based facial cues. Complementing this,
global context and iden-tity features are extracted using pretrained ResNet18
and VGGFace back-bones. To capture temporal dynamics, frame-level features are
processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio
representations are derived from the VGGish network, and linguistic se-mantics
are captured via the XLM-Roberta transformer. To achieve effective multimodal
integration, we propose a Channel Attention-based Fusion module, followed by a
Multi-Layer Perceptron (MLP) regression head for predicting personality traits.
Extensive experiments show that GAME con-sistently outperforms existing methods
across multiple benchmarks, vali-dating its effectiveness and generalizability.

</details>


### [160] [Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques](https://arxiv.org/abs/2505.03848)
*Janhavi Giri,Attila Lengyel,Don Kent,Edward Kibardin*

Main category: cs.CV

TL;DR: 论文摘要介绍了半导体制造业中图像数据处理的挑战，提出了一种结合深度拓扑数据分析（TDA）、自监督学习和迁移学习的先进聚类框架，实现了无标签图像数据的有效聚类。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中大量图像数据需手动检查，传统聚类方法难以处理高维无标签数据，因此需要一种新方法来捕捉细微模式，提高缺陷识别和良率优化效率。

Method: 利用深度TDA捕捉拓扑特征，结合自监督学习从无标签数据中提取有用表示，并通过迁移学习增强框架的适应性和可扩展性。

Result: 在合成和开源半导体图像数据集上验证成功，识别出与缺陷模式及工艺变化一致的聚类。

Conclusion: 研究展示了TDA、自监督学习与迁移学习的结合潜力，为半导体制造等领域的图像数据处理提供了可扩展的解决方案。

Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for
defect identification and yield optimization, yet often exceeds manual
inspection capabilities. Traditional clustering techniques struggle with
high-dimensional, unlabeled data, limiting their effectiveness in capturing
nuanced patterns. This paper introduces an advanced clustering framework that
integrates deep Topological Data Analysis (TDA) with self-supervised and
transfer learning techniques, offering a novel approach to unsupervised image
clustering. TDA captures intrinsic topological features, while self-supervised
learning extracts meaningful representations from unlabeled data, reducing
reliance on labeled datasets. Transfer learning enhances the framework's
adaptability and scalability, allowing fine-tuning to new datasets without
retraining from scratch. Validated on synthetic and open-source semiconductor
image datasets, the framework successfully identifies clusters aligned with
defect patterns and process variations. This study highlights the
transformative potential of combining TDA, self-supervised learning, and
transfer learning, providing a scalable solution for proactive process
monitoring and quality control in semiconductor manufacturing and other domains
with large-scale image datasets.

</details>


### [161] [An Active Inference Model of Covert and Overt Visual Attention](https://arxiv.org/abs/2505.03856)
*Tin Mišić,Karlo Koledić,Fabio Bonsignorio,Ivan Petrović,Ivan Marković*

Main category: cs.CV

TL;DR: 模型基于主动推理框架，通过动态优化感官精度以减少自由能，实现了隐蔽和显性视觉注意力的模拟，并在Posner提示任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为复杂感官输入的代理提供一种选择性注意力机制，以过滤干扰并优化信息处理。

Method: 使用主动推理框架动态优化感官精度，并通过Posner提示任务和简单目标聚焦任务测试模型行为。

Result: 外源性和有效提示通常反应更快，模型表现出类似于返回抑制的行为，且非自愿反射性眼动比有意眼动更快但适应性较差。

Conclusion: 模型成功模拟了隐蔽和显性注意力机制，为理解大脑如何处理复杂感官输入提供了新视角。

Abstract: The ability to selectively attend to relevant stimuli while filtering out
distractions is essential for agents that process complex, high-dimensional
sensory input. This paper introduces a model of covert and overt visual
attention through the framework of active inference, utilizing dynamic
optimization of sensory precisions to minimize free-energy. The model
determines visual sensory precisions based on both current environmental
beliefs and sensory input, influencing attentional allocation in both covert
and overt modalities. To test the effectiveness of the model, we analyze its
behavior in the Posner cueing task and a simple target focus task using
two-dimensional(2D) visual data. Reaction times are measured to investigate the
interplay between exogenous and endogenous attention, as well as valid and
invalid cueing. The results show that exogenous and valid cues generally lead
to faster reaction times compared to endogenous and invalid cues. Furthermore,
the model exhibits behavior similar to inhibition of return, where previously
attended locations become suppressed after a specific cue-target onset
asynchrony interval. Lastly, we investigate different aspects of overt
attention and show that involuntary, reflexive saccades occur faster than
intentional ones, but at the expense of adaptability.

</details>


### [162] [Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation](https://arxiv.org/abs/2505.03896)
*Shuang Zeng,Chee Hong Lee,Micky C Nnamdi,Wenqi Shi,J Ben Tamo,Lei Zhu,Hangzhou He,Xinliang Zhang,Qian Chen,May D. Wang,Yanye Lu,Qiushi Ren*

Main category: cs.CV

TL;DR: 本文提出了一种名为AttUKAN的新型注意力U型Kolmogorov-Arnold网络及标签引导的像素级对比损失，用于视网膜血管分割，通过增强特征提取的判别性，在多个数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在视网膜血管分割方面取得进展，但现有方法未能充分利用编码器的细粒度特征表示，且缺乏对特征级判别性的关注。

Method: 结合注意力门的Kolmogorov-Arnold网络（AttUKAN）和标签引导的像素级对比损失，以增强模型敏感性和特征判别性。

Result: 在DRIVE、STARE等数据集上，AttUKAN的F1分数和MIoU均超过11种现有方法，达到最高性能。

Conclusion: AttUKAN通过改进特征提取和损失设计，显著提升了视网膜血管分割的准确性和判别性，为相关疾病早期检测提供了更优方案。

Abstract: Retinal vessel segmentation is a vital early detection method for several
severe ocular diseases. Despite significant progress in retinal vessel
segmentation with the advancement of Neural Networks, there are still
challenges to overcome. Specifically, retinal vessel segmentation aims to
predict the class label for every pixel within a fundus image, with a primary
focus on intra-image discrimination, making it vital for models to extract more
discriminative features. Nevertheless, existing methods primarily focus on
minimizing the difference between the output from the decoder and the label,
but ignore fully using feature-level fine-grained representations from the
encoder. To address these issues, we propose a novel Attention U-shaped
Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided
Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we
implement Attention Gates into Kolmogorov-Arnold Networks to enhance model
sensitivity by suppressing irrelevant feature activations and model
interpretability by non-linear modeling of KAN blocks. Additionally, we also
design a novel Label-guided Pixel-wise Contrastive Loss to supervise our
proposed AttUKAN to extract more discriminative features by distinguishing
between foreground vessel-pixel pairs and background pairs. Experiments are
conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF
and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%,
80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and
66.94% in the above datasets, which are the highest compared to 11 networks for
retinal vessel segmentation. Quantitative and qualitative results show that our
AttUKAN achieves state-of-the-art performance and outperforms existing retinal
vessel segmentation methods. Our code will be available at
https://github.com/stevezs315/AttUKAN.

</details>


### [163] [Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces](https://arxiv.org/abs/2505.03974)
*Nikhil M. Pawar,Jorge A. Prozzi,Feng Hong,Surya Sarat Chandra Congress*

Main category: cs.CV

TL;DR: 为解决基础设施管理中的低分辨率图像问题，研究结合CNN和ESPCNN框架，高效分类并超分辨率处理图像，减少误报和计算成本。


<details>
  <summary>Details</summary>
Motivation: 基础设施管理中的无人机图像常因传感器限制和环境条件导致分辨率不足，现有超分辨率技术易产生误报且计算成本高。

Method: 提出结合CNN和轻量级ESPCNN的框架：CNN分类图像，ESPCNN对正类图像进行高效超分辨率处理。

Result: ESPCNN在超分辨率上优于双三次插值，框架有效减少误报和计算成本，并能捕捉细微裂缝特征。

Conclusion: 该框架可帮助高速公路机构高效进行病害检测和资产管理。

Abstract: Recently, there has been an impetus for the application of cutting-edge data
collection platforms such as drones mounted with camera sensors for
infrastructure asset management. However, the sensor characteristics, proximity
to the structure, hard-to-reach access, and environmental conditions often
limit the resolution of the datasets. A few studies used super-resolution
techniques to address the problem of low-resolution images. Nevertheless, these
techniques were observed to increase computational cost and false alarms of
distress detection due to the consideration of all the infrastructure images
i.e., positive and negative distress classes. In order to address the
pre-processing of false alarm and achieve efficient super-resolution, this
study developed a framework consisting of convolutional neural network (CNN)
and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately
classified both the classes. ESPCNN, which is the lightweight super-resolution
technique, generated high-resolution infrastructure image of positive distress
obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the
evaluation metrics for super-resolution. Based on the performance metrics, the
combination of CNN and ESPCNN was observed to be effective in preprocessing the
infrastructure images with negative distress, reducing the computational cost
and false alarms in the next step of super-resolution. The visual inspection
showed that EPSCNN is able to capture crack propagation, complex geometry of
even minor cracks. The proposed framework is expected to help the highway
agencies in accurately performing distress detection and assist in efficient
asset management practices.

</details>


### [164] [R^3-VQA: "Read the Room" by Video Social Reasoning](https://arxiv.org/abs/2505.04147)
*Lixing Niu,Jiapeng Li,Xingping Yu,Shu Wang,Ruining Feng,Bo Wu,Ping Wei,Yisen Wang,Lifeng Fan*

Main category: cs.CV

TL;DR: 该论文提出了一个复杂社交推理数据集R^3-VQA，包含丰富的社交事件和心理状态标注，并评估了现有大型视觉语言模型的社交推理能力。


<details>
  <summary>Details</summary>
Motivation: 弥补现有社交推理任务和数据集的不足，提升对复杂社交场景的理解能力。

Method: 构建R^3-VQA数据集，涵盖社交事件、心理状态和因果链，并设计三项评估任务。

Result: 实验显示当前LVLMs在复杂社交场景中的推理能力远不及人类，但ToM提示能提升其表现。

Conclusion: R^3-VQA为社交推理研究提供了新基准，ToM提示对模型性能有积极影响。

Abstract: "Read the room" is a significant social reasoning capability in human daily
life. Humans can infer others' mental states from subtle social cues. Previous
social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic
interactions, incomplete mental state variables, single-step reasoning, etc.)
and fall far short of the challenges present in real-life social interactions.
In this paper, we contribute a valuable, high-quality, and comprehensive video
dataset named R^3-VQA with precise and fine-grained annotations of social
events and mental states (i.e., belief, intent, desire, and emotion) as well as
corresponding social causal chains in complex social scenarios. Moreover, we
include human-annotated and model-generated QAs. Our task R^3-VQA includes
three aspects: Social Event Understanding, Mental State Estimation, and Social
Causal Reasoning. As a benchmark, we comprehensively evaluate the social
reasoning capabilities and consistencies of current state-of-the-art large
vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs
are still far from human-level consistent social reasoning in complex social
scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on
social reasoning tasks. We provide some of our dataset and codes in
supplementary material and will release our full dataset and codes upon
acceptance.

</details>


### [165] [DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.04175)
*Naphat Nithisopa,Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: 论文提出了一种结合ResNet和Vision Transformer的端到端框架，采用Deformable Convolutions、Retrieval-Augmented Generation和CRF等技术提升OCR性能。实验表明其在多个基准数据集上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 自然图像中的文本识别任务既具挑战性又应用广泛，现有方法在特征表示和序列建模上仍有提升空间。

Method: 框架在ResNet和Vision Transformer基础上，引入Deformable Convolutions替换部分卷积层，采用自适应dropout和CRF优化序列建模。

Result: 在六个数据集（IC13、IC15等）上平均准确率达77.77%，其中IC13表现最佳（97.32%），证明了方法的鲁棒性。

Conclusion: 该框架通过创新技术组合显著提升了文本识别性能，为复杂场景下的OCR任务提供了新思路。

Abstract: Text recognition in natural images remains a challenging yet essential task,
with broad applications spanning computer vision and natural language
processing. This paper introduces a novel end-to-end framework that combines
ResNet and Vision Transformer backbones with advanced methodologies, including
Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random
Fields (CRF). These innovations collectively enhance feature representation and
improve Optical Character Recognition (OCR) performance. Specifically, the
framework substitutes standard convolution layers in the third and fourth
blocks with Deformable Convolutions, leverages adaptive dropout for
regularization, and incorporates CRF for more refined sequence modeling.
Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT,
IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving
notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on
IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy
of 77.77%. These results establish a new state-of-the-art for text recognition,
demonstrating the robustness of the approach across diverse and challenging
datasets.

</details>


### [166] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/abs/2505.04185)
*Hail Song,Wonsik Shin,Naeun Lee,Soomin Chung,Nojun Kwak,Woontack Woo*

Main category: cs.CV

TL;DR: 提出S3D框架，通过U-Net架构和风格对齐损失，将2D草图高效转换为高质量3D模型，并公开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决因草图数据稀疏性和歧义性导致的2D转3D生成质量难题。

Method: 采用U-Net编码器-解码器结构生成面部分割掩码，结合风格对齐损失和草图数据增强提升一致性。

Result: S3D能生成高质量、多视角一致的3D模型。

Conclusion: S3D框架显著提升了草图到3D的生成效果，代码已开源。

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [167] [An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement](https://arxiv.org/abs/2505.04207)
*Mustafa Yurdakul,Şakir Tasdemir*

Main category: cs.CV

TL;DR: 论文提出了一种改进的YOLOv8模型，用于基于RGB-D图像的坑洼检测及其物理特征分析，显著提升了检测精度和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有的坑洼检测方法通常仅基于2D RGB图像，难以准确分析坑洼的物理特征（如深度和周长），影响安全和经济问题。

Method: 利用Intel RealSense D415深度摄像头采集RGB-D数据并构建PothRGBD数据集；改进YOLOv8n-seg架构，引入动态蛇形卷积（DSConv）、简单注意力模块（SimAM）和高斯误差线性单元（GELU）。

Result: 改进模型的精确度（93.7%）、召回率（90.4%）和mAP@50（93.8%）分别比基准模型提升了1.96%、6.13%和2.07%，并能高精度测量坑洼的周长和深度。

Conclusion: 该模型轻量高效，适用于实时智能交通系统，为基于深度学习的坑洼检测提供了可靠解决方案。

Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety
and economic problems. Therefore, early and accurate detection of potholes is
crucial. Existing detection methods are usually only based on 2D RGB images and
cannot accurately analyze the physical characteristics of potholes. In this
paper, a publicly available dataset of RGB-D images (PothRGBD) is created and
an improved YOLOv8-based model is proposed for both pothole detection and
pothole physical features analysis. The Intel RealSense D415 depth camera was
used to collect RGB and depth data from the road surfaces, resulting in a
PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable
for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg
architecture, which is structurally improved with Dynamic Snake Convolution
(DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit
(GELU). The proposed model segmented potholes with irregular edge structure
more accurately, and performed perimeter and depth measurements on depth maps
with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision,
85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to
93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in
precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model
performs pothole detection as well as perimeter and depth measurement with high
accuracy and is suitable for real-time applications due to its low model
complexity. In this way, a lightweight and effective model that can be used in
deep learning-based intelligent transportation solutions has been acquired.

</details>


### [168] [Object-Shot Enhanced Grounding Network for Egocentric Video](https://arxiv.org/abs/2505.04270)
*Yisen Feng,Haoyu Zhang,Meng Liu,Weili Guan,Liqiang Nie*

Main category: cs.CV

TL;DR: 论文提出OSGNet，一种用于自我中心视频的增强型目标镜头网络，通过提取对象信息和镜头运动特征来改进视频表示和模态对齐，实验表明其性能达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法忽略自我中心视频关键特征及查询问题细粒度信息的问题。

Method: 提出OSGNet，利用对象信息和镜头运动特征增强视频表示，并提取佩戴者的注意力信息以改进模态对齐。

Result: 在三个数据集上实现最先进性能。

Conclusion: OSGNet通过对象和镜头特征的增强，有效提升了自我中心视频定位任务的性能。

Abstract: Egocentric video grounding is a crucial task for embodied intelligence
applications, distinct from exocentric video moment localization. Existing
methods primarily focus on the distributional differences between egocentric
and exocentric videos but often neglect key characteristics of egocentric
videos and the fine-grained information emphasized by question-type queries. To
address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding
Network for egocentric video. Specifically, we extract object information from
videos to enrich video representation, particularly for objects highlighted in
the textual query but not directly captured in the video features.
Additionally, we analyze the frequent shot movements inherent to egocentric
videos, leveraging these features to extract the wearer's attention
information, which enhances the model's ability to perform modality alignment.
Experiments conducted on three datasets demonstrate that OSGNet achieves
state-of-the-art performance, validating the effectiveness of our approach. Our
code can be found at https://github.com/Yisen-Feng/OSGNet.

</details>


### [169] [Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise](https://arxiv.org/abs/2505.04375)
*Moseli Mots'oehli,Hope Mogale,Kyungim Baek*

Main category: cs.CV

TL;DR: 研究发现，更大的ViT模型（如ViTl32）在标签噪声下表现更优，而Swin Transformer的鲁棒性较弱。小尺寸补丁不一定更好，信息驱动的主动学习策略仅在中等噪声率下有效。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉变换器在低预算和标签噪声下的实用性，为资源受限环境下的模型部署提供指导。

Method: 评估四种ViT和三种Swin Transformer配置在CIFAR10/100数据集上的表现，分析对称标签噪声对分类准确性和校准的影响。

Result: ViTl32在准确性和校准上表现最佳，Swin Transformer在所有噪声水平下鲁棒性较差，主动学习策略仅在中噪声率下有效。

Conclusion: 为资源受限场景下的视觉变换器部署提供了平衡模型复杂度、标签噪声和计算效率的实用建议。

Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for
downstream tasks is well-established. Still, the impact of model size on the
performance of vision transformers in similar scenarios, particularly under
label noise, remains largely unexplored. Given the utility and versatility of
transformer architectures, this study investigates their practicality under
low-budget constraints and noisy labels. We explore how classification accuracy
and calibration are affected by symmetric label noise in active learning
settings, evaluating four vision transformer configurations (Base and Large
with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations
(Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label
noise rates. Our findings show that larger ViT models (ViTl32 in particular)
consistently outperform their smaller counterparts in both accuracy and
calibration, even under moderate to high label noise, while Swin Transformers
exhibit weaker robustness across all noise levels. We find that smaller patch
sizes do not always lead to better performance, as ViTl16 performs consistently
worse than ViTl32 while incurring a higher computational cost. We also find
that information-based Active Learning strategies only provide meaningful
accuracy improvements at moderate label noise rates, but they result in poorer
calibration compared to models trained on randomly acquired labels, especially
at high label noise rates. We hope these insights provide actionable guidance
for practitioners looking to deploy vision transformers in resource-constrained
environments, where balancing model complexity, label noise, and compute
efficiency is critical in model fine-tuning or distillation.

</details>


### [170] [Deep residual learning with product units](https://arxiv.org/abs/2505.04397)
*Ziyuan Li,Uwe Jaekel,Babette Dellen*

Main category: cs.CV

TL;DR: PURe神经网络通过将乘积单元集成到残差块中，提升了卷积网络的表达能力和参数效率，在多项基准测试中表现优于传统ResNet，同时收敛更快且对噪声更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统卷积网络通过求和神经元进行特征交互，而乘积单元能实现乘性交互，可能更有效地表达复杂模式。PURe旨在通过乘积单元提升网络性能。

Method: PURe在每个残差块的第二层用2D乘积单元替代传统卷积层，并移除非线性激活函数以保留结构信息。

Result: 在Galaxy10 DECaLS、ImageNet和CIFAR-10上，PURe均超越同深度或更深的ResNet，准确率更高、参数更少且收敛更快。例如，PURe34在ImageNet上达到80.27% top-1准确率。

Conclusion: PURe在准确性、效率和鲁棒性之间取得了平衡，展现了乘积单元架构在计算机视觉中的潜力。

Abstract: We propose a deep product-unit residual neural network (PURe) that integrates
product units into residual blocks to improve the expressiveness and parameter
efficiency of deep convolutional networks. Unlike standard summation neurons,
product units enable multiplicative feature interactions, potentially offering
a more powerful representation of complex patterns. PURe replaces conventional
convolutional layers with 2D product units in the second layer of each residual
block, eliminating nonlinear activation functions to preserve structural
information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS,
PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper
ResNet152, while converging nearly five times faster and demonstrating strong
robustness to Poisson noise. On ImageNet, PURe architectures outperform
standard ResNet models at similar depths, with PURe34 achieving a top-1
accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet
variants (ResNet50, ResNet101) while utilizing significantly fewer parameters
and computational resources. On CIFAR-10, PURe consistently outperforms ResNet
variants across varying depths, with PURe272 reaching 95.01% test accuracy,
comparable to ResNet1001 but at less than half the model size. These results
demonstrate that PURe achieves a favorable balance between accuracy,
efficiency, and robustness. Compared to traditional residual networks, PURe not
only achieves competitive classification performance with faster convergence
and fewer parameters, but also demonstrates greater robustness to noise. Its
effectiveness across diverse datasets highlights the potential of
product-unit-based architectures for scalable and reliable deep learning in
computer vision.

</details>


### [171] [Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages](https://arxiv.org/abs/2505.04150)
*Yu Yamaoka or Weng Ian Chan,Shigeto Seno,Soichiro Fukada,Hideo Matsuda*

Main category: cs.CV

TL;DR: 论文提出了一种名为OSLSP的弱监督学习方法，用于自动化评估肌肉组织再生过程，解决了现有LLP方法的两个局限性，并在分类任务中优于预训练模型。


<details>
  <summary>Details</summary>
Motivation: 传统肌肉组织再生评估依赖专家视觉检查，存在主观性和定量化不足的问题。自动化方法的引入可以提升分析的客观性和效率，但现有弱监督学习方法（LLP）无法适应肌肉组织特征提取，且忽略了恢复阶段的顺序信息。

Method: 提出Ordinal Scale Learning from Similarity Proportion (OSLSP)，通过相似性比例损失和类别比例注意力机制，保留恢复阶段的顺序信息，并优化特征提取器。

Result: OSLSP模型在骨骼肌恢复阶段分类任务中表现优于大规模预训练和微调模型。

Conclusion: OSLSP为肌肉再生研究提供了一种定量化、自动化的解决方案，解决了现有方法的局限性，并在实验中验证了其有效性。

Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental
analysis in muscle research to measure experimental effect sizes and uncover
mechanisms behind muscle weakness due to aging and disease. The conventional
approach to assessing muscle tissue regeneration involves whole-slide imaging
and expert visual inspection of the recovery stages based on the morphological
information of cells and fibers. There is a need to replace these tasks with
automated methods incorporating machine learning techniques to ensure a
quantitative and objective analysis. Given the limited availability of fully
labeled data, a possible approach is Learning from Label Proportions (LLP), a
weakly supervised learning method using class label proportions. However,
current LLP methods have two limitations: (1) they cannot adapt the feature
extractor for muscle tissues, and (2) they treat the classes representing
recovery stages and cell morphological changes as nominal, resulting in the
loss of ordinal information. To address these issues, we propose Ordinal Scale
Learning from Similarity Proportion (OSLSP), which uses a similarity proportion
loss derived from two bag combinations. OSLSP can update the feature extractor
by using class proportion attention to the ordinal scale of the class. Our
model with OSLSP outperforms large-scale pre-trained and fine-tuning models in
classification tasks of skeletal muscle recovery stages.

</details>


### [172] [Efficient Flow Matching using Latent Variables](https://arxiv.org/abs/2505.04486)
*Anirban Samaddar,Yixuan Sun,Viktor Nilsson,Sandeep Madireddy*

Main category: cs.CV

TL;DR: Latent-CFM是一种改进的流匹配模型，通过利用预训练的深度潜在变量模型简化训练，结合多模态数据结构，显著提升生成质量并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配模型在从简单先验分布到数据的学习过程中通常未明确建模数据的底层结构或流形，导致学习效率低下，尤其是在处理高维真实数据时表现不佳。因此，本文提出Latent-CFM，旨在通过预训练模型简化训练策略，有效利用多模态数据结构。

Method: 提出了Latent-CFM，利用预训练的深度潜在变量模型简化训练和推理策略，从而更高效地结合多模态数据结构。

Result: 实验表明，Latent-CFM在生成质量上优于现有流匹配模型，训练和计算成本显著降低（某些情况下减少50%），并且在多模态合成数据和实际图像数据集上表现优异，同时还能够生成物理上更准确的样本。

Conclusion: Latent-CFM通过结合潜在变量模型和多模态数据结构，在生成质量、效率和应用广度上均表现出显著优势，为复杂数据的流匹配生成提供了更高效的解决方案。

Abstract: Flow matching models have shown great potential in image generation tasks
among probabilistic generative models. Building upon the ideas of continuous
normalizing flows, flow matching models generalize the transport path of the
diffusion models from a simple prior distribution to the data. Most flow
matching models in the literature do not explicitly model the underlying
structure/manifold in the target data when learning the flow from a simple
source distribution like the standard Gaussian. This leads to inefficient
learning, especially for many high-dimensional real-world datasets, which often
reside in a low-dimensional manifold. Existing strategies of incorporating
manifolds, including data with underlying multi-modal distribution, often
require expensive training and hence frequently lead to suboptimal performance.
To this end, we present \texttt{Latent-CFM}, which provides simplified
training/inference strategies to incorporate multi-modal data structures using
pretrained deep latent variable models. Through experiments on multi-modal
synthetic data and widely used image benchmark datasets, we show that
\texttt{Latent-CFM} exhibits improved generation quality with significantly
less training ($\sim 50\%$ less in some cases) and computation than
state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we
demonstrate that our approach generates more physically accurate samples than
competitive approaches. In addition, through latent space analysis, we
demonstrate that our approach can be used for conditional image generation
conditioned on latent features.

</details>


### [173] ["I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments](https://arxiv.org/abs/2505.04488)
*Ziyi Zhang,Zhen Sun,Zongmin Zhang,Zifan Peng,Yuemeng Zhao,Zichun Wang,Zeren Luo,Ruiting Zuo,Xinlei He*

Main category: cs.CV

TL;DR: 论文探讨了视觉辅助AI对重度视障人士的帮助，构建了VisAssistDaily数据集评估GPT-4o等模型在动态环境中的有效性，并提出了SafeVid数据集和轮询机制以提升环境风险感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦静态内容，无法满足视障人士在动态复杂环境中的实时感知需求，需结合先进视觉理解技术提升辅助效果。

Method: 1. 构建VisAssistDaily基准数据集（含三类辅助任务）；2. 评估GPT-4o等模型的封闭/开放场景表现；3. 提出SafeVid数据集和轮询机制优化风险感知。

Result: GPT-4o任务成功率最高；用户研究揭示了模型在动态环境中感知潜在危险的不足，轮询机制有效提升了风险检测能力。

Conclusion: 研究为动态环境下的视障辅助技术提供了实践洞见，强调需进一步优化实时风险感知能力。

Abstract: The visually impaired population, especially the severely visually impaired,
is currently large in scale, and daily activities pose significant challenges
for them. Although many studies use large language and vision-language models
to assist the blind, most focus on static content and fail to meet real-time
perception needs in dynamic and complex environments, such as daily activities.
To provide them with more effective intelligent assistance, it is imperative to
incorporate advanced visual understanding technologies. Although real-time
vision and speech interaction VideoLLMs demonstrate strong real-time visual
understanding, no prior work has systematically evaluated their effectiveness
in assisting visually impaired individuals. In this work, we conduct the first
such evaluation. First, we construct a benchmark dataset (VisAssistDaily),
covering three categories of assistive tasks for visually impaired individuals:
Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that
GPT-4o achieves the highest task success rate. Next, we conduct a user study to
evaluate the models in both closed-world and open-world scenarios, further
exploring the practical challenges of applying VideoLLMs in assistive contexts.
One key issue we identify is the difficulty current models face in perceiving
potential hazards in dynamic environments. To address this, we build an
environment-awareness dataset named SafeVid and introduce a polling mechanism
that enables the model to proactively detect environmental risks. We hope this
work provides valuable insights and inspiration for future research in this
field.

</details>


### [174] [Defining and Quantifying Creative Behavior in Popular Image Generators](https://arxiv.org/abs/2505.04497)
*Aditi Ramaswamy*

Main category: cs.CV

TL;DR: 本文通过定量方法评估生成AI模型的创造力，并提出衡量指标帮助用户选择合适的模型，实验结果表明这些指标符合人类直觉。


<details>
  <summary>Details</summary>
Motivation: 探讨生成AI模型的创造力，并为用户提供实用指标以选择适合任务的模型。

Method: 提出定量衡量创造力指标，并在多个流行的图像生成模型上进行评估。

Result: 实验结果显示这些定量指标与人类直觉一致。

Conclusion: 本文的衡量指标能有效帮助用户评估生成AI模型的创造力。

Abstract: Creativity of generative AI models has been a subject of scientific debate in
the last years, without a conclusive answer. In this paper, we study creativity
from a practical perspective and introduce quantitative measures that help the
user to choose a suitable AI model for a given task. We evaluated our measures
on a number of popular image-to-image generation models, and the results of
this suggest that our measures conform to human intuition.

</details>


### [175] [DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once](https://arxiv.org/abs/2505.04526)
*Qi Zhou,Yukai Shi,Xiaojun Yang,Xiaoyu Xian,Lunjia Liao,Ruimao Zhang,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出了一种名为DFVO的网络，用于在低光照条件下进行可见光和红外图像融合，通过多任务级联方法解决传统两阶段方法的信息损失问题，生成更清晰、信息更丰富的融合图像。


<details>
  <summary>Details</summary>
Motivation: 现有图像融合方法在可见光图像严重退化时，融合结果模糊且昏暗，对自动驾驶等高层次视觉任务构成挑战。

Method: 设计了Darkness-Free网络（DFVO），通过级联多任务策略（细节提取模块、超交叉注意力模块）和联合损失函数，实现图像解耦与融合的一体化。

Result: 在LLVIP数据集上达到63.258 dB PSNR和0.724 CC，生成更清晰、光照均匀的融合结果。

Conclusion: DFVO在低光环境下显著优于现有方法，为高层次视觉任务提供更有效信息。

Abstract: Visible and infrared image fusion is one of the most crucial tasks in the
field of image fusion, aiming to generate fused images with clear structural
information and high-quality texture features for high-level vision tasks.
However, when faced with severe illumination degradation in visible images, the
fusion results of existing image fusion methods often exhibit blurry and dim
visual effects, posing major challenges for autonomous driving. To this end, a
Darkness-Free network is proposed to handle Visible and infrared image
disentanglement and fusion all at Once (DFVO), which employs a cascaded
multi-task approach to replace the traditional two-stage cascaded training
(enhancement and fusion), addressing the issue of information entropy loss
caused by hierarchical data transmission. Specifically, we construct a
latent-common feature extractor (LCFE) to obtain latent features for the
cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised
to acquire high-frequency semantic information. Secondly, we design a hyper
cross-attention module (HCAM) to extract low-frequency information and preserve
texture features from source images. Finally, a relevant loss function is
designed to guide the holistic network learning, thereby achieving better image
fusion. Extensive experiments demonstrate that our proposed approach
outperforms state-of-the-art alternatives in terms of qualitative and
quantitative evaluations. Particularly, DFVO can generate clearer, more
informative, and more evenly illuminated fusion results in the dark
environments, achieving best performance on the LLVIP dataset with 63.258 dB
PSNR and 0.724 CC, providing more effective information for high-level vision
tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.

</details>


### [176] [Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration](https://arxiv.org/abs/2505.04524)
*Asma Baobaid,Mahmoud Meribout*

Main category: cs.CV

TL;DR: 本文提出了一种结合硬件和软件的方法，优化了基于NVIDIA Jetson AGX Orin边缘GPU的人脸检测与识别系统，通过同时利用所有硬件引擎和集成人脸跟踪模块，实现了290 FPS的高吞吐量，并节省了约800 mW的功耗。


<details>
  <summary>Details</summary>
Motivation: 尽管现有AI硬件加速器在高性能人脸检测与识别方面表现优异，但在吞吐量和功耗方面仍有改进空间。本文旨在通过优化边缘GPU的硬件-软件协同设计，提升公共场合视频监控系统的性能。

Method: 1. 同时利用NVIDIA Jetson AGX Orin的所有硬件引擎（而非仅CPU或GPU核心）以提升处理速度；2. 集成人脸跟踪模块，避免对每一帧重复运行识别算法，仅在新人脸出现时触发识别。

Result: 在1920x1080分辨率（平均每帧6张人脸）下，系统实现了290 FPS的高吞吐量，相比仅使用CPU/GPU引擎且未集成跟踪器的方案，功耗降低了约800 mW。

Conclusion: 该硬件-软件协同设计方法为边缘高性能机器视觉系统（如多摄像头公共监控场景）提供了有效的解决方案，显著提升了吞吐量和能效。

Abstract: Cost-effective machine vision systems dedicated to real-time and accurate
face detection and recognition in public places are crucial for many modern
applications. However, despite their high performance, which could be reached
using specialized edge or cloud AI hardware accelerators, there is still room
for improvement in throughput and power consumption. This paper aims to suggest
a combined hardware-software approach that optimizes face detection and
recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX
Orin. First, it leverages the simultaneous usage of all its hardware engines to
improve processing time. This offers an improvement over previous works where
these tasks were mainly allocated automatically and exclusively to the CPU or,
to a higher extent, to the GPU core. Additionally, the paper suggests
integrating a face tracker module to avoid redundantly running the face
recognition algorithm for every frame but only when a new face appears in the
scene. The results of extended experiments suggest that simultaneous usage of
all the hardware engines that are available in the Orin GPU and tracker
integration into the pipeline yield an impressive throughput of 290 FPS (frames
per second) on 1920 x 1080 input size frames containing in average of 6
faces/frame. Additionally, a substantial saving of power consumption of around
800 mW was achieved when compared to running the task on the CPU/GPU engines
only and without integrating a tracker into the Orin GPU\'92s pipeline. This
hardware-codesign approach can pave the way to design high-performance machine
vision systems at the edge, critically needed in video monitoring in public
places where several nearby cameras are usually deployed for a same scene.

</details>


### [177] [Componential Prompt-Knowledge Alignment for Domain Incremental Learning](https://arxiv.org/abs/2505.04575)
*Kunlun Xu,Xu Zou,Gang Hua,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 论文提出了KA-Prompt方法，通过组件感知的提示-知识对齐解决多域知识融合中的冲突问题，显著提升了模型的学习和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在多域学习中存在知识组件不对齐问题，导致知识融合冲突和性能下降。为解决这一问题，研究提出了组件感知的对齐方法。

Method: KA-Prompt分为两阶段：1) 初始组件结构配置，挖掘与新区相关的旧提示以初始化新提示；2) 在线对齐保持，动态识别目标旧提示并应用一致性约束。

Result: 在DIL基准测试上的广泛实验验证了KA-Prompt的有效性。

Conclusion: KA-Prompt通过组件对齐显著提升了多域知识融合的性能，为领域增量学习提供了新的解决方案。

Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data
streams across domains while retaining and utilizing past knowledge. Although
prompt-based methods effectively store multi-domain knowledge in prompt
parameters and obtain advanced performance through cross-domain prompt fusion,
we reveal an intrinsic limitation: component-wise misalignment between
domain-specific prompts leads to conflicting knowledge integration and degraded
predictions. This arises from the random positioning of knowledge components
within prompts, where irrelevant component fusion introduces interference.To
address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a
novel prompt-based DIL method that introduces component-aware prompt-knowledge
alignment during training, significantly improving both the learning and
inference capacity of the model. KA-Prompt operates in two phases: (1) Initial
Componential Structure Configuring, where a set of old prompts containing
knowledge relevant to the new domain are mined via greedy search, which is then
exploited to initialize new prompts to achieve reusable knowledge transfer and
establish intrinsic alignment between new and old prompts. (2) Online Alignment
Preservation, which dynamically identifies the target old prompts and applies
adaptive componential consistency constraints as new prompts evolve. Extensive
experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.
Our source code is available at
https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

</details>


### [178] [Active Sampling for MRI-based Sequential Decision Making](https://arxiv.org/abs/2505.04586)
*Yuning Du,Jingshuai Liu,Rohan Dharmakumar,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 该论文提出了一种多目标强化学习框架，通过改进采样策略，减少MRI的磁场强度需求，使其更适合作为即时诊断设备。


<details>
  <summary>Details</summary>
Motivation: MRI的高成本和复杂性限制了其作为即时诊断设备的使用，通过减少磁场强度和优化采样策略，可以推动MRI向即时诊断方向发展。

Method: 采用多目标强化学习框架，动态调整采样策略，并结合逐步加权奖励函数优化诊断目标所需的样本。

Result: 在膝关节病理评估任务中，该方法在保持竞争性诊断性能的同时显著减少了k空间样本的需求。

Conclusion: 该研究为MRI成为全面且经济的即时诊断设备铺平了道路。

Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging
(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and
complexity. To enable such a future by reducing the magnetic field strength,
one key approach will be to improve sampling strategies. Previous work has
shown that it is possible to make diagnostic decisions directly from k-space
with fewer samples. Such work shows that single diagnostic decisions can be
made, but if we aspire to see MRI as a true PoC, multiple and sequential
decisions are necessary while minimizing the number of samples acquired. We
present a novel multi-objective reinforcement learning framework enabling
comprehensive, sequential, diagnostic evaluation from undersampled k-space
data. Our approach during inference actively adapts to sequential decisions to
optimally sample. To achieve this, we introduce a training methodology that
identifies the samples that contribute the best to each diagnostic objective
using a step-wise weighting reward function. We evaluate our approach in two
sequential knee pathology assessment tasks: ACL sprain detection and cartilage
thickness loss assessment. Our framework achieves diagnostic performance
competitive with various policy-based benchmarks on disease detection, severity
quantification, and overall sequential diagnosis, while substantially saving
k-space samples. Our approach paves the way for the future of MRI as a
comprehensive and affordable PoC device. Our code is publicly available at
https://github.com/vios-s/MRI_Sequential_Active_Sampling

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [179] [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
*Kai Ruan,Mowen Huang,Ji-Rong Wen,Hao Sun*

Main category: cs.MA

TL;DR: SwarmBench是一个新基准，用于系统评估大型语言模型在去中心化多代理系统中的群体智能能力，发现其在局部信息约束下存在性能差异和局限性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在严格约束下（如局部感知和通信）的群体智能潜力，并填补现有基准在去中心化协调挑战上的不足。

Method: 引入SwarmBench基准，包含五种基础协调任务和可配置的2D网格环境，分析不同模型的零样本表现和群体动态。

Result: 发现不同任务间性能差异显著，局部信息约束导致规划与策略形成的局限性。

Conclusion: SwarmBench为研究LLM在去中心化系统中的潜力提供了开源工具，并揭示了其在群体智能场景中的局限。

Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their
capacity for emergent coordination in Multi-Agent Systems (MAS) when operating
under strict constraints-such as limited local perception and communication,
characteristic of natural swarms-remains largely unexplored, particularly
concerning the nuances of swarm intelligence. Existing benchmarks often do not
fully capture the unique challenges of decentralized coordination that arise
when agents operate with incomplete spatio-temporal information. To bridge this
gap, we introduce SwarmBench, a novel benchmark designed to systematically
evaluate the swarm intelligence capabilities of LLMs acting as decentralized
agents. SwarmBench features five foundational MAS coordination tasks within a
configurable 2D grid environment, forcing agents to rely primarily on local
sensory input (k x k view) and local communication. We propose metrics for
coordination effectiveness and analyze emergent group dynamics. Evaluating
several leading LLMs in a zero-shot setting, we find significant performance
variations across tasks, highlighting the difficulties posed by local
information constraints. While some coordination emerges, results indicate
limitations in robust planning and strategy formation under uncertainty in
these decentralized scenarios. Assessing LLMs under swarm-like conditions is
crucial for realizing their potential in future decentralized systems. We
release SwarmBench as an open, extensible toolkit-built upon a customizable and
scalable physical system with defined mechanical properties. It provides
environments, prompts, evaluation scripts, and the comprehensive experimental
datasets generated, aiming to foster reproducible research into LLM-based MAS
coordination and the theoretical underpinnings of Embodied MAS. Our code
repository is available at https://github.com/x66ccff/swarmbench.

</details>


### [180] [From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems](https://arxiv.org/abs/2505.03864)
*Qiaomu Li,Ying Xie*

Main category: cs.MA

TL;DR: A2A和MCP协议结合为多智能体系统提供了架构基础，但在语义互操作性、安全性和治理等方面带来了新的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中A2A和MCP协议结合时的语义互操作性、安全风险和治理问题，以实现更高效的协作和工具访问。

Method: 通过批判性分析，评估A2A和MCP集成的实际影响和难点，包括安全性、隐私和调试等问题。

Result: A2A和MCP的集成为系统提供了专业化、可扩展性等优势，但同时也带来了新的安全漏洞和复杂性。

Conclusion: 虽然A2A+MCP是重要的架构基础，但需进一步优化以应对其集成带来的复杂性。

Abstract: Artificial intelligence is rapidly evolving towards multi-agent systems where
numerous AI agents collaborate and interact with external tools. Two key open
standards, Google's Agent to Agent (A2A) protocol for inter-agent communication
and Anthropic's Model Context Protocol (MCP) for standardized tool access,
promise to overcome the limitations of fragmented, custom integration
approaches. While their potential synergy is significant, this paper argues
that effectively integrating A2A and MCP presents unique, emergent challenges
at their intersection, particularly concerning semantic interoperability
between agent tasks and tool capabilities, the compounded security risks
arising from combined discovery and execution, and the practical governance
required for the envisioned "Agent Economy". This work provides a critical
analysis, moving beyond a survey to evaluate the practical implications and
inherent difficulties of combining these horizontal and vertical integration
standards. We examine the benefits (e.g., specialization, scalability) while
critically assessing their dependencies and trade-offs in an integrated
context. We identify key challenges increased by the integration, including
novel security vulnerabilities, privacy complexities, debugging difficulties
across protocols, and the need for robust semantic negotiation mechanisms. In
summary, A2A+MCP offers a vital architectural foundation, but fully realizing
its potential requires substantial advancements to manage the complexities of
their combined operation.

</details>


### [181] [Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic](https://arxiv.org/abs/2505.04379)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.MA

TL;DR: 论文研究了自动驾驶车辆（AV）与人类驾驶车辆（HDV）在交通系统中的共识问题，通过高分辨率轨迹数据分析发现，AV与弱势道路使用者（VRU）在安全性、交互质量和交通性能维度上的完全共识极为罕见，仅占1.63%，强调了设计多维度平衡的AV模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 交通系统的复杂性和异构性使得自动驾驶车辆的部署面临共识挑战，需在安全性、交互质量和交通性能之间达成平衡。

Method: 利用第三代仿真（TGSIM）数据集的高分辨率轨迹数据，实证分析了AV和HDV在信号交叉口和弱势道路使用者周围的碰撞时间（TTC）、侵入后时间（PET）、减速模式、车距和队列稳定性等关键指标。

Result: 结果显示，AV与VRU在所有三个性能维度上的完全共识极为罕见，仅1.63%的交互帧满足全部条件。

Conclusion: 研究强调了在混合交通环境中设计明确平衡多维度性能的AV模型的重要性，并提供了开源代码支持复现性。

Abstract: Transportation systems have long been shaped by complexity and heterogeneity,
driven by the interdependency of agent actions and traffic outcomes. The
deployment of automated vehicles (AVs) in such systems introduces a new
challenge: achieving consensus across safety, interaction quality, and traffic
performance. In this work, we position consensus as a fundamental property of
the traffic system and aim to quantify it. We use high-resolution trajectory
data from the Third Generation Simulation (TGSIM) dataset to empirically
analyze AV and human-driven vehicle (HDV) behavior at a signalized urban
intersection and around vulnerable road users (VRUs). Key metrics, including
Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,
headways, and string stability, are evaluated across the three performance
dimensions. Results show that full consensus across safety, interaction, and
performance is rare, with only 1.63% of AV-VRU interaction frames meeting all
three conditions. These findings highlight the need for AV models that
explicitly balance multi-dimensional performance in mixed-traffic environments.
Full reproducibility is supported via our open-source codebase on
https://github.com/wissamkontar/Consensus-AV-Analysis.

</details>


### [182] [Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions](https://arxiv.org/abs/2505.04579)
*Stéphane Aroca-Ouellette,Miguel Aroca-Ouellette,Katharina von der Wense,Alessandro Roncone*

Main category: cs.MA

TL;DR: HA$^2$框架通过分层强化学习模仿人类协作方式，显著提升了与未见代理或人类队友的零-shot协作能力，并在Overcooked环境中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 自主智能体在与新队友协作时适应能力不足，缺乏类似人类的任务抽象机制，限制了零-shot协作能力。

Method: 引入HA$^2$框架，利用分层强化学习模拟人类的分层协作策略。

Result: 在Overcooked环境中实验显示，HA$^2$显著优于现有基线，对未见代理或人类队友均表现更优，且对环境变化更具韧性。

Conclusion: HA$^2$通过模拟人类任务抽象机制，有效提升了智能体的零-shot协作能力，为多智能体协作提供了新思路。

Abstract: In collaborative tasks, autonomous agents fall short of humans in their
capability to quickly adapt to new and unfamiliar teammates. We posit that a
limiting factor for zero-shot coordination is the lack of shared task
abstractions, a mechanism humans rely on to implicitly align with teammates. To
address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework
leveraging hierarchical reinforcement learning to mimic the structured approach
humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,
demonstrating statistically significant improvement over existing baselines
when paired with both unseen agents and humans, providing better resilience to
environmental shifts, and outperforming all state-of-the-art methods.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [183] [The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea](https://arxiv.org/abs/2505.03835)
*Simon Suh,Jihyuk Bang,Ji Woo Han*

Main category: cs.DL

TL;DR: 该研究探讨了COVID-19大流行和ChatGPT发布两大事件对2015至2024年间美国、欧洲和韩国AI政策研究中预印本引用区域趋势的影响，发现不同地区的预印本采用增长模式和速度受本地研究文化和开放科学成熟度的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解全球性事件如何影响AI政策研究中预印本的采用，以及这种影响在不同地区的差异性。

Method: 通过Web of Science的文献计量数据，标记关键事件时间点，分析预印本引用的区域趋势变化。

Result: 结果显示美国呈现事件驱动的快速增长，欧洲体现为制度化增长，而韩国则保持线性稳定增长，表明本地因素对预印本采纳影响显著。

Conclusion: 研究强调未来AI治理策略需考虑区域差异性，并建议进一步开展纵向和比较研究以深化对开放科学采纳的理解。

Abstract: The adoption of open science has quickly changed how artificial intelligence
(AI) policy research is distributed globally. This study examines the regional
trends in the citation of preprints, specifically focusing on the impact of two
major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on
research dissemination patterns in the United States, Europe, and South Korea
from 2015 to 2024. Using bibliometrics data from the Web of Science, this study
tracks how global disruptive events influenced the adoption of preprints in AI
policy research and how such shifts vary by region. By marking the timing of
these disruptive events, the analysis reveals that while all regions
experienced growth in preprint citations, the magnitude and trajectory of
change varied significantly. The United States exhibited sharp, event-driven
increases; Europe demonstrated institutional growth; and South Korea maintained
consistent, linear growth in preprint adoption. These findings suggest that
global disruptions may have accelerated preprint adoption, but the extent and
trajectory are shaped by local research cultures, policy environments, and
levels of open science maturity. This paper emphasizes the need for future AI
governance strategies to consider regional variability in research
dissemination and highlights opportunities for further longitudinal and
comparative research to deepen our understanding of open-access adoption in AI
policy development.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [184] [High-speed multiwavelength photonic temporal integration using silicon photonics](https://arxiv.org/abs/2505.04405)
*Yi Zhang,Nikolaos Farmakidis,Ioannis Roumpos,Miltiadis Moralis-Pegios,Apostolos Tsakyridis,June Sang Lee,Bowei Dong,Yuhan He,Samarth Aggarwal,Nikolaos Pleros,Harish Bhaskaran*

Main category: physics.optics

TL;DR: 该论文提出了一种名为PHIL的光学时间积分单元，利用热耗散过程在50 GHz高速调制下整合光信号，克服了光学硬件在AI大向量映射中的扩展难题，实现了高效的光信号端到端处理。


<details>
  <summary>Details</summary>
Motivation: 当前光学加速器在AI任务中映射大向量尺寸时面临扩展性不足的问题，且普遍存在电光转换效率低下的挑战，本文旨在通过热驱动积分技术解决这些问题。

Method: 通过时间展开标量运算并引入PHIL单元，利用热耗散慢过程整合高速调制的光信号，实现全光学时间积分，支持线性和非线性操作。

Result: 实验证明该架构能有效整合50 GHz调制的光信号，消除电光转换的低效问题，为高速光子计算提供了可扩展的解决方案。

Conclusion: PHIL技术通过热驱动积分成功克服了光学硬件扩展速度限制，为高速、高效的光子计算开辟了新路径。

Abstract: Optical systems have been pivotal for energy-efficient computing, performing
high-speed, parallel operations in low-loss carriers. While these predominantly
analog optical accelerators bypass digitization to perform parallel
floating-point computations, scaling optical hardware to map large-vector sizes
for AI tasks remains challenging. Here, we overcome this limitation by
unfolding scalar operations in time and introducing a
photonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration.
Counterintuitively, we exploit a slow heat dissipation process to integrate
optical signals modulated at 50 GHz bridging the speed gap between the widely
applied thermo-optic effects and ultrafast photonics. This architecture
supports optical end-to-end signal processing, eliminates inefficient
electro-optical conversions, and enables both linear and nonlinear operations
within a unified framework. Our results demonstrate a scalable path towards
high-speed photonic computing through thermally driven integration.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [185] [AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03745)
*Yanbiao Liang,Huihong Shi,Haikuo Shao,Zhongfeng Wang*

Main category: cs.AR

TL;DR: 提出了AccLLM框架，通过算法和硬件协同设计，实现了资源受限边缘设备上高效、快速的LLM推理，包括剪枝、特殊注意力机制和量化方案，并在FPGA上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在NLP领域的成功，将其部署到资源受限的边缘设备面临计算、内存和带宽的挑战，需要一种高效的加速框架来解决这些问题。

Method: AccLLM采用算法和硬件协同设计，算法层面结合剪枝、Λ形注意力机制和W2A8KV4量化方案；硬件层面设计了基于FPGA的可重构加速器。

Result: 在Xilinx Alveo U280 FPGA上验证，能效比提升4.07倍，吞吐量提升2.98倍，优于现有技术FlightLLM。

Conclusion: AccLLM框架有效解决了边缘设备部署LLM的挑战，为实际应用提供了高效的解决方案。

Abstract: Recently, large language models (LLMs) have achieved huge success in the
natural language processing (NLP) field, driving a growing demand to extend
their deployment from the cloud to edge devices. However, deploying LLMs on
resource-constrained edge devices poses significant challenges, including (1)
intensive computations and huge model sizes, (2) great memory and bandwidth
demands introduced by the autoregressive generation process, and (3) limited
scalability for handling long sequences. To address these challenges, we
propose AccLLM, a comprehensive acceleration framework that enables efficient
and fast long-context LLM inference through algorithm and hardware co-design.
At the algorithmic level, we integrate (1) pruning, (2) {\Lambda}-shaped
attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and
4-bit KV cache) quantization scheme, thus effectively reducing memory and
bandwidth requirements while facilitating LLMs' long-sequence generation. At
the hardware level, we design a dedicated FPGA-based accelerator with a
reconfigurable computing engine to effectively and flexibly accommodate diverse
operations arising from our compression algorithm, thereby fully translating
the algorithmic innovations into tangible hardware efficiency. We validate
AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency
and a 2.98x throughput compared to the state-of-the-art work FlightLLM.

</details>


### [186] [APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design](https://arxiv.org/abs/2505.03748)
*Yonghao Tan,Pingcheng Dong,Yongkun Wu,Yu Liu,Xuejiao Liu,Peng Luo,Shih-Yang Liu,Xijie Huang,Dong Zhang,Luhong Liang,Kwang-Ting Cheng*

Main category: cs.AR

TL;DR: 该论文提出了一种称为加性部分和量化（APSQ）的新方法，专注于减少DNN加速器中高精度部分和（PSUM）的内存需求和能耗，通过将PSUM累积无缝集成到量化框架中。结合可重构架构的分组策略进一步优化PSUM量化，实验表明在NLP和CV任务（如BERT、Segformer和EfficientViT）上近乎无损，并将PSUM压缩至INT8，能耗降低28-87%。


<details>
  <summary>Details</summary>
Motivation: 传统模型压缩方法通常忽略部分和（PSUM）量化，而高精度PSUM的频繁访问在输入/权重固定数据流架构中导致过高的内存需求和能耗（占功率消耗的69%）。因此，研究PSUM量化成为解决这一问题的关键。

Method: 论文提出了加性部分和量化（APSQ）方法，将PSUM累积无缝整合到量化框架中，并进一步提出结合可重构架构的分组策略增强PSUM量化。

Result: 在BERT、Segformer和EfficientViT模型上的实验显示，APSQ在NLP和CV任务中表现近乎无损，并将PSUM压缩至INT8，能耗降低28-87%。此外，LLaMA2-7B的实验表明APSQ在大型语言模型中也有潜力。

Conclusion: APSQ通过量化PSUM显著减少了内存需求和能耗，同时保持了模型性能，为DNN加速器的优化提供了有效方案。

Abstract: DNN accelerators, significantly advanced by model compression and specialized
dataflow techniques, have marked considerable progress. However, the frequent
access of high-precision partial sums (PSUMs) leads to excessive memory demands
in architectures utilizing input/weight stationary dataflows. Traditional
compression strategies have typically overlooked PSUM quantization, which may
account for 69% of power consumption. This study introduces a novel Additive
Partial Sum Quantization (APSQ) method, seamlessly integrating PSUM
accumulation into the quantization framework. A grouping strategy that combines
APSQ with PSUM quantization enhanced by a reconfigurable architecture is
further proposed. The APSQ performs nearly lossless on NLP and CV tasks across
BERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This
leads to a notable reduction in energy costs by 28-87%. Extended experiments on
LLaMA2-7B demonstrate the potential of APSQ for large language models. Code is
available at https://github.com/Yonghao-Tan/APSQ.

</details>


### [187] [AI-Powered Agile Analog Circuit Design and Optimization](https://arxiv.org/abs/2505.03750)
*Jinhai Hu,Wang Ling Goh,Yuan Gao*

Main category: cs.AR

TL;DR: 该论文结合了AI辅助的晶体管尺寸优化和系统级电路建模，通过多目标贝叶斯优化优化了线性可调跨导器，并在关键词识别应用中展示了AI如何联合优化模拟组件和应用级指标。


<details>
  <summary>Details</summary>
Motivation: 研究目的是利用AI技术自动化模拟电路设计中的设备级调优，并实现系统级协同优化，以提高性能并减少设计迭代工作量。

Method: 方法包括：(1)使用多目标贝叶斯优化（MOBO）进行晶体管尺寸的直接电路参数优化；(2)在关键词识别（KWS）应用中集成AI进行系统级优化。

Result: 结果表明，AI技术能有效提升模拟电路性能、减少设计迭代时间，并实现模拟组件与应用指标的联合优化。

Conclusion: 论文展示了AI在模拟电路设计中的潜力，尤其在自动化优化和系统级协同设计方面具有显著优势。

Abstract: Artificial intelligence (AI) techniques are transforming analog circuit
design by automating device-level tuning and enabling system-level
co-optimization. This paper integrates two approaches: (1) AI-assisted
transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct
circuit parameter optimization, demonstrated on a linearly tunable
transconductor; and (2) AI-integrated circuit transfer function modeling for
system-level optimization in a keyword spotting (KWS) application, demonstrated
by optimizing an analog bandpass filter within a machine learning training
loop. The combined insights highlight how AI can improve analog performance,
reduce design iteration effort, and jointly optimize analog components and
application-level metrics.

</details>


### [188] [Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management](https://arxiv.org/abs/2505.03756)
*Hang Zhang,Jiuchen Shi,Yixiao Wang,Quan Chen,Yizhou Shan,Minyi Guo*

Main category: cs.AR

TL;DR: FASTLIBRA是一个多LoRA缓存系统，通过依赖感知的缓存管理和性能驱动的缓存交换优化服务性能，显著减少了首次令牌时间（TTFT）。


<details>
  <summary>Details</summary>
Motivation: 现有的多LoRA推理系统未能优化服务性能，忽视了缓存LoRA和KV时的使用依赖性，因此需要一种新的系统来提高性能。

Method: FASTLIBRA由依赖感知的缓存管理器和性能驱动的缓存交换器组成，管理LoRA和KV缓存的使用依赖关系，并根据统一的成本模型决定缓存交换。

Result: 实验结果表明，FASTLIBRA平均减少TTFT达63.4%。

Conclusion: FASTLIBRA通过优化缓存管理和交换策略，显著提升了多LoRA推理系统的性能。

Abstract: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for
task-specific Large Language Model (LLM) applications. For multi-LoRA serving,
caching hot KV caches and LoRA adapters in high bandwidth memory of
accelerations can improve inference performance. However, existing Multi-LoRA
inference systems fail to optimize serving performance like Time-To-First-Toke
(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore
propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving
performance. FASTLIBRA comprises a dependency-aware cache manager and a
performance-driven cache swapper. The cache manager maintains the usage
dependencies between LoRAs and KV caches during the inference with a unified
caching pool. The cache swapper determines the swap-in or out of LoRAs and KV
caches based on a unified cost model, when the HBM is idle or busy,
respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on
average, compared to state-of-the-art works.

</details>


### [189] [Splitwiser: Efficient LM inference with constrained resources](https://arxiv.org/abs/2505.03763)
*Asad Aali,Adney Cardoza,Melissa Capo*

Main category: cs.AR

TL;DR: Splitwiser是一种优化LLM推理的方法，将计算密集型提示计算和内存密集型令牌生成拆分到同一GPU上，减少开销并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理中令牌生成阶段资源利用率不足的问题，尤其是在计算资源与内存访问之间的不平衡。

Method: 通过Splitwiser方法将提示计算和令牌生成拆分到同一GPU上，减少数据传输和网络开销，并开源了在Huggingface和vLLM上的实现。

Result: 初步结果显示Splitwiser能有效减少开销并优化资源利用。

Conclusion: Splitwiser为LLM推理提供了一种高效的方法，通过在同一GPU上拆分阶段来提升性能。

Abstract: Efficient inference of LLMs remains a crucial challenge, with two main
phases: a compute-intensive prompt computation and a memory-intensive token
generation. Despite existing batching and scheduling techniques, token
generation phases fail to fully utilize compute resources, especially when
compared to prompt computation phases. To address these challenges, we propose
Splitwiser, a methodology that splits the two phases of an LLM inference
request onto the same GPU, thereby reducing overhead and improving memory
access and cache utilization. By eliminating the need to transfer data across
devices, Splitwiser aims to minimize network-related overheads. In this report,
we describe the basic structure of our proposed pipeline while sharing
preliminary results and analysis. We implement our proposed multiprocessing
design on two widely-used and independent LLM architectures: Huggingface and
vLLM. We open-source our code for the respective implementations: 1)
Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM
(https://github.com/adney11/vllm-sysml).

</details>


### [190] [GPU Performance Portability needs Autotuning](https://arxiv.org/abs/2505.03780)
*Burkhard Ringlein,Thomas Parnell,Radu Stoica*

Main category: cs.AR

TL;DR: 通过结合即时编译与内核参数自动调优，实现了无需代码修改的高性能LLM执行，提升了15倍配置探索能力，性能超越供应商方案230%，并减少70倍代码量。


<details>
  <summary>Details</summary>
Motivation: 当前单一平台依赖限制了LLM的可移植性并造成供应商锁定，需解决硬件多样性下的性能优化问题。

Method: 采用即时编译（JIT）与内核参数自动调优技术，以Flash Attention为案例进行验证。

Result: 探索了15倍多的参数配置，性能提升最高230%，代码量减少70倍，且生成更多样化代码。

Conclusion: 自动调优是实现跨GPU供应商模型可移植性的有效路径。

Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires
tight co-design across algorithms, software, and hardware. Today's reliance on
a single dominant platform limits portability, creates vendor lock-in, and
raises barriers for new AI hardware. In this work, we make the case for
combining just-in-time (JIT) compilation with kernel parameter autotuning to
enable portable, state-of-the-art performance LLM execution without code
changes. Focusing on flash attention -- a widespread performance-critical LLM
kernel -- we demonstrate that this approach explores up to 15x more kernel
parameter configurations, produces significantly more diverse code across
multiple dimensions, and even outperforms vendor-optimized implementations by
up to 230%, all while reducing kernel code size by 70x and eliminating manual
code optimizations. Our results highlight autotuning as a promising path to
unlocking model portability across GPU vendors.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [191] [Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems](https://arxiv.org/abs/2505.03946)
*Matthew Sgambati,Aleksandar Vakanski,Matthew Anderson*

Main category: cs.DC

TL;DR: 论文提出了一种基于DD-PPO算法的RL调度器，解决了现有方法在小数据集上的局限性，并通过分布式训练提升了扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: HPC资源分配复杂，传统规则调度算法在异构和大规模系统中效率不足，现有RL方法因数据集小面临扩展性问题。

Method: 采用DD-PPO算法，支持多工作者分布式训练，避免每一步参数同步，提升训练效率和样本利用率。

Result: 在1150万HPC任务轨迹数据集上验证，DD-PPO调度器优于传统规则调度器和现有RL算法。

Conclusion: DD-PPO算法为大规模HPC调度提供了高效、可扩展的解决方案。

Abstract: Resource allocation in High Performance Computing (HPC) environments presents
a complex and multifaceted challenge for job scheduling algorithms. Beyond the
efficient allocation of system resources, schedulers must account for and
optimize multiple performance metrics, including job wait time and system
utilization. While traditional rule-based scheduling algorithms dominate the
current deployments of HPC systems, the increasing heterogeneity and scale of
those systems is expected to challenge the efficiency and flexibility of those
algorithms in minimizing job wait time and maximizing utilization. Recent
research efforts have focused on leveraging advancements in Reinforcement
Learning (RL) to develop more adaptable and intelligent scheduling strategies.
Recent RL-based scheduling approaches have explored a range of algorithms, from
Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently,
hybrid methods that integrate Graph Neural Networks with RL techniques.
However, a common limitation across these methods is their reliance on
relatively small datasets, and these methods face scalability issues when using
large datasets. This study introduces a novel RL-based scheduler utilizing the
Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm,
which supports large-scale distributed training across multiple workers without
requiring parameter synchronization at every step. By eliminating reliance on
centralized updates to a shared policy, the DD-PPO scheduler enhances
scalability, training efficiency, and sample utilization. The validation
dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO
performance between traditional and advanced scheduling approaches, and the
experimental results demonstrate improved scheduling performance in comparison
to both rule-based schedulers and existing RL-based scheduling algorithms.

</details>


### [192] [Can Large Language Models Predict Parallel Code Performance?](https://arxiv.org/abs/2505.03988)
*Gregory Bolet,Giorgis Georgakoudis,Harshitha Menon,Konstantinos Parasyris,Niranjan Hasabnis,Hayden Estes,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 这篇论文研究了是否可以用大型语言模型（LLMs）替代硬件分析来预测GPU代码的性能，通过将其建模为Roofline分类任务。实验表明，LLMs在提供分析数据时表现优异，但在少样本和无样本情况下仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 由于高端GPU的访问受限，亟需一种无需硬件执行的GPU性能预测方法。LLMs的潜力是否能解决这一问题成为研究动机。

Method: 构建了340个GPU内核的平衡数据集，评估LLMs在四种场景下的表现：带分析数据、仅源码（零样本）、带少量标签样本（少样本）以及微调。

Result: LLMs在提供分析数据时分类准确率达100%，推理能力强的LLMs在零样本和少样本下表现更优（最高64%）。微调需要更多数据支持。

Conclusion: LLMs在性能预测上有潜力，尤其在分析数据可访问时。未来通过优化数据集和提示策略，可能成为HPC性能分析的实用工具。

Abstract: Accurate determination of the performance of parallel GPU code typically
requires execution-time profiling on target hardware -- an increasingly
prohibitive step due to limited access to high-end GPUs. This paper explores
whether Large Language Models (LLMs) can offer an alternative approach for GPU
performance prediction without relying on hardware. We frame the problem as a
roofline classification task: given the source code of a GPU kernel and the
hardware specifications of a target GPU, can an LLM predict whether the GPU
kernel is compute-bound or bandwidth-bound?
  For this study, we build a balanced dataset of 340 GPU kernels, obtained from
HeCBench benchmark and written in CUDA and OpenMP, along with their
ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs
across four scenarios: (1) with access to profiling data of the kernel source,
(2) zero-shot with source code only, (3) few-shot with code and label pairs,
and (4) fine-tuned on a small custom dataset.
  Our results show that state-of-the-art LLMs have a strong understanding of
the Roofline model, achieving 100% classification accuracy when provided with
explicit profiling data. We also find that reasoning-capable LLMs significantly
outperform standard LLMs in zero- and few-shot settings, achieving up to 64%
accuracy on GPU source codes, without profiling information. Lastly, we find
that LLM fine-tuning will require much more data than what we currently have
available.
  This work is among the first to use LLMs for source-level roofline
performance prediction via classification, and illustrates their potential to
guide optimization efforts when runtime profiling is infeasible. Our findings
suggest that with better datasets and prompt strategies, LLMs could become
practical tools for HPC performance analysis and performance portability.

</details>


### [193] [Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving](https://arxiv.org/abs/2505.04021)
*Shan Yu,Jiarong Xing,Yifan Qiao,Mingyuan Ma,Yangmin Li,Yang Wang,Shuo Yang,Zhiqiang Xie,Shiyi Cao,Ke Bao,Ion Stoica,Harry Xu,Ying Sheng*

Main category: cs.DC

TL;DR: Prism是一个多LLM服务系统，通过动态GPU内存共享和调度策略降低成本并满足延迟SLO。


<details>
  <summary>Details</summary>
Motivation: 降低LLM服务成本，同时应对多模型共享GPU时的动态负载挑战。

Method: 1. 动态内存分配：物理到虚拟内存页的动态映射；2. 两级调度策略：基于运行时需求动态调整共享策略。

Result: 比现有系统节省2倍成本，SLO满足率提升3.3倍。

Conclusion: Prism通过跨模型内存协调和高效调度，实现了成本与性能的双重优化。

Abstract: Serving large language models (LLMs) is expensive, especially for providers
hosting many models, making cost reduction essential. The unique workload
patterns of serving multiple LLMs (i.e., multi-LLM serving) create new
opportunities and challenges for this task. The long-tail popularity of models
and their long idle periods present opportunities to improve utilization
through GPU sharing. However, existing GPU sharing systems lack the ability to
adjust their resource allocation and sharing policies at runtime, making them
ineffective at meeting latency service-level objectives (SLOs) under rapidly
fluctuating workloads.
  This paper presents Prism, a multi-LLM serving system that unleashes the full
potential of GPU sharing to achieve both cost efficiency and SLO attainment. At
its core, Prism tackles a key limitation of existing
systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$,
which is essential for flexibly sharing GPU memory across models under dynamic
workloads. Prism achieves this with two key designs. First, it supports
on-demand memory allocation by dynamically mapping physical to virtual memory
pages, allowing flexible memory redistribution among models that space- and
time-share a GPU. Second, it improves memory efficiency through a two-level
scheduling policy that dynamically adjusts sharing strategies based on models'
runtime demands. Evaluations on real-world traces show that Prism achieves more
than $2\times$ cost savings and $3.3\times$ SLO attainment compared to
state-of-the-art systems.

</details>


### [194] [MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models](https://arxiv.org/abs/2505.03906)
*Asif Rahman,Veljko Cvetkovic,Kathleen Reece,Aidan Walters,Yasir Hassan,Aneesh Tummeti,Bryan Torres,Denise Cooney,Margaret Ellis,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: MARCO是一个多智能体框架，通过代码生成和性能评估智能体的协作，结合实时网络搜索优化技术，显著提升LLM在HPC代码生成中的性能。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（LLM）在高性能计算（HPC）代码生成中常忽略并行性、内存效率等优化需求，MARCO旨在解决这一局限性。

Method: MARCO采用多智能体架构，包含代码生成和性能评估智能体，通过反馈循环逐步优化，并结合实时网络搜索补充LLM知识。

Result: 在LeetCode 75测试集上，MARCO比Claude 3.5 Sonnet平均减少14.6%运行时间，网络搜索组件进一步带来30.9%性能提升。

Conclusion: 多智能体系统能有效满足HPC代码生成的专门需求，为领域特定模型微调提供了经济高效的替代方案。

Abstract: Large language models (LLMs) have transformed software development through
code generation capabilities, yet their effectiveness for high-performance
computing (HPC) remains limited. HPC code requires specialized optimizations
for parallelism, memory efficiency, and architecture-specific considerations
that general-purpose LLMs often overlook. We present MARCO (Multi-Agent
Reactive Code Optimizer), a novel framework that enhances LLM-generated code
for HPC through a specialized multi-agent architecture. MARCO employs separate
agents for code generation and performance evaluation, connected by a feedback
loop that progressively refines optimizations. A key innovation is MARCO's
web-search component that retrieves real-time optimization techniques from
recent conference proceedings and research publications, bridging the knowledge
gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem
set demonstrates that MARCO achieves a 14.6% average runtime reduction compared
to Claude 3.5 Sonnet alone, while the integration of the web-search component
yields a 30.9% performance improvement over the base MARCO system. These
results highlight the potential of multi-agent systems to address the
specialized requirements of high-performance code generation, offering a
cost-effective alternative to domain-specific model fine-tuning.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [195] [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
*Nouar Aldahoul,Hazem Ibrahim,Matteo Varvello,Aaron Kaufman,Talal Rahwan,Yasir Zaki*

Main category: cs.CY

TL;DR: 本文挑战了大型语言模型（LLMs）政治偏见较小的普遍观点，通过比较31个LLMs与立法者、法官和美国选民样本，发现LLMs的党派偏好是极端观点的抵消结果。实验显示，LLMs在政治说服中具有显著影响力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是质疑LLMs政治偏见较小的主流观点，揭示其在特定议题上的极端偏好，并探讨LLMs作为政治影响力工具的潜力。

Method: 方法包括将31个LLMs与立法者、法官及美国选民样本进行比较，并通过随机实验评估LLMs的政治说服效果。

Result: 结果显示，LLMs的总体党派偏好是极端观点的抵消结果，且在政治讨论中能显著影响选民偏好（最高增加5个百分点），且这种影响不受用户背景的调节。

Conclusion: 结论指出，LLMs可能成为私企或政府进行针对性政治影响的强大工具。

Abstract: Large Language Models (LLMs) are a transformational technology, fundamentally
changing how people obtain information and interact with the world. As people
become increasingly reliant on them for an enormous variety of tasks, a body of
academic research has developed to examine these models for inherent biases,
especially political biases, often finding them small. We challenge this
prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a
nationally representative sample of U.S. voters, we show that LLMs' apparently
small overall partisan preference is the net result of offsetting extreme views
on specific topics, much like moderate voters. Second, in a randomized
experiment, we show that LLMs can promulgate their preferences into political
persuasiveness even in information-seeking contexts: voters randomized to
discuss political issues with an LLM chatbot are as much as 5 percentage points
more likely to express the same preferences as that chatbot. Contrary to
expectations, these persuasive effects are not moderated by familiarity with
LLMs, news consumption, or interest in politics. LLMs, especially those
controlled by private companies or governments, may become a powerful and
targeted vector for political influence.

</details>


### [196] [Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators](https://arxiv.org/abs/2505.03859)
*Will Hawkins,Chris Russell,Brent Mittelstadt*

Main category: cs.CY

TL;DR: 本文通过分析Hugging Face和Civitai上的数千个公开可下载模型变体，揭示了深伪技术模型的易获取性和广泛传播问题，96%的模型针对女性，且意图生成非自愿亲密图像（NCII）。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索文本到图像（T2I）模型中深伪技术模型的在线可获取性及其潜在风险，尤其是针对个人的非自愿描绘。

Method: 方法包括对Hugging Face和Civitai上公开可下载模型变体的元数据分析，识别深伪模型的数量、下载量及目标人群。

Result: 研究发现近35,000个公开可下载的深伪模型变体，下载量达1,500万次，96%针对女性，且主要利用LoRA技术高效微调。

Conclusion: 结论强调需采取更强力措施打击深伪技术和NCII的传播，尽管现有平台条款和法规已禁止此类行为。

Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models
increasingly accessible and popular. However, T2I models introduce risks such
as the generation of non-consensual depictions of identifiable individuals,
otherwise known as deepfakes. This paper presents an empirical study exploring
the accessibility of deepfake model variants online. Through a metadata
analysis of thousands of publicly downloadable model variants on two popular
repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily
accessible deepfake models. Almost 35,000 examples of publicly downloadable
deepfake model variants are identified, primarily hosted on Civitai. These
deepfake models have been downloaded almost 15 million times since November
2022, with the models targeting a range of individuals from global celebrities
to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux
models are used for the creation of deepfake models, with 96% of these
targeting women and many signalling intent to generate non-consensual intimate
imagery (NCII). Deepfake model variants are often created via the
parameter-efficient fine-tuning technique known as low rank adaptation (LoRA),
requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this
process widely accessible via consumer-grade computers. Despite these models
violating the Terms of Service of hosting platforms, and regulation seeking to
prevent dissemination, these results emphasise the pressing need for greater
action to be taken against the creation of deepfakes and NCII.

</details>


### [197] [AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions](https://arxiv.org/abs/2505.04592)
*Peter Barnett,Aaron Scher*

Main category: cs.CY

TL;DR: AI发展可能导致人类灭绝等灾难性风险，研究提出了四种应对场景，其中国际协调的『关机开关』方案最为推荐，呼吁紧急行动。


<details>
  <summary>Details</summary>
Motivation: AI可能超越人类智能，失控或滥用将带来灾难性风险，需研究如何控制以降低危害。

Method: 提出四种高级场景分析AI发展的地缘政治应对，重点关注国际协调、国家单边行动、轻监管和破坏威慑。

Result: 最推荐的『关机开关』方案能有效降低风险，其他场景风险较高。

Conclusion: 需紧急行动，研究关键问题并构建国际AI协议能力以避免灾难。

Abstract: Humanity appears to be on course to soon develop AI systems that
substantially outperform human experts in all cognitive domains and activities.
We believe the default trajectory has a high likelihood of catastrophe,
including human extinction. Risks come from failure to control powerful AI
systems, misuse of AI by malicious rogue actors, war between great powers, and
authoritarian lock-in. This research agenda has two aims: to describe the
strategic landscape of AI development and to catalog important governance
research questions. These questions, if answered, would provide important
insight on how to successfully reduce catastrophic risks.
  We describe four high-level scenarios for the geopolitical response to
advanced AI development, cataloging the research questions most relevant to
each. Our favored scenario involves building the technical, legal, and
institutional infrastructure required to internationally restrict dangerous AI
development and deployment (which we refer to as an Off Switch), which leads
into an internationally coordinated Halt on frontier AI activities at some
point in the future. The second scenario we describe is a US National Project
for AI, in which the US Government races to develop advanced AI systems and
establish unilateral control over global AI development. We also describe two
additional scenarios: a Light-Touch world similar to that of today and a Threat
of Sabotage situation where countries use sabotage and deterrence to slow AI
development.
  In our view, apart from the Off Switch and Halt scenario, all of these
trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent
action is needed from the US National Security community and AI governance
ecosystem to answer key research questions, build the capability to halt
dangerous AI activities, and prepare for international AI agreements.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [198] [Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures](https://arxiv.org/abs/2505.03764)
*Logan Larsh,Raiyan Siddique,Sarah Sharif Yaser Mike Banad*

Main category: cs.NE

TL;DR: 论文比较了三种脉冲神经元电路架构（LIF、ML、AH）在7纳米FinFET技术中的表现，重点优化了脉冲频率、能耗和静态功耗，结果显示AH设计吞吐量最高，而ML在低功耗下表现优越。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过优化脉冲神经元电路架构，为大规模人工智能应用提供兼具高能效和高计算吞吐量的神经形态硬件解决方案。

Method: 通过SPICE仿真对LIF、ML和AH三种电路架构在7纳米FinFET中的性能进行了全面比较，分析了脉冲频率、能耗和静态功耗等指标。

Result: AH设计实现最高吞吐量（3 GHz脉冲频率），ML在低功耗区间表现最佳（0.385 aJ/脉冲），LIF在高频操作中稍逊但静态泄漏较高。7纳米FinFET显著提升了能效和速度，但增加了亚阈值泄漏。

Conclusion: 研究量化了不同神经元架构的设计权衡，为先进纳米技术中脉冲神经元电路的优化提供了路线图，以实现超低功耗和高计算吞吐量的神经形态硬件。

Abstract: Neuromorphic computing aims to replicate the brain's remarkable energy
efficiency and parallel processing capabilities for large-scale artificial
intelligence applications. In this work, we present a comprehensive comparative
study of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire
(LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET
technology. Through extensive SPICE simulations, we explore the optimization of
spiking frequency, energy per spike, and static power consumption. Our results
show that the AH design achieves the highest throughput, demonstrating
multi-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By
contrast, the ML architecture excels in subthreshold to near-threshold regimes,
offering robust low-power operation (as low as 0.385 aJ/spike) and biological
bursting behavior. Although LIF benefits from a decoupled current mirror for
high-frequency operation, it exhibits slightly higher static leakage compared
to ML and AH at elevated supply voltages. Comparisons with previous node
implementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically
boost energy efficiency and speed albeit at the cost of increased subthreshold
leakage in deep subthreshold regions. By quantifying design trade-offs for each
neuron architecture, our work provides a roadmap for optimizing spiking neuron
circuits in advanced nanoscale technologies to deliver neuromorphic hardware
capable of both ultra-low-power operation and high computational throughput.

</details>


### [199] [Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks](https://arxiv.org/abs/2505.04034)
*Ayana Moshruba,Hamed Poursiami,Maryam Parsa*

Main category: cs.NE

TL;DR: 提出两种概率驱动的输入级时间脉冲变换（Poisson-Burst和Delayed-Burst），通过生物启发的时序变异性增强LIF神经元的隐私性、泛化性和学习性能。


<details>
  <summary>Details</summary>
Motivation: 生物神经元的多样时序脉冲模式支持高效、鲁棒和自适应的信息处理，但现有复杂模型难以直接集成到可扩展的SNN训练流程中，因此需要简化方法。

Method: 设计Poisson-Burst（基于输入强度调制脉冲群）和Delayed-Burst（通过脉冲群起始时间编码输入强度），将其应用于标准LIF神经元。

Result: 实验表明，Poisson-Burst在保持高精度的同时提升隐私鲁棒性；Delayed-Burst以轻微精度代价提供更强的隐私保护。

Conclusion: 生物启发的时序脉冲动态可提升神经形态学习系统的隐私性、泛化性和生物合理性。

Abstract: Biological neurons exhibit diverse temporal spike patterns, which are
believed to support efficient, robust, and adaptive neural information
processing. While models such as Izhikevich can replicate a wide range of these
firing dynamics, their complexity poses challenges for directly integrating
them into scalable spiking neural networks (SNN) training pipelines. In this
work, we propose two probabilistically driven, input-level temporal spike
transformations: Poisson-Burst and Delayed-Burst that introduce biologically
inspired temporal variability directly into standard Leaky Integrate-and-Fire
(LIF) neurons. This enables scalable training and systematic evaluation of how
spike timing dynamics affect privacy, generalization, and learning performance.
Poisson-Burst modulates burst occurrence based on input intensity, while
Delayed-Burst encodes input strength through burst onset timing. Through
extensive experiments across multiple benchmarks, we demonstrate that
Poisson-Burst maintains competitive accuracy and lower resource overhead while
exhibiting enhanced privacy robustness against membership inference attacks,
whereas Delayed-Burst provides stronger privacy protection at a modest accuracy
trade-off. These findings highlight the potential of biologically grounded
temporal spike dynamics in improving the privacy, generalization and biological
plausibility of neuromorphic learning systems.

</details>


### [200] [TS-SNN: Temporal Shift Module for Spiking Neural Networks](https://arxiv.org/abs/2505.04165)
*Kairong Yu,Tianqing Zhang,Qi Xu,Gang Pan,Hongwei Wang*

Main category: cs.NE

TL;DR: 论文提出了一种名为TS-SNN的脉冲神经网络模块，通过引入轻量级的Temporal Shift模块，有效整合时序信息，提升了模型性能，同时在CIFAR和ImageNet等基准测试中取得了最优结果，且保持低能耗。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）因其生物合理性和高能效在神经形态计算中具有潜力，但如何在利用时序信息的同时保持低能耗仍是一大挑战。本文旨在解决这一问题。

Method: 提出Temporal Shift模块（TS-SNN），通过简单的位移操作整合过去、现在和未来的脉冲特征，并采用残差组合避免信息丢失。模块轻量，仅需一个可学习参数。

Result: 在CIFAR-10（96.72%）、CIFAR-100（80.28%）和ImageNet（70.61%）上达到最优性能，且用时序步数更少，能耗低。

Conclusion: TS-SNN为高效且准确的SNN架构设计迈出了重要一步，解决了时序特征利用与能耗权衡的难题。

Abstract: Spiking Neural Networks (SNNs) are increasingly recognized for their
biological plausibility and energy efficiency, positioning them as strong
alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing
applications. SNNs inherently process temporal information by leveraging the
precise timing of spikes, but balancing temporal feature utilization with low
energy consumption remains a challenge. In this work, we introduce Temporal
Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel
Temporal Shift (TS) module to integrate past, present, and future spike
features within a single timestep via a simple yet effective shift operation. A
residual combination method prevents information loss by integrating shifted
and original features. The TS module is lightweight, requiring only one
additional learnable parameter, and can be seamlessly integrated into existing
architectures with minimal additional computational cost. TS-SNN achieves
state-of-the-art performance on benchmarks like CIFAR-10 (96.72\%), CIFAR-100
(80.28\%), and ImageNet (70.61\%) with fewer timesteps, while maintaining low
energy consumption. This work marks a significant step forward in developing
efficient and accurate SNN architectures.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [201] [Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework](https://arxiv.org/abs/2505.03746)
*Silvia García-Méndez,Francisco De Arriba-Pérez*

Main category: cs.SI

TL;DR: 论文提出了一种结合流式机器学习和大型语言模型的实时网络欺凌检测方法，性能接近90%，超越现有研究。


<details>
  <summary>Details</summary>
Motivation: 社交媒体虽然促进了连接，但也带来了网络欺凌等问题。当前生成式AI的研究多集中于零/少样本学习，而缺少对其性能的进一步探索。

Method: 利用流式机器学习模型增量处理数据，并通过大型语言模型进行特征工程，以应对网络虐待和仇恨言论的动态变化。同时提供了可解释性仪表板。

Result: 实验数据表明，该方法在所有评估指标中接近90%的性能，优于现有文献中的竞争方法。

Conclusion: 该研究通过实时检测虐待行为，为在线社区安全做出贡献，减少了社会负面影响。

Abstract: Social media platforms enable instant and ubiquitous connectivity and are
essential to social interaction and communication in our technological society.
Apart from its advantages, these platforms have given rise to negative
behaviors in the online community, the so-called cyberbullying. Despite the
many works involving generative Artificial Intelligence (AI) in the literature
lately, there remain opportunities to study its performance apart from
zero/few-shot learning strategies. Accordingly, we propose an innovative and
real-time solution for cyberbullying detection that leverages stream-based
Machine Learning (ML) models able to process the incoming samples incrementally
and Large Language Models (LLMS) for feature engineering to address the
evolving nature of abusive and hate speech online. An explainability dashboard
is provided to promote the system's trustworthiness, reliability, and
accountability. Results on experimental data report promising performance close
to 90 % in all evaluation metrics and surpassing those obtained by competing
works in the literature. Ultimately, our proposal contributes to the safety of
online communities by timely detecting abusive behavior to prevent long-lasting
harassment and reduce the negative consequences in society.

</details>


### [202] [The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing](https://arxiv.org/abs/2505.03769)
*Yibo Hu,Yiqiao Jin,Meng Ye,Ajay Divakaran,Srijan Kumar*

Main category: cs.SI

TL;DR: 研究分析了改写Reddit帖子标题对用户参与度的影响，发现改写能显著提升参与度，有效的改写通常具有情感共鸣、词汇丰富和社区规范对齐的特点。


<details>
  <summary>Details</summary>
Motivation: 在跨平台社交媒体中，理解多模态内容（尤其是文本与视觉结合）如何驱动用户参与度仍然复杂，研究旨在探索改写标题对参与度的影响。

Method: 首先分析了Reddit分享YouTube视频的大数据集，然后设计了一个多阶段实验来隔离文本变化的影响，最后使用微调的BERT分类器进行排名预测实验。

Result: 统计测试显示改写标题显著提升参与度，BERT分类器准确率达到74%，显著优于包括GPT-4o在内的基线模型。

Conclusion: 研究验证了数据集能有效最小化混淆效应，揭示了文本特征对参与度的影响，为跨平台多模态内容策略提供了框架。

Abstract: In today's cross-platform social media landscape, understanding factors that
drive engagement for multimodal content, especially text paired with visuals,
remains complex. This study investigates how rewriting Reddit post titles
adapted from YouTube video titles affects user engagement. First, we build and
analyze a large dataset of Reddit posts sharing YouTube videos, revealing that
21% of post titles are minimally modified. Statistical analysis demonstrates
that title rewrites measurably improve engagement. Second, we design a
controlled, multi-phase experiment to rigorously isolate the effects of textual
variations by neutralizing confounding factors like video popularity, timing,
and community norms. Comprehensive statistical tests reveal that effective
title rewrites tend to feature emotional resonance, lexical richness, and
alignment with community-specific norms. Lastly, pairwise ranking prediction
experiments using a fine-tuned BERT classifier achieves 74% accuracy,
significantly outperforming near-random baselines, including GPT-4o. These
results validate that our controlled dataset effectively minimizes confounding
effects, allowing advanced models to both learn and demonstrate the impact of
textual features on engagement. By bridging quantitative rigor with qualitative
insights, this study uncovers engagement dynamics and offers a robust framework
for future cross-platform, multimodal content strategies.

</details>


### [203] [Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics](https://arxiv.org/abs/2505.03795)
*Jacob W. Crandall,Jonathan Skaggs*

Main category: cs.SI

TL;DR: 该论文研究了如何通过不同方法建模人类在战略网络游戏中的行为，发现社区感知且建模分布的方法（hCAB）表现最佳，能较好地模拟人群动态且难以与真人区分。


<details>
  <summary>Details</summary>
Motivation: 人类社会网络对财富、健康等社会结果有重要影响，理解人类行为建模方法有助于推动有利社会结果。

Method: 对比了基于行为匹配和社区感知的两种假设，以及建模均值与分布的统计方法。

Result: hCAB方法（社区感知且建模分布）在模拟人群动态时表现最优，且人类参与者难以区分hCAB代理与真人。

Conclusion: 社区感知和分布建模的结合能更真实地反映人类行为，为理解社会网络提供了有效工具。

Abstract: Human networks greatly impact important societal outcomes, including wealth
and health inequality, poverty, and bullying. As such, understanding human
networks is critical to learning how to promote favorable societal outcomes. As
a step toward better understanding human networks, we compare and contrast
several methods for learning models of human behavior in a strategic network
game called the Junior High Game (JHG). These modeling methods differ with
respect to the assumptions they use to parameterize human behavior (behavior
vs. community-aware behavior) and the statistical moments they model (mean vs.
distribution). Results show that the highest-performing method models the
population's distribution rather than the mean and assumes humans use
community-aware behavior rather than behavior matching. When applied to small
societies (6-11 individuals), this learned model, called hCAB, closely mirrors
the population dynamics of human groups (with some differences). Additionally,
a user study reveals that human participants were unable to distinguish hCAB
agents from other humans, thus illustrating that individual hCAB behavior
plausibly mirrors human behavior in this strategic network game.

</details>


### [204] [Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries](https://arxiv.org/abs/2505.03816)
*Bidyarthi Paul,Fariha Tasnim Chowdhury,Dipta Biswas,Meherin Sultana*

Main category: cs.SI

TL;DR: 论文通过分析纽约和达卡的交通数据，使用EDA、SARIMAX模型和聚类技术，揭示了需求模式和高峰时段，优化了城市交通和配送服务。


<details>
  <summary>Details</summary>
Motivation: 研究现代城市中高效人员和货物流动的重要性，通过数据分析提升交通服务效率。

Method: 采用探索性数据分析（EDA）、地理空间分析、SARIMAX时间序列模型和聚类技术。

Result: 识别出需求高峰时段和热点区域，为车队管理和资源分配提供了优化建议。

Conclusion: 研究结果为提升城市交通和配送服务效率提供了实用见解，适用于不同城市环境。

Abstract: Urban transportation plays a vital role in modern city life, affecting how
efficiently people and goods move around. This study analyzes transportation
patterns using two datasets: the NYC Taxi Trip dataset from New York City and
the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify
key trends in demand, peak times, and important geographical hotspots. We start
with Exploratory Data Analysis (EDA) to understand the basic characteristics of
the datasets. Next, we perform geospatial analysis to map out high-demand and
low-demand regions. We use the SARIMAX model for time series analysis to
forecast demand patterns, capturing seasonal and weekly variations. Lastly, we
apply clustering techniques to identify significant areas of high and low
demand. Our findings provide valuable insights for optimizing fleet management
and resource allocation in both passenger transport and food delivery services.
These insights can help improve service efficiency, better meet customer needs,
and enhance urban transportation systems in diverse urban environments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [205] [Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication](https://arxiv.org/abs/2504.13777)
*Anqi Shao*

Main category: cs.HC

TL;DR: 这篇论文提出了一个概念框架，用于理解AI幻觉作为一种独特形式的错误信息。虽然传统上错误信息研究关注人类意图，但生成式AI系统现在能在缺乏意图的情况下产生虚假但可信的输出。作者认为这些AI幻觉不应仅被视为技术故障，而应是具有社会影响的传播现象。


<details>
  <summary>Details</summary>
Motivation: 传统错误信息研究集中于人为主观意图，而生成式AI的出现引入了无意图的虚假信息产生方式。作者试图填补这一空白，探讨AI幻觉作为传播现象的独特性及其社会影响。

Method: 采用供给-需求模型和分布式代理概念，从生产、感知和机构回应三方面比较AI幻觉与人类错误信息的差异。

Result: 提出了一个多层级研究框架（宏观机构、中观群体、微观个体），用于分析幻觉内容的产生、传播与受众接收。

Conclusion: 呼吁传播学者重新思考错误信息理论的边界，以适应概率性非人类行为体在知识生产中日益增长的角色。

Abstract: This paper proposes a conceptual framework for understanding AI
hallucinations as a distinct form of misinformation. While misinformation
scholarship has traditionally focused on human intent, generative AI systems
now produce false yet plausible outputs absent of such intent. I argue that
these AI hallucinations should not be treated merely as technical failures but
as communication phenomena with social consequences. Drawing on a
supply-and-demand model and the concept of distributed agency, the framework
outlines how hallucinations differ from human-generated misinformation in
production, perception, and institutional response. I conclude by outlining a
research agenda for communication scholars to investigate the emergence,
dissemination, and audience reception of hallucinated content, with attention
to macro (institutional), meso (group), and micro (individual) levels. This
work urges communication researchers to rethink the boundaries of
misinformation theory in light of probabilistic, non-human actors increasingly
embedded in knowledge production.

</details>


### [206] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)
*Yiwen Zhang,Jianing Hao,Zhan Wang,Hongling Sheng,Wei Zeng*

Main category: cs.HC

TL;DR: 本文提出了一种基于用户意图的视频故事交互系统，利用视觉语言模型、检索增强生成和多智能体系统，实现个性化角色成长和场景定制。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于用户选择和特定设计的叙事，缺乏个性化定制，因此需要一种更灵活的系统来提升交互体验。

Method: 系统分为三个阶段：1)视频故事处理，利用视觉语言模型模拟人类理解；2)多空间对话，通过多智能体系统生成成长型角色；3)场景定制，扩展和可视化对话中的故事场景。

Result: 应用于《哈利·波特》系列，系统成功展现了角色的社会化行为和成长，提升了视频故事的交互体验。

Conclusion: 该系统通过结合先进技术实现了高度个性化的视频故事交互，为未来叙事体验提供了新方向。

Abstract: Video story interaction enables viewers to engage with and explore narrative
content for personalized experiences. However, existing methods are limited to
user selection, specially designed narratives, and lack customization. To
address this, we propose an interactive system based on user intent. Our system
uses a Vision Language Model (VLM) to enable machines to understand video
stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent
System (MAS) to create evolving characters and scene experiences. It includes
three stages: 1) Video story processing, utilizing VLM and prior knowledge to
simulate human understanding of stories across three modalities. 2) Multi-space
chat, creating growth-oriented characters through MAS interactions based on
user queries and story stages. 3) Scene customization, expanding and
visualizing various story scenes mentioned in dialogue. Applied to the Harry
Potter series, our study shows the system effectively portrays emergent
character social behavior and growth, enhancing the interactive experience in
the video story world.

</details>


### [207] [Scratch Copilot: Supporting Youth Creative Coding with AI](https://arxiv.org/abs/2505.03867)
*Stefania Druga,Amy J. Ko*

Main category: cs.HC

TL;DR: AI助手Cognimates Scratch Copilot为儿童提供Scratch编程实时支持，通过18名儿童的定性评估显示其增强了创意编程能力和自主性。


<details>
  <summary>Details</summary>
Motivation: 解决儿童在Scratch等平台编程时难以实现创意的问题，并填补面向儿童的AI辅助工具空白。

Method: 开发集成了AI的Scratch Copilot系统，并进行18名国际儿童的定性评估。

Result: AI助手支持了创意和调试过程，儿童在保持创意控制的同时展现了自主性。

Conclusion: 提出了注重儿童自主性和批判性交互的AI编程助手设计指南，以增强创意自我效能和参与度。

Abstract: Creative coding platforms like Scratch have democratized programming for
children, yet translating imaginative ideas into functional code remains a
significant hurdle for many young learners. While AI copilots assist adult
programmers, few tools target children in block-based environments. Building on
prior research \cite{druga_how_2021,druga2023ai, druga2023scratch}, we present
Cognimates Scratch Copilot: an AI-powered assistant integrated into a
Scratch-like environment, providing real-time support for ideation, code
generation, debugging, and asset creation. This paper details the system
architecture and findings from an exploratory qualitative evaluation with 18
international children (ages 7--12). Our analysis reveals how the AI Copilot
supported key creative coding processes, particularly aiding ideation and
debugging. Crucially, it also highlights how children actively negotiated the
use of AI, demonstrating strong agency by adapting or rejecting suggestions to
maintain creative control. Interactions surfaced design tensions between
providing helpful scaffolding and fostering independent problem-solving, as
well as learning opportunities arising from navigating AI limitations and
errors. Findings indicate Cognimates Scratch Copilot's potential to enhance
creative self-efficacy and engagement. Based on these insights, we propose
initial design guidelines for AI coding assistants that prioritize youth agency
and critical interaction alongside supportive scaffolding.

</details>


### [208] [Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering](https://arxiv.org/abs/2505.04260)
*Jessica Y. Bo,Tianyu Xu,Ishan Chatterjee,Katrina Passarella-Ward,Achin Kulshrestha,D Shin*

Main category: cs.HC

TL;DR: 这篇论文探讨了如何通过激活引导技术改进大型语言模型（LLMs），使其能更好地根据用户的隐性偏好生成个性化回答，从而提升用户满意度和留存率。


<details>
  <summary>Details</summary>
Motivation: 用户通常难以通过提示词准确表达其隐性偏好，导致LLMs生成的回答与用户期望不一致。论文旨在通过轻量级的激活引导技术解决这一问题，而无需依赖冗长的用户历史记录。

Method: 利用激活引导技术，在推理过程中将LLMs的输出与可解释的偏好维度对齐。通过三个不同的交互式聊天机器人界面嵌入引导机制，并进行了用户研究（n=14）。

Result: 实验表明，基于偏好的引导能有效对齐实际对话与用户隐性偏好，并揭示了用户对不同界面（控制、易用性、透明度）的偏好差异。

Conclusion: 激活引导是一种轻量且灵活的方法，能显著提升LLMs的个性化能力，满足多样化的用户需求。

Abstract: As large language models (LLMs) improve in their capacity to serve as
personal AI assistants, their ability to output uniquely tailored, personalized
responses that align with the soft preferences of their users is essential for
enhancing user satisfaction and retention. However, untrained lay users have
poor prompt specification abilities and often struggle with conveying their
latent preferences to AI assistants. To address this, we leverage activation
steering to guide LLMs to align with interpretable preference dimensions during
inference. In contrast to memory-based personalization methods that require
longer user history, steering is extremely lightweight and can be easily
controlled by the user via an linear strength factor. We embed steering into
three different interactive chatbot interfaces and conduct a within-subjects
user study (n=14) to investigate how end users prefer to personalize their
conversations. The results demonstrate the effectiveness of preference-based
steering for aligning real-world conversations with hidden user preferences,
and highlight further insights on how diverse values around control, usability,
and transparency lead users to prefer different interfaces.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [209] [On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation](https://arxiv.org/abs/2505.03757)
*Vinicius Francisco Rofatto,Luiz Felipe Rodrigues de Almeida,Marcelo Tomio Matsuoka,Ivandro Klein,Mauricio Roberto Veronez,Luiz Gonzaga Da Silveira Junior*

Main category: physics.geo-ph

TL;DR: 该论文提出一种基于残差的神经校正策略，通过神经网络学习建模几何变换后的系统性失真，从而降低模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有坐标变换模型常因无法处理非线性及空间依赖性失真而导致较大残差，尤其在稀疏或结构化控制点配置的场景中表现不佳。

Method: 采用残差学习方法，神经网络仅针对初始几何变换后的剩余失真进行建模，简化模型并提升效果。

Result: 在模拟数据集和真实图像地理配准任务中，该方法相比传统变换模型和直接神经网络转换器，在挑战性条件下表现更准确稳定，理想情况下性能相当。

Conclusion: 残差建模是一种轻量且鲁棒的方法，可显著提升坐标变换精度，尤其适用于复杂或稀疏数据场景。

Abstract: Coordinate transformation models often fail to account for nonlinear and
spatially dependent distortions, leading to significant residual errors in
geospatial applications. Here we propose a residual-based neural correction
strategy, in which a neural network learns to model only the systematic
distortions left by an initial geometric transformation. By focusing solely on
residual patterns, the proposed method reduces model complexity and improves
performance, particularly in scenarios with sparse or structured control point
configurations. We evaluate the method using both simulated datasets with
varying distortion intensities and sampling strategies, as well as under the
real-world image georeferencing tasks. Compared with direct neural network
coordinate converter and classical transformation models, the residual-based
neural correction delivers more accurate and stable results under challenging
conditions, while maintaining comparable performance in ideal cases. These
findings demonstrate the effectiveness of residual modelling as a lightweight
and robust alternative for improving coordinate transformation accuracy.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [210] [A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs](https://arxiv.org/abs/2505.04401)
*Wei Wang,Peizheng Li,Angela Doufexi,Mark A. Beach*

Main category: eess.SP

TL;DR: 提出了一种启发式与深度强化学习（DRL）结合的框架，通过双深度Q网络（DDQN）逐列控制RIS，并结合贪心算法（GA）进行细粒度优化，有效解决大规模RIS相位优化问题。


<details>
  <summary>Details</summary>
Motivation: 大规模可重构智能表面（RIS）的离散相位优化问题由于其非凸和非线性特性而具有挑战性，因此需要一种高效的解决方案。

Method: 结合启发式与DRL框架，利用DDQN进行RIS逐列控制，并在每个DRL步骤中集成GA进行细粒度的元素级优化。

Result: 提出的方法在小DRL动作空间内有效优化了大规模RIS的相位配置，展示了其解决复杂优化问题的能力。

Conclusion: 该框架通过结合DRL和启发式方法，为大规模RIS的相位优化提供了一种高效且可行的解决方案。

Abstract: Optimizing discrete phase shifts in large-scale reconfigurable intelligent
surfaces (RISs) is challenging due to their non-convex and non-linear nature.
In this letter, we propose a heuristic-integrated deep reinforcement learning
(DRL) framework that (1) leverages accumulated actions over multiple steps in
the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates
a greedy algorithm (GA) into each DRL step to refine the state via
fine-grained, element-wise optimization of RIS configurations. By learning from
GA-included states, the proposed approach effectively addresses RIS
optimization within a small DRL action space, demonstrating its capability to
optimize phase-shift configurations of large-scale RISs.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [211] [Differentially Private Densest-$k$-Subgraph](https://arxiv.org/abs/2505.03858)
*Alireza Khayatian,Anil Vullikanti,Aritra Konar*

Main category: cs.DS

TL;DR: 该论文首次提出了一种在保护差分隐私（DP）的条件下解决图的Densest-$k$-subgraph（D$k$S）问题的算法，通过利用图的邻接矩阵主成分（PC）来输出$k$个顶点子集，并讨论了输出扰动、实例特定敏感度、Propose-Test-Release（PTR）框架以及私有幂方法（PPM）等方法，展示了在不同规模真实网络上的隐私-效用权衡和性能优势。


<details>
  <summary>Details</summary>
Motivation: 图数据中常包含敏感信息，因此需要开发保护隐私的图挖掘算法。D$k$S问题是图挖掘中的核心任务，旨在提取内部连通性最强的$k$顶点子集。此前的研究缺乏针对该问题的正式差分隐私保护算法。

Method: 论文基于图的邻接矩阵主成分（PC）设计算法，探讨了输出扰动、实例特定敏感度和PTR框架等差分隐私技术。特别提出了结合PTR框架的高效实现方法，使其计算复杂度与非私有PC相当，同时提出了私有幂方法（PPM）作为备选方案。

Result: 在实际网络中测试表明，PTR框架在较大网络（如300万顶点）上性能显著优于PPM，平均提升180倍运行速度，同时保持了良好的隐私-效用权衡。

Conclusion: 论文展示了在保护差分隐私的前提下解决D$k$S问题的可行性，并通过PTR框架实现了高效且实用的解决方案，适用于大规模网络。

Abstract: Many graph datasets involve sensitive network data, motivating the need for
privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a
key primitive in graph mining that aims to extract a subset of $k$ vertices
with the maximum internal connectivity. Although non-private algorithms are
known for D$k$S, this paper is the first to design algorithms that offer formal
differential privacy (DP) guarantees for the problem. We base our general
approach on using the principal component (PC) of the graph adjacency matrix to
output a subset of $k$ vertices under edge DP. For this task, we first consider
output perturbation, which traditionally offer good scalability, but at the
expense of utility. Our tight on the local sensitivity indicate a big gap with
the global sensitivity, motivating the use of instance specific sensitive
methods for private PC. Next, we derive a tight bound on the smooth sensitivity
and show that it can be close to the global sensitivity. This leads us to
consider the Propose-Test-Release (PTR) framework for private PC. Although
computationally expensive in general, we design a novel approach for
implementing PTR in the same time as computation of a non-private PC, while
offering good utility for \DkS{}. Additionally, we also consider the iterative
private power method (PPM) for private PC, albeit it is significantly slower
than PTR on large networks. We run our methods on diverse real-world networks,
with the largest having 3 million vertices, and show good privacy-utility
trade-offs. Although PTR requires a slightly larger privacy budget, on average,
it achieves a 180-fold improvement in runtime over PPM.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [212] [AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection](https://arxiv.org/abs/2505.03796)
*Lokesh Koli,Shubham Kalra,Rohan Thakur,Anas Saifi,Karanpreet Singh*

Main category: cs.CR

TL;DR: 本文提出了一种AI驱动的内部风险管理（IRM）系统，结合行为分析、动态风险评分和实时策略执行，显著提升了内部威胁检测的准确性和适应性。通过混合评分机制和自动编码器神经网络，系统减少了59%的误报并提高了30%的真实检测率。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的检测系统难以应对内部威胁的隐蔽性和上下文相关性，需要一种更智能的解决方案来提升检测精度和响应速度。

Method: 采用混合评分机制（从静态PRISM模型过渡到基于自动编码器神经网络的AI模型），并结合行为分析和实时策略执行。

Result: 系统误报减少59%，真实检测率提高30%，每日可处理1000万条日志事件，响应时间缩短47%。

Conclusion: 该IRM系统为内部风险管理提供了可扩展且主动的框架，未来可通过解释性AI、联邦学习等技术进一步提升适应性和透明度。

Abstract: Insider threats pose a significant challenge to organizational security,
often evading traditional rule-based detection systems due to their subtlety
and contextual nature. This paper presents an AI-powered Insider Risk
Management (IRM) system that integrates behavioral analytics, dynamic risk
scoring, and real-time policy enforcement to detect and mitigate insider
threats with high accuracy and adaptability. We introduce a hybrid scoring
mechanism - transitioning from the static PRISM model to an adaptive AI-based
model utilizing an autoencoder neural network trained on expert-annotated user
activity data. Through iterative feedback loops and continuous learning, the
system reduces false positives by 59% and improves true positive detection
rates by 30%, demonstrating substantial gains in detection precision.
Additionally, the platform scales efficiently, processing up to 10 million log
events daily with sub-300ms query latency, and supports automated enforcement
actions for policy violations, reducing manual intervention. The IRM system's
deployment resulted in a 47% reduction in incident response times, highlighting
its operational impact. Future enhancements include integrating explainable AI,
federated learning, graph-based anomaly detection, and alignment with Zero
Trust principles to further elevate its adaptability, transparency, and
compliance-readiness. This work establishes a scalable and proactive framework
for mitigating emerging insider risks in both on-premises and hybrid
environments.

</details>


### [213] [Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning](https://arxiv.org/abs/2505.03817)
*Aditya Shinde,Prashant Doshi*

Main category: cs.CR

TL;DR: 该论文提出了一种利用逆向强化学习（IRL）从系统级审计日志中建模攻击者偏好的方法，展示了通过低级别取证数据自动揭示攻击者主观偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击者建模方法依赖于不断更新的攻击工具和技术文档，但攻击行为偏好是内在且稳定的。本文旨在通过取证数据学习攻击者的行为偏好，为威胁归因提供新维度。

Method: 将攻击者建模为具有未知行为偏好的专家决策代理，利用审计日志的攻击来源图推导攻击的状态-动作轨迹，采用逆向强化学习方法学习其偏好。

Result: 实验表明，低级别取证数据可自动揭示攻击者的主观偏好，这些偏好不随工具变化，可作为独特的行为签名改进威胁归因。

Conclusion: 攻击者的行为偏好是具有区分性的稳定特征，可作为威胁建模和归因的新维度，补足了现有基于工具和技术的跟踪方法。

Abstract: This paper presents a holistic approach to attacker preference modeling from
system-level audit logs using inverse reinforcement learning (IRL). Adversary
modeling is an important capability in cybersecurity that lets defenders
characterize behaviors of potential attackers, which enables attribution to
known cyber adversary groups. Existing approaches rely on documenting an
ever-evolving set of attacker tools and techniques to track known threat
actors. Although attacks evolve constantly, attacker behavioral preferences are
intrinsic and less volatile. Our approach learns the behavioral preferences of
cyber adversaries from forensics data on their tools and techniques. We model
the attacker as an expert decision-making agent with unknown behavioral
preferences situated in a computer host. We leverage attack provenance graphs
of audit logs to derive a state-action trajectory of the attack. We test our
approach on open datasets of audit logs containing real attack data. Our
results demonstrate for the first time that low-level forensics data can
automatically reveal an adversary's subjective preferences, which serves as an
additional dimension to modeling and documenting cyber adversaries. Attackers'
preferences tend to be invariant despite their different tools and indicate
predispositions that are inherent to the attacker. As such, these inferred
preferences can potentially serve as unique behavioral signatures of attackers
and improve threat attribution.

</details>


### [214] [Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles](https://arxiv.org/abs/2505.03850)
*Hanlin Chen,Simin Chen,Wenyu Li,Wei Yang,Yiheng Feng*

Main category: cs.CR

TL;DR: 本文提出了一种基于推理时间攻击对自动驾驶车辆的影响分析，模拟显示此类攻击可能威胁车辆及交通参与者的安全。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）的感知模块是易受攻击的，现有研究多关注感知正确性，但缺乏对推理时间攻击影响的分析。

Method: 通过模拟系统研究推理时间攻击对AVs安全的影响。

Result: 推理时间攻击可能威胁自动驾驶车辆及其周边交通参与者的安全。

Conclusion: 强调了自动驾驶感知安全研究中考虑推理时间攻击的重要性，需进一步研究以提升安全性。

Abstract: As a safety-critical cyber-physical system, cybersecurity and related safety
issues for Autonomous Vehicles (AVs) have been important research topics for a
while. Among all the modules on AVs, perception is one of the most accessible
attack surfaces, as drivers and AVs have no control over the outside
environment. Most current work targeting perception security for AVs focuses on
perception correctness. In this work, we propose an impact analysis based on
inference time attacks for autonomous vehicles. We demonstrate in a simulation
system that such inference time attacks can also threaten the safety of both
the ego vehicle and other traffic participants.

</details>


### [215] [Data-Driven Falsification of Cyber-Physical Systems](https://arxiv.org/abs/2505.03863)
*Atanu Kundu,Sauvik Gon,Rajarshi Ray*

Main category: cs.CR

TL;DR: 本文提出了一种用于验证CPS操作安全性的框架，通过结合DNN和决策树的优点来加速CPS的伪造（即寻找不安全的执行路径）。


<details>
  <summary>Details</summary>
Motivation: CPS在安全关键领域应用广泛，其操作安全性的形式化验证至关重要。本文关注于伪造问题，即寻找系统中的不安全执行路径，而非证明其不存在。

Method: 1. 建立CPS的替代模型（如DNN或决策树）；2. 应用DNN伪造工具伪造CPS；3. 提出一种由决策树解释指导的新型伪造算法。

Result: 该框架（工具FlexiFal）能有效检测CPS中的难以发现的异常情况，并在ARCH-COMP 2024基准测试中高效找到多个异常案例。

Conclusion: 该方法结合DNN和决策树的优势，显著提升了CPS伪造的效率，适用于线性和非线性动态系统。

Abstract: Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as
healthcare, avionics, and autonomous vehicles. Formal verification of their
operational safety is, therefore, of utmost importance. In this paper, we
address the falsification problem, where the focus is on searching for an
unsafe execution in the system instead of proving their absence. The
contribution of this paper is a framework that (a) connects the falsification
of CPS with the falsification of deep neural networks (DNNs) and (b) leverages
the inherent interpretability of Decision Trees for faster falsification of
CPS. This is achieved by: (1) building a surrogate model of the CPS under test,
either as a DNN model or a Decision Tree, (2) application of various DNN
falsification tools to falsify CPS, and (3) a novel falsification algorithm
guided by the explanations of safety violations of the CPS model extracted from
its Decision Tree surrogate. The proposed framework has the potential to
exploit a repertoire of \emph{adversarial attack} algorithms designed to
falsify robustness properties of DNNs, as well as state-of-the-art
falsification algorithms for DNNs. Although the presented methodology is
applicable to systems that can be executed/simulated in general, we demonstrate
its effectiveness, particularly in CPS. We show that our framework, implemented
as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS
that have linear and non-linear dynamics. Decision tree-guided falsification
shows promising results in efficiently finding multiple counterexamples in the
ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.

</details>


### [216] [AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience](https://arxiv.org/abs/2505.03945)
*Shamnad Mohamed Shaffi,Sunish Vengathattil,Jezeena Nikarthil Sidhick,Resmi Vijayan*

Main category: cs.CR

TL;DR: AI enhances cloud安全通过预测分析、行为检测和智能加密，解决了传统方案的不足，但也存在隐私、偏见和合规问题，未来需要结合区块链等技术进一步提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统安全方案无法实时应对复杂威胁，AI被视为提升云安全的关键技术，能通过机器学习等主动检测和预防威胁。

Method: 应用预测分析、基于行为的威胁检测和AI驱动的加密技术，对比传统模型并分析AI的优势与挑战。

Result: AI能有效提升云安全防护，但需解决隐私、偏见和伦理问题，未来可结合区块链等技术优化框架。

Conclusion: AI是云安全的变革性工具，但需进一步研究以提高技术可靠性和合规性，同时探索与其他新兴技术的融合。

Abstract: Cloud security concerns have been greatly realized in recent years due to the
increase of complicated threats in the computing world. Many traditional
solutions do not work well in real-time to detect or prevent more complex
threats. Artificial intelligence is today regarded as a revolution in
determining a protection plan for cloud data architecture through machine
learning, statistical visualization of computing infrastructure, and detection
of security breaches followed by counteraction. These AI-enabled systems make
work easier as more network activities are scrutinized, and any anomalous
behavior that might be a precursor to a more serious breach is prevented. This
paper examines ways AI can enhance cloud security by applying predictive
analytics, behavior-based security threat detection, and AI-stirring
encryption. It also outlines the problems of the previous security models and
how AI overcomes them. For a similar reason, issues like data privacy, biases
in the AI model, and regulatory compliance are also covered. So, AI improves
the protection of cloud computing contexts; however, more efforts are needed in
the subsequent phases to extend the technology's reliability, modularity, and
ethical aspects. This means that AI can be blended with other new computing
technologies, including blockchain, to improve security frameworks further. The
paper discusses the current trends in securing cloud data architecture using AI
and presents further research and application directions.

</details>


### [217] [MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models](https://arxiv.org/abs/2505.04015)
*Soheil Zibakhsh Shabgahi,Yaman Jandali,Farinaz Koushanfar*

Main category: cs.CR

TL;DR: MergeGuard是一种新颖的方法，用于减轻AI木马攻击，通过在训练后线性化和合并全连接层，提高模型的泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: AI模型中的木马攻击导致嵌入触发器的输入被错误分类到攻击者的目标类别，这对由不可信第三方训练的模型构成了重大威胁。

Method: MergeGuard通过一种新的训练后方法，线性化和合并全连接层，以提高模型的泛化性和性能。

Result: 在Transformer模型的概念验证评估中，MergeGuard在保持模型准确性的同时，降低了木马攻击的成功率，优于常用的微调方法。

Conclusion: MergeGuard为防御AI木马攻击提供了一种有效的训练后解决方案，同时提升了模型性能。

Abstract: This paper proposes MergeGuard, a novel methodology for mitigation of AI
Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers
to be misclassified to an adversary's target class, posing a significant threat
to model usability trained by an untrusted third party. The core of MergeGuard
is a new post-training methodology for linearizing and merging fully connected
layers which we show simultaneously improves model generalizability and
performance. Our Proof of Concept evaluation on Transformer models demonstrates
that MergeGuard maintains model accuracy while decreasing trojan attack success
rate, outperforming commonly used (post-training) Trojan mitigation by
fine-tuning methodologies.

</details>


### [218] [LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling](https://arxiv.org/abs/2505.04101)
*AbdulAziz AbdulGhaffar,Ashraf Matrawy*

Main category: cs.CR

TL;DR: 该论文探讨了大语言模型（LLMs）在网络安全中的适用性，特别是在STRIDE威胁建模中的应用。通过实验，作者指出了调整和微调LLMs以适应网络安全用例的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在6G网络中应用广泛，但LLMs在网络安全的适用性研究几乎为零。为此，作者旨在填补这一空白，研究LLMs在网络安全中的潜力。

Method: 作者使用四种提示技术和五种LLMs，对5G威胁进行STRIDE分类，并分析LLMs的行为及其影响因素。

Result: 实验结果表明，LLMs在威胁建模中表现不一，揭示了其行为背后的潜在因素，并支持对LLMs进行调整和微调的必要性。

Conclusion: 该研究强调了对LLMs进行针对性调整的重要性，以更好地满足网络安全的需求，为未来研究提供了方向。

Abstract: Artificial Intelligence (AI) is expected to be an integral part of
next-generation AI-native 6G networks. With the prevalence of AI, researchers
have identified numerous use cases of AI in network security. However, there
are almost nonexistent studies that analyze the suitability of Large Language
Models (LLMs) in network security. To fill this gap, we examine the suitability
of LLMs in network security, particularly with the case study of STRIDE threat
modeling. We utilize four prompting techniques with five LLMs to perform STRIDE
classification of 5G threats. From our evaluation results, we point out key
findings and detailed insights along with the explanation of the possible
underlying factors influencing the behavior of LLMs in the modeling of certain
threats. The numerical results and the insights support the necessity for
adjusting and fine-tuning LLMs for network security use cases.

</details>


### [219] [A Comprehensive Analysis of Adversarial Attacks against Spam Filters](https://arxiv.org/abs/2505.03831)
*Esra Hotoğlu,Sevil Sen,Burcu Can*

Main category: cs.CR

TL;DR: 本文探讨了对抗攻击对基于深度学习的垃圾邮件检测系统的影响，通过真实数据集评估了六种模型，并引入了新的评分函数以提高攻击效果，揭示了垃圾邮件过滤器的漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析对抗攻击对深度学习邮件过滤系统的挑战，尤其是在垃圾邮件检测中的影响，以提升其安全性。

Method: 使用真实数据集评估六种深度学习模型，分析词语、字符、句子及AI生成段落的攻击，并引入垃圾权重和注意力权重等新评分函数。

Result: 研究全面揭示了垃圾邮件过滤器的脆弱性，并提出了改进对抗攻击防御的方法。

Conclusion: 本文为提升垃圾邮件过滤器对抗不断演变的对抗威胁的安全性提供了重要见解。

Abstract: Deep learning has revolutionized email filtering, which is critical to
protect users from cyber threats such as spam, malware, and phishing. However,
the increasing sophistication of adversarial attacks poses a significant
challenge to the effectiveness of these filters. This study investigates the
impact of adversarial attacks on deep learning-based spam detection systems
using real-world datasets. Six prominent deep learning models are evaluated on
these datasets, analyzing attacks at the word, character sentence, and
AI-generated paragraph-levels. Novel scoring functions, including spam weights
and attention weights, are introduced to improve attack effectiveness. This
comprehensive analysis sheds light on the vulnerabilities of spam filters and
contributes to efforts to improve their security against evolving adversarial
threats.

</details>


### [220] [Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper](https://arxiv.org/abs/2505.04265)
*Abdulrahman S Almuhaidib,Azlan Mohd Zain,Zalmiyah Zakaria,Izyan Izzati Kamsani,Abdulaziz S Almuhaidib*

Main category: cs.CR

TL;DR: 本文研究了大型语言模型（LLMs）在漏洞评估（VA）报告自动化验证中的应用，填补了现有研究中关于LLMs在网络安全进攻性用途的空白。


<details>
  <summary>Details</summary>
Motivation: 随着网络战日益复杂，需要新的解决方案。LLMs在防御性和进攻性网络安全策略中显示出巨大潜力，但现有研究多关注防御用途，对其在进攻性用途（如VA报告验证）的研究较少。

Method: 通过文献综述，提出了一种利用LLMs自动化分析和验证VA报告的新方法，旨在减少误报并提高效率。

Result: 研究结果表明，LLMs在自动化VA报告验证中表现良好，能提高准确性并减少人工投入，从而改善安全态势。

Conclusion: 本文进一步证明了LLMs在进攻性和防御性网络安全中的潜力，为制定更合适的网络安全策略和工具提供了依据。

Abstract: This, with the ever-increasing sophistication of cyberwar, calls for novel
solutions. In this regard, Large Language Models (LLMs) have emerged as a
highly promising tool for defensive and offensive cybersecurity-related
strategies. While existing literature has focused much on the defensive use of
LLMs, when it comes to their offensive utilization, very little has been
reported-namely, concerning Vulnerability Assessment (VA) report validation.
Consequentially, this paper tries to fill that gap by investigating the
capabilities of LLMs in automating and improving the validation process of the
report of the VA. From the critical review of the related literature, this
paper hereby proposes a new approach to using the LLMs in the automation of the
analysis and within the validation process of the report of the VA that could
potentially reduce the number of false positives and generally enhance
efficiency. These results are promising for LLM automatization for improving
validation on reports coming from VA in order to improve accuracy while
reducing human effort and security postures. The contribution of this paper
provides further evidence about the offensive and defensive LLM capabilities
and therefor helps in devising more appropriate cybersecurity strategies and
tools accordingly.

</details>


### [221] [Guardians of the Web: The Evolution and Future of Website Information Security](https://arxiv.org/abs/2505.04308)
*Md Saiful Islam,Li Xiangdong*

Main category: cs.CR

TL;DR: 本文概述了网站信息安全的历史发展、当前实践及未来趋势，强调了随着技术进步，持续的研究和创新对保护数字世界的信任至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨网站信息安全的重要性及其在不同时代的发展，以应对日益复杂的网络威胁。

Method: 通过历史回顾和技术分析，梳理了从ARPANET到现代加密技术的演变过程，并展望了未来技术如AI和区块链的影响。

Result: 指出当前多层安全措施的有效性，并强调未来需依赖新技术和国际合作以应对安全挑战。

Conclusion: 网站信息安全需持续创新和全球协作，以保护数字环境中的敏感信息和信任。

Abstract: Website information security has become a critical concern in the digital
age. This article explores the evolution of website information security,
examining its historical development, current practices, and future directions.
The early beginnings from the 1960s to the 1980s laid the groundwork for modern
cybersecurity, with the development of ARPANET, TCP/IP, public-key
cryptography, and the first antivirus programs. The 1990s marked a
transformative era, driven by the commercialization of the Internet and the
emergence of web-based services. As the Internet grew, so did the range and
sophistication of cyber threats, leading to advancements in security
technologies such as the Secure Sockets Layer (SSL) protocol, password
protection, and firewalls. Current practices in website information security
involve a multi-layered approach, including encryption, secure coding
practices, regular security audits, and user education. The future of website
information security is expected to be shaped by emerging technologies such as
artificial intelligence, blockchain, and quantum computing, as well as the
increasing importance of international cooperation and standardization efforts.
As cyber threats continue to evolve, ongoing research and innovation in website
information security will be essential to protect sensitive information and
maintain trust in the digital world.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [222] [Model-Based AI planning and Execution Systems for Robotics](https://arxiv.org/abs/2505.04493)
*Or Wertheim,Ronen I. Brafman*

Main category: cs.RO

TL;DR: 本文探讨了基于模型的规划与执行系统在机器人任务控制中的发展历程、现有解决方案及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过基于模型的系统实现机器人的多任务自动化，结合现代机器人平台推动通用系统的集成与发展。

Method: 综述现有系统的设计选择、关键问题及解决方案，并分析其优缺点。

Result: 指出ROSPlan等系统推动了机器人任务控制的通用化，但仍有优化空间。

Conclusion: 未来需进一步探索通用系统的灵活性和扩展性，以应对多样化任务需求。

Abstract: Model-based planning and execution systems offer a principled approach to
building flexible autonomous robots that can perform diverse tasks by
automatically combining a host of basic skills. This idea is almost as old as
modern robotics. Yet, while diverse general-purpose reasoning architectures
have been proposed since, general-purpose systems that are integrated with
modern robotic platforms have emerged only recently, starting with the
influential ROSPlan system. Since then, a growing number of model-based systems
for robot task-level control have emerged. In this paper, we consider the
diverse design choices and issues existing systems attempt to address, the
different solutions proposed so far, and suggest avenues for future
development.

</details>


### [223] [Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees](https://arxiv.org/abs/2505.04583)
*Nathaniel Dennler,Zhonghao Shi,Uksang Yoo,Stefanos Nikolaidis,Maja Matarić*

Main category: cs.RO

TL;DR: 提出了一种基于因果树的方法，根据用户表现动态调整康复机器人训练难度，以个性化提升康复效果和用户动机。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有用户的训练难度相同，但中风幸存者对难度感知存在个体差异，需个性化调整以优化康复效果和用户参与度。

Method: 采用因果树模型，基于用户实际表现数据动态计算训练动作的个性化难度。

Result: 该方法能准确建模难度，并为用户和护理人员提供直观的难度解释。

Conclusion: 因果树方法有效解决了康复训练中难度个性化的需求，兼具准确性与可解释性。

Abstract: Rehabilitation robots are often used in game-like interactions for
rehabilitation to increase a person's motivation to complete rehabilitation
exercises. By adjusting exercise difficulty for a specific user throughout the
exercise interaction, robots can maximize both the user's rehabilitation
outcomes and the their motivation throughout the exercise. Previous approaches
have assumed exercises have generic difficulty values that apply to all users
equally, however, we identified that stroke survivors have varied and unique
perceptions of exercise difficulty. For example, some stroke survivors found
reaching vertically more difficult than reaching farther but lower while others
found reaching farther more challenging than reaching vertically. In this
paper, we formulate a causal tree-based method to calculate exercise difficulty
based on the user's performance. We find that this approach accurately models
exercise difficulty and provides a readily interpretable model of why that
exercise is difficult for both users and caretakers.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [224] [Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions](https://arxiv.org/abs/2505.04553)
*Shanyu Han,Yang Liu,Xiang Yu*

Main category: q-fin.MF

TL;DR: 该论文提出了一种强化学习框架，基于凸评分函数定义的风险目标，解决了时间不一致性问题，并提出了一种定制化的Actor-Critic算法和辅助变量采样方法，在金融统计套利交易中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决强化学习中广泛风险目标下的时间不一致性问题，涵盖常见的风险度量如方差、预期损失等。

Method: 通过扩展状态空间和引入辅助变量，将问题转化为两阶段优化问题，并提出了定制的Actor-Critic算法及辅助变量采样方法。

Result: 理论结果表明算法无需马尔可夫决策过程的连续性，仿真实验验证了算法在金融应用中的有效性。

Conclusion: 所提框架和算法能够有效处理风险目标下的强化学习问题，并在实际应用中展示了优越性能。

Abstract: We propose a reinforcement learning (RL) framework under a broad class of
risk objectives, characterized by convex scoring functions. This class covers
many common risk measures, such as variance, Expected Shortfall, entropic
Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue,
we consider an augmented state space and an auxiliary variable and recast the
problem as a two-state optimization problem. We propose a customized
Actor-Critic algorithm and establish some theoretical approximation guarantees.
A key theoretical contribution is that our results do not require the Markov
decision process to be continuous. Additionally, we propose an auxiliary
variable sampling method inspired by the alternating minimization algorithm,
which is convergent under certain conditions. We validate our approach in
simulation experiments with a financial application in statistical arbitrage
trading, demonstrating the effectiveness of the algorithm.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [225] [The Evolution of Rough Sets 1970s-1981](https://arxiv.org/abs/2505.03747)
*Viktor Marek,Ewa Orłowska,Ivo Düntsch*

Main category: math.HO

TL;DR: 本文回顾了Zdzisław Pawlak及其合作者在20世纪70年代和1981年的研究与出版物，重点关注这些出版物中的灵感来源，并概述了1981年与粗糙集和信息系统相关的发展。


<details>
  <summary>Details</summary>
Motivation: 回顾Pawlak及其合作者的工作，以识别其中的灵感来源，并展示粗糙集和信息系统的早期发展。

Method: 通过分析Pawlak及其合作者在70年代和1981年的出版物，探讨其研究背景和灵感来源。

Result: 揭示了Pawlak研究的灵感来源，并梳理了粗糙集和信息系统在1981年的关键进展。

Conclusion: Pawlak的研究为粗糙集和信息系统奠定了基础，其灵感来源和早期发展为后续研究提供了重要参考。

Abstract: In this note research and publications by Zdzis{\l}aw Pawlak and his
collaborators from 1970s and 1981 are recalled. Focus is placed on the sources
of inspiration which one can identify on the basis of those publications.
Finally, developments from 1981 related to rough sets and information systems
are outlined.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [226] [Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach](https://arxiv.org/abs/2505.03760)
*Arishi Orra,Aryan Bhambu,Himanshu Choudhary,Manoj Thakur,Selvaraju Natarajan*

Main category: q-fin.PM

TL;DR: 该研究提出了一个基于波动性引导的深度强化学习（DRL）投资组合优化框架，结合GARCH模型对股票波动性进行分类，并根据投资者的风险偏好动态构建投资组合。在道琼斯30指数上的测试表明，该方法优于基准策略。


<details>
  <summary>Details</summary>
Motivation: 投资组合优化需要动态平衡风险与回报，而现有AI方法（如DRL）虽能提供适应性策略，但忽略了投资者对资产预筛选的偏好。因此，本研究旨在结合波动性预测和投资者风险偏好，优化投资组合策略。

Method: 使用GARCH模型预测股票波动性，并将股票按波动性分类为激进、中性和保守。随后通过DRL代理与历史市场数据交互，学习最优投资策略。

Result: 在道琼斯30指数上的实验显示，提出的投资者特定DRL投资组合能够产生更稳定的风险调整后收益，优于基准策略。

Conclusion: 波动性引导的DRL框架能够有效结合投资者风险偏好和市场动态，显著提升投资组合表现，展现了AI在金融优化中的潜力。

Abstract: Portfolio optimization requires dynamic allocation of funds by balancing the
risk and return tradeoff under dynamic market conditions. With the recent
advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in
providing adaptive and scalable strategies for portfolio optimization. However,
the success of these strategies depends not only on their ability to adapt to
market dynamics but also on the careful pre-selection of assets that influence
overall portfolio performance. Incorporating the investor's preference in
pre-selecting assets for a portfolio is essential in refining their investment
strategies. This study proposes a volatility-guided DRL-based portfolio
optimization framework that dynamically constructs portfolios based on
investors' risk profiles. The Generalized Autoregressive Conditional
Heteroscedasticity (GARCH) model is utilized for volatility forecasting of
stocks and categorizes them based on their volatility as aggressive, moderate,
and conservative. The DRL agent is then employed to learn an optimal investment
policy by interacting with the historical market data. The efficacy of the
proposed methodology is established using stocks from the Dow $30$ index. The
proposed investor-specific DRL-based portfolios outperformed the baseline
strategies by generating consistent risk-adjusted returns.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [227] [In-Context Adaptation to Concept Drift for Learned Database Operations](https://arxiv.org/abs/2505.04404)
*Jiaqi Zhu,Shaofeng Cai,Yanyan Shen,Gang Chen,Fang Deng,Beng Chin Ooi*

Main category: cs.DB

TL;DR: FLAIR is an online adaptation framework that introduces in-context adaptation for learned database operations, addressing concept drift in dynamic database environments without runtime parameter optimization.


<details>
  <summary>Details</summary>
Motivation: Dynamic database environments with frequent updates and evolving data distributions cause concept drift, degrading performance of learned models and limiting their practical use. FLAIR aims to address this by enabling adaptive learning without the overhead of retraining.

Method: FLAIR uses in-context adaptation, leveraging execution results for predictions. It includes a Task Featurization Module for standardized feature encoding and a Dynamic Decision Engine pre-trained via Bayesian meta-training for runtime adaptation.

Result: FLAIR outperforms state-of-the-art baselines, achieving up to 5.2x faster adaptation and reducing error by 22.5% for cardinality estimation.

Conclusion: FLAIR successfully addresses concept drift in learned database operations through efficient in-context adaptation, demonstrating significant performance improvements over existing methods.

Abstract: Machine learning has demonstrated transformative potential for database
operations, such as query optimization and in-database data analytics. However,
dynamic database environments, characterized by frequent updates and evolving
data distributions, introduce concept drift, which leads to performance
degradation for learned models and limits their practical applicability.
Addressing this challenge requires efficient frameworks capable of adapting to
shifting concepts while minimizing the overhead of retraining or fine-tuning.
  In this paper, we propose FLAIR, an online adaptation framework that
introduces a new paradigm called \textit{in-context adaptation} for learned
database operations. FLAIR leverages the inherent property of data systems,
i.e., immediate availability of execution results for predictions, to enable
dynamic context construction. By formalizing adaptation as $f:(\mathbf{x} \,|
\,\mathcal{C}_t) \to \mathbf{y}$, with $\mathcal{C}_t$ representing a dynamic
context memory, FLAIR delivers predictions aligned with the current concept,
eliminating the need for runtime parameter optimization. To achieve this, FLAIR
integrates two key modules: a Task Featurization Module for encoding
task-specific features into standardized representations, and a Dynamic
Decision Engine, pre-trained via Bayesian meta-training, to adapt seamlessly
using contextual information at runtime. Extensive experiments across key
database tasks demonstrate that FLAIR outperforms state-of-the-art baselines,
achieving up to 5.2x faster adaptation and reducing error by 22.5% for
cardinality estimation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [228] [Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching](https://arxiv.org/abs/2505.04603)
*Wenhui Sophia Lu,Wing Hung Wong*

Main category: stat.ME

TL;DR: 该论文提出了自适应贝叶斯推理 (ABI) 框架，通过后验空间的非参数分布匹配，显著提升高维或分散先验下的计算效率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统近似贝叶斯计算 (ABC) 在似然不可解析或计算不可行时存在高维或分散先验下的严重计算效率问题，需要改进。

Method: ABI 采用边际增强切片 Wasserstein (MSW) 距离直接比较后验分布，并通过生成密度估计迭代优化后验近似。

Result: ABI 在高维或依赖观测场景下显著优于基于数据的 Wasserstein ABC 和最先进的免似然模拟器。

Conclusion: ABI 提供了高效的后验推理框架，理论支持其收敛性，并在实验验证中表现卓越。

Abstract: When the likelihood is analytically unavailable and computationally
intractable, approximate Bayesian computation (ABC) has emerged as a widely
used methodology for approximate posterior inference; however, it suffers from
severe computational inefficiency in high-dimensional settings or under diffuse
priors. To overcome these limitations, we propose Adaptive Bayesian Inference
(ABI), a framework that bypasses traditional data-space discrepancies and
instead compares distributions directly in posterior space through
nonparametric distribution matching. By leveraging a novel Marginally-augmented
Sliced Wasserstein (MSW) distance on posterior measures and exploiting its
quantile representation, ABI transforms the challenging problem of measuring
divergence between posterior distributions into a tractable sequence of
one-dimensional conditional quantile regression tasks. Moreover, we introduce a
new adaptive rejection sampling scheme that iteratively refines the posterior
approximation by updating the proposal distribution via generative density
estimation. Theoretically, we establish parametric convergence rates for the
trimmed MSW distance and prove that the ABI posterior converges to the true
posterior as the tolerance threshold vanishes. Through extensive empirical
evaluation, we demonstrate that ABI significantly outperforms data-based
Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free
simulators, especially in high-dimensional or dependent observation regimes.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [229] [Is the end of Insight in Sight ?](https://arxiv.org/abs/2505.04627)
*Jean-Michel Tucny,Mihir Durve,Sauro Succi*

Main category: physics.comp-ph

TL;DR: 论文指出，基于物理信息神经网络（PINN）的深度学习在稀薄气体动力学问题中的权重矩阵与物理问题的数学结构无直接联系，权重接近高斯随机分布。这表明深度学习与Boltzmann方程的数值解可能是两条等价但独立的路径，Explainable AI目标可能不现实。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索深度学习在解决物理问题（如Boltzmann方程描述的稀薄气体动力学）时的内在机制，尤其是权重矩阵是否反映物理结构。

Method: 使用物理信息神经网络（PINN）应用于Boltzmann方程，分析其权重矩阵的统计特性。

Result: 权重矩阵与物理问题数学结构无直接关联，接近高斯随机分布，表明深度学习和传统数值解可能是两条独立路径。

Conclusion: 深度学习可能无法直接解释物理问题的内在结构，Explainable AI的目标可能不现实或定义不清。

Abstract: It is shown that the weight matrices of a Physics-informed neural network
(PINN)-based deep learning application to a rarefied gas dynamics problem
described by the Boltzmann equation bear no evident link to the mathematical
structure of the physical problem. Instead, the weights appear close to
Gaussian distributed random matrices. Although significantly more work is
needed to support a robust assessment in this direction, these results suggest
that deep-learning and the numerical solution of the Boltzmann equation
represent two equivalent, but largely distinct paths to the same physical
knowledge. If so, Explainable AI might be an unrealistic target and possibly
even an ill-posed one.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [230] [A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions](https://arxiv.org/abs/2505.03899)
*Danial Davarnia,Mohammadreza Kiaghadi*

Main category: math.OC

TL;DR: 该论文提出了一种基于图的新方法，用于全局解决涉及广义范数约束的优化问题，涵盖标准 ℓ_p-范数和非凸惩罚项，通过决策图和空间分支切割框架保证全局最优。


<details>
  <summary>Details</summary>
Motivation: 针对现有解决范数约束优化问题的方法（如引入二元变量或依赖特定结构）的局限性，以及非凸惩罚项难以全局优化的问题，作者旨在提出一种通用的全局优化方法。

Method: 利用决策图在原始变量空间中构建强凸松弛，避免引入辅助变量或人工界限，并结合空间分支切割框架确保全局收敛。

Result: 初步计算实验表明，该方法在具有复杂非凸惩罚项的稀疏线性回归问题上有效，优于现有全局优化技术。

Conclusion: 该方法为广义范数约束优化问题提供了一种通用的全局解决方案，尤其在处理非凸惩罚项时展现出显著优势。

Abstract: Optimization problems with norm-bounding constraints arise in a variety of
applications, including portfolio optimization, machine learning, and feature
selection. A common approach to these problems involves relaxing the norm
constraint via Lagrangian relaxation, transforming it into a regularization
term in the objective function. A particularly challenging class includes the
zero-norm function, which promotes sparsity in statistical parameter
estimation. Most existing exact methods for solving these problems introduce
binary variables and artificial bounds to reformulate them as
higher-dimensional mixed-integer programs, solvable by standard solvers. Other
exact approaches exploit specific structural properties of the objective,
making them difficult to generalize across different problem types. Alternative
methods employ nonconvex penalties with favorable statistical characteristics,
but these are typically addressed using heuristic or local optimization
techniques due to their structural complexity. In this paper, we propose a
novel graph-based method to globally solve optimization problems involving
generalized norm-bounding constraints. Our approach encompasses standard
$\ell_p$-norms for $p \in [0, \infty)$ and nonconvex penalties such as SCAD and
MCP. We leverage decision diagrams to construct strong convex relaxations
directly in the original variable space, eliminating the need for auxiliary
variables or artificial bounds. Integrated into a spatial branch-and-cut
framework, our method guarantees convergence to the global optimum. We
demonstrate its effectiveness through preliminary computational experiments on
benchmark sparse linear regression problems involving complex nonconvex
penalties, which are not tractable using existing global optimization
techniques.

</details>


### [231] [Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows](https://arxiv.org/abs/2505.04354)
*Wenhao Li,Bo Jin,Mingyi Hong,Changhong Lu,Xiangfeng Wang*

Main category: math.OC

TL;DR: 该论文提出优化问题求解可从依赖专家转向进化代理工作流，利用基础模型和进化搜索自动导航问题、算法和参数空间，通过案例展示其工业应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法依赖专家，形成瓶颈，阻碍先进方法的工业应用，需更自动化和可扩展的方案。

Method: 采用进化代理工作流，结合基础模型与进化搜索，自动探索问题定义、算法选择和超参数调优空间。

Result: 案例研究（如云资源调度和ADMM参数调优）验证了该方法的有效性，能弥合学术创新与工业实施的鸿沟。

Conclusion: 挑战以人为中心的优化现状，倡导通过自适应、可扩展的代理工作流解决现实优化问题。

Abstract: This position paper argues that optimization problem solving can transition
from expert-dependent to evolutionary agentic workflows. Traditional
optimization practices rely on human specialists for problem formulation,
algorithm selection, and hyperparameter tuning, creating bottlenecks that
impede industrial adoption of cutting-edge methods. We contend that an
evolutionary agentic workflow, powered by foundation models and evolutionary
search, can autonomously navigate the optimization space, comprising problem,
formulation, algorithm, and hyperparameter spaces. Through case studies in
cloud resource scheduling and ADMM parameter adaptation, we demonstrate how
this approach can bridge the gap between academic innovation and industrial
implementation. Our position challenges the status quo of human-centric
optimization workflows and advocates for a more scalable, adaptive approach to
solving real-world optimization problems.

</details>


### [232] [Learning based convex approximation for constrained parametric optimization](https://arxiv.org/abs/2505.04037)
*Kang Liu,Wei Peng,Jianchen Hu*

Main category: math.OC

TL;DR: 提出了一种基于输入凸神经网络(ICNN)的自监督学习框架，用于解决连续约束优化问题，结合增强拉格朗日方法和约束修正机制，确保非严格约束可行性、更优的最优性差距和最佳收敛速度。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合神经网络和传统优化方法，解决现有学习型方法在精确性、可行性和计算效率上的不平衡问题。

Method: 使用ICNN和增强拉格朗日方法(ALM)以及约束修正机制，进行自监督学习，并提供严格的收敛分析。

Result: 在多个基准任务(如二次规划、非凸规划和大规模交流最优潮流问题)上验证，显示出比现有求解器和最新学习型方法更优的平衡性能。

Conclusion: 该框架在神经网络的近似误差可控的情况下，能够收敛到原始问题的KKT点，并展现出更高的综合性能。

Abstract: We propose an input convex neural network (ICNN)-based self-supervised
learning framework to solve continuous constrained optimization problems. By
integrating the augmented Lagrangian method (ALM) with the constraint
correction mechanism, our framework ensures \emph{non-strict constraint
feasibility}, \emph{better optimality gap}, and \emph{best convergence rate}
with respect to the state-of-the-art learning-based methods. We provide a
rigorous convergence analysis, showing that the algorithm converges to a
Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal
solver is a neural network, and the approximation error is bounded. We test our
approach on a range of benchmark tasks including quadratic programming (QP),
nonconvex programming, and large-scale AC optimal power flow problems. The
results demonstrate that compared to existing solvers (e.g., \texttt{OSQP},
\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our
approach achieves a superior balance among accuracy, feasibility, and
computational efficiency.

</details>


### [233] [A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance](https://arxiv.org/abs/2505.04494)
*Axel Friedrich Wolter,Tobias Sutter*

Main category: math.OC

TL;DR: 这篇论文提出了PGDA-RL算法，结合正则化线性规划和随机逼近理论，解决了在利用离策略数据的同时保持策略探索的问题。通过异步更新的方式，算法能在单一相关数据轨迹下在线优化策略，并证明了其几乎必然收敛到最优值函数和策略。


<details>
  <summary>Details</summary>
Motivation: 为了设计一种能够利用离策略数据同时保持策略探索的强化学习算法，结合正则化线性规划和随机逼近理论，提出了PGDA-RL算法。

Method: 提出了PGDA-RL算法，通过双时间尺度的嵌套优化问题分解，结合经验回放梯度估计，以异步方式更新策略和关联MDP占用测度的对偶变量。

Result: 证明了PGDA-RL几乎必然收敛到正则化MDP的最优值函数和策略，且在更弱的假设下成立，无需模拟器或固定行为策略。

Conclusion: PGDA-RL是一种高效、理论保证的强化学习算法，适用于实际环境中的在线策略优化。

Abstract: We study reinforcement learning by combining recent advances in regularized
linear programming formulations with the classical theory of stochastic
approximation. Motivated by the challenge of designing algorithms that leverage
off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a
novel primal-dual Projected Gradient Descent-Ascent algorithm for solving
regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience
replay-based gradient estimation with a two-timescale decomposition of the
underlying nested optimization problem. The algorithm operates asynchronously,
interacts with the environment through a single trajectory of correlated data,
and updates its policy online in response to the dual variable associated with
the occupation measure of the underlying MDP. We prove that PGDA-RL converges
almost surely to the optimal value function and policy of the regularized MDP.
Our convergence analysis relies on tools from stochastic approximation theory
and holds under weaker assumptions than those required by existing primal-dual
RL approaches, notably removing the need for a simulator or a fixed behavioral
policy.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [234] [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
*Ganghua Wang,Zhaorun Chen,Bo Li,Haifeng Xu*

Main category: stat.ML

TL;DR: 论文提出了一种可认证且高效的评估框架Cer-Eval，通过自适应选择测试点来减少评估LLMs所需的测试数据量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着基座模型规模指数级增长，评估成本急剧上升，而当前方法缺乏对测试数据充分性和样本选择的系统性指导，亟需高效且可靠的评估方案。

Method: 作者提出可认证评估框架，基于理论推导的测试样本复杂度边界，开发了分区算法Cer-Eval，自适应选择测试点以最小化评估成本。

Result: 实验表明，Cer-Eval能在保持与现有方法相当的误差水平下，节省20%-40%测试样本，并提供95%置信度保证。

Conclusion: 该框架为LLM评估提供了可认证的高效解决方案，显著降低评估成本同时确保结果可靠性。

Abstract: As foundation models continue to scale, the size of trained models grows
exponentially, presenting significant challenges for their evaluation. Current
evaluation practices involve curating increasingly large datasets to assess the
performance of large language models (LLMs). However, there is a lack of
systematic analysis and guidance on determining the sufficiency of test data or
selecting informative samples for evaluation. This paper introduces a
certifiable and cost-efficient evaluation framework for LLMs. Our framework
adapts to different evaluation objectives and outputs confidence intervals that
contain true values with high probability. We use ``test sample complexity'' to
quantify the number of test points needed for a certifiable evaluation and
derive tight bounds on test sample complexity. Based on the developed theory,
we develop a partition-based algorithm, named Cer-Eval, that adaptively selects
test points to minimize the cost of LLM evaluation. Real-world experiments
demonstrate that Cer-Eval can save 20% to 40% test points across various
benchmarks, while maintaining an estimation error level comparable to the
current evaluation process and providing a 95% confidence guarantee.

</details>


### [235] [Categorical and geometric methods in statistical, manifold, and machine learning](https://arxiv.org/abs/2505.03862)
*Hông Vân Lê,Hà Quang Minh,Frederic Protin,Wilderich Tuschmann*

Main category: stat.ML

TL;DR: 该论文探讨了概率态射范畴及其几何方法在统计、机器和流形学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 旨在将概率态射范畴的理论扩展到实际问题中，尤其是在统计、机器和流形学习领域，以提供新的解决方案。

Method: 使用概率态射范畴和几何方法，应用于多种学习问题。

Result: 展示了这些理论在多个学习问题中的潜在应用价值。

Conclusion: 概率态射范畴及其几何方法为统计、机器和流形学习提供了新的研究工具和方向。

Abstract: We present and discuss applications of the category of probabilistic
morphisms, initially developed in \cite{Le2023}, as well as some geometric
methods to several classes of problems in statistical, machine and manifold
learning which shall be, along with many other topics, considered in depth in
the forthcoming book \cite{LMPT2024}.

</details>


### [236] [Variational Formulation of the Particle Flow Particle Filter](https://arxiv.org/abs/2505.04007)
*Yinzhuang Yi,Jorge Cortés,Nikolay Atanasov*

Main category: stat.ML

TL;DR: 本文从变分推断的角度提出了粒子流粒子滤波器的形式化方法，证明了推导过程中使用的瞬态密度遵循Fisher-Rao梯度流在概率密度空间中的时间缩放轨迹，并将其作为变分推断的连续时间算法。


<details>
  <summary>Details</summary>
Motivation: 通过变分推断的视角重新形式化粒子流粒子滤波器，以更清晰地理解其理论基础并优化性能。

Method: 利用Fisher-Rao梯度流在概率密度空间中的轨迹，作为连续时间变分推断算法，最小化变分密度与真实后验密度之间的Kullback-Leibler散度。

Result: 证明了粒子流粒子滤波器的瞬态密度遵循Fisher-Rao梯度流的时间缩放轨迹，验证了其作为变分推断算法的有效性。

Conclusion: 该方法为粒子流粒子滤波器提供了更坚实的理论基础，并为未来优化和扩展提供了新的视角。

Abstract: This paper provides a formulation of the particle flow particle filter from
the perspective of variational inference. We show that the transient density
used to derive the particle flow particle filter follows a time-scaled
trajectory of the Fisher-Rao gradient flow in the space of probability
densities. The Fisher-Rao gradient flow is obtained as a continuous-time
algorithm for variational inference, minimizing the Kullback-Leibler divergence
between a variational density and the true posterior density.

</details>


### [237] [A Tutorial on Discriminative Clustering and Mutual Information](https://arxiv.org/abs/2505.04484)
*Louis Ohl,Pierre-Alexandre Mattei,Frédéric Precioso*

Main category: stat.ML

TL;DR: 该论文综述了判别式聚类方法的演变，重点讨论了深度聚类方法在处理高维数据时的进展，特别是互信息的作用及其局限性，并介绍了相关Python工具GemClus的应用。


<details>
  <summary>Details</summary>
Motivation: 由于高维数据处理的需求增加，深度聚类方法在过去十年中快速发展。本文旨在提供一个历史视角，展示判别式聚类方法的演变及其假设的变迁。

Method: 通过回顾和分析判别式聚类的发展历程，尤其是从决策边界到不变性批评的假设变化，以及互信息在其中的关键作用。

Result: 总结了互信息的优势与局限性，并探讨了如何绕过这些局限。此外，还展示了GemClus工具的实用性。

Conclusion: 判别式聚类在聚类数量选择方面仍面临挑战，互信息在其中发挥了重要作用，但需进一步研究以解决其局限性。

Abstract: To cluster data is to separate samples into distinctive groups that should
ideally have some cohesive properties. Today, numerous clustering algorithms
exist, and their differences lie essentially in what can be perceived as
``cohesive properties''. Therefore, hypotheses on the nature of clusters must
be set: they can be either generative or discriminative. As the last decade
witnessed the impressive growth of deep clustering methods that involve neural
networks to handle high-dimensional data often in a discriminative manner; we
concentrate mainly on the discriminative hypotheses. In this paper, our aim is
to provide an accessible historical perspective on the evolution of
discriminative clustering methods and notably how the nature of assumptions of
the discriminative models changed over time: from decision boundaries to
invariance critics. We notably highlight how mutual information has been a
historical cornerstone of the progress of (deep) discriminative clustering
methods. We also show some known limitations of mutual information and how
discriminative clustering methods tried to circumvent those. We then discuss
the challenges that discriminative clustering faces with respect to the
selection of the number of clusters. Finally, we showcase these techniques
using the dedicated Python package, GemClus, that we have developed for
discriminative clustering.

</details>


### [238] [From Two Sample Testing to Singular Gaussian Discrimination](https://arxiv.org/abs/2505.04613)
*Leonardo V. Santoro,Kartik G. Waghmare,Victor M. Panaretos*

Main category: stat.ML

TL;DR: 本文证明了在一般可分紧度量空间上测试两个概率测度等价性，等同于在合适的再生核希尔伯特空间中测试两个对应高斯测度的奇异性。利用核均值与协方差嵌入，高斯测度的奇异性判别比非参数双样本测试更简单，尤其在维数高时。证明基于Feldman-Hajek准则，并显示差异通过高斯嵌入被显著放大。这是维度灾难的一个新例，可用于设计高效推理工具。


<details>
  <summary>Details</summary>
Motivation: 研究如何在一般可分紧度量空间中高效测试概率测度的等价性，利用高斯嵌入在再生核希尔伯特空间的奇异性判别简化问题，尤其针对高维场景。

Method: 通过核均值与协方差嵌入将概率测度转化为高斯测度，利用Feldman-Hajek准则分析奇异性，证明差异在高斯嵌入中被放大。

Result: 发现高斯嵌入显著放大概率测度间的差异，使得奇异性判别比传统非参数测试更高效，尤其在维数高时。

Conclusion: 提出了一种利用高斯嵌入从信息论角度简化概率测度等价性测试的新方法，为高维场景下的推理工具设计提供了新思路。

Abstract: We establish that testing for the equality of two probability measures on a
general separable and compact metric space is equivalent to testing for the
singularity between two corresponding Gaussian measures on a suitable
Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via
the notion of kernel mean and covariance embedding of a probability measure.
Discerning two singular Gaussians is fundamentally simpler from an
information-theoretic perspective than non-parametric two-sample testing,
particularly in high-dimensional settings. Our proof leverages the
Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert
spaces, and shows that discrepancies between distributions are heavily
magnified through their corresponding Gaussian embeddings: at a population
level, distinct probability measures lead to essentially separated Gaussian
embeddings. This appears to be a new instance of the blessing of dimensionality
that can be harnessed for the design of efficient inference tools in great
generality.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [Recognizing Ornaments in Vocal Indian Art Music with Active Annotation](https://arxiv.org/abs/2505.04419)
*Sumit Kumar,Parampreet Singh,Vipul Arora*

Main category: eess.AS

TL;DR: 该论文提出了一个名为ROD的新数据集，用于检测印度古典音乐中的装饰音，并通过深度时间序列分析模型实现了优于基线CRNN的性能。


<details>
  <summary>Details</summary>
Motivation: 音乐中的装饰音对于旋律表达至关重要，但在音乐信息检索领域缺乏注释数据集和专门建模方法，这阻碍了相关研究进展。

Method: 研究团队引入ROD数据集，并开发了一种基于深度时间序列分析的装饰音检测模型，保留了长音频分块中的装饰音边界。

Result: 实验结果表明，该方法在ROD数据集上优于基线CRNN模型，并在另一个印度古典音乐会录音数据集上也表现良好。

Conclusion: ROD数据集和提出的模型为装饰音检测研究提供了重要资源和方法，推动了该领域的进一步发展。

Abstract: Ornamentations, embellishments, or microtonal inflections are essential to
melodic expression across many musical traditions, adding depth, nuance, and
emotional impact to performances. Recognizing ornamentations in singing voices
is key to MIR, with potential applications in music pedagogy, singer
identification, genre classification, and controlled singing voice generation.
However, the lack of annotated datasets and specialized modeling approaches
remains a major obstacle for progress in this research area. In this work, we
introduce R\=aga Ornamentation Detection (ROD), a novel dataset comprising
Indian classical music recordings curated by expert musicians. The dataset is
annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked
as event-based labels. Using this dataset, we develop an ornamentation
detection model based on deep time-series analysis, preserving ornament
boundaries during the chunking of long audio recordings. We conduct experiments
using different train-test configurations within the ROD dataset and also
evaluate our approach on a separate, manually annotated dataset of Indian
classical concert recordings. Our experimental results support the superior
performance of our proposed approach over the baseline CRNN.

</details>


### [240] [Discrete Optimal Transport and Voice Conversion](https://arxiv.org/abs/2505.04382)
*Anton Selitskiy,Maitreya Kocharekar*

Main category: eess.AS

TL;DR: 本文提出了一种基于离散最优传输的语音转换方法，通过矢量接口对齐不同说话人的音频嵌入，提升语音生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换任务中音频嵌入对齐问题，提升语音生成的自然性和效果。

Method: 采用离散最优传输映射来对齐不同说话人之间的音频嵌入，并将其作为语音生成的后处理步骤。

Result: 评估结果显示该方法高质量且有效，且可导致合成音频被错误分类为真实音频。

Conclusion: 离散最优传输在语音转换中具有实际应用潜力，能显著提升生成语音的自然度。

Abstract: In this work, we address the voice conversion (VC) task using a vector-based
interface. To align audio embeddings between speakers, we employ discrete
optimal transport mapping. Our evaluation results demonstrate the high quality
and effectiveness of this method. Additionally, we show that applying discrete
optimal transport as a post-processing step in audio generation can lead to the
incorrect classification of synthetic audio as real.

</details>


### [241] [EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.04623)
*Zhenghao Xing,Xiaowei Hu,Chi-Wing Fu,Wenhai Wang,Jifeng Dai,Pheng-Ann Heng*

Main category: eess.AS

TL;DR: EchoInk-R1 是一个基于强化学习的框架，用于提升多模态大语言模型在音频和视觉信号中的结构化跨模态推理能力，并在 AVQA-R1-6K 数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在音频和视觉信号的跨模态推理上表现不足，特别是结构化推理任务。

Method: 基于 Qwen2.5-Omni-7B 和 GRPO 优化，通过强化学习框架 EchoInk-R1 处理音频-图像对的多选题任务。

Result: EchoInk-R1-7B 在验证集上达到 85.77% 准确率，优于基础模型的 80.53%，且仅需少量强化学习步骤。

Conclusion: 轻量级的强化学习微调能显著增强多模态模型的跨模态推理能力，尤其是在处理模糊多模态输入时。

Abstract: Multimodal large language models (MLLMs) have advanced perception across
text, vision, and audio, yet they often struggle with structured cross-modal
reasoning, particularly when integrating audio and visual signals. We introduce
EchoInk-R1, a reinforcement learning framework that enhances such reasoning in
MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group
Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice
question answering over synchronized audio-image pairs. To enable this, we
curate AVQA-R1-6K, a dataset pairing such audio-image inputs with
multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves
85.77% accuracy on the validation set, outperforming the base model, which
scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,
EchoInk-R1 demonstrates reflective reasoning by revisiting initial
interpretations and refining responses when facing ambiguous multimodal inputs.
These results suggest that lightweight reinforcement learning fine-tuning
enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to
unify audio, visual, and textual modalities for general open-world reasoning
via reinforcement learning. Code and data are publicly released to facilitate
further research.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [242] [An Empirical Study of OpenAI API Discussions on Stack Overflow](https://arxiv.org/abs/2505.04084)
*Xiang Chen,Jibin Wang,Chaoyang Gao,Xiaolin Ju,Zhanqi Cui*

Main category: cs.SE

TL;DR: 论文通过对2,874个Stack Overflow上关于OpenAI API讨论的实证研究，首次全面探讨了开发者在使用OpenAI API时面临的挑战，并提出了针对开发者、LLM供应商和研究者的实用建议。


<details>
  <summary>Details</summary>
Motivation: OpenAI API的使用引入了与传统API不同的独特挑战，但目前缺乏相关实证研究，本文旨在填补这一空白。

Method: 通过分析Stack Overflow上的2,874个帖子，手动分类为9类，并利用主题建模分析每类的具体挑战。

Result: 研究发现开发者在使用OpenAI API时面临多种挑战，例如提示工程复杂性、成本管理、非确定性输出等。

Conclusion: 论文提出了针对不同利益相关者的实用建议，有助于改进OpenAI API的使用和开发。

Abstract: The rapid advancement of large language models (LLMs), represented by
OpenAI's GPT series, has significantly impacted various domains such as natural
language processing, software development, education, healthcare, finance, and
scientific research. However, OpenAI APIs introduce unique challenges that
differ from traditional APIs, such as the complexities of prompt engineering,
token-based cost management, non-deterministic outputs, and operation as black
boxes. To the best of our knowledge, the challenges developers encounter when
using OpenAI APIs have not been explored in previous empirical studies. To fill
this gap, we conduct the first comprehensive empirical study by analyzing 2,874
OpenAI API-related discussions from the popular Q&A forum Stack Overflow. We
first examine the popularity and difficulty of these posts. After manually
categorizing them into nine OpenAI API-related categories, we identify specific
challenges associated with each category through topic modeling analysis. Based
on our empirical findings, we finally propose actionable implications for
developers, LLM vendors, and researchers.

</details>


### [243] [Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering](https://arxiv.org/abs/2505.04251)
*Krishna Ronanki*

Main category: cs.SE

TL;DR: 论文提出了一种基于RACI的框架，用于在人类和基于LLM的多智能体自主系统（LMA）之间实现可信任务分配，以解决软件工程中LLM驱动自动化的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的多智能体自主系统（LMA）在软件工程中的引入，如何可信地在人类与LMA系统之间分配任务成为主要挑战之一。

Method: 提出了一种基于RACI的框架，并提供了实施指南和示例实现。

Result: 该框架能够促进高效协作、确保责任明晰，并降低LLM驱动自动化的潜在风险，同时符合可信AI准则。

Conclusion: 论文为LMA系统的任务分配提供了实用框架，并计划进一步通过实证验证其效果。

Abstract: Multi-agent autonomous systems (MAS) are better at addressing challenges that
spans across multiple domains than singular autonomous agents. This holds true
within the field of software engineering (SE) as well. The state-of-the-art
research on MAS within SE focuses on integrating LLMs at the core of autonomous
agents to create LLM-based multi-agent autonomous (LMA) systems. However, the
introduction of LMA systems into SE brings a plethora of challenges. One of the
major challenges is the strategic allocation of tasks between humans and the
LMA system in a trustworthy manner. To address this challenge, a RACI-based
framework is proposed in this work in progress article, along with
implementation guidelines and an example implementation of the framework. The
proposed framework can facilitate efficient collaboration, ensure
accountability, and mitigate potential risks associated with LLM-driven
automation while aligning with the Trustworthy AI guidelines. The future steps
for this work delineating the planned empirical validation method are also
presented.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [244] [GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype](https://arxiv.org/abs/2505.03853)
*Changxi Chi,Jun Xia,Jingbo Zhou,Jiabei Cheng,Chang Yu,Stan Z. Li*

Main category: q-bio.QM

TL;DR: 该论文提出了GRAPE，一种利用预训练模型提取基因特征并首次引入基因生物型信息的异构图神经网络，用于构建基因调控网络，显著提高了预测基因扰动的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用基因相关信息，且忽略了基因生物型的功能差异，限制了基因相互作用的捕捉能力。因此，作者希望通过结合基因描述和DNA序列特征，并引入生物型信息，提升基因调控网络的构建效果。

Method: 利用预训练的大型语言模型和DNA序列模型分别提取基因描述和序列特征，结合基因生物型信息，通过异构图神经网络（HGNN）和图结构学习（GSL）动态优化基因调控网络。

Result: 在公开数据集上，GRAPE方法实现了最先进的性能，证明了其有效性。

Conclusion: 通过结合多源基因特征和生物型信息，GRAPE显著提升了基因调控网络的构建质量，为基因扰动预测提供了更精准的工具。

Abstract: Predicting genetic perturbations enables the identification of potentially
crucial genes prior to wet-lab experiments, significantly improving overall
experimental efficiency. Since genes are the foundation of cellular life,
building gene regulatory networks (GRN) is essential to understand and predict
the effects of genetic perturbations. However, current methods fail to fully
leverage gene-related information, and solely rely on simple evaluation metrics
to construct coarse-grained GRN. More importantly, they ignore functional
differences between biotypes, limiting the ability to capture potential gene
interactions. In this work, we leverage pre-trained large language model and
DNA sequence model to extract features from gene descriptions and DNA sequence
data, respectively, which serve as the initialization for gene representations.
Additionally, we introduce gene biotype information for the first time in
genetic perturbation, simulating the distinct roles of genes with different
biotypes in regulating cellular processes, while capturing implicit gene
relationships through graph structure learning (GSL). We propose GRAPE, a
heterogeneous graph neural network (HGNN) that leverages gene representations
initialized with features from descriptions and sequences, models the distinct
roles of genes with different biotypes, and dynamically refines the GRN through
GSL. The results on publicly available datasets show that our method achieves
state-of-the-art performance.

</details>


### [245] [Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning](https://arxiv.org/abs/2505.04300)
*Isabella Caranzano,Corrado Pancotti,Cesare Rollo,Flavio Sartori,Pietro Liò,Piero Fariselli,Tiziana Sanavia*

Main category: q-bio.QM

TL;DR: 研究发现，基于随机信息的神经网络模型在性能上与基于生物途径的模型相当，甚至有时表现更好，表明途径注释可能过于嘈杂或当前方法未能充分利用。


<details>
  <summary>Details</summary>
Motivation: 验证生物途径注释是否真正提升了神经网络的性能，还是仅仅因为引入了稀疏性。

Method: 对现有基于途径的神经网络模型进行全面分析，比较生物途径模型与随机化信息模型的性能。

Result: 随机化模型在性能上与生物途径模型相当（3/15情况下表现更好），且未显示生物途径模型在可解释性上的优势。

Conclusion: 生物途径注释可能噪声较大或未被充分挖掘，提出了一种系统性比较新模型的基准方法。

Abstract: Biologically-informed neural networks typically leverage pathway annotations
to enhance performance in biomedical applications. We hypothesized that the
benefits of pathway integration does not arise from its biological relevance,
but rather from the sparsity it introduces. We conducted a comprehensive
analysis of all relevant pathway-based neural network models for predictive
tasks, critically evaluating each study's contributions. From this review, we
curated a subset of methods for which the source code was publicly available.
The comparison of the biologically informed state-of-the-art deep learning
models and their randomized counterparts showed that models based on randomized
information performed equally well as biologically informed ones across
different metrics and datasets. Notably, in 3 out of the 15 analyzed models,
the randomized versions even outperformed their biologically informed
counterparts. Moreover, pathway-informed models did not show any clear
advantage in interpretability, as randomized models were still able to identify
relevant disease biomarkers despite lacking explicit pathway information. Our
findings suggest that pathway annotations may be too noisy or inadequately
explored by current methods. Therefore, we propose a methodology that can be
applied to different domains and can serve as a robust benchmark for
systematically comparing novel pathway-informed models against their randomized
counterparts. This approach enables researchers to rigorously determine whether
observed performance improvements can be attributed to biological insights.

</details>
