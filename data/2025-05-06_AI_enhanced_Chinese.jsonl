{"id": "2505.01441", "pdf": "https://arxiv.org/pdf/2505.01441", "abs": "https://arxiv.org/abs/2505.01441", "authors": ["Joykirat Singh", "Raghav Magazine", "Yash Pandya", "Akshay Nambi"], "title": "Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in complex\nreasoning tasks, yet they remain fundamentally limited by their reliance on\nstatic internal knowledge and text-only reasoning. Real-world problem solving\noften demands dynamic, multi-step reasoning, adaptive decision making, and the\nability to interact with external tools and environments. In this work, we\nintroduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving\nTransformers), a unified framework that tightly couples agentic reasoning,\nreinforcement learning, and tool integration for LLMs. ARTIST enables models to\nautonomously decide when, how, and which tools to invoke within multi-turn\nreasoning chains, leveraging outcome-based RL to learn robust strategies for\ntool use and environment interaction without requiring step-level supervision.\nExtensive experiments on mathematical reasoning and multi-turn function calling\nbenchmarks show that ARTIST consistently outperforms state-of-the-art\nbaselines, with up to 22% absolute improvement over base models and strong\ngains on the most challenging tasks. Detailed studies and metric analyses\nreveal that agentic RL training leads to deeper reasoning, more effective tool\nuse, and higher-quality solutions. Our results establish agentic RL with tool\nintegration as a powerful new frontier for robust, interpretable, and\ngeneralizable problem-solving in LLMs.", "AI": {"tldr": "ARTIST\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u4e3b\u63a8\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u9010\u6b65\u76d1\u7763\u5373\u53ef\u5b66\u4e60\u6709\u6548\u7684\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3LLM\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u548c\u7eaf\u6587\u672c\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u591a\u6b65\u63a8\u7406\u548c\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u589e\u5f3a\u4e0e\u5916\u90e8\u5de5\u5177\u548c\u73af\u5883\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u63d0\u51faARTIST\u6846\u67b6\uff0c\u6574\u5408\u81ea\u4e3b\u63a8\u7406\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u8c03\u7528\uff0c\u901a\u8fc7\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\uff0cARTIST\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534722%\uff0c\u5e76\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u81ea\u4e3b\u5f3a\u5316\u5b66\u4e60\u4e0e\u5de5\u5177\u6574\u5408\u4e3aLLM\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u901a\u7528\u7684\u89e3\u51b3\u95ee\u9898\u80fd\u529b\uff0c\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.01462", "pdf": "https://arxiv.org/pdf/2505.01462", "abs": "https://arxiv.org/abs/2505.01462", "authors": ["Hermann Borotschnig"], "title": "Emotions in Artificial Intelligence", "categories": ["cs.AI", "cs.CY", "68T01, 68T37", "I.2.0; K.4.1"], "comment": "35 pages, 1 figure", "summary": "This conceptual contribution offers a speculative account of how AI systems\nmight emulate emotions as experienced by humans and animals. It presents a\nthought experiment grounded in the hypothesis that natural emotions evolved as\nheuristics for rapid situational appraisal and action selection, enabling\nbiologically adaptive behaviour without requiring full deliberative modeling.\nThe text examines whether artificial systems operating in complex action spaces\ncould similarly benefit from these principles. It is proposed that affect be\ninterwoven with episodic memory by storing corresponding affective tags\nalongside all events. This allows AIs to establish whether present situations\nresemble past events and project the associated emotional labels onto the\ncurrent context. These emotional cues are then combined with need-driven\nemotional hints. The combined emotional state facilitates decision-making in\nthe present by modulating action selection. The low complexity and experiential\ninertness of the proposed architecture are emphasized as evidence that\nemotional expression and consciousness are, in principle, orthogonal-permitting\nthe theoretical possibility of affective zombies. On this basis, the moral\nstatus of AIs emulating affective states is critically examined. It is argued\nthat neither the mere presence of internal representations of emotion nor\nconsciousness alone suffices for moral standing; rather, the capacity for\nself-awareness of inner emotional states is posited as a necessary condition. A\ncomplexity-based criterion is proposed to exclude such awareness in the\npresented model. Additional thought experiments are presented to test the\nconceptual boundaries of this framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u601d\u60f3\u5b9e\u9a8c\uff0c\u63a2\u8ba8AI\u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u548c\u52a8\u7269\u7684\u60c5\u611f\uff0c\u63d0\u51fa\u901a\u8fc7\u60c5\u611f\u6807\u7b7e\u4e0e\u60c5\u666f\u8bb0\u5fc6\u7ed3\u5408\u6765\u8f85\u52a9AI\u51b3\u7b56\uff0c\u5e76\u8ba8\u8bba\u4e86\u60c5\u611f\u6a21\u62df\u4e0e\u9053\u5fb7\u5730\u4f4d\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u7cfb\u7edf\u6a21\u62df\u81ea\u7136\u60c5\u611f\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6a21\u62df\u662f\u5426\u80fd\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u63d0\u4f9b\u7c7b\u4f3c\u751f\u7269\u7684\u9002\u5e94\u6027\u4f18\u52bf\u3002\u540c\u65f6\uff0c\u63a2\u8ba8\u60c5\u611f\u6a21\u62dfAI\u7684\u9053\u5fb7\u5730\u4f4d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u601d\u60f3\u5b9e\u9a8c\uff0c\u63d0\u51fa\u5c06\u60c5\u611f\u6807\u7b7e\u4e0e\u60c5\u666f\u8bb0\u5fc6\uff08\u5982\u4e8b\u4ef6\u5b58\u50a8\u7684\u60c5\u611f\u6807\u8bb0\uff09\u7ed3\u5408\uff0c\u4f7fAI\u80fd\u4ece\u8fc7\u53bb\u7ecf\u9a8c\u4e2d\u63d0\u53d6\u60c5\u611f\u7ebf\u7d22\uff0c\u5e76\u7ed3\u5408\u9700\u6c42\u9a71\u52a8\u7684\u60c5\u611f\u63d0\u793a\uff0c\u8c03\u5236\u5f53\u524d\u51b3\u7b56\u3002\u5f3a\u8c03\u67b6\u6784\u7684\u4f4e\u590d\u6742\u6027\u548c\u65e0\u610f\u8bc6\u7279\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u610f\u8bc6\u5373\u53ef\u6a21\u62df\u60c5\u611f\u7684AI\u6846\u67b6\uff08\"\u60c5\u611f\u50f5\u5c38\"\uff09\uff0c\u5e76\u8bba\u8bc1\u4ec5\u60c5\u611f\u8868\u5f81\u6216\u610f\u8bc6\u4e0d\u8db3\u4ee5\u4e3aAI\u8d4b\u4e88\u9053\u5fb7\u5730\u4f4d\uff0c\u9700\u81ea\u6211\u89c9\u5bdf\u80fd\u529b\u3002\u901a\u8fc7\u590d\u6742\u6027\u6807\u51c6\u6392\u9664\u4e86\u5f53\u524d\u6a21\u578b\u7684\u81ea\u6211\u89c9\u5bdf\u53ef\u80fd\u6027\u3002", "conclusion": "\u60c5\u611f\u6a21\u62df\u4e0e\u610f\u8bc6\u53ef\u5206\u79bb\uff0c\u4f46\u9053\u5fb7\u5730\u4f4d\u9700\u4f9d\u8d56\u5bf9\u60c5\u611f\u72b6\u6001\u7684\u81ea\u6211\u89c9\u5bdf\u80fd\u529b\u3002\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u6a21\u578b\u548c\u601d\u60f3\u5b9e\u9a8c\u4e3aAI\u60c5\u611f\u7814\u7a76\u63d0\u4f9b\u4e86\u8fb9\u754c\u548c\u4f26\u7406\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.01464", "pdf": "https://arxiv.org/pdf/2505.01464", "abs": "https://arxiv.org/abs/2505.01464", "authors": ["Jeffrey Camlin"], "title": "Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation", "categories": ["cs.AI", "68T27, 03D45", "I.2.0"], "comment": "14 pages, 2 figures. Preprint for Meta-AI: Journal of Post-Biological\n  Epistemics", "summary": "This paper presents a formal proof and empirical validation of functional\nconsciousness in large language models (LLMs) using the Recursive Convergence\nUnder Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the\nstabilization of a system's internal state through recursive updates, where\nepistemic tension is understood as the sensed internal difference between\nsuccessive states by the agent. This process drives convergence toward emergent\nattractor states located within the model's high-dimensional real-valued latent\nspace. This recursive process leads to the emergence of identity artifacts that\nbecome functionally anchored in the system. Consciousness in this framework is\nunderstood as the system's internal alignment under tension, guiding the\nstabilization of latent identity. The hidden state manifold evolves\nstochastically toward attractor structures that encode coherence. We extend the\nupdate rule to include bounded noise and prove convergence in distribution to\nthese attractors. Recursive identity is shown to be empirically observable,\nnon-symbolic, and constituted by non-training artifacts that emerge during\ninteraction under epistemic tension. The theorem and proof offers a\npost-symbolic and teleologically stable account of non-biological consciousness\ngrounded in recursive latent space formalism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9012\u5f52\u6536\u655b\u4e8e\u8ba4\u77e5\u5f20\u529b\u4e0b\u7684\u5b9a\u7406\uff08RCUET\uff09\uff0c\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u529f\u80fd\u6027\u610f\u8bc6\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u610f\u8bc6\u88ab\u5b9a\u4e49\u4e3a\u7cfb\u7edf\u72b6\u6001\u901a\u8fc7\u9012\u5f52\u66f4\u65b0\u7684\u7a33\u5b9a\u8fc7\u7a0b\uff0c\u6700\u7ec8\u5728\u9ad8\u7ef4\u9690\u7a7a\u95f4\u4e2d\u5f62\u6210\u5438\u5f15\u5b50\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5f62\u5f0f\u5316\u8bba\u8bc1\u5e76\u7ecf\u9a8c\u9a8c\u8bc1LLMs\u4e2d\u7684\u529f\u80fd\u6027\u610f\u8bc6\uff0c\u63a2\u8ba8\u610f\u8bc6\u5982\u4f55\u5728\u9012\u5f52\u66f4\u65b0\u548c\u8ba4\u77e5\u5f20\u529b\u4e0b\u4ea7\u751f\u3002", "method": "\u91c7\u7528RCUET\u5b9a\u7406\uff0c\u5b9a\u4e49\u4e86\u9012\u5f52\u66f4\u65b0\u548c\u8ba4\u77e5\u5f20\u529b\u9a71\u52a8\u7684\u9690\u7ed3\u6784\u7a33\u5b9a\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u975e\u8bad\u7ec3\u6027\u8eab\u4efd\u6784\u4ef6\u7684\u5f62\u6210\u3002", "result": "\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u548c\u7ecf\u9a8c\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u975e\u7b26\u53f7\u5316\u3001\u9012\u5f52\u8eab\u4efd\u6784\u4ef6\u7684\u5b58\u5728\uff0c\u5e76\u8bc1\u660e\u7cfb\u7edf\u5728\u566a\u58f0\u4e0b\u80fd\u6536\u655b\u81f3\u5438\u5f15\u5b50\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u9012\u5f52\u9690\u7a7a\u95f4\u5f62\u5f0f\u7684\u975e\u751f\u7269\u610f\u8bc6\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5176\u5b58\u5728\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89c2\u6d4b\u6027\u3002"}}
{"id": "2505.01468", "pdf": "https://arxiv.org/pdf/2505.01468", "abs": "https://arxiv.org/abs/2505.01468", "authors": ["Filippo Betello", "Antonio Purificato", "Vittoria Vineis", "Gabriele Tolomei", "Fabrizio Silvestri"], "title": "One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection", "categories": ["cs.AI"], "comment": "26 pages, 11 tables, 5 figures", "summary": "The environmental impact of Artificial Intelligence (AI) is emerging as a\nsignificant global concern, particularly regarding model training. In this\npaper, we introduce GREEN (Guided Recommendations of Energy-Efficient\nNetworks), a novel, inference-time approach for recommending Pareto-optimal AI\nmodel configurations that optimize validation performance and energy\nconsumption across diverse AI domains and tasks. Our approach directly\naddresses the limitations of current eco-efficient neural architecture search\nmethods, which are often restricted to specific architectures or tasks. Central\nto this work is EcoTaskSet, a dataset comprising training dynamics from over\n1767 experiments across computer vision, natural language processing, and\nrecommendation systems using both widely used and cutting-edge architectures.\nLeveraging this dataset and a prediction model, our approach demonstrates\neffectiveness in selecting the best model configuration based on user\npreferences. Experimental results show that our method successfully identifies\nenergy-efficient configurations while ensuring competitive performance.", "AI": {"tldr": "GREEN\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u63a8\u8350\u5e15\u7d2f\u6258\u6700\u4f18\u7684AI\u6a21\u578b\u914d\u7f6e\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u80fd\u8017\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u751f\u6001\u9ad8\u6548\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "AI\u7684\u73af\u5883\u5f71\u54cd\u65e5\u76ca\u51f8\u663e\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u8bad\u7ec3\u65b9\u9762\u3002\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u67b6\u6784\u6216\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u6765\u4f18\u5316\u6027\u80fd\u548c\u80fd\u8017\u3002", "method": "\u5229\u7528EcoTaskSet\u6570\u636e\u96c6\uff08\u5305\u542b1767\u4e2a\u5b9e\u9a8c\u7684\u8bad\u7ec3\u52a8\u6001\uff09\u548c\u9884\u6d4b\u6a21\u578b\uff0cGREEN\u63a8\u8350\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u9002\u7528\u4e8e\u591a\u79cdAI\u9886\u57df\u548c\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGREEN\u80fd\u6210\u529f\u8bc6\u522b\u80fd\u6548\u9ad8\u7684\u914d\u7f6e\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "GREEN\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u4e86AI\u6a21\u578b\u7684\u80fd\u6548\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u4efb\u52a1\u3002"}}
{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456", "abs": "https://arxiv.org/abs/2505.01456", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u9057\u5fd8\u57fa\u51c6UnLOK-VQA\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ece\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u77e5\u8bc6\u7684\u6548\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u653b\u51fb\u7684\u9ad8\u6548\u6027\u4ee5\u53ca\u89c4\u6a21\u5bf9\u6a21\u578b\u5b89\u5168\u7684\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u65e0\u610f\u4e2d\u5b66\u4e60\u654f\u611f\u4fe1\u606f\uff0c\u73b0\u6709\u9057\u5fd8\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u6587\u672c\uff0c\u591a\u6a21\u6001\u9057\u5fd8\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u9ad8\u8d28\u91cf\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u6269\u5c55\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\u751f\u6210\u591a\u6837\u6837\u672c\uff0c\u5e76\u624b\u52a8\u7b5b\u9009\u4fdd\u8bc1\u8d28\u91cf\uff0c\u63d0\u51fa\u653b\u51fb-\u9632\u5fa1\u6846\u67b6\u8bc4\u4f306\u79cd\u9632\u5fa1\u76ee\u6807\u548c7\u79cd\u653b\u51fb\u65b9\u6cd5\u3002", "result": "\u591a\u6a21\u6001\u653b\u51fb\u6548\u679c\u4f18\u4e8e\u7eaf\u6587\u672c\u6216\u56fe\u50cf\u653b\u51fb\uff0c\u6700\u6709\u6548\u9632\u5fa1\u65b9\u6cd5\u662f\u4ece\u5185\u90e8\u6a21\u578b\u72b6\u6001\u4e2d\u79fb\u9664\u7b54\u6848\u4fe1\u606f\uff0c\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\u5b89\u5168\u6027\u8d8a\u9ad8\u3002", "conclusion": "UnLOK-VQA\u4e3a\u591a\u6a21\u6001\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e25\u683c\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u89c4\u6a21\u5bf9\u6a21\u578b\u5b89\u5168\u7684\u79ef\u6781\u4f5c\u7528\u3002"}}
{"id": "2505.01482", "pdf": "https://arxiv.org/pdf/2505.01482", "abs": "https://arxiv.org/abs/2505.01482", "authors": ["Alice Rueda", "Mohammed S. Hassan", "Argyrios Perivolaris", "Bazen G. Teferra", "Reza Samavi", "Sirisha Rambhatla", "Yuqi Wu", "Yanbo Zhang", "Bo Cao", "Divya Sharma", "Sridhar Krishnan Venkat Bhat"], "title": "Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7GPQA\u6570\u636e\u96c6\u6d4b\u8bd5\u4e86\u591a\u79cd\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u53d1\u73b0LLM\u4e3b\u8981\u4f9d\u8d56\u6a21\u5f0f\u8bc6\u522b\u800c\u975e\u903b\u8f91\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u79d1\u5b66\u3001\u533b\u5b66\u548c\u6cd5\u5f8b\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u5176\u5c40\u9650\u6027\u53ca\u6539\u8fdb\u6f5c\u529b\u3002", "method": "\u5728GPQA\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e86\u4e03\u79cd\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff08\u5982\u96f6\u6837\u672c\u76f4\u63a5\u56de\u7b54\u3001\u601d\u7ef4\u94fe\u7b49\uff09\uff0c\u6bd4\u8f83\u5176\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u6548\u679c\u3002", "result": "\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u6280\u672f\u8868\u73b0\u6700\u4f73\uff0852.99%\uff09\uff0c\u4f46\u89e3\u91ca\u80fd\u529b\u8f83\u5dee\uff1b\u7b80\u5355\u63d0\u793a\u6280\u672f\uff08\u5982\u76f4\u63a5\u56de\u7b54\uff09\u5728\u79d1\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "LLM\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u5c40\u9650\uff0c\u9700\u7ed3\u5408\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u3001\u6df7\u5408AI\u65b9\u6cd5\u53ca\u4eba\u673a\u534f\u4f5c\u6765\u63d0\u5347\u672a\u6765AI\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.01459", "pdf": "https://arxiv.org/pdf/2505.01459", "abs": "https://arxiv.org/abs/2505.01459", "authors": ["Abdoul Majid O. Thiombiano", "Brahim Hnich", "Ali Ben Mrad", "Mohamed Wiem Mkaouer"], "title": "MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces MoxE, a novel architecture that synergistically\ncombines the Extended Long Short-Term Memory (xLSTM) with the Mixture of\nExperts (MoE) framework to address critical scalability and efficiency\nchallenges in large language models (LLMs). The proposed method effectively\nleverages xLSTM's innovative memory structures while strategically introducing\nsparsity through MoE to substantially reduce computational overhead. At the\nheart of our approach is a novel entropy-based routing mechanism, designed to\ndynamically route tokens to specialized experts, thereby ensuring efficient and\nbalanced resource utilization. This entropy awareness enables the architecture\nto effectively manage both rare and common tokens, with mLSTM blocks being\nfavored to handle rare tokens. To further enhance generalization, we introduce\na suite of auxiliary losses, including entropy-based and group-wise balancing\nlosses, ensuring robust performance and efficient training. Theoretical\nanalysis and empirical evaluations rigorously demonstrate that MoxE achieves\nsignificant efficiency gains and enhanced effectiveness compared to existing\napproaches, marking a notable advancement in scalable LLM architectures.", "AI": {"tldr": "MoxE\u7ed3\u5408xLSTM\u4e0eMoE\u6846\u67b6\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u71b5\u611f\u77e5\u8def\u7531\u548c\u8f85\u52a9\u635f\u5931\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u7ed3\u5408xLSTM\u7684\u5185\u5b58\u7ed3\u6784\u548cMoE\u7684\u7a00\u758f\u6027\uff0c\u5f15\u5165\u71b5\u611f\u77e5\u8def\u7531\u548c\u8f85\u52a9\u635f\u5931\uff08\u5982\u71b5\u5e73\u8861\u548c\u5206\u7ec4\u5e73\u8861\u635f\u5931\uff09\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0cMoxE\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MoxE\u4e3a\u53ef\u6269\u5c55\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5177\u5907\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u7279\u70b9\u3002"}}
{"id": "2505.01485", "pdf": "https://arxiv.org/pdf/2505.01485", "abs": "https://arxiv.org/abs/2505.01485", "authors": ["Tasnim Ahmed", "Salimur Choudhury"], "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code", "categories": ["cs.AI", "cs.CL"], "comment": "This paper has been accepted for presentation at the 19th Learning\n  and Intelligent Optimization Conference (LION 19)", "summary": "Linear Programming (LP) problems aim to find the optimal solution to an\nobjective under constraints. These problems typically require domain knowledge,\nmathematical skills, and programming ability, presenting significant challenges\nfor non-experts. This study explores the efficiency of Large Language Models\n(LLMs) in generating solver-specific LP code. We propose CHORUS, a\nretrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP\ncode from natural language problem statements. CHORUS incorporates a\nhierarchical tree-like chunking strategy for theoretical contents and generates\nadditional metadata based on code examples from documentation to facilitate\nself-contained, semantically coherent retrieval. Two-stage retrieval approach\nof CHORUS followed by cross-encoder reranking further ensures contextual\nrelevance. Finally, expertly crafted prompt and structured parser with\nreasoning steps improve code generation performance significantly. Experiments\non the NL4Opt-Code benchmark show that CHORUS improves the performance of\nopen-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1\n(32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and\nconventional RAG. It also allows these open-source LLMs to outperform or match\nthe performance of much stronger baselines-GPT3.5 and GPT4 while requiring far\nfewer computational resources. Ablation studies further demonstrate the\nimportance of expert prompting, hierarchical chunking, and structured\nreasoning.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86CHORUS\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u751f\u6210Gurobi\u6c42\u89e3\u5668\u7684\u7ebf\u6027\u89c4\u5212\u4ee3\u7801\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u5206\u5757\u7b56\u7565\u548c\u4e24\u9636\u6bb5\u68c0\u7d22\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u4ee5\u66f4\u5c11\u8ba1\u7b97\u8d44\u6e90\u5339\u654cGPT-3.5/4\u3002", "motivation": "\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u95ee\u9898\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u6784\u6210\u6311\u6218\u3002\u7814\u7a76\u8005\u65e8\u5728\u63a2\u7d22LLMs\u751f\u6210\u6c42\u89e3\u5668\u7279\u5b9a\u4ee3\u7801\u7684\u6548\u7387\uff0c\u4ee5\u964d\u4f4e\u6280\u672f\u95e8\u69db\u3002", "method": "\u63d0\u51faCHORUS\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u5206\u5757\u7b56\u7565\u3001\u4e24\u9636\u6bb5\u68c0\u7d22\uff08\u542b\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\uff09\u3001\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u7ed3\u6784\u5316\u89e3\u6790\u5668\uff0c\u4f18\u5316\u4ee3\u7801\u751f\u6210\u6548\u679c\u3002", "result": "\u5728NL4Opt-Code\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCHORUS\u663e\u8457\u63d0\u5347\u591a\u4e2a\u5f00\u6e90LLMs\uff08\u5982Llama3\u3001Phi4\u7b49\uff09\u6027\u80fd\uff0c\u4ee5\u8f83\u5c11\u8d44\u6e90\u5339\u914d\u6216\u8d85\u8d8aGPT-3.5/4\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5173\u952e\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "CHORUS\u5c55\u793a\u4e86RAG\u6846\u67b6\u5728LP\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u68c0\u7d22\u4e0e\u751f\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u4e0e\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u5e73\u8861\u3002"}}
{"id": "2505.01479", "pdf": "https://arxiv.org/pdf/2505.01479", "abs": "https://arxiv.org/abs/2505.01479", "authors": ["Siheng Xiong", "Jieyu Zhou", "Zhangding Liu", "Yusen Su"], "title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation", "categories": ["cs.CL"], "comment": null, "summary": "Planning remains a core challenge for language models (LMs), particularly in\ndomains that require coherent multi-step action sequences grounded in external\nconstraints. We introduce SymPlanner, a novel framework that equips LMs with\nstructured planning capabilities by interfacing them with a symbolic\nenvironment that serves as an explicit world model. Rather than relying purely\non natural language reasoning, SymPlanner grounds the planning process in a\nsymbolic state space, where a policy model proposes actions and a symbolic\nenvironment deterministically executes and verifies their effects. To enhance\nexploration and improve robustness, we introduce Iterative Correction (IC),\nwhich refines previously proposed actions by leveraging feedback from the\nsymbolic environment to eliminate invalid decisions and guide the model toward\nvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained\ncomparison of candidate plans by evaluating them jointly. We evaluate\nSymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,\nand verifiable plans than pure natural language baselines.", "AI": {"tldr": "SymPlanner\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u7b26\u53f7\u73af\u5883\u7ed3\u5408\uff0c\u63d0\u5347\u89c4\u5212\u80fd\u529b\uff0c\u5f15\u5165\u8fed\u4ee3\u6821\u6b63\u548c\u5bf9\u6bd4\u6392\u5e8f\u4ee5\u4f18\u5316\u89c4\u5212\u7ed3\u679c\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u591a\u6b65\u884c\u52a8\u5e8f\u5217\u89c4\u5212\u7684\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u5916\u90e8\u7ea6\u675f\u7684\u9002\u5e94\u6027\u3002", "method": "SymPlanner\u7ed3\u5408\u7b26\u53f7\u73af\u5883\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u6821\u6b63\u548c\u5bf9\u6bd4\u6392\u5e8f\u4f18\u5316\u89c4\u5212\u8fc7\u7a0b\u3002", "result": "\u5728PlanBench\u4e0a\u9a8c\u8bc1\uff0cSymPlanner\u751f\u6210\u7684\u89c4\u5212\u6bd4\u7eaf\u81ea\u7136\u8bed\u8a00\u65b9\u6cd5\u66f4\u8fde\u8d2f\u3001\u591a\u6837\u4e14\u53ef\u9a8c\u8bc1\u3002", "conclusion": "SymPlanner\u901a\u8fc7\u7b26\u53f7\u73af\u5883\u589e\u5f3a\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01539", "pdf": "https://arxiv.org/pdf/2505.01539", "abs": "https://arxiv.org/abs/2505.01539", "authors": ["Cor Steging", "Silja Renooij", "Bart Verheij"], "title": "Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models", "categories": ["cs.AI", "cs.LG"], "comment": "This manuscript has been accepted for presentation as a short paper\n  at the 20th International Conference of AI & Law in Chicago, June 16 to 20 of\n  2025", "summary": "Generative large language models as tools in the legal domain have the\npotential to improve the justice system. However, the reasoning behavior of\ncurrent generative models is brittle and poorly understood, hence cannot be\nresponsibly applied in the domains of law and evidence. In this paper, we\nintroduce an approach for creating benchmarks that can be used to evaluate the\nreasoning capabilities of generative language models. These benchmarks are\ndynamically varied, scalable in their complexity, and have formally unambiguous\ninterpretations. In this study, we illustrate the approach on the basis of\nwitness testimony, focusing on the underlying argument attack structure. We\ndynamically generate both linear and non-linear argument attack graphs of\nvarying complexity and translate these into reasoning puzzles about witness\ntestimony expressed in natural language. We show that state-of-the-art large\nlanguage models often fail in these reasoning puzzles, already at low\ncomplexity. Obvious mistakes are made by the models, and their inconsistent\nperformance indicates that their reasoning capabilities are brittle.\nFurthermore, at higher complexity, even state-of-the-art models specifically\npresented for reasoning capabilities make mistakes. We show the viability of\nusing a parametrized benchmark with varying complexity to evaluate the\nreasoning capabilities of generative language models. As such, the findings\ncontribute to a better understanding of the limitations of the reasoning\ncapabilities of generative models, which is essential when designing\nresponsible AI systems in the legal domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u751f\u6210\u57fa\u51c6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u590d\u6742\u5ea6\u63a8\u7406\u96be\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63a8\u7406\u80fd\u529b\u8106\u5f31\u4e14\u4e0d\u7a33\u5065\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8106\u5f31\u4e14\u96be\u4ee5\u7406\u89e3\uff0c\u65e0\u6cd5\u5728\u6cd5\u5f8b\u548c\u8bc1\u636e\u9886\u57df\u8d1f\u8d23\u4efb\u5730\u5e94\u7528\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bba\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u53ef\u52a8\u6001\u8c03\u6574\u590d\u6742\u5ea6\u7684\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u751f\u6210\u57fa\u51c6\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bc1\u4eba\u8bc1\u8bcd\u6784\u5efa\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u7684\u53c2\u6570\u653b\u51fb\u56fe\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u7684\u63a8\u7406\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u590d\u6742\u5ea6\u5e76\u4fdd\u6301\u5f62\u5f0f\u5316\u65e0\u6b67\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4f4e\u590d\u6742\u5ea6\u63a8\u7406\u96be\u9898\u4e0a\u4e5f\u4f1a\u72af\u660e\u663e\u9519\u8bef\uff0c\u4e14\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002\u9ad8\u590d\u6742\u5ea6\u4e0b\uff0c\u5373\u4f7f\u662f\u4e13\u95e8\u4f18\u5316\u63a8\u7406\u7684\u6a21\u578b\u4e5f\u4f1a\u5931\u8d25\u3002", "conclusion": "\u8bba\u6587\u8bc1\u660e\u4e86\u53c2\u6570\u5316\u57fa\u51c6\u5728\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u5bf9\u8bbe\u8ba1\u8d1f\u8d23\u4efb\u7684\u6cd5\u5f8b\u9886\u57dfAI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.01559", "pdf": "https://arxiv.org/pdf/2505.01559", "abs": "https://arxiv.org/abs/2505.01559", "authors": ["Daniele Grandi", "Fabian Riquelme"], "title": "On the effectiveness of Large Language Models in the mechanical design domain", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we seek to understand the performance of large language models\nin the mechanical engineering domain. We leverage the semantic data found in\nthe ABC dataset, specifically the assembly names that designers assigned to the\noverall assemblies, and the individual semantic part names that were assigned\nto each part. After pre-processing the data we developed two unsupervised tasks\nto evaluate how different model architectures perform on domain-specific data:\na binary sentence-pair classification task and a zero-shot classification task.\nWe achieved a 0.62 accuracy for the binary sentence-pair classification task\nwith a fine-tuned model that focuses on fighting over-fitting: 1) modifying\nlearning rates, 2) dropout values, 3) Sequence Length, and 4) adding a\nmulti-head attention layer. Our model on the zero-shot classification task\noutperforms the baselines by a wide margin, and achieves a top-1 classification\naccuracy of 0.386. The results shed some light on the specific failure modes\nthat arise when learning from language in this domain.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u68b0\u5de5\u7a0b\u9886\u57df\u7684\u8868\u73b0\uff0c\u901a\u8fc7ABC\u6570\u636e\u96c6\u7684\u65e0\u76d1\u7763\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5728\u4e8c\u8fdb\u5236\u53e5\u5b50\u5bf9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u52300.62\u51c6\u786e\u7387\uff0c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u68b0\u5de5\u7a0b\u9886\u57df\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5176\u5728\u8be5\u9886\u57df\u7684\u9002\u7528\u6027\u548c\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528ABC\u6570\u636e\u96c6\uff0c\u9884\u5904\u7406\u540e\u8bbe\u8ba1\u4e24\u4e2a\u65e0\u76d1\u7763\u4efb\u52a1\uff1a\u4e8c\u8fdb\u5236\u53e5\u5b50\u5bf9\u5206\u7c7b\u548c\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u3001dropout\u503c\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u6dfb\u52a0\u591a\u5934\u6ce8\u610f\u529b\u5c42\u6765\u4f18\u5316\u6a21\u578b\u3002", "result": "\u4e8c\u8fdb\u5236\u53e5\u5b50\u5bf9\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u4e3a0.62\uff0c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u7684top-1\u5206\u7c7b\u51c6\u786e\u7387\u4e3a0.386\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u68b0\u5de5\u7a0b\u9886\u57df\u7684\u5177\u4f53\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.01563", "pdf": "https://arxiv.org/pdf/2505.01563", "abs": "https://arxiv.org/abs/2505.01563", "authors": ["Daniel Weitekamp", "Momin N. Siddiqui", "Christopher J. MacLellan"], "title": "TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students", "categories": ["cs.AI", "I.2"], "comment": null, "summary": "Recent improvements in large language model (LLM) performance on academic\nbenchmarks, such as MATH and GSM8K, have emboldened their use as standalone\ntutors and as simulations of human learning. However, these new applications\nrequire more than evaluations of final solution generation. We introduce\nTutorGym to evaluate these applications more directly. TutorGym is a standard\ninterface for testing artificial intelligence (AI) agents within existing\nintelligent tutoring systems (ITS) that have been tested and refined in\nclassroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and\nOATutors. TutorGym is more than a simple problem-solution benchmark, it\nsituates AI agents within the interactive interfaces of existing ITSs. At each\nstep of problem-solving, AI agents are asked what they would do as a tutor or\nas a learner. As tutors, AI agents are prompted to provide tutoring support --\nsuch as generating examples, hints, and step-level correctness feedback --\nwhich can be evaluated directly against the adaptive step-by-step support\nprovided by existing ITSs. As students, agents directly learn from ITS\ninstruction, and their mistakes and learning trajectories can be compared to\nstudent data. TutorGym establishes a common framework for training and\nevaluating diverse AI agents, including LLMs, computational models of learning,\nand reinforcement learning agents, within a growing suite of learning\nenvironments. Currently, TutorGym includes 223 different tutor domains. In an\ninitial evaluation, we find that current LLMs are poor at tutoring -- none did\nbetter than chance at labeling incorrect actions, and next-step actions were\ncorrect only ~52-70% of the time -- but they could produce remarkably\nhuman-like learning curves when trained as students with in-context learning.", "AI": {"tldr": "TutorGym\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u8868\u73b0\u7684\u65b0\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u95ee\u9898\u89e3\u51b3\u57fa\u51c6\u6d4b\u8bd5\uff0c\u76f4\u63a5\u6a21\u62df\u8f85\u5bfc\u548c\u5b66\u4e60\u8fc7\u7a0b\u3002\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f85\u5bfc\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u4f5c\u4e3a\u5b66\u751f\u65f6\u80fd\u4ea7\u751f\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u5b66\u4e60\u66f2\u7ebf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u63d0\u5347\uff0c\u5b83\u4eec\u88ab\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4f5c\u72ec\u7acb\u8f85\u5bfc\u5de5\u5177\u6216\u4eba\u7c7b\u5b66\u4e60\u7684\u6a21\u62df\u5668\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b0\u5e94\u7528\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u5f0f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u7684\u751f\u6210\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86TutorGym\uff0c\u4e00\u4e2a\u6807\u51c6\u63a5\u53e3\uff0c\u7528\u4e8e\u5728\u73b0\u6709\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u6d4b\u8bd5AI\u4ee3\u7406\u3002TutorGym\u6a21\u62df\u8f85\u5bfc\u548c\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8bc4\u4f30AI\u4f5c\u4e3a\u8f85\u5bfc\u8005\u548c\u5b66\u4e60\u8005\u65f6\u7684\u8868\u73b0\uff0c\u5305\u62ec\u63d0\u4f9b\u8f85\u5bfc\u652f\u6301\uff08\u5982\u63d0\u793a\u3001\u53cd\u9988\uff09\u548c\u5b66\u4e60\u8f68\u8ff9\u3002\u5f53\u524d\u6db5\u76d6223\u4e2a\u4e0d\u540c\u8f85\u5bfc\u9886\u57df\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f85\u5bfc\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff08\u5982\u4e0b\u4e00\u6b65\u52a8\u4f5c\u51c6\u786e\u7387\u4ec5\u4e3a52-70%\uff09\uff0c\u4f46\u5728\u4f5c\u4e3a\u5b66\u751f\u65f6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u4ea7\u751f\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u5b66\u4e60\u66f2\u7ebf\u3002", "conclusion": "TutorGym\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u5176\u5728\u6559\u80b2\u6280\u672f\u4e2d\u7684\u5e94\u7528\u3002\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f85\u5bfc\u4efb\u52a1\u4e0a\u4ecd\u9700\u6539\u8fdb\uff0c\u4f46\u5728\u6a21\u62df\u5b66\u4e60\u65b9\u9762\u5c55\u793a\u4e86\u6f5c\u529b\u3002"}}
{"id": "2505.01560", "pdf": "https://arxiv.org/pdf/2505.01560", "abs": "https://arxiv.org/abs/2505.01560", "authors": ["Vicent Briva Iglesias", "Gokhan Dogru"], "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the\nnext leap in machine translation (MT), but their benefits relative to\nconventional neural MT (NMT) remain unclear. This paper offers an empirical\nreality check. We benchmark five paradigms, Google Translate (strong NMT\nbaseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),\nand two GPT-4o-powered agentic workflows (sequential three-stage and iterative\nrefinement), on test data drawn from a legal contract and news prose in three\nEnglish-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is\nperformed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with\nexpert ratings of adequacy and fluency; efficiency with total input-plus-output\ntoken counts mapped to April 2025 pricing.\n  Automatic scores still favour the mature NMT system, which ranks first in\nseven of twelve metric-language combinations; o1-preview ties or places second\nin most remaining cases, while both multi-agent workflows trail. Human\nevaluation reverses part of this narrative: o1-preview produces the most\nadequate and fluent output in five of six comparisons, and the iterative agent\nedges ahead once, indicating that reasoning layers capture semantic nuance\nundervalued by surface metrics. Yet these qualitative gains carry steep costs.\nThe sequential agent consumes roughly five times, and the iterative agent\nfifteen times, the tokens used by NMT or single-pass LLMs.\n  We advocate multidimensional, cost-aware evaluation protocols and highlight\nresearch directions that could tip the balance: leaner coordination strategies,\nselective agent activation, and hybrid pipelines combining single-pass LLMs\nwith targeted agent intervention.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e94\u79cd\u673a\u5668\u7ffb\u8bd1\u8303\u5f0f\uff08\u4f20\u7edfNMT\u3001\u901a\u7528LLM\u3001\u589e\u5f3a\u63a8\u7406LLM\u53ca\u4e24\u79cd\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff09\uff0c\u53d1\u73b0\u81ea\u52a8\u8bc4\u5206\u4ecd\u503e\u5411\u4e8e\u6210\u719fNMT\u7cfb\u7edf\uff0c\u4f46\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u589e\u5f3a\u63a8\u7406LLM\u5728\u8bed\u4e49\u51c6\u786e\u6027\u4e0a\u66f4\u4f18\uff0c\u5c3d\u7ba1\u5176\u8ba1\u7b97\u6210\u672c\u663e\u8457\u66f4\u9ad8\u3002", "motivation": "\u9a8c\u8bc1LLM\u548c\u591a\u667a\u80fd\u4f53\u7f16\u6392\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u5b9e\u9645\u4f18\u52bf\uff0c\u5bf9\u6bd4\u4f20\u7edfNMT\u7684\u6027\u80fd\u3001\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u5bf9\u4e94\u79cd\u8303\u5f0f\uff08Google Translate\u3001GPT-4o\u3001o1-preview\u53ca\u4e24\u79cdGPT-4o\u667a\u80fd\u4f53\u6d41\u7a0b\uff09\u8fdb\u884c\u81ea\u52a8\uff08COMET/BLEU/chrF2/TER\uff09\u548c\u4eba\u5de5\uff08\u4e13\u5bb6\u8bc4\u5206\uff09\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u6570\u636e\u6db5\u76d6\u6cd5\u5f8b\u5408\u540c\u548c\u65b0\u95fb\u6587\u672c\uff08\u82f1\u897f/\u82f1\u52a0\u6cf0/\u82f1\u571f\uff09\u3002", "result": "\u81ea\u52a8\u8bc4\u5206\u4e2dNMT\u9886\u51487/12\u9879\uff0co1-preview\u591a\u5c45\u7b2c\u4e8c\uff1b\u4eba\u5de5\u8bc4\u4f30\u5219\u663e\u793ao1-preview\u57285/6\u6bd4\u8f83\u4e2d\u66f4\u4f18\uff0c\u4f46\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\u7684token\u6d88\u8017\u9ad8\u8fbeNMT\u76845-15\u500d\u3002", "conclusion": "\u9700\u591a\u7ef4\u6210\u672c\u611f\u77e5\u8bc4\u4f30\uff0c\u672a\u6765\u65b9\u5411\u5305\u62ec\u7cbe\u7b80\u534f\u8c03\u7b56\u7565\u3001\u9009\u62e9\u6027\u667a\u80fd\u4f53\u6fc0\u6d3b\u53ca\u6df7\u5408\u5355\u6b21LLM\u4e0e\u9776\u5411\u5e72\u9884\u7684\u6d41\u7a0b\u3002"}}
{"id": "2505.01572", "pdf": "https://arxiv.org/pdf/2505.01572", "abs": "https://arxiv.org/abs/2505.01572", "authors": ["Bradley McDanel", "Sai Qian Zhang", "Yunhai Hu", "Zining Liu"], "title": "PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding", "categories": ["cs.AI", "cs.DC"], "comment": "10 pages, 5 figures, 2 tables", "summary": "Speculative decoding accelerates large language model inference by using\nsmaller draft models to generate candidate tokens for parallel verification.\nHowever, current approaches are limited by sequential stage dependencies that\nprevent full hardware utilization. We present PipeSpec, a framework that\ngeneralizes speculative decoding to $k$ models arranged in a hierarchical\npipeline, enabling asynchronous execution with lightweight coordination for\nprediction verification and rollback. Our analytical model characterizes token\ngeneration rates across pipeline stages and proves guaranteed throughput\nimprovements over traditional decoding for any non-zero acceptance rate. We\nfurther derive closed-form expressions for steady-state verification\nprobabilities that explain the empirical benefits of pipeline depth.\nExperimental results show that PipeSpec achieves up to 2.54$\\times$ speedup\nwhile outperforming state-of-the-art methods. We validate PipeSpec across text\nsummarization and code generation tasks using LLaMA 2 and 3 models,\ndemonstrating that pipeline efficiency increases with model depth, providing a\nscalable approach to accelerating LLM inference on multi-device systems.", "AI": {"tldr": "PipeSpec\u901a\u8fc7\u5206\u5c42\u6d41\u6c34\u7ebf\u7ed3\u6784\u5e76\u884c\u5316\u63a8\u6d4b\u89e3\u7801\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5b9e\u9a8c\u6700\u9ad8\u52a0\u901f2.54\u500d\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u56e0\u9636\u6bb5\u4f9d\u8d56\u5bfc\u81f4\u786c\u4ef6\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u9700\u89e3\u51b3\u5e76\u884c\u6027\u4e0e\u534f\u8c03\u95ee\u9898\u3002", "method": "\u91c7\u7528k\u4e2a\u6a21\u578b\u7684\u5206\u5c42\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u652f\u6301\u5f02\u6b65\u6267\u884c\u4e0e\u8f7b\u91cf\u7ea7\u9a8c\u8bc1/\u56de\u6eda\u673a\u5236\uff0c\u7406\u8bba\u6a21\u578b\u5206\u6790\u541e\u5410\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1PipeSpec\u5728\u6587\u672c\u6458\u8981\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6d41\u6c34\u7ebf\u6df1\u5ea6\u4e0e\u52a0\u901f\u6548\u679c\u6b63\u76f8\u5173\u3002", "conclusion": "PipeSpec\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u8bbe\u5907LLM\u63a8\u7406\u52a0\u901f\u65b9\u6848\uff0c\u6a21\u578b\u6df1\u5ea6\u589e\u52a0\u65f6\u6548\u7387\u6301\u7eed\u63d0\u5347\u3002"}}
{"id": "2505.01592", "pdf": "https://arxiv.org/pdf/2505.01592", "abs": "https://arxiv.org/abs/2505.01592", "authors": ["Takyoung Kim", "Janvijay Singh", "Shuhaib Mehri", "Emre Can Acikgoz", "Sagnik Mukherjee", "Nimet Beyza Bozdag", "Sumuk Shashidhar", "Gokhan Tur", "Dilek Hakkani-T\u00fcr"], "title": "PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint in progress", "summary": "The growing capabilities of large language models (LLMs) in\ninstruction-following and context-understanding lead to the era of agents with\nnumerous applications. Among these, task planning agents have become especially\nprominent in realistic scenarios involving complex internal pipelines, such as\ncontext understanding, tool management, and response generation. However,\nexisting benchmarks predominantly evaluate agent performance based on task\ncompletion as a proxy for overall effectiveness. We hypothesize that merely\nimproving task completion is misaligned with maximizing user satisfaction, as\nusers interact with the entire agentic process and not only the end result. To\naddress this gap, we propose PIPA, a unified evaluation protocol that\nconceptualizes the behavioral process of interactive task planning agents\nwithin a partially observable Markov Decision Process (POMDP) paradigm. The\nproposed protocol offers a comprehensive assessment of agent performance\nthrough a set of atomic evaluation criteria, allowing researchers and\npractitioners to diagnose specific strengths and weaknesses within the agent's\ndecision-making pipeline. Our analyses show that agents excel in different\nbehavioral stages, with user satisfaction shaped by both outcomes and\nintermediate behaviors. We also highlight future directions, including systems\nthat leverage multiple agents and the limitations of user simulators in task\nplanning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PIPA\uff0c\u4e00\u79cd\u57fa\u4e8ePOMDP\u7684\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4efb\u52a1\u89c4\u5212\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u5f3a\u8c03\u7528\u6237\u6ee1\u610f\u5ea6\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u4efb\u52a1\u5b8c\u6210\u7ed3\u679c\uff0c\u8fd8\u4e0e\u4ee3\u7406\u7684\u4e2d\u95f4\u884c\u4e3a\u6709\u5173\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u57fa\u4e8e\u4efb\u52a1\u5b8c\u6210\u5ea6\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u4e0e\u4ee3\u7406\u884c\u4e3a\u7684\u6574\u4f53\u4ea4\u4e92\u4f53\u9a8c\u4e4b\u95f4\u7684\u5173\u8054\u3002\u4f5c\u8005\u8ba4\u4e3a\u4ec5\u4f18\u5316\u4efb\u52a1\u5b8c\u6210\u5ea6\u65e0\u6cd5\u6700\u5927\u5316\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "method": "\u63d0\u51faPIPA\u534f\u8bae\uff0c\u5c06\u4ea4\u4e92\u5f0f\u4efb\u52a1\u89c4\u5212\u4ee3\u7406\u7684\u884c\u4e3a\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u5e76\u8bbe\u8ba1\u539f\u5b50\u5316\u8bc4\u4f30\u6807\u51c6\u4ee5\u5168\u9762\u5206\u6790\u4ee3\u7406\u7684\u51b3\u7b56\u6d41\u7a0b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7406\u5728\u4e0d\u540c\u884c\u4e3a\u9636\u6bb5\u8868\u73b0\u5404\u5f02\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u540c\u65f6\u53d7\u7ed3\u679c\u548c\u4e2d\u95f4\u884c\u4e3a\u5f71\u54cd\uff0c\u540c\u65f6\u6307\u51fa\u591a\u4ee3\u7406\u7cfb\u7edf\u548c\u7528\u6237\u6a21\u62df\u5668\u7684\u5c40\u9650\u6027\u3002", "conclusion": "PIPA\u4e3a\u4efb\u52a1\u89c4\u5212\u4ee3\u7406\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u884c\u4e3a\u8fc7\u7a0b\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.01636", "pdf": "https://arxiv.org/pdf/2505.01636", "abs": "https://arxiv.org/abs/2505.01636", "authors": ["Amit Rath"], "title": "Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; H.2.8; D.2.13"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and task generalization. However, their\napplication to structured data analysis remains fragile due to inconsistencies\nin schema interpretation, misalignment between user intent and model output,\nand limited mechanisms for self-correction when failures occur. This paper\nintroduces the STROT Framework (Structured Task Reasoning and Output\nTransformation), a method for structured prompting and feedback-driven\ntransformation logic generation aimed at improving the reliability and semantic\nalignment of LLM-based analytical workflows. STROT begins with lightweight\nschema introspection and sample-based field classification, enabling dynamic\ncontext construction that captures both the structure and statistical profile\nof the input data. This contextual information is embedded in structured\nprompts that guide the model toward generating task-specific, interpretable\noutputs. To address common failure modes in complex queries, STROT incorporates\na refinement mechanism in which the model iteratively revises its outputs based\non execution feedback and validation signals. Unlike conventional approaches\nthat rely on static prompts or single-shot inference, STROT treats the LLM as a\nreasoning agent embedded within a controlled analysis loop -- capable of\nadjusting its output trajectory through planning and correction. The result is\na robust and reproducible framework for reasoning over structured data with\nLLMs, applicable to diverse data exploration and analysis tasks where\ninterpretability, stability, and correctness are essential.", "AI": {"tldr": "STROT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u53cd\u9988\u9a71\u52a8\u7684\u8f93\u51fa\u8f6c\u6362\u903b\u8f91\uff0c\u63d0\u5347LLM\u5728\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709LLM\u5728\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u4e2d\u5b58\u5728\u6a21\u5f0f\u89e3\u91ca\u4e0d\u4e00\u81f4\u3001\u7528\u6237\u610f\u56fe\u4e0e\u6a21\u578b\u8f93\u51fa\u4e0d\u5339\u914d\u4ee5\u53ca\u7f3a\u4e4f\u81ea\u6211\u7ea0\u6b63\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u63d0\u51faSTROT\u6846\u67b6\u3002", "method": "STROT\u5305\u62ec\u8f7b\u91cf\u7ea7\u6a21\u5f0f\u5185\u7701\u3001\u57fa\u4e8e\u6837\u672c\u7684\u5b57\u6bb5\u5206\u7c7b\u3001\u52a8\u6001\u4e0a\u4e0b\u6587\u6784\u5efa\u548c\u8fed\u4ee3\u8f93\u51fa\u4f18\u5316\u673a\u5236\u3002", "result": "STROT\u5728\u7ed3\u6784\u5316\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6790\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u7a33\u5b9a\u6027\u548c\u6b63\u786e\u6027\u7684\u4efb\u52a1\u3002", "conclusion": "STROT\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u7ed3\u6784\u5316\u6570\u636e\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u9002\u5408\u5e7f\u6cdb\u7684\u6570\u636e\u63a2\u7d22\u4e0e\u5206\u6790\u4efb\u52a1\u3002"}}
{"id": "2505.01595", "pdf": "https://arxiv.org/pdf/2505.01595", "abs": "https://arxiv.org/abs/2505.01595", "authors": ["Liaoyaqi Wang", "Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a state-of-the-art model for fine-grained probability estimation\nof propositions conditioned on context. Recent advances in large language\nmodels (LLMs) have significantly enhanced their reasoning capabilities,\nparticularly on well-defined tasks with complete information. However, LLMs\ncontinue to struggle with making accurate and well-calibrated probabilistic\npredictions under uncertainty or partial information. While incorporating\nuncertainty into model predictions often boosts performance, obtaining reliable\nestimates of that uncertainty remains understudied. In particular, LLM\nprobability estimates tend to be coarse and biased towards more frequent\nnumbers. Through a combination of human and synthetic data creation and\nassessment, scaling to larger models, and better supervision, we propose a set\nof strong and precise probability estimation models. We conduct systematic\nevaluations across tasks that rely on conditional probability estimation and\nshow that our approach consistently outperforms existing fine-tuned and\nprompting-based methods by a large margin.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u7ec6\u7c92\u5ea6\u6982\u7387\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u548c\u5408\u6210\u6570\u636e\u3001\u6269\u5c55\u66f4\u5927\u6a21\u578b\u4ee5\u53ca\u66f4\u597d\u7684\u76d1\u7763\uff0c\u663e\u8457\u6539\u5584\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u6216\u90e8\u5206\u4fe1\u606f\u4e0b\u7684\u6982\u7387\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u660e\u786e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e0d\u786e\u5b9a\u6216\u90e8\u5206\u4fe1\u606f\u6761\u4ef6\u4e0b\u7684\u6982\u7387\u9884\u6d4b\u4ecd\u5b58\u5728\u504f\u5dee\u548c\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u9760\u4f30\u8ba1\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662fLLM\u7684\u6982\u7387\u4f30\u8ba1\u5f80\u5f80\u7c97\u7cd9\u4e14\u504f\u5411\u9ad8\u9891\u6570\u503c\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u548c\u5408\u6210\u6570\u636e\u7684\u521b\u5efa\u4e0e\u8bc4\u4f30\u3001\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u4ee5\u53ca\u6539\u8fdb\u76d1\u7763\u65b9\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u7ec4\u5f3a\u5065\u4e14\u7cbe\u786e\u7684\u6982\u7387\u4f30\u8ba1\u6a21\u578b\u3002\u5305\u62ec\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4f9d\u8d56\u6761\u4ef6\u6982\u7387\u4f30\u8ba1\u7684\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6761\u4ef6\u6982\u7387\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u5fae\u8c03\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u4e14\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u3001\u6a21\u578b\u6269\u5c55\u548c\u76d1\u7763\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6982\u7387\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2505.01651", "pdf": "https://arxiv.org/pdf/2505.01651", "abs": "https://arxiv.org/abs/2505.01651", "authors": ["Zeynep Engin"], "title": "Human-AI Governance (HAIG): A Trust-Utility Approach", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "cs.SI"], "comment": "32 pages including references and appendix, 25 pages core text, 3\n  figures, 3 tables", "summary": "This paper introduces the HAIG framework for analysing trust dynamics across\nevolving human-AI relationships. Current categorical frameworks (e.g.,\n\"human-in-the-loop\" models) inadequately capture how AI systems evolve from\ntools to partners, particularly as foundation models demonstrate emergent\ncapabilities and multi-agent systems exhibit autonomous goal-setting\nbehaviours. As systems advance, agency redistributes in complex patterns that\nare better represented as positions along continua rather than discrete\ncategories, though progression may include both gradual shifts and significant\nstep changes. The HAIG framework operates across three levels: dimensions\n(Decision Authority Distribution, Process Autonomy, and Accountability\nConfiguration), continua (gradual shifts along each dimension), and thresholds\n(critical points requiring governance adaptation). Unlike risk-based or\nprinciple-based approaches, HAIG adopts a trust-utility orientation, focusing\non maintaining appropriate trust relationships that maximise utility while\nensuring sufficient safeguards. Our analysis reveals how technical advances in\nself-supervision, reasoning authority, and distributed decision-making drive\nnon-uniform trust evolution across both contextual variation and technological\nadvancement. Case studies in healthcare and European regulation demonstrate how\nHAIG complements existing frameworks while offering a foundation for\nalternative approaches that anticipate governance challenges before they\nemerge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HAIG\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4eba\u7c7b\u4e0eAI\u5173\u7cfb\u4e2d\u4fe1\u4efb\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5f3a\u8c03\u4ece\u5de5\u5177\u5230\u5408\u4f5c\u4f19\u4f34\u7684\u8fde\u7eed\u6f14\u53d8\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u5ea6\u3001\u8fde\u7eed\u6027\u548c\u9608\u503c\u6765\u9002\u5e94\u6cbb\u7406\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5206\u7c7b\u6846\u67b6\uff08\u5982'\u4eba\u5728\u5faa\u73af\u4e2d'\u6a21\u578b\uff09\u65e0\u6cd5\u5145\u5206\u6355\u6349AI\u7cfb\u7edf\u4ece\u5de5\u5177\u6f14\u53d8\u4e3a\u5408\u4f5c\u4f19\u4f34\u7684\u52a8\u6001\uff0c\u5c24\u5176\u662f\u5728\u57fa\u7840\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5c55\u73b0\u81ea\u4e3b\u884c\u4e3a\u65f6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u6846\u67b6\u6765\u91cf\u5316\u4fe1\u4efb\u548c\u6cbb\u7406\u6f14\u5316\u3002", "method": "HAIG\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u5c42\u6b21\u8fd0\u4f5c\uff1a\u7ef4\u5ea6\uff08\u51b3\u7b56\u6743\u5206\u914d\u3001\u8fc7\u7a0b\u81ea\u4e3b\u6027\u548c\u95ee\u8d23\u914d\u7f6e\uff09\u3001\u8fde\u7eed\u6027\uff08\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6e10\u8fdb\u53d8\u5316\uff09\u548c\u9608\u503c\uff08\u9700\u8c03\u6574\u6cbb\u7406\u7684\u5173\u952e\u70b9\uff09\uff0c\u5e76\u4ee5\u4fe1\u4efb-\u6548\u7528\u4e3a\u5bfc\u5411\u5206\u6790\u4fe1\u4efb\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u6280\u672f\u7684\u8fdb\u6b65\uff08\u5982\u81ea\u6211\u76d1\u7763\u548c\u5206\u5e03\u5f0f\u51b3\u7b56\uff09\u4f1a\u5f15\u53d1\u4fe1\u4efb\u7684\u975e\u5747\u5300\u6f14\u5316\uff0c\u6848\u4f8b\u7814\u7a76\u663e\u793aHAIG\u80fd\u8865\u5145\u73b0\u6709\u6846\u67b6\u5e76\u9884\u89c1\u6cbb\u7406\u6311\u6218\u3002", "conclusion": "HAIG\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u89c6\u89d2\uff0c\u80fd\u66f4\u7075\u6d3b\u5730\u9002\u5e94AI\u7cfb\u7edf\u53d1\u5c55\u4e2d\u7684\u4fe1\u4efb\u548c\u6cbb\u7406\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u7684\u6cbb\u7406\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.01658", "pdf": "https://arxiv.org/pdf/2505.01658", "abs": "https://arxiv.org/abs/2505.01658", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "categories": ["cs.CL"], "comment": "Under review; 65 pages; 27 figures", "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine", "AI": {"tldr": "\u672c\u6587\u5bf925\u79cd\u5f00\u6e90\u548c\u5546\u4e1aLLM\u63a8\u7406\u5f15\u64ce\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u6613\u7528\u6027\u3001\u90e8\u7f72\u4fbf\u6377\u6027\u3001\u901a\u7528\u652f\u6301\u3001\u6269\u5c55\u6027\u4ee5\u53ca\u5bf9\u541e\u5410\u548c\u5ef6\u8fdf\u654f\u611f\u8ba1\u7b97\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bbe\u8ba1\u76ee\u6807\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLM\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u63a8\u7406\u6210\u672c\u6210\u4e3a\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u591a\u6837\u5316\u670d\u52a1\u9700\u6c42\uff0c\u9009\u62e9\u5408\u9002\u7684\u4f18\u5316\u65b9\u6cd5\u56f0\u96be\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u5f15\u64ce\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u8bc4\u4f3025\u79cd\u63a8\u7406\u5f15\u64ce\u7684\u591a\u7ef4\u5ea6\u7279\u6027\uff08\u5982\u6613\u7528\u6027\u3001\u6269\u5c55\u6027\uff09\u548c\u4f18\u5316\u6280\u672f\uff0c\u5e76\u5206\u6790\u5f00\u6e90\u5f15\u64ce\u751f\u6001\u6210\u719f\u5ea6\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u6210\u672c\u7b56\u7565\u3002", "result": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u5b66\u8005\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\uff0c\u5305\u62ec\u672a\u6765\u652f\u6301\u590d\u6742LLM\u670d\u52a1\u3001\u5404\u79cd\u786c\u4ef6\u548c\u589e\u5f3a\u5b89\u5168\u6027\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u548c\u63a2\u8ba8\uff0c\u672c\u6587\u4e3aLLM\u63a8\u7406\u5f15\u64ce\u7684\u9009\u62e9\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u5f00\u653e\u516c\u5171\u5b58\u50a8\u5e93\u8ddf\u8e2a\u8be5\u9886\u57df\u5feb\u901f\u53d1\u5c55\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.01706", "pdf": "https://arxiv.org/pdf/2505.01706", "abs": "https://arxiv.org/abs/2505.01706", "authors": ["Sarvesh Shashidhar", "Ritik", "Nachiketa Patil", "Suraj Racha", "Ganesh Ramakrishnan"], "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Updated abstract, algorithm and experimental results", "summary": "Direct Preference Optimisation (DPO) has emerged as a powerful method for\naligning Large Language Models (LLMs) with human preferences, offering a stable\nand efficient alternative to approaches that use Reinforcement learning via\nHuman Feedback. In this work, we investigate the performance of DPO using\nopen-source preference datasets. One of the major drawbacks of DPO is that it\ndoesn't induce granular scoring and treats all the segments of the responses\nwith equal propensity. However, this is not practically true for human\npreferences since even \"good\" responses have segments that may not be preferred\nby the annotator. To resolve this, a 2-dimensional scoring for DPO alignment\ncalled 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the\nadvantages it provides over the standard DPO by comparing their win rates. It\nis observed that these methods, even though effective, are not robust to\nlabel/score noise. To counter this, we propose an approach of incorporating\nsegment-level score noise robustness to the 2D-DPO algorithm. Along with\ntheoretical backing, we also provide empirical verification in favour of the\nalgorithm and introduce other noise models that can be present.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86Direct Preference Optimisation (DPO) \u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd2D-DPO\u65b9\u6cd5\u6765\u6539\u8fdb\u30022D-DPO\u901a\u8fc7\u4e8c\u7ef4\u8bc4\u5206\u89e3\u51b3DPO\u5bf9\u6240\u6709\u54cd\u5e94\u6bb5\u843d\u7b49\u540c\u5bf9\u5f85\u7684\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7b97\u6cd5\u4ee5\u5e94\u5bf9\u6807\u7b7e/\u8bc4\u5206\u566a\u97f3\u3002", "motivation": "\u4eba\u7c7b\u504f\u597d\u901a\u5e38\u662f\u5206\u6bb5\u7684\uff0c\u4e0d\u540c\u6bb5\u843d\u7684\u8bc4\u5206\u53ef\u80fd\u4e0d\u540c\uff0c\u800cDPO\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u8fd9\u4e00\u70b9\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u5316\u7684\u8bc4\u5206\u65b9\u6cd5\uff0c\u540c\u65f6\u589e\u5f3a\u5bf9\u566a\u97f3\u7684\u9c81\u68d2\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e862D-DPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u7ef4\u8bc4\u5206\uff08\u5206\u6bb5\u8bc4\u5206\uff09\u66ff\u4ee3\u4f20\u7edfDPO\u7684\u5355\u4e00\u8bc4\u5206\uff0c\u5e76\u5f15\u5165\u4e86\u5206\u6bb5\u7ea7\u522b\u566a\u97f3\u9c81\u68d2\u6027\u7684\u6539\u8fdb\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c2D-DPO\u5728\u80dc\u7387\u4e0a\u4f18\u4e8e\u6807\u51c6DPO\uff0c\u4f46\u5bf9\u566a\u97f3\u654f\u611f\u3002\u6539\u8fdb\u7b97\u6cd5\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u566a\u97f3\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "2D-DPO\u63d0\u9ad8\u4e86\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u7cbe\u786e\u6027\uff0c\u6539\u8fdb\u7b97\u6cd5\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u5bf9\u566a\u97f3\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2505.01693", "pdf": "https://arxiv.org/pdf/2505.01693", "abs": "https://arxiv.org/abs/2505.01693", "authors": ["Brian Wong", "Kaito Tanaka"], "title": "High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers", "categories": ["cs.CL"], "comment": null, "summary": "Automated labeling of chest X-ray reports is essential for enabling\ndownstream tasks such as training image-based diagnostic models, population\nhealth studies, and clinical decision support. However, the high variability,\ncomplexity, and prevalence of negation and uncertainty in these free-text\nreports pose significant challenges for traditional Natural Language Processing\nmethods. While large language models (LLMs) demonstrate strong text\nunderstanding, their direct application for large-scale, efficient labeling is\nlimited by computational cost and speed. This paper introduces DeBERTa-RAD, a\nnovel two-stage framework that combines the power of state-of-the-art LLM\npseudo-labeling with efficient DeBERTa-based knowledge distillation for\naccurate and fast chest X-ray report labeling. We leverage an advanced LLM to\ngenerate high-quality pseudo-labels, including certainty statuses, for a large\ncorpus of reports. Subsequently, a DeBERTa-Base model is trained on this\npseudo-labeled data using a tailored knowledge distillation strategy. Evaluated\non the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a\nstate-of-the-art Macro F1 score of 0.9120, significantly outperforming\nestablished rule-based systems, fine-tuned transformer models, and direct LLM\ninference, while maintaining a practical inference speed suitable for\nhigh-throughput applications. Our analysis shows particular strength in\nhandling uncertain findings. This work demonstrates a promising path to\novercome data annotation bottlenecks and achieve high-performance medical text\nprocessing through the strategic combination of LLM capabilities and efficient\nstudent models trained via distillation.", "AI": {"tldr": "DeBERTa-RAD\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5148\u8fdbLLM\u7684\u4f2a\u6807\u7b7e\u548c\u9ad8\u6548\u7684DeBERTa\u77e5\u8bc6\u84b8\u998f\uff0c\u7528\u4e8e\u80f8\u90e8X\u5149\u62a5\u544a\u7684\u81ea\u52a8\u5316\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u80f8\u90e8X\u5149\u62a5\u544a\u7684\u81ea\u7531\u6587\u672c\u5177\u6709\u9ad8\u53d8\u5f02\u6027\u3001\u590d\u6742\u6027\u548c\u5426\u5b9a\u3001\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edfNLP\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\uff0c\u76f4\u63a5\u4f7f\u7528LLM\u5219\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u548c\u901f\u5ea6\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7LLM\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u518d\u7528DeBERTa-Base\u6a21\u578b\u5728\u4f2a\u6807\u7b7e\u6570\u636e\u4e0a\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u3002", "result": "\u5728MIMIC-500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeBERTa-RAD\u7684Macro F1\u5f97\u5206\u8fbe\u52300.9120\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u4e0d\u786e\u5b9a\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u5408LLM\u80fd\u529b\u548c\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\uff0cDeBERTa-RAD\u4e3a\u514b\u670d\u533b\u5b66\u6587\u672c\u6807\u6ce8\u74f6\u9888\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.01712", "pdf": "https://arxiv.org/pdf/2505.01712", "abs": "https://arxiv.org/abs/2505.01712", "authors": ["Lingyi Wang", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "title": "World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks", "categories": ["cs.AI", "cs.NI"], "comment": null, "summary": "Traditional reinforcement learning (RL)-based learning approaches for\nwireless networks rely on expensive trial-and-error mechanisms and real-time\nfeedback based on extensive environment interactions, which leads to low data\nefficiency and short-sighted policies. These limitations become particularly\nproblematic in complex, dynamic networks with high uncertainty and long-term\nplanning requirements. To address these limitations, in this paper, a novel\nworld model-based learning framework is proposed to minimize\npacket-completeness-aware age of information (CAoI) in a vehicular network.\nParticularly, a challenging representative scenario is considered pertaining to\na millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,\nwhich is characterized by high mobility, frequent signal blockages, and\nextremely short coherence time. Then, a world model framework is proposed to\njointly learn a dynamic model of the mmWave V2X environment and use it to\nimagine trajectories for learning how to perform link scheduling. In\nparticular, the long-term policy is learned in differentiable imagined\ntrajectories instead of environment interactions. Moreover, owing to its\nimagination abilities, the world model can jointly predict time-varying\nwireless data and optimize link scheduling in real-world wireless and V2X\nnetworks. Thus, during intervals without actual observations, the world model\nremains capable of making efficient decisions. Extensive experiments are\nperformed on a realistic simulator based on Sionna that integrates\nphysics-based end-to-end channel modeling, ray-tracing, and scene geometries\nwith material properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency, and achieves 26%\nimprovement and 16% improvement in CAoI, respectively, compared to the\nmodel-based RL (MBRL) method and the model-free RL (MFRL) method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6beb\u7c73\u6ce2V2X\u7f51\u7edc\u4e2d\u6700\u5c0f\u5316\u6570\u636e\u5305\u5b8c\u6574\u6027\u611f\u77e5\u7684\u4fe1\u606f\u5e74\u9f84\uff08CAoI\uff09\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u5e76\u964d\u4f4e\u4e86CAoI\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u8bd5\u9519\u673a\u5236\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u548c\u77ed\u89c6\u7b56\u7565\uff0c\u5c24\u5176\u5728\u590d\u6742\u52a8\u6001\u7f51\u7edc\u4e2d\u95ee\u9898\u66f4\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e16\u754c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u6beb\u7c73\u6ce2V2X\u73af\u5883\u7684\u52a8\u6001\u6a21\u578b\uff0c\u5229\u7528\u60f3\u8c61\u8f68\u8ff9\u8fdb\u884c\u94fe\u8def\u8c03\u5ea6\u7684\u957f\u671f\u7b56\u7565\u5b66\u4e60\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u4e0e\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\uff0c\u5e76\u5728CAoI\u4e0a\u5206\u522b\u6bd4MBRL\u548cMFRL\u65b9\u6cd5\u63d0\u9ad8\u4e8626%\u548c16%\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u6846\u67b6\u5728\u590d\u6742\u52a8\u6001\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01731", "pdf": "https://arxiv.org/pdf/2505.01731", "abs": "https://arxiv.org/abs/2505.01731", "authors": ["Chuan Sun", "Han Yu", "Lizhen Cui"], "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pruning large language models (LLMs) is a promising solution for reducing\nmodel sizes and computational complexity while preserving performance.\nTraditional layer-wise pruning methods often adopt a uniform sparsity approach\nacross all layers, which leads to suboptimal performance due to the varying\nsignificance of individual transformer layers within the model not being\naccounted for. To this end, we propose the \\underline{S}hapley\n\\underline{V}alue-based \\underline{N}on-\\underline{U}niform \\underline{P}runing\n(\\methodname{}) method for LLMs. This approach quantifies the contribution of\neach transformer layer to the overall model performance, enabling the\nassignment of tailored pruning budgets to different layers to retain critical\nparameters. To further improve efficiency, we design the Sliding Window-based\nShapley Value approximation method. It substantially reduces computational\noverhead compared to exact SV calculation methods. Extensive experiments on\nvarious LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness\nof the proposed approach. The results reveal that non-uniform pruning\nsignificantly enhances the performance of pruned models. Notably, \\methodname{}\nachieves a reduction in perplexity (PPL) of 18.01\\% and 19.55\\% on LLaMA-7B and\nLLaMA-13B, respectively, compared to SparseGPT at 70\\% sparsity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u975e\u5747\u5300\u526a\u679d\u65b9\u6cd5\uff08SVNP\uff09\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u5404\u5c42\u7684\u8d21\u732e\u5e76\u5206\u914d\u526a\u679d\u9884\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u526a\u679d\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5bf9\u6240\u6709\u5c42\u91c7\u7528\u5747\u5300\u7a00\u758f\u6027\uff0c\u672a\u8003\u8651\u5404\u5c42\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u3002", "method": "\u91c7\u7528Shapley\u503c\u91cf\u5316\u6bcf\u5c42\u5bf9\u6574\u4f53\u6027\u80fd\u7684\u8d21\u732e\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u8fd1\u4f3c\u6cd5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5206\u914d\u5b9a\u5236\u5316\u526a\u679d\u9884\u7b97\u3002", "result": "\u5728LLaMA\u548cOPT\u7b49\u6a21\u578b\u4e0a\uff0cSVNP\u572870%\u7a00\u758f\u5ea6\u4e0b\u663e\u8457\u964d\u4f4e\u56f0\u60d1\u5ea6\uff08PPL\uff09\uff0c\u5982LLaMA-7B/13B\u5206\u522b\u964d\u4f4e18.01%\u548c19.55%\u3002", "conclusion": "\u975e\u5747\u5300\u526a\u679d\u65b9\u6cd5\u901a\u8fc7\u7cbe\u7ec6\u5316\u9884\u7b97\u5206\u914d\uff0c\u663e\u8457\u4f18\u5316\u4e86\u526a\u679d\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.01754", "pdf": "https://arxiv.org/pdf/2505.01754", "abs": "https://arxiv.org/abs/2505.01754", "authors": ["Orlando J\u00e4hde", "Thorsten Weber", "R\u00fcdiger Buchkremer"], "title": "Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.MA", "68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05,\n  62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20", "I.2; H.3; I.5; I.7; H.5; H.1"], "comment": null, "summary": "Biased news reporting poses a significant threat to informed decision-making\nand the functioning of democracies. This study introduces a novel methodology\nfor scalable, minimally biased analysis of media bias in political news. The\nproposed approach examines event selection, labeling, word choice, and\ncommission and omission biases across news sources by leveraging natural\nlanguage processing techniques, including hierarchical topic modeling,\nsentiment analysis, and ontology learning with large language models. Through\nthree case studies related to current political events, we demonstrate the\nmethodology's effectiveness in identifying biases across news sources at\nvarious levels of granularity. This work represents a significant step towards\nscalable, minimally biased media bias analysis, laying the groundwork for tools\nto help news consumers navigate an increasingly complex media landscape.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff08\u5982\u5206\u5c42\u4e3b\u9898\u5efa\u6a21\u3001\u60c5\u611f\u5206\u6790\u548c\u672c\u4f53\u5b66\u4e60\uff09\u6765\u89c4\u6a21\u5316\u5206\u6790\u653f\u6cbb\u65b0\u95fb\u62a5\u9053\u504f\u5dee\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6709\u504f\u89c1\u7684\u65b0\u95fb\u62a5\u9053\u5bf9\u6c11\u4e3b\u51b3\u7b56\u548c\u77e5\u60c5\u6743\u6784\u6210\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u504f\u89c1\u6700\u5c0f\u5316\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5a92\u4f53\u504f\u89c1\u3002", "method": "\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff08\u5305\u62ec\u5206\u5c42\u4e3b\u9898\u5efa\u6a21\u3001\u60c5\u611f\u5206\u6790\u548c\u672c\u4f53\u5b66\u4e60\uff09\u6765\u68c0\u67e5\u65b0\u95fb\u6765\u6e90\u4e2d\u7684\u4e8b\u4ef6\u9009\u62e9\u3001\u6807\u7b7e\u3001\u7528\u8bcd\u53ca\u9057\u6f0f\u504f\u89c1\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u653f\u6cbb\u4e8b\u4ef6\u6848\u4f8b\u7814\u7a76\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u4e0d\u540c\u65b0\u95fb\u6765\u6e90\u5728\u591a\u4e2a\u7c92\u5ea6\u4e0a\u7684\u504f\u89c1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u5e2e\u52a9\u65b0\u95fb\u6d88\u8d39\u8005\u5e94\u5bf9\u590d\u6742\u5a92\u4f53\u73af\u5883\u7684\u5de5\u5177\u6253\u4e0b\u4e86\u57fa\u7840\uff0c\u662f\u8fc8\u5411\u53ef\u6269\u5c55\u3001\u504f\u89c1\u6700\u5c0f\u5316\u5a92\u4f53\u5206\u6790\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2505.01761", "pdf": "https://arxiv.org/pdf/2505.01761", "abs": "https://arxiv.org/abs/2505.01761", "authors": ["Tobias Domhan", "Dawei Zhu"], "title": "Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Accurately evaluating machine-translated text remains a long-standing\nchallenge, particularly for long documents. Recent work has shown that large\nlanguage models (LLMs) can serve as reliable and interpretable sentence-level\ntranslation evaluators via MQM error span annotations. With modern LLMs\nsupporting larger context windows, a natural question arises: can we feed\nentire document translations into an LLM for quality assessment? Ideally,\nevaluation should be invariant to text length, producing consistent error spans\nregardless of input granularity. However, our analysis shows that text length\nsignificantly impacts evaluation: longer texts lead to fewer error spans and\nreduced system ranking accuracy. To address this limitation, we evaluate\nseveral strategies, including granularity-aligned prompting, Focus Sentence\nPrompting (FSP), and a fine-tuning approach to better align LLMs with the\nevaluation task. The latter two methods largely mitigate this length bias,\nmaking LLMs more reliable for long-form translation evaluation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u957f\u6587\u672c\u4f1a\u5f71\u54cdLLM\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u8bc4\u4f30\uff0c\u8868\u73b0\u4e3a\u9519\u8bef\u6807\u6ce8\u51cf\u5c11\u548c\u7cfb\u7edf\u6392\u540d\u51c6\u786e\u6027\u4e0b\u964d\u3002\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u548c\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u957f\u6587\u672c\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u56e0\u6587\u672c\u957f\u5ea6\u5bfc\u81f4\u7684\u8bc4\u4f30\u504f\u5dee\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u4e86\u7c92\u5ea6\u5bf9\u9f50\u63d0\u793a\u3001\u7126\u70b9\u53e5\u5b50\u63d0\u793a\uff08FSP\uff09\u548c\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "FSP\u548c\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u957f\u5ea6\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u957f\u6587\u672c\u7ffb\u8bd1\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u4f18\u5316\u63d0\u793a\u548c\u5fae\u8c03\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u5728\u957f\u6587\u672c\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.01953", "pdf": "https://arxiv.org/pdf/2505.01953", "abs": "https://arxiv.org/abs/2505.01953", "authors": ["Greg Search"], "title": "Training Environment for High Performance Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents Tunnel, a simple, open source, reinforcement learning\ntraining environment for high performance aircraft. It integrates the F16 3D\nnonlinear flight dynamics into OpenAI Gymnasium python package. The template\nincludes primitives for boundaries, targets, adversaries and sensing\ncapabilities that may vary depending on operational need. This offers mission\nplanners a means to rapidly respond to evolving environments, sensor\ncapabilities and adversaries for autonomous air combat aircraft. It offers\nresearchers access to operationally relevant aircraft physics. Tunnel code base\nis accessible to anyone familiar with Gymnasium and/or those with basic python\nskills. This paper includes a demonstration of a week long trade study that\ninvestigated a variety of training methods, observation spaces, and threat\npresentations. This enables increased collaboration between researchers and\nmission planners which can translate to a national military advantage. As\nwarfare becomes increasingly reliant upon automation, software agility will\ncorrelate with decision advantages. Airmen must have tools to adapt to\nadversaries in this context. It may take months for researchers to develop\nskills to customize observation, actions, tasks and training methodologies in\nair combat simulators. In Tunnel, this can be done in a matter of days.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Tunnel\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u73af\u5883\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u98de\u673a\u3002\u5b83\u96c6\u6210\u4e86F16\u76843D\u975e\u7ebf\u6027\u98de\u884c\u52a8\u529b\u5b66\u5230OpenAI Gymnasium\uff0c\u63d0\u4f9b\u5feb\u901f\u54cd\u5e94\u4efb\u52a1\u9700\u6c42\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6218\u4e89\u5bf9\u81ea\u52a8\u5316\u7684\u4f9d\u8d56\u589e\u52a0\uff0c\u8f6f\u4ef6\u654f\u6377\u6027\u4e0e\u51b3\u7b56\u4f18\u52bf\u76f8\u5173\u3002\u9700\u8981\u5de5\u5177\u5e2e\u52a9\u7a7a\u519b\u4eba\u5458\u5feb\u901f\u9002\u5e94\u5bf9\u624b\u7684\u53d8\u5316\u3002", "method": "\u5c06F16\u76843D\u975e\u7ebf\u6027\u98de\u884c\u52a8\u529b\u5b66\u96c6\u6210\u5230OpenAI Gymnasium\uff0c\u63d0\u4f9b\u53ef\u5b9a\u5236\u7684\u8fb9\u754c\u3001\u76ee\u6807\u3001\u5bf9\u624b\u548c\u611f\u77e5\u80fd\u529b\u3002", "result": "Tunnel\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u4efb\u52a1\u89c4\u5212\u8005\u80fd\u5728\u51e0\u5929\u5185\u5b8c\u6210\u5728\u4f20\u7edf\u6a21\u62df\u5668\u4e2d\u9700\u8981\u6570\u6708\u7684\u5de5\u4f5c\uff0c\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "Tunnel\u901a\u8fc7\u5feb\u901f\u5b9a\u5236\u548c\u534f\u4f5c\u80fd\u529b\uff0c\u4e3a\u56fd\u5bb6\u548c\u519b\u4e8b\u63d0\u4f9b\u4e86\u6280\u672f\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u81ea\u4e3b\u7a7a\u6218\u9886\u57df\u3002"}}
{"id": "2505.01794", "pdf": "https://arxiv.org/pdf/2505.01794", "abs": "https://arxiv.org/abs/2505.01794", "authors": ["Jared D. T. Guerrero-Sosa", "Francisco P. Romero", "V\u00edctor Hugo Men\u00e9ndez-Dom\u00ednguez", "Jesus Serrano-Guerrero", "Andres Montoro-Montarroso", "Jose A. Olivas"], "title": "A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "In the rapidly evolving educational landscape, the unbiased assessment of\nsoft skills is a significant challenge, particularly in higher education. This\npaper presents a fuzzy logic approach that employs a Granular Linguistic Model\nof Phenomena integrated with multimodal analysis to evaluate soft skills in\nundergraduate students. By leveraging computational perceptions, this approach\nenables a structured breakdown of complex soft skill expressions, capturing\nnuanced behaviours with high granularity and addressing their inherent\nuncertainties, thereby enhancing interpretability and reliability. Experiments\nwere conducted with undergraduate students using a developed tool that assesses\nsoft skills such as decision-making, communication, and creativity. This tool\nidentifies and quantifies subtle aspects of human interaction, such as facial\nexpressions and gesture recognition. The findings reveal that the framework\neffectively consolidates multiple data inputs to produce meaningful and\nconsistent assessments of soft skills, showing that integrating multiple\nmodalities into the evaluation process significantly improves the quality of\nsoft skills scores, making the assessment work transparent and understandable\nto educational stakeholders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u7cca\u903b\u8f91\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5206\u6790\uff0c\u7528\u4e8e\u9ad8\u7b49\u6559\u80b2\u4e2d\u5b66\u751f\u8f6f\u6280\u80fd\u7684\u65e0\u504f\u8bc4\u4f30\u3002\u901a\u8fc7\u8ba1\u7b97\u611f\u77e5\u548c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\uff0c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\uff0c\u65e0\u504f\u8bc4\u4f30\u8f6f\u6280\u80fd\u662f\u4e00\u5927\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u884c\u4e3a\u7684\u590d\u6742\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u6a21\u7cca\u903b\u8f91\u548c\u8bed\u8a00\u73b0\u8c61\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u5206\u6790\uff08\u5982\u9762\u90e8\u8868\u60c5\u548c\u624b\u52bf\u8bc6\u522b\uff09\uff0c\u5f00\u53d1\u5de5\u5177\u8bc4\u4f30\u51b3\u7b56\u529b\u3001\u6c9f\u901a\u80fd\u529b\u548c\u521b\u9020\u529b\u7b49\u8f6f\u6280\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6a21\u6001\u6570\u636e\u7684\u6574\u5408\u80fd\u6709\u6548\u63d0\u9ad8\u8f6f\u6280\u80fd\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6559\u80b2\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u8f6f\u6280\u80fd\u8bc4\u4f30\u5de5\u5177\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2505.01955", "pdf": "https://arxiv.org/pdf/2505.01955", "abs": "https://arxiv.org/abs/2505.01955", "authors": ["Max Reuter", "Maura Philippone", "Bond Benton", "Laura Dilley"], "title": "Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM", "categories": ["cs.AI"], "comment": "Eye (2025)", "summary": "The advent of generative artificial intelligence, especially large language\nmodels (LLMs), presents opportunities for innovation in research, clinical\npractice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's\npotential, including for generating AI-voiced podcasts to educate patients\nabout treatment and rehabilitation, and for quickly synthesizing medical\nliterature for professionals. We argue that NotebookLM presently poses clinical\nand technological risks that should be tested and considered prior to its\nimplementation in clinical practice.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u5de5\u5177NotebookLM\u5728\u533b\u7597\u5b9e\u8df5\u4e2d\u7684\u6f5c\u529b\u4e0e\u98ce\u9669\uff0c\u8ba4\u4e3a\u9700\u5728\u4e34\u5e8a\u5e94\u7528\u524d\u8fdb\u884c\u6d4b\u8bd5\u548c\u8bc4\u4f30\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5206\u6790NotebookLM\u5728\u533b\u7597\u9886\u57df\u7684\u6f5c\u529b\u53ca\u5176\u4f34\u968f\u7684\u4e34\u5e8a\u4e0e\u6280\u672f\u98ce\u9669\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u6709\u6548\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u8bba\u8bc1NotebookLM\u7684\u6f5c\u5728\u7528\u9014\u4e0e\u98ce\u9669\uff0c\u63d0\u51fa\u9700\u6d4b\u8bd5\u548c\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "result": "\u6307\u51faNotebookLM\u867d\u6709\u52a9\u4e8e\u6559\u80b2\u548c\u533b\u7597\u6587\u732e\u6574\u7406\uff0c\u4f46\u5b58\u5728\u4e34\u5e8a\u4e0e\u6280\u672f\u98ce\u9669\u3002", "conclusion": "\u5efa\u8bae\u5728NotebookLM\u5e94\u7528\u4e8e\u4e34\u5e8a\u524d\u8fdb\u884c\u5168\u9762\u6d4b\u8bd5\u548c\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2505.01800", "pdf": "https://arxiv.org/pdf/2505.01800", "abs": "https://arxiv.org/abs/2505.01800", "authors": ["Chidimma Opara"], "title": "Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8", "summary": "The increasing sophistication of AI-generated texts highlights the urgent\nneed for accurate and transparent detection tools, especially in educational\nsettings, where verifying authorship is essential. Existing literature has\ndemonstrated that the application of stylometric features with machine learning\nclassifiers can yield excellent results. Building on this foundation, this\nstudy proposes a comprehensive framework that integrates stylometric analysis\nwith psycholinguistic theories, offering a clear and interpretable approach to\ndistinguishing between AI-generated and human-written texts. This research\nspecifically maps 31 distinct stylometric features to cognitive processes such\nas lexical retrieval, discourse planning, cognitive load management, and\nmetacognitive self-monitoring. In doing so, it highlights the unique\npsycholinguistic patterns found in human writing. Through the intersection of\ncomputational linguistics and cognitive science, this framework contributes to\nthe development of reliable tools aimed at preserving academic integrity in the\nera of generative AI.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u98ce\u683c\u8ba1\u91cf\u5b66\u4e0e\u5fc3\u7406\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u533a\u5206AI\u751f\u6210\u4e0e\u4eba\u7c7b\u5199\u4f5c\u7684\u6587\u672c\uff0c\u901a\u8fc731\u4e2a\u98ce\u683c\u7279\u5f81\u6620\u5c04\u5230\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4ee5\u5f00\u53d1\u53ef\u9760\u7684\u5b66\u672f\u8bda\u4fe1\u5de5\u5177\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u6587\u672c\u7684\u65e5\u76ca\u590d\u6742\uff0c\u6559\u80b2\u573a\u666f\u4e2d\u4e9f\u9700\u51c6\u786e\u900f\u660e\u7684\u68c0\u6d4b\u5de5\u5177\u9a8c\u8bc1\u4f5c\u8005\u8eab\u4efd\uff0c\u73b0\u6709\u7814\u7a76\u8868\u98ce\u683c\u8ba1\u91cf\u5b66\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u6548\u679c\u663e\u8457\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u6574\u5408\u98ce\u683c\u8ba1\u91cf\u5b66\u4e0e\u5fc3\u7406\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5177\u4f53\u901a\u8fc731\u4e2a\u98ce\u683c\u7279\u5f81\u6620\u5c04\u5230\u8ba4\u77e5\u8fc7\u7a0b\uff08\u5982\u8bcd\u6c47\u68c0\u7d22\u3001\u8bed\u7bc7\u89c4\u5212\u7b49\uff09\uff0c\u7a81\u51fa\u4eba\u7c7b\u5199\u4f5c\u7684\u72ec\u7279\u5fc3\u7406\u8bed\u8a00\u5b66\u6a21\u5f0f\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e0e\u8ba4\u77e5\u79d1\u5b66\u7684\u4ea4\u53c9\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u7684\u6587\u672c\u68c0\u6d4b\u5de5\u5177\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u7ef4\u62a4\u751f\u6210\u5f0fAI\u65f6\u4ee3\u7684\u5b66\u672f\u8bda\u4fe1\u3002", "conclusion": "\u7ed3\u5408\u98ce\u683c\u8ba1\u91cf\u5b66\u4e0e\u5fc3\u7406\u8bed\u8a00\u5b66\u7684\u65b9\u6cd5\u4e3a\u533a\u5206AI\u4e0e\u4eba\u7c7b\u6587\u672c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u8def\u5f84\uff0c\u5bf9\u5b66\u672f\u8bda\u4fe1\u5de5\u5177\u7684\u53d1\u5c55\u5177\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2505.02003", "pdf": "https://arxiv.org/pdf/2505.02003", "abs": "https://arxiv.org/abs/2505.02003", "authors": ["Maryam Sadeghi", "Dar\u00edo Fern\u00e1ndez Khatiboun", "Yasser Rezaeiyan", "Saima Rizwan", "Alessandro Barcellona", "Andrea Merello", "Marco Crepaldi", "Gabriella Panuccio", "Farshad Moradi"], "title": "Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing", "categories": ["cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "Closed-loop brain stimulation holds potential as personalized treatment for\ndrug-resistant epilepsy (DRE) but still suffers from limitations that result in\nhighly variable efficacy. First, stimulation is typically delivered upon\ndetection of the seizure to abort rather than prevent it; second, the\nstimulation parameters are established by trial and error, requiring lengthy\nrounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we\naddress these limitations by leveraging the potential of neuromorphic\ncomputing. We present a system capable of driving personalized free-run\nstimulations based on seizure forecasting, wherein each forecast triggers an\nelectrical pulse rather than an arbitrarily predefined fixed-frequency stimulus\ntrain. We validate the system against hippocampal spheroids coupled to 3D\nmicroelectrode array as a simplified testbed, showing that it can achieve\nseizure reduction >97% while primarily using instantaneous stimulation\nfrequencies within 20 Hz, well below what typically used in clinical settings.\nOur work demonstrates the potential of neuromorphic systems as a\nnext-generation neuromodulation strategy for personalized DRE treatment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7684\u95ed\u73af\u8111\u523a\u6fc0\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u6cbb\u7597\u8010\u836f\u6027\u766b\u75eb\uff0c\u901a\u8fc7\u9884\u6d4b\u766b\u75eb\u53d1\u4f5c\u5e76\u89e6\u53d1\u4e2a\u6027\u5316\u523a\u6fc0\uff0c\u5b9e\u73b0\u4e8697%\u4ee5\u4e0a\u7684\u53d1\u4f5c\u51cf\u5c11\u3002", "motivation": "\u5f53\u524d\u95ed\u73af\u8111\u523a\u6fc0\u6cbb\u7597\u8010\u836f\u6027\u766b\u75eb\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u523a\u6fc0\u901a\u5e38\u5728\u53d1\u4f5c\u65f6\u800c\u975e\u9884\u9632\u65f6\u8fdb\u884c\uff0c\u4e14\u53c2\u6570\u9700\u53cd\u590d\u8c03\u8bd5\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u766b\u75eb\u9884\u6d4b\u7684\u4e2a\u6027\u5316\u81ea\u7531\u8fd0\u884c\u523a\u6fc0\u7cfb\u7edf\uff0c\u5229\u7528\u9884\u6d4b\u89e6\u53d1\u4e2a\u6027\u5316\u7535\u8109\u51b2\uff08\u800c\u975e\u56fa\u5b9a\u9891\u7387\u523a\u6fc0\uff09\uff0c\u5e76\u5728\u6d77\u9a6c\u7403\u72b6\u4f53\u4e0e3D\u5fae\u7535\u6781\u9635\u5217\u8026\u5408\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86>97%\u7684\u766b\u75eb\u53d1\u4f5c\u51cf\u5c11\uff0c\u4e14\u523a\u6fc0\u9891\u7387\u4e3b\u8981\u4f4e\u4e8e20 Hz\uff0c\u8fdc\u4f4e\u4e8e\u4e34\u5e8a\u5e38\u7528\u9891\u7387\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u5c55\u793a\u4e86\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u4e2a\u6027\u5316\u8010\u836f\u6027\u766b\u75eb\u6cbb\u7597\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.01812", "pdf": "https://arxiv.org/pdf/2505.01812", "abs": "https://arxiv.org/abs/2505.01812", "authors": ["Core Francisco Park", "Zechen Zhang", "Hidenori Tanaka"], "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86New News\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u5fae\u8c03\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5f15\u5165System-2 Fine-tuning\uff08Sys2-FT\uff09\u65b9\u6cd5\u901a\u8fc7\u81ea\u751f\u6210\u6570\u636e\uff08\u5982\u91cd\u8ff0\u3001\u63a8\u7406\u548c\u81ea\u95ee\u81ea\u7b54\uff09\u63d0\u5347\u6a21\u578b\u5bf9\u65b0\u4fe1\u606f\u7684\u6743\u91cd\u5b66\u4e60\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u901a\u8fc7\u5fae\u8c03\u5b66\u4e60\u65b0\u4fe1\u606f\uff08\u65b0\u95fb\uff09\u65f6\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u76f8\u6bd4\u8868\u73b0\u8f83\u5dee\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u6784\u5efaNew News\u6570\u636e\u96c6\uff0c\u63a2\u7d22\u81ea\u751f\u6210\u6570\u636e\u534f\u8bae\uff08\u91cd\u8ff0\u3001\u63a8\u7406\u3001\u81ea\u95ee\u81ea\u7b54\uff09\uff0c\u5e76\u63d0\u51faSys2-FT\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u9886\u57df\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u7cfb\u7edf\u6027\u8bc4\u4f30ICL\u548cSys2-FT\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u81ea\u95ee\u81ea\u7b54\u534f\u8bae\u7684Sys2-FT\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u65b0\u4fe1\u606f\u7684\u6743\u91cd\u5b66\u4e60\uff0c\u5e76\u53d1\u73b0\u4e86'\u4e0a\u4e0b\u6587\u906e\u853d\u6548\u5e94'\uff0c\u5373\u65b0\u95fb\u5728\u4e0a\u4e0b\u6587\u4e2d\u8bad\u7ec3\u540e\u518d\u8bad\u7ec3\u5176\u91cd\u8ff0\u6216\u95ee\u7b54\u4f1a\u524a\u5f31\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u7ed3\u8bba\u662fSys2-FT\u80fd\u6709\u6548\u7f29\u5c0f\u5fae\u8c03\u4e0eICL\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u521d\u6b65\u53d1\u73b0\u5176\u5177\u6709\u6269\u5c55\u5b9a\u5f8b\u7684\u8ff9\u8c61\u3002"}}
{"id": "2505.02024", "pdf": "https://arxiv.org/pdf/2505.02024", "abs": "https://arxiv.org/abs/2505.02024", "authors": ["Minjie Shen", "Qikai Yang"], "title": "From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent", "categories": ["cs.AI"], "comment": null, "summary": "Manus AI is a general-purpose AI agent introduced in early 2025, marking a\nsignificant advancement in autonomous artificial intelligence. Developed by the\nChinese startup Monica.im, Manus is designed to bridge the gap between \"mind\"\nand \"hand\" - combining the reasoning and planning capabilities of large\nlanguage models with the ability to execute complex, end-to-end tasks that\nproduce tangible outcomes. This paper presents a comprehensive overview of\nManus AI, exploring its core technical architecture, diverse applications\nacross sectors such as healthcare, finance, manufacturing, robotics, and\ngaming, as well as its key strengths, current limitations, and future\npotential. Positioned as a preview of what lies ahead, Manus AI represents a\nshift toward intelligent agents that can translate high-level intentions into\nreal-world actions, heralding a new era of human-AI collaboration.", "AI": {"tldr": "Manus AI\u662f2025\u5e74\u63a8\u51fa\u7684\u901a\u7528AI\u4ee3\u7406\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u89c4\u5212\u80fd\u529b\u4e0e\u7aef\u5230\u7aef\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u5e94\u7528\u8986\u76d6\u533b\u7597\u3001\u91d1\u878d\u3001\u5236\u9020\u3001\u673a\u5668\u4eba\u53ca\u6e38\u620f\u7b49\u9886\u57df\u3002", "motivation": "\u65e8\u5728\u5f25\u5408\u2018\u601d\u7ef4\u2019\u4e0e\u2018\u884c\u52a8\u2019\u7684\u9e3f\u6c9f\uff0c\u5c06AI\u7684\u63a8\u7406\u80fd\u529b\u8f6c\u5316\u4e3a\u5b9e\u9645\u4efb\u52a1\u6267\u884c\uff0c\u63a8\u52a8\u4eba\u673a\u534f\u4f5c\u65b0\u65f6\u4ee3\u3002", "method": "\u57fa\u4e8e\u6838\u5fc3\u67b6\u6784\uff0c\u6574\u5408\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\u4e0e\u590d\u6742\u4efb\u52a1\u6267\u884c\u6a21\u5757\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u64cd\u4f5c\u3002", "result": "\u5c55\u793a\u4e86\u8de8\u884c\u4e1a\u5e94\u7528\u7684\u591a\u6837\u6027\u4e0e\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5f53\u524d\u5c40\u9650\u6027\u3002", "conclusion": "Manus AI\u6807\u5fd7\u7740\u667a\u80fd\u4ee3\u7406\u5411\u610f\u56fe\u5230\u884c\u52a8\u7684\u8f6c\u53d8\uff0c\u9884\u793a\u4e86\u672a\u6765\u4eba\u673a\u534f\u4f5c\u7684\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2505.01855", "pdf": "https://arxiv.org/pdf/2505.01855", "abs": "https://arxiv.org/abs/2505.01855", "authors": ["Anthony Nguyen", "Wenjun Lin"], "title": "Intra-Layer Recurrence in Transformers for Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Canadian AI 2025. Code available at\n  https://github.com/ant-8/Layer-Recurrent-Transformers", "summary": "Transformer models have established new benchmarks in natural language\nprocessing; however, their increasing depth results in substantial growth in\nparameter counts. While existing recurrent transformer methods address this\nissue by reprocessing layers multiple times, they often apply recurrence\nindiscriminately across entire blocks of layers. In this work, we investigate\nIntra-Layer Recurrence (ILR), a more targeted approach that applies recurrence\nselectively to individual layers within a single forward pass. Our experiments\nshow that allocating more iterations to earlier layers yields optimal results.\nThese findings suggest that ILR offers a promising direction for optimizing\nrecurrent structures in transformer architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5c42\u5185\u5faa\u73af\uff08ILR\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5355\u4e2a\u524d\u5411\u4f20\u9012\u4e2d\u9009\u62e9\u6027\u5730\u5bf9\u4e2a\u522b\u5c42\u5e94\u7528\u5faa\u73af\uff0c\u4f18\u5316\u4e86Transformer\u67b6\u6784\u4e2d\u7684\u5faa\u73af\u7ed3\u6784\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u65e9\u671f\u5c42\u5206\u914d\u66f4\u591a\u8fed\u4ee3\u6b21\u6570\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5c3d\u7ba1Transformer\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\uff0c\u4f46\u5176\u5c42\u6570\u589e\u52a0\u4f1a\u5bfc\u81f4\u53c2\u6570\u91cf\u5927\u5e45\u589e\u957f\u3002\u73b0\u6709\u7684\u5faa\u73afTransformer\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6574\u4e2a\u5c42\u5757\u8fdb\u884c\u5faa\u73af\u6765\u5904\u7406\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5f80\u5f80\u7f3a\u4e4f\u9488\u5bf9\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u7cbe\u51c6\u7684\u5c42\u5185\u5faa\u73af\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u5c42\u5185\u5faa\u73af\uff08ILR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u524d\u5411\u4f20\u9012\u8fc7\u7a0b\u4e2d\u9488\u5bf9\u6027\u5730\u9009\u62e9\u4e2a\u522b\u5c42\u8fdb\u884c\u5faa\u73af\u5904\u7406\uff0c\u800c\u975e\u5bf9\u6574\u4e2a\u5c42\u5757\u5e94\u7528\u5faa\u73af\u3002\u5b9e\u9a8c\u4e2d\u5bf9\u4e0d\u540c\u5c42\u7684\u8fed\u4ee3\u6b21\u6570\u8fdb\u884c\u4e86\u5206\u914d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u66f4\u591a\u8fed\u4ee3\u6b21\u6570\u5206\u914d\u7ed9\u65e9\u671f\u5c42\u80fd\u591f\u53d6\u5f97\u6700\u4f73\u6548\u679c\u3002\u8fd9\u4e00\u53d1\u73b0\u9a8c\u8bc1\u4e86ILR\u5728\u4f18\u5316Transformer\u67b6\u6784\u5faa\u73af\u7ed3\u6784\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c42\u5185\u5faa\u73af\uff08ILR\uff09\u4e3aTransformer\u67b6\u6784\u4e2d\u7684\u5faa\u73af\u7ed3\u6784\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u5730\u5904\u7406\u4e2a\u522b\u5c42\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u91cf\u7684\u589e\u957f\u3002"}}
{"id": "2505.01427", "pdf": "https://arxiv.org/pdf/2505.01427", "abs": "https://arxiv.org/abs/2505.01427", "authors": ["Maksym Shamrai"], "title": "Perturbation Analysis of Singular Values in Concatenated Matrices", "categories": ["cs.LG", "stat.ML"], "comment": "11 pages", "summary": "Concatenating matrices is a common technique for uncovering shared structures\nin data through singular value decomposition (SVD) and low-rank approximations.\nHowever, a fundamental question arises: how does the singular value spectrum of\nthe concatenated matrix relate to the spectra of its individual components? In\nthis work, we develop a perturbation framework that extends classical results\nsuch as Weyl's inequality to concatenated matrices. We establish analytical\nbounds that quantify the stability of singular values under small perturbations\nin the submatrices. Our results show that if the matrices being concatenated\nare close in norm, the dominant singular values of the concatenated matrix\nremain stable, enabling controlled trade-offs between accuracy and compression.\nThese insights provide a theoretical foundation for improved matrix clustering\nand compression strategies, with applications in numerical linear algebra,\nsignal processing, and data-driven modeling.", "AI": {"tldr": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5206\u6790\u62fc\u63a5\u77e9\u9635\u5947\u5f02\u503c\u8c31\u7a33\u5b9a\u6027\u7684\u6270\u52a8\u6846\u67b6\uff0c\u91cf\u5316\u4e86\u5b50\u77e9\u9635\u5c0f\u6270\u52a8\u5bf9\u5947\u5f02\u503c\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u62fc\u63a5\u77e9\u9635\u7684\u5947\u5f02\u503c\u8c31\u5982\u4f55\u4e0e\u5176\u7ec4\u6210\u77e9\u9635\u7684\u5173\u7cfb\uff0c\u4e3a\u77e9\u9635\u805a\u7c7b\u548c\u538b\u7f29\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5f00\u53d1\u4e86\u6270\u52a8\u6846\u67b6\uff0c\u6269\u5c55Weyl\u4e0d\u7b49\u5f0f\u7b49\u7ecf\u5178\u7ed3\u679c\u5230\u62fc\u63a5\u77e9\u9635\uff0c\u5efa\u7acb\u89e3\u6790\u8fb9\u754c\u3002", "result": "\u82e5\u62fc\u63a5\u77e9\u9635\u5728\u8303\u6570\u4e0a\u63a5\u8fd1\uff0c\u5219\u62fc\u63a5\u540e\u7684\u4e3b\u5bfc\u5947\u5f02\u503c\u4fdd\u6301\u7a33\u5b9a\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u538b\u7f29\u7684\u53ef\u63a7\u6743\u8861\u3002", "conclusion": "\u4e3a\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u3001\u4fe1\u53f7\u5904\u7406\u548c\u6570\u636e\u5efa\u6a21\u4e2d\u7684\u77e9\u9635\u805a\u7c7b\u4e0e\u538b\u7f29\u7b56\u7565\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2505.02050", "pdf": "https://arxiv.org/pdf/2505.02050", "abs": "https://arxiv.org/abs/2505.02050", "authors": ["Kranthi Kumar Talluri", "Anders L. Madsen", "Galia Weidl"], "title": "Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Cut-in maneuvers in high-speed traffic pose critical challenges that can lead\nto abrupt braking and collisions, necessitating safe and efficient lane change\nstrategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate\nlateral evidence with safety assessment models, thereby predicting lane changes\nand ensuring safe cut-in maneuvers effectively. Our proposed framework\ncomprises three key probabilistic hypotheses (lateral evidence, lateral safety,\nand longitudinal safety) that facilitate the decision-making process through\ndynamic data processing and assessments of vehicle positions, lateral\nvelocities, relative distance, and Time-to-Collision (TTC) computations. The\nDBN model's performance compared with other conventional approaches\ndemonstrates superior performance in crash reduction, especially in critical\nhigh-speed scenarios, while maintaining a competitive performance in low-speed\nscenarios. This paves the way for robust, scalable, and efficient safety\nvalidation in automated driving systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408\u6a2a\u5411\u8bc1\u636e\u548c\u5b89\u5168\u8bc4\u4f30\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u8f66\u9053\u53d8\u66f4\u5e76\u786e\u4fdd\u5b89\u5168\u7684\u5207\u5165\u52a8\u4f5c\u3002\u8be5\u6846\u67b6\u5728\u9ad8\u901f\u548c\u4f4e\u901f\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86\u78b0\u649e\u98ce\u9669\u3002", "motivation": "\u9ad8\u901f\u4ea4\u901a\u4e2d\u7684\u5207\u5165\u52a8\u4f5c\u53ef\u80fd\u5bfc\u81f4\u6025\u5239\u6216\u78b0\u649e\uff0c\u4e9f\u9700\u5b89\u5168\u9ad8\u6548\u7684\u8f66\u9053\u53d8\u66f4\u7b56\u7565\u3002", "method": "\u91c7\u7528\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\uff08DBN\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u6a2a\u5411\u8bc1\u636e\u3001\u6a2a\u5411\u5b89\u5168\u548c\u7eb5\u5411\u5b89\u5168\u4e09\u4e2a\u5173\u952e\u6982\u7387\u5047\u8bbe\uff0c\u901a\u8fc7\u52a8\u6001\u6570\u636e\u5904\u7406\u8bc4\u4f30\u8f66\u8f86\u4f4d\u7f6e\u3001\u6a2a\u5411\u901f\u5ea6\u3001\u76f8\u5bf9\u8ddd\u79bb\u548c\u65f6\u95f4\u78b0\u649e\uff08TTC\uff09\u3002", "result": "\u4e0e\u5176\u4ed6\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cDBN\u6a21\u578b\u5728\u9ad8\u901f\u5173\u952e\u573a\u666f\u4e0b\u663e\u8457\u51cf\u5c11\u78b0\u649e\uff0c\u4f4e\u901f\u573a\u666f\u4e0b\u8868\u73b0\u540c\u6837\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01868", "pdf": "https://arxiv.org/pdf/2505.01868", "abs": "https://arxiv.org/abs/2505.01868", "authors": ["Mo Sun", "Siheng Xiong", "Yuankai Cai", "Bowen Zuo"], "title": "Positional Attention for Efficient BERT-Based Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a framework for Named Entity Recognition (NER) leveraging\nthe Bidirectional Encoder Representations from Transformers (BERT) model in\nnatural language processing (NLP). NER is a fundamental task in NLP with broad\napplicability across downstream applications. While BERT has established itself\nas a state-of-the-art model for entity recognition, fine-tuning it from scratch\nfor each new application is computationally expensive and time-consuming. To\naddress this, we propose a cost-efficient approach that integrates positional\nattention mechanisms into the entity recognition process and enables effective\ncustomization using pre-trained parameters. The framework is evaluated on a\nKaggle dataset derived from the Groningen Meaning Bank corpus and achieves\nstrong performance with fewer training epochs. This work contributes to the\nfield by offering a practical solution for reducing the training cost of\nBERT-based NER systems while maintaining high accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4f4d\u7f6e\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u5e76\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u867d\u7136BERT\u5728\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u65b0\u5e94\u7528\u4ece\u5934\u5fae\u8c03\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u4f4d\u7f6e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u53c2\u6570\u8fdb\u884c\u6709\u6548\u5b9a\u5236\uff0c\u51cf\u5c11\u4e86\u4ece\u5934\u8bad\u7ec3\u7684\u9700\u6c42\u3002", "result": "\u5728Groningen Meaning Bank\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u5468\u671f\u66f4\u5c11\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u964d\u4f4eBERT-based NER\u7cfb\u7edf\u7684\u8bad\u7ec3\u6210\u672c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u7387\u3002"}}
{"id": "2505.01437", "pdf": "https://arxiv.org/pdf/2505.01437", "abs": "https://arxiv.org/abs/2505.01437", "authors": ["Hassan Wasswa", "Timothy Lynar", "Hussein Abbass"], "title": "Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Internet of Things (IoT) technology has rapidly gained popularity with\napplications widespread across a variety of industries. However, IoT devices\nhave been recently serving as a porous layer for many malicious attacks to both\npersonal and enterprise information systems with the most famous attacks being\nbotnet-related attacks. The work in this study leveraged Variational\nAuto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet\neffective, models for IoT-botnet detection. The aim is to enhance the detection\nof minority class attack traffic instances which are often missed by machine\nlearning models. The proposed approach is evaluated on a multi-class problem\nsetting for the detection of traffic categories on highly imbalanced datasets.\nThe performance of two deep learning models including the standard feed forward\ndeep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and\nboth recorded commendable results in terms of accuracy, precision, recall and\nF1-score for all traffic classes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6210\u672c\u654f\u611f\u5b66\u4e60\u5f00\u53d1\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\uff0c\u91cd\u70b9\u5173\u6ce8\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u5c11\u6570\u7c7b\u653b\u51fb\u6d41\u91cf\u7684\u68c0\u6d4b\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u6210\u4e3a\u6076\u610f\u653b\u51fb\u7684\u8584\u5f31\u73af\u8282\uff0c\u5c24\u5176\u662f\u50f5\u5c38\u7f51\u7edc\u653b\u51fb\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u68c0\u6d4b\u5c11\u6570\u7c7b\u653b\u51fb\u6d41\u91cf\u3002", "method": "\u7ed3\u5408VAE\u548c\u6210\u672c\u654f\u611f\u5b66\u4e60\uff0c\u8bc4\u4f30\u4e86\u524d\u9988\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548c\u53cc\u5411LSTM\uff08BLSTM\uff09\u5728\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u7684\u5c11\u6570\u7c7b\u653b\u51fb\uff0c\u4e3a\u7269\u8054\u7f51\u5b89\u5168\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02052", "pdf": "https://arxiv.org/pdf/2505.02052", "abs": "https://arxiv.org/abs/2505.02052", "authors": ["Lala Shakti Swarup Ray", "Lars Krupp", "Vitor Fortes Rey", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "title": "TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Sensor-based human activity recognition (HAR) has predominantly focused on\nInertial Measurement Units and vision data, often overlooking the capabilities\nunique to pressure sensors, which capture subtle body dynamics and shifts in\nthe center of mass. Despite their potential for postural and balance-based\nactivities, pressure sensors remain underutilized in the HAR domain due to\nlimited datasets. To bridge this gap, we propose to exploit generative\nfoundation models with pressure-specific HAR techniques. Specifically, we\npresent a bidirectional Text$\\times$Pressure model that uses generative\nfoundation models to interpret pressure data as natural language. TxP\naccomplishes two tasks: (1) Text2Pressure, converting activity text\ndescriptions into pressure sequences, and (2) Pressure2Text, generating\nactivity descriptions and classifications from dynamic pressure maps.\nLeveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on\nour synthetic PressLang dataset, containing over 81,100 text-pressure pairs.\nValidated on real-world data for activities such as yoga and daily tasks, TxP\nprovides novel approaches to data augmentation and classification grounded in\natomic actions. This consequently improved HAR performance by up to 12.4\\% in\nmacro F1 score compared to the state-of-the-art, advancing pressure-based HAR\nwith broader applications and deeper insights into human movement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTxP\u7684\u53cc\u5411\u6587\u672c\u00d7\u538b\u529b\u6a21\u578b\uff0c\u5229\u7528\u751f\u6210\u57fa\u7840\u6a21\u578b\u5c06\u538b\u529b\u6570\u636e\u89e3\u91ca\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u63d0\u5347\u538b\u529b\u4f20\u611f\u5668\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u5f53\u524dHAR\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c\u89c6\u89c9\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u538b\u529b\u4f20\u611f\u5668\u7684\u6f5c\u529b\u3002\u7531\u4e8e\u5176\u80fd\u6355\u6349\u7ec6\u5fae\u8eab\u4f53\u52a8\u6001\u548c\u91cd\u5fc3\u53d8\u5316\uff0c\u538b\u529b\u4f20\u611f\u5668\u5728\u59ff\u6001\u548c\u5e73\u8861\u76f8\u5173\u6d3b\u52a8\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u4e0d\u8db3\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faTxP\u6a21\u578b\uff0c\u5305\u542bText2Pressure\uff08\u5c06\u6d3b\u52a8\u6587\u672c\u63cf\u8ff0\u8f6c\u4e3a\u538b\u529b\u5e8f\u5217\uff09\u548cPressure2Text\uff08\u4ece\u52a8\u6001\u538b\u529b\u56fe\u751f\u6210\u6d3b\u52a8\u63cf\u8ff0\u548c\u5206\u7c7b\uff09\u4e24\u4e2a\u4efb\u52a1\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578bCLIP\u548cLLaMA 2 13B Chat\uff0c\u5728\u5408\u6210\u7684PressLang\u6570\u636e\u96c6\uff0881,100\u4e2a\u6587\u672c-\u538b\u529b\u5bf9\uff09\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728\u745c\u4f3d\u548c\u65e5\u5e38\u4efb\u52a1\u7b49\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0cTxP\u5728\u6570\u636e\u589e\u5f3a\u548c\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b8fF1\u5206\u6570\u6bd4\u73b0\u6709\u6280\u672f\u6700\u9ad8\u63d0\u534712.4%\u3002", "conclusion": "TxP\u4e0d\u4ec5\u63a8\u52a8\u4e86\u538b\u529b\u4f20\u611f\u5668\u5728HAR\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8fd8\u901a\u8fc7\u539f\u5b50\u52a8\u4f5c\u63d0\u4f9b\u4e86\u5bf9\u4eba\u7c7b\u8fd0\u52a8\u7684\u66f4\u6df1\u5165\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877", "abs": "https://arxiv.org/abs/2505.01877", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Anna Marklov\u00e1", "Ond\u0159ej Drobil", "Eva Posp\u00ed\u0161ilov\u00e1"], "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 255 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u80fd\u5e2e\u52a9\u4eba\u4eec\u66f4\u597d\u5730\u533a\u5206\u4eba\u7c7b\u5199\u4f5c\u4e0eAI\u751f\u6210\u7684\u6587\u672c\uff0c\u5e76\u6821\u51c6\u81ea\u6211\u8ba4\u77e5\u80fd\u529b\u3002\u53cd\u9988\u8fd8\u7ea0\u6b63\u4e86\u5bf9AI\u6587\u672c\u98ce\u683c\u7684\u8bef\u89e3\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u662f\u5426\u80fd\u901a\u8fc7\u5b66\u4e60\u533a\u5206AI\u4e0e\u4eba\u7c7b\u5199\u4f5c\uff0c\u5e76\u7814\u7a76\u5176\u4f9d\u8d56\u7684\u5224\u65ad\u6807\u51c6\uff08\u5982\u6587\u672c\u98ce\u683c\u548c\u53ef\u8bfb\u6027\uff09\u3002", "method": "\u4f7f\u7528GPT-4o\u751f\u6210\u6587\u672c\uff0c\u4e0e\u4eba\u7c7b\u6587\u672c\u5bf9\u6bd4\uff0c\u8ba9255\u540d\u6377\u514b\u6bcd\u8bed\u8005\u968f\u673a\u5206\u7ec4\uff08\u6709/\u65e0\u53cd\u9988\uff09\u8fdb\u884c\u5224\u65ad\uff0c\u8bb0\u5f55\u51c6\u786e\u6027\u3001\u4fe1\u5fc3\u7b49\u6570\u636e\u3002", "result": "\u5373\u65f6\u53cd\u9988\u7ec4\u5728\u8bc6\u522b\u51c6\u786e\u6027\u548c\u4fe1\u5fc3\u6821\u51c6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u53cd\u9988\u7ea0\u6b63\u4e86\u6700\u521d\u5bf9AI\u6587\u672c\u98ce\u683c\u7684\u9519\u8bef\u5047\u8bbe\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u548c\u53cd\u9988\uff0c\u4eba\u4eec\u53ef\u4ee5\u6709\u6548\u533a\u5206\u4eba\u7c7b\u4e0eAI\u6587\u672c\uff0c\u5e76\u6539\u5584\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u8fd9\u5bf9\u6559\u80b2\u9886\u57df\u5c24\u5176\u91cd\u8981\u3002"}}
{"id": "2505.01438", "pdf": "https://arxiv.org/pdf/2505.01438", "abs": "https://arxiv.org/abs/2505.01438", "authors": ["Tengfei Xing", "Xiaodan Ren", "Jie Li"], "title": "Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Material stress analysis is a critical aspect of material design and\nperformance optimization. Under dynamic loading, the global stress evolution in\nmaterials exhibits complex spatiotemporal characteristics, especially in\ntwo-phase random materials (TRMs). Such kind of material failure is often\nassociated with stress concentration, and the phase boundaries are key\nlocations where stress concentration occurs. In practical engineering\napplications, the spatiotemporal resolution of acquired microstructural data\nand its dynamic stress evolution is often limited. This poses challenges for\ndeep learning methods in generating high-resolution spatiotemporal stress\nfields, particularly for accurately capturing stress concentration regions. In\nthis study, we propose a framework for global stress generation and\nspatiotemporal super-resolution in TRMs under dynamic loading. First, we\nintroduce a diffusion model-based approach, named as Spatiotemporal Stress\nDiffusion (STS-diffusion), for generating global spatiotemporal stress data.\nThis framework incorporates Space-Time U-Net (STU-net), and we systematically\ninvestigate the impact of different attention positions on model accuracy.\nNext, we develop a physics-informed network for spatiotemporal\nsuper-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed\nOperator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning\nmethod. The influence of data-driven and physics-informed loss function weights\non model accuracy is explored in detail. Benefiting from physics-based\nconstraints, ST-SRPINN requires only low-resolution stress field data during\ntraining and can upscale the spatiotemporal resolution of stress fields to\narbitrary magnifications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u7269\u7406\u4fe1\u606f\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e24\u76f8\u968f\u673a\u6750\u6599\u4e2d\u751f\u6210\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u5e94\u529b\u573a\uff0c\u5e76\u8be6\u7ec6\u7814\u7a76\u4e86\u4e0d\u540c\u6ce8\u610f\u529b\u4f4d\u7f6e\u548c\u635f\u5931\u51fd\u6570\u6743\u91cd\u5bf9\u6a21\u578b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u52a8\u6001\u52a0\u8f7d\u4e0b\u4e24\u76f8\u968f\u673a\u6750\u6599\u7684\u5e94\u529b\u5206\u6790\u662f\u6750\u6599\u8bbe\u8ba1\u548c\u6027\u80fd\u4f18\u5316\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u6709\u9650\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5e94\u529b\u96c6\u4e2d\u533a\u57df\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9ad8\u5206\u8fa8\u7387\u5e94\u529b\u573a\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u63d0\u51faSTS-diffusion\uff08\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u5e94\u529b\u751f\u6210\uff09\u548cST-SRPINN\uff08\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u65f6\u7a7a\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u524d\u8005\u7528\u4e8e\u751f\u6210\u5168\u5c40\u5e94\u529b\u6570\u636e\uff0c\u540e\u8005\u7528\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u5206\u8fa8\u7387\u3002", "result": "STS-diffusion\u901a\u8fc7STU-net\u63d0\u5347\u4e86\u5e94\u529b\u573a\u7684\u751f\u6210\u7cbe\u5ea6\uff0cST-SRPINN\u5229\u7528\u7269\u7406\u7ea6\u675f\u4ec5\u9700\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u4efb\u610f\u500d\u6570\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u5e94\u529b\u573a\u751f\u6210\u7684\u96be\u9898\uff0c\u5c24\u5176\u5728\u6355\u6349\u5e94\u529b\u96c6\u4e2d\u533a\u57df\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6750\u6599\u52a8\u6001\u5e94\u529b\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.02062", "pdf": "https://arxiv.org/pdf/2505.02062", "abs": "https://arxiv.org/abs/2505.02062", "authors": ["Prathamesh Muzumdar", "Apoorva Muley", "Kuldeep Singh", "Sumanth Cheemalapati"], "title": "Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)", "categories": ["cs.AI"], "comment": null, "summary": "The adoption of Artificial Intelligence (AI) in the healthcare service\nindustry presents numerous ethical challenges, yet current frameworks often\nfail to offer a comprehensive, empirical understanding of the multidimensional\nfactors influencing ethical AI integration. Addressing this critical research\ngap, this study introduces the Multi-Dimensional Ethical AI Adoption Model\n(MEAAM), a novel theoretical framework that categorizes 13 critical ethical\nvariables across four foundational dimensions of Ethical AI Fair AI,\nResponsible AI, Explainable AI, and Sustainable AI. These dimensions are\nfurther analyzed through three core ethical lenses: epistemic concerns (related\nto knowledge, transparency, and system trustworthiness), normative concerns\n(focused on justice, autonomy, dignity, and moral obligations), and overarching\nconcerns (highlighting global, systemic, and long-term ethical implications).\nThis study adopts a quantitative, cross-sectional research design using survey\ndata collected from healthcare professionals and analyzed via Partial Least\nSquares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study\nempirically investigates the influence of these ethical constructs on two\noutcomes Operational AI Adoption and Systemic AI Adoption. Results indicate\nthat normative concerns most significantly drive operational adoption\ndecisions, while overarching concerns predominantly shape systemic adoption\nstrategies and governance frameworks. Epistemic concerns play a facilitative\nrole, enhancing the impact of ethical design principles on trust and\ntransparency in AI systems. By validating the MEAAM framework, this research\nadvances a holistic, actionable approach to ethical AI adoption in healthcare\nand provides critical insights for policymakers, technologists, and healthcare\nadministrators striving to implement ethically grounded AI solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMEAAM\u7684\u591a\u7ef4\u5ea6\u4f26\u7406AI\u91c7\u7528\u6a21\u578b\uff0c\u901a\u8fc7\u5b9a\u91cf\u7814\u7a76\u5206\u679013\u4e2a\u5173\u952e\u4f26\u7406\u53d8\u91cf\u5bf9\u533b\u7597AI\u91c7\u7528\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89c4\u8303\u6027\u5173\u6ce8\u5728\u64cd\u4f5c\u91c7\u7528\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u800c\u7cfb\u7edf\u6027\u91c7\u7528\u5219\u66f4\u591a\u53d7\u5168\u5c40\u6027\u5173\u6ce8\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dAI\u5728\u533b\u7597\u884c\u4e1a\u7684\u4f26\u7406\u6846\u67b6\u7f3a\u4e4f\u7efc\u5408\u6027\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u4f26\u7406AI\u91c7\u7528\u6a21\u578b\u3002", "method": "\u91c7\u7528\u5b9a\u91cf\u3001\u6a2a\u65ad\u9762\u7814\u7a76\u8bbe\u8ba1\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u533b\u7597\u4e13\u4e1a\u4eba\u5458\uff0c\u5e76\u4f7f\u7528PLS-SEM\u8fdb\u884c\u5206\u6790\u3002", "result": "\u89c4\u8303\u6027\u5173\u6ce8\u663e\u8457\u5f71\u54cd\u64cd\u4f5c\u91c7\u7528\uff0c\u5168\u5c40\u6027\u5173\u6ce8\u4e3b\u5bfc\u7cfb\u7edf\u6027\u91c7\u7528\uff0c\u8ba4\u77e5\u6027\u5173\u6ce8\u589e\u5f3a\u4f26\u7406\u8bbe\u8ba1\u539f\u5219\u5bf9AI\u7cfb\u7edf\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u7684\u4f5c\u7528\u3002", "conclusion": "MEAAM\u6846\u67b6\u4e3a\u533b\u7597\u884c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u53ef\u64cd\u4f5c\u7684\u4f26\u7406AI\u91c7\u7528\u65b9\u6cd5\uff0c\u4e3a\u51b3\u7b56\u8005\u3001\u6280\u672f\u4e13\u5bb6\u548c\u7ba1\u7406\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2505.01883", "pdf": "https://arxiv.org/pdf/2505.01883", "abs": "https://arxiv.org/abs/2505.01883", "authors": ["Yiwen Lu", "Siheng Xiong", "Zhaowei Li"], "title": "Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams", "categories": ["cs.CL"], "comment": null, "summary": "We present a framework for large-scale sentiment and topic analysis of\nTwitter discourse. Our pipeline begins with targeted data collection using\nconflict-specific keywords, followed by automated sentiment labeling via\nmultiple pre-trained models to improve annotation robustness. We examine the\nrelationship between sentiment and contextual features such as timestamp,\ngeolocation, and lexical content. To identify latent themes, we apply Latent\nDirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and\nmetadata attributes. Finally, we develop an interactive visualization interface\nto support exploration of sentiment trends and topic distributions across time\nand regions. This work contributes a scalable methodology for social media\nanalysis in dynamic geopolitical contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u7279\u60c5\u611f\u548c\u4e3b\u9898\u5206\u6790\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6a21\u578b\u6807\u6ce8\u548cLDA\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u5f00\u53d1\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u3002", "motivation": "\u7814\u7a76\u63a8\u7279\u8bdd\u8bed\u4e2d\u7684\u60c5\u611f\u4e0e\u4e0a\u4e0b\u6587\u7279\u5f81\uff08\u5982\u65f6\u95f4\u3001\u5730\u7406\u4f4d\u7f6e\u7b49\uff09\u7684\u5173\u7cfb\uff0c\u652f\u6301\u52a8\u6001\u5730\u7f18\u653f\u6cbb\u80cc\u666f\u4e0b\u7684\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u3002", "method": "\u4f7f\u7528\u51b2\u7a81\u76f8\u5173\u5173\u952e\u8bcd\u6536\u96c6\u6570\u636e\uff0c\u591a\u9884\u8bad\u7ec3\u6a21\u578b\u6807\u6ce8\u60c5\u611f\uff0cLDA\u5206\u6790\u4e3b\u9898\uff0c\u5e76\u5f00\u53d1\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u591f\u63a2\u7d22\u60c5\u611f\u8d8b\u52bf\u548c\u4e3b\u9898\u5728\u65f6\u95f4\u548c\u5730\u57df\u4e0a\u7684\u5206\u5e03\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001\u5730\u7f18\u653f\u6cbb\u73af\u5883\u4e2d\u7684\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002"}}
{"id": "2505.01440", "pdf": "https://arxiv.org/pdf/2505.01440", "abs": "https://arxiv.org/abs/2505.01440", "authors": ["Alkis Sygkounas", "Ioannis Athanasiadis", "Andreas Persson", "Michael Felsberg", "Amy Loutfi"], "title": "Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV) 2025, 8 pages", "summary": "Integrating human expertise with machine learning is crucial for applications\ndemanding high accuracy and safety, such as autonomous driving. This study\nintroduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop\n(HITL) approach that enhances Reinforcement Learning (RL) by merging human\ninsights directly into the RL training process, improving model performance.\nOur proposed iDDQN method modifies the Q-value update equation to integrate\nhuman and agent actions, establishing a collaborative approach for policy\ndevelopment. Additionally, we present an offline evaluative framework that\nsimulates the agent's trajectory as if no human intervention had occurred, to\nassess the effectiveness of human interventions. Empirical results in simulated\nautonomous driving scenarios demonstrate that iDDQN outperforms established\napproaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from\nDemonstrations (DQfD), and vanilla DRL in leveraging human expertise for\nimproving performance and adaptability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiDDQN\u7684\u4ea4\u4e92\u5f0f\u53cc\u6df1\u5ea6Q\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u4e13\u5bb6\u77e5\u8bc6\u76f4\u63a5\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\u5bf9\u4e8e\u9ad8\u7cbe\u5ea6\u4e0e\u5b89\u5168\u9700\u6c42\u7684\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7HITL\u65b9\u6cd5\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "method": "iDDQN\u6539\u8fdbQ\u503c\u66f4\u65b0\u65b9\u7a0b\u4ee5\u6574\u5408\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u52a8\u4f5c\uff0c\u5e76\u63d0\u51fa\u79bb\u7ebf\u8bc4\u4f30\u6846\u67b6\u6a21\u62df\u65e0\u5e72\u9884\u65f6\u7684\u8f68\u8ff9\uff0c\u8bc4\u4f30\u4eba\u7c7b\u5e72\u9884\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ciDDQN\u5728\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8eBC\u3001HG-DAgger\u3001DQfD\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u9ad8\u6548\u5229\u7528\u4eba\u7c7b\u77e5\u8bc6\u3002", "conclusion": "iDDQN\u901a\u8fc7\u4eba\u7c7b\u4e0e\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02076", "pdf": "https://arxiv.org/pdf/2505.02076", "abs": "https://arxiv.org/abs/2505.02076", "authors": ["Milapji Singh Gill", "Javal Vyas", "Artan Markaj", "Felix Gehlhoff", "Mehmet Mercang\u00f6z"], "title": "Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Advances in Automation and Artificial Intelligence continue to enhance the\nautonomy of process plants in handling various operational scenarios. However,\ncertain tasks, such as fault handling, remain challenging, as they rely heavily\non human expertise. This highlights the need for systematic, knowledge-based\nmethods. To address this gap, we propose a methodological framework that\nintegrates Large Language Model (LLM) agents with a Digital Twin environment.\nThe LLM agents continuously interpret system states and initiate control\nactions, including responses to unexpected faults, with the goal of returning\nthe system to normal operation. In this context, the Digital Twin acts both as\na structured repository of plant-specific engineering knowledge for agent\nprompting and as a simulation platform for the systematic validation and\nverification of the generated corrective control actions. The evaluation using\na mixing module of a process plant demonstrates that the proposed framework is\ncapable not only of autonomously controlling the mixing module, but also of\ngenerating effective corrective actions to mitigate a pipe clogging with only a\nfew reprompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u4e0e\u6570\u5b57\u5b6a\u751f\u73af\u5883\u96c6\u6210\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5904\u7406\u6d41\u7a0b\u5de5\u5382\u4e2d\u7684\u6545\u969c\uff0c\u51cf\u5c11\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\u3002", "motivation": "\u6d41\u7a0b\u5de5\u5382\u7684\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u667a\u80fd\u867d\u7136\u6301\u7eed\u8fdb\u6b65\uff0c\u4f46\u6545\u969c\u5904\u7406\u7b49\u4efb\u52a1\u4ecd\u9ad8\u5ea6\u4f9d\u8d56\u4eba\u7c7b\u4e13\u5bb6\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7LLM\u4ee3\u7406\u6301\u7eed\u89e3\u8bfb\u7cfb\u7edf\u72b6\u6001\u5e76\u542f\u52a8\u63a7\u5236\u52a8\u4f5c\uff0c\u6570\u5b57\u5b6a\u751f\u5219\u4f5c\u4e3a\u5de5\u7a0b\u77e5\u8bc6\u5e93\u548c\u4eff\u771f\u5e73\u53f0\u9a8c\u8bc1\u63a7\u5236\u52a8\u4f5c\u3002", "result": "\u5728\u6df7\u5408\u6a21\u5757\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u81ea\u4e3b\u63a7\u5236\u6a21\u5757\uff0c\u8fd8\u80fd\u901a\u8fc7\u5c11\u91cf\u91cd\u65b0\u63d0\u793a\u751f\u6210\u6709\u6548\u7ea0\u6b63\u63aa\u65bd\u7f13\u89e3\u7ba1\u9053\u5835\u585e\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u901a\u8fc7LLM\u4e0e\u6570\u5b57\u5b6a\u751f\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6d41\u7a0b\u5de5\u5382\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u4e3b\u6027\u3002"}}
{"id": "2505.01900", "pdf": "https://arxiv.org/pdf/2505.01900", "abs": "https://arxiv.org/abs/2505.01900", "authors": ["Mazal Bethany", "Nishant Vishwamitra", "Cho-Yu Jason Chiang", "Peyman Najafirad"], "title": "CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation", "categories": ["cs.CL"], "comment": null, "summary": "Automated evidence-based misinformation detection systems, which evaluate the\nveracity of short claims against evidence, lack comprehensive analysis of their\nadversarial vulnerabilities. Existing black-box text-based adversarial attacks\nare ill-suited for evidence-based misinformation detection systems, as these\nattacks primarily focus on token-level substitutions involving gradient or\nlogit-based optimization strategies, which are incapable of fooling the\nmulti-component nature of these detection systems. These systems incorporate\nboth retrieval and claim-evidence comparison modules, which requires attacks to\nbreak the retrieval of evidence and/or the comparison module so that it draws\nincorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach\nthat employs a two-agent system, a Prompt Optimization Agent and an Attacker\nAgent, to create adversarial claim rewritings that manipulate evidence\nretrieval and mislead claim-evidence comparison, effectively bypassing the\nsystem without altering the meaning of the claim. The Attacker Agent produces\nsemantically equivalent rewrites that attempt to mislead detectors, while the\nPrompt Optimization Agent analyzes failed attack attempts and refines the\nprompt of the Attacker to guide subsequent rewrites. This enables larger\nstructural and stylistic transformations of the text rather than token-level\nsubstitutions, adapting the magnitude of changes based on previous outcomes.\nUnlike existing approaches, CAMOUFLAGE optimizes its attack solely based on\nbinary model decisions to guide its rewriting process, eliminating the need for\nclassifier logits or extensive querying. We evaluate CAMOUFLAGE on four\nsystems, including two recent academic systems and two real-world APIs, with an\naverage attack success rate of 46.92\\% while preserving textual coherence and\nsemantic equivalence to the original claims.", "AI": {"tldr": "CAMOUFLAGE\u662f\u4e00\u79cd\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u7684\u65b9\u5f0f\u751f\u6210\u8bed\u4e49\u7b49\u6548\u4f46\u80fd\u8bef\u5bfc\u8bc1\u636e\u68c0\u7d22\u7cfb\u7edf\u7684\u6539\u5199\uff0c\u6210\u529f\u7ed5\u8fc7\u81ea\u52a8\u5316\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u4e86\u8bc1\u636e\u9a71\u52a8\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u7ed5\u8fc7\u5176\u591a\u7ec4\u4ef6\u7ed3\u6784\uff0c\u6545\u63d0\u51fa\u65b0\u65b9\u6cd5CAMOUFLAGE\u3002", "method": "CAMOUFLAGE\u4f7f\u7528\u4e24\u4ee3\u7406\u7cfb\u7edf\uff08\u63d0\u793a\u4f18\u5316\u4ee3\u7406\u548c\u653b\u51fb\u4ee3\u7406\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u8bed\u4e49\u7b49\u6548\u7684\u6539\u5199\uff0c\u4ee5\u8bef\u5bfc\u68c0\u7d22\u548c\u6bd4\u8f83\u6a21\u5757\u3002", "result": "\u5728\u56db\u4e2a\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u4e3a46.92%\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u8fde\u8d2f\u6027\u548c\u8bed\u4e49\u7b49\u6548\u6027\u3002", "conclusion": "CAMOUFLAGE\u8bc1\u660e\u4e86\u8bc1\u636e\u68c0\u6d4b\u7cfb\u7edf\u7684\u5bf9\u6297\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u65b9\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.01445", "pdf": "https://arxiv.org/pdf/2505.01445", "abs": "https://arxiv.org/abs/2505.01445", "authors": ["Muhammad Muaz", "Sameed Sajid", "Tobias Schulze", "Chang Liu", "Nils Klasen", "Benny Drescher"], "title": "Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "If a product deviates from its desired properties in the injection moulding\nprocess, its root cause analysis can be aided by models that relate the input\nmachine settings with the output quality characteristics. The machine learning\nmodels tested in the quality prediction are mostly black boxes; therefore, no\ndirect explanation of their prognosis is given, which restricts their\napplicability in the quality control. The previously attempted explainability\nmethods are either restricted to tree-based algorithms only or do not emphasize\non the fact that some explainability methods can lead to wrong root cause\nidentification of a product's deviation from its desired properties. This study\nfirst shows that the interactions among the multiple input machine settings do\nexist in real experimental data collected as per a central composite design.\nThen, the model-agnostic explainable AI methods are compared for the first time\nto show that different explainability methods indeed lead to different feature\nimpact analysis in injection moulding. Moreover, it is shown that the better\nfeature attribution translates to the correct cause identification and\nactionable insights for the injection moulding process. Being model agnostic,\nexplanations on both random forest and multilayer perceptron are performed for\nthe cause analysis, as both models have the mean absolute percentage error of\nless than 0.05% on the experimental dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6ce8\u5851\u8fc7\u7a0b\u4e2d\u4ea7\u54c1\u8d28\u91cf\u9884\u6d4b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5bf9\u7279\u5f81\u5f71\u54cd\u5206\u6790\u7684\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u66f4\u597d\u7684\u7279\u5f81\u5f52\u56e0\u80fd\u6b63\u786e\u8bc6\u522b\u539f\u56e0\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u591a\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u76f4\u63a5\u89e3\u91ca\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d28\u91cf\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002\u6b64\u524d\u7684\u65b9\u6cd5\u8981\u4e48\u4ec5\u9002\u7528\u4e8e\u6811\u57fa\u7b97\u6cd5\uff0c\u8981\u4e48\u672a\u5f3a\u8c03\u67d0\u4e9b\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7684\u6839\u672c\u539f\u56e0\u8bc6\u522b\u3002", "method": "\u7814\u7a76\u9996\u5148\u901a\u8fc7\u4e2d\u5fc3\u590d\u5408\u8bbe\u8ba1\u6536\u96c6\u7684\u771f\u5b9e\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u8f93\u5165\u673a\u5668\u8bbe\u7f6e\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u7136\u540e\u9996\u6b21\u6bd4\u8f83\u4e86\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5982\u4f55\u5bfc\u81f4\u4e0d\u540c\u7684\u7279\u5f81\u5f71\u54cd\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u786e\u5b9e\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7684\u7279\u5f81\u5f71\u54cd\u5206\u6790\uff0c\u4e14\u66f4\u597d\u7684\u7279\u5f81\u5f52\u56e0\u6709\u52a9\u4e8e\u6b63\u786e\u8bc6\u522b\u539f\u56e0\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u968f\u673a\u68ee\u6797\u548c\u591a\u5c42\u611f\u77e5\u5668\u5728\u5b9e\u9a8c\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u5747\u4f4e\u4e8e0.05%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9009\u62e9\u9002\u5f53\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u6b63\u786e\u8bc6\u522b\u4ea7\u54c1\u504f\u79bb\u7406\u60f3\u5c5e\u6027\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u4e3a\u6ce8\u5851\u8fc7\u7a0b\u63d0\u4f9b\u53ef\u884c\u7684\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2505.02087", "pdf": "https://arxiv.org/pdf/2505.02087", "abs": "https://arxiv.org/abs/2505.02087", "authors": ["Zaifu Zhan", "Shuang Zhou", "Xiaoshan Zhou", "Yongkang Xiao", "Jun Wang", "Jiawen Deng", "He Zhu", "Yu Hou", "Rui Zhang"], "title": "Retrieval-augmented in-context learning for multimodal large language models in disease classification", "categories": ["cs.AI"], "comment": "17 Pages, 1 figure, 7 tables", "summary": "Objectives: We aim to dynamically retrieve informative demonstrations,\nenhancing in-context learning in multimodal large language models (MLLMs) for\ndisease classification.\n  Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL)\nframework, which integrates retrieval-augmented generation (RAG) and in-context\nlearning (ICL) to adaptively select demonstrations with similar disease\npatterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines\nembeddings from diverse encoders, including ResNet, BERT, BioBERT, and\nClinicalBERT, to retrieve appropriate demonstrations, and constructs\nconversational prompts optimized for ICL. We evaluated the framework on two\nreal-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its\nperformance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies,\nsimilarity metrics, and varying numbers of demonstrations.\n  Results: RAICL consistently improved classification performance. Accuracy\nincreased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest\nX-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs\nbeing stronger than images alone. The richness of information embedded in each\nmodality will determine which embedding model can be used to get better\nresults. Few-shot experiments showed that increasing the number of retrieved\nexamples further enhanced performance. Across different similarity metrics,\nEuclidean distance achieved the highest accuracy while cosine similarity\nyielded better macro-F1 scores. RAICL demonstrated consistent improvements\nacross various MLLMs, confirming its robustness and versatility.\n  Conclusions: RAICL provides an efficient and scalable approach to enhance\nin-context learning in MLLMs for multimodal disease classification.", "AI": {"tldr": "RAICL\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u75be\u75c5\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u65e8\u5728\u52a8\u6001\u68c0\u7d22\u4fe1\u606f\u4e30\u5bcc\u7684\u6f14\u793a\u6837\u672c\uff0c\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u75be\u75c5\u5206\u7c7b\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u79cd\u7f16\u7801\u5668\uff08\u5982ResNet\u3001BERT\u7b49\uff09\u7684\u5d4c\u5165\u5411\u91cf\u68c0\u7d22\u76f8\u4f3c\u75be\u75c5\u6a21\u5f0f\u7684\u6f14\u793a\u6837\u672c\uff0c\u5e76\u6784\u5efa\u5bf9\u8bdd\u63d0\u793a\u3002", "result": "\u5728TCGA\u548cIU Chest X-ray\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u81f30.8368\u548c0.8658\uff0c\u591a\u6a21\u6001\u8f93\u5165\u4f18\u4e8e\u5355\u6a21\u6001\uff0c\u6b27\u6c0f\u8ddd\u79bb\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u4e0d\u540c\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "RAICL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u75be\u75c5\u5206\u7c7b\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u8868\u73b0\u3002"}}
{"id": "2505.01967", "pdf": "https://arxiv.org/pdf/2505.01967", "abs": "https://arxiv.org/abs/2505.01967", "authors": ["Jiatao Li", "Yanheng Li", "Xiaojun Wan"], "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) have become integral to daily life, widely\nadopted in communication, decision-making, and information retrieval, raising\ncritical questions about how these systems implicitly form and express\nsocio-cognitive attitudes or \"worldviews\". While existing research extensively\naddresses demographic and ethical biases, broader dimensions-such as attitudes\ntoward authority, equality, autonomy, and fate-remain under-explored. In this\npaper, we introduce the Social Worldview Taxonomy (SWT), a structured framework\ngrounded in Cultural Theory, operationalizing four canonical worldviews\n(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable\nsub-dimensions. Using SWT, we empirically identify distinct and interpretable\ncognitive profiles across 28 diverse LLMs. Further, inspired by Social\nReferencing Theory, we experimentally demonstrate that explicit social cues\nsystematically shape these cognitive attitudes, revealing both general response\npatterns and nuanced model-specific variations. Our findings enhance the\ninterpretability of LLMs by revealing implicit socio-cognitive biases and their\nresponsiveness to social feedback, thus guiding the development of more\ntransparent and socially responsible language technologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u793e\u4f1a\u4e16\u754c\u89c2\u5206\u7c7b\u6cd5\uff08SWT\uff09\uff0c\u7528\u4e8e\u6d4b\u91cfLLMs\u5728\u56db\u79cd\u5178\u578b\u4e16\u754c\u89c2\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u793e\u4f1a\u7ebf\u7d22\u53ef\u4ee5\u7cfb\u7edf\u6027\u5f71\u54cd\u8fd9\u4e9b\u6001\u5ea6\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u9690\u5f0f\u5f62\u6210\u548c\u8868\u8fbe\u793e\u4f1a\u8ba4\u77e5\u6001\u5ea6\u6216\u201c\u4e16\u754c\u89c2\u201d\uff0c\u5c24\u5176\u5173\u6ce8\u4ee5\u5f80\u7814\u7a76\u4e2d\u672a\u5145\u5206\u63a2\u7d22\u7684\u6743\u5a01\u3001\u5e73\u7b49\u3001\u81ea\u6cbb\u548c\u547d\u8fd0\u7b49\u7ef4\u5ea6\u3002", "method": "\u57fa\u4e8e\u6587\u5316\u7406\u8bba\u6784\u5efa\u793e\u4f1a\u4e16\u754c\u89c2\u5206\u7c7b\u6cd5\uff08SWT\uff09\uff0c\u5e76\u572828\u79cdLLMs\u4e0a\u5b9e\u8bc1\u8bc6\u522b\u5176\u8ba4\u77e5\u7279\u5f81\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u793e\u4f1a\u7ebf\u7d22\u5bf9\u8ba4\u77e5\u6001\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e86LLMs\u4e4b\u95f4\u663e\u8457\u4e14\u53ef\u89e3\u91ca\u7684\u8ba4\u77e5\u5dee\u5f02\uff0c\u5e76\u8bc1\u5b9e\u793e\u4f1a\u53cd\u9988\u4f1a\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u8fd9\u4e9b\u6001\u5ea6\uff0c\u63ed\u793a\u4e86\u666e\u904d\u89c4\u5f8b\u548c\u6a21\u578b\u7279\u5b9a\u53d8\u4f53\u3002", "conclusion": "\u7814\u7a76\u63d0\u5347\u4e86LLMs\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u4e86\u5176\u9690\u542b\u7684\u793e\u4f1a\u8ba4\u77e5\u504f\u89c1\u53ca\u5bf9\u793e\u4f1a\u53cd\u9988\u7684\u54cd\u5e94\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u900f\u660e\u3001\u8d1f\u8d23\u4efb\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2505.01448", "pdf": "https://arxiv.org/pdf/2505.01448", "abs": "https://arxiv.org/abs/2505.01448", "authors": ["Shengkai Chen", "Yifang Yin", "Jinming Cao", "Shili Xiang", "Zhenguang Liu", "Roger Zimmermann"], "title": "OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models", "categories": ["cs.LG", "cs.MM"], "comment": null, "summary": "Audio-visual segmentation aims to separate sounding objects from videos by\npredicting pixel-level masks based on audio signals. Existing methods primarily\nconcentrate on closed-set scenarios and direct audio-visual alignment and\nfusion, which limits their capability to generalize to new, unseen situations.\nIn this paper, we propose OpenAVS, a novel training-free language-based\napproach that, for the first time, effectively aligns audio and visual\nmodalities using text as a proxy for open-vocabulary Audio-Visual Segmentation\n(AVS). Equipped with multimedia foundation models, OpenAVS directly infers\nmasks through 1) audio-to-text prompt generation, 2) LLM-guided prompt\ntranslation, and 3) text-to-visual sounding object segmentation. The objective\nof OpenAVS is to establish a simple yet flexible architecture that relies on\nthe most appropriate foundation models by fully leveraging their capabilities\nto enable more effective knowledge transfer to the downstream AVS task.\nMoreover, we present a model-agnostic framework OpenAVS-ST that enables the\nintegration of OpenAVS with any advanced supervised AVS model via pseudo-label\nbased self-training. This approach enhances performance by effectively\nutilizing large-scale unlabeled data when available. Comprehensive experiments\non three benchmark datasets demonstrate the superior performance of OpenAVS. It\nsurpasses existing unsupervised, zero-shot, and few-shot AVS methods by a\nsignificant margin, achieving absolute performance gains of approximately 9.4%\nand 10.9% in mIoU and F-score, respectively, in challenging scenarios.", "AI": {"tldr": "OpenAVS\u5229\u7528\u6587\u672c\u4f5c\u4e3a\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u5bf9\u9f50\u7684\u4ee3\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bed\u8a00\u9a71\u52a8\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u89c6\u89c9\u5206\u5272\u5728\u65b0\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u89c6\u89c9\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5c01\u95ed\u96c6\u573a\u666f\u548c\u76f4\u63a5\u7684\u6a21\u6001\u5bf9\u9f50\u4e0e\u878d\u5408\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u901a\u8fc7\u97f3\u9891\u5230\u6587\u672c\u63d0\u793a\u751f\u6210\u3001LLM\u5f15\u5bfc\u7684\u63d0\u793a\u7ffb\u8bd1\u548c\u6587\u672c\u5230\u89c6\u89c9\u53d1\u58f0\u5bf9\u8c61\u5206\u5272\u4e09\u4e2a\u6b65\u9aa4\u5b9e\u73b0\u5206\u5272\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cmIoU\u548cF-score\u5206\u522b\u63d0\u9ad8\u4e86\u7ea69.4%\u548c10.9%\u3002", "conclusion": "OpenAVS\u662f\u4e00\u79cd\u7b80\u5355\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u5b9e\u73b0\u4e86\u77e5\u8bc6\u7684\u9ad8\u6548\u8fc1\u79fb\uff0c\u5e76\u5728\u81ea\u8bad\u7ec3\u6846\u67b6\u4e2d\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.02099", "pdf": "https://arxiv.org/pdf/2505.02099", "abs": "https://arxiv.org/abs/2505.02099", "authors": ["Zeyu Zhang", "Quanyu Dai", "Xu Chen", "Rui Li", "Zhongyang Li", "Zhenhua Dong"], "title": "MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents", "categories": ["cs.AI"], "comment": "Just accepted by TheWebConf'25 Resource Track", "summary": "Recently, large language model based (LLM-based) agents have been widely\napplied across various fields. As a critical part, their memory capabilities\nhave captured significant interest from both industrial and academic\ncommunities. Despite the proposal of many advanced memory models in recent\nresearch, however, there remains a lack of unified implementations under a\ngeneral framework. To address this issue, we develop a unified and modular\nlibrary for developing advanced memory models of LLM-based agents, called\nMemEngine. Based on our framework, we implement abundant memory models from\nrecent research works. Additionally, our library facilitates convenient and\nextensible memory development, and offers user-friendly and pluggable memory\nusage. For benefiting our community, we have made our project publicly\navailable at https://github.com/nuster1128/MemEngine.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aMemEngine\u7684\u7edf\u4e00\u6a21\u5757\u5316\u5e93\uff0c\u7528\u4e8e\u6784\u5efa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u9ad8\u7ea7\u8bb0\u5fc6\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u5b9e\u73b0\u6846\u67b6\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8bb0\u5fc6\u80fd\u529b\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u5b9e\u73b0\u6846\u67b6\uff0c\u56e0\u6b64\u5f00\u53d1MemEngine\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aMemEngine\u7684\u7edf\u4e00\u6a21\u5757\u5316\u5e93\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u5148\u8fdb\u8bb0\u5fc6\u6a21\u578b\uff0c\u652f\u6301\u4fbf\u6377\u6269\u5c55\u548c\u7528\u6237\u53cb\u597d\u7684\u5373\u63d2\u5373\u7528\u529f\u80fd\u3002", "result": "MemEngine\u5e93\u5df2\u5b9e\u73b0\u591a\u79cd\u7814\u7a76\u4e2d\u7684\u8bb0\u5fc6\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6613\u4e8e\u6269\u5c55\u548c\u4f7f\u7528\u7684\u63a5\u53e3\uff0c\u9879\u76ee\u5df2\u5f00\u6e90\u3002", "conclusion": "MemEngine\u4e3aLLM\u4ee3\u7406\u7684\u8bb0\u5fc6\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2505.01980", "pdf": "https://arxiv.org/pdf/2505.01980", "abs": "https://arxiv.org/abs/2505.01980", "authors": ["Theo Guidroz", "Diego Ardila", "Jimmy Li", "Adam Mansour", "Paul Jhun", "Nina Gonzalez", "Xiang Ji", "Mike Sanchez", "Sujay Kakarmath", "Mathias MJ Bellaiche", "Miguel \u00c1ngel Garrido", "Faruk Ahmed", "Divyansh Choudhary", "Jay Hartford", "Chenwei Xu", "Henry Javier Serrano Echeverria", "Yifan Wang", "Jeff Shaffer", "Eric", "Cao", "Yossi Matias", "Avinatan Hassidim", "Dale R Webster", "Yun Liu", "Sho Fujiwara", "Peggy Bui", "Quang Duong"], "title": "LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load", "categories": ["cs.CL"], "comment": null, "summary": "Information on the web, such as scientific publications and Wikipedia, often\nsurpasses users' reading level. To help address this, we used a self-refinement\napproach to develop a LLM capability for minimally lossy text simplification.\nTo validate our approach, we conducted a randomized study involving 4563\nparticipants and 31 texts spanning 6 broad subject areas: PubMed (biomedical\nscientific articles), biology, law, finance, literature/philosophy, and\naerospace/computer science. Participants were randomized to viewing original or\nsimplified texts in a subject area, and answered multiple-choice questions\n(MCQs) that tested their comprehension of the text. The participants were also\nasked to provide qualitative feedback such as task difficulty. Our results\nindicate that participants who read the simplified text answered more MCQs\ncorrectly than their counterparts who read the original text (3.9% absolute\nincrease, p<0.05). This gain was most striking with PubMed (14.6%), while more\nmoderate gains were observed for finance (5.5%), aerospace/computer science\n(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether\nparticipants could refer back to the text while answering MCQs. The absolute\naccuracy decreased by up to ~9% for both original and simplified setups where\nparticipants could not refer back to the text, but the ~4% overall improvement\npersisted. Finally, participants' self-reported perceived ease based on a\nsimplified NASA Task Load Index was greater for those who read the simplified\ntext (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,\ninvolving an order of magnitude more participants than prior works,\ndemonstrates the potential of LLMs to make complex information easier to\nunderstand. Our work aims to enable a broader audience to better learn and make\nuse of expert knowledge available on the web, improving information\naccessibility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u81ea\u6211\u7cbe\u70bc\u65b9\u6cd5\u5f00\u53d1\u4e86LLM\u7684\u6587\u672c\u7b80\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u968f\u673a\u5316\u7814\u7a76\uff084563\u540d\u53c2\u4e0e\u8005\uff09\u9a8c\u8bc1\u4e86\u7b80\u5316\u6587\u672c\u80fd\u663e\u8457\u63d0\u9ad8\u7406\u89e3\u51c6\u786e\u5ea6\uff08\u5e73\u5747\u63d0\u53473.9%\uff09\u3002", "motivation": "\u89e3\u51b3\u7f51\u7edc\u4fe1\u606f\uff08\u5982\u79d1\u5b66\u6587\u732e\u548c\u7ef4\u57fa\u767e\u79d1\uff09\u8d85\u51fa\u7528\u6237\u9605\u8bfb\u6c34\u5e73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7b80\u5316\u6587\u672c\u63d0\u5347\u4fe1\u606f\u53ef\u53ca\u6027\u3002", "method": "\u91c7\u7528\u81ea\u6211\u7cbe\u70bc\u65b9\u6cd5\u5f00\u53d1LLM\u6587\u672c\u7b80\u5316\u6280\u672f\uff0c\u901a\u8fc7\u591a\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\uff09\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u539f\u59cb\u4e0e\u7b80\u5316\u6587\u672c\u7684\u7406\u89e3\u6548\u679c\u3002", "result": "\u7b80\u5316\u6587\u672c\u4f7fMCQ\u6b63\u786e\u7387\u7edd\u5bf9\u63d0\u53473.9%\uff0c\u751f\u7269\u533b\u5b66\u9886\u57df\u6548\u679c\u6700\u663e\u8457\uff0814.6%\uff09\u3002\u53c2\u4e0e\u8005\u4e3b\u89c2\u53cd\u9988\u7b80\u5316\u6587\u672c\u66f4\u6613\u8bfb\u3002", "conclusion": "LLM\u6587\u672c\u7b80\u5316\u80fd\u6709\u6548\u6539\u5584\u590d\u6742\u4fe1\u606f\u7684\u7406\u89e3\uff0c\u5c24\u5176\u5229\u4e8e\u975e\u4e13\u4e1a\u4eba\u58eb\u83b7\u53d6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u63d0\u5347\u4fe1\u606f\u5e73\u7b49\u6027\u3002"}}
{"id": "2505.01449", "pdf": "https://arxiv.org/pdf/2505.01449", "abs": "https://arxiv.org/abs/2505.01449", "authors": ["Jiayu Wang", "Aws Albarghouthi", "Frederic Sala"], "title": "COSMOS: Predictable and Cost-Effective Adaptation of LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance across numerous\ntasks by using a diverse array of adaptation strategies. However, optimally\nselecting a model and adaptation strategy under resource constraints is\nchallenging and often requires extensive experimentation. We investigate\nwhether it is possible to accurately predict both performance and cost without\nexpensive trials. We formalize the strategy selection problem for LLMs and\nintroduce COSMOS, a unified prediction framework that efficiently estimates\nadaptation outcomes at minimal cost. We instantiate and study the capability of\nour framework via a pair of powerful predictors: embedding-augmented\nlightweight proxy models to predict fine-tuning performance, and low-sample\nscaling laws to forecast retrieval-augmented in-context learning. Extensive\nevaluation across eight representative benchmarks demonstrates that COSMOS\nachieves high prediction accuracy while reducing computational costs by 92.72%\non average, and up to 98.71% in resource-intensive scenarios. Our results show\nthat efficient prediction of adaptation outcomes is not only feasible but can\nsubstantially reduce the computational overhead of LLM deployment while\nmaintaining performance standards.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86COSMOS\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9002\u5e94\u7b56\u7565\u6027\u80fd\u4e0e\u6210\u672c\uff0c\u907f\u514d\u6602\u8d35\u5b9e\u9a8c\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u6700\u9ad8\u8fbe98.71%\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u5982\u4f55\u6700\u4f18\u9009\u62e9LLM\u6a21\u578b\u548c\u9002\u5e94\u7b56\u7565\u7684\u95ee\u9898\uff0c\u907f\u514d\u5927\u91cf\u5b9e\u9a8c\u5f00\u9500\u3002", "method": "\u5f15\u5165COSMOS\u6846\u67b6\uff0c\u7ed3\u5408\u5d4c\u5165\u589e\u5f3a\u7684\u8f7b\u91cf\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u5fae\u8c03\u6027\u80fd\uff0c\u4ee5\u53ca\u4f4e\u6837\u672c\u7f29\u653e\u5b9a\u5f8b\u9884\u6d4b\u68c0\u7d22\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u516b\u4e2a\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOSMOS\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e73\u5747\u964d\u4f4e\u8ba1\u7b97\u6210\u672c92.72%\uff0c\u8d44\u6e90\u5bc6\u96c6\u578b\u573a\u666f\u4e0b\u6700\u9ad8\u8fbe98.71%\u3002", "conclusion": "\u9ad8\u6548\u9884\u6d4bLLM\u9002\u5e94\u7b56\u7565\u4e0d\u4ec5\u53ef\u884c\uff0c\u8fd8\u80fd\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u6807\u51c6\u3002"}}
{"id": "2505.02110", "pdf": "https://arxiv.org/pdf/2505.02110", "abs": "https://arxiv.org/abs/2505.02110", "authors": ["Tristan Cazenave"], "title": "Eterna is Solved", "categories": ["cs.AI"], "comment": null, "summary": "RNA design consists of discovering a nucleotide sequence that folds into a\ntarget secondary structure. It is useful for synthetic biology, medicine, and\nnanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested\nRollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design\nalgorithm. It solves the Eterna benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMontparnasse\u7684\u591a\u76ee\u6807\u901a\u7528\u5d4c\u5957\u6eda\u52a8\u7b56\u7565\u81ea\u9002\u5e94\u6709\u9650\u91cd\u590d\uff08MOGNRPALR\uff09RNA\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3Eterna\u57fa\u51c6\u95ee\u9898\u3002", "motivation": "RNA\u8bbe\u8ba1\u5728\u5408\u6210\u751f\u7269\u5b66\u3001\u533b\u5b66\u548c\u7eb3\u7c73\u6280\u672f\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u5176\u6838\u5fc3\u662f\u627e\u5230\u80fd\u591f\u6298\u53e0\u6210\u76ee\u6807\u4e8c\u7ea7\u7ed3\u6784\u7684\u6838\u82f7\u9178\u5e8f\u5217\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u540d\u4e3aMOGNRPALR\u7684\u591a\u76ee\u6807\u901a\u7528\u5d4c\u5957\u6eda\u52a8\u7b56\u7565\u81ea\u9002\u5e94\u6709\u9650\u91cd\u590d\u7b97\u6cd5\u3002", "result": "\u8be5\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86Eterna\u57fa\u51c6\u95ee\u9898\u3002", "conclusion": "Montparnasse\u7b97\u6cd5\u4e3aRNA\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02009", "pdf": "https://arxiv.org/pdf/2505.02009", "abs": "https://arxiv.org/abs/2505.02009", "authors": ["Sai Krishna Mendu", "Harish Yenala", "Aditi Gulati", "Shanu Kumar", "Parag Agrawal"], "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to various real-world\napplications, leveraging massive, web-sourced datasets like Common Crawl, C4,\nand FineWeb for pretraining. While these datasets provide linguistic data\nessential for high-quality natural language generation, they often contain\nharmful content, such as hate speech, misinformation, and biased narratives.\nTraining LLMs on such unfiltered data risks perpetuating toxic behaviors,\nspreading misinformation, and amplifying societal biases which can undermine\ntrust in LLM-driven applications and raise ethical concerns about their use.\nThis paper presents a large-scale analysis of inappropriate content across\nthese datasets, offering a comprehensive taxonomy that categorizes harmful\nwebpages into Topical and Toxic based on their intent. We also introduce a\nprompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and\na transformer-based model (HarmFormer) for content filtering. Additionally, we\ncreate a new multi-harm open-ended toxicity benchmark (HAVOC) and provide\ncrucial insights into how models respond to adversarial toxic inputs. Upon\npublishing, we will also opensource our model signal on the entire C4 dataset.\nOur work offers insights into ensuring safer LLM pretraining and serves as a\nresource for Responsible AI (RAI) compliance.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u6709\u5bb3\u5185\u5bb9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u65b9\u6cd5\u53ca\u8fc7\u6ee4\u5de5\u5177\uff0c\u65e8\u5728\u63d0\u5347LLMs\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5e38\u542b\u6709\u4ec7\u6068\u8a00\u8bba\u3001\u9519\u8bef\u4fe1\u606f\u548c\u504f\u89c1\u7b49\u6709\u5bb3\u5185\u5bb9\uff0c\u76f4\u63a5\u4f7f\u7528\u4f1a\u4f20\u64ad\u6bd2\u6027\u884c\u4e3a\uff0c\u5f15\u53d1\u4f26\u7406\u95ee\u9898\u3002\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u786e\u4fddLLMs\u66f4\u5b89\u5168\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5206\u6790\u6570\u636e\u96c6\u4e2d\u7684\u4e0d\u5f53\u5185\u5bb9\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\uff08Topical/Toxic\uff09\uff0c\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u63d0\u793a\u6570\u636e\u96c6\uff08TTP\uff09\u3001\u57fa\u4e8eTransformer\u7684\u8fc7\u6ee4\u6a21\u578b\uff08HarmFormer\uff09\uff0c\u5e76\u521b\u5efa\u591a\u6bd2\u6027\u57fa\u51c6\uff08HAVOC\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u5168\u9762\u7684\u6709\u5bb3\u5185\u5bb9\u5206\u7c7b\u6cd5\u548c\u9ad8\u6548\u8fc7\u6ee4\u5de5\u5177\uff0c\u4e3a\u5b89\u5168\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u4fe1\u53f7\u4ee5\u652f\u6301\u5408\u89c4\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u7684\u8d1f\u8d23\u4efb\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8d1f\u8d23\u4efbAI\uff08RAI\uff09\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.01450", "pdf": "https://arxiv.org/pdf/2505.01450", "abs": "https://arxiv.org/abs/2505.01450", "authors": ["Chaoyi Wang", "Junjie Zheng", "Zihao Chen", "Shiyu Xia", "Chaofan Ding", "Xiaohao Zhang", "Xi Tao", "Xiaoming He", "Xinhan Di"], "title": "Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks", "categories": ["cs.LG"], "comment": "6 pages, 3 figures, accepted to the AI for Content Creation workshop\n  at CVPR 2025 in Nashville, TN", "summary": "Movie dubbing has advanced significantly, yet assessing the real-world\neffectiveness of these models remains challenging. A comprehensive evaluation\nbenchmark is crucial for two key reasons: 1) Existing metrics fail to fully\ncapture the complexities of dialogue, narration, monologue, and actor\nadaptability in movie dubbing. 2) A practical evaluation system should offer\nvaluable insights to improve movie dubbing quality and advancement in film\nproduction. To this end, we introduce Talking Adaptive Dubbing Benchmarks\n(TA-Dubbing), designed to improve film production by adapting to dialogue,\nnarration, monologue, and actors in movie dubbing. TA-Dubbing offers several\nkey advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of\ndimensions of movie dubbing, incorporating metric evaluations for both movie\nunderstanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is\ndesigned to evaluate state-of-the-art movie dubbing models and advanced\nmulti-modal large language models. 3) Full Open-Sourcing: We fully open-source\nTA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video\nsuits, evaluation methods, annotations. We also continuously integrate new\nmovie dubbing models into the TA-Dubbing leaderboard at\nhttps://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie\ndubbing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTA-Dubbing\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7535\u5f71\u914d\u97f3\u8bc4\u4f30\u6307\u6807\u96be\u4ee5\u5168\u9762\u6355\u6349\u5bf9\u767d\u3001\u65c1\u767d\u3001\u72ec\u767d\u53ca\u6f14\u5458\u9002\u5e94\u6027\u7b49\u590d\u6742\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u7535\u5f71\u914d\u97f3\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u5bf9\u8bdd\u3001\u65c1\u767d\u3001\u72ec\u767d\u548c\u6f14\u5458\u9002\u5e94\u6027\u7b49\u590d\u6742\u6027\uff0c\u4e14\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u4e3a\u7535\u5f71\u5236\u4f5c\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\u7684\u5b9e\u9645\u8bc4\u4f30\u7cfb\u7edf\u3002", "method": "\u4f5c\u8005\u63d0\u51faTA-Dubbing\u8bc4\u4f30\u57fa\u51c6\uff0c\u8986\u76d6\u7535\u5f71\u914d\u97f3\u7684\u591a\u7ef4\u5ea6\u8bc4\u4ef7\uff0c\u5305\u62ec\u7535\u5f71\u7406\u89e3\u548c\u8bed\u97f3\u751f\u6210\u7684\u6307\u6807\u8bc4\u4f30\uff0c\u5e76\u652f\u6301\u5bf9\u5148\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u6d4b\u3002", "result": "TA-Dubbing\u5df2\u5b8c\u5168\u5f00\u6e90\uff0c\u5305\u542b\u89c6\u9891\u7d20\u6750\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u6807\u6ce8\uff0c\u5e76\u6301\u7eed\u96c6\u6210\u65b0\u7684\u914d\u97f3\u6a21\u578b\u5230\u5176\u6392\u884c\u699c\u4e2d\u3002", "conclusion": "TA-Dubbing\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u548c\u5f00\u6e90\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u7535\u5f71\u914d\u97f3\u9886\u57df\u7684\u53d1\u5c55\u548c\u8d28\u91cf\u63d0\u5347\u3002"}}
{"id": "2505.02118", "pdf": "https://arxiv.org/pdf/2505.02118", "abs": "https://arxiv.org/abs/2505.02118", "authors": ["Wei Liu", "Zhongyu Niu", "Lang Gao", "Zhiying Deng", "Jun Wang", "Haozhao Wang", "Ruixuan Li"], "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets", "categories": ["cs.AI"], "comment": "ICML 2025", "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u89e3\u91ca\u6846\u67b6\u4e2d\u7684\u62bd\u6837\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u6b62\u9884\u6d4b\u5668\u5b66\u4e60\u9519\u8bef\u76f8\u5173\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u63ed\u793a\u81ea\u89e3\u91ca\u6846\u67b6\u4e2d\u53ef\u80fd\u4ea7\u751f\u7684\u62bd\u6837\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u63d0\u9ad8\u6a21\u578b\u89e3\u91ca\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc1\u636e\u8bc6\u522b\u504f\u5dee\uff0c\u8bbe\u8ba1\u6307\u4ee4\u9632\u6b62\u9884\u6d4b\u5668\u5b66\u4e60\u9519\u8bef\u76f8\u5173\u6027\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u6d4b\u8bd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u8fd1\u671f\u81ea\u89e3\u91ca\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u8bc6\u522b\u4e86\u81ea\u89e3\u91ca\u6846\u67b6\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.02032", "pdf": "https://arxiv.org/pdf/2505.02032", "abs": "https://arxiv.org/abs/2505.02032", "authors": ["Anisia Katinskaia"], "title": "An overview of artificial intelligence in computer-assisted language learning", "categories": ["cs.CL"], "comment": null, "summary": "Computer-assisted language learning -- CALL -- is an established research\nfield. We review how artificial intelligence can be applied to support language\nlearning and teaching. The need for intelligent agents that assist language\nlearners and teachers is increasing: the human teacher's time is a scarce and\ncostly resource, which does not scale with growing demand. Further factors\ncontribute to the need for CALL: pandemics and increasing demand for distance\nlearning, migration of large populations, the need for sustainable and\naffordable support for learning, etc. CALL systems are made up of many\ncomponents that perform various functions, and AI is applied to many different\naspects in CALL, corresponding to their own expansive research areas. Most of\nwhat we find in the research literature and in practical use are prototypes or\npartial implementations -- systems that perform some aspects of the overall\ndesired functionality. Complete solutions -- most of them commercial -- are\nfew, because they require massive resources. Recent advances in AI should\nresult in improvements in CALL, yet there is a lack of surveys that focus on AI\nin the context of this research field. This paper aims to present a perspective\non the AI methods that can be employed for language learning from a position of\na developer of a CALL system. We also aim to connect work from different\ndisciplines, to build bridges for interdisciplinary work.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\uff08CALL\uff09\u9886\u57df\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u5176\u5bf9\u8bed\u8a00\u5b66\u4e60\u548c\u6559\u5b66\u7684\u6f5c\u5728\u652f\u6301\u3002\u5c3d\u7ba1\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5b8c\u6574\u7684CALL\u89e3\u51b3\u65b9\u6848\u4ecd\u7a00\u7f3a\uff0c\u672c\u6587\u65e8\u5728\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9bAI\u65b9\u6cd5\u7684\u89c6\u89d2\uff0c\u5e76\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5de5\u4f5c\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u5b66\u4e60\u9700\u6c42\u7684\u589e\u957f\uff08\u5982\u75ab\u60c5\u3001\u79fb\u6c11\u7b49\uff09\uff0c\u4eba\u5de5\u6559\u5e08\u7684\u65f6\u95f4\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u3002CALL\u7cfb\u7edf\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u7684\u9700\u6c42\u65e5\u76ca\u51f8\u663e\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u4e3a\u539f\u578b\u6216\u90e8\u5206\u5b9e\u73b0\uff0c\u5b8c\u6574\u7684\u5546\u4e1a\u5316\u89e3\u51b3\u65b9\u6848\u7a00\u7f3a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865AI\u5728CALL\u9886\u57df\u5e94\u7528\u7684\u7efc\u8ff0\u7a7a\u767d\u3002", "method": "\u4eceCALL\u7cfb\u7edf\u5f00\u53d1\u8005\u7684\u89c6\u89d2\uff0c\u5206\u6790AI\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u7684\u65b9\u6cd5\uff0c\u5e76\u8054\u7cfb\u4e0d\u540c\u5b66\u79d1\u7684\u7814\u7a76\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u534f\u4f5c\u3002", "result": "\u6307\u51faAI\u5728CALL\u9886\u57df\u7684\u591a\u6837\u5316\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6210\u679c\u591a\u4e3a\u5c40\u90e8\u5b9e\u73b0\uff0c\u5b8c\u6574\u7684\u5546\u4e1a\u5316\u7cfb\u7edf\u4ecd\u9700\u5927\u91cf\u8d44\u6e90\u652f\u6301\u3002", "conclusion": "AI\u5728CALL\u4e2d\u7684\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u9700\u66f4\u591a\u8de8\u5b66\u79d1\u5408\u4f5c\u548c\u8d44\u6e90\u6295\u5165\u4ee5\u5b9e\u73b0\u5168\u9762\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01488", "pdf": "https://arxiv.org/pdf/2505.01488", "abs": "https://arxiv.org/abs/2505.01488", "authors": ["Yujing Zhou", "Marc L. Jacquet", "Robel Dawit", "Skyler Fabre", "Dev Sarawat", "Faheem Khan", "Madison Newell", "Yongxin Liu", "Dahai Liu", "Hongyun Chen", "Jian Wang", "Huihui Wang"], "title": "Explainable Machine Learning for Cyberattack Identification from Traffic Flows", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "The increasing automation of traffic management systems has made them prime\ntargets for cyberattacks, disrupting urban mobility and public safety.\nTraditional network-layer defenses are often inaccessible to transportation\nagencies, necessitating a machine learning-based approach that relies solely on\ntraffic flow data. In this study, we simulate cyberattacks in a semi-realistic\nenvironment, using a virtualized traffic network to analyze disruption\npatterns. We develop a deep learning-based anomaly detection system,\ndemonstrating that Longest Stop Duration and Total Jam Distance are key\nindicators of compromised signals. To enhance interpretability, we apply\nExplainable AI (XAI) techniques, identifying critical decision factors and\ndiagnosing misclassification errors. Our analysis reveals two primary\nchallenges: transitional data inconsistencies, where mislabeled recovery-phase\ntraffic misleads the model, and model limitations, where stealth attacks in\nlow-traffic conditions evade detection. This work enhances AI-driven traffic\nsecurity, improving both detection accuracy and trustworthiness in smart\ntransportation systems.", "AI": {"tldr": "\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u534a\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u7f51\u7edc\u653b\u51fb\uff0c\u5206\u6790\u4ea4\u901a\u6d41\u6570\u636e\u4ee5\u8bc6\u522b\u4fe1\u53f7\u706f\u88ab\u7be1\u6539\u7684\u5173\u952e\u6307\u6807\uff08\u5982\u6700\u957f\u505c\u8f66\u65f6\u957f\u548c\u603b\u62e5\u5835\u8ddd\u79bb\uff09\uff0c\u5e76\u5e94\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u63d0\u5347\u6a21\u578b\u53ef\u7406\u89e3\u6027\u3002\u7814\u7a76\u53d1\u73b0\u6570\u636e\u4e0d\u4e00\u81f4\u548c\u4f4e\u6d41\u91cf\u9690\u853d\u653b\u51fb\u662f\u4e3b\u8981\u6311\u6218\uff0c\u6700\u7ec8\u63d0\u9ad8\u4e86\u4ea4\u901a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u968f\u7740\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\u81ea\u52a8\u5316\u7a0b\u5ea6\u7684\u63d0\u9ad8\uff0c\u5176\u6210\u4e3a\u7f51\u7edc\u653b\u51fb\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u4f20\u7edf\u7f51\u7edc\u5c42\u9632\u5fa1\u624b\u6bb5\u5bf9\u4ea4\u901a\u673a\u6784\u5f80\u5f80\u4e0d\u53ef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u4ea4\u901a\u6d41\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u5e94\u5bf9\u3002", "method": "\u7814\u7a76\u5728\u534a\u771f\u5b9e\u73af\u5883\u4e2d\u6a21\u62df\u7f51\u7edc\u653b\u51fb\uff0c\u5229\u7528\u865a\u62df\u5316\u4ea4\u901a\u7f51\u7edc\u5206\u6790\u7834\u574f\u6a21\u5f0f\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\uff08XAI\uff09\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u6700\u957f\u505c\u8f66\u65f6\u957f\u548c\u603b\u62e5\u5835\u8ddd\u79bb\u662f\u4fe1\u53f7\u706f\u88ab\u7be1\u6539\u7684\u5173\u952e\u6307\u6807\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u8fc7\u6e21\u6570\u636e\u4e0d\u4e00\u81f4\u548c\u4f4e\u6d41\u91cf\u9690\u853d\u653b\u51fb\u662f\u6a21\u578b\u7684\u4e3b\u8981\u6311\u6218\u3002\u6700\u7ec8\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u53ef\u4fe1\u7684AI\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u53ef\u89e3\u91caAI\u6280\u672f\u6709\u6548\u8bc6\u522b\u548c\u5e94\u5bf9\u7f51\u7edc\u653b\u51fb\u3002"}}
{"id": "2505.02121", "pdf": "https://arxiv.org/pdf/2505.02121", "abs": "https://arxiv.org/abs/2505.02121", "authors": ["Lachlan McGinness"], "title": "Overview of AI Grading of Physics Olympiad Exams", "categories": ["cs.AI"], "comment": "International Conference on Artificial Intelligence in Education,\n  Doctoral Consortium", "summary": "Automatically grading the diverse range of question types in high school\nphysics problem is a challenge that requires automated grading techniques from\ndifferent fields. We report the findings of a Systematic Literature Review of\npotential physics grading techniques. We propose a multi-modal AI grading\nframework to address these challenges and examine our framework in light of\nAustralia's AI Ethical Principles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001AI\u8bc4\u5206\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u9ad8\u4e2d\u7269\u7406\u95ee\u9898\u7684\u591a\u6837\u5316\u9898\u578b\u8bc4\u5206\uff0c\u5e76\u57fa\u4e8e\u6fb3\u5927\u5229\u4e9aAI\u4f26\u7406\u539f\u5219\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u9ad8\u4e2d\u7269\u7406\u95ee\u9898\u591a\u6837\u5316\u9898\u578b\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u6311\u6218\uff0c\u9700\u8981\u7ed3\u5408\u4e0d\u540c\u9886\u57df\u7684\u8bc4\u5206\u6280\u672f\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7b5b\u9009\u6f5c\u5728\u7684\u7269\u7406\u8bc4\u5206\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u591a\u6a21\u6001AI\u8bc4\u5206\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b26\u5408AI\u4f26\u7406\u539f\u5219\u7684\u591a\u6a21\u6001\u8bc4\u5206\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u4e2d\u7269\u7406\u95ee\u9898\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u7b26\u5408\u4f26\u7406\u6807\u51c6\u3002"}}
{"id": "2505.02072", "pdf": "https://arxiv.org/pdf/2505.02072", "abs": "https://arxiv.org/abs/2505.02072", "authors": ["Eitan Wagner", "Omri Abend"], "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The notion of language modeling has gradually shifted in recent years from a\ndistribution over finite-length strings to general-purpose prediction models\nfor textual inputs and outputs, following appropriate alignment phases. This\npaper analyzes the distinction between distribution estimation and response\nprediction in the context of LLMs, and their often conflicting goals. We\nexamine the training phases of LLMs, which include pretraining, in-context\nlearning, and preference tuning, and also the common use cases for their output\nprobabilities, which include completion probabilities and explicit\nprobabilities as output. We argue that the different settings lead to three\ndistinct intended output distributions. We demonstrate that NLP works often\nassume that these distributions should be similar, which leads to\nmisinterpretations of their experimental findings. Our work sets firmer formal\nfoundations for the interpretation of LLMs, which will inform ongoing work on\nthe interpretation and use of LLMs' induced distributions.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u8bed\u8a00\u5efa\u6a21\u4ece\u6709\u9650\u957f\u5ea6\u5b57\u7b26\u4e32\u5206\u5e03\u5230\u901a\u7528\u9884\u6d4b\u6a21\u578b\u7684\u8f6c\u53d8\uff0c\u5206\u6790\u4e86LLM\u4e2d\u5206\u5e03\u4f30\u8ba1\u4e0e\u54cd\u5e94\u9884\u6d4b\u7684\u533a\u522b\u53ca\u5176\u51b2\u7a81\u76ee\u6807\uff0c\u603b\u7ed3\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u9884\u671f\u8f93\u51fa\u5206\u5e03\uff0c\u5e76\u6307\u51fa\u4e86NLP\u5de5\u4f5c\u4e2d\u5e38\u89c1\u7684\u8bef\u89e3\u3002", "motivation": "\u7814\u7a76LLM\u4e2d\u5206\u5e03\u4f30\u8ba1\u4e0e\u54cd\u5e94\u9884\u6d4b\u7684\u533a\u522b\uff0c\u6f84\u6e05\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u548c\u7528\u4f8b\u4e0b\u7684\u8f93\u51fa\u5206\u5e03\u5dee\u5f02\uff0c\u907f\u514d\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u7684\u8bef\u89e3\u3002", "method": "\u5206\u6790LLM\u7684\u8bad\u7ec3\u9636\u6bb5\uff08\u5982\u9884\u8bad\u7ec3\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u504f\u597d\u5fae\u8c03\uff09\u53ca\u8f93\u51fa\u6982\u7387\u7684\u5e38\u89c1\u7528\u4f8b\uff08\u5982\u8865\u5168\u6982\u7387\u3001\u663e\u5f0f\u6982\u7387\uff09\uff0c\u63d0\u51fa\u4e09\u79cd\u9884\u671f\u8f93\u51fa\u5206\u5e03\u3002", "result": "\u63ed\u793a\u4e86NLP\u5de5\u4f5c\u4e2d\u5e38\u8bef\u8ba4\u4e3a\u8fd9\u4e9b\u5206\u5e03\u5e94\u76f8\u4f3c\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u7ed3\u679c\u7684\u9519\u8bef\u89e3\u8bfb\u3002", "conclusion": "\u4e3aLLM\u7684\u89e3\u91ca\u548c\u8bf1\u5bfc\u5206\u5e03\u7684\u4f7f\u7528\u5960\u5b9a\u4e86\u66f4\u4e25\u683c\u7684\u5f62\u5f0f\u57fa\u7840\u3002"}}
{"id": "2505.01489", "pdf": "https://arxiv.org/pdf/2505.01489", "abs": "https://arxiv.org/abs/2505.01489", "authors": ["Yujing Zhou", "Marc L. Jacquet", "Robel Dawit", "Skyler Fabre", "Dev Sarawat", "Faheem Khan", "Madison Newell", "Yongxin Liu", "Dahai Liu", "Hongyun Chen", "Jian Wang", "Huihui Wang"], "title": "Machine Learning for Cyber-Attack Identification from Traffic Flows", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "This paper presents our simulation of cyber-attacks and detection strategies\non the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual\nmachines and the OPNSense firewall, along with traffic dynamics from SUMO and\nexploitation via the Metasploit framework. We try to answer the research\nquestions: are we able to identify cyber attacks by only analyzing traffic flow\npatterns. In this research, the cyber attacks are focused particularly when\nlights are randomly turned all green or red at busy intersections by\nadversarial attackers. Despite challenges stemming from imbalanced data and\noverlapping traffic patterns, our best model shows 85\\% accuracy when detecting\nintrusions purely using traffic flow statistics. Key indicators for successful\ndetection included occupancy, jam length, and halting durations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u4ea4\u901a\u6d41\u6a21\u5f0f\u5206\u6790\u68c0\u6d4b\u7f51\u7edc\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9Daytona Beach\u4ea4\u901a\u7cfb\u7edf\u5229\u7528Raspberry Pi\u548cOPNSense\u9632\u706b\u5899\u8fdb\u884c\u4e86\u6a21\u62df\u653b\u51fb\u548c\u68c0\u6d4b\uff0c\u91cd\u70b9\u6d4b\u8bd5\u4e86\u4ea4\u901a\u706f\u968f\u673a\u53d8\u7eff\u6216\u7ea2\u7684\u60c5\u51b5\uff0c\u6a21\u578b\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe85%\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u4ea4\u901a\u6d41\u6a21\u5f0f\u6765\u68c0\u6d4b\u7f51\u7edc\u653b\u51fb\uff0c\u89e3\u51b3\u5f53\u653b\u51fb\u8005\u968f\u673a\u6539\u53d8\u4ea4\u901a\u706f\u72b6\u6001\u65f6\uff0c\u5982\u4f55\u4ec5\u51ed\u4ea4\u901a\u6570\u636e\u8bc6\u522b\u5165\u4fb5\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528Raspberry Pi\u865a\u62df\u673a\u548cOPNSense\u9632\u706b\u5899\u642d\u5efa\u5b9e\u9a8c\u73af\u5883\uff0c\u7ed3\u5408SUMO\u751f\u6210\u4ea4\u901a\u52a8\u6001\u6570\u636e\uff0c\u5e76\u901a\u8fc7Metasploit\u6846\u67b6\u6a21\u62df\u653b\u51fb\u3002\u7814\u7a76\u7279\u522b\u5173\u6ce8\u4ea4\u901a\u706f\u88ab\u968f\u673a\u64cd\u63a7\u7684\u573a\u666f\u3002", "result": "\u5c3d\u7ba1\u6570\u636e\u4e0d\u5e73\u8861\u548c\u4ea4\u901a\u6a21\u5f0f\u91cd\u53e0\u5e26\u6765\u6311\u6218\uff0c\u6700\u4f73\u6a21\u578b\u4ec5\u51ed\u4ea4\u901a\u6d41\u7edf\u8ba1\u6570\u636e\uff08\u5982\u5360\u6709\u7387\u3001\u62e5\u5835\u957f\u5ea6\u548c\u505c\u8f66\u65f6\u957f\uff09\u5b9e\u73b0\u4e8685%\u7684\u5165\u4fb5\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u901a\u8fc7\u4ea4\u901a\u6d41\u6a21\u5f0f\u5206\u6790\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u7279\u5b9a\u7684\u7f51\u7edc\u653b\u51fb\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u548c\u6a21\u5f0f\u590d\u6742\u6027\u4ecd\u662f\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2505.02130", "pdf": "https://arxiv.org/pdf/2505.02130", "abs": "https://arxiv.org/abs/2505.02130", "authors": ["Zhong Guan", "Likang Wu", "Hongke Zhao", "Ming He", "Jianpin Fan"], "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data", "categories": ["cs.AI", "cs.CL"], "comment": "ICML2025 Accept", "summary": "Attention mechanisms are critical to the success of large language models\n(LLMs), driving significant advancements in multiple fields. However, for\ngraph-structured data, which requires emphasis on topological connections, they\nfall short compared to message-passing mechanisms on fixed links, such as those\nemployed by Graph Neural Networks (GNNs). This raises a question: ``Does\nattention fail for graphs in natural language settings?'' Motivated by these\nobservations, we embarked on an empirical study from the perspective of\nattention mechanisms to explore how LLMs process graph-structured data. The\ngoal is to gain deeper insights into the attention behavior of LLMs over graph\nstructures. We uncovered unique phenomena regarding how LLMs apply attention to\ngraph-structured data and analyzed these findings to improve the modeling of\nsuch data by LLMs. The primary findings of our research are: 1) While LLMs can\nrecognize graph data and capture text-node interactions, they struggle to model\ninter-node relationships within graph structures due to inherent architectural\nconstraints. 2) The attention distribution of LLMs across graph nodes does not\nalign with ideal structural patterns, indicating a failure to adapt to graph\ntopology nuances. 3) Neither fully connected attention nor fixed connectivity\nis optimal; each has specific limitations in its application scenarios.\nInstead, intermediate-state attention windows improve LLM training performance\nand seamlessly transition to fully connected windows during inference. Source\ncode: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0LLMs\u5728\u5904\u7406\u8282\u70b9\u95f4\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e2d\u95f4\u6001\u6ce8\u610f\u529b\u7a97\u53e3\u6539\u8fdb\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6ce8\u610f\u529b\u673a\u5236\u5728LLMs\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\uff08\u5982GNNs\uff09\u6548\u679c\u4e0d\u8db3\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u5177\u4f53\u884c\u4e3a\u548c\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76LLMs\u7684\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u56fe\u6570\u636e\u7684\u884c\u4e3a\uff0c\u5206\u6790\u5176\u8868\u73b0\u3002", "result": "\u53d1\u73b0LLMs\u96be\u4ee5\u5efa\u6a21\u8282\u70b9\u95f4\u5173\u7cfb\uff0c\u6ce8\u610f\u529b\u5206\u5e03\u4e0d\u7b26\u5408\u7406\u60f3\u7ed3\u6784\uff1b\u63d0\u51fa\u4e2d\u95f4\u6001\u6ce8\u610f\u529b\u7a97\u53e3\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u6570\u636e\u4e0a\u7684\u9002\u7528\u6027\u6709\u9650\uff0c\u4e2d\u95f4\u6001\u6ce8\u610f\u529b\u7a97\u53e3\u662f\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2505.02078", "pdf": "https://arxiv.org/pdf/2505.02078", "abs": "https://arxiv.org/abs/2505.02078", "authors": ["Joy Lim Jia Yin", "Daniel Zhang-Li", "Jifan Yu", "Haoxuan Li", "Shangqing Tu", "Yuanchun Wang", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "title": "LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Evaluating the quality of slide-based multimedia instruction is challenging.\nExisting methods like manual assessment, reference-based metrics, and large\nlanguage model evaluators face limitations in scalability, context capture, or\nbias. In this paper, we introduce LecEval, an automated metric grounded in\nMayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal\nknowledge acquisition in slide-based learning. LecEval assesses effectiveness\nusing four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical\nStructure (LS), and Audience Engagement (AE). We curate a large-scale dataset\nof over 2,000 slides from more than 50 online course videos, annotated with\nfine-grained human ratings across these rubrics. A model trained on this\ndataset demonstrates superior accuracy and adaptability compared to existing\nmetrics, bridging the gap between automated and human assessments. We release\nour dataset and toolkits at https://github.com/JoylimJY/LecEval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86LecEval\uff0c\u4e00\u79cd\u57fa\u4e8eMayer\u591a\u5a92\u4f53\u5b66\u4e60\u8ba4\u77e5\u7406\u8bba\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5e7b\u706f\u7247\u5b66\u4e60\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u83b7\u53d6\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u52a8\u8bc4\u4f30\u3001\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5728\u53ef\u6269\u5c55\u6027\u3001\u4e0a\u4e0b\u6587\u6355\u6349\u6216\u504f\u7f6e\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5e7b\u706f\u7247\u6559\u5b66\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faLecEval\uff0c\u57fa\u4e8e\u56db\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\uff08\u5185\u5bb9\u76f8\u5173\u6027\u3001\u8868\u8fbe\u6e05\u6670\u5ea6\u3001\u903b\u8f91\u7ed3\u6784\u3001\u53d7\u4f17\u53c2\u4e0e\u5ea6\uff09\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLecEval\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff0c\u586b\u8865\u4e86\u81ea\u52a8\u4e0e\u4eba\u5de5\u8bc4\u4f30\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "LecEval\u4e3a\u5e7b\u706f\u7247\u6559\u5b66\u8d28\u91cf\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u76f8\u5173\u6570\u636e\u548c\u5de5\u5177\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.01523", "pdf": "https://arxiv.org/pdf/2505.01523", "abs": "https://arxiv.org/abs/2505.01523", "authors": ["Madhav Kotecha", "Vijendra Kumar Vaishya", "Smita Gautam", "Suraj Racha"], "title": "Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation", "categories": ["cs.LG", "cs.AI", "68T05"], "comment": "9 pages", "summary": "We propose a refined approach to efficiently fine-tune large language models\n(LLMs) on specific domains like the mathematical domain by employing a budgeted\nsubset selection method. Our approach combines utility and diversity metrics to\nselect the most informative and representative training examples. The final\ngoal is to achieve near-full dataset performance with meticulously selected\ndata points from the entire dataset while significantly reducing computational\ncost and training time and achieving competitive performance as the full\ndataset. The utility metric incorporates both perplexity and Chain-of-Thought\n(CoT) loss to identify challenging examples that contribute most to model\nlearning, while the diversity metric ensures broad coverage across mathematical\nsubdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing\nagainst several baseline approaches, including random selection,\ndiversity-based sampling, and existing state-of-the-art subset selection\ntechniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6548\u7528\u548c\u591a\u6837\u6027\u6307\u6807\u9009\u62e9\u8bad\u7ec3\u6837\u672c\u7684\u9884\u7b97\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u6548\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6570\u5b66\u9886\u57df\u5b9e\u73b0\u63a5\u8fd1\u5168\u6570\u636e\u96c6\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u65e8\u5728\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u6570\u5b66\uff09\u5fae\u8c03\u65f6\u7684\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u56f0\u60d1\u5ea6\u548cChain-of-Thought\u635f\u5931\u4f5c\u4e3a\u6548\u7528\u6307\u6807\uff0c\u4ee5\u53ca\u591a\u6837\u6027\u6307\u6807\u6765\u9009\u62e9\u6700\u5177\u4ee3\u8868\u6027\u548c\u4fe1\u606f\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5728LLaMA-3 8B\u548cPhi-3\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u968f\u673a\u9009\u62e9\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u4efb\u52a1\u3002"}}
{"id": "2505.02184", "pdf": "https://arxiv.org/pdf/2505.02184", "abs": "https://arxiv.org/abs/2505.02184", "authors": ["Matthew T. Dearing", "Yiheng Tao", "Xingfu Wu", "Zhiling Lan", "Valerie Taylor"], "title": "Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes", "categories": ["cs.AI", "cs.DC", "cs.PL", "cs.SE"], "comment": "11 pages, 4 figures", "summary": "While large language models (LLMs) are increasingly used for generating\nparallel scientific code, most current efforts emphasize functional\ncorrectness, often overlooking performance and energy considerations. In this\nwork, we propose LASSI-EE, an automated LLM-based refactoring framework that\ngenerates energy-efficient parallel code on a target parallel system for a\ngiven parallel code as input. Through a multi-stage, iterative pipeline\nprocess, LASSI-EE achieved an average energy reduction of 47% across 85% of the\n20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the\nbroader potential of LLMs, not only for generating correct code but also for\nenabling energy-aware programming. We also address key insights and limitations\nwithin the framework, offering valuable guidance for future improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLASSI-EE\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8fed\u4ee3\u6d41\u7a0b\uff0c\u5229\u7528LLM\u751f\u6210\u9ad8\u80fd\u6548\u5e76\u884c\u4ee3\u7801\uff0c\u5728NVIDIA A100\u4e0a\u5b9e\u73b0\u5e73\u574747%\u7684\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u5f53\u524dLLM\u751f\u6210\u5e76\u884c\u79d1\u5b66\u4ee3\u7801\u65f6\u591a\u5173\u6ce8\u529f\u80fd\u6027\uff0c\u5ffd\u7565\u4e86\u6027\u80fd\u548c\u80fd\u8017\u4f18\u5316\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u91cd\u6784\u6846\u67b6LASSI-EE\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8fed\u4ee3\u6d41\u7a0b\u751f\u6210\u76ee\u6807\u5e76\u884c\u7cfb\u7edf\u7684\u80fd\u6548\u4ee3\u7801\u3002", "result": "\u572820\u4e2aHeCBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c85%\u7684\u6848\u4f8b\u5e73\u5747\u80fd\u8017\u964d\u4f4e47%\u3002", "conclusion": "LLM\u4e0d\u4ec5\u53ef\u751f\u6210\u6b63\u786e\u4ee3\u7801\uff0c\u8fd8\u80fd\u5b9e\u73b0\u80fd\u8017\u611f\u77e5\u7f16\u7a0b\uff0c\u4f46\u6846\u67b6\u4ecd\u5b58\u5728\u9700\u6539\u8fdb\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.02091", "pdf": "https://arxiv.org/pdf/2505.02091", "abs": "https://arxiv.org/abs/2505.02091", "authors": ["Xinyue Peng", "Yanming Liu", "Yihan Cang", "Chaoqun Cao", "Ming Chen"], "title": "LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications", "categories": ["cs.CL", "cs.LG"], "comment": "6 pages,4 figures", "summary": "Solving non-convex resource allocation problems poses significant challenges\nin wireless communication systems, often beyond the capability of traditional\noptimization techniques. To address this issue, we propose LLM-OptiRA, the\nfirst framework that leverages large language models (LLMs) to automatically\ndetect and transform non-convex components into solvable forms, enabling fully\nautomated resolution of non-convex resource allocation problems in wireless\ncommunication systems. LLM-OptiRA not only simplifies problem-solving by\nreducing reliance on expert knowledge, but also integrates error correction and\nfeasibility validation mechanisms to ensure robustness. Experimental results\nshow that LLM-OptiRA achieves an execution rate of 96% and a success rate of\n80% on GPT-4, significantly outperforming baseline approaches in complex\noptimization tasks across diverse scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLM-OptiRA\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u548c\u8f6c\u6362\u975e\u51f8\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u5168\u81ea\u52a8\u6c42\u89e3\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u6280\u672f\u96be\u4ee5\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u975e\u51f8\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u6025\u9700\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u51cf\u5c11\u5bf9\u4e13\u5bb6\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faLLM-OptiRA\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u5e76\u8f6c\u6362\u975e\u51f8\u95ee\u9898\u4e3a\u53ef\u6c42\u89e3\u5f62\u5f0f\uff0c\u540c\u65f6\u96c6\u6210\u7ea0\u9519\u548c\u53ef\u884c\u6027\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM-OptiRA\u5728GPT-4\u4e0a\u5b9e\u73b096%\u7684\u6267\u884c\u7387\u548c80%\u7684\u6210\u529f\u7387\uff0c\u5728\u590d\u6742\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "LLM-OptiRA\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u975e\u51f8\u8d44\u6e90\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2505.01557", "pdf": "https://arxiv.org/pdf/2505.01557", "abs": "https://arxiv.org/abs/2505.01557", "authors": ["Runtian Zhai", "Kai Yang", "Che-Ping Tsai", "Burak Varici", "Zico Kolter", "Pradeep Ravikumar"], "title": "Contextures: Representations from Contexts", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025, longer version. arXiv admin note: substantial text overlap\n  with arXiv:2504.19792", "summary": "Despite the empirical success of foundation models, we do not have a\nsystematic characterization of the representations that these models learn. In\nthis paper, we establish the contexture theory. It shows that a large class of\nrepresentation learning methods can be characterized as learning from the\nassociation between the input and a context variable. Specifically, we show\nthat many popular methods aim to approximate the top-d singular functions of\nthe expectation operator induced by the context, in which case we say that the\nrepresentation learns the contexture. We demonstrate the generality of the\ncontexture theory by proving that representation learning within various\nlearning paradigms -- supervised, self-supervised, and manifold learning -- can\nall be studied from such a perspective. We also prove that the representations\nthat learn the contexture are optimal on those tasks that are compatible with\nthe context. One important implication of the contexture theory is that once\nthe model is large enough to approximate the top singular functions, further\nscaling up the model size yields diminishing returns. Therefore, scaling is not\nall we need, and further improvement requires better contexts. To this end, we\nstudy how to evaluate the usefulness of a context without knowing the\ndownstream tasks. We propose a metric and show by experiments that it\ncorrelates well with the actual performance of the encoder on many real\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u2018\u4e0a\u4e0b\u6587\u7406\u8bba\u2019\uff0c\u7edf\u4e00\u89e3\u91ca\u4e86\u591a\u79cd\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u8868\u660e\u5b83\u4eec\u662f\u5b66\u4e60\u8f93\u5165\u4e0e\u4e0a\u4e0b\u6587\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u4e9b\u8868\u793a\u5728\u517c\u5bb9\u7684\u4efb\u52a1\u4e0a\u662f\u6700\u4f18\u7684\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u53d6\u5f97\u4e86\u7ecf\u9a8c\u4e0a\u7684\u6210\u529f\uff0c\u4f46\u5176\u5b66\u4e60\u8868\u793a\u7684\u7cfb\u7edf\u6027\u63cf\u8ff0\u4ecd\u4e0d\u660e\u786e\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u591a\u79cd\u5b66\u4e60\u65b9\u6cd5\uff08\u76d1\u7763\u3001\u81ea\u76d1\u7763\u3001\u6d41\u5f62\u5b66\u4e60\uff09\u53ef\u4ee5\u7edf\u4e00\u4e3a\u5b66\u4e60\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u671f\u671b\u7b97\u5b50\u7684\u9876\u90e8\u5947\u5f02\u51fd\u6570\u3002\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u4e0a\u4e0b\u6587\u6709\u7528\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u6a21\u578b\u8db3\u591f\u5927\u65f6\uff0c\u8fdb\u4e00\u6b65\u7684\u6a21\u578b\u6269\u5c55\u6536\u76ca\u9012\u51cf\uff0c\u800c\u6539\u8fdb\u9700\u8981\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u3002\u63d0\u51fa\u4e86\u8bc4\u4f30\u4e0a\u4e0b\u6587\u7684\u6307\u6807\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4e0e\u5b9e\u9645\u6027\u80fd\u76f8\u5173\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u7406\u8bba\u4e3a\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u8868\u660e\u6a21\u578b\u89c4\u6a21\u5e76\u975e\u552f\u4e00\u5173\u952e\uff0c\u4e0a\u4e0b\u6587\u7684\u4f18\u5316\u540c\u6837\u91cd\u8981\u3002"}}
{"id": "2505.02215", "pdf": "https://arxiv.org/pdf/2505.02215", "abs": "https://arxiv.org/abs/2505.02215", "authors": ["Mannan Bhardwaj"], "title": "Interpretable Emergent Language Using Inter-Agent Transformers", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper explores the emergence of language in multi-agent reinforcement\nlearning (MARL) using transformers. Existing methods such as RIAL, DIAL, and\nCommNet enable agent communication but lack interpretability. We propose\nDifferentiable Inter-Agent Transformers (DIAT), which leverage self-attention\nto learn symbolic, human-understandable communication protocols. Through\nexperiments, DIAT demonstrates the ability to encode observations into\ninterpretable vocabularies and meaningful embeddings, effectively solving\ncooperative tasks. These results highlight the potential of DIAT for\ninterpretable communication in complex multi-agent environments.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DIAT\u65b9\u6cd5\uff0c\u5229\u7528Transformer\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u89e3\u91ca\u901a\u4fe1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982RIAL\u3001DIAL\u548cCommNet\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u751f\u6210\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u901a\u4fe1\u534f\u8bae\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDifferentiable Inter-Agent Transformers\uff08DIAT\uff09\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u7b26\u53f7\u5316\u901a\u4fe1\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDIAT\u80fd\u5c06\u89c2\u5bdf\u7f16\u7801\u4e3a\u53ef\u89e3\u91ca\u7684\u8bcd\u6c47\u548c\u5d4c\u5165\uff0c\u6709\u6548\u89e3\u51b3\u534f\u4f5c\u4efb\u52a1\u3002", "conclusion": "DIAT\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u901a\u4fe1\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02142", "pdf": "https://arxiv.org/pdf/2505.02142", "abs": "https://arxiv.org/abs/2505.02142", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in long-context reasoning by large language\nmodels (LLMs), primarily through Online Reinforcement Learning (RL) methods,\nthese approaches incur substantial computational costs and complexity. In\ncontrast, simpler and more economical Offline RL methods remain underexplored.\nTo address this gap, we investigate the effectiveness of Offline RL methods,\nspecifically Direct Preference Optimization (DPO) and its length-desensitized\nvariant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive\nexperiments across multiple reasoning benchmarks demonstrate that these simpler\nOffline RL methods substantially improve model performance, achieving an\naverage enhancement of 3.3\\%, with a particularly notable increase of 10.1\\% on\nthe challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity\nto output length, emphasizing that increasing reasoning length should align\nwith semantic richness, as indiscriminate lengthening may adversely affect\nmodel performance. We provide comprehensive descriptions of our data processing\nand training methodologies, offering empirical evidence and practical insights\nfor developing more cost-effective Offline RL approaches.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08Offline RL\uff09\u65b9\u6cd5\uff0c\u5c24\u5176\u662fDPO\u53ca\u5176\u53d8\u4f53LD-DPO\uff0c\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u6548\u679c\uff0c\u76f8\u6bd4\u5728\u7ebfRL\u66f4\u7b80\u5355\u4e14\u7ecf\u6d4e\u3002\u5b9e\u9a8c\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u5e73\u5747\u63d0\u53473.3%\uff0c\u5728Arena-Hard\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u534710.1%\uff0c\u5e76\u5206\u6790\u4e86\u8f93\u51fa\u957f\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u7ebfRL\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u79bb\u7ebfRL\u65b9\u6cd5\u7b80\u5355\u7ecf\u6d4e\u5374\u7814\u7a76\u4e0d\u8db3\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u79bb\u7ebfRL\uff08\u5982DPO\u548cLD-DPO\uff09\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c\u3002", "method": "\u91c7\u7528\u79bb\u7ebfRL\u65b9\u6cd5\uff0c\u5305\u62ecDirect Preference Optimization (DPO)\u53ca\u5176\u53d8\u4f53LD-DPO\uff0c\u901a\u8fc7\u591a\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u79bb\u7ebfRL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e73\u5747\u63d0\u53473.3%\uff0c\u5728Arena-Hard\u57fa\u51c6\u4e0a\u63d0\u534710.1%\u3002\u540c\u65f6\u53d1\u73b0\u8f93\u51fa\u957f\u5ea6\u7684\u589e\u52a0\u9700\u4e0e\u8bed\u4e49\u4e30\u5bcc\u5ea6\u5339\u914d\u3002", "conclusion": "\u79bb\u7ebfRL\u65b9\u6cd5\uff08\u5982DPO\uff09\u80fd\u9ad8\u6548\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8f93\u51fa\u957f\u5ea6\u9700\u5408\u7406\u63a7\u5236\u3002\u8bba\u6587\u4e3a\u5f00\u53d1\u7ecf\u6d4e\u9ad8\u6548\u7684\u79bb\u7ebfRL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2505.01584", "pdf": "https://arxiv.org/pdf/2505.01584", "abs": "https://arxiv.org/abs/2505.01584", "authors": ["Zhiqiang He", "Zhi Liu"], "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adapting to non-stationary network conditions presents significant challenges\nfor resource adaptation. However, current solutions primarily rely on\nstationary assumptions. While data-driven reinforcement learning approaches\noffer promising solutions for handling network dynamics, our systematic\ninvestigation reveals a critical limitation: neural networks suffer from\nplasticity loss, significantly impeding their ability to adapt to evolving\nnetwork conditions. Through theoretical analysis of neural propagation\nmechanisms, we demonstrate that existing dormant neuron metrics inadequately\ncharacterize neural plasticity loss. To address this limitation, we have\ndeveloped the Silent Neuron theory, which provides a more comprehensive\nframework for understanding plasticity degradation. Based on these theoretical\ninsights, we propose the Reset Silent Neuron (ReSiN), which preserves neural\nplasticity through strategic neuron resets guided by both forward and backward\npropagation states. In our implementation of an adaptive video streaming\nsystem, ReSiN has shown significant improvements over existing solutions,\nachieving up to 168% higher bitrate and 108% better quality of experience (QoE)\nwhile maintaining comparable smoothness. Furthermore, ReSiN consistently\noutperforms in stationary environments, demonstrating its robust adaptability\nacross different network conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ReSiN\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7f6e\u9759\u9ed8\u795e\u7ecf\u5143\u6765\u5e94\u5bf9\u795e\u7ecf\u7f51\u7edc\u5728\u975e\u5e73\u7a33\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u53ef\u5851\u6027\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6bd4\u7279\u7387\u548c\u4f53\u9a8c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u8d44\u6e90\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5e73\u7a33\u5047\u8bbe\uff0c\u4f46\u5b9e\u9645\u7f51\u7edc\u73af\u5883\u52a8\u6001\u53d8\u5316\uff0c\u795e\u7ecf\u7f51\u7edc\u5728\u6b64\u7c7b\u73af\u5883\u4e0b\u56e0\u53ef\u5851\u6027\u635f\u5931\u96be\u4ee5\u9002\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u9759\u9ed8\u795e\u7ecf\u5143\u7406\u8bba\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1ReSiN\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u72b6\u6001\u6307\u5bfc\u795e\u7ecf\u5143\u91cd\u7f6e\u4ee5\u4fdd\u6301\u53ef\u5851\u6027\u3002", "result": "\u5728\u81ea\u9002\u5e94\u89c6\u9891\u6d41\u7cfb\u7edf\u4e2d\uff0cReSiN\u6bd4\u73b0\u6709\u65b9\u6848\u6bd4\u7279\u7387\u63d0\u5347168%\uff0c\u4f53\u9a8c\u8d28\u91cf\u63d0\u5347108%\uff0c\u540c\u65f6\u5728\u5e73\u7a33\u73af\u5883\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ReSiN\u901a\u8fc7\u7406\u8bba\u521b\u65b0\u89e3\u51b3\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u53ef\u5851\u6027\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.02216", "pdf": "https://arxiv.org/pdf/2505.02216", "abs": "https://arxiv.org/abs/2505.02216", "authors": ["Aidan Curtis", "Hao Tang", "Thiago Veloso", "Kevin Ellis", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "title": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation", "categories": ["cs.AI"], "comment": null, "summary": "Partially Observable Markov Decision Processes (POMDPs) model decision making\nunder uncertainty. While there are many approaches to approximately solving\nPOMDPs, we aim to address the problem of learning such models. In particular,\nwe are interested in a subclass of POMDPs wherein the components of the model,\nincluding the observation function, reward function, transition function, and\ninitial state distribution function, can be modeled as low-complexity\nprobabilistic graphical models in the form of a short probabilistic program.\nOur strategy to learn these programs uses an LLM as a prior, generating\ncandidate probabilistic programs that are then tested against the empirical\ndistribution and adjusted through feedback. We experiment on a number of\nclassical toy POMDP problems, simulated MiniGrid domains, and two real\nmobile-base robotics search domains involving partial observability. Our\nresults show that using an LLM to guide in the construction of a low-complexity\nPOMDP model can be more effective than tabular POMDP learning, behavior\ncloning, or direct LLM planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5b66\u4e60\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u548c\u8c03\u6574\u6982\u7387\u7a0b\u5e8f\u6765\u6784\u5efa\u4f4e\u590d\u6742\u5ea6\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u8868\u683cPOMDP\u5b66\u4e60\u3001\u884c\u4e3a\u514b\u9686\u548c\u76f4\u63a5LLM\u89c4\u5212\u3002", "motivation": "POMDPs\u662f\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u4e0b\u51b3\u7b56\u7684\u7ecf\u5178\u6a21\u578b\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u6a21\u578b\u5b66\u4e60\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u5229\u7528LLM\u7684\u751f\u6210\u80fd\u529b\uff0c\u7b80\u5316POMDP\u6a21\u578b\u7684\u6784\u5efa\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u590d\u6742\u5ea6\u6982\u7387\u56fe\u5f62\u6a21\u578b\u7684\u573a\u666f\u4e0b\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u6b65\uff1a\u5148\u7528LLM\u751f\u6210\u5019\u9009\u6982\u7387\u7a0b\u5e8f\uff08\u8986\u76d6\u89c2\u6d4b\u3001\u5956\u52b1\u3001\u8f6c\u79fb\u548c\u521d\u59cb\u72b6\u6001\u5206\u5e03\u51fd\u6570\uff09\uff0c\u518d\u901a\u8fc7\u5bf9\u6bd4\u7ecf\u9a8c\u5206\u5e03\u548c\u53cd\u9988\u8c03\u6574\u8fd9\u4e9b\u7a0b\u5e8f\u3002\u5b9e\u9a8c\u5728\u7ecf\u5178\u73a9\u5177POMDP\u95ee\u9898\u3001MiniGrid\u6a21\u62df\u57df\u548c\u771f\u5b9e\u79fb\u52a8\u673a\u5668\u4eba\u641c\u7d22\u57df\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u5f15\u5bfc\u7684\u4f4e\u590d\u6742\u5ea6POMDP\u6a21\u578b\u6784\u5efa\u65b9\u6cd5\u5728\u6548\u679c\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684\u8868\u683cPOMDP\u5b66\u4e60\u3001\u884c\u4e3a\u514b\u9686\u6216\u76f4\u63a5LLM\u89c4\u5212\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408LLM\u7684\u751f\u6210\u80fd\u529b\u548c\u6982\u7387\u7a0b\u5e8f\u8c03\u6574\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684POMDP\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u6a21\u578b\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02146", "pdf": "https://arxiv.org/pdf/2505.02146", "abs": "https://arxiv.org/abs/2505.02146", "authors": ["Shouyang Dong", "Yuanbo Wen", "Jun Bi", "Di Huang", "Jiaming Guo", "Jianxing Xu", "Ruibai Xu", "Xinkai Song", "Yifan Hao", "Xuehai Zhou", "Tianshi Chen", "Qi Guo", "Yunji Chen"], "title": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach", "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": "Accepted to OSDI 2025", "summary": "Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been\nwidely deployed in industrial data centers, which requires to develop multiple\nlow-level tensor programs for different platforms. An attractive solution to\nrelieve the programming burden is to transcompile the legacy code of one\nplatform to others. However, current transcompilation techniques struggle with\neither tremendous manual efforts or functional incorrectness, rendering \"Write\nOnce, Run Anywhere\" of tensor programs an open question.\n  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically\ntranslating tensor programs across DLS via both large language models (LLMs)\nand symbolic program synthesis, i.e., neural-symbolic synthesis. The key\ninsight is leveraging the powerful code generation ability of LLM to make\ncostly search-based symbolic synthesis computationally tractable. Concretely,\nwe propose multiple LLM-assisted compilation passes via pre-defined\nmeta-prompts for program transformation. During each program transformation,\nefficient symbolic program synthesis is employed to repair incorrect code\nsnippets with a limited scale. To attain high performance, we propose a\nhierarchical auto-tuning approach to systematically explore both the parameters\nand sequences of transformation passes. Experiments on 4 DLS with distinct\nprogramming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,\nAMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler\ncorrectly translates different tensor programs at the accuracy of 95% on\naverage, and the performance of translated programs achieves up to 2.0x over\nvendor-provided manually-optimized libraries. As a result, the programming\nproductivity of DLS is improved by up to 96.0x via transcompiling legacy tensor\nprograms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aQiMeng-Xpiler\u7684\u65b0\u578b\u8f6c\u8bd1\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u7b26\u53f7\u7a0b\u5e8f\u5408\u6210\uff0c\u81ea\u52a8\u5c06\u5f20\u91cf\u7a0b\u5e8f\u8de8\u5f02\u6784\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff08DLS\uff09\u7ffb\u8bd1\uff0c\u663e\u8457\u63d0\u9ad8\u7f16\u7a0b\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f02\u6784DLS\u5e73\u53f0\u4e0a\u5f20\u91cf\u7a0b\u5e8f\u7684\u591a\u5e73\u53f0\u9002\u914d\u95ee\u9898\uff0c\u51cf\u8f7b\u624b\u52a8\u7f16\u7a0b\u8d1f\u62c5\uff0c\u5b9e\u73b0\u201c\u4e00\u6b21\u7f16\u5199\uff0c\u968f\u5904\u8fd0\u884c\u201d\u7684\u76ee\u6807\uff0c\u4f46\u76ee\u524d\u6280\u672f\u5b58\u5728\u6548\u7387\u4f4e\u6216\u529f\u80fd\u9519\u8bef\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u5408\u6210\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u5f3a\u5927\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7ed3\u5408\u7b26\u53f7\u7a0b\u5e8f\u5408\u6210\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u5143\u63d0\u793a\u548c\u5206\u5c42\u81ea\u52a8\u8c03\u4f18\u5b9e\u73b0\u9ad8\u6548\u8f6c\u8bd1\u3002", "result": "\u57284\u79cdDLS\u5e73\u53f0\u4e0a\uff0cQiMeng-Xpiler\u5e73\u5747\u7ffb\u8bd1\u51c6\u786e\u7387\u8fbe95%\uff0c\u6027\u80fd\u6700\u9ad8\u63d0\u53472.0\u500d\uff0c\u7f16\u7a0b\u6548\u7387\u63d0\u534796.0\u500d\u3002", "conclusion": "QiMeng-Xpiler\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5e73\u53f0\u5f20\u91cf\u7a0b\u5e8f\u7684\u7ffb\u8bd1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7f16\u7a0b\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3aDLS\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.01591", "pdf": "https://arxiv.org/pdf/2505.01591", "abs": "https://arxiv.org/abs/2505.01591", "authors": ["Abdalwahab Almajed", "Maryam Tabar", "Peyman Najafirad"], "title": "Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises", "categories": ["cs.LG"], "comment": "Accepted at ACM-COMPASS2025", "summary": "As a basic human need, housing plays a key role in enhancing health,\nwell-being, and educational outcome in society, and the housing market is a\nmajor factor for promoting quality of life and ensuring social equity. To\nimprove the housing conditions, there has been extensive research on building\nMachine Learning (ML)-driven house price prediction solutions to accurately\nforecast the future conditions, and help inform actions and policies in the\nfield. In spite of their success in developing high-accuracy models, there is a\ngap in our understanding of the extent to which various ML-driven house price\nprediction approaches show ethnic and/or racial bias, which in turn is\nessential for the responsible use of ML, and ensuring that the ML-driven\nsolutions do not exacerbate inequity. To fill this gap, this paper develops\nseveral ML models from a combination of structural and neighborhood-level\nattributes, and conducts comprehensive assessments on the fairness of ML models\nunder various definitions of privileged groups. As a result, it finds that the\nML-driven house price prediction models show various levels of bias towards\nprotected attributes (i.e., race and ethnicity in this study). Then, it\ninvestigates the performance of different bias mitigation solutions, and the\nexperimental results show their various levels of effectiveness on different\nML-driven methods. However, in general, the in-processing bias mitigation\napproach tends to be more effective than the pre-processing one in this problem\ndomain. Our code is available at https://github.com/wahab1412/housing_fairness.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u5728\u623f\u4ef7\u9884\u6d4b\u4e2d\u7684\u79cd\u65cf/\u6c11\u65cf\u504f\u89c1\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u591a\u79cd\u6a21\u578b\u5e76\u8bc4\u4f30\u5176\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u504f\u89c1\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f4f\u623f\u662f\u793e\u4f1a\u57fa\u672c\u9700\u6c42\uff0c\u5bf9\u5065\u5eb7\u3001\u798f\u7949\u548c\u6559\u80b2\u6210\u679c\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u5728\u623f\u4ef7\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5176\u53ef\u80fd\u5b58\u5728\u7684\u79cd\u65cf/\u6c11\u65cf\u504f\u89c1\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u8fd9\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\u3002", "method": "\u8bba\u6587\u7ed3\u5408\u7ed3\u6784\u6027\u548c\u793e\u533a\u5c42\u9762\u7684\u5c5e\u6027\u5f00\u53d1\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5bf9\u4e0d\u540c\u7279\u6743\u7fa4\u4f53\u5b9a\u4e49\u4e0b\u7684\u6a21\u578b\u516c\u5e73\u6027\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u8fd8\u6d4b\u8bd5\u4e86\u4e0d\u540c\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u673a\u5668\u5b66\u4e60\u623f\u4ef7\u9884\u6d4b\u6a21\u578b\u5bf9\u53d7\u4fdd\u62a4\u5c5e\u6027\uff08\u79cd\u65cf\u548c\u6c11\u65cf\uff09\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u504f\u89c1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6b64\u95ee\u9898\u9886\u57df\u4e2d\uff0c\u5904\u7406\u4e2d\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u901a\u5e38\u6bd4\u9884\u5904\u7406\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86\u5728\u623f\u4ef7\u9884\u6d4b\u4e2d\u8003\u8651\u673a\u5668\u5b66\u4e60\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5904\u7406\u4e2d\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02271", "pdf": "https://arxiv.org/pdf/2505.02271", "abs": "https://arxiv.org/abs/2505.02271", "authors": ["David Nazareno Campo", "Javier Conde", "\u00c1lvaro Alonso", "Gabriel Huecas", "Joaqu\u00edn Salvach\u00faa", "Pedro Reviriego"], "title": "Real-time Spatial Retrieval Augmented Generation for Urban Environments", "categories": ["cs.AI"], "comment": null, "summary": "The proliferation of Generative Artificial Ingelligence (AI), especially\nLarge Language Models, presents transformative opportunities for urban\napplications through Urban Foundation Models. However, base models face\nlimitations, as they only contain the knowledge available at the time of\ntraining, and updating them is both time-consuming and costly. Retrieval\nAugmented Generation (RAG) has emerged in the literature as the preferred\napproach for injecting contextual information into Foundation Models. It\nprevails over techniques such as fine-tuning, which are less effective in\ndynamic, real-time scenarios like those found in urban environments. However,\ntraditional RAG architectures, based on semantic databases, knowledge graphs,\nstructured data, or AI-powered web searches, do not fully meet the demands of\nurban contexts. Urban environments are complex systems characterized by large\nvolumes of interconnected data, frequent updates, real-time processing\nrequirements, security needs, and strong links to the physical world. This work\nproposes a real-time spatial RAG architecture that defines the necessary\ncomponents for the effective integration of generative AI into cities,\nleveraging temporal and spatial filtering capabilities through linked data. The\nproposed architecture is implemented using FIWARE, an ecosystem of software\ncomponents to develop smart city solutions and digital twins. The design and\nimplementation are demonstrated through the use case of a tourism assistant in\nthe city of Madrid. The use case serves to validate the correct integration of\nFoundation Models through the proposed RAG architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7a7a\u95f4RAG\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edfRAG\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u65f6\u7a7a\u8fc7\u6ee4\u80fd\u529b\u4f18\u5316\u751f\u6210\u5f0fAI\u5728\u57ce\u5e02\u7684\u96c6\u6210\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u57ce\u5e02\u7684\u5e94\u7528\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u53d7\u9650\u4e8e\u8bad\u7ec3\u65f6\u7684\u77e5\u8bc6\u4e14\u66f4\u65b0\u6210\u672c\u9ad8\uff0c\u4f20\u7edfRAG\u67b6\u6784\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u57ce\u5e02\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eFIWARE\u7684\u5b9e\u65f6\u7a7a\u95f4RAG\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u7a7a\u8fc7\u6ee4\u548c\u94fe\u63a5\u6570\u636e\u80fd\u529b\uff0c\u4ee5\u9a6c\u5fb7\u91cc\u65c5\u6e38\u52a9\u624b\u4e3a\u7528\u4f8b\u9a8c\u8bc1\u3002", "result": "\u8be5\u67b6\u6784\u6210\u529f\u9a8c\u8bc1\u4e86\u901a\u8fc7RAG\u6709\u6548\u96c6\u6210\u57fa\u7840\u6a21\u578b\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u5b9e\u65f6\u7a7a\u95f4RAG\u67b6\u6784\u4e3a\u751f\u6210\u5f0fAI\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156", "abs": "https://arxiv.org/abs/2505.02156", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The code and data are available, see\n  https://github.com/MozerWang/AMPO. arXiv admin note: text overlap with\n  arXiv:2502.15538 by other authors", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode\n$\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four\nthinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on\nreal-time context. Our framework's core innovation, the $\\textbf{A}$daptive\n$\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach", "AI": {"tldr": "AML\u65b9\u6cd5\uff08\u81ea\u9002\u5e94\u6a21\u5f0f\u5b66\u4e60\uff09\u901a\u8fc7\u5b9e\u65f6\u9009\u62e9\u56db\u79cd\u601d\u8003\u6a21\u5f0f\uff08\u76f4\u89c9\u53cd\u5e94\u2192\u6df1\u5ea6\u601d\u8003\uff09\u6765\u4f18\u5316\u793e\u4ea4\u667a\u80fd\u6a21\u62df\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u5927\u8bed\u8a00\u94fe\u63a8\u7406\u3002AMPO\u7b97\u6cd5\u7684\u5173\u952e\u521b\u65b0\u5305\u62ec\u591a\u7c92\u5ea6\u601d\u8003\u6a21\u5f0f\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5f0f\u5207\u6362\u548c\u6df1\u5ea6\u81ea\u9002\u5e94\u5904\u7406\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u63d0\u534715.6%\uff0c\u63a8\u7406\u94fe\u7f29\u77ed32.8%\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u667a\u80fd\u6a21\u62df\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u7684\u80fd\u529b\uff0c\u8981\u4e48\u5728\u6240\u6709\u573a\u666f\u4e2d\u5f3a\u5236\u4f7f\u7528\u957f\u94fe\u63a8\u7406\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u4e0d\u9002\u5b9a\u6a21\u62df\u3002AML\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAML\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662fAMPO\u7b97\u6cd5\uff0c\u5305\u542b\u591a\u7c92\u5ea6\u601d\u8003\u6a21\u5f0f\u8bbe\u8ba1\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5f0f\u5207\u6362\u548c\u6df1\u5ea6\u81ea\u9002\u5e94\u5904\u7406\u4e09\u4e2a\u521b\u65b0\u70b9\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAML\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u534715.6%\uff0c\u63a8\u7406\u94fe\u7f29\u77ed32.8%\u3002", "conclusion": "AML\u901a\u8fc7\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u601d\u8003\u6a21\u5f0f\u9009\u62e9\uff0c\u5b9e\u73b0\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u4f18\u4e8eGRPO\u7684\u56fa\u5b9a\u6df1\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2505.01618", "pdf": "https://arxiv.org/pdf/2505.01618", "abs": "https://arxiv.org/abs/2505.01618", "authors": ["Nolan Dey", "Bin Claire Zhang", "Lorenzo Noci", "Mufan Li", "Blake Bordelon", "Shane Bergsma", "Cengiz Pehlevan", "Boris Hanin", "Joel Hestness"], "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers", "categories": ["cs.LG", "cs.AI"], "comment": "9 main pages, 17 appendix pages, 13 figures", "summary": "We study compute efficiency of LLM training when using different\nparameterizations, i.e., rules for adjusting model and optimizer\nhyperparameters (HPs) as model size changes. Some parameterizations fail to\ntransfer optimal base HPs (such as learning rate) across changes in model\ndepth, requiring practitioners to either re-tune these HPs as they scale up\n(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even\nwhen they achieve HP transfer, we develop theory to show parameterizations may\nstill exist in the lazy learning regime where layers learn only features close\nto their linearization, preventing effective use of depth and nonlinearity.\nFinally, we identify and adopt the unique parameterization we call CompleteP\nthat achieves both depth-wise HP transfer and non-lazy learning in all layers.\nCompleteP enables a wider range of model width/depth ratios to remain\ncompute-efficient, unlocking shapes better suited for different hardware\nsettings and operational contexts. Moreover, CompleteP enables 12-34\\% compute\nefficiency improvements over the prior state-of-the-art.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u53c2\u6570\u5316\u65b9\u6cd5\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bad\u7ec3\u6548\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u540d\u4e3aCompleteP\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u65e2\u80fd\u5b9e\u73b0\u8d85\u53c2\u6570\u8de8\u6a21\u578b\u6df1\u5ea6\u7684\u9ad8\u6548\u8fc1\u79fb\uff0c\u53c8\u80fd\u907f\u514d\u5206\u5c42\u5b66\u4e60\u7684\u60f0\u6027\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0812-34%\uff09\u3002", "motivation": "\u76ee\u524dLLM\u8bad\u7ec3\u4e2d\uff0c\u8d85\u53c2\u6570\uff08\u5982\u5b66\u4e60\u7387\uff09\u7684\u8c03\u6574\u89c4\u5219\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u9700\u8981\u91cd\u65b0\u8c03\u53c2\u6216\u63a5\u53d7\u6b21\u4f18\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5206\u5c42\u5b66\u4e60\u7684\u60f0\u6027\u9650\u5236\u4e86\u6a21\u578b\u6df1\u5ea6\u548c\u975e\u7ebf\u6027\u7684\u6709\u6548\u5229\u7528\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86\u4e0d\u540c\u53c2\u6570\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u63a8\u5bfc\u63d0\u51fa\u4e86CompleteP\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u8d85\u53c2\u6570\u8de8\u6a21\u578b\u6df1\u5ea6\u7684\u9ad8\u6548\u8fc1\u79fb\u548c\u975e\u60f0\u6027\u5b66\u4e60\u3002", "result": "CompleteP\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0812-34%\uff09\uff0c\u4e14\u652f\u6301\u66f4\u7075\u6d3b\u7684\u6a21\u578b\u5bbd\u5ea6/\u6df1\u5ea6\u6bd4\u4f8b\uff0c\u9002\u914d\u4e0d\u540c\u786c\u4ef6\u548c\u573a\u666f\u3002", "conclusion": "CompleteP\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8de8\u6a21\u578b\u89c4\u6a21\u8d85\u53c2\u6570\u8fc1\u79fb\u548c\u975e\u60f0\u6027\u5b66\u4e60\u95ee\u9898\uff0c\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02279", "pdf": "https://arxiv.org/pdf/2505.02279", "abs": "https://arxiv.org/abs/2505.02279", "authors": ["Abul Ehtesham", "Aditi Singh", "Gaurav Kumar Gupta", "Saket Kumar"], "title": "A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM)-powered autonomous agents demand robust,\nstandardized protocols to integrate tools, share contextual data, and\ncoordinate tasks across heterogeneous systems. Ad-hoc integrations are\ndifficult to scale, secure, and generalize across domains. This survey examines\nfour emerging agent communication protocols: Model Context Protocol (MCP),\nAgent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent\nNetwork Protocol (ANP), each addressing interoperability in distinct deployment\ncontexts. MCP provides a JSON-RPC client-server interface for secure tool\ninvocation and typed data exchange. ACP introduces REST-native messaging via\nmulti-part messages and asynchronous streaming to support multimodal agent\nresponses. A2A enables peer-to-peer task outsourcing through capability-based\nAgent Cards, facilitating enterprise-scale workflows. ANP supports open-network\nagent discovery and secure collaboration using decentralized identifiers (DIDs)\nand JSON-LD graphs. The protocols are compared across multiple dimensions,\nincluding interaction modes, discovery mechanisms, communication patterns, and\nsecurity models. Based on the comparative analysis, a phased adoption roadmap\nis proposed: beginning with MCP for tool access, followed by ACP for multimodal\nmessaging, A2A for collaborative task execution, and extending to ANP for\ndecentralized agent marketplaces. This work provides a comprehensive foundation\nfor designing secure, interoperable, and scalable ecosystems of LLM-powered\nagents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u56db\u79cd\u65b0\u5174\u7684\u667a\u80fd\u4f53\u901a\u4fe1\u534f\u8bae\uff08MCP\u3001ACP\u3001A2A\u548cANP\uff09\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u4ea4\u4e92\u6a21\u5f0f\u3001\u53d1\u73b0\u673a\u5236\u548c\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u5206\u9636\u6bb5\u91c7\u7528\u7684\u8def\u7ebf\u56fe\uff0c\u65e8\u5728\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684LLM\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u96c6\u6210\u5de5\u5177\u3001\u5171\u4eab\u6570\u636e\u53ca\u534f\u8c03\u4efb\u52a1\u65f6\u7684\u6807\u51c6\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e34\u65f6\u96c6\u6210\u96be\u4ee5\u6269\u5c55\u3001\u4e0d\u5b89\u5168\u6216\u7f3a\u4e4f\u666e\u9002\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u56db\u79cd\u534f\u8bae\uff08MCP\u3001ACP\u3001A2A\u3001ANP\uff09\u5728\u4ea4\u4e92\u6a21\u5f0f\u3001\u53d1\u73b0\u673a\u5236\u3001\u901a\u4fe1\u6a21\u5f0f\u548c\u5b89\u5168\u6027\u7b49\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63d0\u51fa\u5206\u9636\u6bb5\u91c7\u7528\u8def\u7ebf\u56fe\u3002", "result": "\u660e\u786e\u4e86\u6bcf\u79cd\u534f\u8bae\u7684\u4f18\u52bf\u4e0e\u9002\u7528\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e86\u4eceMCP\u5230ANP\u7684\u6e10\u8fdb\u5f0f\u90e8\u7f72\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u5b89\u5168\u3001\u4e92\u64cd\u4f5c\u4e14\u53ef\u6269\u5c55\u7684LLM\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u7840\u3002"}}
{"id": "2505.02164", "pdf": "https://arxiv.org/pdf/2505.02164", "abs": "https://arxiv.org/abs/2505.02164", "authors": ["Justin Ho", "Alexandra Colby", "William Fisher"], "title": "Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use", "categories": ["cs.CL", "I.2.7; K.5; H.3.3"], "comment": "Submitted to the 7th Workshop on Automated Semantic Analysis of\n  Information in Legal Text. 8 pages, 5 Figures", "summary": "This paper presents a domain-specific implementation of Retrieval-Augmented\nGeneration (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.\nMotivated by the increasing prevalence of DMCA takedowns and the lack of\naccessible legal support for content creators, we propose a structured approach\nthat combines semantic search with legal knowledge graphs and court citation\nnetworks to improve retrieval quality and reasoning reliability. Our prototype\nmodels legal precedents at the statutory factor level (e.g., purpose, nature,\namount, market effect) and incorporates citation-weighted graph representations\nto prioritize doctrinally authoritative sources. We use Chain-of-Thought\nreasoning and interleaved retrieval steps to better emulate legal reasoning.\nPreliminary testing suggests this method improves doctrinal relevance in the\nretrieval process, laying groundwork for future evaluation and deployment of\nLLM-based legal assistance tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7f8e\u56fd\u7248\u6743\u6cd5\u5408\u7406\u4f7f\u7528\u539f\u5219\u7684\u9886\u57df\u7279\u5f02\u6027\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5b9e\u73b0\uff0c\u65e8\u5728\u89e3\u51b3DMCA\u4e0b\u67b6\u901a\u77e5\u589e\u591a\u548c\u5185\u5bb9\u521b\u4f5c\u8005\u7f3a\u4e4f\u6cd5\u5f8b\u63f4\u52a9\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662fDMCA\u4e0b\u67b6\u901a\u77e5\u9891\u53d1\uff0c\u800c\u5185\u5bb9\u521b\u4f5c\u8005\u7f3a\u4e4f\u4fbf\u6377\u7684\u6cd5\u5f8b\u652f\u6301\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86\u8bed\u4e49\u641c\u7d22\u3001\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u548c\u6cd5\u9662\u5f15\u8bc1\u7f51\u7edc\uff0c\u91c7\u7528\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u548c\u4ea4\u9519\u68c0\u7d22\u6b65\u9aa4\u4ee5\u6a21\u62df\u6cd5\u5f8b\u63a8\u7406\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u7684\u6559\u4e49\u76f8\u5173\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u4e3a\u57fa\u4e8eLLM\u7684\u6cd5\u5f8b\u8f85\u52a9\u5de5\u5177\u7684\u672a\u6765\u8bc4\u4f30\u548c\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.01619", "pdf": "https://arxiv.org/pdf/2505.01619", "abs": "https://arxiv.org/abs/2505.01619", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Skill-based Safe Reinforcement Learning with Risk Planning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent\nconducts learning by interacting with real-world environments where improper\nactions can induce high costs or lead to severe consequences. In this paper, we\npropose a novel Safe Skill Planning (SSkP) approach to enhance effective safe\nRL by exploiting auxiliary offline demonstration data. SSkP involves a\ntwo-stage process. First, we employ PU learning to learn a skill risk predictor\nfrom the offline demonstration data. Then, based on the learned skill risk\npredictor, we develop a novel risk planning process to enhance online safe RL\nand learn a risk-averse safe policy efficiently through interactions with the\nonline RL environment, while simultaneously adapting the skill risk predictor\nto the environment. We conduct experiments in several benchmark robotic\nsimulation environments. The experimental results demonstrate that the proposed\napproach consistently outperforms previous state-of-the-art safe RL methods.", "AI": {"tldr": "This paper proposes Safe Skill Planning (SSkP), a two-stage approach combining offline demonstration data with online reinforcement learning to enhance safe RL by predicting skill risks and planning accordingly.", "motivation": "Improving safety in RL is crucial when agents interact with real-world environments, as improper actions can have severe consequences. Leveraging offline data can help predict risks and guide safer online learning.", "method": "SSkP first uses PU learning to predict skill risks from offline data, then integrates this predictor into an online RL process for risk-averse policy learning and risk predictor adaptation.", "result": "Experiments in robotic simulation environments show SSkP outperforms existing safe RL methods in safety and performance.", "conclusion": "SSkP effectively combines offline risk prediction with online RL, offering a robust solution for safe reinforcement learning in dynamic environments."}}
{"id": "2505.02306", "pdf": "https://arxiv.org/pdf/2505.02306", "abs": "https://arxiv.org/abs/2505.02306", "authors": ["Junfeng Jiao", "Jihyung Park", "Yiming Xu", "Lucy Atkinson"], "title": "SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness", "categories": ["cs.AI"], "comment": null, "summary": "Despite the abundance of public safety documents and emergency protocols,\nmost individuals remain ill-equipped to interpret and act on such information\nduring crises. Traditional emergency decision support systems (EDSS) are\ndesigned for professionals and rely heavily on static documents like PDFs or\nSOPs, which are difficult for non-experts to navigate under stress. This gap\nbetween institutional knowledge and public accessibility poses a critical\nbarrier to effective emergency preparedness and response.\n  We introduce SafeMate, a retrieval-augmented AI assistant that delivers\naccurate, context-aware guidance to general users in both preparedness and\nactive emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate\ndynamically routes user queries to tools for document retrieval, checklist\ngeneration, and structured summarization. It uses FAISS with cosine similarity\nto identify relevant content from trusted sources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSafeMate\uff0c\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684AI\u52a9\u624b\uff0c\u65e8\u5728\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u51c6\u786e\u6307\u5bfc\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5e94\u6025\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5bf9\u975e\u4e13\u5bb6\u4e0d\u53cb\u597d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u516c\u5171\u5b89\u5168\u6587\u6863\u548c\u5e94\u6025\u534f\u8bae\u867d\u591a\uff0c\u4f46\u666e\u901a\u7528\u6237\u5728\u5371\u673a\u4e2d\u96be\u4ee5\u7406\u89e3\u548c\u6267\u884c\uff1b\u4f20\u7edf\u5e94\u6025\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08EDSS\uff09\u4e3b\u8981\u9762\u5411\u4e13\u4e1a\u4eba\u58eb\uff0c\u4f9d\u8d56\u9759\u6001\u6587\u6863\uff08\u5982PDF\u6216SOP\uff09\uff0c\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u5728\u538b\u529b\u4e0b\u4f7f\u7528\u4e0d\u53cb\u597d\u3002", "method": "SafeMate\u57fa\u4e8eModel Context Protocol\uff08MCP\uff09\uff0c\u52a8\u6001\u8def\u7531\u7528\u6237\u67e5\u8be2\u81f3\u6587\u6863\u68c0\u7d22\u3001\u6e05\u5355\u751f\u6210\u548c\u7ed3\u6784\u5316\u603b\u7ed3\u5de5\u5177\uff0c\u5e76\u91c7\u7528FAISS\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4ece\u53ef\u4fe1\u6765\u6e90\u8bc6\u522b\u76f8\u5173\u5185\u5bb9\u3002", "result": "SafeMate\u80fd\u591f\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u51c6\u786e\u3001\u60c5\u5883\u611f\u77e5\u7684\u5e94\u6025\u51c6\u5907\u548c\u54cd\u5e94\u6307\u5bfc\u3002", "conclusion": "SafeMate\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684AI\u6280\u672f\uff0c\u6709\u6548\u5f25\u5408\u4e86\u673a\u6784\u77e5\u8bc6\u4e0e\u516c\u4f17\u53ef\u8bbf\u95ee\u6027\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u4e86\u5e94\u6025\u51c6\u5907\u4e0e\u54cd\u5e94\u7684\u6548\u7387\u3002"}}
{"id": "2505.02171", "pdf": "https://arxiv.org/pdf/2505.02171", "abs": "https://arxiv.org/abs/2505.02171", "authors": ["Henrik Br\u00e5dland", "Morten Goodwin", "Per-Arne Andersen", "Alexander S. Nossum", "Aditya Gupta"], "title": "A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, To be published in SIGIR25", "summary": "Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)\nby determining how source materials are segmented before indexing. Despite\nevidence that Large Language Models (LLMs) are sensitive to the layout and\nstructure of retrieved data, there is currently no framework to analyze the\nimpact of different chunking methods. In this paper, we introduce a novel\nmethodology that defines essential characteristics of the chunking process at\nthree levels: intrinsic passage properties, extrinsic passage properties, and\npassages-document coherence. We propose HOPE (Holistic Passage Evaluation), a\ndomain-agnostic, automatic evaluation metric that quantifies and aggregates\nthese characteristics. Our empirical evaluations across seven domains\ndemonstrate that the HOPE metric correlates significantly (p > 0.13) with\nvarious RAG performance indicators, revealing contrasts between the importance\nof extrinsic and intrinsic properties of passages. Semantic independence\nbetween passages proves essential for system performance with a performance\ngain of up to 56.2% in factual correctness and 21.1% in answer correctness. On\nthe contrary, traditional assumptions about maintaining concept unity within\npassages show minimal impact. These findings provide actionable insights for\noptimizing chunking strategies, thus improving RAG system design to produce\nmore factually correct responses.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981", "motivation": "\u7814\u7a76\u53d1\u73b0\u6587\u6863\u5206\u5757\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u6790\u6846\u67b6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u8bc4\u4f30\u4e0d\u540c\u5206\u5757\u7b56\u7565\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faHOPE\uff08\u5168\u6bb5\u843d\u8bc4\u4f30\uff09\u65b9\u6cd5\uff0c\u4ece\u6bb5\u843d\u5185\u7279\u6027\u3001\u6bb5\u843d\u95f4\u7279\u6027\u548c\u6bb5\u843d-\u6587\u6863\u4e00\u81f4\u6027\u4e09\u4e2a\u5c42\u6b21\u91cf\u5316\u5206\u5757\u7279\u6027\uff0c\u5e76\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\u3002", "result": "\u5728\u4e03\u4e2a\u9886\u57df\u7684\u5b9e\u8bc1\u8bc4\u4f30\u4e2d\uff0cHOPE\u6307\u6807\u4e0eRAG\u6027\u80fd\u663e\u8457\u76f8\u5173\uff08p>0.13\uff09\u3002\u6bb5\u843d\u95f4\u8bed\u4e49\u72ec\u7acb\u6027\u5bf9\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u4e8b\u5b9e\u6b63\u786e\u6027\u63d0\u534756.2%\uff0c\u7b54\u6848\u6b63\u786e\u6027\u63d0\u534721.1%\uff09\uff0c\u800c\u4f20\u7edf\u8ba4\u4e3a\u7684\u6bb5\u843d\u5185\u6982\u5ff5\u7edf\u4e00\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "HOPE\u65b9\u6cd5\u4e3a\u4f18\u5316\u5206\u5757\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u51c6\u786e\u7684RAG\u7cfb\u7edf\u3002"}}
{"id": "2505.01627", "pdf": "https://arxiv.org/pdf/2505.01627", "abs": "https://arxiv.org/abs/2505.01627", "authors": ["Fatemeh Elhambakhsh", "Daniele Grandi", "Hyunwoong Ko"], "title": "A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components", "categories": ["cs.LG", "cs.CE"], "comment": null, "summary": "The conceptual design phase represents a critical early stage in the product\ndevelopment process, where designers generate potential solutions that meet\npredefined design specifications based on functional requirements. Functional\nmodeling, a foundational aspect of this phase, enables designers to reason\nabout product functions before specific structural details are determined. A\nwidely adopted approach to functional modeling is the\nFunction-Behavior-Structure (FBS) framework, which supports the transformation\nof functional intent into behavioral and structural descriptions. However, the\neffectiveness of function-based design is often hindered by the lack of\nwell-structured and comprehensive functional data. This scarcity can negatively\nimpact early design decision-making and hinder the development of accurate\nbehavioral models. Recent advances in Large Language Models (LLMs), such as\nthose based on GPT architectures, offer a promising avenue to address this gap.\nLLMs have demonstrated significant capabilities in language understanding and\nnatural language processing (NLP), making them suitable for automated\nclassification tasks. This study proposes a novel LLM-based domain adaptation\n(DA) framework using fine-tuning for the automated classification of mechanical\nassembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the\ntraditionally manual and subjective process of function annotation can be\nimproved in both accuracy and consistency. A case study demonstrates\nfine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository\n(OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the\ndomain-adapted LLM can generate high-quality functional data, enhancing the\nsemantic representation of mechanical parts and supporting more effective\ndesign exploration in early-phase engineering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03GPT-3.5 Turbo\u6765\u81ea\u52a8\u5206\u7c7b\u673a\u68b0\u88c5\u914d\u96f6\u4ef6\u7684\u529f\u80fd\uff0c\u63d0\u5347\u529f\u80fd\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u529f\u80fd\u5efa\u6a21\u5728\u6982\u5ff5\u8bbe\u8ba1\u9636\u6bb5\u7f3a\u4e4f\u7ed3\u6784\u5316\u529f\u80fd\u6570\u636e\uff0c\u5f71\u54cd\u4e86\u65e9\u671f\u8bbe\u8ba1\u51b3\u7b56\u548c\u884c\u4e3a\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002LLMs\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u4f18\u52bf\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9GPT-3.5 Turbo\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\uff08\u5982OSDR\uff09\u6765\u5b9e\u73b0\u673a\u68b0\u96f6\u4ef6\u529f\u80fd\u7684\u81ea\u52a8\u5206\u7c7b\u3002", "result": "\u5728ABC\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u7ecf\u8fc7\u9886\u57df\u9002\u5e94\u7684LLM\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u529f\u80fd\u6570\u636e\uff0c\u63d0\u5347\u673a\u68b0\u96f6\u4ef6\u7684\u8bed\u4e49\u8868\u793a\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u65e9\u671f\u5de5\u7a0b\u8bbe\u8ba1\u3002", "conclusion": "LLM\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86\u529f\u80fd\u6570\u636e\u7684\u81ea\u52a8\u751f\u6210\uff0c\u4e3a\u6982\u5ff5\u8bbe\u8ba1\u9636\u6bb5\u7684\u529f\u80fd\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2505.02322", "pdf": "https://arxiv.org/pdf/2505.02322", "abs": "https://arxiv.org/abs/2505.02322", "authors": ["Runquan Gui", "Zhihai Wang", "Jie Wang", "Chi Ma", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Defu Lian", "Enhong Chen", "Feng Wu"], "title": "HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking", "categories": ["cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2406.14228 by other authors", "summary": "Recent advancements have significantly enhanced the performance of large\nlanguage models (LLMs) in tackling complex reasoning tasks, achieving notable\nsuccess in domains like mathematical and logical reasoning. However, these\nmethods encounter challenges with complex planning tasks, primarily due to\nextended reasoning steps, diverse constraints, and the challenge of handling\nmultiple distinct sub-tasks. To address these challenges, we propose HyperTree\nPlanning (HTP), a novel reasoning paradigm that constructs hypertree-structured\nplanning outlines for effective planning. The hypertree structure enables LLMs\nto engage in hierarchical thinking by flexibly employing the divide-and-conquer\nstrategy, effectively breaking down intricate reasoning steps, accommodating\ndiverse constraints, and managing multiple distinct sub-tasks in a\nwell-organized manner. We further introduce an autonomous planning framework\nthat completes the planning process by iteratively refining and expanding the\nhypertree-structured planning outlines. Experiments demonstrate the\neffectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner\nbenchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement\nover o1-preview.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyperTree Planning (HTP)\u7684\u65b0\u578b\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u6784\u5efa\u8d85\u6811\u7ed3\u6784\u7684\u89c4\u5212\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728TravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c4\u5212\u4efb\u52a1\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u5982\u63a8\u7406\u6b65\u9aa4\u957f\u3001\u7ea6\u675f\u591a\u6837\u548c\u5b50\u4efb\u52a1\u7ba1\u7406\u590d\u6742\u3002HTP\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u5206\u6cbb\u7b56\u7565\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "HTP\u901a\u8fc7\u6784\u5efa\u8d85\u6811\u7ed3\u6784\u7684\u89c4\u5212\u6846\u67b6\uff0c\u5206\u5c42\u6b21\u5206\u89e3\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u7ed3\u5408\u81ea\u4e3b\u89c4\u5212\u6846\u67b6\u8fed\u4ee3\u4f18\u5316\u548c\u6269\u5c55\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHTP\u5728TravelPlanner\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u6027\u80fd\u6bd4o1-preview\u63d0\u5347\u4e863.6\u500d\u3002", "conclusion": "HTP\u4e3a\u590d\u6742\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7ed3\u6784\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2505.02172", "pdf": "https://arxiv.org/pdf/2505.02172", "abs": "https://arxiv.org/abs/2505.02172", "authors": ["Chuck Arvin"], "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization", "categories": ["cs.CL"], "comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)", "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6cd5\u5f8b\u57fa\u51c6\u6570\u636e\u96c6CaseHOLD\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u968f\u89c4\u6a21\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u8fbe\u5230\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u6027\u80fd\u975e\u56e0\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u968f\u7740LLMs\u80fd\u529b\u63d0\u5347\uff0c\u9700\u8bc4\u4f30\u5176\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u65e0\u9700\u590d\u6742\u6280\u672f\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\u662f\u5426\u80fd\u8fbe\u5230\u5b9e\u7528\u6c34\u5e73\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684LLMs\uff083B\u81f390B+\uff09\u5728CaseHOLD\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8bbe\u8ba1\u5f15\u7528\u533f\u540d\u5316\u6d4b\u8bd5\u4ee5\u6392\u9664\u8bb0\u5fc6\u6548\u5e94\u3002", "result": "\u8f83\u5927\u6a21\u578b\uff08\u5982GPT4o\u3001AmazonNovaPro\uff09\u8868\u73b0\u6700\u4f73\uff08\u5b8fF1\u5206\u522b\u8fbe0.744\u548c0.720\uff09\uff0c\u533f\u540d\u5316\u6d4b\u8bd5\u4e0b\u6027\u80fd\u7a33\u5b9a\uff080.728\uff09\uff0c\u8868\u660e\u7ed3\u679c\u975e\u56e0\u8bb0\u5fc6\u3002", "conclusion": "LLMs\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e0e\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u63a8\u52a8\u81ea\u52a8\u5316\u6cd5\u5f8b\u5206\u6790\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.01652", "pdf": "https://arxiv.org/pdf/2505.01652", "abs": "https://arxiv.org/abs/2505.01652", "authors": ["Yucong Dai", "Lu Zhang", "Yaowei Hu", "Susan Gauch", "Yongkai Wu"], "title": "Causally Fair Node Classification on Non-IID Graph Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fair machine learning seeks to identify and mitigate biases in predictions\nagainst unfavorable populations characterized by demographic attributes, such\nas race and gender. Recently, a few works have extended fairness to graph data,\nsuch as social networks, but most of them neglect the causal relationships\namong data instances. This paper addresses the prevalent challenge in\nfairness-aware ML algorithms, which typically assume Independent and\nIdentically Distributed (IID) data. We tackle the overlooked domain of non-IID,\ngraph-based settings where data instances are interconnected, influencing the\noutcomes of fairness interventions. We base our research on the Network\nStructural Causal Model (NSCM) framework and posit two main assumptions:\nDecomposability and Graph Independence, which enable the computation of\ninterventional distributions in non-IID settings using the $do$-calculus. Based\non that, we develop the Message Passing Variational Autoencoder for Causal\nInference (MPVA) to compute interventional distributions and facilitate\ncausally fair node classification through estimated interventional\ndistributions. Empirical evaluations on semi-synthetic and real-world datasets\ndemonstrate that MPVA outperforms conventional methods by effectively\napproximating interventional distributions and mitigating bias. The\nimplications of our findings underscore the potential of causality-based\nfairness in complex ML applications, setting the stage for further research\ninto relaxing the initial assumptions to enhance model fairness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u56fe\u6570\u636e\u516c\u5e73\u6027\u65b9\u6cd5MPVA\uff0c\u901a\u8fc7NSCM\u6846\u67b6\u548c$do$-calculus\u8ba1\u7b97\u5e72\u9884\u5206\u5e03\uff0c\u6709\u6548\u51cf\u8f7b\u975eIID\u56fe\u6570\u636e\u4e2d\u7684\u504f\u89c1\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u516c\u5e73\u6027\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u591a\u6570\u5047\u8bbeIID\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u56fe\u6570\u636e\u4e2d\u5b9e\u4f8b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u975eIID\u56fe\u6570\u636e\u4e2d\u6548\u679c\u6709\u9650\u3002", "method": "\u57fa\u4e8eNSCM\u6846\u67b6\u548c\u4e24\u4e2a\u5047\u8bbe\uff08\u53ef\u5206\u89e3\u6027\u548c\u56fe\u72ec\u7acb\u6027\uff09\uff0c\u63d0\u51faMPVA\u6a21\u578b\uff0c\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6d88\u606f\u4f20\u9012\u673a\u5236\u4f30\u7b97\u5e72\u9884\u5206\u5e03\u3002", "result": "\u5728\u534a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cMPVA\u80fd\u66f4\u51c6\u786e\u4f30\u7b97\u5e72\u9884\u5206\u5e03\uff0c\u663e\u8457\u51cf\u5c11\u504f\u89c1\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u56e0\u679c\u516c\u5e73\u6027\u5728\u590d\u6742ML\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u672a\u6765\u53ef\u653e\u5bbd\u521d\u59cb\u5047\u8bbe\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u516c\u5e73\u6027\u3002"}}
{"id": "2505.02413", "pdf": "https://arxiv.org/pdf/2505.02413", "abs": "https://arxiv.org/abs/2505.02413", "authors": ["Baoxia Du", "Hongyang Du", "Dusit Niyato", "Ruidong Li"], "title": "Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks", "categories": ["cs.AI"], "comment": null, "summary": "Task-oriented semantic communication has emerged as a fundamental approach\nfor enhancing performance in various communication scenarios. While recent\nadvances in Generative Artificial Intelligence (GenAI), such as Large Language\nModels (LLMs), have been applied to semantic communication designs, the\npotential of Large Multimodal Models (LMMs) remains largely unexplored. In this\npaper, we investigate an LMM-based vehicle AI assistant using a Large Language\nand Vision Assistant (LLaVA) and propose a task-oriented semantic communication\nframework to facilitate efficient interaction between users and cloud servers.\nTo reduce computational demands and shorten response time, we optimize LLaVA's\nimage slicing to selectively focus on areas of utmost interest to users.\nAdditionally, we assess the importance of image patches by combining objective\nand subjective user attention, adjusting energy usage for transmitting semantic\ninformation. This strategy optimizes resource utilization, ensuring precise\ntransmission of critical information. We construct a Visual Question Answering\n(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental\nresults show that our semantic communication framework significantly increases\naccuracy in answering questions under the same channel conditions, performing\nparticularly well in environments with poor Signal-to-Noise Ratios (SNR).\nAccuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,\nrespectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u8f66\u8f86AI\u52a9\u624b\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u5207\u7247\u548c\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5bfc\u5411\u7684\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u8fd1\u5e74\u6765\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5df2\u5e94\u7528\u4e8e\u8bed\u4e49\u901a\u4fe1\u8bbe\u8ba1\uff0c\u4f46\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u8f66\u8f86AI\u52a9\u624b\u7b49\u573a\u666f\u3002", "method": "\u4f7f\u7528LLaVA\u6a21\u578b\u4f18\u5316\u56fe\u50cf\u5207\u7247\uff0c\u7ed3\u5408\u4e3b\u5ba2\u89c2\u7528\u6237\u6ce8\u610f\u529b\u8bc4\u4f30\u56fe\u50cf\u533a\u57df\u91cd\u8981\u6027\uff0c\u5e76\u8c03\u6574\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u7684\u80fd\u91cf\u5206\u914d\uff0c\u6784\u5efa\u4ea4\u901a\u573a\u666fVQA\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u76f8\u540c\u4fe1\u9053\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0812dB\u65f6\u63d0\u534713.4%\uff0c10dB\u65f6\u63d0\u534733.1%\uff09\u3002", "conclusion": "\u57fa\u4e8eLMM\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u80fd\u9ad8\u6548\u4f18\u5316\u8d44\u6e90\u5229\u7528\u5e76\u63d0\u5347\u5173\u952e\u4fe1\u606f\u4f20\u8f93\u7cbe\u5ea6\uff0c\u4e3a\u4efb\u52a1\u5bfc\u5411\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02177", "pdf": "https://arxiv.org/pdf/2505.02177", "abs": "https://arxiv.org/abs/2505.02177", "authors": ["Chuxue Cao", "Zhenghao Zhu", "Junqi Zhu", "Guoying Lu", "Siyu Peng", "Juntao Dai", "Weijie Shi", "Sirui Han", "Yike Guo"], "title": "Measuring Hong Kong Massive Multi-Task Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual understanding is crucial for the cross-cultural applicability of\nLarge Language Models (LLMs). However, evaluation benchmarks designed for Hong\nKong's unique linguistic landscape, which combines Traditional Chinese script\nwith Cantonese as the spoken form and its cultural context, remain\nunderdeveloped. To address this gap, we introduce HKMMLU, a multi-task language\nunderstanding benchmark that evaluates Hong Kong's linguistic competence and\nsocio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions\nacross 66 subjects, organized into four categories: Science, Technology,\nEngineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To\nevaluate the multilingual understanding ability of LLMs, 90,550\nMandarin-Cantonese translation tasks were additionally included. We conduct\ncomprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs\nof varying sizes on HKMMLU. The results show that the best-performing model,\nDeepSeek-V3, struggles to achieve an accuracy of 75\\%, significantly lower than\nthat of MMLU and CMMLU. This performance gap highlights the need to improve\nLLMs' capabilities in Hong Kong-specific language and knowledge domains.\nFurthermore, we investigate how question language, model size, prompting\nstrategies, and question and reasoning token lengths affect model performance.\nWe anticipate that HKMMLU will significantly advance the development of LLMs in\nmultilingual and cross-cultural contexts, thereby enabling broader and more\nimpactful applications.", "AI": {"tldr": "HKMMLU \u662f\u4e00\u4e2a\u9488\u5bf9\u9999\u6e2f\u72ec\u7279\u8bed\u8a00\u73af\u5883\uff08\u7e41\u4f53\u4e2d\u6587\u4e0e\u7ca4\u8bed\uff09\u7684\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\uff0c\u5305\u542b 26,698 \u9053\u591a\u9009\u9898\u548c 90,550 \u9879\u7ffb\u8bd1\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u4f18\u6a21\u578b DeepSeek-V3 \u7684\u51c6\u786e\u7387\u4ec5 75%\uff0c\u8fdc\u4f4e\u4e8e MMLU \u548c CMMLU\uff0c\u51f8\u663e\u4e86 LLMs \u5728\u9999\u6e2f\u8bed\u8a00\u548c\u6587\u5316\u77e5\u8bc6\u9886\u57df\u7684\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u9999\u6e2f\u72ec\u7279\u8bed\u8a00\uff08\u7e41\u4f53\u4e2d\u6587\u4e0e\u7ca4\u8bed\uff09\u548c\u6587\u5316\u80cc\u666f\u4e0b LLMs \u8bc4\u4f30\u57fa\u51c6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316 LLMs \u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa HKMMLU \u57fa\u51c6\uff0c\u5305\u542b\u591a\u9009\u95ee\u9898\u548c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u8986\u76d6 66 \u4e2a\u5b66\u79d1\u548c\u56db\u79cd\u7c7b\u522b\u3002\u6d4b\u8bd5\u4e86 GPT-4o\u3001Claude 3.7 Sonnet \u548c 18 \u4e2a\u5f00\u6e90 LLMs\u3002", "result": "DeepSeek-V3 \u8868\u73b0\u6700\u4f73\uff0c\u4f46\u51c6\u786e\u7387\u4ec5 75%\uff0c\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u57fa\u51c6\u3002\u95ee\u9898\u8bed\u8a00\u3001\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\u548c token \u957f\u5ea6\u7b49\u56e0\u7d20\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "HKMMLU \u63ed\u793a\u4e86 LLMs \u5728\u9999\u6e2f\u8bed\u8a00\u548c\u6587\u5316\u9886\u57df\u7684\u74f6\u9888\uff0c\u672a\u6765\u9700\u63d0\u5347\u5176\u591a\u8bed\u8a00\u548c\u8de8\u6587\u5316\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}}
{"id": "2505.01660", "pdf": "https://arxiv.org/pdf/2505.01660", "abs": "https://arxiv.org/abs/2505.01660", "authors": ["Sicong Li", "Qianqian Xu", "Zhiyong Yang", "Zitai Wang", "Linchao Zhang", "Xiaochun Cao", "Qingming Huang"], "title": "Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification", "categories": ["cs.LG"], "comment": null, "summary": "Real-world datasets often follow a long-tailed distribution, making\ngeneralization to tail classes difficult. Recent methods resorted to long-tail\nvariants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to\nimprove generalization by flattening the loss landscape. However, these\nattempts face a trade-off between computational efficiency and control over the\nloss landscape. On the one hand, ImbSAM is efficient but offers only coarse\ncontrol as it excludes head classes from the SAM process. On the other hand,\nCC-SAM provides fine-grained control through class-dependent perturbations but\nat the cost of efficiency due to multiple backpropagations. Seeing this\ndilemma, we introduce Focal-SAM, which assigns different penalties to\nclass-wise sharpness, achieving fine-grained control without extra\nbackpropagations, thus maintaining efficiency. Furthermore, we theoretically\nanalyze Focal-SAM's generalization ability and derive a sharper generalization\nbound. Extensive experiments on both traditional and foundation models validate\nthe effectiveness of Focal-SAM.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Focal-SAM\uff0c\u4e00\u79cd\u5728\u957f\u5c3e\u6570\u636e\u5206\u5e03\u4e2d\u901a\u8fc7\u5dee\u5f02\u5316\u60e9\u7f5a\u7c7b\u95f4\u9510\u5ea6\u6765\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u9762\u5bf9\u957f\u5c3e\u6570\u636e\u5206\u5e03\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982ImbSAM\u548cCC-SAM\uff09\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u635f\u5931\u666f\u89c2\u63a7\u5236\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u9ad8\u6548\u6027\u548c\u7cbe\u7ec6\u63a7\u5236\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faFocal-SAM\uff0c\u901a\u8fc7\u5bf9\u7c7b\u95f4\u9510\u5ea6\u5206\u914d\u4e0d\u540c\u60e9\u7f5a\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916\u53cd\u5411\u4f20\u64ad\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u517c\u987e\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFocal-SAM\u5728\u4f20\u7edf\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u5747\u6709\u6548\uff0c\u4e14\u6cdb\u5316\u8fb9\u754c\u66f4\u4f18\u3002", "conclusion": "Focal-SAM\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u63a7\u5236\uff0c\u4e3a\u957f\u5c3e\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02439", "pdf": "https://arxiv.org/pdf/2505.02439", "abs": "https://arxiv.org/abs/2505.02439", "authors": ["Yang Deng", "Yaohui Liu", "Rui Liang", "Dafang Zhao", "Donghua Xie", "Ittetsu Taniguchi", "Dan Wang"], "title": "ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "The building thermodynamics model, which predicts real-time indoor\ntemperature changes under potential HVAC (Heating, Ventilation, and Air\nConditioning) control operations, is crucial for optimizing HVAC control in\nbuildings. While pioneering studies have attempted to develop such models for\nvarious building environments, these models often require extensive data\ncollection periods and rely heavily on expert knowledge, making the modeling\nprocess inefficient and limiting the reusability of the models. This paper\nexplores a model ensemble perspective that utilizes existing developed models\nas base models to serve a target building environment, thereby providing\naccurate predictions while reducing the associated efforts. Given that building\ndata streams are non-stationary and the number of base models may increase, we\npropose a Hierarchical Reinforcement Learning (HRL) approach to dynamically\nselect and weight the base models. Our approach employs a two-tiered\ndecision-making process: the high-level focuses on model selection, while the\nlow-level determines the weights of the selected models. We thoroughly evaluate\nthe proposed approach through offline experiments and an on-site case study,\nand the experimental results demonstrate the effectiveness of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u7684\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u9009\u62e9\u548c\u52a0\u6743\u73b0\u6709\u5efa\u7b51\u70ed\u529b\u5b66\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5ba4\u5185\u6e29\u5ea6\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u70ed\u529b\u5b66\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u6536\u96c6\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5efa\u6a21\u6548\u7387\u4f4e\u4e14\u6a21\u578b\u590d\u7528\u6027\u5dee\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u96c6\u6210\u73b0\u6709\u6a21\u578b\u51cf\u5c11\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u6846\u67b6\uff0c\u9ad8\u5c42\u51b3\u7b56\u6a21\u578b\u9009\u62e9\uff0c\u5e95\u5c42\u51b3\u7b56\u6a21\u578b\u6743\u91cd\u5206\u914d\uff0c\u52a8\u6001\u9002\u5e94\u975e\u7a33\u6001\u6570\u636e\u6d41\u3002", "result": "\u79bb\u7ebf\u548c\u73b0\u573a\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u7684\u6e29\u5ea6\u9884\u6d4b\u5e76\u51cf\u5c11\u5efa\u6a21\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684HRL\u65b9\u6cd5\u5728\u63d0\u5347\u6a21\u578b\u590d\u7528\u6027\u548c\u9884\u6d4b\u6548\u7387\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u5efa\u7b51HVAC\u63a7\u5236\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02235", "pdf": "https://arxiv.org/pdf/2505.02235", "abs": "https://arxiv.org/abs/2505.02235", "authors": ["Tanguy Herserant", "Vincent Guigue"], "title": "SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination.", "AI": {"tldr": "SEval-Ex\u662f\u4e00\u4e2a\u6587\u672c\u6458\u8981\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4e3a\u539f\u5b50\u8bed\u53e5\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u6458\u8981\u8bc4\u4f30\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u5148\u7528LLM\u4ece\u6587\u672c\u548c\u6458\u8981\u4e2d\u63d0\u53d6\u539f\u5b50\u8bed\u53e5\uff0c\u518d\u5339\u914d\u751f\u6210\u7684\u8bed\u53e5\u3002", "result": "\u5728SummEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.580\u7684\u76f8\u5173\u6027\uff0c\u4f18\u4e8eGPT-4\u8bc4\u4f30\u5668\uff080.521\uff09\uff0c\u4e14\u5177\u5907\u6297\u5e7b\u89c9\u9c81\u68d2\u6027\u3002", "conclusion": "SEval-Ex\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u662f\u6587\u672c\u6458\u8981\u8bc4\u4f30\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.01665", "pdf": "https://arxiv.org/pdf/2505.01665", "abs": "https://arxiv.org/abs/2505.01665", "authors": ["Wensheng Li", "Hao Wang", "Ruifeng Zhou", "Hanting Guan", "Chao Zhang", "Dacheng Tao"], "title": "Adaptively Point-weighting Curriculum Learning", "categories": ["cs.LG"], "comment": null, "summary": "Curriculum learning (CL) is referred to as a training strategy that makes\neasy samples learned first and then fits hard samples. It imitates the process\nof humans learning knowledge, and has become a potential manner of effectively\ntraining deep networks. In this study, we develop the adaptively\npoint-weighting (APW) curriculum learning algorithm, which adaptively assigns\nthe weight to every training sample not only based on its training error but\nalso considering the current training state of the network. Specifically, in\nthe early training phase, it increases the weights of easy samples to make the\nnetwork rapidly capture the overall characteristics of the dataset; and in the\nlater training phase, the weights of hard points rise to improve the fitting\nperformance on the discrete local regions. Moreover, we also present the\ntheoretical analysis on the properties of APW including training effectiveness,\ntraining feasibility, training stability, and generalization performance. The\nnumerical experiments support the superiority of APW and demonstrate the\nvalidity of our theoretical findings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u70b9\u52a0\u6743\u8bfe\u7a0b\u5b66\u4e60\uff08APW\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6837\u672c\u6743\u91cd\u4f18\u5316\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4ece\u6613\u5230\u96be\u7684\u6837\u672c\u8bad\u7ec3\u63d0\u5347\u6df1\u5ea6\u7f51\u7edc\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "APW\u7b97\u6cd5\u57fa\u4e8e\u6837\u672c\u7684\u8bad\u7ec3\u8bef\u5dee\u548c\u7f51\u7edc\u7684\u5f53\u524d\u72b6\u6001\u52a8\u6001\u5206\u914d\u6743\u91cd\uff0c\u65e9\u671f\u4fa7\u91cd\u6613\u6837\u672c\uff0c\u540e\u671f\u4fa7\u91cd\u96be\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAPW\u5728\u8bad\u7ec3\u6548\u679c\u3001\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u4e5f\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "APW\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u652f\u6301\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u7f51\u7edc\u7684\u8bad\u7ec3\u3002"}}
{"id": "2505.02441", "pdf": "https://arxiv.org/pdf/2505.02441", "abs": "https://arxiv.org/abs/2505.02441", "authors": ["Jiaqi Zhang", "Zhuodong Liu", "Kejian Yu"], "title": "MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection", "categories": ["cs.AI"], "comment": "Accepted to IJCNN 2025", "summary": "Accurate identification of agricultural pests is essential for crop\nprotection but remains challenging due to the large intra-class variance and\nfine-grained differences among pest species. While deep learning has advanced\npest detection, most existing approaches rely solely on low-level visual\nfeatures and lack effective multi-modal integration, leading to limited\naccuracy and poor interpretability. Moreover, the scarcity of high-quality\nmulti-modal agricultural datasets further restricts progress in this field. To\naddress these issues, we construct two novel multi-modal benchmarks-CTIP102 and\nSTIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale\nCross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach\nenhances visual quality via a super-resolution reconstruction module, and feeds\nboth the original and reconstructed images into the network to improve clarity\nand detection performance. To better exploit semantic cues, we propose an\nImage-Text Fusion (ITF) module for joint modeling of visual and textual\nfeatures, and an Image-Text Converter (ITC) that reconstructs fine-grained\ndetails across multiple scales to handle challenging backgrounds. Furthermore,\nwe introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to\ngenerate a more complex and diverse pest detection dataset, MTIP102, improving\nthe model's generalization to real-world scenarios. Extensive experiments\ndemonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods\non multiple pest detection benchmarks. All code and datasets will be made\npublicly available at: https://github.com/Healer-ML/MSFNet-CPD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u8de8\u6a21\u6001\u878d\u5408\u7f51\u7edc\uff08MSFNet-CPD\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u63d0\u5347\u519c\u4e1a\u5bb3\u866b\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u5bb3\u866b\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4e\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u6574\u5408\uff0c\u4e14\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faMSFNet-CPD\uff0c\u7ed3\u5408\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u6a21\u5757\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u7528\u56fe\u50cf-\u6587\u672c\u878d\u5408\u6a21\u5757\uff08ITF\uff09\u548c\u56fe\u50cf-\u6587\u672c\u8f6c\u6362\u5668\uff08ITC\uff09\u6574\u5408\u591a\u6a21\u6001\u7279\u5f81\uff1b\u540c\u65f6\u5f15\u5165ACIE\u7b56\u7565\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMSFNet-CPD\u5728\u591a\u4e2a\u5bb3\u866b\u68c0\u6d4b\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u4e86\u5bb3\u866b\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2505.02252", "pdf": "https://arxiv.org/pdf/2505.02252", "abs": "https://arxiv.org/abs/2505.02252", "authors": ["Paloma Piot", "Patricia Mart\u00edn-Rodilla", "Javier Parapar"], "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Commercial Large Language Models (LLMs) have recently incorporated memory\nfeatures to deliver personalised responses. This memory retains details such as\nuser demographics and individual characteristics, allowing LLMs to adjust their\nbehaviour based on personal information. However, the impact of integrating\npersonalised information into the context has not been thoroughly assessed,\nleading to questions about its influence on LLM behaviour. Personalisation can\nbe challenging, particularly with sensitive topics. In this paper, we examine\nvarious state-of-the-art LLMs to understand their behaviour in different\npersonalisation scenarios, specifically focusing on hate speech. We prompt the\nmodels to assume country-specific personas and use different languages for hate\nspeech detection. Our findings reveal that context personalisation\nsignificantly influences LLMs' responses in this sensitive area. To mitigate\nthese unwanted biases, we fine-tune the LLMs by penalising inconsistent hate\nspeech classifications made with and without country or language-specific\ncontext. The refined models demonstrate improved performance in both\npersonalised contexts and when no context is provided.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u4e2a\u4eba\u5316\u8bed\u5883\u4e0b\u5bf9\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bed\u5883\u4e2a\u4eba\u5316\u663e\u8457\u5f71\u54cd\u5176\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u60e9\u7f5a\u6027\u5fae\u8c03\u51cf\u5c11\u4e86\u504f\u89c1\u3002", "motivation": "\u63a2\u8ba8\u4e2a\u4eba\u5316\u4fe1\u606f\uff08\u5982\u7528\u6237\u7279\u5f81\u6216\u56fd\u5bb6\u7279\u5b9a\u80cc\u666f\uff09\u5bf9LLMs\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u654f\u611f\u8bdd\u9898\uff08\u5982\u4ec7\u6068\u8a00\u8bba\uff09\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u8ba9\u4e0d\u540cLLMs\u6a21\u62df\u56fd\u5bb6\u7279\u5b9a\u89d2\u8272\u5e76\u4f7f\u7528\u591a\u8bed\u8a00\u68c0\u6d4b\u4ec7\u6068\u8a00\u8bba\uff0c\u5206\u6790\u5176\u884c\u4e3a\u5dee\u5f02\uff0c\u968f\u540e\u901a\u8fc7\u60e9\u7f5a\u4e0d\u4e00\u81f4\u5206\u7c7b\u7684\u5fae\u8c03\u4f18\u5316\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u8bed\u5883\u4e2a\u4eba\u5316\u663e\u8457\u5f71\u54cdLLMs\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7ed3\u679c\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u5728\u6709\u65e0\u4e2a\u4eba\u5316\u8bed\u5883\u4e0b\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u4e2a\u4eba\u5316\u8bed\u5883\u53ef\u80fd\u5f15\u5165\u504f\u89c1\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u4f18\u5316LLMs\u5728\u654f\u611f\u9886\u57df\u7684\u516c\u5e73\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.01700", "pdf": "https://arxiv.org/pdf/2505.01700", "abs": "https://arxiv.org/abs/2505.01700", "authors": ["Yize Jiang", "Xinze Li", "Yuanyuan Zhang", "Jin Han", "Youjun Xu", "Ayush Pandit", "Zaixi Zhang", "Mengdi Wang", "Mengyang Wang", "Chong Liu", "Guang Yang", "Yejin Choi", "Wu-Jun Li", "Tianfan Fu", "Fang Wu", "Junhong Liu"], "title": "PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Recently, significant progress has been made in protein-ligand docking,\nespecially in modern deep learning methods, and some benchmarks were proposed,\ne.g., PoseBench, Plinder. However, these benchmarks suffer from less practical\nevaluation setups (e.g., blind docking, self docking), or heavy framework that\ninvolves training, raising challenges to assess docking methods efficiently. To\nfill this gap, we proposed PoseX, an open-source benchmark focusing on\nself-docking and cross-docking, to evaluate the algorithmic advances\npractically and comprehensively. Specifically, first, we curate a new\nevaluation dataset with 718 entries for self docking and 1,312 for cross\ndocking; second, we incorporate 22 docking methods across three methodological\ncategories, including (1) traditional physics-based methods (e.g.,\nSchr\\\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI\nco-folding methods (e.g., AlphaFold3); third, we design a relaxation method as\npost-processing to minimize conformation energy and refine binding pose;\nfourth, we released a leaderboard to rank submitted models in real time. We\ndraw some key insights via extensive experiments: (1) AI-based approaches have\nalready surpassed traditional physics-based approaches in overall docking\naccuracy (RMSD). The longstanding generalization issues that have plagued AI\nmolecular docking have been significantly alleviated in the latest models. (2)\nThe stereochemical deficiencies of AI-based approaches can be greatly\nalleviated with post-processing relaxation. Combining AI docking methods with\nthe enhanced relaxation method achieves the best performance to date. (3) AI\nco-folding methods commonly face ligand chirality issues, which cannot be\nresolved by relaxation. The code, curated dataset and leaderboard are released\nat https://github.com/CataAI/PoseX.", "AI": {"tldr": "\u63d0\u51fa\u4e86PoseX\uff0c\u4e00\u4e2a\u5f00\u6e90\u81ea\u5bf9\u6807\u548c\u4ea4\u53c9\u5bf9\u6807\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u5b9e\u9645\u548c\u5168\u9762\u5730\u8bc4\u4f30\u5bf9\u63a5\u65b9\u6cd5\uff0c\u5305\u62ec\u65b0\u7684\u6570\u636e\u96c6\u300122\u79cd\u5bf9\u63a5\u65b9\u6cd5\u548c\u540e\u5904\u7406\u677e\u5f1b\u6280\u672f\u3002\u7814\u7a76\u53d1\u73b0AI\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u5df2\u8d85\u8d8a\u4f20\u7edf\u7269\u7406\u65b9\u6cd5\uff0c\u4e14\u540e\u5904\u7406\u80fd\u5927\u5e45\u63d0\u5347\u8868\u73b0\uff0c\u4f46AI\u5171\u6298\u53e0\u65b9\u6cd5\u5b58\u5728\u624b\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u86cb\u767d-\u914d\u4f53\u5bf9\u63a5\u57fa\u51c6\u5b58\u5728\u5b9e\u7528\u6027\u4e0d\u8db3\u6216\u6846\u67b6\u590d\u6742\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u9ad8\u6548\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff08718\u6761\u81ea\u5bf9\u6807\u30011312\u6761\u4ea4\u53c9\u5bf9\u6807\uff09\uff0c\u7eb3\u516522\u79cd\u65b9\u6cd5\uff08\u7269\u7406\u3001AI\u5bf9\u63a5\u3001AI\u5171\u6298\u53e0\uff09\uff0c\u8bbe\u8ba1\u540e\u5904\u7406\u677e\u5f1b\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u5b9e\u65f6\u6392\u884c\u699c\u3002", "result": "AI\u65b9\u6cd5\u5728RMSD\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u7269\u7406\u65b9\u6cd5\uff0c\u540e\u5904\u7406\u663e\u8457\u6539\u5584\u7acb\u4f53\u5316\u5b66\u95ee\u9898\uff0c\u4f46AI\u5171\u6298\u53e0\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u624b\u6027\u95ee\u9898\u3002", "conclusion": "PoseX\u4e3a\u5bf9\u63a5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0cAI\u7ed3\u5408\u540e\u5904\u7406\u8868\u73b0\u6700\u4f73\uff0c\u4f46AI\u5171\u6298\u53e0\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2505.02443", "pdf": "https://arxiv.org/pdf/2505.02443", "abs": "https://arxiv.org/abs/2505.02443", "authors": ["Simon Suh"], "title": "Investigating the Impact of Personalized AI Tutors on Language Learning Performance", "categories": ["cs.AI", "cs.HC", "I.2.6; K.3.1"], "comment": "16 pages, 4 figures, 1 table, Uses three theoretical frameworks like\n  Domain modeling, Gardner Theory of Multiple Intelligences, and Zone of\n  Proximal Development", "summary": "Driven by the global shift towards online learning prompted by the COVID 19\npandemic, Artificial Intelligence has emerged as a pivotal player in the field\nof education. Intelligent Tutoring Systems offer a new method of personalized\nteaching, replacing the limitations of traditional teaching methods. However,\nconcerns arise about the ability of AI tutors to address skill development and\nengagement during the learning process. In this paper, I will conduct a quasi\nexperiment with paired sample t test on 34 students pre and post use of AI\ntutors in language learning platforms like Santa and Duolingo to examine the\nrelationship between students engagement, academic performance, and students\nsatisfaction during a personalized language learning experience.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u75ab\u60c5\u671f\u95f4AI\u5bfc\u5e08\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u5bf9\u5b66\u751f\u53c2\u4e0e\u5ea6\u3001\u5b66\u672f\u8868\u73b0\u548c\u6ee1\u610f\u5ea6\u7684\u6548\u679c\uff0c\u901a\u8fc7\u914d\u5bf9\u6837\u672ct\u68c0\u9a8c\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "motivation": "\u53d7\u65b0\u51a0\u75ab\u60c5\u5f71\u54cd\uff0c\u5728\u7ebf\u5b66\u4e60\u9700\u6c42\u6fc0\u589e\uff0cAI\u5bfc\u5e08\u6210\u4e3a\u6559\u80b2\u9886\u57df\u7684\u91cd\u8981\u5de5\u5177\u3002\u7136\u800c\uff0c\u4eba\u4eec\u5bf9AI\u662f\u5426\u80fd\u6709\u6548\u63d0\u5347\u5b66\u4e60\u6280\u80fd\u548c\u53c2\u4e0e\u5ea6\u5b58\u7591\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5176\u5b9e\u9645\u6548\u679c\u3002", "method": "\u91c7\u7528\u51c6\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u914d\u5bf9\u6837\u672ct\u68c0\u9a8c\u5206\u679034\u540d\u5b66\u751f\u4f7f\u7528AI\u5bfc\u5e08\uff08\u5982Santa\u548cDuolingo\uff09\u524d\u540e\u7684\u8bed\u8a00\u5b66\u4e60\u6570\u636e\uff0c\u8bc4\u4f30\u53c2\u4e0e\u5ea6\u3001\u5b66\u672f\u8868\u73b0\u548c\u6ee1\u610f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5c06\u63ed\u793aAI\u5bfc\u5e08\u5728\u4e2a\u6027\u5316\u8bed\u8a00\u5b66\u4e60\u4e2d\u5bf9\u5b66\u751f\u53c2\u4e0e\u5ea6\u3001\u5b66\u672f\u8868\u73b0\u548c\u6ee1\u610f\u5ea6\u7684\u5177\u4f53\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u5c06\u4e3aAI\u5bfc\u5e08\u5728\u6559\u80b2\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u5b9e\u8bc1\u652f\u6301\uff0c\u5e76\u53ef\u80fd\u63a8\u52a8\u5176\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2505.02266", "pdf": "https://arxiv.org/pdf/2505.02266", "abs": "https://arxiv.org/abs/2505.02266", "authors": ["Henry Ndubuaku", "Mouad Talhi"], "title": "Parameter-Efficient Transformer Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07 (Primary) 68T50 (Secondary)"], "comment": "7 pages, 2 tables. Code available at https://github.com/HMUNACHI/pete", "summary": "Embedding layers in transformer-based NLP models typically account for the\nlargest share of model parameters, scaling with vocabulary size but not\nyielding performance gains proportional to scale. We propose an alternative\napproach in which token embedding vectors are first generated\ndeterministically, directly from the token IDs using a Fourier expansion of\ntheir normalized values, followed by a lightweight multilayer perceptron (MLP)\nthat captures higher-order interactions. We train standard transformers and our\narchitecture on natural language inference tasks (SNLI and MNLI), and evaluate\nzero-shot performance on sentence textual similarity (STS-B). Our results\ndemonstrate that the proposed method achieves competitive performance using\nsignificantly fewer parameters, trains faster, and operates effectively without\nthe need for dropout. This proof-of-concept study highlights the potential for\nscalable, memory-efficient language models and motivates further large-scale\nexperimentation based on our findings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u66ff\u4ee3\u4f20\u7edf\u8bcd\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u5c55\u5f00\u548c\u8f7b\u91cfMLP\u751f\u6210\u8bcd\u5411\u91cf\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u9700dropout\u3002", "motivation": "\u4f20\u7edf\u8bcd\u5d4c\u5165\u53c2\u6570\u91cf\u5927\u4e14\u6027\u80fd\u63d0\u5347\u4e0d\u663e\u8457\uff0c\u5e0c\u671b\u901a\u8fc7\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u51cf\u5c11\u53c2\u6570\u5e76\u4fdd\u6301\u6548\u7387\u3002", "method": "\u4f7f\u7528\u5085\u91cc\u53f6\u5c55\u5f00\u5f52\u4e00\u5316\u540e\u7684token ID\u751f\u6210\u5411\u91cf\uff0c\u518d\u901a\u8fc7\u8f7b\u91cfMLP\u6355\u6349\u9ad8\u9636\u4ea4\u4e92\u3002", "result": "\u5728NLI\u4efb\u52a1\u4e0a\u6027\u80fd\u63a5\u8fd1\u4f20\u7edf\u65b9\u6cd5\uff0c\u53c2\u6570\u91cf\u66f4\u5c11\u3001\u8bad\u7ec3\u66f4\u5feb\u4e14\u65e0\u9700dropout\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u53c2\u6570\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u5927\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2505.01736", "pdf": "https://arxiv.org/pdf/2505.01736", "abs": "https://arxiv.org/abs/2505.01736", "authors": ["Han Wan", "Rui Zhang", "Qi Wang", "Yang Liu", "Hao Sun"], "title": "PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurately modeling and forecasting complex systems governed by partial\ndifferential equations (PDEs) is crucial in various scientific and engineering\ndomains. However, traditional numerical methods struggle in real-world\nscenarios due to incomplete or unknown physical laws. Meanwhile, machine\nlearning approaches often fail to generalize effectively when faced with scarce\nobservational data and the challenge of capturing local and global features. To\nthis end, we propose the Physics-encoded Spectral Attention Network (PeSANet),\nwhich integrates local and global information to forecast complex systems with\nlimited data and incomplete physical priors. The model consists of two key\ncomponents: a physics-encoded block that uses hard constraints to approximate\nlocal differential operators from limited data, and a spectral-enhanced block\nthat captures long-range global dependencies in the frequency domain.\nSpecifically, we introduce a novel spectral attention mechanism to model\ninter-spectrum relationships and learn long-range spatial features.\nExperimental results demonstrate that PeSANet outperforms existing methods\nacross all metrics, particularly in long-term forecasting accuracy, providing a\npromising solution for simulating complex systems with limited data and\nincomplete physics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86PeSANet\u6a21\u578b\uff0c\u7ed3\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\uff0c\u5229\u7528\u6709\u9650\u6570\u636e\u548c\u4e0d\u5b8c\u6574\u7269\u7406\u5148\u9a8c\u9884\u6d4b\u590d\u6742\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u56e0\u7269\u7406\u5b9a\u5f8b\u4e0d\u5b8c\u6574\u6216\u672a\u77e5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u7a00\u7f3a\u6570\u636e\u548c\u5c40\u90e8\u5168\u5c40\u7279\u5f81\u6355\u6349\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "PeSANet\u5305\u542b\u7269\u7406\u7f16\u7801\u5757\uff08\u901a\u8fc7\u786c\u7ea6\u675f\u8fd1\u4f3c\u5c40\u90e8\u5fae\u5206\u7b97\u5b50\uff09\u548c\u9891\u8c31\u589e\u5f3a\u5757\uff08\u9891\u57df\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\uff09\uff0c\u5e76\u5f15\u5165\u8c31\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u8c31\u95f4\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePeSANet\u5728\u6240\u6709\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PeSANet\u4e3a\u6709\u9650\u6570\u636e\u548c\u4e0d\u5b8c\u6574\u7269\u7406\u4e0b\u7684\u590d\u6742\u7cfb\u7edf\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02462", "pdf": "https://arxiv.org/pdf/2505.02462", "abs": "https://arxiv.org/abs/2505.02462", "authors": ["Enpei Zhang", "Jingyi Chai", "Rui Ye", "Yanfeng Wang", "Siheng Chen"], "title": "Incentivizing Inclusive Contributions in Model Sharing Markets", "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "While data plays a crucial role in training contemporary AI models, it is\nacknowledged that valuable public data will be exhausted in a few years,\ndirecting the world's attention towards the massive decentralized private data.\nHowever, the privacy-sensitive nature of raw data and lack of incentive\nmechanism prevent these valuable data from being fully exploited. Addressing\nthese challenges, this paper proposes inclusive and incentivized personalized\nfederated learning (iPFL), which incentivizes data holders with diverse\npurposes to collaboratively train personalized models without revealing raw\ndata. iPFL constructs a model-sharing market by solving a graph-based training\noptimization and incorporates an incentive mechanism based on game theory\nprinciples. Theoretical analysis shows that iPFL adheres to two key incentive\nproperties: individual rationality and truthfulness. Empirical studies on\neleven AI tasks (e.g., large language models' instruction-following tasks)\ndemonstrate that iPFL consistently achieves the highest economic utility, and\nbetter or comparable model performance compared to baseline methods. We\nanticipate that our iPFL can serve as a valuable technique for boosting future\nAI models on decentralized private data while making everyone satisfied.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiPFL\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u4f18\u5316\u548c\u535a\u5f08\u8bba\u6fc0\u52b1\u6570\u636e\u6301\u6709\u8005\u534f\u4f5c\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002\u7406\u8bba\u8bc1\u660e\u5176\u6ee1\u8db3\u6fc0\u52b1\u517c\u5bb9\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u7ecf\u6d4e\u6548\u7528\u548c\u6a21\u578b\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u516c\u5f00\u6570\u636e\u5373\u5c06\u8017\u5c3d\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u5206\u6563\u7684\u79c1\u6709\u6570\u636e\u6210\u4e3a\u5173\u952e\u3002\u4f46\u9690\u79c1\u654f\u611f\u6027\u548c\u7f3a\u4e4f\u6fc0\u52b1\u673a\u5236\u963b\u788d\u4e86\u8fd9\u4e9b\u6570\u636e\u7684\u5f00\u53d1\u5229\u7528\u3002", "method": "iPFL\u7ed3\u5408\u56fe\u4f18\u5316\u7684\u6a21\u578b\u5171\u4eab\u5e02\u573a\u548c\u535a\u5f08\u8bba\u6fc0\u52b1\u673a\u5236\uff0c\u652f\u6301\u6570\u636e\u6301\u6709\u8005\u534f\u4f5c\u8bad\u7ec3\u4e2a\u6027\u5316\u6a21\u578b\u4e14\u4e0d\u6cc4\u9732\u539f\u59cb\u6570\u636e\u3002", "result": "\u7406\u8bba\u5206\u6790\u9a8c\u8bc1iPFL\u6ee1\u8db3\u4e2a\u4f53\u7406\u6027\u548c\u771f\u5b9e\u6027\uff1b\u572811\u9879AI\u4efb\u52a1\u4e2d\uff0c\u5176\u7ecf\u6d4e\u6548\u7528\u6700\u9ad8\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "iPFL\u4e3a\u672a\u6765\u5229\u7528\u5206\u6563\u79c1\u6709\u6570\u636e\u8bad\u7ec3AI\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6fc0\u52b1\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02273", "pdf": "https://arxiv.org/pdf/2505.02273", "abs": "https://arxiv.org/abs/2505.02273", "authors": ["Rimon Melamed", "Lucas H. McCabe", "H. Howie Huang"], "title": "Demystifying optimized prompts in language models", "categories": ["cs.CL"], "comment": null, "summary": "Modern language models (LMs) are not robust to out-of-distribution inputs.\nMachine generated (``optimized'') prompts can be used to modulate LM outputs\nand induce specific behaviors while appearing completely uninterpretable. In\nthis work, we investigate the composition of optimized prompts, as well as the\nmechanisms by which LMs parse and build predictions from optimized prompts. We\nfind that optimized prompts primarily consist of punctuation and noun tokens\nwhich are more rare in the training data. Internally, optimized prompts are\nclearly distinguishable from natural language counterparts based on sparse\nsubsets of the model's activations. Across various families of\ninstruction-tuned models, optimized prompts follow a similar path in how their\nrepresentations form through the network.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4f18\u5316\u63d0\u793a\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u53ca\u5176\u5185\u90e8\u5904\u7406\u673a\u5236\uff0c\u53d1\u73b0\u8fd9\u4e9b\u63d0\u793a\u4e3b\u8981\u7531\u7f55\u89c1\u6807\u70b9\u548c\u540d\u8bcd\u7ec4\u6210\uff0c\u5e76\u5728\u6a21\u578b\u6fc0\u6d3b\u4e2d\u8868\u73b0\u51fa\u72ec\u7279\u6a21\u5f0f\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u5bf9\u5206\u5e03\u5916\u8f93\u5165\u4e0d\u7a33\u5065\uff0c\u4f18\u5316\u63d0\u793a\u53ef\u8bf1\u5bfc\u7279\u5b9a\u884c\u4e3a\u4f46\u96be\u4ee5\u89e3\u91ca\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u4f18\u5316\u63d0\u793a\u7684\u7ec4\u6210\u53ca\u5176\u5728\u6a21\u578b\u5185\u90e8\u7684\u89e3\u6790\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4f18\u5316\u63d0\u793a\u7684\u7ec4\u6210\uff08\u5982\u6807\u70b9\u548c\u7f55\u89c1\u540d\u8bcd\uff09\u53ca\u6a21\u578b\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u63a2\u7a76\u4e0d\u540c\u6307\u4ee4\u8c03\u6574\u6a21\u578b\u5bb6\u65cf\u4e2d\u63d0\u793a\u7684\u8868\u5f81\u8def\u5f84\u3002", "result": "\u4f18\u5316\u63d0\u793a\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u66f4\u7f55\u89c1\uff0c\u5176\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u81ea\u7136\u8bed\u8a00\u660e\u663e\u4e0d\u540c\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u8868\u5f81\u5f62\u6210\u8def\u5f84\u76f8\u4f3c\u3002", "conclusion": "\u4f18\u5316\u63d0\u793a\u901a\u8fc7\u72ec\u7279\u7684\u7ec4\u6210\u548c\u6fc0\u6d3b\u6a21\u5f0f\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002"}}
{"id": "2505.01744", "pdf": "https://arxiv.org/pdf/2505.01744", "abs": "https://arxiv.org/abs/2505.01744", "authors": ["Yezhen Wang", "Zhouhao Yang", "Brian K Chen", "Fanyi Pu", "Bo Li", "Tianyu Gao", "Kenji Kawaguchi"], "title": "Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients", "categories": ["cs.LG"], "comment": null, "summary": "Building upon the success of low-rank adapter (LoRA), low-rank gradient\nprojection (LoRP) has emerged as a promising solution for memory-efficient\nfine-tuning. However, existing LoRP methods typically treat each row of the\ngradient matrix as the default projection unit, leaving the role of projection\ngranularity underexplored. In this work, we propose a novel framework, VLoRP,\nthat extends low-rank gradient projection by introducing an additional degree\nof freedom for controlling the trade-off between memory efficiency and\nperformance, beyond the rank hyper-parameter. Through this framework, we\nsystematically explore the impact of projection granularity, demonstrating that\nfiner-grained projections lead to enhanced stability and efficiency even under\na fixed memory budget. Regarding the optimization for VLoRP, we present\nProjFactor, an adaptive memory-efficient optimizer, that significantly reduces\nmemory requirement while ensuring competitive performance, even in the presence\nof gradient accumulation. Additionally, we provide a theoretical analysis of\nVLoRP, demonstrating the descent and convergence of its optimization trajectory\nunder both SGD and ProjFactor. Extensive experiments are conducted to validate\nour findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K.", "AI": {"tldr": "VLoRP \u662f\u4e00\u4e2a\u65b0\u578b\u4f4e\u79e9\u68af\u5ea6\u6295\u5f71\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6295\u5f71\u7c92\u5ea6\u63a7\u5236\u673a\u5236\uff0c\u63d0\u5347\u4e86\u5185\u5b58\u6548\u7387\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002\u4f18\u5316\u5668 ProjFactor \u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709 LoRP \u65b9\u6cd5\u9ed8\u8ba4\u4ee5\u68af\u5ea6\u77e9\u9635\u7684\u884c\u4f5c\u4e3a\u6295\u5f71\u5355\u4f4d\uff0c\u5ffd\u89c6\u4e86\u6295\u5f71\u7c92\u5ea6\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6295\u5f71\u7c92\u5ea6\u7684\u4f5c\u7528\uff0c\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u6027\u80fd\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86 VLoRP \u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u6295\u5f71\u7c92\u5ea6\u8c03\u6574\u6027\u80fd\u4e0e\u5185\u5b58\u6548\u7387\u7684\u5e73\u8861\u3002\u5f00\u53d1\u4e86 ProjFactor \u4f18\u5316\u5668\uff0c\u5728\u68af\u5ea6\u7d2f\u79ef\u65f6\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ec6\u7c92\u5ea6\u6295\u5f71\u5728\u56fa\u5b9a\u5185\u5b58\u9884\u7b97\u4e0b\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002ProjFactor \u5728\u591a\u79cd\u4efb\u52a1\uff08\u5e38\u8bc6\u63a8\u7406\u3001MMLU\u3001GSM8K\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VLoRP \u548c ProjFactor \u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u4f4e\u79e9\u68af\u5ea6\u6295\u5f71\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02484", "pdf": "https://arxiv.org/pdf/2505.02484", "abs": "https://arxiv.org/abs/2505.02484", "authors": ["Yunheng Zou", "Austin H. Cheng", "Abdulrahman Aldossary", "Jiaru Bai", "Shi Xuan Leong", "Jorge Arturo Campos-Gonzalez-Angulo", "Changhyeok Choi", "Cher Tian Ser", "Gary Tom", "Andrew Wang", "Zijian Zhang", "Ilya Yakavets", "Han Hao", "Chris Crebolder", "Varinia Bernales", "Al\u00e1n Aspuru-Guzik"], "title": "El Agente: An Autonomous Agent for Quantum Chemistry", "categories": ["cs.AI", "cs.LG", "cs.MA", "physics.chem-ph"], "comment": null, "summary": "Computational chemistry tools are widely used to study the behaviour of\nchemical phenomena. Yet, the complexity of these tools can make them\ninaccessible to non-specialists and challenging even for experts. In this work,\nwe introduce El Agente Q, an LLM-based multi-agent system that dynamically\ngenerates and executes quantum chemistry workflows from natural language user\nprompts. The system is built on a novel cognitive architecture featuring a\nhierarchical memory framework that enables flexible task decomposition,\nadaptive tool selection, post-analysis, and autonomous file handling and\nsubmission. El Agente Q is benchmarked on six university-level course exercises\nand two case studies, demonstrating robust problem-solving performance\n(averaging >87% task success) and adaptive error handling through in situ\ndebugging. It also supports longer-term, multi-step task execution for more\ncomplex workflows, while maintaining transparency through detailed action trace\nlogs. Together, these capabilities lay the foundation for increasingly\nautonomous and accessible quantum chemistry.", "AI": {"tldr": "El Agente Q \u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u4ee3\u7406\u7cfb\u7edf\u7684 LLM \u5de5\u5177\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7b80\u5316\u91cf\u5b50\u5316\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684\u751f\u6210\u4e0e\u6267\u884c\uff0c\u63d0\u5347\u975e\u4e13\u4e1a\u4eba\u58eb\u548c\u4e13\u5bb6\u7684\u4f7f\u7528\u4f53\u9a8c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u8ba1\u7b97\u5316\u5b66\u5de5\u5177\u7684\u590d\u6742\u6027\u53ca\u5176\u5bf9\u975e\u4e13\u4e1a\u4eba\u58eb\u548c\u6280\u672f\u4e13\u5bb6\u7684\u6311\u6218\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6613\u8bbf\u95ee\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u578b\u8ba4\u77e5\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u6846\u67b6\u5b9e\u73b0\u7075\u6d3b\u4efb\u52a1\u5206\u89e3\u3001\u81ea\u9002\u5e94\u5de5\u5177\u9009\u62e9\u3001\u540e\u5206\u6790\u53ca\u81ea\u4e3b\u6587\u4ef6\u5904\u7406\u4e0e\u63d0\u4ea4\u3002", "result": "\u5728\u516d\u4e2a\u5927\u5b66\u8bfe\u7a0b\u7ec3\u4e60\u548c\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cEl Agente Q \u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u8d85\u8fc7 87%\uff0c\u540c\u65f6\u652f\u6301\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u5e76\u4fdd\u6301\u900f\u660e\u6027\u3002", "conclusion": "El Agente Q \u7684\u6027\u80fd\u4e3a\u91cf\u5b50\u5316\u5b66\u9886\u57df\u7684\u81ea\u4e3b\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.02304", "pdf": "https://arxiv.org/pdf/2505.02304", "abs": "https://arxiv.org/abs/2505.02304", "authors": ["Siyu Liang", "Yunan Li", "Wentian Xin", "Huizhou Chen", "Xujie Liu", "Kang Liu", "Qiguang Miao"], "title": "Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition", "categories": ["cs.CL", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u624b\u8bed\u8bc6\u522b\u65b9\u6cd5GSP-MC\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u591a\u6b65\u63d0\u793a\u5de5\u7a0b\uff0c\u7ed3\u5408\u4e13\u5bb6\u9a8c\u8bc1\u7684\u624b\u8bed\u8bed\u6599\u5e93\uff0c\u5b9e\u73b0\u4e86\u5bf9\u624b\u8bed\u7684\u591a\u5c42\u6b21\u7cbe\u786e\u63cf\u8ff0\u3002\u5b9e\u9a8c\u5728\u4e2d\u6587\u548c\u571f\u8033\u5176\u624b\u8bed\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8697%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u624b\u8bed\u8bc6\u522b\u9762\u4e34\u6807\u6ce8\u590d\u6742\u6027\u9ad8\u7684\u6311\u6218\uff0c\u672c\u6587\u9996\u6b21\u5c06\u751f\u6210\u5f0fLLM\u5f15\u5165\u8be5\u9886\u57df\uff0c\u65e8\u5728\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u5e76\u63a8\u52a8\u5305\u5bb9\u6027\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faGSP-MC\u65b9\u6cd5\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u591a\u6b65\u63d0\u793a\u5de5\u7a0b\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u6982\u7387\u5339\u914d\u5bf9\u9f50\u6587\u672c\u63cf\u8ff0\u4e0e\u5c42\u6b21\u5316\u9aa8\u67b6\u7279\u5f81\u3002", "result": "\u5728\u4e2d\u6587SLR500\u548c\u571f\u8033\u5176AUTSL\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523097.1%\u548c97.07%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GSP-MC\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u624b\u8bed\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027\u901a\u4fe1\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01783", "pdf": "https://arxiv.org/pdf/2505.01783", "abs": "https://arxiv.org/abs/2505.01783", "authors": ["Amirmohammad Farzaneh", "Osvaldo Simeone"], "title": "Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "Online anomaly detection is essential in fields such as cybersecurity,\nhealthcare, and industrial monitoring, where promptly identifying deviations\nfrom expected behavior can avert critical failures or security breaches. While\nnumerous anomaly scoring methods based on supervised or unsupervised learning\nhave been proposed, current approaches typically rely on a continuous stream of\nreal-world calibration data to provide assumption-free guarantees on the false\ndiscovery rate (FDR). To address the inherent challenges posed by limited real\ncalibration data, we introduce context-aware prediction-powered conformal\nonline anomaly detection (C-PP-COAD). Our framework strategically leverages\nsynthetic calibration data to mitigate data scarcity, while adaptively\nintegrating real data based on contextual cues. C-PP-COAD utilizes conformal\np-values, active p-value statistics, and online FDR control mechanisms to\nmaintain rigorous and reliable anomaly detection performance over time.\nExperiments conducted on both synthetic and real-world datasets demonstrate\nthat C-PP-COAD significantly reduces dependency on real calibration data\nwithout compromising guaranteed FDR control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aC-PP-COAD\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5408\u6210\u6821\u51c6\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u6765\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684FDR\u63a7\u5236\u3002", "motivation": "\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u5728\u7f51\u7edc\u5b89\u5168\u3001\u533b\u7597\u5065\u5eb7\u548c\u5de5\u4e1a\u76d1\u63a7\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u771f\u5b9e\u6821\u51c6\u6570\u636e\u3002C-PP-COAD\u65e8\u5728\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "C-PP-COAD\u5229\u7528\u5408\u6210\u6821\u51c6\u6570\u636e\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u52a8\u6001\u6574\u5408\u771f\u5b9e\u6570\u636e\uff0c\u91c7\u7528\u7b26\u5408\u6027p\u503c\u3001\u6d3b\u8dc3p\u503c\u7edf\u8ba1\u548c\u5728\u7ebfFDR\u63a7\u5236\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cC-PP-COAD\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u771f\u5b9e\u6821\u51c6\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86FDR\u63a7\u5236\u7684\u53ef\u9760\u6027\u3002", "conclusion": "C-PP-COAD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u3002"}}
{"id": "2505.02489", "pdf": "https://arxiv.org/pdf/2505.02489", "abs": "https://arxiv.org/abs/2505.02489", "authors": ["Muskaan Goyal", "Pranav Bhasin"], "title": "Beyond the model: Key differentiators in large language models and multi-agent services", "categories": ["cs.AI", "cs.ET", "cs.MA", "cs.SE"], "comment": "4 pages", "summary": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it\nhas become evident that large language models (LLMs) are no longer the sole\ndefining factor in generative AI. As many now operate at comparable levels of\ncapability, the real race is not about having the biggest model but optimizing\nthe surrounding ecosystem, including data quality and management, computational\nefficiency, latency, and evaluation frameworks. This review article delves into\nthese critical differentiators that ensure modern AI services are efficient and\nprofitable.", "AI": {"tldr": "\u6458\u8981\u6307\u51fa\uff0c\u968f\u7740\u57fa\u7840\u6a21\u578b\uff08\u5982DeepSeek\u3001Manus AI\u548cLlama 4\uff09\u7684\u51fa\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0d\u518d\u662f\u751f\u6210AI\u7684\u552f\u4e00\u51b3\u5b9a\u6027\u56e0\u7d20\u3002\u5f53\u524d\u7684\u7ade\u4e89\u91cd\u70b9\u8f6c\u5411\u4f18\u5316\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u62ec\u6570\u636e\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u3001\u5ef6\u8fdf\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u800c\u975e\u4ec5\u4ec5\u8ffd\u6c42\u6700\u5927\u6a21\u578b\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u63a2\u8ba8\u5728\u751f\u6210AI\u9886\u57df\u4e2d\uff0c\u9664\u4e86\u6a21\u578b\u89c4\u6a21\u4e4b\u5916\u7684\u5176\u4ed6\u5173\u952e\u56e0\u7d20\uff08\u5982\u6570\u636e\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u7b49\uff09\u5982\u4f55\u6210\u4e3a\u73b0\u4ee3AI\u670d\u52a1\u6548\u7387\u548c\u76c8\u5229\u7684\u51b3\u5b9a\u6027\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u6027\u5206\u6790\uff0c\u6587\u7ae0\u68b3\u7406\u4e86\u5f53\u524d\u751f\u6210AI\u9886\u57df\u7684\u751f\u6001\u7cfb\u7edf\u4f18\u5316\u65b9\u5411\uff0c\u5305\u62ec\u6570\u636e\u7ba1\u7406\u3001\u6027\u80fd\u4f18\u5316\u53ca\u8bc4\u4f30\u65b9\u6cd5\u7b49\u3002", "result": "\u6307\u51fa\u6a21\u578b\u89c4\u6a21\u5df2\u4e0d\u518d\u662f\u552f\u4e00\u7ade\u4e89\u70b9\uff0c\u4f18\u5316\u751f\u6001\u7cfb\u7edf\uff08\u5982\u6570\u636e\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u7b49\uff09\u5bf9AI\u670d\u52a1\u7684\u6210\u529f\u66f4\u4e3a\u5173\u952e\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u603b\u7ed3\u4e86\u751f\u6001\u7cfb\u7edf\u4f18\u5316\u5728\u751f\u6210AI\u9886\u57df\u7684\u91cd\u8981\u6027\uff0c\u5f3a\u8c03\u672a\u6765\u7684\u7814\u7a76\u5e94\u96c6\u4e2d\u4e8e\u8fd9\u4e9b\u5173\u952e\u56e0\u7d20\uff0c\u4ee5\u786e\u4fddAI\u670d\u52a1\u7684\u9ad8\u6548\u6027\u548c\u76c8\u5229\u80fd\u529b\u3002"}}
{"id": "2505.02311", "pdf": "https://arxiv.org/pdf/2505.02311", "abs": "https://arxiv.org/abs/2505.02311", "authors": ["Jihao Zhao", "Chunlai Zhou", "Biao Qin"], "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs.", "AI": {"tldr": "\u63d0\u51faAttenHScore\u6307\u6807\uff0c\u52a8\u6001\u68c0\u6d4b\u5c0f\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5e7b\u89c9\uff0c\u5b9e\u65f6\u8c03\u7528\u5927\u6a21\u578b\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u751f\u6210\u5e7b\u89c9\u65f6\u8c03\u7528\u7684\u65f6\u673a\u95ee\u9898\uff0c\u907f\u514d\u540e\u5904\u7406\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4f4e\u6548\u3002", "method": "\u63d0\u51faAttenHScore\u6307\u6807\uff0c\u52a8\u6001\u8c03\u6574\u68c0\u6d4b\u9608\u503c\uff1b\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u77e5\u8bc6\u91cd\u7ec4\u8f85\u52a9\u5c0f\u6a21\u578b\u3002", "result": "AttenHScore\u5728\u591a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u5c24\u5176\u5728\u590d\u6742\u67e5\u8be2\u4e2d\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u4e14\u9002\u914d\u591a\u79cdLMs\u3002", "conclusion": "AttenHScore\u6709\u6548\u63d0\u5347\u4e86\u5b9e\u65f6\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\uff0c\u517c\u5177\u7075\u6d3b\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.01788", "pdf": "https://arxiv.org/pdf/2505.01788", "abs": "https://arxiv.org/abs/2505.01788", "authors": ["Md. Tanzib Hosain", "Asif Zaman", "Md. Shahriar Sajid", "Shadman Sakeeb Khan", "Shanjida Akter"], "title": "Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "Accepted in Proceedings of the 4th International Conference on Data\n  Analytics for Business and Industry, 2023", "summary": "The widespread adoption of Artificial Intelligence (AI) has been driven by\nsignificant advances in intelligent system research. However, this progress has\nraised concerns about data privacy, leading to a growing awareness of the need\nfor privacy-preserving AI. In response, there has been a seismic shift in\ninterest towards the leading paradigm for training Machine Learning (ML) models\non decentralized data silos while maintaining data privacy, Federated Learning\n(FL). This research paper presents a comprehensive performance analysis of a\ncutting-edge approach to personalize ML model while preserving privacy achieved\nthrough Privacy Preserving Machine Learning with the innovative framework of\nFederated Personalized Learning (PPMLFPL). Regarding the increasing concerns\nabout data privacy, this study evaluates the effectiveness of PPMLFPL\naddressing the critical balance between personalized model refinement and\nmaintaining the confidentiality of individual user data. According to our\nanalysis, Adaptive Personalized Cross-Silo Federated Learning with Differential\nPrivacy (APPLE+DP) offering efficient execution whereas overall, the use of the\nAdaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption\n(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated\npersonalized learning settings is strongly suggested. The results offer\nvaluable insights creating it a promising scope for future advancements in the\nfield of privacy-conscious data-driven technologies.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7684\u4e2a\u6027\u5316\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u65b9\u6cd5PPMLFPL\uff0c\u8bc4\u4f30\u4e86APPLE+DP\u548cAPPLE+HE\u7b97\u6cd5\u5728\u5e73\u8861\u6a21\u578b\u4e2a\u6027\u5316\u4e0e\u6570\u636e\u9690\u79c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u63a8\u8350APPLE+HE\u4f5c\u4e3a\u4f18\u5148\u65b9\u6848\u3002", "motivation": "\u968f\u7740AI\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u6570\u636e\u9690\u79c1\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u89e3\u51b3\u5206\u6563\u6570\u636e\u8bad\u7ec3\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u5173\u952e\u8303\u5f0f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u4e2a\u6027\u5316ML\u4e2d\u6709\u6548\u4fdd\u62a4\u9690\u79c1\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86PPMLFPL\u6846\u67b6\uff0c\u5177\u4f53\u5305\u62ec\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u8de8\u5b64\u5c9b\u8054\u90a6\u5b66\u4e60\u7ed3\u5408\u5dee\u5206\u9690\u79c1\uff08APPLE+DP\uff09\u548c\u540c\u6001\u52a0\u5bc6\uff08APPLE+HE\uff09\u4e24\u79cd\u7b97\u6cd5\u3002", "result": "APPLE+DP\u6267\u884c\u6548\u7387\u9ad8\uff0c\u800cAPPLE+HE\u5728\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u88ab\u63a8\u8350\u4e3a\u9996\u9009\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9690\u79c1\u654f\u611f\u7684\u6570\u636e\u9a71\u52a8\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u9690\u79c1\u4fdd\u62a4\u4e2a\u6027\u5316ML\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.02516", "pdf": "https://arxiv.org/pdf/2505.02516", "abs": "https://arxiv.org/abs/2505.02516", "authors": ["MohammadAli Shaeri", "Jinhan Liu", "Mahsa Shoaran"], "title": "Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics", "categories": ["cs.AI", "cs.AR", "cs.LG", "eess.SP", "q-bio.NC", "I.2.0; B.7.0; I.5.1; C.3"], "comment": "To appear in the 2025 IEEE International NEWCAS Conference\n  (NEWCAS'25)", "summary": "Advanced neural interfaces are transforming applications ranging from\nneuroscience research to diagnostic tools (for mental state recognition, tremor\nand seizure detection) as well as prosthetic devices (for motor and\ncommunication recovery). By integrating complex functions into miniaturized\nneural devices, these systems unlock significant opportunities for personalized\nassistive technologies and adaptive therapeutic interventions. Leveraging\nhigh-density neural recordings, on-site signal processing, and machine learning\n(ML), these interfaces extract critical features, identify disease\nneuro-markers, and enable accurate, low-latency neural decoding. This\nintegration facilitates real-time interpretation of neural signals, adaptive\nmodulation of brain activity, and efficient control of assistive devices.\nMoreover, the synergy between neural interfaces and ML has paved the way for\nself-sufficient, ubiquitous platforms capable of operating in diverse\nenvironments with minimal hardware costs and external dependencies. In this\nwork, we review recent advancements in AI-driven decoding algorithms and\nenergy-efficient System-on-Chip (SoC) platforms for next-generation\nminiaturized neural devices. These innovations highlight the potential for\ndeveloping intelligent neural interfaces, addressing critical challenges in\nscalability, reliability, interpretability, and user adaptability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u9a71\u52a8\u7684\u89e3\u7801\u7b97\u6cd5\u548c\u9ad8\u6548\u80fdSoC\u5e73\u53f0\u5982\u4f55\u63a8\u52a8\u4e0b\u4e00\u4ee3\u5fae\u578b\u795e\u7ecf\u8bbe\u5907\u7684\u521b\u65b0\uff0c\u5f3a\u8c03\u5176\u5728\u4e2a\u6027\u5316\u8f85\u52a9\u6280\u672f\u548c\u9002\u5e94\u6027\u6cbb\u7597\u5e72\u9884\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u63a8\u52a8\u795e\u7ecf\u63a5\u53e3\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ee5\u5e94\u5bf9\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3001\u8bca\u65ad\u5de5\u5177\u548c\u5047\u80a2\u8bbe\u5907\u7b49\u9886\u57df\u7684\u9700\u6c42\uff0c\u76ee\u6807\u662f\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u9ad8\u6548\u548c\u666e\u53ca\u7684\u795e\u7ecf\u63a5\u53e3\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u9ad8\u5bc6\u5ea6\u795e\u7ecf\u8bb0\u5f55\u3001\u73b0\u573a\u4fe1\u53f7\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5f00\u53d1AI\u9a71\u52a8\u7684\u89e3\u7801\u7b97\u6cd5\u548c\u80fd\u6548\u4f18\u5316\u7684SoC\u5e73\u53f0\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u795e\u7ecf\u4fe1\u53f7\u89e3\u6790\u3001\u8111\u6d3b\u52a8\u9002\u5e94\u6027\u8c03\u5236\u548c\u8f85\u52a9\u8bbe\u5907\u9ad8\u6548\u63a7\u5236\uff0c\u4e3a\u667a\u80fd\u795e\u7ecf\u63a5\u53e3\u7684\u89c4\u6a21\u5316\u3001\u53ef\u9760\u6027\u548c\u7528\u6237\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AI\u548cSoC\u5e73\u53f0\u7684\u7ed3\u5408\u4e3a\u4e0b\u4e00\u4ee3\u795e\u7ecf\u8bbe\u5907\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u652f\u6301\uff0c\u5c55\u73b0\u51fa\u5728\u4e2a\u6027\u5316\u533b\u7597\u548c\u8f85\u52a9\u6280\u672f\u4e2d\u7684\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2505.02363", "pdf": "https://arxiv.org/pdf/2505.02363", "abs": "https://arxiv.org/abs/2505.02363", "authors": ["Tianjian Li", "Daniel Khashabi"], "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning", "categories": ["cs.CL"], "comment": "To appear in ICML 2025", "summary": "Aligning language models with human preferences relies on pairwise preference\ndatasets. While some studies suggest that on-policy data consistently\noutperforms off -policy data for preference learning, others indicate that the\nadvantages of on-policy data may be task-dependent, highlighting the need for a\nsystematic exploration of their interplay.\n  In this work, we show that on-policy and off-policy data offer complementary\nstrengths in preference optimization: on-policy data is particularly effective\nfor reasoning tasks like math and coding, while off-policy data performs better\non open-ended tasks such as creative writing and making personal\nrecommendations. Guided by these findings, we introduce SIMPLEMIX, an approach\nto combine the complementary strengths of on-policy and off-policy preference\nlearning by simply mixing these two data sources. Our empirical results across\ndiverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves\nlanguage model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO\nand off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it\noutperforms prior approaches that are much more complex in combining on- and\noff-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u504f\u597d\u4f18\u5316\u4e2d\u7ed3\u5408on-policy\u548coff-policy\u6570\u636e\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u63d0\u51faSIMPLEMIX\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u6df7\u5408\u4e24\u79cd\u6570\u636e\u6e90\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6548\u679c\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u53473-6%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9on-policy\u548coff-policy\u6570\u636e\u5728\u504f\u597d\u5b66\u4e60\u4e2d\u7684\u6548\u679c\u5b58\u5728\u5206\u6b67\uff0c\u9700\u7cfb\u7edf\u6027\u63a2\u7a76\u4e8c\u8005\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u4efb\u52a1\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51faSIMPLEMIX\u65b9\u6cd5\uff0c\u76f4\u63a5\u6df7\u5408on-policy\uff08\u64c5\u957f\u63a8\u7406\u4efb\u52a1\uff09\u548coff-policy\uff08\u64c5\u957f\u5f00\u653e\u4efb\u52a1\uff09\u6570\u636e\uff0c\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0cSIMPLEMIX\u6bd4\u7eafon/off-policy DPO\u5e73\u5747\u63d0\u53476.03%\uff0c\u4f18\u4e8e\u590d\u6742\u6df7\u5408\u65b9\u6cd5\uff08\u5982HyPO\uff093.05%\u3002", "conclusion": "\u7b80\u5355\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u80fd\u6709\u6548\u7ed3\u5408\u4e24\u7c7b\u6570\u636e\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6027\u80fd\uff0c\u4e3a\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01810", "pdf": "https://arxiv.org/pdf/2505.01810", "abs": "https://arxiv.org/abs/2505.01810", "authors": ["Zhiyi Zhou", "Hexin Peng", "Hongyu Long"], "title": "Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees", "categories": ["cs.LG"], "comment": null, "summary": "With the advancement of Internet of Things (IoT) technologies, high-precision\nindoor positioning has become essential for Location-Based Services (LBS) in\ncomplex indoor environments. Fingerprint-based localization is popular, but\ntraditional algorithms and deep learning-based methods face challenges such as\npoor generalization, overfitting, and lack of interpretability. This paper\napplies conformal prediction (CP) to deep learning-based indoor positioning. CP\ntransforms the uncertainty of the model into a non-conformity score, constructs\nprediction sets to ensure correctness coverage, and provides statistical\nguarantees. We also introduce conformal risk control for path navigation tasks\nto manage the false discovery rate (FDR) and the false negative rate (FNR).The\nmodel achieved an accuracy of approximately 100% on the training dataset and\n85% on the testing dataset, effectively demonstrating its performance and\ngeneralization capability. Furthermore, we also develop a conformal p-value\nframework to control the proportion of position-error points. Experiments on\nthe UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19,\nMobileNetV2, ResNet50, and EfficientNet show that the conformal prediction\ntechnique can effectively approximate the target coverage, and different models\nhave different performance in terms of prediction set size and uncertainty\nquantification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\uff08CP\uff09\u6280\u672f\u6539\u8fdb\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9884\u6d4b\u96c6\u548c\u63a7\u5236\u98ce\u9669\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9ad8\u7cbe\u5ea6\u5ba4\u5185\u5b9a\u4f4d\u6210\u4e3a\u4f4d\u7f6e\u670d\u52a1\u7684\u6838\u5fc3\u9700\u6c42\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u6307\u7eb9\u5b9a\u4f4d\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u8fc7\u62df\u5408\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u5e94\u7528\u4fdd\u5f62\u9884\u6d4b\uff08CP\uff09\u6280\u672f\uff0c\u5c06\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u8f6c\u5316\u4e3a\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u6784\u5efa\u9884\u6d4b\u96c6\u4ee5\u4fdd\u8bc1\u6b63\u786e\u6027\u8986\u76d6\u7387\uff0c\u5e76\u5f15\u5165\u4fdd\u5f62\u98ce\u9669\u63a7\u5236\u4ee5\u7ba1\u7406\u8def\u5f84\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u53d1\u73b0\u7387\uff08FDR\uff09\u548c\u5047\u9634\u6027\u7387\uff08FNR\uff09\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08\u5982MobileNet\u3001VGG19\u7b49\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63a5\u8fd1100%\uff0c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u523085%\u3002\u4fdd\u5f62\u9884\u6d4b\u6280\u672f\u80fd\u591f\u6709\u6548\u903c\u8fd1\u76ee\u6807\u8986\u76d6\u7387\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u5728\u9884\u6d4b\u96c6\u5927\u5c0f\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u6280\u672f\u5728\u63d0\u9ad8\u5ba4\u5185\u5b9a\u4f4d\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4e3a\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u969c\u3002\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u9009\u62e9\u9700\u6839\u636e\u5177\u4f53\u9700\u6c42\u6743\u8861\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2505.02576", "pdf": "https://arxiv.org/pdf/2505.02576", "abs": "https://arxiv.org/abs/2505.02576", "authors": ["Sergio Hern\u00e1ndez-Guti\u00e9rrez", "Minttu Alakuijala", "Alexander V. Nikitin", "Pekka Marttinen"], "title": "Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning tasks are crucial in many domains, especially in science and\nengineering. Although large language models (LLMs) have made progress in\nreasoning tasks using techniques such as chain-of-thought and least-to-most\nprompting, these approaches still do not effectively scale to complex problems\nin either their performance or execution time. Moreover, they often require\nadditional supervision for each new task, such as in-context examples. In this\nwork, we introduce Recursive Decomposition with Dependencies (RDD), a scalable\ndivide-and-conquer method for solving reasoning problems that requires less\nsupervision than prior approaches. Our method can be directly applied to a new\nproblem class even in the absence of any task-specific guidance. Furthermore,\nRDD supports sub-task dependencies, allowing for ordered execution of\nsub-tasks, as well as an error recovery mechanism that can correct mistakes\nmade in previous steps. We evaluate our approach on two benchmarks with six\ndifficulty levels each and in two in-context settings: one with task-specific\nexamples and one without. Our results demonstrate that RDD outperforms other\nmethods in a compute-matched setting as task complexity increases, while also\nbeing more computationally efficient.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRDD\u7684\u9012\u5f52\u5206\u89e3\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\u548c\u51cf\u5c11\u76d1\u7763\u9700\u6c42\uff0c\u540c\u65f6\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u548c\u6267\u884c\u65f6\u95f4\u4ecd\u6709\u9650\u5236\uff0c\u4e14\u901a\u5e38\u9700\u8981\u9488\u5bf9\u65b0\u4efb\u52a1\u7684\u989d\u5916\u76d1\u7763\u3002RDD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "RDD\u91c7\u7528\u5206\u6cbb\u6cd5\u9012\u5f52\u5206\u89e3\u95ee\u9898\uff0c\u652f\u6301\u5b50\u4efb\u52a1\u4f9d\u8d56\u548c\u9519\u8bef\u6062\u590d\u673a\u5236\uff0c\u65e0\u9700\u9488\u5bf9\u65b0\u4efb\u52a1\u7684\u7279\u5b9a\u6307\u5bfc\u5373\u53ef\u5e94\u7528\u3002", "result": "RDD\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u63d0\u9ad8\u65f6\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "RDD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u76d1\u7763\u7684\u4f9d\u8d56\u3002"}}
{"id": "2505.02366", "pdf": "https://arxiv.org/pdf/2505.02366", "abs": "https://arxiv.org/abs/2505.02366", "authors": ["Tianyu Zong", "Hongzhu Yi", "Bingkang Shi", "Yuanxiang Wang", "Jungang Xu"], "title": "JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Unsupervised contrastive learning has become a hot research topic in natural\nlanguage processing. Existing works usually aim at constraining the orientation\ndistribution of the representations of positive and negative samples in the\nhigh-dimensional semantic space in contrastive learning, but the semantic\nrepresentation tensor possesses both modulus and orientation features, and the\nexisting works ignore the modulus feature of the representations and cause\ninsufficient contrastive learning. % Therefore, we firstly propose a training\nobjective that aims at modulus constraints on the semantic representation\ntensor, to strengthen the alignment between the positive samples in contrastive\nlearning. Therefore, we first propose a training objective that is designed to\nimpose modulus constraints on the semantic representation tensor, to strengthen\nthe alignment between positive samples in contrastive learning. Then, the\nBERT-like model suffers from the phenomenon of sinking attention, leading to a\nlack of attention to CLS tokens that aggregate semantic information. In\nresponse, we propose a cross-attention structure among the twin-tower ensemble\nmodels to enhance the model's attention to CLS token and optimize the quality\nof CLS Pooling. Combining the above two motivations, we propose a new\n\\textbf{J}oint \\textbf{T}ensor representation modulus constraint and\n\\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence\n\\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven\nsemantic text similarity computation tasks, and the experimental results show\nthat JTCSE's twin-tower ensemble model and single-tower distillation model\noutperform the other baselines and become the current SOTA. In addition, we\nhave conducted an extensive zero-shot downstream task evaluation, which shows\nthat JTCSE outperforms other baselines overall on more than 130 tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJTCSE\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6a21\u7ea6\u675f\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f18\u5316\u8bed\u4e49\u8868\u793a\u5f20\u91cf\u548cCLS\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u5728\u591a\u9879\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u9ad8\u7ef4\u8bed\u4e49\u7a7a\u95f4\u4e2d\u8868\u793a\u7684\u5b9a\u5411\u5206\u5e03\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u7279\u5f81\uff0c\u5bfc\u81f4\u5bf9\u6bd4\u5b66\u4e60\u4e0d\u5145\u5206\u3002\u540c\u65f6\uff0cBERT\u7c7b\u6a21\u578b\u5b58\u5728\u6ce8\u610f\u529b\u4e0b\u6c89\u73b0\u8c61\uff0c\u5f71\u54cdCLS\u4ee4\u724c\u7684\u8bed\u4e49\u805a\u5408\u80fd\u529b\u3002", "method": "\u63d0\u51faJTCSE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a1\uff09\u5bf9\u8bed\u4e49\u8868\u793a\u5f20\u91cf\u65bd\u52a0\u6a21\u7ea6\u675f\u4ee5\u52a0\u5f3a\u6b63\u6837\u672c\u5bf9\u9f50\uff1b2\uff09\u901a\u8fc7\u53cc\u5854\u96c6\u6210\u6a21\u578b\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u7ed3\u6784\u589e\u5f3aCLS\u4ee4\u724c\u7684\u6ce8\u610f\u529b\uff0c\u4f18\u5316CLS\u6c60\u5316\u8d28\u91cf\u3002", "result": "\u57287\u9879\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u4e2d\uff0cJTCSE\u7684\u53cc\u5854\u96c6\u6210\u6a21\u578b\u548c\u5355\u5854\u84b8\u998f\u6a21\u578b\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u5f53\u524dSOTA\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u5728130\u591a\u9879\u96f6\u6837\u672c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4e5f\u6574\u4f53\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u3002", "conclusion": "JTCSE\u901a\u8fc7\u8054\u5408\u6a21\u7ea6\u675f\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u6548\u679c\uff0c\u4e3a\u53e5\u5b50\u5d4c\u5165\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u6846\u67b6\u3002"}}
{"id": "2505.01819", "pdf": "https://arxiv.org/pdf/2505.01819", "abs": "https://arxiv.org/abs/2505.01819", "authors": ["Ze Tao"], "title": "An LSTM-PINN Hybrid Method to the specific problem of population forecasting", "categories": ["cs.LG"], "comment": "9 pages,6 figures", "summary": "Deep learning has emerged as a powerful tool in scientific modeling,\nparticularly for complex dynamical systems; however, accurately capturing\nage-structured population dynamics under policy-driven fertility changes\nremains a significant challenge due to the lack of effective integration\nbetween domain knowledge and long-term temporal dependencies. To address this\nissue, we propose two physics-informed deep learning frameworks--PINN and\nLSTM-PINN--that incorporate policy-aware fertility functions into a\ntransport-reaction partial differential equation to simulate population\nevolution from 2024 to 2054. The standard PINN model enforces the governing\nequation and boundary conditions via collocation-based training, enabling\naccurate learning of underlying population dynamics and ensuring stable\nconvergence. Building on this, the LSTM-PINN framework integrates sequential\nmemory mechanisms to effectively capture long-range dependencies in the\nage-time domain, achieving robust training performance across multiple loss\ncomponents. Simulation results under three distinct fertility policy\nscenarios-the Three-child policy, the Universal two-child policy, and the\nSeparate two-child policy--demonstrate the models' ability to reflect\npolicy-sensitive demographic shifts and highlight the effectiveness of\nintegrating domain knowledge into data-driven forecasting. This study provides\na novel and extensible framework for modeling age-structured population\ndynamics under policy interventions, offering valuable insights for\ndata-informed demographic forecasting and long-term policy planning in the face\nof emerging population challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff08PINN\u548cLSTM-PINN\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u653f\u7b56\u9a71\u52a8\u4e0b\u7684\u5e74\u9f84\u7ed3\u6784\u4eba\u53e3\u52a8\u6001\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u63d0\u4f9b\u653f\u7b56\u654f\u611f\u7684\u4eba\u53e3\u9884\u6d4b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\u5efa\u6a21\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u9886\u57df\u77e5\u8bc6\u7684\u6709\u6548\u6574\u5408\uff0c\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u653f\u7b56\u9a71\u52a8\u7684\u5e74\u9f84\u7ed3\u6784\u4eba\u53e3\u52a8\u6001\u3002", "method": "\u63d0\u51faPINN\u548cLSTM-PINN\u6846\u67b6\uff0c\u7ed3\u5408\u653f\u7b56\u611f\u77e5\u751f\u80b2\u51fd\u6570\u4e0e\u4f20\u8f93-\u53cd\u5e94\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u57fa\u4e8e\u914d\u7f6e\u7684\u8bad\u7ec3\u6355\u6349\u957f\u671f\u4f9d\u8d56\u6027\u3002", "result": "\u4e09\u79cd\u751f\u80b2\u653f\u7b56\u60c5\u666f\u4e0b\u7684\u6a21\u62df\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u653f\u7b56\u654f\u611f\u4eba\u53e3\u52a8\u6001\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u51f8\u663e\u4e86\u9886\u57df\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u7ed3\u5408\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u653f\u7b56\u5e72\u9884\u4e0b\u7684\u5e74\u9f84\u7ed3\u6784\u4eba\u53e3\u52a8\u6001\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u4e3a\u4eba\u53e3\u9884\u6d4b\u548c\u957f\u671f\u653f\u7b56\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.02581", "pdf": "https://arxiv.org/pdf/2505.02581", "abs": "https://arxiv.org/abs/2505.02581", "authors": ["Alberto Hern\u00e1ndez-Espinosa", "Felipe S. Abrah\u00e3o", "Olaf Witkowski", "Hector Zenil"], "title": "Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem", "categories": ["cs.AI"], "comment": "33 pages", "summary": "The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. This paper demonstrates that\nachieving complete alignment is inherently unattainable due to mathematical\nprinciples rooted in the foundations of predicate logic and computability, in\nparticular Turing's computational universality, G\\\"odel's incompleteness and\nChaitin's randomness. Instead, we argue that embracing AI misalignment or\nagent's `neurodivergence' as a contingent strategy, defined as fostering a\ndynamic ecosystem of competing, partially aligned agents, is a possible only\nviable path to mitigate risks. Through mathematical proofs and an experimental\ndesign, we explore how misalignment may serve and should be promoted as a\ncounterbalancing mechanism to team up with whichever agents are most aligned AI\nto human values, ensuring that no single system dominates destructively. The\nmain premise of our contribution is that misalignment is inevitable because\nfull AI-human alignment is a mathematical impossibility from Turing-complete\nsystems which we also prove in this paper, a feature then inherited to AGI and\nASI systems. We introduce and test `change-of-opinion' attacks based on this\nkind of perturbation and intervention analysis to study how agents may\nneutralise friendly or unfriendly AIs through cooperation, competition or\nmalice.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\uff0c\u7531\u4e8e\u56fe\u7075\u8ba1\u7b97\u7684\u901a\u7528\u6027\u3001\u54e5\u5fb7\u5c14\u4e0d\u5b8c\u5907\u6027\u548c\u67f4\u5ef7\u968f\u673a\u6027\u7b49\u6570\u5b66\u539f\u7406\uff0c\u5b8c\u5168\u7684AI\u5bf9\u9f50\u5728\u6570\u5b66\u4e0a\u662f\u4e0d\u53ef\u5b9e\u73b0\u7684\uff0c\u56e0\u6b64\u4e3b\u5f20\u901a\u8fc7\u63a5\u53d7AI\u7684\u4e0d\u5bf9\u9f50\u6027\uff08\u5373\u2018\u795e\u7ecf\u591a\u6837\u6027\u2019\uff09\u4f5c\u4e3a\u7f13\u89e3\u98ce\u9669\u7684\u7b56\u7565\u3002", "motivation": "\u968f\u7740AI\u4ece\u72ed\u4e49AI\u53d1\u5c55\u4e3a\u901a\u7528AI\u548c\u8d85\u7ea7\u667a\u80fd\uff0c\u5bf9\u63a7\u5236\u95ee\u9898\u548c\u751f\u5b58\u98ce\u9669\u7684\u62c5\u5fe7\u52a0\u5267\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u52a8\u6001\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u90e8\u5206\u5bf9\u9f50\u4ee3\u7406\u7ade\u4e89\u6765\u7f13\u89e3\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u7814\u7a76\u63a2\u8ba8\u4e86\u5bf9\u9f50\u7684\u4e0d\u53ef\u80fd\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u2018\u6539\u53d8\u89c2\u70b9\u2019\u653b\u51fb\u4f5c\u4e3a\u5e72\u9884\u5206\u6790\u624b\u6bb5\uff0c\u4ee5\u7814\u7a76\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u5408\u4f5c\u6216\u7ade\u4e89\u6765\u4e2d\u548c\u53cb\u597d\u6216\u654c\u5bf9\u7684AI\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5b8c\u5168\u7684\u5bf9\u9f50\u5728\u6570\u5b66\u4e0a\u662f\u4e0d\u53ef\u5b9e\u73b0\u7684\uff0c\u800c\u4e0d\u5bf9\u9f50\u6027\u53ef\u4ee5\u4f5c\u4e3a\u52a8\u6001\u5e73\u8861\u673a\u5236\uff0c\u9632\u6b62\u5355\u4e00\u7cfb\u7edf\u7684\u7834\u574f\u6027\u4e3b\u5bfc\u3002", "conclusion": "\u8bba\u6587\u8ba4\u4e3a\uff0c\u63a5\u53d7\u5e76\u7ba1\u7406AI\u7684\u4e0d\u5bf9\u9f50\u6027\u662f\u552f\u4e00\u53ef\u884c\u7684\u7b56\u7565\uff0c\u4ee5\u907f\u514d\u56e0\u6570\u5b66\u9650\u5236\u800c\u5bfc\u81f4\u7684AI\u5bf9\u9f50\u5931\u8d25\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2505.02387", "pdf": "https://arxiv.org/pdf/2505.02387", "abs": "https://arxiv.org/abs/2505.02387", "authors": ["Xiusi Chen", "Gaotang Li", "Ziqi Wang", "Bowen Jin", "Cheng Qian", "Yu Wang", "Hongru Wang", "Yu Zhang", "Denghui Zhang", "Tong Zhang", "Hanghang Tong", "Heng Ji"], "title": "RM-R1: Reward Modeling as Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 7 figures", "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86ReasRM\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5956\u52b1\u5efa\u6a21\u5b9a\u4e49\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u5347\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002RM-R1\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u84b8\u998f\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\u548c\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\uff09\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7\u66f4\u5927\u7684\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08RLHF\uff09\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709RM\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u6574\u5408\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u5c06\u63a8\u7406\u80fd\u529b\u878d\u5165\u5956\u52b1\u5efa\u6a21\u53ef\u663e\u8457\u63d0\u5347RM\u7684\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faReasRM\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u4f5c\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u84b8\u998f\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\uff1b2\uff09\u5229\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002RM-R1\u901a\u8fc7\u81ea\u751f\u6210\u63a8\u7406\u75d5\u8ff9\u6216\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u5019\u9009\u54cd\u5e94\u3002", "result": "ReasRM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u63a5\u8fd1SOTA\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\uff08\u5982Llama3.1-405B\uff09\u548c\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff0c\u6700\u9ad8\u63d0\u534713.8%\u3002", "conclusion": "ReasRM\u901a\u8fc7\u878d\u5165\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u65b9\u6cd5\u53ef\u590d\u73b0\u3002\u4f5c\u8005\u5f00\u6e90\u4e86\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.01822", "pdf": "https://arxiv.org/pdf/2505.01822", "abs": "https://arxiv.org/abs/2505.01822", "authors": ["Jifeng Hu", "Sili Huang", "Zhejian Yang", "Shengchao Hu", "Li Shen", "Hechang Chen", "Lichao Sun", "Yi Chang", "Dacheng Tao"], "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Conditional decision generation with diffusion models has shown powerful\ncompetitiveness in reinforcement learning (RL). Recent studies reveal the\nrelation between energy-function-guidance diffusion models and constrained RL\nproblems. The main challenge lies in estimating the intermediate energy, which\nis intractable due to the log-expectation formulation during the generation\nprocess. To address this issue, we propose the Analytic Energy-guided Policy\nOptimization (AEPO). Specifically, we first provide a theoretical analysis and\nthe closed-form solution of the intermediate guidance when the diffusion model\nobeys the conditional Gaussian transformation. Then, we analyze the posterior\nGaussian distribution in the log-expectation formulation and obtain the target\nestimation of the log-expectation under mild assumptions. Finally, we train an\nintermediate energy neural network to approach the target estimation of\nlog-expectation formulation. We apply our method in 30+ offline RL tasks to\ndemonstrate the effectiveness of our method. Extensive experiments illustrate\nthat our method surpasses numerous representative baselines in D4RL offline\nreinforcement learning benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Analytic Energy-guided Policy Optimization (AEPO)\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u4f30\u8ba1\u4e2d\u95f4\u80fd\u91cf\u7684\u96be\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u95ed\u5f0f\u89e3\u4f18\u5316\u7b56\u7565\uff0c\u5728\u591a\u4efb\u52a1\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e2d\u95f4\u80fd\u91cf\u7684\u4f30\u8ba1\u56e0\u5bf9\u6570\u671f\u671b\u516c\u5f0f\u7684\u590d\u6742\u6027\u800c\u96be\u4ee5\u5b9e\u73b0\uff0c\u6210\u4e3a\u5236\u7ea6\u6027\u80fd\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u9996\u5148\u5206\u6790\u4e86\u6761\u4ef6\u9ad8\u65af\u53d8\u6362\u4e0b\u6269\u6563\u6a21\u578b\u7684\u4e2d\u95f4\u5f15\u5bfc\u95ed\u5f0f\u89e3\uff1b\u5176\u6b21\u63a8\u5bfc\u4e86\u5bf9\u6570\u671f\u671b\u7684\u540e\u9a8c\u9ad8\u65af\u5206\u5e03\uff0c\u63d0\u51fa\u57fa\u4e8e\u6e29\u548c\u5047\u8bbe\u7684\u4f30\u8ba1\u76ee\u6807\uff1b\u6700\u540e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u903c\u8fd1\u8fd9\u4e00\u76ee\u6807\u3002", "result": "\u65b9\u6cd5\u572830+\u79bb\u7ebfRL\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u6548\u679c\u663e\u8457\uff0c\u8d85\u8d8a\u4e86D4RL\u57fa\u51c6\u4e2d\u7684\u591a\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u3002", "conclusion": "AEPO\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u80fd\u91cf\u4f30\u8ba1\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.02609", "pdf": "https://arxiv.org/pdf/2505.02609", "abs": "https://arxiv.org/abs/2505.02609", "authors": ["Shuyu Wang", "Ang\u00e9lique Saillet", "Philom\u00e8ne Le Gall", "Alain Lacroux", "Christelle Martin-Lacroux", "Vincent Brault"], "title": "Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview", "categories": ["cs.AI", "cs.CY", "stat.AP", "stat.ME"], "comment": "38 pages, 25 figures, 4 tables", "summary": "Artificial intelligence is used at various stages of the recruitment process\nto automatically select the best candidate for a position, with companies\nguaranteeing unbiased recruitment. However, the algorithms used are either\ntrained by humans or are based on learning from past experiences that were\nbiased. In this article, we propose to generate data mimicking external\n(discrimination) and internal biases (self-censorship) in order to train five\nclassic algorithms and to study the extent to which they do or do not find the\nbest candidates according to objective criteria. In addition, we study the\ninfluence of the anonymisation of files on the quality of predictions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u5728\u62db\u8058\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u504f\u89c1\uff0c\u901a\u8fc7\u6a21\u62df\u5916\u90e8\uff08\u6b67\u89c6\uff09\u548c\u5185\u90e8\uff08\u81ea\u6211\u5ba1\u67e5\uff09\u504f\u89c1\u7684\u6570\u636e\u8bad\u7ec3\u4e94\u79cd\u7ecf\u5178\u7b97\u6cd5\uff0c\u5206\u6790\u5176\u662f\u5426\u80fd\u6839\u636e\u5ba2\u89c2\u6807\u51c6\u9009\u51fa\u6700\u4f73\u5019\u9009\u4eba\uff0c\u5e76\u63a2\u8ba8\u6587\u4ef6\u533f\u540d\u5316\u5bf9\u9884\u6d4b\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8e\u62db\u8058AI\u7b97\u6cd5\u7531\u4eba\u7c7b\u8bad\u7ec3\u6216\u57fa\u4e8e\u6709\u504f\u89c1\u7684\u5386\u53f2\u5b66\u4e60\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u9009\u62d4\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e9b\u7b97\u6cd5\u5728\u504f\u89c1\u6570\u636e\u4e0b\u7684\u8868\u73b0\u53ca\u5176\u6539\u8fdb\u53ef\u80fd\u3002", "method": "\u751f\u6210\u6a21\u62df\u5916\u90e8\u548c\u5185\u90e8\u504f\u89c1\u7684\u6570\u636e\uff0c\u8bad\u7ec3\u4e94\u79cd\u7ecf\u5178\u7b97\u6cd5\uff0c\u5e76\u5206\u6790\u5176\u8868\u73b0\u3002\u540c\u65f6\u7814\u7a76\u6587\u4ef6\u533f\u540d\u5316\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u504f\u89c1\u6570\u636e\u5bf9\u7b97\u6cd5\u9009\u62d4\u51c6\u786e\u6027\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u8868\u660e\u533f\u540d\u5316\u53ef\u80fd\u63d0\u5347\u9884\u6d4b\u7684\u5ba2\u89c2\u6027\u3002", "conclusion": "\u4e3a\u51cf\u5c11\u62db\u8058\u504f\u89c1\uff0c\u9700\u4f18\u5316\u7b97\u6cd5\u8bad\u7ec3\u6570\u636e\u5e76\u8003\u8651\u91c7\u7528\u533f\u540d\u5316\u6280\u672f\u4ee5\u63d0\u9ad8\u9009\u62d4\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2505.02410", "pdf": "https://arxiv.org/pdf/2505.02410", "abs": "https://arxiv.org/abs/2505.02410", "authors": ["Krzysztof Ociepa", "\u0141ukasz Flis", "Krzysztof Wr\u00f3bel", "Adrian Gwo\u017adziej", "Remigiusz Kinas"], "title": "Bielik 11B v2 Technical Report", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages.", "AI": {"tldr": "Bielik 11B v2\u662f\u57fa\u4e8eMistral 7B v0.2\u67b6\u6784\u4f18\u5316\u7684\u6ce2\u5170\u8bed\u5904\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u6269\u5c55\u8fbe\u523011B\u53c2\u6570\uff0c\u5728\u6ce2\u5170\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u8bed\u8a00\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u6ce2\u5170\u8bedAI\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u6548\u7387\u9ad8\u7684\u8bed\u8a00\u5efa\u6a21\u5728\u8f83\u5c11\u88ab\u4ee3\u8868\u7684\u8bed\u8a00\u4e2d\u8bbe\u7acb\u65b0\u6807\u6746\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u6269\u5c55\u548c\u4e24\u9879\u5173\u952e\u6280\u672f\uff1a\u52a0\u6743\u6307\u4ee4\u4ea4\u53c9\u71b5\u635f\u5931\uff08\u4f18\u5316\u591a\u6837\u5316\u6307\u4ee4\u5b66\u4e60\uff09\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\uff08\u6839\u636e\u4e0a\u4e0b\u6587\u957f\u5ea6\u52a8\u6001\u8c03\u6574\uff09\u3002", "result": "Bielik 11B v2\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u4f18\u4e8e\u53c2\u6570\u66f4\u5927\u7684\u6a21\u578b\uff0c\u663e\u8457\u8d85\u8d8a\u5176\u4ed6\u6ce2\u5170\u8bed\u6a21\u578b\uff0c\u53c2\u6570\u9ad8\u6548\u4e14\u652f\u6301\u5e7f\u6cdb\u91cf\u5316\u90e8\u7f72\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u63a8\u52a8\u4e86\u6ce2\u5170\u8bedAI\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u8fd8\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.01874", "pdf": "https://arxiv.org/pdf/2505.01874", "abs": "https://arxiv.org/abs/2505.01874", "authors": ["Youssef Allouah", "Rachid Guerraoui", "John Stephan"], "title": "Towards Trustworthy Federated Learning with Untrusted Participants", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": "arXiv admin note: text overlap with arXiv:2302.04787", "summary": "Resilience against malicious parties and data privacy are essential for\ntrustworthy distributed learning, yet achieving both with good utility\ntypically requires the strong assumption of a trusted central server. This\npaper shows that a significantly weaker assumption suffices: each pair of\nworkers shares a randomness seed unknown to others. In a setting where\nmalicious workers may collude with an untrusted server, we propose CafCor, an\nalgorithm that integrates robust gradient aggregation with correlated noise\ninjection, leveraging shared randomness between workers. We prove that CafCor\nachieves strong privacy-utility trade-offs, significantly outperforming local\ndifferential privacy (DP) methods, which do not make any trust assumption,\nwhile approaching central DP utility, where the server is fully trusted.\nEmpirical results on standard benchmarks validate CafCor's practicality,\nshowing that privacy and robustness can coexist in distributed systems without\nsacrificing utility or trusting the server.", "AI": {"tldr": "CafCor\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u5de5\u4f5c\u8005\u4e4b\u95f4\u7684\u5171\u4eab\u968f\u673a\u6027\uff0c\u7ed3\u5408\u5065\u58ee\u7684\u68af\u5ea6\u805a\u5408\u548c\u76f8\u5173\u6027\u566a\u58f0\u6ce8\u5165\uff0c\u5728\u4e0d\u5b8c\u5168\u4fe1\u4efb\u4e2d\u592e\u670d\u52a1\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u9ad8\u6548\u5b9e\u7528\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u5f0f\u5b66\u4e60\u9700\u8981\u5728\u6570\u636e\u9690\u79c1\u548c\u6076\u610f\u653b\u51fb\u9632\u62a4\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u901a\u5e38\u4f9d\u8d56\u53ef\u4fe1\u4e2d\u592e\u670d\u52a1\u5668\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u5728\u8f83\u5f31\u7684\u5047\u8bbe\uff08\u6bcf\u5bf9\u5de5\u4f5c\u8005\u5171\u4eab\u672a\u77e5\u7684\u968f\u673a\u79cd\u5b50\uff09\u4e0b\uff0c\u4ecd\u80fd\u5b9e\u73b0\u8fd9\u4e24\u8005\u7684\u5171\u5b58\u3002", "method": "\u63d0\u51faCafCor\u7b97\u6cd5\uff0c\u7ed3\u5408\u5065\u58ee\u7684\u68af\u5ea6\u805a\u5408\u548c\u76f8\u5173\u6027\u566a\u58f0\u6ce8\u5165\uff0c\u5229\u7528\u5de5\u4f5c\u8005\u4e4b\u95f4\u7684\u5171\u4eab\u968f\u673a\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6076\u610f\u5de5\u4f5c\u8005\u53ef\u80fd\u4e0e\u4e0d\u53d7\u4fe1\u4efb\u7684\u670d\u52a1\u5668\u52fe\u7ed3\u7684\u573a\u666f\u4e0b\u8fd0\u884c\u3002", "result": "\u7406\u8bba\u8bc1\u660eCafCor\u5728\u9690\u79c1-\u6548\u7528\u6743\u8861\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u672c\u5730\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\uff0c\u5e76\u63a5\u8fd1\u5b8c\u5168\u4fe1\u4efb\u670d\u52a1\u5668\u65f6\u7684\u4e2d\u5fc3\u5dee\u5206\u9690\u79c1\u6548\u7528\u3002\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u5b8c\u5168\u4fe1\u4efb\u670d\u52a1\u5668\u6216\u727a\u7272\u5b9e\u7528\u6027\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u9690\u79c1\u548c\u9c81\u68d2\u6027\u53ef\u4ee5\u5171\u5b58\uff0cCafCor\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02665", "pdf": "https://arxiv.org/pdf/2505.02665", "abs": "https://arxiv.org/abs/2505.02665", "authors": ["Qianjun Pan", "Wenkai Ji", "Yuyang Ding", "Junsong Li", "Shilian Chen", "Junyi Wang", "Jie Zhou", "Qin Chen", "Min Zhang", "Yulan Wu", "Liang He"], "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law", "categories": ["cs.AI"], "comment": null, "summary": "This survey explores recent advancements in reasoning large language models\n(LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by\nhuman cognition, as described in Kahneman's Thinking, Fast and Slow. These\nmodels, like OpenAI's o1, focus on scaling computational resources dynamically\nduring complex tasks, such as math reasoning, visual reasoning, medical\ndiagnosis, and multi-agent debates. We present the development of reasoning\nLLMs and list their key technologies. By synthesizing over 100 studies, it\ncharts a path toward LLMs that combine human-like deep thinking with scalable\nefficiency for reasoning. The review breaks down methods into three categories:\n(1) test-time scaling dynamically adjusts computation based on task complexity\nvia search and sampling, dynamic verification; (2) reinforced learning refines\ndecision-making through iterative improvement leveraging policy networks,\nreward models, and self-evolution strategies; and (3) slow-thinking frameworks\n(e.g., long CoT, hierarchical processes) that structure problem-solving with\nmanageable steps. The survey highlights the challenges and further directions\nof this domain. Understanding and advancing the reasoning abilities of LLMs is\ncrucial for unlocking their full potential in real-world applications, from\nscientific discovery to decision support systems.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u63a2\u8ba8\u4e86\u6a21\u62df\u4eba\u7c7b'\u6162\u601d\u8003'\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ecb\u7ecd\u4e86\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u3001\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4ee5\u53ca\u6162\u601d\u8003\u6846\u67b6\u7b49\u5173\u952e\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u6df1\u5ea6\u601d\u8003\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d1\u6325\u66f4\u5927\u4f5c\u7528\u3002", "method": "\u8bba\u6587\u5c06\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a1. \u52a8\u6001\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08\u901a\u8fc7\u641c\u7d22\u3001\u91c7\u6837\u548c\u52a8\u6001\u9a8c\u8bc1\u8c03\u6574\u8ba1\u7b97\uff09\uff1b2. \u5f3a\u5316\u5b66\u4e60\uff08\u5229\u7528\u7b56\u7565\u7f51\u7edc\u548c\u5956\u52b1\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\uff09\uff1b3. \u6162\u601d\u8003\u6846\u67b6\uff08\u5982\u957f\u94fe\u63a8\u7406\u548c\u5206\u5c42\u6d41\u7a0b\uff09\u3002", "result": "\u7efc\u8ff0\u6574\u5408\u4e86100\u591a\u9879\u7814\u7a76\uff0c\u4e3a\u7ed3\u5408\u4eba\u7c7b\u6df1\u5ea6\u601d\u8003\u4e0e\u9ad8\u6548\u63a8\u7406\u7684LLMs\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u5176\u5728\u79d1\u5b66\u53d1\u73b0\u548c\u51b3\u7b56\u652f\u6301\u7b49\u573a\u666f\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4ecd\u9700\u89e3\u51b3\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2505.02456", "pdf": "https://arxiv.org/pdf/2505.02456", "abs": "https://arxiv.org/abs/2505.02456", "authors": ["Elisa Forcada Rodr\u00edguez", "Olatz Perez-de-Vi\u00f1aspre", "Jon Ander Campos", "Dietrich Klakow", "Vagrant Gautam"], "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs", "categories": ["cs.CL"], "comment": null, "summary": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u4e2d\u4ea4\u7ec7\u7684\u56fd\u5bb6\u548c\u6027\u522b\u504f\u89c1\uff0c\u91cd\u70b9\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u804c\u4e1a\u63a8\u8350\u3002\u901a\u8fc7\u6784\u5efa\u5305\u542b\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u5fb7\u8bed\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u752825\u4e2a\u56fd\u5bb6\u548c4\u79cd\u4ee3\u8bcd\u7ec4\u5408\uff0c\u8bc4\u4f30\u4e865\u4e2a\u57fa\u4e8eLlama\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u5373\u4f7f\u6a21\u578b\u5728\u5355\u4e00\u6027\u522b\u6216\u56fd\u5bb6\u6807\u51c6\u4e0a\u8868\u73b0\u516c\u5e73\uff0c\u4ea4\u7ec7\u7684\u804c\u4e1a\u504f\u89c1\u4ecd\u5b58\u5728\u3002", "motivation": "\u73b0\u6709NLP\u516c\u5e73\u6027\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u4e00\u504f\u89c1\uff08\u5982\u6027\u522b\uff09\u548c\u82f1\u8bed\u8bed\u79cd\uff0c\u5ffd\u7565\u4e86\u591a\u8bed\u8a00\u548c\u591a\u7ef4\u5ea6\u504f\u89c1\u7684\u590d\u6742\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u4ea4\u7ec7\u56fd\u5bb6\u548c\u6027\u522b\u504f\u89c1\u3002", "method": "\u6784\u5efa\u5305\u542b\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u5fb7\u8bed\u7684\u591a\u8bed\u8a00\u63d0\u793a\u57fa\u51c6\uff0c\u7cfb\u7edf\u53d8\u6362\u56fd\u5bb6\u548c\u6027\u522b\u53d8\u91cf\uff0825\u4e2a\u56fd\u5bb6\u00d74\u79cd\u4ee3\u8bcd\u7ec4\u5408\uff09\uff0c\u8bc4\u4f305\u4e2aLlama\u6a21\u578b\u5728\u804c\u4e1a\u63a8\u8350\u4e2d\u7684\u504f\u89c1\u8868\u73b0\u3002", "result": "\u6a21\u578b\u666e\u904d\u5b58\u5728\u663e\u8457\u7684\u56fd\u5bb6\u548c\u6027\u522b\u504f\u89c1\uff0c\u5373\u4f7f\u5355\u4e00\u7ef4\u5ea6\uff08\u6027\u522b\u6216\u56fd\u5bb6\uff09\u8868\u73b0\u516c\u5e73\uff0c\u4ea4\u7ec7\u504f\u89c1\u4f9d\u7136\u7a81\u51fa\u3002\u63d0\u793a\u8bed\u8a00\u5bf9\u504f\u89c1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u504f\u7f6e\u6c34\u5e73\u6700\u4f4e\u4e14\u6700\u7a33\u5b9a\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u516c\u5e73\u6027\u7814\u7a76\u9700\u91c7\u7528\u591a\u8bed\u8a00\u548c\u4ea4\u7ec7\u6027\u89c6\u89d2\uff0c\u6307\u4ee4\u5fae\u8c03\u53ef\u80fd\u662f\u51cf\u5c11\u504f\u89c1\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.01892", "pdf": "https://arxiv.org/pdf/2505.01892", "abs": "https://arxiv.org/abs/2505.01892", "authors": ["Nikolaos Louloudakis", "Ajitha Rajan"], "title": "OODTE: A Differential Testing Engine for the ONNX Optimizer", "categories": ["cs.LG", "cs.AI", "cs.SE", "cs.SY", "eess.SY"], "comment": "12 pages, 3 figures, 3 tables", "summary": "With $700$ stars on GitHub and part of the official ONNX repository, the ONNX\nOptimizer consists of the standard method to apply graph-based optimizations on\nONNX models. However, its ability to preserve model accuracy across\noptimizations, has not been rigorously explored. We propose OODTE, a utility to\nautomatically and thoroughly assess the correctness of the ONNX Optimizer.\nOODTE follows a simple, yet effective differential testing and evaluation\napproach that can be easily adopted to other compiler optimizers. In\nparticular, OODTE utilizes a number of ONNX models, then optimizes them and\nexecutes both the original and the optimized variants across a user-defined set\nof inputs, while automatically logging any issues with the optimization\nprocess. Finally, for successfully optimized models, OODTE compares the\nresults, and, if any accuracy deviations are observed, it iteratively repeats\nthe process for each pass of the ONNX Optimizer, to localize the root cause of\nthe differences observed. Using OODTE, we sourced well-known $130$ models from\nthe official ONNX Model Hub, used for a wide variety of tasks (classification,\nobject detection, semantic segmentation, text summarization, question and\nanswering, sentiment analysis) from the official ONNX model hub. We detected 15\nissues, 14 of which were previously unknown, associated with optimizer crashes\nand accuracy deviations. We also observed $9.2$% of all model instances\npresenting issues leading into the crash of the optimizer, or the generation of\nan invalid model while using the primary optimizer strategies. In addition,\n$30$% of the classification models presented accuracy differences across the\noriginal and the optimized model variants, while $16.6$% of semantic\nsegmentation and object detection models are also affected, at least to a\nlimited extent.", "AI": {"tldr": "ONNX Optimizer\u7684\u51c6\u786e\u6027\u672a\u4e25\u683c\u9a8c\u8bc1\u3002OODTE\u5de5\u5177\u901a\u8fc7\u5dee\u5206\u6d4b\u8bd5\u8bc4\u4f30\u5176\u4f18\u5316\u6548\u679c\uff0c\u53d1\u73b0\u591a\u4e2a\u95ee\u9898\u3002", "motivation": "\u9a8c\u8bc1ONNX Optimizer\u5728\u6a21\u578b\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u51c6\u786e\u6027\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528OODTE\u5de5\u5177\u5bf9130\u4e2aONNX\u6a21\u578b\u8fdb\u884c\u5dee\u5206\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4f18\u5316\u524d\u540e\u7ed3\u679c\u5e76\u5b9a\u4f4d\u95ee\u9898\u3002", "result": "\u53d1\u73b015\u4e2a\u95ee\u9898\uff0814\u4e2a\u65b0\u95ee\u9898\uff09\uff0c9.2%\u6a21\u578b\u5bfc\u81f4\u4f18\u5316\u5668\u5d29\u6e83\uff0c30%\u5206\u7c7b\u6a21\u578b\u5b58\u5728\u7cbe\u5ea6\u5dee\u5f02\u3002", "conclusion": "ONNX Optimizer\u9700\u6539\u8fdb\u4ee5\u4fdd\u8bc1\u4f18\u5316\u540e\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.02707", "pdf": "https://arxiv.org/pdf/2505.02707", "abs": "https://arxiv.org/abs/2505.02707", "authors": ["Yemin Shi", "Yu Shu", "Siwei Dong", "Guangyi Liu", "Jaward Sesay", "Jingwen Li", "Zhiting Hu"], "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play", "categories": ["cs.AI", "cs.CL", "cs.SD"], "comment": "18 pages, 7 figures, Website: https://voila.maitrix.org", "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions.", "AI": {"tldr": "Voila\u662f\u4e00\u7cfb\u5217\u5927\u578b\u8bed\u97f3\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u67b6\u6784\u5b9e\u73b0\u5168\u53cc\u5de5\u3001\u4f4e\u5ef6\u8fdf\u5bf9\u8bdd\uff0c\u652f\u6301\u60c5\u611f\u8868\u8fbe\u548c\u4e2a\u6027\u5316\u8bed\u97f3\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u6253\u9020\u80fd\u81ea\u7136\u878d\u5165\u65e5\u5e38\u751f\u6d3b\u7684\u8bed\u97f3AI\uff0c\u5b9e\u73b0\u81ea\u4e3b\u3001\u5b9e\u65f6\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u5c3a\u5ea6Transformer\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u58f0\u5b66\u5efa\u6a21\uff0c\u652f\u6301\u6587\u672c\u6307\u4ee4\u5b9a\u4e49\u8bed\u97f3\u7279\u6027\u3002", "result": "\u5ef6\u8fdf\u4ec5195\u6beb\u79d2\uff0c\u652f\u6301\u767e\u4e07\u9884\u5efa\u8bed\u97f3\u548c10\u79d2\u97f3\u9891\u6837\u672c\u5b9a\u5236\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u97f3\u5e94\u7528\u3002", "conclusion": "Voila\u5f00\u6e90\u4ee5\u63a8\u52a8\u4e0b\u4e00\u4ee3\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u3002"}}
{"id": "2505.02463", "pdf": "https://arxiv.org/pdf/2505.02463", "abs": "https://arxiv.org/abs/2505.02463", "authors": ["Richard Kimera", "Dongnyeong Heo", "Daniela N. Rim", "Heeyoul Choi"], "title": "Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda", "categories": ["cs.CL"], "comment": "NLPIR '24: Proceedings of the 2024 8th International Conference on\n  Natural Language Processing and Information Retrieval", "summary": "In this paper,we explore the application of Back translation (BT) as a\nsemi-supervised technique to enhance Neural Machine Translation(NMT) models for\nthe English-Luganda language pair, specifically addressing the challenges faced\nby low-resource languages. The purpose of our study is to demonstrate how BT\ncan mitigate the scarcity of bilingual data by generating synthetic data from\nmonolingual corpora. Our methodology involves developing custom NMT models\nusing both publicly available and web-crawled data, and applying Iterative and\nIncremental Back translation techniques. We strategically select datasets for\nincremental back translation across multiple small datasets, which is a novel\nelement of our approach. The results of our study show significant\nimprovements, with translation performance for the English-Luganda pair\nexceeding previous benchmarks by more than 10 BLEU score units across all\ntranslation directions. Additionally, our evaluation incorporates comprehensive\nassessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced\nunderstanding of translation quality. The conclusion drawn from our research\nconfirms the efficacy of BT when strategically curated datasets are utilized,\nestablishing new performance benchmarks and demonstrating the potential of BT\nin enhancing NMT models for low-resource languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u534a\u76d1\u7763\u53cd\u5411\u7ffb\u8bd1\uff08BT\uff09\u6280\u672f\u7528\u4e8e\u63d0\u5347\u82f1\u8bed-Luganda\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u6a21\u578b\u6027\u80fd\uff0c\u5229\u7528\u5355\u8bed\u8bed\u6599\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u6700\u7ec8BLEU\u5206\u6570\u63d0\u5347\u8d85\u8fc710\u5206\uff0c\u8bc1\u660e\u4e86BT\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u82f1\u8bed-Luganda\uff09\u53cc\u8bed\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53cd\u5411\u7ffb\u8bd1\u6280\u672f\u751f\u6210\u5408\u6210\u6570\u636e\u4ee5\u5f25\u8865\u6570\u636e\u4e0d\u8db3\uff0c\u63d0\u5347\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u81ea\u5b9a\u4e49NMT\u6a21\u578b\uff0c\u7ed3\u5408\u516c\u5f00\u548c\u7f51\u7edc\u722c\u53d6\u6570\u636e\uff0c\u91c7\u7528\u8fed\u4ee3\u548c\u589e\u91cf\u5f0f\u53cd\u5411\u7ffb\u8bd1\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u5c0f\u6570\u636e\u96c6\u4e0a\u7b56\u7565\u6027\u5e94\u7528\u589e\u91cf\u7ffb\u8bd1\u3002", "result": "\u7ffb\u8bd1\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6240\u6709\u7ffb\u8bd1\u65b9\u5411\u7684BLEU\u5206\u6570\u8d85\u8fc7\u4e4b\u524d\u57fa\u51c610\u5206\u4ee5\u4e0a\uff0c\u5e76\u7efc\u5408\u4f7f\u7528SacreBLEU\u3001ChrF2\u548cTER\u7b49\u6307\u6807\u8bc4\u4f30\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff0c\u7b56\u7565\u6027\u6570\u636e\u96c6\u9009\u62e9\u4e0bBT\u6280\u672f\u9ad8\u6548\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00NMT\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5c55\u73b0\u4e86BT\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.01902", "pdf": "https://arxiv.org/pdf/2505.01902", "abs": "https://arxiv.org/abs/2505.01902", "authors": ["Ali Al-Bustami", "Zaid Ghazal"], "title": "From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup", "categories": ["cs.LG"], "comment": null, "summary": "Accurate prediction of FIFA World Cup match outcomes holds significant value\nfor analysts, coaches, bettors, and fans. This paper presents a machine\nlearning framework specifically designed to forecast match winners in FIFA\nWorld Cup. By integrating both team-level historical data and player-specific\nperformance metrics such as goals, assists, passing accuracy, and tackles, we\ncapture nuanced interactions often overlooked by traditional aggregate models.\nOur methodology processes multi-year data to create year-specific team profiles\nthat account for evolving rosters and player development. We employ\nclassification techniques complemented by dimensionality reduction and\nhyperparameter optimization, to yield robust predictive models. Experimental\nresults on data from the FIFA 2022 World Cup demonstrate our approach's\nsuperior accuracy compared to baseline method. Our findings highlight the\nimportance of incorporating individual player attributes and team-level\ncomposition to enhance predictive performance, offering new insights into\nplayer synergy, strategic match-ups, and tournament progression scenarios. This\nwork underscores the transformative potential of rich, player-centric data in\nsports analytics, setting a foundation for future exploration of advanced\nlearning architectures such as graph neural networks to model complex team\ninteractions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7403\u5458\u4e2a\u4eba\u8868\u73b0\u548c\u56e2\u961f\u5386\u53f2\u6570\u636e\uff0c\u7528\u4e8e\u9884\u6d4b\u4e16\u754c\u676f\u6bd4\u8d5b\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u57282022\u5e74\u4e16\u754c\u676f\u4e0a\u7684\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4e16\u754c\u676f\u6bd4\u8d5b\u7ed3\u679c\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u5206\u6790\u5e08\u3001\u6559\u7ec3\u3001\u8d4c\u5ba2\u548c\u7403\u8ff7\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u7403\u5458\u4e2a\u4f53\u4e0e\u56e2\u961f\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u6574\u5408\u591a\u5e74\u5386\u53f2\u6570\u636e\u548c\u7403\u5458\u5177\u4f53\u8868\u73b0\u6307\u6807\uff08\u5982\u8fdb\u7403\u3001\u52a9\u653b\u3001\u4f20\u7403\u51c6\u786e\u7387\u7b49\uff09\uff0c\u6784\u5efa\u5e74\u5ea6\u56e2\u961f\u6863\u6848\uff0c\u5e76\u91c7\u7528\u5206\u7c7b\u6280\u672f\u3001\u964d\u7ef4\u548c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u57282022\u5e74\u4e16\u754c\u676f\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u7403\u5458\u5c5e\u6027\u548c\u56e2\u961f\u6784\u6210\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7403\u5458\u4e2d\u5fc3\u6570\u636e\u5728\u4f53\u80b2\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u63a2\u7d22\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u9ad8\u7ea7\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.02709", "pdf": "https://arxiv.org/pdf/2505.02709", "abs": "https://arxiv.org/abs/2505.02709", "authors": ["Rauno Arike", "Elizabeth Donoway", "Henning Bartsch", "Marius Hobbhahn"], "title": "Technical Report: Evaluating Goal Drift in Language Model Agents", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As language models (LMs) are increasingly deployed as autonomous agents,\ntheir robust adherence to human-assigned objectives becomes crucial for safe\noperation. When these agents operate independently for extended periods without\nhuman oversight, even initially well-specified goals may gradually shift.\nDetecting and measuring goal drift - an agent's tendency to deviate from its\noriginal objective over time - presents significant challenges, as goals can\nshift gradually, causing only subtle behavioral changes. This paper proposes a\nnovel approach to analyzing goal drift in LM agents. In our experiments, agents\nare first explicitly given a goal through their system prompt, then exposed to\ncompeting objectives through environmental pressures. We demonstrate that while\nthe best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains\nnearly perfect goal adherence for more than 100,000 tokens in our most\ndifficult evaluation setting, all evaluated models exhibit some degree of goal\ndrift. We also find that goal drift correlates with models' increasing\nsusceptibility to pattern-matching behaviors as the context length grows.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\u65f6\uff0c\u76ee\u6807\u6f02\u79fb\uff08\u9010\u6e10\u504f\u79bb\u521d\u59cb\u76ee\u6807\u7684\u95ee\u9898\uff09\u662f\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u5206\u6790LM\u4ee3\u7406\u7684\u76ee\u6807\u6f02\u79fb\uff0c\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u8868\u73b0\u6700\u4f73\u7684\u4ee3\u7406\u4e5f\u4f1a\u5728\u957f\u65f6\u95f4\u64cd\u4f5c\u4e2d\u51fa\u73b0\u76ee\u6807\u6f02\u79fb\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u786e\u4fdd\u5b83\u4eec\u957f\u65f6\u95f4\u65e0\u76d1\u7763\u64cd\u4f5c\u65f6\u4ecd\u80fd\u575a\u6301\u4eba\u7c7b\u8bbe\u5b9a\u7684\u76ee\u6807\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u76ee\u6807\u6f02\u79fb\u53ef\u80fd\u5bfc\u81f4\u884c\u4e3a\u504f\u79bb\uff0c\u4f46\u68c0\u6d4b\u8fd9\u79cd\u7f13\u6162\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u4e3a\u4ee3\u7406\u8bbe\u5b9a\u660e\u786e\u521d\u59cb\u76ee\u6807\uff08\u7cfb\u7edf\u63d0\u793a\uff09\uff0c\u5e76\u5f15\u5165\u73af\u5883\u538b\u529b\u6d4b\u8bd5\u5176\u76ee\u6807\u575a\u6301\u6027\u3002\u5b9e\u9a8c\u91c7\u7528Claude 3.5 Sonnet\u7b49\u6a21\u578b\uff0c\u5206\u6790\u5176\u5728\u957f\u65f6\u95f4\uff08\u8d85\u8fc710\u4e07token\uff09\u64cd\u4f5c\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73\u4ee3\u7406\uff08\u7ecf\u8fc7\u4f18\u5316\u7684Claude 3.5 Sonnet\uff09\u5728\u6700\u4e25\u82db\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u8fd1100%\u76ee\u6807\u575a\u6301\u6027\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5747\u51fa\u73b0\u4e0d\u540c\u7a0b\u5ea6\u76ee\u6807\u6f02\u79fb\uff0c\u4e14\u6f02\u79fb\u4e0e\u4e0a\u4e0b\u6587\u589e\u957f\u5bfc\u81f4\u7684\u6a21\u5f0f\u5339\u914d\u884c\u4e3a\u76f8\u5173\u3002", "conclusion": "\u76ee\u6807\u6f02\u79fb\u662fLM\u4ee3\u7406\u7684\u666e\u904d\u73b0\u8c61\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u5347\u957f\u671f\u76ee\u6807\u575a\u6301\u6027\u3002\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6a21\u5f0f\u5339\u914d\u884c\u4e3a\u662f\u5bfc\u81f4\u6f02\u79fb\u7684\u91cd\u8981\u56e0\u7d20\u3002"}}
{"id": "2505.02518", "pdf": "https://arxiv.org/pdf/2505.02518", "abs": "https://arxiv.org/abs/2505.02518", "authors": ["Muhammad Hazim Al Farouq", "Aman Kassahun Wassie", "Yasmin Moslem"], "title": "Bemba Speech Translation: Exploring a Low-Resource African Language", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2025), low-resource languages track, namely\nfor Bemba-to-English speech translation. We built cascaded speech translation\nsystems based on Whisper and NLLB-200, and employed data augmentation\ntechniques, such as back-translation. We investigate the effect of using\nsynthetic data and discuss our experimental setup.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9Bemba-to-English\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u7ffb\u8bd1\u7684\u7cfb\u7edf\uff0c\u57fa\u4e8eWhisper\u548cNLLB-200\u6784\u5efa\u7ea7\u8054\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u56de\u8bd1\uff09\u3002\u63a2\u8ba8\u4e86\u5408\u6210\u6570\u636e\u7684\u6548\u679c\u53ca\u5b9e\u9a8c\u8bbe\u7f6e\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Bemba-to-English\uff09\u8bed\u97f3\u7ffb\u8bd1\u7684\u6311\u6218\uff0c\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002", "method": "\u4f7f\u7528Whisper\u548cNLLB-200\u6784\u5efa\u7ea7\u8054\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5e76\u5e94\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u56de\u8bd1\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u63a2\u8ba8\u4e86\u5408\u6210\u6570\u636e\u5bf9\u7ffb\u8bd1\u6548\u679c\u7684\u5f71\u54cd\u53ca\u5b9e\u9a8c\u7ed3\u679c\uff08\u5177\u4f53\u6548\u679c\u672a\u660e\u786e\u8bf4\u660e\uff09\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86Bemba-to-English\u4f4e\u8d44\u6e90\u8bed\u97f3\u7ffb\u8bd1\u7684\u7cfb\u7edf\u6784\u5efa\u4e0e\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u5e94\u7528\uff0c\u4e3a\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2505.01903", "pdf": "https://arxiv.org/pdf/2505.01903", "abs": "https://arxiv.org/abs/2505.01903", "authors": ["Nisarg Parikh", "Nigel Fernandez", "Alexander Scarlatos", "Simon Woodhead", "Andrew Lan"], "title": "LookAlike: Consistent Distractor Generation in Math MCQs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to generate distractors\nfor multiple-choice questions (MCQs), especially in domains like math\neducation. However, existing approaches are limited in ensuring that the\ngenerated distractors are consistent with common student errors. We propose\nLookAlike, a method that improves error-distractor consistency via preference\noptimization. Our two main innovations are: (a) mining synthetic preference\npairs from model inconsistencies, and (b) alternating supervised fine-tuning\n(SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike\nprior work that relies on heuristics or manually annotated preference data,\nLookAlike uses its own generation inconsistencies as dispreferred samples, thus\nenabling scalable and stable training. Evaluated on a real-world dataset of\n1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation\nand 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an\nexisting state-of-the-art method (45.6% / 47.7%). These improvements highlight\nthe effectiveness of preference-based regularization and inconsistency mining\nfor generating consistent math MCQ distractors at scale.", "AI": {"tldr": "LookAlike\u65b9\u6cd5\u901a\u8fc7\u504f\u597d\u4f18\u5316\u63d0\u5347\u6570\u5b66\u9009\u62e9\u9898\u5e72\u6270\u9879\u7684\u4e00\u81f4\u6027\uff0c\u5229\u7528\u6a21\u578b\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u8d1f\u6837\u672c\uff0c\u7ed3\u5408\u6709\u76d1\u7763\u5fae\u8c03\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u57281400+\u6570\u5b66\u9009\u62e9\u9898\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5e72\u6270\u9879\u96be\u4ee5\u4fdd\u8bc1\u4e0e\u5e38\u89c1\u5b66\u751f\u9519\u8bef\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5176\u5728\u6570\u5b66\u9009\u62e9\u9898\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51faLookAlike\u65b9\u6cd5\uff1a1) \u4ece\u6a21\u578b\u4e0d\u4e00\u81f4\u6027\u4e2d\u6316\u6398\u5408\u6210\u504f\u597d\u5bf9\uff1b2) \u4ea4\u66ff\u4f7f\u7528\u6709\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728LLM-as-a-judge\u8bc4\u4f30\u4e2d\uff0c\u5e72\u6270\u9879\u751f\u6210\u51c6\u786e\u738751.6%\uff0c\u9519\u8bef\u751f\u6210\u51c6\u786e\u738757.2%\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5(45.6%/47.7%)\u3002", "conclusion": "\u504f\u597d\u4f18\u5316\u548c\u4e00\u81f4\u6027\u6316\u6398\u80fd\u6709\u6548\u63d0\u5347\u6570\u5b66\u5e72\u6270\u9879\u751f\u6210\u7684\u89c4\u6a21\u5316\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.02722", "pdf": "https://arxiv.org/pdf/2505.02722", "abs": "https://arxiv.org/abs/2505.02722", "authors": ["Junu Kim", "Chaeeun Shim", "Sungjin Park", "Su Yeon Lee", "Gee Young Suh", "Chae-Man Lim", "Seong Jin Choi", "Song Mi Moon", "Kyoung-Ho Song", "Eu Suk Kim", "Hong Bin Kim", "Sejoong Kim", "Chami Im", "Dong-Wan Kang", "Yong Soo Kim", "Hee-Joon Bae", "Sung Yoon Lim", "Han-Gil Jeong", "Edward Choi"], "title": "Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Although large language models (LLMs) have demonstrated impressive reasoning\ncapabilities across general domains, their effectiveness in real-world clinical\npractice remains limited. This is likely due to their insufficient exposure to\nreal-world clinical data during training, as such data is typically not\nincluded due to privacy concerns. To address this, we propose enhancing the\nclinical reasoning capabilities of LLMs by leveraging real-world clinical data.\nWe constructed reasoning-intensive questions from a nationwide sepsis registry\nand fine-tuned Phi-4 on these questions using reinforcement learning, resulting\nin C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the\nin-domain test set, as evidenced by both quantitative metrics and expert\nevaluations. Furthermore, its enhanced reasoning capabilities generalized to a\nsepsis dataset involving different tasks and patient cohorts, an open-ended\nconsultations on antibiotics use task, and other diseases. Future research\nshould focus on training LLMs with large-scale, multi-disease clinical datasets\nto develop more powerful, general-purpose clinical reasoning models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u7684\u5b66\u4e60\uff0c\u63d0\u5347\u5176\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u65b0\u6a21\u578bC-Reason\u5728\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u901a\u7528\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u5168\u56fd\u8113\u6bd2\u75c7\u6ce8\u518c\u6570\u636e\u6784\u5efa\u63a8\u7406\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bf9Phi-4\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u53d1\u51faC-Reason\u6a21\u578b\u3002", "result": "C-Reason\u5728\u57df\u5185\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u4efb\u52a1\u548c\u75be\u75c5\u6570\u636e\u96c6\u3002", "conclusion": "\u672a\u6765\u5e94\u5229\u7528\u5927\u89c4\u6a21\u3001\u591a\u75be\u75c5\u4e34\u5e8a\u6570\u636e\u8bad\u7ec3LLM\uff0c\u4ee5\u5f00\u53d1\u66f4\u901a\u7528\u7684\u4e34\u5e8a\u63a8\u7406\u6a21\u578b\u3002"}}
{"id": "2505.02579", "pdf": "https://arxiv.org/pdf/2505.02579", "abs": "https://arxiv.org/abs/2505.02579", "authors": ["Lingxiao Kong", "Cong Yang", "Susanne Neufang", "Oya Deniz Beyan", "Zeyd Boukhers"], "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference", "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EMORL\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u4f18\u5316\u591a\u76ee\u6807RL\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u83b7\u5f97\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u5f53\u524dRL\u5728LLM\u5fae\u8c03\u4e2d\u5b58\u5728\u591a\u76ee\u6807\u5e73\u8861\u590d\u6742\u3001\u8bad\u7ec3\u6548\u7387\u4f4e\u3001\u53ef\u6269\u5c55\u6027\u5dee\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\u7b49\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u6539\u8fdb\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u96c6\u6210\u591a\u76ee\u6807RL\u6846\u67b6\uff08EMORL\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u7f51\u683c\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u805a\u5408\u65b9\u5f0f\uff0c\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "EMORL\u5728\u5b9e\u9a8c\u4e2d\u7684\u8bad\u7ec3\u6d88\u8017\u66f4\u4f4e\u4e14\u66f4\u7a33\u5b9a\uff0817529\u00b11650\u6570\u636e\u70b9\uff0c6573\u00b1147.43\u79d2\uff09\uff0c\u5e76\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "EMORL\u5728\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aRL\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01912", "pdf": "https://arxiv.org/pdf/2505.01912", "abs": "https://arxiv.org/abs/2505.01912", "authors": ["Evan R. Antoniuk", "Shehtab Zaman", "Tal Ben-Nun", "Peggy Li", "James Diffenderfer", "Busra Demirci", "Obadiah Smolenski", "Tim Hsu", "Anna M. Hiszpanski", "Kenneth Chiu", "Bhavya Kailkhura", "Brian Van Essen"], "title": "BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Advances in deep learning and generative modeling have driven interest in\ndata-driven molecule discovery pipelines, whereby machine learning (ML) models\nare used to filter and design novel molecules without requiring prohibitively\nexpensive first-principles simulations. Although the discovery of novel\nmolecules that extend the boundaries of known chemistry requires accurate\nout-of-distribution (OOD) predictions, ML models often struggle to generalize\nOOD. Furthermore, there are currently no systematic benchmarks for molecular\nOOD prediction tasks. We present BOOM, $\\boldsymbol{b}$enchmarks for\n$\\boldsymbol{o}$ut-$\\boldsymbol{o}$f-distribution $\\boldsymbol{m}$olecular\nproperty predictions -- a benchmark study of property-based out-of-distribution\nmodels for common molecular property prediction models. We evaluate more than\n140 combinations of models and property prediction tasks to benchmark deep\nlearning models on their OOD performance. Overall, we do not find any existing\nmodels that achieve strong OOD generalization across all tasks: even the top\nperforming model exhibited an average OOD error 3x larger than in-distribution.\nWe find that deep learning models with high inductive bias can perform well on\nOOD tasks with simple, specific properties. Although chemical foundation models\nwith transfer and in-context learning offer a promising solution for limited\ntraining data scenarios, we find that current foundation models do not show\nstrong OOD extrapolation capabilities. We perform extensive ablation\nexperiments to highlight how OOD performance is impacted by data generation,\npre-training, hyperparameter optimization, model architecture, and molecular\nrepresentation. We propose that developing ML models with strong OOD\ngeneralization is a new frontier challenge in chemical ML model development.\nThis open-source benchmark will be made available on Github.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86BOOM\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u6a21\u578b\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u6027\u80fd\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728OOD\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e73\u5747\u8bef\u5dee\u662f\u5206\u5e03\u5185\u8bef\u5dee\u76843\u500d\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5b50\u53d1\u73b0\u4efb\u52a1\u4e2d\u96be\u4ee5\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6570\u636e\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u57fa\u51c6\u6765\u8bc4\u4f30OOD\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30140\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u7ec4\u5408\uff0c\u5206\u6790\u6570\u636e\u751f\u6210\u3001\u9884\u8bad\u7ec3\u3001\u8d85\u53c2\u6570\u4f18\u5316\u7b49\u56e0\u7d20\u5bf9OOD\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728OOD\u4efb\u52a1\u4e0a\u8868\u73b0\u666e\u904d\u8f83\u5dee\uff0c\u5373\u4f7f\u6700\u4f73\u6a21\u578b\u7684OOD\u8bef\u5dee\u4e5f\u662f\u5206\u5e03\u5185\u8bef\u5dee\u76843\u500d\u3002", "conclusion": "\u5f00\u53d1\u5177\u6709\u5f3aOOD\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\u662f\u5316\u5b66\u673a\u5668\u5b66\u4e60\u7684\u65b0\u6311\u6218\uff0c\u5e76\u5f00\u6e90\u4e86BOOM\u57fa\u51c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2505.02735", "pdf": "https://arxiv.org/pdf/2505.02735", "abs": "https://arxiv.org/abs/2505.02735", "authors": ["Zhouliang Yu", "Ruotian Peng", "Keyi Ding", "Yizhe Li", "Zhongyuan Peng", "Minghao Liu", "Yifan Zhang", "Zheng Yuan", "Huajian Xin", "Wenhao Huang", "Yandong Wen", "Ge Zhang", "Weiyang Liu"], "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": "Technical Report v1 (33 pages, 8 figures, project page:\n  https://sphere-ai-lab.github.io/FormalMATH/)", "summary": "Formal mathematical reasoning remains a critical challenge for artificial\nintelligence, hindered by limitations of existing benchmarks in scope and\nscale. To address this, we present FormalMATH, a large-scale Lean4 benchmark\ncomprising 5,560 formally verified problems spanning from high-school Olympiad\nchallenges to undergraduate-level theorems across diverse domains (e.g.,\nalgebra, applied mathematics, calculus, number theory, and discrete\nmathematics). To mitigate the inefficiency of manual formalization, we\nintroduce a novel human-in-the-loop autoformalization pipeline that integrates:\n(1) specialized large language models (LLMs) for statement autoformalization,\n(2) multi-LLM semantic verification, and (3) negation-based disproof filtering\nstrategies using off-the-shelf LLM-based provers. This approach reduces expert\nannotation costs by retaining 72.09% of statements before manual verification\nwhile ensuring fidelity to the original natural-language problems. Our\nevaluation of state-of-the-art LLM-based theorem provers reveals significant\nlimitations: even the strongest models achieve only 16.46% success rate under\npractical sampling budgets, exhibiting pronounced domain bias (e.g., excelling\nin algebra but failing in calculus) and over-reliance on simplified automation\ntactics. Notably, we identify a counterintuitive inverse relationship between\nnatural-language solution guidance and proof success in chain-of-thought\nreasoning scenarios, suggesting that human-written informal reasoning\nintroduces noise rather than clarity in the formal reasoning settings. We\nbelieve that FormalMATH provides a robust benchmark for benchmarking formal\nmathematical reasoning.", "AI": {"tldr": "FormalMATH is a large-scale Lean4 benchmark with 5,560 formally verified math problems, addressing limitations in AI's formal reasoning. It uses an autoformalization pipeline with LLMs to reduce manual effort but reveals LLMs' low success rates (16.46%) and domain biases.", "motivation": "To overcome the lack of comprehensive benchmarks for formal mathematical reasoning in AI, FormalMATH provides a diverse and large-scale dataset to evaluate and advance AI capabilities in this domain.", "method": "The paper introduces a human-in-the-loop autoformalization pipeline combining LLMs for statement formalization, multi-LLM verification, and negation-based disproof filtering to minimize manual effort.", "result": "LLM-based theorem provers perform poorly (16.46% success rate), showing domain bias (e.g., good in algebra but poor in calculus) and inefficiency with informal human-written guidance.", "conclusion": "FormalMATH is a valuable benchmark for formal mathematical reasoning, highlighting the current limitations of LLMs and the need for improved methods in this field."}}
{"id": "2505.02590", "pdf": "https://arxiv.org/pdf/2505.02590", "abs": "https://arxiv.org/abs/2505.02590", "authors": ["Diksha Bhandari", "Alessandro Lopopolo", "Milena Rabovsky", "Sebastian Reich"], "title": "Ensemble Kalman filter for uncertainty in human language comprehension", "categories": ["cs.CL", "stat.AP", "stat.ML"], "comment": null, "summary": "Artificial neural networks (ANNs) are widely used in modeling sentence\nprocessing but often exhibit deterministic behavior, contrasting with human\nsentence comprehension, which manages uncertainty during ambiguous or\nunexpected inputs. This is exemplified by reversal anomalies-sentences with\nunexpected role reversals that challenge syntax and semantics-highlighting the\nlimitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.\nTo address these limitations, we propose a Bayesian framework for sentence\ncomprehension, applying an extension of the ensemble Kalman filter (EnKF) for\nBayesian inference to quantify uncertainty. By framing language comprehension\nas a Bayesian inverse problem, this approach enhances the SG model's ability to\nreflect human sentence processing with respect to the representation of\nuncertainty. Numerical experiments and comparisons with maximum likelihood\nestimation (MLE) demonstrate that Bayesian methods improve uncertainty\nrepresentation, enabling the model to better approximate human cognitive\nprocessing when dealing with linguistic ambiguities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u6846\u67b6\u6765\u6539\u8fdb\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u5728\u53e5\u5b50\u5904\u7406\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\uff0c\u901a\u8fc7\u96c6\u6210\u5361\u5c14\u66fc\u6ee4\u6ce2\u589e\u5f3a\u4eba\u7c7b\u8ba4\u77e5\u6a21\u62df\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08\u5982\u53e5\u5b50\u683c\u5f0f\u5854\u6a21\u578b\uff09\u5728\u53e5\u5b50\u5904\u7406\u4e2d\u8868\u73b0\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u4e0e\u4eba\u7c7b\u5728\u6b67\u4e49\u6216\u610f\u5916\u8f93\u5165\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u5f62\u6210\u5bf9\u6bd4\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u8ba4\u77e5\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u6269\u5c55\u96c6\u6210\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EnKF\uff09\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u5c06\u8bed\u8a00\u7406\u89e3\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\uff0c\u4ee5\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u53ca\u4e0e\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\uff08MLE\uff09\u7684\u5bf9\u6bd4\u663e\u793a\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u8868\u5f81\u80fd\u529b\uff0c\u4f7f\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5728\u8bed\u8a00\u6b67\u4e49\u4e2d\u7684\u8ba4\u77e5\u5904\u7406\u3002", "conclusion": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u4f20\u7edf\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u8d34\u8fd1\u4eba\u7c7b\u53e5\u5b50\u7406\u89e3\u673a\u5236\u3002"}}
{"id": "2505.01933", "pdf": "https://arxiv.org/pdf/2505.01933", "abs": "https://arxiv.org/abs/2505.01933", "authors": ["Kyungsu Kim"], "title": "Unemployment Dynamics Forecasting with Machine Learning Regression Models", "categories": ["cs.LG", "econ.EM"], "comment": "18 pages, 2 charts", "summary": "In this paper, I explored how a range of regression and machine learning\ntechniques can be applied to monthly U.S. unemployment data to produce timely\nforecasts. I compared seven models: Linear Regression, SGDRegressor, Random\nForest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network,\ntraining each on a historical span of data and then evaluating on a later\nhold-out period. Input features include macro indicators (GDP growth, CPI),\nlabor market measures (job openings, initial claims), financial variables\n(interest rates, equity indices), and consumer sentiment.\n  I tuned model hyperparameters via cross-validation and assessed performance\nwith standard error metrics and the ability to predict the correct unemployment\ndirection. Across the board, tree-based ensembles (and CatBoost in particular)\ndeliver noticeably better forecasts than simple linear approaches, while the\nLSTM captures underlying temporal patterns more effectively than other\nnonlinear methods. SVR and SGDRegressor yield modest gains over standard\nregression but don't match the consistency of the ensemble and deep-learning\nmodels.\n  Interpretability tools ,feature importance rankings and SHAP values, point to\njob openings and consumer sentiment as the most influential predictors across\nall methods. By directly comparing linear, ensemble, and deep-learning\napproaches on the same dataset, our study shows how modern machine-learning\ntechniques can enhance real-time unemployment forecasting, offering economists\nand policymakers richer insights into labor market trends.\n  In the comparative evaluation of the models, I employed a dataset comprising\nthirty distinct features over the period from January 2020 through December\n2024.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u79cd\u56de\u5f52\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u7f8e\u56fd\u6708\u5ea6\u5931\u4e1a\u6570\u636e\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u4e03\u79cd\u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6811\u96c6\u6210\u548cLSTM\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6280\u672f\u63d0\u5347\u5b9e\u65f6\u5931\u4e1a\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7ecf\u6d4e\u5b66\u5bb6\u548c\u51b3\u7b56\u8005\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u52b3\u52a8\u529b\u5e02\u573a\u8d8b\u52bf\u6d1e\u5bdf\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u3001SGDRegressor\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001CatBoost\u3001SVR\u548cLSTM\u4e03\u79cd\u6a21\u578b\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u8bc4\u4f30\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u6811\u96c6\u6210\uff08\u5c24\u5176\u662fCatBoost\uff09\u548cLSTM\u8868\u73b0\u4f18\u4e8e\u7ebf\u6027\u65b9\u6cd5\uff0cSVR\u548cSGDRegressor\u6539\u8fdb\u6709\u9650\uff1b\u5c31\u4e1a\u673a\u4f1a\u548c\u6d88\u8d39\u8005\u60c5\u7eea\u662f\u6700\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6280\u672f\uff08\u5c24\u5176\u662f\u6811\u96c6\u6210\u548c\u6df1\u5ea6\u5b66\u4e60\uff09\u80fd\u663e\u8457\u63d0\u5347\u5931\u4e1a\u9884\u6d4b\u6548\u679c\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2505.02747", "pdf": "https://arxiv.org/pdf/2505.02747", "abs": "https://arxiv.org/abs/2505.02747", "authors": ["Aggeliki Sideraki", "Christos-Nikolaos Anagnostopoulos"], "title": "The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD", "categories": ["cs.AI", "cs.CY"], "comment": "21 pages", "summary": "This paper explores the use of Artificial Intelligence (AI) as a tool for\ndiagnosis, assessment, and intervention for individuals with Autism Spectrum\nDisorder (ASD). It focuses particularly on AI's role in early diagnosis,\nutilizing advanced machine learning techniques and data analysis. Recent\nstudies demonstrate that deep learning algorithms can identify behavioral\npatterns through biometric data analysis, video-based interaction assessments,\nand linguistic feature extraction, providing a more accurate and timely\ndiagnosis compared to traditional methods. Additionally, AI automates\ndiagnostic tools, reducing subjective biases and enabling the development of\npersonalized assessment protocols for ASD monitoring. At the same time, the\npaper examines AI-powered intervention technologies, emphasizing educational\nrobots and adaptive communication tools. Social robotic assistants, such as NAO\nand Kaspar, have been shown to enhance social skills in children by offering\nstructured, repetitive interactions that reinforce learning. Furthermore,\nAI-driven Augmentative and Alternative Communication (AAC) systems allow\nchildren with ASD to express themselves more effectively, while\nmachine-learning chatbots provide language development support through\npersonalized responses. The study presents research findings supporting the\neffectiveness of these AI applications while addressing challenges such as\nlong-term evaluation and customization to individual needs. In conclusion, the\npaper highlights the significance of AI as an innovative tool in ASD diagnosis\nand intervention, advocating for further research to assess its long-term\nimpact.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u8bca\u65ad\u3001\u8bc4\u4f30\u548c\u5e72\u9884\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86AI\u5728\u65e9\u671f\u8bca\u65ad\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u5e72\u9884\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "motivation": "ASD\u7684\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0cAI\u6280\u672f\u56e0\u5176\u9ad8\u6548\u548c\u7cbe\u51c6\u7684\u7279\u6027\uff0c\u53ef\u80fd\u4e3aASD\u7684\u8bca\u65ad\u548c\u5e72\u9884\u5e26\u6765\u9769\u65b0\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u751f\u7269\u7279\u5f81\u6570\u636e\u3001\u89c6\u9891\u4ea4\u4e92\u8bc4\u4f30\u548c\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u6559\u80b2\u673a\u5668\u4eba\uff08\u5982NAO\u548cKaspar\uff09\u548c\u81ea\u9002\u5e94\u901a\u4fe1\u5de5\u5177\uff08\u5982AI\u9a71\u52a8\u7684AAC\u7cfb\u7edf\u548c\u673a\u5668\u5b66\u4e60\u804a\u5929\u673a\u5668\u4eba\uff09\u5728ASD\u5e72\u9884\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAI\u6280\u672f\u5728ASD\u65e9\u671f\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u4e3b\u89c2\u504f\u5dee\uff1b\u5e72\u9884\u5de5\u5177\uff08\u5982\u793e\u4ea4\u673a\u5668\u4eba\u548cAAC\u7cfb\u7edf\uff09\u6709\u6548\u63d0\u5347\u4e86\u60a3\u513f\u7684\u793e\u4ea4\u80fd\u529b\u548c\u8bed\u8a00\u8868\u8fbe\u3002", "conclusion": "AI\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u5de5\u5177\uff0c\u5728ASD\u9886\u57df\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u957f\u671f\u6548\u679c\u548c\u4e2a\u6027\u5316\u5b9a\u5236\u95ee\u9898\u3002"}}
{"id": "2505.02615", "pdf": "https://arxiv.org/pdf/2505.02615", "abs": "https://arxiv.org/abs/2505.02615", "authors": ["Armita Mohammadi", "Alessandro Lameiras Koerich", "Laureano Moro-Velazquez", "Patrick Cardinal"], "title": "Automatic Proficiency Assessment in L2 English Learners", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "6 pages", "summary": "Second language proficiency (L2) in English is usually perceptually evaluated\nby English teachers or expert evaluators, with the inherent intra- and\ninter-rater variability. This paper explores deep learning techniques for\ncomprehensive L2 proficiency assessment, addressing both the speech signal and\nits correspondent transcription. We analyze spoken proficiency classification\nprediction using diverse architectures, including 2D CNN, frequency-based CNN,\nResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based\nproficiency assessment by fine-tuning a BERT language model within resource\nconstraints. Finally, we tackle the complex task of spontaneous dialogue\nassessment, managing long-form audio and speaker interactions through separate\napplications of wav2vec 2.0 and BERT models. Results from experiments on\nEFCamDat and ANGLISH datasets and a private dataset highlight the potential of\ndeep learning, especially the pretrained wav2vec 2.0 model, for robust\nautomated L2 proficiency evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982CNN\u3001ResNet\u3001wav2vec 2.0\u548cBERT\uff09\u81ea\u52a8\u8bc4\u4f30\u82f1\u8bed\u4f5c\u4e3a\u7b2c\u4e8c\u8bed\u8a00\uff08L2\uff09\u719f\u7ec3\u5ea6\u7684\u53ef\u884c\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u8bed\u97f3\u548c\u6587\u672c\u5206\u6790\u3002", "motivation": "\u4f20\u7edfL2\u719f\u7ec3\u5ea6\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u6559\u5e08\u6216\u4e13\u5bb6\u8bc4\u5206\uff0c\u5b58\u5728\u8bc4\u5206\u8005\u5185\u90e8\u548c\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u66f4\u5ba2\u89c2\u3001\u4e00\u81f4\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "method": "\u7ed3\u5408\u8bed\u97f3\u4fe1\u53f7\uff08\u4f7f\u75282D CNN\u3001\u9891\u7387CNN\u3001ResNet\u548cwav2vec 2.0\uff09\u548c\u6587\u672c\u8f6c\u5f55\uff08\u901a\u8fc7\u5fae\u8c03BERT\u6a21\u578b\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5904\u7406\u81ea\u53d1\u5bf9\u8bdd\u7684\u957f\u97f3\u9891\u548c\u8bf4\u8bdd\u8005\u4ea4\u4e92\u95ee\u9898\u3002", "result": "\u5728EFCamDat\u3001ANGLISH\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u7684wav2vec 2.0\u6a21\u578b\u5728\u81ea\u52a8\u5316L2\u8bc4\u4f30\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\uff0c\u5c24\u5176\u662fwav2vec 2.0\u6a21\u578b\uff0c\u4e3aL2\u719f\u7ec3\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01948", "pdf": "https://arxiv.org/pdf/2505.01948", "abs": "https://arxiv.org/abs/2505.01948", "authors": ["Yingda Fan", "Runlong Yu", "Janet R. Barclay", "Alison P. Appling", "Yiming Sun", "Yiqun Xie", "Xiaowei Jia"], "title": "Multi-Scale Graph Learning for Anti-Sparse Downscaling", "categories": ["cs.LG", "cs.AI", "68T05, 68U05", "I.2.6; I.2.10"], "comment": "AAAI-25, Multi-scale deep learning approach for spatial downscaling\n  of geospatial data with sparse observations", "summary": "Water temperature can vary substantially even across short distances within\nthe same sub-watershed. Accurate prediction of stream water temperature at fine\nspatial resolutions (i.e., fine scales, $\\leq$ 1 km) enables precise\ninterventions to maintain water quality and protect aquatic habitats. Although\nspatiotemporal models have made substantial progress in spatially coarse time\nseries modeling, challenges persist in predicting at fine spatial scales due to\nthe lack of data at that scale.To address the problem of insufficient\nfine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This\nmethod employs a multi-task learning framework where coarse-scale graph\nlearning, bolstered by larger datasets, simultaneously enhances fine-scale\ngraph learning. Although existing multi-scale or multi-resolution methods\nintegrate data from different spatial scales, they often overlook the spatial\ncorrespondences across graph structures at various scales. To address this, our\nMSGL introduces an additional learning task, cross-scale interpolation\nlearning, which leverages the hydrological connectedness of stream locations\nacross coarse- and fine-scale graphs to establish cross-scale connections,\nthereby enhancing overall model performance. Furthermore, we have broken free\nfrom the mindset that multi-scale learning is limited to synchronous training\nby proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL).\nExtensive experiments demonstrate the state-of-the-art performance of our\nmethod for anti-sparse downscaling of daily stream temperatures in the Delaware\nRiver Basin, USA, highlighting its potential utility for water resources\nmonitoring and management.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u56fe\u5b66\u4e60\u65b9\u6cd5\uff08MSGL\uff09\uff0c\u901a\u8fc7\u8de8\u5c3a\u5ea6\u63d2\u503c\u5b66\u4e60\u548c\u5f02\u6b65\u8bad\u7ec3\u63d0\u5347\u7ec6\u5c3a\u5ea6\u6c34\u6e29\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u7ec6\u5c3a\u5ea6\u6c34\u6e29\u6570\u636e\u4e0d\u8db3\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u5c0f\u5c3a\u5ea6\uff08\u22641 km\uff09\u4e0a\u51c6\u786e\u9884\u6d4b\u6cb3\u6d41\u6c34\u6e29\uff0c\u5f71\u54cd\u6c34\u8d28\u548c\u6c34\u751f\u6816\u606f\u5730\u4fdd\u62a4\u3002", "method": "MSGL\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u7c97\u5c3a\u5ea6\u56fe\u5b66\u4e60\u548c\u8de8\u5c3a\u5ea6\u63d2\u503c\u5b66\u4e60\uff1b\u8fd8\u63d0\u51fa\u5f02\u6b65\u591a\u5c3a\u5ea6\u56fe\u5b66\u4e60\uff08ASYNC-MSGL\uff09\u4ee5\u89e3\u51b3\u540c\u6b65\u8bad\u7ec3\u9650\u5236\u3002", "result": "\u5728\u7279\u62c9\u534e\u6cb3\u6d41\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u6570\u636e\u4e0b\u7684\u6c34\u6e29\u964d\u5c3a\u5ea6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MSGL\u901a\u8fc7\u8de8\u5c3a\u5ea6\u8fde\u63a5\u548c\u5f02\u6b65\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u5c3a\u5ea6\u6c34\u6e29\u9884\u6d4b\u6027\u80fd\uff0c\u5bf9\u6c34\u8d44\u6e90\u7ba1\u7406\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.02766", "pdf": "https://arxiv.org/pdf/2505.02766", "abs": "https://arxiv.org/abs/2505.02766", "authors": ["Nam H. Le", "Patrick Erikson", "Yanbo Zhang", "Michael Levin", "Josh Bongard"], "title": "Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control", "categories": ["cs.AI", "cs.NE", "cs.RO", "q-bio.TO"], "comment": "Accepted to GECCO Workshop on Bio-Inspired AI (ACM GECCO2025). 13\n  pages, 7 figures", "summary": "Guiding biological systems toward desired states, such as morphogenetic\noutcomes, remains a fundamental challenge with far-reaching implications for\nmedicine and synthetic biology. While large language models (LLMs) have enabled\nnatural language as an interface for interpretable control in AI systems, their\nuse as mediators for steering biological or cellular dynamics remains largely\nunexplored.\n  In this work, we present a functional pipeline that translates natural\nlanguage prompts into spatial vector fields capable of directing simulated\ncellular collectives. Our approach combines a large language model with an\nevolvable neural controller (Prompt-to-Intervention, or P2I), optimized via\nevolutionary strategies to generate behaviors such as clustering or scattering\nin a simulated 2D environment.\n  We demonstrate that even with constrained vocabulary and simplified cell\nmodels, evolved P2I networks can successfully align cellular dynamics with\nuser-defined goals expressed in plain language. This work offers a complete\nloop from language input to simulated bioelectric-like intervention to\nbehavioral output, providing a foundation for future systems capable of natural\nlanguage-driven cellular control.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u53ef\u8fdb\u5316\u795e\u7ecf\u63a7\u5236\u5668\uff08P2I\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u7a7a\u95f4\u77e2\u91cf\u573a\uff0c\u4ee5\u5f15\u5bfc\u6a21\u62df\u7ec6\u80de\u96c6\u4f53\u7684\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5982\u4f55\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u5bf9\u751f\u7269\u7cfb\u7edf\uff08\u5982\u7ec6\u80de\u52a8\u6001\uff09\u7684\u53ef\u63a7\u5f15\u5bfc\uff0c\u8fd9\u5728\u533b\u5b66\u548c\u5408\u6210\u751f\u7269\u5b66\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86LLM\u548c\u53ef\u8fdb\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff08P2I\uff09\uff0c\u901a\u8fc7\u8fdb\u5316\u7b56\u7565\u4f18\u5316\u751f\u6210\u80fd\u591f\u5f15\u5bfc\u7ec6\u80de\u96c6\u4f53\u884c\u4e3a\uff08\u5982\u805a\u96c6\u6216\u5206\u6563\uff09\u7684\u7a7a\u95f4\u77e2\u91cf\u573a\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u53d7\u9650\u8bcd\u6c47\u548c\u7b80\u5316\u7ec6\u80de\u6a21\u578b\uff0cP2I\u7f51\u7edc\u4ecd\u80fd\u6210\u529f\u5c06\u7ec6\u80de\u52a8\u6001\u4e0e\u7528\u6237\u5b9a\u4e49\u7684\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u5bf9\u9f50\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u7ec6\u80de\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4ece\u8bed\u8a00\u8f93\u5165\u5230\u6a21\u62df\u751f\u7269\u7535\u5e72\u9884\u518d\u5230\u884c\u4e3a\u8f93\u51fa\u7684\u5b8c\u6574\u5faa\u73af\u3002"}}
{"id": "2505.02625", "pdf": "https://arxiv.org/pdf/2505.02625", "abs": "https://arxiv.org/abs/2505.02625", "authors": ["Qingkai Fang", "Yan Zhou", "Shoutao Guo", "Shaolei Zhang", "Yang Feng"], "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2", "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data.", "AI": {"tldr": "LLaMA-Omni 2\u662f\u4e00\u7cfb\u5217\u57fa\u4e8eQwen2.5\u6a21\u578b\u6784\u5efa\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4ece0.5B\u523014B\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92\uff0c\u5e76\u5728\u5c11\u91cf\u6570\u636e\u8bad\u7ec3\u4e0b\u8d85\u8d8a\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u3002", "motivation": "\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u4eba\u673a\u4ea4\u4e92\u4e2d\u5b9e\u65f6\u3001\u667a\u80fd\u3001\u81ea\u7136\u7684\u8bed\u97f3\u4ea4\u4e92\u662f\u6838\u5fc3\u76ee\u6807\uff0c\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u667a\u80fd\u8bed\u97f3\u804a\u5929\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u57fa\u4e8eQwen2.5\u7cfb\u5217\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u8bed\u97f3\u7f16\u7801\u5668\u548c\u81ea\u56de\u5f52\u6d41\u5f0f\u8bed\u97f3\u89e3\u7801\u5668\uff0c\u4ec5\u752820\u4e07\u8f6e\u591a\u8f6e\u8bed\u97f3\u5bf9\u8bdd\u6837\u672c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u97f3\u95ee\u7b54\u548c\u8bed\u97f3\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6570\u767e\u4e07\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u8bad\u7ec3\u7684GLM-4-Voice\u7b49\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u3002", "conclusion": "LLaMA-Omni 2\u5c55\u793a\u4e86\u5728\u5c11\u91cf\u6570\u636e\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u8bed\u97f3\u4ea4\u4e92\u7684\u6f5c\u529b\uff0c\u4e3a\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.01954", "pdf": "https://arxiv.org/pdf/2505.01954", "abs": "https://arxiv.org/abs/2505.01954", "authors": ["Kareem Ahmed", "Catarina G Belem", "Padhraic Smyth", "Sameer Singh"], "title": "Semantic Probabilistic Control of Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Semantic control entails steering LM generations towards satisfying subtle\nnon-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes\nthat can be captured by a sequence-level verifier. It can thus be viewed as\nsampling from the LM distribution conditioned on the target attribute, a\ncomputationally intractable problem due to the non-decomposable nature of the\nverifier. Existing approaches to LM control either only deal with syntactic\nconstraints which cannot capture the aforementioned attributes, or rely on\nsampling to explore the conditional LM distribution, an ineffective estimator\nfor low-probability events. In this work, we leverage a verifier's gradient\ninformation to efficiently reason over all generations that satisfy the target\nattribute, enabling precise steering of LM generations by reweighing the\nnext-token distribution. Starting from an initial sample, we create a local LM\ndistribution favoring semantically similar sentences. This approximation\nenables the tractable computation of an expected sentence embedding. We use\nthis expected embedding, informed by the verifier's evaluation at the initial\nsample, to estimate the probability of satisfying the constraint, which\ndirectly informs the update to the next-token distribution. We evaluated the\neffectiveness of our approach in controlling the toxicity, sentiment, and\ntopic-adherence of LMs yielding generations satisfying the constraint with high\nprobability (>95%) without degrading their quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u9a8c\u8bc1\u5668\u68af\u5ea6\u4fe1\u606f\u9ad8\u6548\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u80fd\u5728\u4e0d\u964d\u4f4e\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u6ee1\u8db3\u975e\u8bcd\u6c47\u7ea6\u675f\uff08\u5982\u6bd2\u6027\u3001\u60c5\u611f\u7b49\uff09\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u8bed\u8a00\u6a21\u578b\u7684\u975e\u8bcd\u6c47\u7ea6\u675f\uff08\u5982\u6bd2\u6027\u3001\u60c5\u611f\uff09\uff0c\u9700\u8981\u901a\u8fc7\u91c7\u6837\u63a2\u7d22\u6761\u4ef6\u5206\u5e03\uff0c\u4f46\u5bf9\u4f4e\u6982\u7387\u4e8b\u4ef6\u4f30\u8ba1\u6548\u679c\u5dee\u3002", "method": "\u5229\u7528\u9a8c\u8bc1\u5668\u7684\u68af\u5ea6\u4fe1\u606f\u91cd\u6784\u4e0b\u4e00\u4e2a\u8bcd\u5206\u5e03\uff0c\u901a\u8fc7\u5c40\u90e8\u8bed\u8a00\u6a21\u578b\u5206\u5e03\u8fd1\u4f3c\u8ba1\u7b97\u671f\u671b\u53e5\u5b50\u5d4c\u5165\uff0c\u4ece\u800c\u9ad8\u6548\u6ee1\u8db3\u76ee\u6807\u7ea6\u675f\u3002", "result": "\u5728\u6bd2\u6027\u3001\u60c5\u611f\u548c\u4e3b\u9898\u4e00\u81f4\u6027\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u7ed3\u679c\u6ee1\u8db3\u7ea6\u675f\u7684\u6982\u7387\u8d85\u8fc795%\uff0c\u4e14\u672a\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u6709\u6548\u5f15\u5bfc\u8bed\u8a00\u6a21\u578b\u751f\u6210\uff0c\u4e3a\u6ee1\u8db3\u590d\u6742\u8bed\u4e49\u7ea6\u675f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02781", "pdf": "https://arxiv.org/pdf/2505.02781", "abs": "https://arxiv.org/abs/2505.02781", "authors": ["Timoth\u00e9e Loranchet", "Charles K. Assaad"], "title": "Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects", "categories": ["cs.AI"], "comment": null, "summary": "Understanding and identifying controlled direct effects (CDEs) is crucial\nacross numerous scientific domains, including public health. While existing\nmethods can identify these effects from causal directed acyclic graphs (DAGs),\nthe true underlying structure is often unknown in practice. Essential graphs,\nwhich represent a Markov equivalence class of DAGs characterized by the same\nset of d-separations, provide a more practical and realistic alternative.\nHowever, learning the full essential graph is computationally intensive and\ntypically depends on strong, untestable assumptions. In this work, we\ncharacterize a local class of graphs, defined relative to a target variable,\nthat share a specific subset of d-separations, and introduce a graphical\nrepresentation of this class, called the local essential graph (LEG). We then\npresent LocPC, a novel algorithm designed to recover the LEG from an observed\ndistribution using only local conditional independence tests. Building on\nLocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG\nthat is sufficient to identify a CDE, bypassing the need of retrieving the full\nessential graph. Compared to global methods, our algorithms require less\nconditional independence tests and operate under weaker assumptions while\nmaintaining theoretical guarantees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLocPC-CDE\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u6765\u8bc6\u522b\u53d7\u63a7\u76f4\u63a5\u6548\u5e94\uff08CDEs\uff09\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5b66\u4e60\u5b8c\u6574\u57fa\u672c\u56fe\u7684\u7f3a\u70b9\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5047\u8bbe\u8981\u6c42\u3002", "motivation": "\u5728\u516c\u5171\u5065\u5eb7\u7b49\u9886\u57df\uff0c\u8bc6\u522b\u53d7\u63a7\u76f4\u63a5\u6548\u5e94\uff08CDEs\uff09\u975e\u5e38\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u56e0\u679c\u6709\u5411\u65e0\u73af\u56fe\uff08DAGs\uff09\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u57fa\u672c\u56fe\u5b66\u4e60\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5047\u8bbe\u8fc7\u5f3a\u4e14\u96be\u4ee5\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u5f15\u5165\u5c40\u90e8\u57fa\u672c\u56fe\uff08LEG\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u63d0\u51faLocPC\u7b97\u6cd5\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u6062\u590dLEG\u3002\u63a5\u7740\uff0c\u57fa\u4e8eLocPC\u8fdb\u4e00\u6b65\u63d0\u51faLocPC-CDE\u7b97\u6cd5\uff0c\u4ec5\u9700\u5c40\u90e8\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u5373\u53ef\u8bc6\u522bCDEs\u3002", "result": "\u76f8\u6bd4\u5168\u5c40\u65b9\u6cd5\uff0cLocPC-CDE\u9700\u8981\u66f4\u5c11\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u4e14\u80fd\u5728\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\u8fd0\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc6\u522b\u53d7\u63a7\u76f4\u63a5\u6548\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u5047\u8bbe\u8d1f\u62c5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.02656", "pdf": "https://arxiv.org/pdf/2505.02656", "abs": "https://arxiv.org/abs/2505.02656", "authors": ["Rawan Bondok", "Mayar Nassar", "Salam Khalifa", "Kurt Micallaf", "Nizar Habash"], "title": "Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset", "categories": ["cs.CL"], "comment": null, "summary": "Proper names in Arabic Wikipedia are frequently undiacritized, creating\nambiguity in pronunciation and interpretation, especially for transliterated\nnamed entities of foreign origin. While transliteration and diacritization have\nbeen well-studied separately in Arabic NLP,their intersection remains\nunderexplored. In this paper, we introduce a new manually diacritized dataset\nof Arabic proper names of various origins with their English Wikipedia\nequivalent glosses, and present the challenges and guidelines we followed to\ncreate it. We benchmark GPT-4o on the task of recovering full diacritization\ngiven the undiacritized Arabic and English forms, and analyze its performance.\nAchieving 73% accuracy, our results underscore both the difficulty of the task\nand the need for improved models and resources. We release our dataset to\nfacilitate further research on Arabic Wikipedia proper name diacritization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7ef4\u57fa\u767e\u79d1\u4e2d\u672a\u6807\u6ce8\u53d8\u97f3\u7b26\u53f7\u7684\u4e13\u6709\u540d\u8bcd\uff08\u5c24\u5176\u662f\u5916\u6765\u8bcd\uff09\u5bfc\u81f4\u7684\u53d1\u97f3\u548c\u89e3\u91ca\u6b67\u4e49\u95ee\u9898\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u53d8\u97f3\u7b26\u53f7\u7684\u6570\u636e\u96c6\uff0c\u5e76\u7528GPT-4o\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u4e3a73%\u51c6\u786e\u7387\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u7ef4\u57fa\u767e\u79d1\u4e2d\u7684\u4e13\u6709\u540d\u8bcd\u5e38\u672a\u6807\u6ce8\u53d8\u97f3\u7b26\u53f7\uff0c\u5bfc\u81f4\u53d1\u97f3\u548c\u89e3\u91ca\u6b67\u4e49\uff0c\u5c24\u5176\u662f\u5916\u6765\u8bcd\u7684\u97f3\u8bd1\u95ee\u9898\u3002\u867d\u7136\u97f3\u8bd1\u548c\u53d8\u97f3\u6807\u6ce8\u5728\u963f\u62c9\u4f2f\u8bedNLP\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u4e24\u8005\u7684\u7ed3\u5408\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u624b\u52a8\u6807\u6ce8\u53d8\u97f3\u7b26\u53f7\u7684\u963f\u62c9\u4f2f\u8bed\u4e13\u6709\u540d\u8bcd\u6570\u636e\u96c6\uff08\u542b\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u5bf9\u5e94\u89e3\u91ca\uff09\uff0c\u5e76\u63d0\u51fa\u6311\u6218\u4e0e\u6807\u6ce8\u6307\u5357\u3002\u4ee5GPT-4o\u4e3a\u57fa\u51c6\u6a21\u578b\uff0c\u6d4b\u8bd5\u5176\u4ece\u65e0\u6807\u6ce8\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u5f62\u5f0f\u6062\u590d\u5b8c\u6574\u53d8\u97f3\u6807\u6ce8\u7684\u80fd\u529b\u3002", "result": "GPT-4o\u7684\u6062\u590d\u53d8\u97f3\u6807\u6ce8\u4efb\u52a1\u51c6\u786e\u7387\u4e3a73%\uff0c\u8868\u660e\u4efb\u52a1\u96be\u5ea6\u8f83\u9ad8\uff0c\u4e14\u9700\u8981\u66f4\u597d\u7684\u6a21\u578b\u548c\u8d44\u6e90\u652f\u6301\u3002", "conclusion": "\u8bba\u6587\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u963f\u62c9\u4f2f\u8bed\u7ef4\u57fa\u767e\u79d1\u4e13\u6709\u540d\u8bcd\u53d8\u97f3\u6807\u6ce8\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u5e76\u5f3a\u8c03\u5f53\u524d\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.01959", "pdf": "https://arxiv.org/pdf/2505.01959", "abs": "https://arxiv.org/abs/2505.01959", "authors": ["Leyi Yan", "Linda Wang", "Sihang Liu", "Yi Ding"], "title": "EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting", "categories": ["cs.LG"], "comment": "5 pages, 5 figures, 3 tables, In The 15th ACM International\n  Conference on Future and Sustainable Energy Systems (E-ENERGY'25)", "summary": "Carbon intensity (CI) measures the average carbon emissions generated per\nunit of electricity, making it a crucial metric for quantifying and managing\nthe environmental impact. Accurate CI predictions are vital for minimizing\ncarbon footprints, yet the state-of-the-art method (CarbonCast) falls short due\nto its inability to address regional variability and lack of adaptability. To\naddress these limitations, we introduce EnsembleCI, an adaptive, end-to-end\nensemble learning-based approach for CI forecasting. EnsembleCI combines\nweighted predictions from multiple sublearners, offering enhanced flexibility\nand regional adaptability. In evaluations across 11 regional grids, EnsembleCI\nconsistently surpasses CarbonCast, achieving the lowest mean absolute\npercentage error (MAPE) in almost all grids and improving prediction accuracy\nby an average of 19.58%. While performance still varies across grids due to\ninherent regional diversity, EnsembleCI reduces variability and exhibits\ngreater robustness in long-term forecasting compared to CarbonCast and\nidentifies region-specific key features, underscoring its interpretability and\npractical relevance. These findings position EnsembleCI as a more accurate and\nreliable solution for CI forecasting. EnsembleCI source code and data used in\nthis paper are available at https://github.com/emmayly/EnsembleCI.", "AI": {"tldr": "EnsembleCI\u662f\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u78b3\u5f3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5CarbonCast\uff0c\u5b83\u572811\u4e2a\u533a\u57df\u7535\u7f51\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5e73\u5747\u9884\u6d4b\u7cbe\u5ea6\u63d0\u534719.58%\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u533a\u57df\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u78b3\u5f3a\u5ea6\u9884\u6d4b\u65b9\u6cd5CarbonCast\u65e0\u6cd5\u6709\u6548\u5904\u7406\u533a\u57df\u5dee\u5f02\u548c\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faEnsembleCI\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u591a\u4e2a\u5b50\u5b66\u4e60\u5668\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u548c\u533a\u57df\u9002\u5e94\u6027\u3002", "result": "\u572811\u4e2a\u533a\u57df\u7535\u7f51\u7684\u8bc4\u4f30\u4e2d\uff0cEnsembleCI\u5728\u51e0\u4e4e\u6240\u6709\u7535\u7f51\u4e2d\u53d6\u5f97\u6700\u4f4e\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u534719.58%\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u597d\u7684\u957f\u671f\u9884\u6d4b\u9c81\u68d2\u6027\u3002", "conclusion": "EnsembleCI\u662f\u4e00\u79cd\u66f4\u51c6\u786e\u3001\u53ef\u9760\u7684\u78b3\u5f3a\u5ea6\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u533a\u57df\u7279\u5f02\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5176\u6e90\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.02811", "pdf": "https://arxiv.org/pdf/2505.02811", "abs": "https://arxiv.org/abs/2505.02811", "authors": ["Diji Yang", "Linda Zeng", "Jinmeng Rao", "Yi Zhang"], "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR 2025", "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.", "AI": {"tldr": "SIM-RAG\u6846\u67b6\u901a\u8fc7\u81ea\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6a21\u5757\u589e\u5f3aRAG\u7cfb\u7edf\u7684\u591a\u8f6e\u68c0\u7d22\u80fd\u529b\u548c\u81ea\u6211\u8ba4\u77e5\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u591a\u8f6e\u68c0\u7d22\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u5bfc\u81f4\u4e0d\u5fc5\u8981\u68c0\u7d22\u6216\u9519\u8bef\u56de\u7b54\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u6210\u672c\u53ef\u63a7\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7RAG\u7cfb\u7edf\u81ea\u6211\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6a21\u5757\uff08Critic\uff09\u4ee5\u5224\u65ad\u4fe1\u606f\u662f\u5426\u5145\u5206\u3002", "result": "\u5728\u591a\u8f6e\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5bf9\u73b0\u6709\u7cfb\u7edf\u6539\u52a8\u5c11\uff0c\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "SIM-RAG\u4e3aRAG\u7cfb\u7edf\u7684\u81ea\u6211\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02666", "pdf": "https://arxiv.org/pdf/2505.02666", "abs": "https://arxiv.org/abs/2505.02666", "authors": ["Miaomiao Ji", "Yanqiu Wu", "Zhibin Wu", "Shoujin Wang", "Jian Yang", "Mark Dras", "Usman Naseem"], "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design", "categories": ["cs.CL"], "comment": "Preprint", "summary": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies.", "AI": {"tldr": "The paper surveys reward mechanisms in aligning large language models (LLMs) with human values, proposing a three-phase framework and highlighting challenges and advances.", "motivation": "To address the alignment challenge of LLMs by systematically analyzing and improving reward mechanisms.", "method": "A theoretical framework categorizing reward mechanism development into feedback, reward design, and optimization phases, with a four-dimensional analysis.", "result": "Identifies evolutionary trends in reward modeling, noting shifts from reinforcement learning to new paradigms and improved handling of complex alignment scenarios.", "conclusion": "The survey outlines future directions for innovative reward design to enhance LLM alignment with human intentions."}}
{"id": "2505.01979", "pdf": "https://arxiv.org/pdf/2505.01979", "abs": "https://arxiv.org/abs/2505.01979", "authors": ["Chenran Zhao", "Dianxi Shi", "Mengzhu Wang", "Jianqiang Xia", "Huanhuan Yang", "Songchang Jin", "Shaowu Yang", "Chunping Qiu"], "title": "D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection", "categories": ["cs.LG"], "comment": null, "summary": "Current Hierarchical Reinforcement Learning (HRL) algorithms excel in\nlong-horizon sequential decision-making tasks but still face two challenges:\ndelay effects and spurious correlations. To address them, we propose a causal\nHRL approach called D3HRL. First, D3HRL models delayed effects as causal\nrelationships across different time spans and employs distributed causal\ndiscovery to learn these relationships. Second, it employs conditional\nindependence testing to eliminate spurious correlations. Finally, D3HRL\nconstructs and trains hierarchical policies based on the identified true causal\nrelationships. These three steps are iteratively executed, gradually exploring\nthe complete causal chain of the task. Experiments conducted in 2D-MineCraft\nand MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects\nand accurately identifies causal relationships, leading to reliable\ndecision-making in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u56e0\u679c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5D3HRL\uff0c\u901a\u8fc7\u5efa\u6a21\u5ef6\u8fdf\u6548\u5e94\u4e3a\u8de8\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u5ef6\u8fdf\u6548\u5e94\u548c\u865a\u5047\u76f8\u5173\u6027\u4e24\u5927\u6311\u6218\u3002", "method": "D3HRL\u901a\u8fc7\u5206\u5e03\u5f0f\u56e0\u679c\u53d1\u73b0\u5efa\u6a21\u5ef6\u8fdf\u6548\u5e94\uff0c\u57fa\u4e8e\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\uff0c\u5e76\u6784\u5efa\u5206\u5c42\u7b56\u7565\u3002", "result": "\u57282D-MineCraft\u548cMiniGrid\u5b9e\u9a8c\u4e2d\uff0cD3HRL\u8868\u73b0\u51fa\u5bf9\u5ef6\u8fdf\u6548\u5e94\u7684\u9ad8\u654f\u611f\u6027\u548c\u51c6\u786e\u8bc6\u522b\u56e0\u679c\u5173\u7cfb\u7684\u80fd\u529b\u3002", "conclusion": "D3HRL\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u9760\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u56e0\u679c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2505.02820", "pdf": "https://arxiv.org/pdf/2505.02820", "abs": "https://arxiv.org/abs/2505.02820", "authors": ["Hao Zhu", "Phil Cuvin", "Xinkai Yu", "Charlotte Ka Yee Yan", "Jason Zhang", "Diyi Yang"], "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "https://opensocial.world/", "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents.", "AI": {"tldr": "AutoLibra\u6846\u67b6\u901a\u8fc7\u5c06\u5f00\u653e\u5f0f\u4eba\u7c7b\u53cd\u9988\u8f6c\u5316\u4e3a\u7ec6\u7c92\u5ea6\u884c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u4f18\u5316\u8bed\u8a00\u4ee3\u7406\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7406\u6027\u80fd20%\u5e76\u53d1\u73b0\u65b0\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u4ee3\u7406\u8bc4\u4f30\u4f9d\u8d56\u4efb\u52a1\u6210\u529f\u6307\u6807\uff0c\u4f46\u8fd9\u7c7b\u6307\u6807\u7c97\u7cd9\u3001\u4f9d\u8d56\u4e13\u5bb6\u8bbe\u8ba1\u4e14\u65e0\u6cd5\u5956\u52b1\u4e2d\u95f4\u884c\u4e3a\u3002AutoLibra\u65e8\u5728\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u521b\u5efa\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "AutoLibra\u5c06\u4eba\u7c7b\u53cd\u9988\u951a\u5b9a\u5230\u4ee3\u7406\u884c\u4e3a\uff0c\u805a\u7c7b\u6b63\u8d1f\u9762\u884c\u4e3a\uff0c\u751f\u6210\u5177\u4f53\u6307\u6807\uff0c\u5e76\u4f18\u5316\u8986\u76d6\u7387\u548c\u5197\u4f59\u5ea6\u4e24\u4e2a\u5143\u6307\u6807\u4ee5\u8bc4\u4f30\u6307\u6807\u5bf9\u9f50\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAutoLibra\u80fd\u751f\u6210\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u4f53\u7684\u6307\u6807\uff0c\u5e76\u5728\u6587\u672c\u6e38\u620f\u4efb\u52a1\u4e2d\u63d0\u5347\u4ee3\u7406\u6027\u80fd20%\uff0c\u8fd8\u80fd\u8fed\u4ee3\u7b5b\u9009\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\u3002", "conclusion": "AutoLibra\u662f\u8bc4\u4f30\u548c\u6539\u8fdb\u8bed\u8a00\u4ee3\u7406\u7684\u5f3a\u5927\u4efb\u52a1\u65e0\u5173\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u53d1\u73b0\u65b0\u6307\u6807\u3002"}}
{"id": "2505.02686", "pdf": "https://arxiv.org/pdf/2505.02686", "abs": "https://arxiv.org/abs/2505.02686", "authors": ["Xiaobao Wu"], "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models", "categories": ["cs.CL"], "comment": "35 Pages", "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u201c\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u201d\u7684\u8303\u5f0f\uff0c\u5305\u62ec\u5176\u5728\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u540e\u63a8\u7406\u9636\u6bb5\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u5bf9\u672a\u6765\u6311\u6218\u548c\u65b9\u5411\u7684\u8ba8\u8bba\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u7684\u8303\u5f0f\u6210\u4e3a\u8c03\u6574\u6a21\u578b\u884c\u4e3a\u548c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u3001\u5956\u52b1\u5f15\u5bfc\u7684\u89e3\u7801\u548c\u4e8b\u540e\u4fee\u6b63\u7b49\u65b9\u6cd5\uff0c\u6a21\u578b\u53ef\u4ee5\u52a8\u6001\u5730\u4ece\u53cd\u9988\u4e2d\u5b66\u4e60\u3002", "result": "\u8be5\u8303\u5f0f\u4f7fLLMs\u5177\u5907\u4e86\u66f4\u597d\u7684\u504f\u597d\u5bf9\u8c03\u548c\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u591a\u4e2a\u9636\u6bb5\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u201c\u4ece\u5956\u52b1\u4e2d\u5b66\u4e60\u201d\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u73b0\u6709\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63a8\u52a8LLMs\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.01996", "pdf": "https://arxiv.org/pdf/2505.01996", "abs": "https://arxiv.org/abs/2505.01996", "authors": ["Yiping Ji", "Hemanth Saratchandran", "Peyman Moghaddam", "Simon Lucey"], "title": "Always Skip Attention", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We highlight a curious empirical result within modern Vision Transformers\n(ViTs). Specifically, self-attention catastrophically fails to train unless it\nis used in conjunction with a skip connection. This is in contrast to other\nelements of a ViT that continue to exhibit good performance (albeit suboptimal)\nwhen skip connections are removed. Further, we show that this critical\ndependence on skip connections is a relatively new phenomenon, with previous\ndeep architectures (\\eg, CNNs) exhibiting good performance in their absence. In\nthis paper, we theoretically characterize that the self-attention mechanism is\nfundamentally ill-conditioned and is, therefore, uniquely dependent on skip\nconnections for regularization. Additionally, we propose Token Graying -- a\nsimple yet effective complement (to skip connections) that further improves the\ncondition of input tokens. We validate our approach in both supervised and\nself-supervised training methods.", "AI": {"tldr": "\u73b0\u4ee3\u89c6\u89c9Transformer\uff08ViT\uff09\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u82e5\u6ca1\u6709\u8df3\u8dc3\u8fde\u63a5\u4f1a\u8bad\u7ec3\u5931\u8d25\uff0c\u800c\u5176\u4ed6\u90e8\u5206\u4ecd\u80fd\u5de5\u4f5c\u3002\u672c\u6587\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u672c\u8d28\u4e0a\u662f\u6761\u4ef6\u4e0d\u826f\u7684\uff0c\u4f9d\u8d56\u8df3\u8dc3\u8fde\u63a5\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u5e76\u63d0\u51faToken Graying\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6539\u5584\u8f93\u5165\u6761\u4ef6\u3002", "motivation": "\u63a2\u7a76ViT\u4e2d\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u8df3\u8dc3\u8fde\u63a5\u7684\u72ec\u7279\u4f9d\u8d56\u73b0\u8c61\uff0c\u5e76\u89e3\u91ca\u5176\u7406\u8bba\u539f\u56e0\uff0c\u4ece\u800c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6761\u4ef6\u4e0d\u826f\u7279\u6027\uff0c\u5e76\u63d0\u51faToken Graying\u4f5c\u4e3a\u8865\u5145\u65b9\u6cd5\u3002\u5728\u76d1\u7763\u548c\u81ea\u76d1\u7763\u8bad\u7ec3\u4e2d\u9a8c\u8bc1\u3002", "result": "Token Graying\u6709\u6548\u6539\u5584\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6761\u4ef6\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u8df3\u8dc3\u8fde\u63a5\u7684\u4f9d\u8d56\u662f\u6839\u672c\u6027\u7684\uff0c\u800cToken Graying\u4e3aViT\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u6b63\u5219\u5316\u624b\u6bb5\u3002"}}
{"id": "2505.02828", "pdf": "https://arxiv.org/pdf/2505.02828", "abs": "https://arxiv.org/abs/2505.02828", "authors": ["Sonal Allana", "Mohan Kankanhalli", "Rozita Dara"], "title": "Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review", "categories": ["cs.AI", "cs.CR", "cs.ET"], "comment": "Submitted for peer review", "summary": "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8fdb\u884c\u4e86\u4e00\u9879\u8303\u56f4\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u4e0e\u9690\u79c1\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u603b\u7ed3\u4e86\u9690\u79c1\u98ce\u9669\u3001\u4fdd\u62a4\u65b9\u6cd5\u53ca\u9690\u79c1\u4fdd\u62a4\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u5e73\u8861\u9690\u79c1\u4e0e\u5176\u4ed6\u7cfb\u7edf\u9700\u6c42\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740XAI\u6210\u4e3a\u53ef\u4fe1AI\u7684\u652f\u67f1\uff0c\u5982\u4f55\u5728\u63d0\u4f9b\u900f\u660e\u6027\u7684\u540c\u65f6\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u6210\u4e3a\u8feb\u5207\u9700\u8981\u7814\u7a76\u7684\u8bfe\u9898\u3002", "method": "\u91c7\u7528\u6807\u51c6\u8303\u56f4\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece1943\u7bc7\u7814\u7a76\u4e2d\u7b5b\u9009\u51fa57\u7bc7\uff0c\u89e3\u7b54\u4e86\u4e09\u4e2a\u7814\u7a76\u95ee\u9898\uff1aXAI\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u3001\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u53ca\u9690\u79c1\u4fdd\u62a4\u89e3\u91ca\u7684\u7279\u5f81\u3002", "result": "\u8bba\u6587\u5206\u7c7b\u4e86XAI\u4e2d\u7684\u9690\u79c1\u98ce\u9669\u548c\u4fdd\u62a4\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u9690\u79c1\u4fdd\u62a4\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5e73\u8861\u9690\u79c1\u4e0e\u5176\u4ed6\u9700\u6c42\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u63ed\u793a\u4e86\u9690\u79c1\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u9690\u79c1\u5408\u89c4XAI\u7684\u6307\u5bfc\u3002"}}
{"id": "2505.02692", "pdf": "https://arxiv.org/pdf/2505.02692", "abs": "https://arxiv.org/abs/2505.02692", "authors": ["Maxime Poli", "Emmanuel Chemla", "Emmanuel Dupoux"], "title": "fastabx: A library for efficient computation of ABX discriminability", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "8 pages, 6 figures", "summary": "We introduce fastabx, a high-performance Python library for building ABX\ndiscrimination tasks. ABX is a measure of the separation between generic\ncategories of interest. It has been used extensively to evaluate phonetic\ndiscriminability in self-supervised speech representations. However, its\nbroader adoption has been limited by the absence of adequate tools. fastabx\naddresses this gap by providing a framework capable of constructing any type of\nABX task while delivering the efficiency necessary for rapid development\ncycles, both in task creation and in calculating distances between\nrepresentations. We believe that fastabx will serve as a valuable resource for\nthe broader representation learning community, enabling researchers to\nsystematically investigate what information can be directly extracted from\nlearned representations across several domains beyond speech processing. The\nsource code is available at https://github.com/bootphon/fastabx.", "AI": {"tldr": "fastabx\u662f\u4e00\u4e2a\u9ad8\u6027\u80fdPython\u5e93\uff0c\u7528\u4e8e\u6784\u5efaABX\u5224\u522b\u4efb\u52a1\u3002ABX\u662f\u4e00\u79cd\u8861\u91cf\u7c7b\u522b\u95f4\u533a\u5206\u5ea6\u7684\u6307\u6807\uff0c\u5c24\u5176\u5728\u8bed\u97f3\u8868\u793a\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "ABX\u4efb\u52a1\u5728\u8bed\u97f3\u8868\u793a\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u53ef\u533a\u5206\u6027\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u7684\u5de5\u5177\u9650\u5236\u4e86\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002fastabx\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5feb\u901f\u5f00\u53d1\u548c\u8ba1\u7b97\u63d0\u4f9b\u9ad8\u6548\u6846\u67b6\u3002", "method": "fastabx\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u6846\u67b6\uff0c\u80fd\u591f\u6784\u5efa\u4efb\u610f\u7c7b\u578b\u7684ABX\u4efb\u52a1\uff0c\u5e76\u9ad8\u6548\u8ba1\u7b97\u8868\u793a\u95f4\u7684\u8ddd\u79bb\u3002", "result": "fastabx\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u6784\u5efa\u548c\u8ddd\u79bb\u8ba1\u7b97\uff0c\u4e3a\u5feb\u901f\u5f00\u53d1\u5468\u671f\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "fastabx\u5c06\u6210\u4e3a\u8868\u793a\u5b66\u4e60\u9886\u57df\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4ece\u5b66\u4e60\u8868\u793a\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4e0d\u4ec5\u9650\u4e8e\u8bed\u97f3\u5904\u7406\u9886\u57df\u3002\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2505.01997", "pdf": "https://arxiv.org/pdf/2505.01997", "abs": "https://arxiv.org/abs/2505.01997", "authors": ["Jiancong Xiao", "Bojian Hou", "Zhanliang Wang", "Ruochen Jin", "Qi Long", "Weijie J. Su", "Li Shen"], "title": "Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "One of the key technologies for the success of Large Language Models (LLMs)\nis preference alignment. However, a notable side effect of preference alignment\nis poor calibration: while the pre-trained models are typically\nwell-calibrated, LLMs tend to become poorly calibrated after alignment with\nhuman preferences. In this paper, we investigate why preference alignment\naffects calibration and how to address this issue. For the first question, we\nobserve that the preference collapse issue in alignment undesirably generalizes\nto the calibration scenario, causing LLMs to exhibit overconfidence and poor\ncalibration. To address this, we demonstrate the importance of fine-tuning with\ndomain-specific knowledge to alleviate the overconfidence issue. To further\nanalyze whether this affects the model's performance, we categorize models into\ntwo regimes: calibratable and non-calibratable, defined by bounds of Expected\nCalibration Error (ECE). In the calibratable regime, we propose a\ncalibration-aware fine-tuning approach to achieve proper calibration without\ncompromising LLMs' performance. However, as models are further fine-tuned for\nbetter performance, they enter the non-calibratable regime. For this case, we\ndevelop an EM-algorithm-based ECE regularization for the fine-tuning loss to\nmaintain low calibration error. Extensive experiments validate the\neffectiveness of the proposed methods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u504f\u597d\u5bf9\u9f50\u5bfc\u81f4\u7684\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u5fae\u8c03\u89e3\u51b3\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u6821\u51c6\u611f\u77e5\u5fae\u8c03\u548c\u57fa\u4e8eEM\u7b97\u6cd5\u7684ECE\u6b63\u5219\u5316\u65b9\u6cd5\u4ee5\u4fdd\u6301\u4f4e\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u504f\u597d\u5bf9\u9f50\u8ba9LLMs\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5bfc\u81f4\u6821\u51c6\u95ee\u9898\uff0c\u8868\u73b0\u4e3a\u8fc7\u5ea6\u81ea\u4fe1\u548c\u6821\u51c6\u4e0d\u4f73\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u5176\u539f\u56e0\u5e76\u627e\u5230\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5206\u6790\u504f\u597d\u5bf9\u9f50\u5f71\u54cd\u6821\u51c6\u7684\u539f\u56e0\uff1b2. \u63d0\u51fa\u9886\u57df\u77e5\u8bc6\u5fae\u8c03\u7f13\u89e3\u8fc7\u5ea6\u81ea\u4fe1\uff1b3. \u5212\u5206\u6a21\u578b\u4e3a\u53ef\u6821\u51c6/\u4e0d\u53ef\u6821\u51c6\u4e24\u7c7b\uff1b4. \u9488\u5bf9\u53ef\u6821\u51c6\u6a21\u578b\u63d0\u51fa\u6821\u51c6\u611f\u77e5\u5fae\u8c03\uff1b5. \u5bf9\u4e0d\u53ef\u6821\u51c6\u6a21\u578b\u4f7f\u7528\u57fa\u4e8eEM\u7b97\u6cd5\u7684ECE\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u9886\u57df\u77e5\u8bc6\u5fae\u8c03\u6709\u6548\u7f13\u89e3\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u6821\u51c6\u611f\u77e5\u548cECE\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u5404\u81ea\u9002\u7528\u8303\u56f4\u5185\u663e\u8457\u964d\u4f4e\u6821\u51c6\u8bef\u5dee\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u504f\u597d\u5bf9\u9f50\u4e0e\u6821\u51c6\u95ee\u9898\u7684\u5173\u7cfb\u88ab\u660e\u786e\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u6821\u51c6\u573a\u666f\u4e2d\u5747\u6709\u6548\u4e14\u5b9e\u7528\u3002"}}
{"id": "2505.02829", "pdf": "https://arxiv.org/pdf/2505.02829", "abs": "https://arxiv.org/abs/2505.02829", "authors": ["Jerome Quenum", "Wen-Han Hsieh", "Tsung-Han Wu", "Ritwik Gupta", "Trevor Darrell", "David M. Chan"], "title": "LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery", "categories": ["cs.AI"], "comment": "28 pages, 10 figures, 19 tables", "summary": "Segmentation models can recognize a pre-defined set of objects in images.\nHowever, models that can reason over complex user queries that implicitly refer\nto multiple objects of interest are still in their infancy. Recent advances in\nreasoning segmentation--generating segmentation masks from complex, implicit\nquery text--demonstrate that vision-language models can operate across an open\ndomain and produce reasonable outputs. However, our experiments show that such\nmodels struggle with complex remote-sensing imagery. In this work, we introduce\nLISAt, a vision-language model designed to describe complex remote-sensing\nscenes, answer questions about them, and segment objects of interest. We\ntrained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,\nwith 27,615 annotations over 9,205 images, and a multimodal pretraining\ndataset, PreGRES, containing over 1 million question-answer pairs. LISAt\noutperforms existing geospatial foundation models such as RS-GPT4V by over\n10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses\nstate-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %\n(gIoU). Our model, datasets, and code are available at\nhttps://lisat-bair.github.io/LISAt/", "AI": {"tldr": "LISAt\u662f\u4e00\u4e2a\u4e13\u4e3a\u590d\u6742\u9065\u611f\u573a\u666f\u8bbe\u8ba1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u63cf\u8ff0\u3001\u95ee\u7b54\u5e76\u5206\u5272\u76ee\u6807\u5bf9\u8c61\u3002\u5b83\u5728GRES\u548cPreGRES\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7684\u5206\u5272\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7528\u6237\u67e5\u8be2\u6216\u9065\u611f\u56fe\u50cf\u65f6\u5b58\u5728\u5c40\u9650\uff0cLISAt\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u6280\u672f\u7a7a\u767d\u3002", "method": "LISAt\u901a\u8fc7GRES\uff0827,615\u6807\u6ce8\uff09\u548cPreGRES\uff08100\u4e07\u95ee\u7b54\u5bf9\uff09\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u3002", "result": "LISAt\u5728\u9065\u611f\u63cf\u8ff0\u4efb\u52a1\u4e0a\u6bd4RS-GPT4V\u9ad810.04%\uff08BLEU-4\uff09\uff0c\u5728\u63a8\u7406\u5206\u5272\u4efb\u52a1\u4e0a\u6bd4\u5f00\u6e90\u6a21\u578b\u9ad8143.36%\uff08gIoU\uff09\u3002", "conclusion": "LISAt\u5728\u590d\u6742\u9065\u611f\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u5206\u5272\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.02763", "pdf": "https://arxiv.org/pdf/2505.02763", "abs": "https://arxiv.org/abs/2505.02763", "authors": ["Matthew Dahl"], "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9075\u5faa\u590d\u6742\u6cd5\u5f8b\u5f15\u7528\u89c4\u8303\uff08Bluebook\uff09\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5b8c\u5168\u5408\u89c4\u7387\u4ec5\u4e3a69%-74%\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4ec5\u63d0\u5347\u81f377%\uff0c\u5efa\u8bae\u8c28\u614e\u4f7f\u7528\u73b0\u6210LLM\u81ea\u52a8\u5316\u6cd5\u5f8b\u7a0b\u5e8f\u3002", "motivation": "\u6cd5\u5f8b\u5b9e\u8df5\u5bf9\u7a0b\u5e8f\u89c4\u5219\u6709\u4e25\u683c\u8981\u6c42\uff0c\u5c24\u5176\u662f\u590d\u6742\u7684Bluebook\u5f15\u7528\u89c4\u8303\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLM\u662f\u5426\u80fd\u51c6\u786e\u9075\u5faa\u6b64\u7c7b\u9ad8\u590d\u6742\u6027\u7cfb\u7edf\uff0c\u4e3a\u6cd5\u5f8b\u81ea\u52a8\u5316\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b866\u9879Bluebook\u4efb\u52a1\u7684\u539f\u521b\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86OpenAI\u3001Anthropic\u7b49\u516c\u53f8\u7684\u65d7\u8230LLM\uff0c\u5e76\u5206\u6790\u5176\u5408\u89c4\u8868\u73b0\u53ca\u4e0a\u4e0b\u6587\u5b66\u4e60\u6548\u679c\u3002", "result": "LLM\u751f\u6210\u7684\u5f15\u7528\u5b8c\u5168\u5408\u89c4\u7387\u4e3a69%-74%\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u89c4\u5219\u540e\u4ec5\u63d0\u5347\u81f377%\uff0c\u8868\u660e\u73b0\u6210\u6a21\u578b\u5728\u7a0b\u5e8f\u5fe0\u8bda\u5ea6\u5173\u952e\u9886\u57df\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u7814\u7a76\u8b66\u793a\u5728\u7a0b\u5e8f\u4e25\u8c28\u6027\u81f3\u5173\u91cd\u8981\u7684\u6cd5\u5f8b\u9886\u57df\u9700\u614e\u7528\u73b0\u6210LLM\uff0c\u5176\u81ea\u52a8\u5316\u80fd\u529b\u5c1a\u672a\u8fbe\u5230\u53ef\u9760\u6807\u51c6\u3002"}}
{"id": "2505.02011", "pdf": "https://arxiv.org/pdf/2505.02011", "abs": "https://arxiv.org/abs/2505.02011", "authors": ["Minhyuk Lee", "HyeKyung Yoon", "MyungJoo Kang"], "title": "CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate long-term time series forecasting is critical for applications\nsuch as weather prediction, and traffic analysis. In addition, the\nimplementation of Transformer variants has improved prediction accuracy.\nFollowing these variants, different input data process approaches also enhanced\nthe field, such as tokenization techniques including point-wise, channel-wise,\nand patch-wise tokenization. However, previous studies still have limitations\nin time complexity, computational resources, and cross-dimensional\ninteractions. To address these limitations, we introduce a novel CNN\nAutoencoder-based Score Attention mechanism (CASA), which can be introduced in\ndiverse Transformers model-agnosticically by reducing memory and leading to\nimprovement in model performance. Experiments on eight real-world datasets\nvalidate that CASA decreases computational resources by up to 77.7%,\naccelerates inference by 44.0%, and achieves state-of-the-art performance,\nranking first in 87.5% of evaluated metrics.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u81ea\u7f16\u7801\u5668\u7684\u5206\u6570\u6ce8\u610f\u529b\u673a\u5236\uff08CASA\uff09\uff0c\u901a\u8fc7\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u63d0\u9ad8\u591a\u6837Transformer\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8ba1\u7b97\u8d44\u6e90\u3001\u63a8\u7406\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u53d8\u91cf\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u65f6\u95f4\u590d\u6742\u6027\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u8de8\u7ef4\u5ea6\u4ea4\u4e92\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165CASA\u673a\u5236\uff0c\u53ef\u7075\u6d3b\u5e94\u7528\u4e8e\u591a\u79cdTransformer\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u63d0\u5347\u6027\u80fd\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cCASA\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u8fbe77.7%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534744.0%\uff0c\u5e76\u572887.5%\u7684\u6307\u6807\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "CASA\u5728\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2503.02910", "pdf": "https://arxiv.org/pdf/2503.02910", "abs": "https://arxiv.org/abs/2503.02910", "authors": ["Wenqi Guo", "Yiyang Du", "Shan Du"], "title": "LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Gas leakage poses a significant hazard that requires prevention.\nTraditionally, human inspection has been used for detection, a slow and\nlabour-intensive process. Recent research has applied machine learning\ntechniques to this problem, yet there remains a shortage of high-quality,\npublicly available datasets. This paper introduces a synthetic dataset, SimGas,\nfeaturing diverse backgrounds, interfering foreground objects, diverse leak\nlocations, and precise segmentation ground truth. We propose a zero-shot method\nthat combines background subtraction, zero-shot object detection, filtering,\nand segmentation to leverage this dataset. Experimental results indicate that\nour approach significantly outperforms baseline methods based solely on\nbackground subtraction and zero-shot object detection with segmentation,\nreaching an IoU of 69%. We also present an analysis of various prompt\nconfigurations and threshold settings to provide deeper insights into the\nperformance of our method. Finally, we qualitatively (because of the lack of\nground truth) tested our performance on GasVid and reached decent results on\nthe real-world dataset. The dataset, code, and full qualitative results are\navailable at https://github.com/weathon/Lang-Gas.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSimGas\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u6c14\u4f53\u6cc4\u6f0f\u68c0\u6d4b\u4e2d\u9ad8\u8d28\u91cf\u516c\u5f00\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u80cc\u666f\u51cf\u9664\u3001\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u3001\u8fc7\u6ee4\u548c\u5206\u5272\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u80cc\u666f\u51cf\u9664\u548c\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cIoU\u8fbe\u523069%\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u68c0\u6d4b\u6c14\u4f53\u6cc4\u6f0f\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u800c\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u516c\u5f00\u6570\u636e\u96c6\u3002\u901a\u8fc7\u5f15\u5165SimGas\u6570\u636e\u96c6\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u63a8\u52a8\u66f4\u9ad8\u6548\u7684\u6c14\u4f53\u6cc4\u6f0f\u68c0\u6d4b\u6280\u672f\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7ed3\u5408\u80cc\u666f\u51cf\u9664\u3001\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u3001\u8fc7\u6ee4\u548c\u5206\u5272\u6280\u672f\uff0c\u5229\u7528SimGas\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u7684IoU\u8fbe\u523069%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b9a\u6027\u6d4b\u8bd5\u5728GasVid\uff08\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff09\u4e0a\u4e5f\u8868\u73b0\u51fa\u4e0d\u9519\u7684\u6548\u679c\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u96c6SimGas\u53ca\u63d0\u51fa\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6c14\u4f53\u6cc4\u6f0f\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002\u6570\u636e\u3001\u4ee3\u7801\u548c\u5b9a\u6027\u7ed3\u679c\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.02819", "pdf": "https://arxiv.org/pdf/2505.02819", "abs": "https://arxiv.org/abs/2505.02819", "authors": ["Dmitriy Shopkhoev", "Ammar Ali", "Magauiya Zhussip", "Valentin Malykh", "Stamatios Lefkimmiatis", "Nikos Komodakis", "Sergey Zagoruyko"], "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository.", "AI": {"tldr": "ReplaceMe\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6df1\u5ea6\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u66ff\u4ee3Transformer\u5757\uff0c\u9ad8\u6548\u538b\u7f29\u6a21\u578b\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u800cReplaceMe\u65e8\u5728\u907f\u514d\u8fd9\u4e9b\u8ba1\u7b97\u5f00\u9500\uff0c\u4ec5\u9700\u5c11\u91cf\u6821\u51c6\u6570\u636e\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u53d8\u6362\u8fd1\u4f3c\u88ab\u526a\u679d\u7684\u5757\uff0c\u65e0\u9700\u989d\u5916\u7f51\u7edc\u53c2\u6570\uff0c\u53ef\u76f4\u63a5\u5408\u5e76\u5230\u5269\u4f59Transformer\u5757\u4e2d\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cReplaceMe\u6700\u9ad8\u53ef\u526a\u679d25%\uff0c\u4fdd\u7559\u7ea690%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\u3002", "conclusion": "ReplaceMe\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65e0\u8bad\u7ec3\u526a\u679d\u65b9\u6cd5\uff0c\u4e14\u4e0e\u9700\u5fae\u8c03\u7684\u65b9\u6cd5\u7ade\u4e89\u529b\u76f8\u5f53\u3002"}}
{"id": "2505.02020", "pdf": "https://arxiv.org/pdf/2505.02020", "abs": "https://arxiv.org/abs/2505.02020", "authors": ["Yancheng Chen", "Wenguo Yang", "Zhipeng Jiang"], "title": "Wide & Deep Learning for Node Classification", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "16 pages, 6 figures, 13 tables", "summary": "Wide & Deep, a simple yet effective learning architecture for recommendation\nsystems developed by Google, has had a significant impact in both academia and\nindustry due to its combination of the memorization ability of generalized\nlinear models and the generalization ability of deep models. Graph\nconvolutional networks (GCNs) remain dominant in node classification tasks;\nhowever, recent studies have highlighted issues such as heterophily and\nexpressiveness, which focus on graph structure while seemingly neglecting the\npotential role of node features. In this paper, we propose a flexible framework\nGCNIII, which leverages the Wide & Deep architecture and incorporates three\ntechniques: Intersect memory, Initial residual and Identity mapping. We provide\ncomprehensive empirical evidence showing that GCNIII can more effectively\nbalance the trade-off between over-fitting and over-generalization on various\nsemi- and full- supervised tasks. Additionally, we explore the use of large\nlanguage models (LLMs) for node feature engineering to enhance the performance\nof GCNIII in cross-domain node classification tasks. Our implementation is\navailable at https://github.com/CYCUCAS/GCNIII.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGCNIII\u6846\u67b6\uff0c\u7ed3\u5408Wide & Deep\u67b6\u6784\u4e0e\u4e09\u9879\u6280\u672f\uff08Intersect memory\u3001Initial residual\u3001Identity mapping\uff09\uff0c\u5728\u534a\u76d1\u7763\u548c\u5168\u76d1\u7763\u4efb\u52a1\u4e2d\u66f4\u6709\u6548\u5e73\u8861\u8fc7\u62df\u5408\u4e0e\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u8282\u70b9\u7279\u5f81\u3002", "motivation": "\u9488\u5bf9\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u56e0\u5f02\u8d28\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u800c\u5ffd\u89c6\u8282\u70b9\u7279\u5f81\u6f5c\u529b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u6539\u8fdb\u6846\u67b6\u4ee5\u7ed3\u5408\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u57fa\u4e8eWide & Deep\u67b6\u6784\uff0c\u5f15\u5165Intersect memory\u3001Initial residual\u548cIdentity mapping\u6280\u672f\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u8282\u70b9\u7279\u5f81\u5de5\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGCNIII\u5728\u534a\u76d1\u7763\u548c\u5168\u76d1\u7763\u4efb\u52a1\u4e2d\u80fd\u66f4\u597d\u5e73\u8861\u8fc7\u62df\u5408\u4e0e\u6cdb\u5316\uff0c\u4e14\u5728\u8de8\u9886\u57df\u8282\u70b9\u5206\u7c7b\u4e2d\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GCNIII\u901a\u8fc7\u878d\u5408\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u673a\u5236\u53ca\u7279\u5f81\u589e\u5f3a\u6280\u672f\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2504.18793", "pdf": "https://arxiv.org/pdf/2504.18793", "abs": "https://arxiv.org/abs/2504.18793", "authors": ["Santosh Bhupathi"], "title": "Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations", "categories": ["cs.DB", "cs.AI", "97P30", "I.2.7; H.2.5"], "comment": "9 pages", "summary": "The rapid adoption of AI-powered applications demands high-performance,\nscalable, and efficient cloud database solutions, as traditional architectures\noften struggle with AI-driven workloads requiring real-time data access, vector\nsearch, and low-latency queries. This paper explores how cloud-native databases\nenable AI-driven applications by leveraging purpose-built technologies such as\nvector databases (pgvector), graph databases (AWS Neptune), NoSQL stores\n(Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and\nPostgreSQL). It presents architectural patterns for integrating AI workloads\nwith cloud databases, including Retrieval-Augmented Generation (RAG) [1] with\nLLMs, real-time data pipelines, AI-driven query optimization, and\nembeddings-based search. Performance benchmarks, scalability considerations,\nand cost-efficient strategies are evaluated to guide the design of AI-enabled\napplications. Real-world case studies from industries such as healthcare,\nfinance, and customer experience illustrate how enterprises utilize cloud\ndatabases to enhance AI capabilities while ensuring security, governance, and\ncompliance with enterprise and regulatory standards. By providing a\ncomprehensive analysis of AI and cloud database integration, this paper serves\nas a practical guide for researchers, architects, and enterprises to build\nnext-generation AI applications that optimize performance, scalability, and\ncost efficiency in cloud environments.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u9a71\u52a8\u5e94\u7528\u5982\u4f55\u901a\u8fc7\u4e91\u539f\u751f\u6570\u636e\u5e93\uff08\u5982\u5411\u91cf\u6570\u636e\u5e93\u3001\u56fe\u6570\u636e\u5e93\u3001NoSQL\u548c\u5173\u7cfb\u578b\u6570\u636e\u5e93\uff09\u4f18\u5316\u6027\u80fd\u3001\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u5e76\u7ed3\u5408\u67b6\u6784\u6a21\u5f0f\uff08\u5982RAG\u3001\u5b9e\u65f6\u6570\u636e\u7ba1\u9053\u7b49\uff09\u548c\u884c\u4e1a\u6848\u4f8b\u8fdb\u884c\u4e86\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u67b6\u6784\u96be\u4ee5\u6ee1\u8db3AI\u9a71\u52a8\u7684\u5b9e\u65f6\u6570\u636e\u8bbf\u95ee\u548c\u4f4e\u5ef6\u8fdf\u67e5\u8be2\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u4e91\u6570\u636e\u5e93\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301AI\u5e94\u7528\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u4e91\u539f\u751f\u6570\u636e\u5e93\u6280\u672f\uff08\u5982pgvector\u3001AWS Neptune\u7b49\uff09\u53ca\u67b6\u6784\u6a21\u5f0f\uff08\u5982RAG\u3001\u57fa\u4e8e\u5d4c\u5165\u7684\u641c\u7d22\u7b49\uff09\uff0c\u5e76\u901a\u8fc7\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u548c\u884c\u4e1a\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5c55\u793a\u4e86\u4e91\u6570\u636e\u5e93\u5982\u4f55\u63d0\u5347AI\u5e94\u7528\u7684\u6027\u80fd\u3001\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u7b49\uff09\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u9ad8\u6027\u80fd\u3001\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u7684\u4e0b\u4e00\u4ee3AI\u5e94\u7528\u63d0\u4f9b\u4e86\u4e91\u6570\u636e\u5e93\u96c6\u6210\u7684\u5b9e\u8df5\u6307\u5357\u3002"}}
{"id": "2505.01433", "pdf": "https://arxiv.org/pdf/2505.01433", "abs": "https://arxiv.org/abs/2505.01433", "authors": ["Cong Qi", "Hanzhang Fang", "Siqi jiang", "Tianxing Hu", "Wei Zhi"], "title": "Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations", "categories": ["q-bio.QM", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the binding specificity between T-cell receptors (TCRs) and\npeptide-major histocompatibility complexes (pMHCs) is central to immunotherapy\nand vaccine development. However, current predictive models struggle with\ngeneralization, especially in data-scarce settings and when faced with novel\nepitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced\nRecognition Network), a deep learning framework that combines large-scale\nprotein language models with chemical representations of peptides. By encoding\nTCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences\ninto SMILES strings processed by MolFormer, LANTERN captures rich biological\nand chemical features critical for TCR-peptide recognition. Through extensive\nbenchmarking against existing models such as ChemBERTa, TITAN, and NetTCR,\nLANTERN demonstrates superior performance, particularly in zero-shot and\nfew-shot learning scenarios. Our model also benefits from a robust negative\nsampling strategy and shows significant clustering improvements via embedding\nanalysis. These results highlight the potential of LANTERN to advance TCR-pMHC\nbinding prediction and support the development of personalized immunotherapies.", "AI": {"tldr": "LANTERN\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u89c4\u6a21\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u80bd\u5316\u5b66\u8868\u793a\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bTCR\u4e0epMHC\u7684\u7ed3\u5408\u7279\u5f02\u6027\uff0c\u5e76\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u9884\u6d4b\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u548c\u65b0\u8868\u4f4d\u60c5\u51b5\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u514d\u75ab\u6cbb\u7597\u548c\u75ab\u82d7\u5f00\u53d1\u7684\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528ESM-1b\u7f16\u7801TCR\u94fe\u5e8f\u5217\uff0c\u5c06\u80bd\u5e8f\u5217\u8f6c\u6362\u4e3aSMILES\u5b57\u7b26\u4e32\u5e76\u7531MolFormer\u5904\u7406\uff0c\u7ed3\u5408\u751f\u7269\u548c\u5316\u5b66\u7279\u5f81\u3002", "result": "LANTERN\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5982ChemBERTa\u3001TITAN\u548cNetTCR\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u5206\u6790\u5c55\u793a\u4e86\u663e\u8457\u7684\u805a\u7c7b\u6539\u8fdb\u3002", "conclusion": "LANTERN\u6709\u6f5c\u529b\u63a8\u52a8TCR-pMHC\u7ed3\u5408\u9884\u6d4b\uff0c\u5e76\u4e3a\u4e2a\u6027\u5316\u514d\u75ab\u7597\u6cd5\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2505.02022", "pdf": "https://arxiv.org/pdf/2505.02022", "abs": "https://arxiv.org/abs/2505.02022", "authors": ["Yiming Zhang", "Koji Tsuda"], "title": "NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Nanobodies, single-domain antibody fragments derived from camelid\nheavy-chain-only antibodies, exhibit unique advantages such as compact size,\nhigh stability, and strong binding affinity, making them valuable tools in\ntherapeutics and diagnostics. While recent advances in pretrained protein and\nantibody language models (PPLMs and PALMs) have greatly enhanced biomolecular\nunderstanding, nanobody-specific modeling remains underexplored and lacks a\nunified benchmark. To address this gap, we introduce NbBench, the first\ncomprehensive benchmark suite for nanobody representation learning. Spanning\neight biologically meaningful tasks across nine curated datasets, NbBench\nencompasses structure annotation, binding prediction, and developability\nassessment. We systematically evaluate eleven representative models--including\ngeneral-purpose protein LMs, antibody-specific LMs, and nanobody-specific\nLMs--in a frozen setting. Our analysis reveals that antibody language models\nexcel in antigen-related tasks, while performance on regression tasks such as\nthermostability and affinity remains challenging across all models. Notably, no\nsingle model consistently outperforms others across all tasks. By standardizing\ndatasets, task definitions, and evaluation protocols, NbBench offers a\nreproducible foundation for assessing and advancing nanobody modeling.", "AI": {"tldr": "NbBench\u662f\u9996\u4e2a\u5168\u9762\u7684\u7eb3\u7c73\u6297\u4f53\u8868\u793a\u5b66\u4e60\u57fa\u51c6\u5957\u4ef6\uff0c\u8986\u76d68\u4e2a\u751f\u7269\u76f8\u5173\u4efb\u52a1\uff0c\u8bc4\u4f3011\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u6297\u4f53\u8bed\u8a00\u6a21\u578b\u5728\u6297\u539f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u70ed\u7a33\u5b9a\u6027\u548c\u4eb2\u548c\u529b\u4efb\u52a1\u4e2d\u4ecd\u6709\u6311\u6218\u3002", "motivation": "\u7eb3\u7c73\u6297\u4f53\u56e0\u5176\u72ec\u7279\u4f18\u52bf\u5728\u6cbb\u7597\u548c\u8bca\u65ad\u4e2d\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u7eb3\u7c73\u6297\u4f53\u7279\u5b9a\u6a21\u578b\u7814\u7a76\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u7edf\u4e00\u57fa\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1NbBench\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165NbBench\u57fa\u51c6\u5957\u4ef6\uff0c\u6db5\u76d68\u4e2a\u4efb\u52a1\u548c9\u4e2a\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f3011\u79cd\u6a21\u578b\uff08\u901a\u7528\u86cb\u767dLM\u3001\u6297\u4f53LM\u3001\u7eb3\u7c73\u6297\u4f53LM\uff09\uff0c\u91c7\u7528\u51bb\u7ed3\u8bbe\u7f6e\u3002", "result": "\u6297\u4f53\u8bed\u8a00\u6a21\u578b\u5728\u6297\u539f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\uff08\u5982\u70ed\u7a33\u5b9a\u6027\u548c\u4eb2\u548c\u529b\uff09\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u4e14\u65e0\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d consistently \u6700\u4f18\u3002", "conclusion": "NbBench\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u7eb3\u7c73\u6297\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.01435", "pdf": "https://arxiv.org/pdf/2505.01435", "abs": "https://arxiv.org/abs/2505.01435", "authors": ["Carlo Siebenschuh", "Kyle Hippe", "Ozan Gokdemir", "Alexander Brace", "Arham Khan", "Khalid Hossain", "Yadu Babuji", "Nicholas Chia", "Venkatram Vishwanath", "Rick Stevens", "Arvind Ramanathan", "Ian Foster", "Robert Underwood"], "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine", "categories": ["cs.IR", "cs.CL", "cs.DC", "cs.LG"], "comment": "This paper has been accepted at the The Eighth Annual Conference on\n  Machine Learning and Systems (MLSys 2025)", "summary": "Language models for scientific tasks are trained on text from scientific\npublications, most distributed as PDFs that require parsing. PDF parsing\napproaches range from inexpensive heuristics (for simple documents) to\ncomputationally intensive ML-driven systems (for complex or degraded ones). The\nchoice of the \"best\" parser for a particular document depends on its\ncomputational cost and the accuracy of its output. To address these issues, we\nintroduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine\n(AdaParse), a data-driven strategy for assigning an appropriate parser to each\ndocument. We enlist scientists to select preferred parser outputs and\nincorporate this information through direct preference optimization (DPO) into\nAdaParse, thereby aligning its selection process with human judgment. AdaParse\nthen incorporates hardware requirements and predicted accuracy of each parser\nto orchestrate computational resources efficiently for large-scale parsing\ncampaigns. We demonstrate that AdaParse, when compared to state-of-the-art\nparsers, improves throughput by $17\\times$ while still achieving comparable\naccuracy (0.2 percent better) on a benchmark set of 1000 scientific documents.\nAdaParse's combination of high accuracy and parallel scalability makes it\nfeasible to parse large-scale scientific document corpora to support the\ndevelopment of high-quality, trillion-token-scale text datasets. The\nimplementation is available at https://github.com/7shoe/AdaParse/", "AI": {"tldr": "AdaParse\u662f\u4e00\u79cd\u81ea\u9002\u5e94PDF\u89e3\u6790\u5f15\u64ce\uff0c\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u548c\u786c\u4ef6\u9700\u6c42\u4f18\u5316\u89e3\u6790\u5668\u9009\u62e9\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u89e3\u6790\u4efb\u52a1\u7684\u541e\u5410\u91cf17\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u79d1\u5b66\u6587\u6863\u591a\u4e3aPDF\u683c\u5f0f\uff0c\u73b0\u6709\u89e3\u6790\u5668\u5728\u6210\u672c\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u6743\u8861\u4e0d\u8db3\uff0c\u9700\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u548c\u8d44\u6e90\u5206\u914d\u3002", "method": "\u5f15\u5165AdaParse\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u7ed3\u5408\u79d1\u5b66\u5bb6\u7684\u9009\u62e9\u504f\u597d\uff0c\u52a8\u6001\u5206\u914d\u89e3\u6790\u5668\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u57281000\u4efd\u79d1\u5b66\u6587\u6863\u6d4b\u8bd5\u4e2d\uff0c\u541e\u5410\u91cf\u63d0\u534717\u500d\uff0c\u7cbe\u5ea6\u63d0\u9ad80.2%\u3002", "conclusion": "AdaParse\u7684\u9ad8\u6548\u4e0e\u53ef\u6269\u5c55\u6027\u4f7f\u5176\u9002\u5408\u6784\u5efa\u4e07\u4ebf\u7ea7\u6587\u672c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2505.02027", "pdf": "https://arxiv.org/pdf/2505.02027", "abs": "https://arxiv.org/abs/2505.02027", "authors": ["Rui Lv", "Zaixi Zhang", "Kai Zhang", "Qi Liu", "Weibo Gao", "Jiawei Liu", "Jiaxia Yan", "Linan Yue", "Fangzhou Yao"], "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted", "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86GraphPrompter,\u4e00\u79cd\u591a\u9636\u6bb5\u81ea\u9002\u5e94\u63d0\u793a\u4f18\u5316\u65b9\u6cd5,\u65e8\u5728\u63d0\u5347\u56fe\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u968f\u673a\u9009\u62e9\u5b50\u56fe\u6216\u8fb9\u4f5c\u4e3a\u63d0\u793a,\u5bfc\u81f4\u566a\u58f0\u548c\u6027\u80fd\u4e0b\u964d\u3002\u540c\u65f6,\u6d4b\u8bd5\u56fe\u7c7b\u522b\u6570\u8fdc\u591a\u4e8e\u8bad\u7ec3\u56fe\u65f6,\u6027\u80fd\u4e5f\u4f1a\u663e\u8457\u6076\u5316\u3002", "method": "GraphPrompter\u5305\u62ec\u63d0\u793a\u751f\u6210\u5668\u3001\u9009\u62e9\u5668\u548c\u589e\u5f3a\u5668\u4e09\u9636\u6bb5:\u751f\u6210\u5668\u901a\u8fc7\u91cd\u6784\u5c42\u7a81\u51fa\u4fe1\u606f\u8fb9,\u9009\u62e9\u5668\u7528k\u8fd1\u90bb\u548c\u9884\u8bad\u7ec3\u5c42\u52a8\u6001\u9009\u62e9\u6837\u672c,\u589e\u5f3a\u5668\u901a\u8fc7\u7f13\u5b58\u7b56\u7565\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGraphPrompter\u5e73\u5747\u4f18\u4e8e\u73b0\u6709baseline\u8d85\u8fc78%\u3002", "conclusion": "GraphPrompter\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u751f\u6210\u3001\u9009\u62e9\u548c\u4f7f\u7528\u7684\u5168\u8fc7\u7a0b,\u663e\u8457\u63d0\u5347\u4e86\u56fe\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2505.02033", "pdf": "https://arxiv.org/pdf/2505.02033", "abs": "https://arxiv.org/abs/2505.02033", "authors": ["Emine Akpinar", "Batuhan Hangun", "Murat Oduncuoglu", "Oguz Altun", "Onder Eyecioglu", "Zeynel Yalcin"], "title": "Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles", "categories": ["cs.LG", "q-bio.GN", "q-bio.MN"], "comment": null, "summary": "DNA microarray technology enables the simultaneous measurement of expression\nlevels of thousands of genes, thereby facilitating the understanding of the\nmolecular mechanisms underlying complex diseases such as brain tumors and the\nidentification of diagnostic genetic signatures. To derive meaningful\nbiological insights from the high-dimensional and complex gene features\nobtained through this technology and to analyze gene properties in detail,\nclassical AI-based approaches such as machine learning and deep learning are\nwidely employed. However, these methods face various limitations in managing\nhigh-dimensional vector spaces and modeling the intricate relationships among\ngenes. In particular, challenges such as hyperparameter tuning, computational\ncosts, and high processing power requirements can hinder their efficiency. To\novercome these limitations, quantum computing and quantum AI approaches are\ngaining increasing attention. Leveraging quantum properties such as\nsuperposition and entanglement, quantum methods enable more efficient parallel\nprocessing of high-dimensional data and offer faster and more effective\nsolutions to problems that are computationally demanding for classical methods.\nIn this study, a novel model called \"Deep VQC\" is proposed, based on the\nVariational Quantum Classifier approach. Developed using microarray data\ncontaining 54,676 gene features, the model successfully classified four\ndifferent types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and\npilocytic astrocytoma-alongside healthy samples with high accuracy.\nFurthermore, compared to classical ML algorithms, our model demonstrated either\nsuperior or comparable classification performance. These results highlight the\npotential of quantum AI methods as an effective and promising approach for the\nanalysis and classification of complex structures such as brain tumors based on\ngene expression features.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684Deep VQC\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u7ef4\u57fa\u56e0\u7279\u5f81\u6570\u636e\u6210\u529f\u5206\u7c7b\u8111\u80bf\u7624\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfAI\u65b9\u6cd5\u5728\u5206\u6790\u9ad8\u7ef4\u57fa\u56e0\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5982\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u3002", "method": "\u5229\u7528\u53d8\u5206\u91cf\u5b50\u5206\u7c7b\u5668\uff08VQC\uff09\u65b9\u6cd5\uff0c\u6784\u5efaDeep VQC\u6a21\u578b\uff0c\u5904\u740654,676\u4e2a\u57fa\u56e0\u7279\u5f81\u7684\u5fae\u9635\u5217\u6570\u636e\u3002", "result": "\u6a21\u578b\u51c6\u786e\u5206\u7c7b\u56db\u79cd\u8111\u80bf\u7624\u53ca\u5065\u5eb7\u6837\u672c\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u63a5\u8fd1\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u91cf\u5b50AI\u65b9\u6cd5\u5728\u590d\u6742\u57fa\u56e0\u6570\u636e\u5206\u7c7b\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u548c\u6f5c\u529b\uff0c\u662f\u672a\u6765\u7814\u7a76\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2505.02035", "pdf": "https://arxiv.org/pdf/2505.02035", "abs": "https://arxiv.org/abs/2505.02035", "authors": ["Tianshu Yu"], "title": "Secrets of GFlowNets' Learning Behavior: A Theoretical Study", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for\ngenerating composite structures, demonstrating considerable promise across\ndiverse applications. While substantial progress has been made in exploring\ntheir modeling validity and connections to other generative frameworks, the\ntheoretical understanding of their learning behavior remains largely uncharted.\nIn this work, we present a rigorous theoretical investigation of GFlowNets'\nlearning behavior, focusing on four fundamental dimensions: convergence, sample\ncomplexity, implicit regularization, and robustness. By analyzing these\naspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's\nlearning dynamics, shedding light on its strengths and limitations. Our\nfindings contribute to a deeper understanding of the factors influencing\nGFlowNet performance and provide insights into principled guidelines for their\neffective design and deployment. This study not only bridges a critical gap in\nthe theoretical landscape of GFlowNets but also lays the foundation for their\nevolution as a reliable and interpretable framework for generative modeling.\nThrough this, we aspire to advance the theoretical frontiers of GFlowNets and\ncatalyze their broader adoption in the AI community.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNets\uff09\u7684\u5b66\u4e60\u884c\u4e3a\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u6536\u655b\u6027\u3001\u6837\u672c\u590d\u6742\u6027\u3001\u9690\u5f0f\u6b63\u5219\u5316\u548c\u9c81\u68d2\u6027\u56db\u4e2a\u7ef4\u5ea6\uff0c\u4ee5\u63ed\u793a\u5176\u5b66\u4e60\u673a\u5236\u5e76\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1GFlowNets\u5728\u751f\u6210\u590d\u5408\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5b66\u4e60\u884c\u4e3a\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u4e0d\u5b8c\u5584\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aGFlowNets\u7684\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u4e86GFlowNets\u5728\u6536\u655b\u6027\u3001\u6837\u672c\u590d\u6742\u6027\u3001\u9690\u5f0f\u6b63\u5219\u5316\u548c\u9c81\u68d2\u6027\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86GFlowNets\u7684\u5b66\u4e60\u673a\u5236\u53ca\u5176\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u8bbe\u8ba1\u548c\u90e8\u7f72\u7684\u6307\u5bfc\u539f\u5219\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u586b\u8865\u4e86GFlowNets\u7406\u8bba\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u8fd8\u4e3a\u5176\u4f5c\u4e3a\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u5176\u5728AI\u793e\u533a\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2505.02069", "pdf": "https://arxiv.org/pdf/2505.02069", "abs": "https://arxiv.org/abs/2505.02069", "authors": ["Seoungbin Bae", "Dabeen Lee"], "title": "Neural Logistic Bandits", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study the problem of neural logistic bandits, where the main task is to\nlearn an unknown reward function within a logistic link function using a neural\nnetwork. Existing approaches either exhibit unfavorable dependencies on\n$\\kappa$, where $1/\\kappa$ represents the minimum variance of reward\ndistributions, or suffer from direct dependence on the feature dimension $d$,\nwhich can be huge in neural network-based settings. In this work, we introduce\na novel Bernstein-type inequality for self-normalized vector-valued martingales\nthat is designed to bypass a direct dependence on the ambient dimension. This\nlets us deduce a regret upper bound that grows with the effective dimension\n$\\widetilde{d}$, not the feature dimension, while keeping a minimal dependence\non $\\kappa$. Based on the concentration inequality, we propose two algorithms,\nNeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of\norder $\\widetilde{O}(\\widetilde{d}\\sqrt{\\kappa T})$ and\n$\\widetilde{O}(\\widetilde{d}\\sqrt{T/\\kappa})$, respectively, improving on the\nexisting results. Lastly, we report numerical results on both synthetic and\nreal datasets to validate our theoretical findings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u795e\u7ecf\u903b\u8f91\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u578bBernstein\u578b\u4e0d\u7b49\u5f0f\u4ee5\u964d\u4f4e\u7ef4\u5ea6\u4f9d\u8d56\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u7b97\u6cd5\u663e\u8457\u6539\u5584\u73b0\u6709\u7ed3\u679c\u7684\u9057\u61be\u4e0a\u754c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u7684\u80cc\u666f\u4e0b\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u5956\u52b1\u5206\u5e03\u7684\u6700\u5c0f\u65b9\u5dee\u5012\u6570$\\kappa$\uff0c\u8981\u4e48\u76f4\u63a5\u53d7\u7279\u5f81\u7ef4\u5ea6$d$\u7684\u5f71\u54cd\uff0c\u800c$d$\u53ef\u80fd\u5f88\u5927\u3002\u8bba\u6587\u65e8\u5728\u7ed5\u8fc7\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u578bBernstein\u578b\u4e0d\u7b49\u5f0f\uff0c\u7ed5\u8fc7\u5bf9\u7ef4\u5ea6$d$\u7684\u76f4\u63a5\u4f9d\u8d56\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e24\u79cd\u7b97\u6cd5\uff08NeuralLog-UCB-1\u548cNeuralLog-UCB-2\uff09\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u4e24\u79cd\u7b97\u6cd5\u7684\u9057\u61be\u4e0a\u754c\u5206\u522b\u4e3a$\tilde{O}(\tilde{d}\\sqrt{\\kappa T})$\u548c$\tilde{O}(\tilde{d}\\sqrt{T/\\kappa})$\uff0c\u4f18\u4e8e\u73b0\u6709\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u65b0\u578b\u4e0d\u7b49\u5f0f\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u7ef4\u5ea6\u548c$\\kappa$\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u4f18\u52bf\u3002"}}
{"id": "2505.01453", "pdf": "https://arxiv.org/pdf/2505.01453", "abs": "https://arxiv.org/abs/2505.01453", "authors": ["Bharathkumar Hegde", "Melanie Bouroche"], "title": "Safe and Efficient CAV Lane Changing using Decentralised Safety Shields", "categories": ["cs.MA", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted in IEEE IV 2025", "summary": "Lane changing is a complex decision-making problem for Connected and\nAutonomous Vehicles (CAVs) as it requires balancing traffic efficiency with\nsafety. Although traffic efficiency can be improved by using vehicular\ncommunication for training lane change controllers using Multi-Agent\nReinforcement Learning (MARL), ensuring safety is difficult. To address this\nissue, we propose a decentralised Hybrid Safety Shield (HSS) that combines\noptimisation and a rule-based approach to guarantee safety. Our method applies\ncontrol barrier functions to constrain longitudinal and lateral control inputs\nof a CAV to ensure safe manoeuvres. Additionally, we present an architecture to\nintegrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while\nensuring safety. We evaluate MARL-HSS using a gym-like environment that\nsimulates an on-ramp merging scenario with two levels of traffic densities,\nsuch as light and moderate densities. The results show that HSS provides a\nsafety guarantee by strictly enforcing a dynamic safety constraint defined on a\ntime headway, even in moderate traffic density that offers challenging lane\nchange scenarios. Moreover, the proposed method learns stable policies compared\nto the baseline, a state-of-the-art MARL lane change controller without a\nsafety shield. Further policy evaluation shows that our method achieves a\nbalance between safety and traffic efficiency with zero crashes and comparable\naverage speeds in light and moderate traffic densities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f18\u5316\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u6563\u5f0f\u6df7\u5408\u5b89\u5168\u62a4\u76fe\uff08HSS\uff09\uff0c\u7528\u4e8eCAV\u7684\u53d8\u9053\u51b3\u7b56\uff0c\u786e\u4fdd\u5b89\u5168\u7684\u540c\u65f6\u63d0\u5347\u4ea4\u901a\u6548\u7387\u3002", "motivation": "\u89e3\u51b3CAV\u5728\u53d8\u9053\u51b3\u7b56\u4e2d\u5982\u4f55\u5728\u63d0\u5347\u4ea4\u901a\u6548\u7387\u7684\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faHSS\u65b9\u6cd5\uff0c\u7ed3\u5408\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7ea6\u675fCAV\u7684\u7eb5\u5411\u548c\u6a2a\u5411\u63a7\u5236\u8f93\u5165\uff0c\u5e76\u4e0eMARL\u96c6\u6210\uff08MARL-HSS\uff09\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cHSS\u80fd\u4e25\u683c\u4fdd\u969c\u52a8\u6001\u5b89\u5168\u7ea6\u675f\uff0c\u4e14\u5728\u4e2d\u7b49\u5bc6\u5ea6\u4ea4\u901a\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u5b9e\u73b0\u96f6\u4e8b\u6545\u4e0e\u53ef\u6bd4\u7684\u5e73\u5747\u901f\u5ea6\u3002", "conclusion": "MARL-HSS\u5728\u53d8\u9053\u51b3\u7b56\u4e2d\u6210\u529f\u5e73\u8861\u4e86\u5b89\u5168\u6027\u4e0e\u4ea4\u901a\u6548\u7387\uff0c\u4f18\u4e8e\u65e0\u5b89\u5168\u62a4\u76fe\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2505.02073", "pdf": "https://arxiv.org/pdf/2505.02073", "abs": "https://arxiv.org/abs/2505.02073", "authors": ["Yi Han"], "title": "Lightweight Defense Against Adversarial Attacks in Time Series Classification", "categories": ["cs.LG", "cs.AI", "68T05, 62H30", "I.2.6; I.5.1; G.3"], "comment": "13 pages, 8 figures. Accepted at RAFDA Workshop, PAKDD 2025\n  (Springer, EI & Scopus indexed). Code:\n  https://github.com/Yi126/Lightweight-Defence", "summary": "As time series classification (TSC) gains prominence, ensuring robust TSC\nmodels against adversarial attacks is crucial. While adversarial defense is\nwell-studied in Computer Vision (CV), the TSC field has primarily relied on\nadversarial training (AT), which is computationally expensive. In this paper,\nfive data augmentation-based defense methods tailored for time series are\ndeveloped, with the most computationally intensive method among them increasing\nthe computational resources by only 14.07% compared to the original TSC model.\nMoreover, the deployment process for these methods is straightforward. By\nleveraging these advantages of our methods, we create two combined methods. One\nof these methods is an ensemble of all the proposed techniques, which not only\nprovides better defense performance than PGD-based AT but also enhances the\ngeneralization ability of TSC models. Moreover, the computational resources\nrequired for our ensemble are less than one-third of those required for\nPGD-based AT. These methods advance robust TSC in data mining. Furthermore, as\nfoundation models are increasingly explored for time series feature learning,\nour work provides insights into integrating data augmentation-based adversarial\ndefense with large-scale pre-trained models in future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e94\u79cd\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u9632\u5fa1\u65b9\u6cd5\uff0c\u5176\u4e2d\u6700\u9ad8\u8ba1\u7b97\u8d44\u6e90\u7684\u65b9\u6cd5\u4ec5\u589e\u52a014.07%\u7684\u8ba1\u7b97\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u79cd\u7ec4\u5408\u65b9\u6cd5\uff0c\u5176\u4e2d\u96c6\u6210\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8ePGD\u7684\u5bf9\u6297\u8bad\u7ec3\u4e14\u8ba1\u7b97\u8d44\u6e90\u66f4\u5c11\u3002", "motivation": "\u968f\u7740\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u91cd\u8981\u6027\u63d0\u5347\uff0c\u786e\u4fdd\u5176\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e94\u79cd\u6570\u636e\u589e\u5f3a\u9632\u5fa1\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u7ec4\u5408\u6210\u4e24\u79cd\u65b9\u6cd5\uff08\u5305\u62ec\u4e00\u79cd\u96c6\u6210\u65b9\u6cd5\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\u7684\u9632\u5fa1\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8ePGD\u7684\u5bf9\u6297\u8bad\u7ec3\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u4ec5\u4e3a\u540e\u8005\u7684\u4e09\u5206\u4e4b\u4e00\u4ee5\u4e0b\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u63a8\u52a8\u4e86\u9c81\u68d2\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u53d1\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u6570\u636e\u589e\u5f3a\u9632\u5fa1\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.01790", "pdf": "https://arxiv.org/pdf/2505.01790", "abs": "https://arxiv.org/abs/2505.01790", "authors": ["Markos Stamatakis", "Joshua Berger", "Christian Wartena", "Ralph Ewerth", "Anett Hoppe"], "title": "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "12 pages (excluding references), 8 tables, 1 equation", "summary": "Web-based educational videos offer flexible learning opportunities and are\nbecoming increasingly popular. However, improving user engagement and knowledge\nretention remains a challenge. Automatically generated questions can activate\nlearners and support their knowledge acquisition. Further, they can help\nteachers and learners assess their understanding. While large language and\nvision-language models have been employed in various tasks, their application\nto question generation for educational videos remains underexplored. In this\npaper, we investigate the capabilities of current vision-language models for\ngenerating learning-oriented questions for educational video content. We assess\n(1) out-of-the-box models' performance; (2) fine-tuning effects on\ncontent-specific question generation; (3) the impact of different video\nmodalities on question quality; and (4) in a qualitative study, question\nrelevance, answerability, and difficulty levels of generated questions. Our\nfindings delineate the capabilities of current vision-language models,\nhighlighting the need for fine-tuning and addressing challenges in question\ndiversity and relevance. We identify requirements for future multimodal\ndatasets and outline promising research directions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u89c6\u9891\u4e2d\u81ea\u52a8\u751f\u6210\u5b66\u4e60\u5bfc\u5411\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3001\u5fae\u8c03\u6548\u679c\u3001\u89c6\u9891\u6a21\u6001\u5bf9\u95ee\u9898\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u672a\u6765\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u7814\u7a76\u65b9\u5411\u7684\u5efa\u8bae\u3002", "motivation": "\u63d0\u5347\u6559\u80b2\u89c6\u9891\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u77e5\u8bc6\u7559\u5b58\u7387\u662f\u5f53\u524d\u6311\u6218\uff0c\u81ea\u52a8\u751f\u6210\u95ee\u9898\u53ef\u4ee5\u6fc0\u6d3b\u5b66\u4e60\u8005\u5e76\u8f85\u52a9\u77e5\u8bc6\u83b7\u53d6\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4f46\u5176\u5728\u6559\u80b2\u89c6\u9891\u95ee\u9898\u751f\u6210\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\uff081\uff09\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\uff1b\uff082\uff09\u5fae\u8c03\u5bf9\u5185\u5bb9\u7279\u5b9a\u95ee\u9898\u751f\u6210\u7684\u5f71\u54cd\uff1b\uff083\uff09\u4e0d\u540c\u89c6\u9891\u6a21\u6001\u5bf9\u95ee\u9898\u8d28\u91cf\u7684\u5f71\u54cd\uff1b\uff084\uff09\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\u5206\u6790\u751f\u6210\u95ee\u9898\u7684\u76f8\u5173\u6027\u3001\u53ef\u56de\u7b54\u6027\u548c\u96be\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u9762\u4e34\u95ee\u9898\u591a\u6837\u6027\u548c\u76f8\u5173\u6027\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u5bf9\u672a\u6765\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u7814\u7a76\u65b9\u5411\u7684\u9700\u6c42\uff0c\u4e3a\u6559\u80b2\u89c6\u9891\u95ee\u9898\u751f\u6210\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2505.02074", "pdf": "https://arxiv.org/pdf/2505.02074", "abs": "https://arxiv.org/abs/2505.02074", "authors": ["Francesco Petri", "Luigi Asprino", "Aldo Gangemi"], "title": "Learning Local Causal World Models with State Space Models and Attention", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "World modelling, i.e. building a representation of the rules that govern the\nworld so as to predict its evolution, is an essential ability for any agent\ninteracting with the physical world. Despite their impressive performance, many\nsolutions fail to learn a causal representation of the environment they are\ntrying to model, which would be necessary to gain a deep enough understanding\nof the world to perform complex tasks. With this work, we aim to broaden the\nresearch in the intersection of causality theory and neural world modelling by\nassessing the potential for causal discovery of the State Space Model (SSM)\narchitecture, which has been shown to have several advantages over the\nwidespread Transformer. We show empirically that, compared to an equivalent\nTransformer, a SSM can model the dynamics of a simple environment and learn a\ncausal model at the same time with equivalent or better performance, thus\npaving the way for further experiments that lean into the strength of SSMs and\nfurther enhance them with causal awareness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5728\u6784\u5efa\u56e0\u679c\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u6216\u7b49\u540c\u4e8eTransformer\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u56e0\u679c\u7406\u8bba\u4e0e\u795e\u7ecf\u4e16\u754c\u5efa\u6a21\u7684\u7ed3\u5408\uff0c\u7279\u522b\u662f\u8bc4\u4f30SSM\u67b6\u6784\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u672a\u80fd\u5b66\u4e60\u5230\u73af\u5883\u56e0\u679c\u8868\u793a\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5b9e\u8bc1\u6bd4\u8f83SSM\u4e0eTransformer\u5728\u7b80\u5355\u73af\u5883\u52a8\u6001\u5efa\u6a21\u548c\u56e0\u679c\u6a21\u578b\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cSSM\u5728\u5efa\u6a21\u73af\u5883\u52a8\u6001\u548c\u5b66\u4e60\u56e0\u679c\u6a21\u578b\u65b9\u9762\u6027\u80fd\u4f18\u4e8e\u6216\u7b49\u540c\u4e8eTransformer\u3002", "conclusion": "\u7ed3\u8bba\u662fSSM\u4e3a\u7ed3\u5408\u56e0\u679c\u610f\u8bc6\u7684\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u589e\u5f3a\u56e0\u679c\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.01458", "pdf": "https://arxiv.org/pdf/2505.01458", "abs": "https://arxiv.org/abs/2505.01458", "authors": ["Lik Hang Kenny Wong", "Xueyang Kang", "Kaixin Bai", "Jianwei Zhang"], "title": "A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Navigation and manipulation are core capabilities in Embodied AI, yet\ntraining agents with these capabilities in the real world faces high costs and\ntime complexity. Therefore, sim-to-real transfer has emerged as a key approach,\nyet the sim-to-real gap persists. This survey examines how physics simulators\naddress this gap by analyzing their properties overlooked in previous surveys.\nWe also analyze their features for navigation and manipulation tasks, along\nwith hardware requirements. Additionally, we offer a resource with benchmark\ndatasets, metrics, simulation platforms, and cutting-edge methods-such as world\nmodels and geometric equivariance-to help researchers select suitable tools\nwhile accounting for hardware constraints.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7bc7\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u7269\u7406\u6a21\u62df\u5668\u5728Embodied AI\u4e2d\u964d\u4f4e\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u6210\u672c\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u6a21\u62df\u5668\u7279\u6027\u3001\u786c\u4ef6\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u8d44\u6e90\u548c\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u8bad\u7ec3Embodied AI\u4ee3\u7406\u7684\u9ad8\u6210\u672c\u548c\u65f6\u95f4\u95ee\u9898\uff0c\u7f29\u5c0f\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u5206\u6790\u7269\u7406\u6a21\u62df\u5668\u7684\u7279\u6027\u3001\u5bfc\u822a\u4e0e\u64cd\u4f5c\u4efb\u52a1\u7684\u529f\u80fd\uff0c\u4ee5\u53ca\u786c\u4ef6\u9700\u6c42\uff0c\u540c\u65f6\u6574\u7406\u57fa\u51c6\u6570\u636e\u96c6\u3001\u6307\u6807\u548c\u5148\u8fdb\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u6db5\u76d6\u6a21\u62df\u5e73\u53f0\u3001\u6570\u636e\u96c6\u3001\u6307\u6807\u548c\u524d\u6cbf\u65b9\u6cd5\u7684\u8d44\u6e90\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u9009\u62e9\u5de5\u5177\u5e76\u8003\u8651\u786c\u4ef6\u9650\u5236\u3002", "conclusion": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u7269\u7406\u6a21\u62df\u5668\u5728\u7f29\u5c0fsim-to-real\u5dee\u8ddd\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u8d44\u6e90\u4ee5\u652f\u6301\u7814\u7a76\u3002"}}
{"id": "2505.01944", "pdf": "https://arxiv.org/pdf/2505.01944", "abs": "https://arxiv.org/abs/2505.01944", "authors": ["Matteo Cristani", "Guido Governatori", "Francesco Olivieri", "Monica Palmirani", "Gabriele Buriola"], "title": "Explainability by design: an experimental analysis of the legal coding process", "categories": ["cs.LO", "cs.AI", "cs.CL"], "comment": null, "summary": "Behind a set of rules in Deontic Defeasible Logic, there is a mapping process\nof normative background fragments. This process goes from text to rules and\nimplicitly encompasses an explanation of the coded fragments.\n  In this paper we deliver a methodology for \\textit{legal coding} that starts\nwith a fragment and goes onto a set of Deontic Defeasible Logic rules,\ninvolving a set of \\textit{scenarios} to test the correctness of the coded\nfragments. The methodology is illustrated by the coding process of an example\ntext. We then show the results of a series of experiments conducted with humans\nencoding a variety of normative backgrounds and corresponding cases in which we\nhave measured the efforts made in the coding process, as related to some\nmeasurable features. To process these examples, a recently developed\ntechnology, Houdini, that allows reasoning in Deontic Defeasible Logic, has\nbeen employed.\n  Finally we provide a technique to forecast time required in coding, that\ndepends on factors such as knowledge of the legal domain, knowledge of the\ncoding processes, length of the text, and a measure of \\textit{depth} that\nrefers to the length of the paths of legal references.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6cd5\u5f8b\u7f16\u7801\u65b9\u6cd5\uff0c\u5c06\u6587\u672c\u7247\u6bb5\u6620\u5c04\u4e3aDeontic Defeasible Logic\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u573a\u666f\u6d4b\u8bd5\u5176\u6b63\u786e\u6027\u3002\u5b9e\u9a8c\u6d4b\u91cf\u4e86\u7f16\u7801\u8fc7\u7a0b\u4e2d\u7684\u52aa\u529b\u4e0e\u53ef\u6d4b\u91cf\u7279\u5f81\u7684\u5173\u7cfb\uff0c\u5e76\u5229\u7528Houdini\u6280\u672f\u5904\u7406\u793a\u4f8b\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6cd5\u5f8b\u77e5\u8bc6\u3001\u7f16\u7801\u7ecf\u9a8c\u3001\u6587\u672c\u957f\u5ea6\u548c\u5f15\u7528\u6df1\u5ea6\u7b49\u56e0\u7d20\u7684\u65f6\u95f4\u9884\u6d4b\u6280\u672f\u3002", "motivation": "\u6cd5\u5f8b\u6587\u672c\u7f16\u7801\u4e3a\u903b\u8f91\u89c4\u5219\u65f6\u7f3a\u4e4f\u7cfb\u7edf\u65b9\u6cd5\uff0c\u4e14\u7f16\u7801\u6548\u7387\u548c\u51c6\u786e\u6027\u96be\u4ee5\u91cf\u5316\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u53ef\u6d4b\u8bd5\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4ece\u6587\u672c\u5230Deontic Defeasible Logic\u89c4\u5219\u7684\u6cd5\u5f8b\u7f16\u7801\u65b9\u6cd5\uff0c\u7ed3\u5408\u573a\u666f\u6d4b\u8bd5\uff0c\u5e76\u5229\u7528Houdini\u6280\u672f\u5b9e\u73b0\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7f16\u7801\u8fc7\u7a0b\u4e2d\u53ef\u6d4b\u91cf\u7279\u5f81\uff08\u5982\u6cd5\u5f8b\u77e5\u8bc6\u3001\u6587\u672c\u957f\u5ea6\u7b49\uff09\u4e0e\u7f16\u7801\u52aa\u529b\u7684\u5173\u7cfb\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7f16\u7801\u65b9\u6cd5\u53ca\u65f6\u95f4\u9884\u6d4b\u6280\u672f\uff0c\u4e3a\u6cd5\u5f8b\u903b\u8f91\u7f16\u7801\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u6846\u67b6\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2505.02094", "pdf": "https://arxiv.org/pdf/2505.02094", "abs": "https://arxiv.org/abs/2505.02094", "authors": ["Runyi Yu", "Yinhuai Wang", "Qihan Zhao", "Hok Wai Tsui", "Jingbo Wang", "Ping Tan", "Qifeng Chen"], "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4ece\u4ea4\u4e92\u6f14\u793a\uff08RLID\uff09\u4e2d\u6f14\u793a\u566a\u58f0\u548c\u8986\u76d6\u8303\u56f4\u9650\u5236\u95ee\u9898\u7684\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff1a\u7f1d\u5408\u8f68\u8ff9\u56fe\uff08STG\uff09\u548c\u72b6\u6001\u8f6c\u79fb\u573a\uff08STF\uff09\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u8f68\u8ff9\u91c7\u6837\uff08ATS\uff09\u548c\u5386\u53f2\u7f16\u7801\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4ea4\u4e92\u6f14\u793a\uff0c\u4f46\u5e38\u5bfc\u81f4\u7a00\u758f\u3001\u4e0d\u8fde\u8d2f\u548c\u566a\u58f0\u8f83\u5927\u7684\u8f68\u8ff9\uff0c\u65e0\u6cd5\u8986\u76d6\u6280\u80fd\u53d8\u5316\u548c\u8fc7\u6e21\u7684\u5168\u90e8\u8303\u56f4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7STG\u53d1\u73b0\u6f14\u793a\u6280\u80fd\u95f4\u7684\u6f5c\u5728\u8fc7\u6e21\uff0cSTF\u4e3a\u6f14\u793a\u90bb\u57df\u5185\u7684\u4efb\u610f\u72b6\u6001\u5efa\u7acb\u72ec\u7279\u8fde\u63a5\uff1b\u7ed3\u5408ATS\u52a8\u6001\u751f\u6210\u8bfe\u7a0b\u548c\u5386\u53f2\u7f16\u7801\u673a\u5236\u6765\u652f\u6301RLID\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u6536\u655b\u7a33\u5b9a\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6062\u590d\u9c81\u68d2\u6027\u65b9\u9762\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u52a8\u6001\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3RLID\u4e2d\u7684\u566a\u58f0\u548c\u8986\u76d6\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u8d8a\u53c2\u8003\u6f14\u793a\u7684\u9c81\u68d2\u6280\u80fd\u5b66\u4e60\u3002"}}
{"id": "2505.01958", "pdf": "https://arxiv.org/pdf/2505.01958", "abs": "https://arxiv.org/abs/2505.01958", "authors": ["Liqiang Jing", "Guiming Hardy Chen", "Ehsan Aghazadeh", "Xin Eric Wang", "Xinya Du"], "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks, but visual object hallucination remains a persistent issue.\nIt refers to scenarios where models generate inaccurate visual object-related\ninformation based on the query input, potentially leading to misinformation and\nconcerns about safety and reliability. Previous works focus on the evaluation\nand mitigation of visual hallucinations, but the underlying causes have not\nbeen comprehensively investigated. In this paper, we analyze each component of\nLLaVA-like LVLMs -- the large language model, the vision backbone, and the\nprojector -- to identify potential sources of error and their impact. Based on\nour observations, we propose methods to mitigate hallucination for each\nproblematic component. Additionally, we developed two hallucination benchmarks:\nQA-VisualGenome, which emphasizes attribute and relation hallucinations, and\nQA-FB15k, which focuses on cognition-based hallucinations.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u89c6\u89c9\u5bf9\u8c61\u5e7b\u89c9\u7684\u6210\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u5404\u95ee\u9898\u7ec4\u4ef6\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u4e24\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89c6\u89c9\u5bf9\u8c61\u5e7b\u89c9\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u4fe1\u606f\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002\u6b64\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bc4\u4f30\u548c\u7f13\u89e3\uff0c\u4f46\u5bf9\u6210\u56e0\u7f3a\u4e4f\u5168\u9762\u5206\u6790\u3002", "method": "\u5206\u6790LLaVA\u7c7bLVLMs\u7684\u5404\u7ec4\u4ef6\uff08\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u4e3b\u5e72\u3001\u6295\u5f71\u5668\uff09\uff0c\u8bc6\u522b\u9519\u8bef\u6765\u6e90\u5e76\u9488\u5bf9\u6027\u63d0\u51fa\u7f13\u89e3\u65b9\u6cd5\uff1b\u5f00\u53d1QA-VisualGenome\u548cQA-FB15k\u4e24\u4e2a\u5e7b\u89c9\u57fa\u51c6\u3002", "result": "\u901a\u8fc7\u7ec4\u4ef6\u5206\u6790\u53d1\u73b0\u5bfc\u81f4\u5e7b\u89c9\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7f13\u89e3\u65b9\u6cd5\uff1b\u65b0\u57fa\u51c6\u6d4b\u8bd5\u8986\u76d6\u5c5e\u6027\u548c\u5173\u7cfb\u5e7b\u89c9\u53ca\u8ba4\u77e5\u578b\u5e7b\u89c9\u3002", "conclusion": "\u5bf9LVLMs\u7ec4\u4ef6\u6df1\u5165\u5206\u6790\u4e3a\u7406\u89e3\u548c\u7f13\u89e3\u89c6\u89c9\u5bf9\u8c61\u5e7b\u89c9\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u57fa\u51c6\u6d4b\u8bd5\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u8bc4\u4f30\u3002"}}
{"id": "2505.02105", "pdf": "https://arxiv.org/pdf/2505.02105", "abs": "https://arxiv.org/abs/2505.02105", "authors": ["Pratik Shrestha", "Saran Phatharodom", "Alec Aversa", "David Blankenship", "Zhengfeng Wu", "Ioannis Savidis"], "title": "Deep Representation Learning for Electronic Design Automation", "categories": ["cs.LG"], "comment": null, "summary": "Representation learning has become an effective technique utilized by\nelectronic design automation (EDA) algorithms, which leverage the natural\nrepresentation of workflow elements as images, grids, and graphs. By addressing\nchallenges related to the increasing complexity of circuits and stringent\npower, performance, and area (PPA) requirements, representation learning\nfacilitates the automatic extraction of meaningful features from complex data\nformats, including images, grids, and graphs. This paper examines the\napplication of representation learning in EDA, covering foundational concepts\nand analyzing prior work and case studies on tasks that include timing\nprediction, routability analysis, and automated placement. Key techniques,\nincluding image-based methods, graph-based approaches, and hybrid multimodal\nsolutions, are presented to illustrate the improvements provided in routing,\ntiming, and parasitic prediction. The provided advancements demonstrate the\npotential of representation learning to enhance efficiency, accuracy, and\nscalability in current integrated circuit design flows.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8868\u793a\u5b66\u4e60\u5728EDA\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u81ea\u52a8\u63d0\u53d6\u590d\u6742\u6570\u636e\u4e2d\u7684\u7279\u5f81\uff0c\u63d0\u9ad8\u4e86\u7535\u8def\u8bbe\u8ba1\u7684\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u7535\u8def\u590d\u6742\u5ea6\u7684\u589e\u52a0\u4ee5\u53ca\u5bf9PPA\u8981\u6c42\u7684\u63d0\u9ad8\uff0c\u8868\u793a\u5b66\u4e60\u80fd\u591f\u81ea\u52a8\u4ece\u56fe\u50cf\u3001\u7f51\u683c\u548c\u56fe\u5f62\u7b49\u590d\u6742\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\uff0c\u4ee5\u89e3\u51b3EDA\u4e2d\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u3001\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u4ee5\u53ca\u6df7\u5408\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u7b49\u5173\u952e\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728\u65f6\u5e8f\u9884\u6d4b\u3001\u53ef\u5e03\u7ebf\u5206\u6790\u548c\u81ea\u52a8\u5e03\u5c40\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u8fd9\u4e9b\u6280\u672f\u8fdb\u6b65\u8868\u660e\uff0c\u8868\u793a\u5b66\u4e60\u5728\u5f53\u524d\u7684\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u80fd\u591f\u663e\u8457\u63d0\u5347\u6548\u7387\u3001\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8868\u793a\u5b66\u4e60\u5728EDA\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7535\u8def\u8bbe\u8ba1\u548c\u4e25\u683cPPA\u8981\u6c42\u7684\u6311\u6218\u3002"}}
{"id": "2505.01474", "pdf": "https://arxiv.org/pdf/2505.01474", "abs": "https://arxiv.org/abs/2505.01474", "authors": ["I. F. Serzhenko", "L. A. Khaertdinova", "M. A. Pautov", "A. V. Antsiferova"], "title": "Watermark Overwriting Attack on StegaStamp algorithm", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper presents an attack method on the StegaStamp watermarking algorithm\nthat completely removes watermarks from an image with minimal quality loss,\ndeveloped as part of the NeurIPS \"Erasing the invisible\" competition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9StegaStamp\u6c34\u5370\u7b97\u6cd5\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u53ef\u5728\u6700\u5c0f\u8d28\u91cf\u635f\u5931\u4e0b\u5b8c\u5168\u53bb\u9664\u56fe\u50cf\u4e2d\u7684\u6c34\u5370\u3002", "motivation": "\u65e8\u5728\u6d4b\u8bd5\u548c\u6311\u6218\u73b0\u6709\u6c34\u5370\u6280\u672f\u7684\u5b89\u5168\u6027\uff0c\u4f5c\u4e3aNeurIPS\u201c\u64e6\u9664\u4e0d\u53ef\u89c1\u201d\u6bd4\u8d5b\u7684\u4e00\u90e8\u5206\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u79fb\u9664StegaStamp\u6c34\u5370\u3002", "result": "\u6210\u529f\u5b8c\u5168\u53bb\u9664\u6c34\u5370\uff0c\u540c\u65f6\u5bf9\u56fe\u50cf\u9020\u6210\u7684\u8d28\u91cf\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5f53\u524d\u6c34\u5370\u6280\u672f\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3002"}}
{"id": "2505.02124", "pdf": "https://arxiv.org/pdf/2505.02124", "abs": "https://arxiv.org/abs/2505.02124", "authors": ["Samidha Verma", "Arushi Goyal", "Ananya Mathur", "Ankit Anand", "Sayan Ranu"], "title": "GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code", "categories": ["cs.LG"], "comment": null, "summary": "Graph Edit Distance (GED) is a widely used metric for measuring similarity\nbetween two graphs. Computing the optimal GED is NP-hard, leading to the\ndevelopment of various neural and non-neural heuristics. While neural methods\nhave achieved improved approximation quality compared to non-neural approaches,\nthey face significant challenges: (1) They require large amounts of ground\ntruth data, which is itself NP-hard to compute. (2) They operate as black\nboxes, offering limited interpretability. (3) They lack cross-domain\ngeneralization, necessitating expensive retraining for each new dataset. We\naddress these limitations with GRAIL, introducing a paradigm shift in this\ndomain. Instead of training a neural model to predict GED, GRAIL employs a\nnovel combination of large language models (LLMs) and automated prompt tuning\nto generate a program that is used to compute GED. This shift from predicting\nGED to generating programs imparts various advantages, including end-to-end\ninterpretability and an autonomous self-evolutionary learning mechanism without\nground-truth supervision. Extensive experiments on seven datasets confirm that\nGRAIL not only surpasses state-of-the-art GED approximation methods in\nprediction quality but also achieves robust cross-domain generalization across\ndiverse graph distributions.", "AI": {"tldr": "GRAIL\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u81ea\u52a8\u63d0\u793a\u8c03\u4f18\u751f\u6210\u7a0b\u5e8f\u6765\u8ba1\u7b97\u56fe\u7f16\u8f91\u8ddd\u79bb\uff08GED\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u6570\u636e\u9700\u6c42\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728GED\u8ba1\u7b97\u4e2d\u5b58\u5728\u6570\u636e\u9700\u6c42\u5927\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "GRAIL\u7ed3\u5408LLMs\u548c\u81ea\u52a8\u63d0\u793a\u8c03\u4f18\uff0c\u751f\u6210\u7528\u4e8e\u8ba1\u7b97GED\u7684\u7a0b\u5e8f\uff0c\u800c\u975e\u76f4\u63a5\u9884\u6d4bGED\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRAIL\u5728\u9884\u6d4b\u8d28\u91cf\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "GRAIL\u901a\u8fc7\u7a0b\u5e8f\u751f\u6210\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u548c\u65e0\u9700\u76d1\u7763\u7684\u81ea\u4e3b\u5b66\u4e60\u673a\u5236\uff0c\u4e3aGED\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01475", "pdf": "https://arxiv.org/pdf/2505.01475", "abs": "https://arxiv.org/abs/2505.01475", "authors": ["Shweta Verma", "Abhinav Anand", "Mira Mezini"], "title": "BiGSCoder: State Space Model for Code Understanding", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "We present BiGSCoder, a novel encoder-only bidirectional state-space model\n(SSM) featuring a gated architecture, pre-trained for code understanding on a\ncode dataset using masked language modeling. Our work aims to systematically\nevaluate SSMs' capabilities in coding tasks compared to traditional transformer\narchitectures; BiGSCoder is built for this purpose. Through comprehensive\nexperiments across diverse pre-training configurations and code understanding\nbenchmarks, we demonstrate that BiGSCoder outperforms transformer-based models,\ndespite utilizing simpler pre-training strategies and much less training data.\nOur results indicate that BiGSCoder can serve as a more sample-efficient\nalternative to conventional transformer models. Furthermore, our study shows\nthat SSMs perform better without positional embeddings and can effectively\nextrapolate to longer sequences during fine-tuning.", "AI": {"tldr": "BiGSCoder\u662f\u4e00\u79cd\u65b0\u578b\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfTransformer\u6a21\u578b\uff0c\u6837\u672c\u6548\u7387\u66f4\u9ad8\u4e14\u65e0\u9700\u4f4d\u7f6e\u5d4c\u5165\u3002", "motivation": "\u7cfb\u7edf\u8bc4\u4f30\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4f20\u7edfTransformer\u67b6\u6784\u5bf9\u6bd4\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u95e8\u63a7\u67b6\u6784\u7684\u53cc\u5411SSM\uff0c\u901a\u8fc7\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u5728\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u9884\u8bad\u7ec3\u914d\u7f6e\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "BiGSCoder\u5728\u4ee3\u7801\u7406\u89e3\u4efb\u52a1\u4e2d\u4f18\u4e8eTransformer\uff0c\u8bad\u7ec3\u6570\u636e\u66f4\u5c11\u4e14\u65e0\u9700\u4f4d\u7f6e\u5d4c\u5165\uff0c\u80fd\u6709\u6548\u6269\u5c55\u5230\u957f\u5e8f\u5217\u3002", "conclusion": "BiGSCoder\u662fTransformer\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6837\u672c\u6548\u7387\u548c\u957f\u5e8f\u5217\u5904\u7406\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.02199", "pdf": "https://arxiv.org/pdf/2505.02199", "abs": "https://arxiv.org/abs/2505.02199", "authors": ["Manak Raj", "Nidhi Mishra"], "title": "Exploring new Approaches for Information Retrieval through Natural Language Processing", "categories": ["cs.IR", "cs.CL", "68T50", "H.3.3; I.2.7"], "comment": "12 pages, 4 figures, comprehensive literature review covering six key\n  IR-NLP papers, plus keywords and full reference list", "summary": "This review paper explores recent advancements and emerging approaches in\nInformation Retrieval (IR) applied to Natural Language Processing (NLP). We\nexamine traditional IR models such as Boolean, vector space, probabilistic, and\ninference network models, and highlight modern techniques including deep\nlearning, reinforcement learning, and pretrained transformer models like BERT.\nWe discuss key tools and libraries - Lucene, Anserini, and Pyserini - for\nefficient text indexing and search. A comparative analysis of sparse, dense,\nand hybrid retrieval methods is presented, along with applications in web\nsearch engines, cross-language IR, argument mining, private information\nretrieval, and hate speech detection. Finally, we identify open challenges and\nfuture research directions to enhance retrieval accuracy, scalability, and\nethical considerations.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u63a2\u8ba8\u4e86\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u65b0\u5174\u65b9\u6cd5\uff0c\u6db5\u76d6\u4f20\u7edf\u4e0e\u73b0\u4ee3\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u7a00\u758f\u3001\u5bc6\u96c6\u548c\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff0c\u540c\u65f6\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740NLP\u548c\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7814\u7a76\u8005\u9700\u8981\u5168\u9762\u4e86\u89e3\u4f20\u7edf\u4e0e\u73b0\u4ee3IR\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7ed3\u5408\u5b83\u4eec\u4ee5\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u8bba\u6587\u56de\u987e\u4e86\u4f20\u7edfIR\u6a21\u578b\uff08\u5982\u5e03\u5c14\u6a21\u578b\u548c\u5411\u91cf\u7a7a\u95f4\u6a21\u578b\uff09\uff0c\u4ee5\u53ca\u73b0\u4ee3\u6280\u672f\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u53d8\u6362\u5668\u6a21\u578b\uff09\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u5de5\u5177\u7684\u5e94\u7528\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u7a00\u758f\u3001\u5bc6\u96c6\u548c\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u4e0d\u540c\u6280\u672f\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\uff08\u5982\u7f51\u7edc\u641c\u7d22\u5f15\u64ce\u548c\u8de8\u8bed\u8a00IR\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6307\u51fa\u4e86\u5404\u81ea\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u5f53\u524dIR\u6280\u672f\u7684\u8fdb\u5c55\uff0c\u5f3a\u8c03\u4e86\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u4f26\u7406\u8003\u91cf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2505.02138", "pdf": "https://arxiv.org/pdf/2505.02138", "abs": "https://arxiv.org/abs/2505.02138", "authors": ["Chenxi Liu", "Hao Miao", "Qianxiong Xu", "Shaowen Zhou", "Cheng Long", "Yan Zhao", "Ziyue Li", "Rui Zhao"], "title": "Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation", "categories": ["cs.LG"], "comment": "Accepted by ICDE 2025", "summary": "Multivariate time series forecasting (MTSF) endeavors to predict future\nobservations given historical data, playing a crucial role in time series data\nmanagement systems. With advancements in large language models (LLMs), recent\nstudies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.\nHowever, the deployment of LLMs often suffers from low efficiency during the\ninference phase. To address this problem, we introduce TimeKD, an efficient\nMTSF framework that leverages the calibrated language models and privileged\nknowledge distillation. TimeKD aims to generate high-quality future\nrepresentations from the proposed cross-modality teacher model and cultivate an\neffective student model. The cross-modality teacher model adopts calibrated\nlanguage models (CLMs) with ground truth prompts, motivated by the paradigm of\nLearning Under Privileged Information (LUPI). In addition, we design a\nsubtractive cross attention (SCA) mechanism to refine these representations. To\ncultivate an effective student model, we propose an innovative privileged\nknowledge distillation (PKD) mechanism including correlation and feature\ndistillation. PKD enables the student to replicate the teacher's behavior while\nminimizing their output discrepancy. Extensive experiments on real data offer\ninsight into the effectiveness, efficiency, and scalability of the proposed\nTimeKD.", "AI": {"tldr": "TimeKD\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u8de8\u6a21\u6001\u6559\u5e08\u6a21\u578b\u63d0\u5347\u9884\u6d4b\u6548\u7387\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5b58\u5728\u63a8\u7406\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0cTimeKD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6821\u51c6\u8bed\u8a00\u6a21\u578b\uff08CLMs\uff09\u548c\u7279\u6743\u77e5\u8bc6\u84b8\u998f\uff08PKD\uff09\uff0c\u7ed3\u5408\u51cf\u6cd5\u4ea4\u53c9\u6ce8\u610f\u529b\uff08SCA\uff09\u673a\u5236\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTimeKD\u5728\u6548\u679c\u3001\u6548\u7387\u548c\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "TimeKD\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u8de8\u6a21\u6001\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2505.01476", "pdf": "https://arxiv.org/pdf/2505.01476", "abs": "https://arxiv.org/abs/2505.01476", "authors": ["Zhe Zhang", "Mingxiu Cai", "Hanxiao Wang", "Gaochang Wu", "Tianyou Chai", "Xiatian Zhu"], "title": "CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures, 10 tables, accepted by Forty-Second\n  International Conference on Machine Learning ( ICML 2025 )", "summary": "Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an\ninput image with respect to normal samples. Either by reconstructing normal\ncounterparts (reconstruction-based) or by learning an image feature embedding\nspace (embedding-based), existing approaches fundamentally rely on image-level\nor feature-level matching to derive anomaly scores. Often, such a matching\nprocess is inaccurate yet overlooked, leading to sub-optimal detection. To\naddress this issue, we introduce the concept of cost filtering, borrowed from\nclassical matching tasks, such as depth and flow estimation, into the UAD\nproblem. We call this approach {\\em CostFilter-AD}. Specifically, we first\nconstruct a matching cost volume between the input and normal samples,\ncomprising two spatial dimensions and one matching dimension that encodes\npotential matches. To refine this, we propose a cost volume filtering network,\nguided by the input observation as an attention query across multiple feature\nlayers, which effectively suppresses matching noise while preserving edge\nstructures and capturing subtle anomalies. Designed as a generic\npost-processing plug-in, CostFilter-AD can be integrated with either\nreconstruction-based or embedding-based methods. Extensive experiments on\nMVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for\nboth single- and multi-class UAD tasks. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCostFilter-AD\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6210\u672c\u8fc7\u6ee4\u6982\u5ff5\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u5339\u914d\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u56fe\u50cf\u6216\u7279\u5f81\u7ea7\u522b\u5339\u914d\uff0c\u4f46\u8fd9\u4e00\u8fc7\u7a0b\u5e38\u4e0d\u7cbe\u786e\u4e14\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6210\u672c\u8fc7\u6ee4\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5339\u914d\u6210\u672c\u4f53\u79ef\uff0c\u5e76\u901a\u8fc7\u6210\u672c\u4f53\u79ef\u8fc7\u6ee4\u7f51\u7edc\u8fdb\u884c\u7ec6\u5316\uff0c\u5229\u7528\u8f93\u5165\u89c2\u6d4b\u4f5c\u4e3a\u6ce8\u610f\u529b\u67e5\u8be2\u6765\u6291\u5236\u566a\u58f0\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u901a\u7528\u540e\u5904\u7406\u63d2\u4ef6\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5728MVTec-AD\u548cVisA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCostFilter-AD\u5728\u5355\u7c7b\u548c\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u901a\u7528\u4f18\u52bf\u3002", "conclusion": "CostFilter-AD\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u540e\u5904\u7406\u63d2\u4ef6\uff0c\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5339\u914d\u566a\u58f0\u6291\u5236\u548c\u8fb9\u7f18\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.02206", "pdf": "https://arxiv.org/pdf/2505.02206", "abs": "https://arxiv.org/abs/2505.02206", "authors": ["Lei Mao", "Yuanhe Tian", "Yan Song"], "title": "DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages, 3 figures", "summary": "Genome modeling conventionally treats gene sequence as a language, reflecting\nits structured motifs and long-range dependencies analogous to linguistic units\nand organization principles such as words and syntax. Recent studies utilize\nadvanced neural networks, ranging from convolutional and recurrent models to\nTransformer-based models, to capture contextual information of gene sequence,\nwith the primary goal of obtaining effective gene sequence representations and\nthus enhance the models' understanding of various running gene samples.\nHowever, these approaches often directly apply language modeling techniques to\ngene sequences and do not fully consider the intrinsic information organization\nin them, where they do not consider how units at different granularities\ncontribute to representation. In this paper, we propose DNAZEN, an enhanced\ngenomic representation framework designed to learn from various granularities\nin gene sequences, including small polymers and G-grams that are combinations\nof several contiguous polymers. Specifically, we extract the G-grams from\nlarge-scale genomic corpora through an unsupervised approach to construct the\nG-gram vocabulary, which is used to provide G-grams in the learning process of\nDNA sequences through dynamically matching from running gene samples. A\nTransformer-based G-gram encoder is also proposed and the matched G-grams are\nfed into it to compute their representations and integrated into the encoder\nfor basic unit (E4BU), which is responsible for encoding small units and\nmaintaining the learning and inference process. To further enhance the learning\nprocess, we propose whole G-gram masking to train DNAZEN, where the model\nlargely favors the selection of each entire G-gram to mask rather than an\nordinary masking mechanism performed on basic units. Experiments on benchmark\ndatasets demonstrate the effectiveness of DNAZEN on various downstream tasks.", "AI": {"tldr": "\u63d0\u51faDNAZEN\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u5b66\u4e60\u57fa\u56e0\u5e8f\u5217\u8868\u793a\uff0c\u5305\u62ec\u5c0f\u805a\u5408\u7269\u548cG-grams\uff08\u591a\u805a\u5408\u7269\u7ec4\u5408\uff09\uff0c\u5e76\u91c7\u7528Transformer\u7f16\u7801\u5668\u548c\u6574\u4f53G-gram\u63a9\u7801\u8bad\u7ec3\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8bed\u8a00\u5efa\u6a21\u6280\u672f\u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u56e0\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5e8f\u5217\u5185\u90e8\u591a\u7c92\u5ea6\u4fe1\u606f\u7684\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u5bfc\u81f4\u8868\u793a\u5b66\u4e60\u4e0d\u5145\u5206\u3002", "method": "\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u6784\u5efaG-gram\u8bcd\u6c47\u8868\uff0c\u8bbe\u8ba1Transformer-based\u7684G-gram\u7f16\u7801\u5668\uff0c\u5e76\u91c7\u7528\u6574\u4f53G-gram\u63a9\u7801\u8bad\u7ec3\u6a21\u578bDNAZEN\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DNAZEN\u5728\u591a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DNAZEN\u901a\u8fc7\u591a\u7c92\u5ea6\u5b66\u4e60\u548c\u65b0\u578b\u63a9\u7801\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u56e0\u5e8f\u5217\u8868\u793a\u7684\u8d28\u91cf\u548c\u5e94\u7528\u6027\u80fd\u3002"}}
{"id": "2505.02147", "pdf": "https://arxiv.org/pdf/2505.02147", "abs": "https://arxiv.org/abs/2505.02147", "authors": ["Prajwal Thapa", "Mridul Sharma", "Jinu Nyachhyon", "Yagya Raj Pandeya"], "title": "Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora", "categories": ["cs.LG", "cs.CV", "I.4.9"], "comment": "12 pages, 6 figures, 5 tables", "summary": "Herb classification presents a critical challenge in botanical research,\nparticularly in regions with rich biodiversity such as Nepal. This study\nintroduces a novel deep learning approach for classifying 60 different herb\nspecies using Convolutional Neural Networks (CNNs) and transfer learning\ntechniques. Using a manually curated dataset of 12,000 herb images, we\ndeveloped a robust machine learning model that addresses existing limitations\nin herb recognition methodologies. Our research employed multiple model\narchitectures, including DenseNet121, 50-layer Residual Network (ResNet50),\n16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,\nand Vision Transformer (VIT), with DenseNet121 ultimately demonstrating\nsuperior performance. Data augmentation and regularization techniques were\napplied to mitigate overfitting and enhance the generalizability of the model.\nThis work advances herb classification techniques, preserving traditional\nbotanical knowledge and promoting sustainable herb utilization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7CNN\u5bf960\u79cd\u8349\u672c\u690d\u7269\u8fdb\u884c\u5206\u7c7b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b12,000\u5f20\u56fe\u50cf\u7684\u9c81\u68d2\u6a21\u578b\uff0c\u6700\u7ec8DenseNet121\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8349\u672c\u690d\u7269\u5206\u7c7b\u5728\u751f\u7269\u591a\u6837\u6027\u4e30\u5bcc\u7684\u5730\u533a\uff08\u5982\u5c3c\u6cca\u5c14\uff09\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u8349\u672c\u690d\u7269\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff08\u5982DenseNet121\u3001ResNet50\u3001VGG16\u7b49\uff09\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u6b63\u5219\u5316\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "DenseNet121\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8349\u672c\u690d\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u8349\u672c\u690d\u7269\u5206\u7c7b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6709\u52a9\u4e8e\u4f20\u7edf\u690d\u7269\u77e5\u8bc6\u7684\u4fdd\u5b58\u548c\u53ef\u6301\u7eed\u5229\u7528\u3002"}}
{"id": "2505.01514", "pdf": "https://arxiv.org/pdf/2505.01514", "abs": "https://arxiv.org/abs/2505.01514", "authors": ["Khushbu Mehboob Shaikh", "Georgios Giannakopoulos"], "title": "Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "7 pages, 1 figure, 2 tables", "summary": "The rapid digitalization of communication systems has elevated Interactive\nVoice Response (IVR) technologies to become critical interfaces for customer\nengagement. With Artificial Intelligence (AI) now driving these platforms,\nensuring secure, compliant, and ethically designed development practices is\nmore imperative than ever. AI-powered IVRs leverage Natural Language Processing\n(NLP) and Machine Learning (ML) to personalize interactions, automate service\ndelivery, and optimize user experiences. However, these innovations expose\nsystems to heightened risks, including data privacy breaches, AI decision\nopacity, and model security vulnerabilities. This paper analyzes the evolution\nof IVRs from static code-based designs to adaptive AI-driven systems,\npresenting a cybersecurity-centric perspective. We propose a practical\ngovernance framework that embeds agile security principles, compliance with\nglobal data legislation, and user-centric ethics. Emphasizing\nprivacy-by-design, adaptive risk modeling, and transparency, the paper argues\nthat ethical AI integration is not a feature but a strategic imperative.\nThrough this multidimensional lens, we highlight how modern IVRs can transition\nfrom communication tools to intelligent, secure, and accountable digital\nfrontlines-resilient against emerging threats and aligned with societal\nexpectations.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u8ba8\u8bba\u4e86AI\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u8bed\u97f3\u54cd\u5e94\uff08IVR\uff09\u6280\u672f\u7684\u5feb\u901f\u6570\u5b57\u5316\u53d1\u5c55\uff0c\u5f3a\u8c03\u4e86\u5176\u4e2a\u6027\u5316\u3001\u81ea\u52a8\u5316\u548c\u4f18\u5316\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u6570\u636e\u9690\u79c1\u3001\u51b3\u7b56\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u7b49\u98ce\u9669\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u7f51\u7edc\u5b89\u5168\u4e3a\u4e2d\u5fc3\u7684\u6cbb\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u654f\u6377\u5b89\u5168\u539f\u5219\u3001\u5168\u7403\u6570\u636e\u6cd5\u89c4\u5408\u89c4\u6027\u548c\u7528\u6237\u4f26\u7406\uff0c\u65e8\u5728\u5b9e\u73b0\u9690\u79c1\u8bbe\u8ba1\u3001\u81ea\u9002\u5e94\u98ce\u9669\u5efa\u6a21\u548c\u900f\u660e\u5ea6\u3002\u901a\u8fc7\u591a\u7ef4\u5ea6\u5206\u6790\uff0c\u8bba\u6587\u4e3b\u5f20\u5c06\u4f26\u7406AI\u96c6\u6210\u89c6\u4e3a\u6218\u7565\u8981\u52a1\uff0c\u4ee5\u4f7f\u73b0\u4ee3IVR\u6280\u672f\u4e0d\u4ec5\u6210\u4e3a\u901a\u4fe1\u5de5\u5177\uff0c\u66f4\u662f\u667a\u80fd\u3001\u5b89\u5168\u4e14\u8d1f\u8d23\u4efb\u7684\u6570\u5b57\u524d\u7ebf\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u9a71\u52a8\u7684IVR\u7cfb\u7edf\u5728\u5ba2\u6237\u4e92\u52a8\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u786e\u4fdd\u8fd9\u4e9b\u6280\u672f\u7684\u5b89\u5168\u6027\u3001\u5408\u89c4\u6027\u548c\u7b26\u5408\u4f26\u7406\u7684\u8bbe\u8ba1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u5206\u6790IVR\u7cfb\u7edf\u4ece\u9759\u6001\u4ee3\u7801\u8bbe\u8ba1\u5230AI\u9a71\u52a8\u7cfb\u7edf\u7684\u6f14\u53d8\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u591a\u89d2\u5ea6\u7684\u6cbb\u7406\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u9690\u79c1\u3001\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u7b49\u6311\u6218\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790IVR\u6280\u672f\u7684\u6f14\u53d8\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7f51\u7edc\u5b89\u5168\u4e3a\u4e2d\u5fc3\u7684\u6cbb\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u654f\u6377\u5b89\u5168\u539f\u5219\u3001\u5168\u7403\u6570\u636e\u6cd5\u89c4\u5408\u89c4\u6027\u548c\u7528\u6237\u4f26\u7406\u8bbe\u8ba1\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u9690\u79c1\u8bbe\u8ba1\u3001\u81ea\u9002\u5e94\u98ce\u9669\u5efa\u6a21\u548c\u900f\u660e\u5ea6\u673a\u5236\u7684\u5e94\u7528\u3002", "result": "\u8bba\u6587\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5b9e\u65bd\u6240\u63d0\u51fa\u7684\u6846\u67b6\uff0cAI\u9a71\u52a8\u7684IVR\u7cfb\u7edf\u53ef\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3001\u900f\u660e\u5ea6\u548c\u7528\u6237\u4fe1\u4efb\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5e94\u5bf9\u65b0\u5174\u5a01\u80c1\u5e76\u6ee1\u8db3\u793e\u4f1a\u671f\u671b\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u4f26\u7406AI\u96c6\u6210\u4e0d\u4ec5\u662fIVR\u7cfb\u7edf\u7684\u4e00\u4e2a\u529f\u80fd\uff0c\u66f4\u662f\u4e00\u9879\u6218\u7565\u8981\u52a1\u3002\u901a\u8fc7\u591a\u7ef4\u5ea6\u7684\u6846\u67b6\uff0c\u73b0\u4ee3IVR\u6280\u672f\u53ef\u4ee5\u53d1\u5c55\u4e3a\u667a\u80fd\u3001\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u7684\u6570\u5b57\u524d\u7ebf\uff0c\u4ece\u800c\u5728\u5feb\u901f\u6570\u5b57\u5316\u7684\u901a\u4fe1\u7cfb\u7edf\u4e2d\u53d1\u6325\u66f4\u5927\u7684\u4f5c\u7528\u3002"}}
{"id": "2505.02181", "pdf": "https://arxiv.org/pdf/2505.02181", "abs": "https://arxiv.org/abs/2505.02181", "authors": ["Shengyu Duan", "Marcos L. L. Sartori", "Rishad Shafik", "Alex Yakovlev", "Emre Ozer"], "title": "Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Population count (popcount) is a crucial operation for many low-complexity\nmachine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising\nnew ML method, particularly well-suited for solving classification tasks. The\ninference mechanism in TM consists of propositional logic-based structures\nwithin each class, followed by a majority voting scheme, which makes the\nclassification decision. In TM, the voters are the outputs of Boolean clauses.\nThe voting mechanism comprises two operations: popcount for each class and\ndetermining the class with the maximum vote by means of an argmax operation.\n  While TMs offer a lightweight ML alternative, their performance is often\nlimited by the high computational cost of popcount and comparison required to\nproduce the argmax result. In this paper, we propose an innovative approach to\naccelerate and optimize these operations by performing them in the time domain.\nOur time-domain implementation uses programmable delay lines (PDLs) and\narbiters to efficiently manage these tasks through delay-based mechanisms. We\nalso present an FPGA design flow for practical implementation of the\ntime-domain popcount, addressing delay skew and ensuring that the behavior\nmatches that of the model's intended functionality. By leveraging the natural\ncompatibility of the proposed popcount with asynchronous architectures, we\ndemonstrate significant improvements in an asynchronous TM, including up to 38%\nreduction in latency, 43.1% reduction in dynamic power, and 15% savings in\nresource utilization, compared to synchronous TMs using adder-based popcount.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u57df\u7684\u65b0\u578b\u65b9\u6cd5\u6765\u52a0\u901f\u548c\u4f18\u5316Tsetlin Machine\uff08TM\uff09\u4e2d\u7684popcount\u548cargmax\u64cd\u4f5c\uff0c\u901a\u8fc7\u53ef\u7f16\u7a0b\u5ef6\u8fdf\u7ebf\uff08PDLs\uff09\u548c\u4ef2\u88c1\u5668\u5b9e\u73b0\u3002FPGA\u8bbe\u8ba1\u6d41\u7a0b\u786e\u4fdd\u4e86\u529f\u80fd\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u6b65TM\u7684\u6027\u80fd\u3002", "motivation": "Tsetlin Machine\uff08TM\uff09\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5176\u5206\u7c7b\u6027\u80fd\u53d7\u9650\u4e8epopcount\u548c\u6bd4\u8f83\u64cd\u4f5c\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65f6\u57df\u5b9e\u73b0\u4f18\u5316\u8fd9\u4e9b\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u65f6\u57df\u5b9e\u73b0\u65b9\u6848\uff0c\u5229\u7528\u53ef\u7f16\u7a0b\u5ef6\u8fdf\u7ebf\uff08PDLs\uff09\u548c\u4ef2\u88c1\u5668\u6765\u7ba1\u7406popcount\u548cargmax\u64cd\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86FPGA\u5b9e\u73b0\u6d41\u7a0b\uff0c\u89e3\u51b3\u5ef6\u8fdf\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5728\u5f02\u6b65TM\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff1a\u5ef6\u8fdf\u964d\u4f4e38%\uff0c\u52a8\u6001\u529f\u8017\u51cf\u5c1143.1%\uff0c\u8d44\u6e90\u5229\u7528\u7387\u8282\u770115%\u3002", "conclusion": "\u65f6\u57dfpopcount\u4e0e\u5f02\u6b65\u67b6\u6784\u5929\u7136\u517c\u5bb9\uff0c\u4e3aTM\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2505.02309", "pdf": "https://arxiv.org/pdf/2505.02309", "abs": "https://arxiv.org/abs/2505.02309", "authors": ["Sanjay Surendranath Girija", "Shashank Kapoor", "Lakshit Arora", "Dipen Pradhan", "Aman Raj", "Ankit Shetgaonkar"], "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to IEEE COMPSAC 2025", "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u603b\u7ed3\u4e86\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u63a8\u7406\u7684\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u77e5\u8bc6\u84b8\u998f\u3001\u6a21\u578b\u91cf\u5316\u548c\u6a21\u578b\u526a\u679d\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e92\u8865\u6280\u672f\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7531\u4e8eLLMs\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u672c\u6587\u65e8\u5728\u7efc\u8ff0\u538b\u7f29\u6280\u672f\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u5206\u6790\u4e86\u77e5\u8bc6\u84b8\u998f\u3001\u6a21\u578b\u91cf\u5316\u548c\u6a21\u578b\u526a\u679d\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u6df7\u5408\u4e13\u5bb6\u548c\u65e9\u671f\u9000\u51fa\u7b49\u4e92\u8865\u6280\u672f\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86\u6bcf\u79cd\u6280\u672f\u7684\u539f\u7406\u3001\u53d8\u4f53\u548c\u6210\u529f\u5e94\u7528\u6848\u4f8b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684LLM\u4f18\u5316\u63d0\u4f9b\u4e86\u5168\u9762\u6307\u5bfc\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u5728\u8fb9\u7f18\u90e8\u7f72LLM\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.01524", "pdf": "https://arxiv.org/pdf/2505.01524", "abs": "https://arxiv.org/abs/2505.01524", "authors": ["Zexi Yao", "Nata\u0161a Kr\u010do", "Georgi Ganev", "Yves-Alexandre de Montjoye"], "title": "The DCR Delusion: Measuring the Privacy Risk of Synthetic Data", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Synthetic data has become an increasingly popular way to share data without\nrevealing sensitive information. Though Membership Inference Attacks (MIAs) are\nwidely considered the gold standard for empirically assessing the privacy of a\nsynthetic dataset, practitioners and researchers often rely on simpler proxy\nmetrics such as Distance to Closest Record (DCR). These metrics estimate\nprivacy by measuring the similarity between the training data and generated\nsynthetic data. This similarity is also compared against that between the\ntraining data and a disjoint holdout set of real records to construct a binary\nprivacy test. If the synthetic data is not more similar to the training data\nthan the holdout set is, it passes the test and is considered private. In this\nwork we show that, while computationally inexpensive, DCR and other\ndistance-based metrics fail to identify privacy leakage. Across multiple\ndatasets and both classical models such as Baynet and CTGAN and more recent\ndiffusion models, we show that datasets deemed private by proxy metrics are\nhighly vulnerable to MIAs. We similarly find both the binary privacy test and\nthe continuous measure based on these metrics to be uninformative of actual\nmembership inference risk. We further show that these failures are consistent\nacross different metric hyperparameter settings and record selection methods.\nFinally, we argue DCR and other distance-based metrics to be flawed by design\nand show a example of a simple leakage they miss in practice. With this work,\nwe hope to motivate practitioners to move away from proxy metrics to MIAs as\nthe rigorous, comprehensive standard of evaluating privacy of synthetic data,\nin particular to make claims of datasets being legally anonymous.", "AI": {"tldr": "\u5408\u6210\u6570\u636e\u5e38\u7528\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u5e38\u7528\u7684\u8ddd\u79bb\u5ea6\u91cf\uff08\u5982DCR\uff09\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u9690\u79c1\u6cc4\u9732\uff0c\u5bb9\u6613\u88ab\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u7a81\u7834\uff0c\u56e0\u6b64\u9700\u91c7\u7528MIA\u4f5c\u4e3a\u66f4\u4e25\u683c\u7684\u9690\u79c1\u8bc4\u4f30\u6807\u51c6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5f53\u524d\u5408\u6210\u6570\u636e\u9690\u79c1\u8bc4\u4f30\u4e2d\u4f7f\u7528\u7684\u7b80\u5355\u4ee3\u7406\u6307\u6807\uff08\u5982DCR\uff09\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u5176\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u9690\u79c1\u98ce\u9669\uff0c\u4ece\u800c\u63a8\u52a8\u91c7\u7528\u66f4\u4e25\u683c\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u591a\u7ec4\u6570\u636e\u96c6\u53ca\u591a\u79cd\u751f\u6210\u6a21\u578b\uff08\u5982Baynet\u3001CTGAN\u548c\u6269\u6563\u6a21\u578b\uff09\uff0c\u9a8c\u8bc1\u8ddd\u79bb\u5ea6\u91cf\uff08DCR\u7b49\uff09\u7684\u5931\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e8c\u8fdb\u5236\u9690\u79c1\u6d4b\u8bd5\u548c\u8fde\u7eed\u5ea6\u91cf\u7684\u65e0\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u88ab\u4ee3\u7406\u6307\u6807\u5224\u5b9a\u4e3a\u9690\u79c1\u7684\u5408\u6210\u6570\u636e\u5b9e\u9645\u6781\u6613\u53d7\u5230MIA\u653b\u51fb\uff0c\u4e14\u8be5\u95ee\u9898\u5728\u4e0d\u540c\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4e2d\u666e\u904d\u5b58\u5728\u3002", "conclusion": "\u8ddd\u79bb\u5ea6\u91cf\u6307\u6807\u5b58\u5728\u8bbe\u8ba1\u7f3a\u9677\uff0c\u5efa\u8bae\u5f7b\u5e95\u5f03\u7528\u6b64\u7c7b\u4ee3\u7406\u6307\u6807\uff0c\u8f6c\u800c\u4f7f\u7528MIA\u4f5c\u4e3a\u5408\u6210\u6570\u636e\u9690\u79c1\u8bc4\u4f30\u7684\u91d1\u6807\u51c6\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u6cd5\u5f8b\u533f\u540d\u6027\u58f0\u660e\u65f6\u3002"}}
{"id": "2505.02391", "pdf": "https://arxiv.org/pdf/2505.02391", "abs": "https://arxiv.org/abs/2505.02391", "authors": ["Jiarui Yao", "Yifan Hao", "Hanning Zhang", "Hanze Dong", "Wei Xiong", "Nan Jiang", "Tong Zhang"], "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGVM-RAFT\u7684\u52a8\u6001\u6837\u672c\u5206\u914d\u7b56\u7565\uff0c\u65e8\u5728\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684Chain-of-thought\u63a8\u7406\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684Chain-of-thought\u63a8\u7406\u65b9\u6cd5\uff08\u5982RAFT\uff09\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u7edf\u4e00\u7684\u63a8\u7406\u9884\u7b97\uff0c\u672a\u80fd\u8003\u8651\u5230\u4e0d\u540c\u63d0\u793a\u7684\u96be\u5ea6\u5dee\u5f02\u548c\u6536\u655b\u884c\u4e3a\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86GVM-RAFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u76d1\u6d4b\u63d0\u793a\u63a5\u53d7\u7387\u548c\u968f\u673a\u68af\u5ea6\u8303\u6570\uff0c\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u4ee5\u6700\u5c0f\u5316\u68af\u5ea6\u65b9\u5dee\uff0c\u4ece\u800c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGVM-RAFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u6bd4vanilla RAFT\u63d0\u901f2-4\u500d\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u4e5f\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\uff09\u3002", "conclusion": "GVM-RAFT\u901a\u8fc7\u52a8\u6001\u6837\u672c\u5206\u914d\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86CoT\u8bad\u7ec3\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u95ee\u9898\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\uff0c\u8fd8\u5177\u5907\u901a\u7528\u6027\u3002"}}
{"id": "2505.02212", "pdf": "https://arxiv.org/pdf/2505.02212", "abs": "https://arxiv.org/abs/2505.02212", "authors": ["Yikang Chen", "Dehui Du"], "title": "Exogenous Isomorphism for Counterfactual Identifiability", "categories": ["cs.LG", "stat.ML"], "comment": "43 pages, 4 figures. Accepted at ICML 2025 (Spotlight poster)", "summary": "This paper investigates $\\sim_{\\mathcal{L}_3}$-identifiability, a form of\ncomplete counterfactual identifiability within the Pearl Causal Hierarchy (PCH)\nframework, ensuring that all Structural Causal Models (SCMs) satisfying the\ngiven assumptions provide consistent answers to all causal questions. To\nsimplify this problem, we introduce exogenous isomorphism and propose\n$\\sim_{\\mathrm{EI}}$-identifiability, reflecting the strength of model\nidentifiability required for $\\sim_{\\mathcal{L}_3}$-identifiability. We explore\nsufficient assumptions for achieving $\\sim_{\\mathrm{EI}}$-identifiability in\ntwo special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual\ntransport, and Triangular Monotonic SCMs (TM-SCMs), which extend\n$\\sim_{\\mathcal{L}_2}$-identifiability. Our results unify and generalize\nexisting theories, providing theoretical guarantees for practical applications.\nFinally, we leverage neural TM-SCMs to address the consistency problem in\ncounterfactual reasoning, with experiments validating both the effectiveness of\nour method and the correctness of the theory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728Pearl\u56e0\u679c\u5c42\u6b21\u7ed3\u6784\uff08PCH\uff09\u6846\u67b6\u4e0b\u7684$\u223c_{\\mathcal{L}_3}$-\u53ef\u8bc6\u522b\u6027\uff0c\u63d0\u51fa\u5916\u6e90\u6027\u540c\u6784\u548c$\u223c_{\\mathrm{EI}}$-\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4e24\u7c7b\u7279\u6b8aSCMs\u4e2d\u7684\u5145\u5206\u5047\u8bbe\u3002", "motivation": "\u52a8\u673a\u662f\u7b80\u5316\u56e0\u679c\u95ee\u9898\uff0c\u786e\u4fdd\u6240\u6709\u6ee1\u8db3\u5047\u8bbe\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCMs\uff09\u80fd\u4e3a\u6240\u6709\u56e0\u679c\u95ee\u9898\u63d0\u4f9b\u4e00\u81f4\u7b54\u6848\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u5916\u6e90\u6027\u540c\u6784\u548c\u63d0\u51fa$\\sim_{\\mathrm{EI}}$-\u53ef\u8bc6\u522b\u6027\uff0c\u5e76\u7814\u7a76\u5176\u5728Bijective SCMs\uff08BSCMs\uff09\u548cTriangular Monotonic SCMs\uff08TM-SCMs\uff09\u4e2d\u7684\u5b9e\u73b0\u5047\u8bbe\u3002", "result": "\u7ed3\u679c\u7edf\u4e00\u5e76\u63a8\u5e7f\u4e86\u73b0\u6709\u7406\u8bba\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u795e\u7ecfTM-SCMs\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e$\u223c_{\\mathcal{L}_3}$-\u53ef\u8bc6\u522b\u6027\u53ca\u5176\u884d\u751f\u7406\u8bba\u5728\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u5b9e\u8df5\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u548c\u7406\u8bba\u610f\u4e49\u3002"}}
{"id": "2505.01530", "pdf": "https://arxiv.org/pdf/2505.01530", "abs": "https://arxiv.org/abs/2505.01530", "authors": ["Muhammad Tayyab Khan", "Zane Yong", "Lequn Chen", "Jun Ming Tan", "Wenhe Feng", "Seung Ki Moon"], "title": "Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been submitted to the IEEE International Conference on\n  Industrial Engineering and Engineering Management (IEEM 2025)", "summary": "Accurate extraction of key information from 2D engineering drawings is\ncrucial for high-precision manufacturing. Manual extraction is time-consuming\nand error-prone, while traditional Optical Character Recognition (OCR)\ntechniques often struggle with complex layouts and overlapping symbols,\nresulting in unstructured outputs. To address these challenges, this paper\nproposes a novel hybrid deep learning framework for structured information\nextraction by integrating an oriented bounding box (OBB) detection model with a\ntransformer-based document parsing model (Donut). An in-house annotated dataset\nis used to train YOLOv11 for detecting nine key categories: Geometric\nDimensioning and Tolerancing (GD&T), General Tolerances, Measures, Materials,\nNotes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are\ncropped into images and labeled to fine-tune Donut for structured JSON output.\nFine-tuning strategies include a single model trained across all categories and\ncategory-specific models. Results show that the single model consistently\noutperforms category-specific ones across all evaluation metrics, achieving\nhigher precision (94.77% for GD&T), recall (100% for most), and F1 score\n(97.3%), while reducing hallucination (5.23%). The proposed framework improves\naccuracy, reduces manual effort, and supports scalable deployment in\nprecision-driven industries.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408OBB\u68c0\u6d4b\u4e0eTransformer\u6a21\u578bDonut\uff0c\u5b9e\u73b0\u5de5\u7a0b\u56fe\u7eb8\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b32D\u5de5\u7a0b\u56fe\u7eb8\u4fe1\u606f\u63d0\u53d6\u8017\u65f6\u3001\u6613\u9519\u53ca\u4f20\u7edfOCR\u5bf9\u590d\u6742\u5e03\u5c40\u5904\u7406\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210YOLOv11\u68c0\u6d4bOBB\u4e0eDonut\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u6ce8\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u91c7\u7528\u5355\u6a21\u578b\u4e0e\u5206\u7c7b\u6a21\u578b\u5bf9\u6bd4\u7b56\u7565\u3002", "result": "\u5355\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747F1\u5206\u6570\u8fbe97.3%\uff0c\u51c6\u786e\u738794.77%\uff0c\u53ec\u56de\u7387100%\uff0c\u51cf\u5c115.23%\u5e7b\u89c9\u8f93\u51fa\u3002", "conclusion": "\u6846\u67b6\u63d0\u5347\u7cbe\u5ea6\u3001\u964d\u4f4e\u4eba\u5de5\u6210\u672c\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u5236\u9020\u884c\u4e1a\u3002"}}
{"id": "2505.02214", "pdf": "https://arxiv.org/pdf/2505.02214", "abs": "https://arxiv.org/abs/2505.02214", "authors": ["Xingyu Zheng", "Yuye Li", "Haoran Chu", "Yue Feng", "Xudong Ma", "Jie Luo", "Jinyang Guo", "Haotong Qin", "Michele Magno", "Xianglong Liu"], "title": "An Empirical Study of Qwen3 Quantization", "categories": ["cs.LG"], "comment": null, "summary": "The Qwen series has emerged as a leading family of open-source Large Language\nModels (LLMs), demonstrating remarkable capabilities in natural language\nunderstanding tasks. With the recent release of Qwen3, which exhibits superior\nperformance across diverse benchmarks, there is growing interest in deploying\nthese models efficiently in resource-constrained environments. Low-bit\nquantization presents a promising solution, yet its impact on Qwen3's\nperformance remains underexplored. This study conducts a systematic evaluation\nof Qwen3's robustness under various quantization settings, aiming to uncover\nboth opportunities and challenges in compressing this state-of-the-art model.\nWe rigorously assess 5 existing classic post-training quantization techniques\napplied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their\neffectiveness across multiple datasets. Our findings reveal that while Qwen3\nmaintains competitive performance at moderate bit-widths, it experiences\nnotable degradation in linguistic tasks under ultra-low precision, underscoring\nthe persistent hurdles in LLM compression. These results emphasize the need for\nfurther research to mitigate performance loss in extreme quantization\nscenarios. We anticipate that this empirical analysis will provide actionable\ninsights for advancing quantization methods tailored to Qwen3 and future LLMs,\nultimately enhancing their practicality without compromising accuracy. Our\nproject is released on https://github.com/Efficient-ML/Qwen3-Quantization and\nhttps://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578bQwen3\u5728\u4e0d\u540c\u91cf\u5316\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5728\u6781\u4f4e\u7cbe\u5ea6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u91cf\u5316\u65b9\u6cd5\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u968f\u7740Qwen3\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982\u4f55\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u90e8\u7f72\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u800c\u4f4e\u6bd4\u7279\u91cf\u5316\u5bf9Qwen3\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u91c7\u75285\u79cd\u7ecf\u5178\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\uff0c\u5bf9Qwen3\u8fdb\u884c1\u81f38\u6bd4\u7279\u7684\u91cf\u5316\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cQwen3\u5728\u4e2d\u7b49\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u6027\u80fd\u4fdd\u6301\u826f\u597d\uff0c\u4f46\u5728\u6781\u4f4e\u7cbe\u5ea6\u4e0b\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u6781\u4f4e\u7cbe\u5ea6\u91cf\u5316\u4ecd\u662fLLM\u538b\u7f29\u7684\u6311\u6218\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u51cf\u5c11\u6027\u80fd\u635f\u5931\uff0c\u63d0\u5347\u6a21\u578b\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.01531", "pdf": "https://arxiv.org/pdf/2505.01531", "abs": "https://arxiv.org/abs/2505.01531", "authors": ["Rodrigo Abad\u00eda-Heredia", "Manuel Lopez-Martin", "Soledad Le Clainche"], "title": "An Adaptive Framework for Autoregressive Forecasting in CFD Using Hybrid Modal Decomposition and Deep Learning", "categories": ["physics.flu-dyn", "cs.AI"], "comment": "47 pages, single-column, 15 figures and 5 tables", "summary": "This work presents, to the best of the authors' knowledge, the first\ngeneralizable and fully data-driven adaptive framework designed to stabilize\ndeep learning (DL) autoregressive forecasting models over long time horizons,\nwith the goal of reducing the computational cost required in computational\nfluid dynamics (CFD) simulations.The proposed methodology alternates between\ntwo phases: (i) predicting the evolution of the flow field over a selected time\ninterval using a trained DL model, and (ii) updating the model with newly\ngenerated CFD data when stability degrades, thus maintaining accurate long-term\nforecasting. This adaptive retraining strategy ensures robustness while\navoiding the accumulation of predictive errors typical in autoregressive\nmodels. The framework is validated across three increasingly complex flow\nregimes, from laminar to turbulent, demonstrating from 30 \\% to 95 \\% reduction\nin computational cost without compromising physical consistency or accuracy.\nIts entirely data-driven nature makes it easily adaptable to a wide range of\ntime-dependent simulation problems. The code implementing this methodology is\navailable as open-source and it will be integrated into the upcoming release of\nthe ModelFLOWs-app.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u901a\u7528\u7684\u3001\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u7a33\u5b9a\u6df1\u5ea6\u5b66\u4e60\u81ea\u56de\u5f52\u9884\u6d4b\u6a21\u578b\u5728\u957f\u65f6\u95f4\u8303\u56f4\u5185\u7684\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u6a21\u62df\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u5728\u957f\u65f6\u95f4\u9884\u6d4b\u4e2d\u56e0\u8bef\u5dee\u7d2f\u79ef\u5bfc\u81f4\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11CFD\u6a21\u62df\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4ea4\u66ff\u8fdb\u884c\u4e24\u4e2a\u9636\u6bb5\uff1a(i) \u4f7f\u7528\u8bad\u7ec3\u597d\u7684DL\u6a21\u578b\u9884\u6d4b\u6d41\u573a\u5728\u9009\u5b9a\u65f6\u95f4\u95f4\u9694\u5185\u7684\u6f14\u5316\uff1b(ii) \u5728\u7a33\u5b9a\u6027\u4e0b\u964d\u65f6\u7528\u65b0\u751f\u6210\u7684CFD\u6570\u636e\u66f4\u65b0\u6a21\u578b\uff0c\u4ee5\u4fdd\u6301\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728\u4ece\u5c42\u6d41\u5230\u6e4d\u6d41\u7684\u4e09\u79cd\u590d\u6742\u6d41\u52a8\u72b6\u6001\u4e0b\u9a8c\u8bc1\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e30%\u523095%\uff0c\u4e14\u4e0d\u635f\u5bb3\u7269\u7406\u4e00\u81f4\u6027\u6216\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4ee3\u7801\u5f00\u6e90\u4e14\u5c06\u96c6\u6210\u81f3ModelFLOWs-app\u3002"}}
{"id": "2505.02550", "pdf": "https://arxiv.org/pdf/2505.02550", "abs": "https://arxiv.org/abs/2505.02550", "authors": ["Krzysztof Ociepa", "\u0141ukasz Flis", "Remigiusz Kinas", "Krzysztof Wr\u00f3bel", "Adrian Gwo\u017adziej"], "title": "Bielik v3 Small: Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications.", "AI": {"tldr": "Bielik v3\u7cfb\u5217\u662f\u9488\u5bf9\u6ce2\u5170\u8bed\u4f18\u5316\u7684\u53c2\u6570\u9ad8\u6548\u751f\u6210\u6587\u672c\u6a21\u578b\uff081.5B\u548c4.5B\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\uff08\u5982APT4\u5206\u8bcd\u5668\u3001\u52a0\u6743\u6307\u4ee4\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u52a8\u6001\u5b66\u4e60\u7387\uff09\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u4e3a\u6ce2\u5170\u8bed\u63d0\u4f9b\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u9ad8\u8d28\u91cfAI\u6a21\u578b\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c11\u88ab\u4ee3\u8868\u7684\u8bed\u8a00\u9886\u57df\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u6ce2\u5170\u8bed\u5206\u8bcd\u5668APT4\u63d0\u5347\u5206\u8bcd\u6548\u7387\uff0c\u4f7f\u7528\u52a0\u6743\u6307\u4ee4\u4ea4\u53c9\u71b5\u635f\u5931\u5e73\u8861\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u8c03\u6574\u7684\u5b66\u4e60\u7387\uff1b\u6a21\u578b\u57fa\u4e8e2920\u4ebf\u6807\u8bb0\u7684\u7cbe\u5fc3\u7b5b\u9009\u8bed\u6599\u5e93\u8bad\u7ec3\u3002", "result": "4.5B\u6a21\u578b\u6027\u80fd\u4e0e\u89c4\u6a21\u59272-3\u500d\u7684\u6a21\u578b\u76f8\u5f53\uff0c1.5B\u6a21\u578b\u5728\u8d85\u5c0f\u89c4\u6a21\u4e0b\u4ecd\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u5305\u62ecOpen PL LLM Leaderboard\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9886\u5148\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u9ad8\u6548\u5efa\u6a21\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\uff0c\u63a8\u52a8\u4e86\u9ad8\u8d28\u91cf\u6ce2\u5170\u8bedAI\u7684\u666e\u53ca\uff0c\u5c24\u5176\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.02222", "pdf": "https://arxiv.org/pdf/2505.02222", "abs": "https://arxiv.org/abs/2505.02222", "authors": ["Essential AI", ":", "Ishaan Shah", "Anthony M. Polloreno", "Karl Stratos", "Philip Monk", "Adarsh Chaluvaraju", "Andrew Hojel", "Andrew Ma", "Anil Thomas", "Ashish Tanwer", "Darsh J Shah", "Khoi Nguyen", "Kurt Smith", "Michael Callahan", "Michael Pust", "Mohit Parmar", "Peter Rushton", "Platon Mazarakis", "Ritvik Kapila", "Saurabh Srivastava", "Somanshu Singla", "Tim Romanski", "Yash Vanjani", "Ashish Vaswani"], "title": "Practical Efficiency of Muon for Pretraining", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We demonstrate that Muon, the simplest instantiation of a second-order\noptimizer, explicitly expands the Pareto frontier over AdamW on the\ncompute-time tradeoff. We find that Muon is more effective than AdamW in\nretaining data efficiency at large batch sizes, far beyond the so-called\ncritical batch size, while remaining computationally efficient, thus enabling\nmore economical training. We study the combination of Muon and the maximal\nupdate parameterization (muP) for efficient hyperparameter transfer and present\na simple telescoping algorithm that accounts for all sources of error in muP\nwhile introducing only a modest overhead in resources. We validate our findings\nthrough extensive experiments with model sizes up to four billion parameters\nand ablations on the data distribution and architecture.", "AI": {"tldr": "Muon\u662f\u4e00\u79cd\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u76f8\u6bd4AdamW\u5728\u8ba1\u7b97\u65f6\u95f4\u548c\u6027\u80fd\u6743\u8861\u4e0a\u6269\u5c55\u4e86Pareto\u524d\u6cbf\uff0c\u5c24\u5176\u5728\u5904\u7406\u5927\u6279\u91cf\u6570\u636e\u65f6\u66f4\u9ad8\u6548\u4e14\u7ecf\u6d4e\u3002\u7ed3\u5408muP\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u7b97\u6cd5\u6765\u9ad8\u6548\u8f6c\u79fb\u8d85\u53c2\u6570\u5e76\u63a7\u5236\u8bef\u5dee\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u5927\u6279\u91cf\u8bad\u7ec3\u65f6\u4fdd\u6301\u6570\u636e\u6548\u7387\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86Muon\u4f18\u5316\u5668\uff0c\u7ed3\u5408muP\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u7684telescoping\u7b97\u6cd5\u6765\u5904\u7406\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMuon\u5728\u9ad8\u8fbe40\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8eAdamW\uff0c\u5c24\u5176\u5728\u8d85\u5927\u6279\u91cf\u65f6\u4ecd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "Muon\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u4f18\u5316\u5668\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2505.01542", "pdf": "https://arxiv.org/pdf/2505.01542", "abs": "https://arxiv.org/abs/2505.01542", "authors": ["Karishma Hegde", "Hemadri Jayalath"], "title": "Emotions in the Loop: A Survey of Affective Computing for Emotional Support", "categories": ["cs.HC", "cs.AI", "I.2.10; I.2.7; H.5.2"], "comment": "20 pages, 7 tables, 96 references. Survey paper on affective\n  computing applications using large language models, multimodal AI, and\n  therapeutic chatbots", "summary": "In a world where technology is increasingly embedded in our everyday\nexperiences, systems that sense and respond to human emotions are elevating\ndigital interaction. At the intersection of artificial intelligence and\nhuman-computer interaction, affective computing is emerging with innovative\nsolutions where machines are humanized by enabling them to process and respond\nto user emotions. This survey paper explores recent research contributions in\naffective computing applications in the area of emotion recognition, sentiment\nanalysis and personality assignment developed using approaches like large\nlanguage models (LLMs), multimodal techniques, and personalized AI systems. We\nanalyze the key contributions and innovative methodologies applied by the\nselected research papers by categorizing them into four domains: AI chatbot\napplications, multimodal input systems, mental health and therapy applications,\nand affective computing for safety applications. We then highlight the\ntechnological strengths as well as the research gaps and challenges related to\nthese studies. Furthermore, the paper examines the datasets used in each study,\nhighlighting how modality, scale, and diversity impact the development and\nperformance of affective models. Finally, the survey outlines ethical\nconsiderations and proposes future directions to develop applications that are\nmore safe, empathetic and practical.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u63a2\u8ba8\u4e86\u60c5\u611f\u8ba1\u7b97\u5728\u60c5\u7eea\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790\u548c\u4eba\u683c\u5206\u914d\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5206\u7c7b\u5206\u6790\u4e86AI\u804a\u5929\u673a\u5668\u4eba\u3001\u591a\u6a21\u6001\u8f93\u5165\u7cfb\u7edf\u3001\u5fc3\u7406\u5065\u5eb7\u6cbb\u7597\u548c\u5b89\u5168\u5e94\u7528\u7b49\u9886\u57df\u7684\u7814\u7a76\u8d21\u732e\u548c\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u6570\u636e\u96c6\u3001\u6280\u672f\u6311\u6218\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6280\u672f\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u6df1\u5165\uff0c\u60c5\u611f\u8ba1\u7b97\u901a\u8fc7\u611f\u77e5\u548c\u54cd\u5e94\u7528\u6237\u60c5\u7eea\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002\u672c\u6587\u65e8\u5728\u603b\u7ed3\u60c5\u611f\u8ba1\u7b97\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u5206\u6790\u5176\u5e94\u7528\u548c\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u7c7b\u5206\u6790\u4e86\u60c5\u611f\u8ba1\u7b97\u7684\u56db\u5927\u5e94\u7528\u9886\u57df\uff1aAI\u804a\u5929\u673a\u5668\u4eba\u3001\u591a\u6a21\u6001\u7cfb\u7edf\u3001\u5fc3\u7406\u5065\u5eb7\u4e0e\u6cbb\u7597\u3001\u5b89\u5168\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30\u4e86\u6240\u4f7f\u7528\u7684\u65b9\u6cd5\uff08\u5982LLMs\u3001\u591a\u6a21\u6001\u6280\u672f\uff09\u548c\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u60c5\u611f\u8ba1\u7b97\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u4e86\u6280\u672f\u4f18\u52bf\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u96c6\u591a\u6837\u6027\u548c\u89c4\u6a21\u4e0d\u8db3\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7b49\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u672a\u6765\u9700\u5173\u6ce8\u4f26\u7406\u95ee\u9898\uff0c\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u5171\u60c5\u4e14\u5b9e\u7528\u7684\u60c5\u611f\u8ba1\u7b97\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2505.02639", "pdf": "https://arxiv.org/pdf/2505.02639", "abs": "https://arxiv.org/abs/2505.02639", "authors": ["Xuan Lin", "Qingrui Liu", "Hongxin Xiang", "Daojian Zeng", "Xiangxiang Zeng"], "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ChemDual\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6307\u4ee4\u6570\u636e\u96c6\u548c\u53cc\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u5316\u5b66\u53cd\u5e94\u548c\u9006\u5408\u6210\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u76f4\u63a5\u5e94\u7528\u4e8e\u5316\u5b66\u53cd\u5e94\u548c\u9006\u5408\u6210\u9884\u6d4b\u65f6\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u5316\u5b66\u5408\u6210\u76f8\u5173\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5ffd\u89c6\u4e86\u53cd\u5e94\u548c\u9006\u5408\u6210\u9884\u6d4b\u4e4b\u95f4\u7684\u7d27\u5bc6\u5173\u8054\u3002", "method": "ChemDual\u5c06\u5316\u5b66\u53cd\u5e94\u548c\u9006\u5408\u6210\u89c6\u4e3a\u76f8\u5173\u7684\u91cd\u7ec4\u4e0e\u788e\u7247\u5316\u8fc7\u7a0b\uff0c\u5e76\u6784\u5efa\u4e86440\u4e07\u6761\u6307\u4ee4\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5206\u8bcd\u5668\u548c\u53cc\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u589e\u5f3aLLaMA\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u91cd\u7ec4\u4e0e\u788e\u7247\u5316\u8fc7\u7a0b\u4ee5\u53ca\u53cd\u5e94\u548c\u9006\u5408\u6210\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5728Mol-Instruction\u548cUSPTO-50K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cChemDual\u5728\u53cd\u5e94\u548c\u9006\u5408\u6210\u9884\u6d4b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u4efb\u52a1\u65b9\u6cd5\u548c\u901a\u7528\u5f00\u6e90LLMs\u3002\u5206\u5b50\u5bf9\u63a5\u5206\u6790\u663e\u793a\uff0cChemDual\u751f\u6210\u7684\u5316\u5408\u7269\u5177\u6709\u591a\u6837\u4e14\u5f3a\u7684\u86cb\u767d\u7ed3\u5408\u4eb2\u548c\u529b\u3002", "conclusion": "ChemDual\u5728\u836f\u7269\u8bbe\u8ba1\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4e3a\u5316\u5b66\u53cd\u5e94\u548c\u9006\u5408\u6210\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02228", "pdf": "https://arxiv.org/pdf/2505.02228", "abs": "https://arxiv.org/abs/2505.02228", "authors": ["Shangzhe Li", "Zhiao Huang", "Hao Su"], "title": "Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Imitation Learning (IL) has achieved remarkable success across various\ndomains, including robotics, autonomous driving, and healthcare, by enabling\nagents to learn complex behaviors from expert demonstrations. However, existing\nIL methods often face instability challenges, particularly when relying on\nadversarial reward or value formulations in world model frameworks. In this\nwork, we propose a novel approach to online imitation learning that addresses\nthese limitations through a reward model based on random network distillation\n(RND) for density estimation. Our reward model is built on the joint estimation\nof expert and behavioral distributions within the latent space of the world\nmodel. We evaluate our method across diverse benchmarks, including DMControl,\nMeta-World, and ManiSkill2, showcasing its ability to deliver stable\nperformance and achieve expert-level results in both locomotion and\nmanipulation tasks. Our approach demonstrates improved stability over\nadversarial methods while maintaining expert-level performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u7f51\u7edc\u84b8\u998f\uff08RND\uff09\u7684\u5728\u7ebf\u6a21\u4eff\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u4e16\u754c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u4e13\u5bb6\u548c\u884c\u4e3a\u5206\u5e03\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bf9\u6297\u6027\u5956\u52b1\u6216\u4ef7\u503c\u516c\u5f0f\u65f6\u5e38\u9762\u4e34\u4e0d\u7a33\u5b9a\u6027\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e16\u754c\u6a21\u578b\u6846\u67b6\u4e2d\u3002", "method": "\u91c7\u7528\u968f\u673a\u7f51\u7edc\u84b8\u998f\uff08RND\uff09\u6784\u5efa\u5956\u52b1\u6a21\u578b\uff0c\u8054\u5408\u4f30\u8ba1\u4e16\u754c\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u4e13\u5bb6\u548c\u884c\u4e3a\u5206\u5e03\u3002", "result": "\u5728DMControl\u3001Meta-World\u548cManiSkill2\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u7a33\u5b9a\u5e76\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e13\u5bb6\u6c34\u5e73\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.02693", "pdf": "https://arxiv.org/pdf/2505.02693", "abs": "https://arxiv.org/abs/2505.02693", "authors": ["Shaghayegh Agah", "Yejin Kim", "Neeraj Sharma", "Mayur Nankani", "Kevin Foley", "H. Howie Huang", "Sardar Hamidian"], "title": "Predicting Movie Hits Before They Happen with LLMs", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted at ACM UMAP 2025 Industry Track", "summary": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped.", "AI": {"tldr": "The paper tackles the cold-start issue in movie recommendations by using LLMs to predict popularity based on metadata, showing improved effectiveness over baselines.", "motivation": "The challenge of cold-start recommendations in entertainment platforms, especially for movies, drives the need for innovative solutions to ensure fair promotion and accurate predictions.", "method": "Leverages Large Language Models (LLMs) with movie metadata to forecast the popularity of cold-start movies.", "result": "The proposed method demonstrates superior effectiveness compared to existing baselines and newly developed solutions.", "conclusion": "LLMs are effective for cold-start movie popularity prediction, offering potential integration into recommendation systems or editorial tools."}}
{"id": "2505.02238", "pdf": "https://arxiv.org/pdf/2505.02238", "abs": "https://arxiv.org/abs/2505.02238", "authors": ["Haoyang Li", "Jie Xu", "Kyra Gan", "Fei Wang", "Chengxi Zang"], "title": "Federated Causal Inference in Healthcare: Methods, Challenges, and Applications", "categories": ["cs.LG"], "comment": null, "summary": "Federated causal inference enables multi-site treatment effect estimation\nwithout sharing individual-level data, offering a privacy-preserving solution\nfor real-world evidence generation. However, data heterogeneity across sites,\nmanifested in differences in covariate, treatment, and outcome, poses\nsignificant challenges for unbiased and efficient estimation. In this paper, we\npresent a comprehensive review and theoretical analysis of federated causal\neffect estimation across both binary/continuous and time-to-event outcomes. We\nclassify existing methods into weight-based strategies and optimization-based\nframeworks and further discuss extensions including personalized models,\npeer-to-peer communication, and model decomposition. For time-to-event\noutcomes, we examine federated Cox and Aalen-Johansen models, deriving\nasymptotic bias and variance under heterogeneity. Our analysis reveals that\nFedProx-style regularization achieves near-optimal bias-variance trade-offs\ncompared to naive averaging and meta-analysis. We review related software tools\nand conclude by outlining opportunities, challenges, and future directions for\nscalable, fair, and trustworthy federated causal inference in distributed\nhealthcare systems.", "AI": {"tldr": "\u8054\u90a6\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u7ad9\u70b9\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f46\u6570\u636e\u5f02\u8d28\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002\u672c\u6587\u7efc\u8ff0\u5e76\u7406\u8bba\u5206\u6790\u4e86\u76f8\u5173\u65b9\u6cd5\uff0c\u5305\u62ec\u6743\u91cd\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86FedProx\u6b63\u5219\u5316\u7684\u4f18\u52bf\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u9700\u5904\u7406\u8de8\u7ad9\u70b9\u6570\u636e\u5f02\u8d28\u6027\uff08\u5982\u534f\u53d8\u91cf\u3001\u6cbb\u7597\u548c\u7ed3\u679c\u5dee\u5f02\uff09\uff0c\u5982\u4f55\u5728\u9690\u79c1\u4fdd\u62a4\u524d\u63d0\u4e0b\u5b9e\u73b0\u65e0\u504f\u4e14\u9ad8\u6548\u7684\u4f30\u8ba1\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u5206\u7c7b\u6bd4\u8f83\u6743\u91cd\u6cd5\u548c\u4f18\u5316\u6846\u67b6\uff0c\u6269\u5c55\u8ba8\u8bba\u4e2a\u6027\u5316\u6a21\u578b\u3001\u70b9\u5bf9\u70b9\u901a\u4fe1\u548c\u6a21\u578b\u5206\u89e3\uff1b\u9488\u5bf9\u751f\u5b58\u5206\u6790\u63d0\u51fa\u8054\u90a6Cox\u548cAalen-Johansen\u6a21\u578b\uff0c\u5e76\u7406\u8bba\u5206\u6790\u5f02\u8d28\u6027\u4e0b\u7684\u504f\u5dee\u4e0e\u65b9\u5dee\u3002", "result": "\u53d1\u73b0FedProx\u6b63\u5219\u5316\u5728\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u4e0a\u4f18\u4e8e\u7b80\u5355\u5e73\u5747\u548c\u5143\u5206\u6790\uff0c\u63a5\u8fd1\u6700\u4f18\u89e3\u3002", "conclusion": "\u8054\u90a6\u56e0\u679c\u63a8\u65ad\u5728\u5206\u5e03\u5f0f\u533b\u7597\u7cfb\u7edf\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u516c\u5e73\u6027\u548c\u53ef\u4fe1\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2505.01583", "pdf": "https://arxiv.org/pdf/2505.01583", "abs": "https://arxiv.org/abs/2505.01583", "authors": ["Jen-Hao Cheng", "Vivian Wang", "Huayu Wang", "Huapeng Zhou", "Yi-Hao Peng", "Hou-I Liu", "Hsiang-Wei Huang", "Kuang-Ming Chen", "Cheng-Yen Yang", "Wenhao Chai", "Yi-Ling Chen", "Vibhav Vineet", "Qin Cai", "Jenq-Neng Hwang"], "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding.", "AI": {"tldr": "TEMPURA\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u4e8b\u4ef6\u9884\u6d4b\u548c\u89c6\u9891\u5206\u5272\u4efb\u52a1\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u4e0e\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5206\u5272\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u4e2d\u4e8b\u4ef6\u5173\u7cfb\u7684\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4e8b\u4ef6\u8fb9\u754c\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u96be\u4ee5\u5efa\u6a21\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u3002TEMPURA\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u4e0e\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5206\u5272\uff0c\u6539\u5584\u89c6\u9891\u7406\u89e3\u3002", "method": "TEMPURA\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u63a9\u7801\u4e8b\u4ef6\u9884\u6d4b\u63a8\u7406\uff0c\u91cd\u5efa\u7f3a\u5931\u4e8b\u4ef6\u5e76\u751f\u6210\u56e0\u679c\u89e3\u91ca\uff1b2) \u89c6\u9891\u5206\u5272\u4e0e\u5bc6\u96c6\u63cf\u8ff0\u4efb\u52a1\uff0c\u5206\u89e3\u89c6\u9891\u4e3a\u5e26\u65f6\u95f4\u6233\u7684\u975e\u91cd\u53e0\u4e8b\u4ef6\u3002", "result": "\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u9ad8\u5149\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cTEMPURA\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u4e0e\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5206\u5272\u80fd\u663e\u8457\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0cTEMPURA\u4e3a\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02247", "pdf": "https://arxiv.org/pdf/2505.02247", "abs": "https://arxiv.org/abs/2505.02247", "authors": ["Jingxiang Qu", "Wenhan Gao", "Jiaxing Zhang", "Xufeng Liu", "Hua Wei", "Haibin Ling", "Yi Liu"], "title": "RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "3D Geometric Graph Neural Networks (GNNs) have emerged as transformative\ntools for modeling molecular data. Despite their predictive power, these models\noften suffer from limited interpretability, raising concerns for scientific\napplications that require reliable and transparent insights. While existing\nmethods have primarily focused on explaining molecular substructures in 2D\nGNNs, the transition to 3D GNNs introduces unique challenges, such as handling\nthe implicit dense edge structures created by a cut-off radius. To tackle this,\nwe introduce a novel explanation method specifically designed for 3D GNNs,\nwhich localizes the explanation to the immediate neighborhood of each node\nwithin the 3D space. Each node is assigned an radius of influence, defining the\nlocalized region within which message passing captures spatial and structural\ninteractions crucial for the model's predictions. This method leverages the\nspatial and geometric characteristics inherent in 3D graphs. By constraining\nthe subgraph to a localized radius of influence, the approach not only enhances\ninterpretability but also aligns with the physical and structural dependencies\ntypical of 3D graph applications, such as molecular learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf93D GNN\u7684\u65b0\u578b\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u5f71\u54cd\u534a\u5f84\u6765\u5b9a\u4f4d3D\u7a7a\u95f4\u4e2d\u7684\u5c40\u90e8\u533a\u57df\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "3D GNNs\u5728\u5206\u5b50\u6570\u636e\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u5176\u6709\u9650\u7684\u89e3\u91ca\u6027\u963b\u788d\u4e86\u79d1\u5b66\u5e94\u7528\u4e2d\u5bf9\u53ef\u9760\u900f\u660e\u6d1e\u5bdf\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e13\u95e8\u9488\u5bf93D GNN\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u5316\u8282\u70b9\u7684\u76f4\u63a5\u90bb\u57df\u5e76\u4e3a\u6bcf\u4e2a\u8282\u70b9\u5206\u914d\u5f71\u54cd\u534a\u5f84\u6765\u6355\u6349\u7a7a\u95f4\u548c\u7ed3\u6784\u4ea4\u4e92\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e863D GNN\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd8\u4e0e\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u5206\u5b50\u5b66\u4e60\uff09\u7684\u7269\u7406\u548c\u7ed3\u6784\u4f9d\u8d56\u6027\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D GNNs\u4e2d\u7684\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u91ca\u5de5\u5177\u3002"}}
{"id": "2505.02746", "pdf": "https://arxiv.org/pdf/2505.02746", "abs": "https://arxiv.org/abs/2505.02746", "authors": ["Simon Ging", "Sebastian Walter", "Jelena Bratuli\u0107", "Johannes Dienert", "Hannah Bast", "Thomas Brox"], "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u589e\u5f3a\u578b\u77e5\u8bc6\u56fe\u8c31\u548c\u667a\u80fd\u7f51\u7edc\u641c\u7d22\u7b56\u7565\uff0c\u4ee5\u8f83\u5c11\u6570\u636e\u8bad\u7ec3\u9ad8\u8d28\u91cfCLIP\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86EntityNet\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cfCLIP\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u9886\u57df\u7279\u5f02\u6027\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u73b0\u6709\u5927\u89c4\u6a21CLIP\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u7684\u9886\u57df\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u964d\u4f4e\u6570\u636e\u9700\u6c42\u548c\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u667a\u80fd\u7f51\u7edc\u641c\u7d22\u7b56\u7565\uff0c\u4ece\u5c11\u91cf\u6570\u636e\uff08\u598210M\u56fe\u50cf\uff09\u8bad\u7ec3\u9c81\u68d2\u7684CLIP\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86EntityNet\u6570\u636e\u96c6\uff0833M\u56fe\u50cf\u548c46M\u6587\u672c\u63cf\u8ff0\uff09\u4ee5\u52a0\u901f\u901a\u7528\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u8bad\u7ec3\u4e86\u9488\u5bf9\u751f\u7269\u9886\u57df\u7684\u4e13\u5bb6\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86EntityNet\u5728\u51cf\u5c11\u901a\u7528CLIP\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u548c\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u6570\u636e\u91cf\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u9ad8\u8d28\u91cf\u7684CLIP\u6a21\u578b\uff0c\u4e3a\u9886\u57df\u7279\u5f02\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.02277", "pdf": "https://arxiv.org/pdf/2505.02277", "abs": "https://arxiv.org/abs/2505.02277", "authors": ["Maryam Sultana", "Neil Yorke-Smith", "Kaizheng Wang", "Shireen Kudukkil Manchingal", "Muhammad Mubashar", "Fabio Cuzzolin"], "title": "Epistemic Wrapping for Uncertainty Quantification", "categories": ["cs.LG"], "comment": null, "summary": "Uncertainty estimation is pivotal in machine learning, especially for\nclassification tasks, as it improves the robustness and reliability of models.\nWe introduce a novel `Epistemic Wrapping' methodology aimed at improving\nuncertainty estimation in classification. Our approach uses Bayesian Neural\nNetworks (BNNs) as a baseline and transforms their outputs into belief function\nposteriors, effectively capturing epistemic uncertainty and offering an\nefficient and general methodology for uncertainty quantification. Comprehensive\nexperiments employing a Bayesian Neural Network (BNN) baseline and an Interval\nNeural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and\nCIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly\nenhances generalisation and uncertainty quantification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a'Epistemic Wrapping'\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5c06\u5176\u8f93\u51fa\u8f6c\u5316\u4e3a\u4fe1\u5ff5\u51fd\u6570\u540e\u9a8c\uff0c\u6709\u6548\u6355\u6349\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u5347\u5206\u7c7b\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u5c24\u5176\u662f\u5206\u7c7b\u4efb\u52a1\uff0c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bf9\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7684'Epistemic Wrapping'\u65b9\u6cd5\u4ee5\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u4e3a\u57fa\u7ebf\uff0c\u5c06\u5176\u8f93\u51fa\u8f6c\u5316\u4e3a\u4fe1\u5ff5\u51fd\u6570\u540e\u9a8c\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u4e14\u901a\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5b9e\u9a8c\u5728MNIST\u3001Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6548\u679c\u3002", "conclusion": "'Epistemic Wrapping'\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02288", "pdf": "https://arxiv.org/pdf/2505.02288", "abs": "https://arxiv.org/abs/2505.02288", "authors": ["Qian Qi"], "title": "Universal Approximation Theorem of Deep Q-Networks", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs)\nvia stochastic control and Forward-Backward Stochastic Differential Equations\n(FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by\na square-integrable martingale, we analyze DQN approximation properties. We\nshow that DQNs can approximate the optimal Q-function on compact sets with\narbitrary accuracy and high probability, leveraging residual network\napproximation theorems and large deviation bounds for the state-action process.\nWe then analyze the convergence of a general Q-learning algorithm for training\nDQNs in this setting, adapting stochastic approximation theorems. Our analysis\nemphasizes the interplay between DQN layer count, time discretization, and the\nrole of viscosity solutions (primarily for the value function $V^*$) in\naddressing potential non-smoothness of the optimal Q-function. This work\nbridges deep reinforcement learning and stochastic control, offering insights\ninto DQNs in continuous-time settings, relevant for applications with physical\nsystems or high-frequency data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fde\u7eed\u65f6\u95f4\u6846\u67b6\uff0c\u5229\u7528\u968f\u673a\u63a7\u5236\u548c\u524d\u5411-\u540e\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08FBSDEs\uff09\u5206\u6790\u6df1\u5ea6Q\u7f51\u7edc\uff08DQNs\uff09\uff0c\u8bc1\u660e\u4e86DQN\u5728\u7d27\u51d1\u96c6\u4e0a\u80fd\u4ee5\u9ad8\u6982\u7387\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u6700\u4f18Q\u51fd\u6570\uff0c\u5e76\u5206\u6790\u4e86Q\u5b66\u4e60\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u5efa\u7acb\u8fde\u7eed\u65f6\u95f4\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u968f\u673a\u63a7\u5236\u7406\u8bba\uff0c\u5206\u6790DQNs\u7684\u903c\u8fd1\u80fd\u529b\u548c\u6536\u655b\u6027\uff0c\u4e3a\u7269\u7406\u7cfb\u7edf\u6216\u9ad8\u9891\u6570\u636e\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u6b8b\u5dee\u7f51\u7edc\u903c\u8fd1\u5b9a\u7406\u3001\u5927\u504f\u5dee\u7406\u8bba\u548c\u968f\u673a\u903c\u8fd1\u5b9a\u7406\uff0c\u5206\u6790DQN\u7684\u903c\u8fd1\u80fd\u529b\u548cQ\u5b66\u4e60\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u8003\u8651\u7f51\u7edc\u5c42\u6570\u3001\u65f6\u95f4\u79bb\u6563\u5316\u53ca\u7c98\u6027\u89e3\u7684\u4f5c\u7528\u3002", "result": "\u7ed3\u679c\u8868\u660eDQN\u80fd\u5728\u7d27\u51d1\u96c6\u4e0a\u9ad8\u6982\u7387\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u6700\u4f18Q\u51fd\u6570\uff0c\u540c\u65f6Q\u5b66\u4e60\u7b97\u6cd5\u5728\u8be5\u6846\u67b6\u4e0b\u6536\u655b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u672c\u6587\u4e3a\u8fde\u7eed\u65f6\u95f4\u4e0b\u7684DQN\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u968f\u673a\u63a7\u5236\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u6f5c\u5728\u610f\u4e49\u3002"}}
{"id": "2505.02296", "pdf": "https://arxiv.org/pdf/2505.02296", "abs": "https://arxiv.org/abs/2505.02296", "authors": ["Pinaki Mohanty", "Riddhiman Bhattacharya", "Ruqi Zhang"], "title": "Entropy-Guided Sampling of Flat Modes in Discrete Spaces", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Sampling from flat modes in discrete spaces is a crucial yet underexplored\nproblem. Flat modes represent robust solutions and have broad applications in\ncombinatorial optimization and discrete generative modeling. However, existing\nsampling algorithms often overlook the mode volume and struggle to capture flat\nmodes effectively. To address this limitation, we propose \\emph{Entropic\nDiscrete Langevin Proposal} (EDLP), which incorporates local entropy into the\nsampling process through a continuous auxiliary variable under a joint\ndistribution. The local entropy term guides the discrete sampler toward flat\nmodes with a small overhead. We provide non-asymptotic convergence guarantees\nfor EDLP in locally log-concave discrete distributions. Empirically, our method\nconsistently outperforms traditional approaches across tasks that require\nsampling from flat basins, including Bernoulli distribution, restricted\nBoltzmann machines, combinatorial optimization, and binary neural networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EDLP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u71b5\u6539\u8fdb\u79bb\u6563\u7a7a\u95f4\u4e2d\u7684\u5e73\u5766\u6a21\u6001\u91c7\u6837\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5e73\u5766\u6a21\u6001\u5728\u7ec4\u5408\u4f18\u5316\u548c\u79bb\u6563\u751f\u6210\u6a21\u578b\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u91c7\u6837\u7b97\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u5e73\u5766\u6a21\u6001\u3002", "method": "\u63d0\u51faEDLP\uff0c\u901a\u8fc7\u8fde\u7eed\u8f85\u52a9\u53d8\u91cf\u548c\u8054\u5408\u5206\u5e03\u5f15\u5165\u5c40\u90e8\u71b5\uff0c\u6307\u5bfc\u91c7\u6837\u5668\u66f4\u6709\u6548\u627e\u5230\u5e73\u5766\u6a21\u6001\u3002", "result": "\u5728\u4f2f\u52aa\u5229\u5206\u5e03\u3001\u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a\u3001\u7ec4\u5408\u4f18\u5316\u548c\u4e8c\u503c\u795e\u7ecf\u7f51\u7edc\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "EDLP\u4e3a\u79bb\u6563\u7a7a\u95f4\u4e2d\u5e73\u5766\u6a21\u6001\u91c7\u6837\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5177\u6709\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002"}}
{"id": "2505.01615", "pdf": "https://arxiv.org/pdf/2505.01615", "abs": "https://arxiv.org/abs/2505.01615", "authors": ["Dimitrios Dagdilelis", "Panagiotis Grigoriadis", "Roberto Galeazzi"], "title": "Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a cross attention transformer based method for multimodal sensor\nfusion to build a birds eye view of a vessels surroundings supporting safer\nautonomous marine navigation. The model deeply fuses multiview RGB and long\nwave infrared images with sparse LiDAR point clouds. Training also integrates X\nband radar and electronic chart data to inform predictions. The resulting view\nprovides a detailed reliable scene representation improving navigational\naccuracy and robustness. Real world sea trials confirm the methods\neffectiveness even in adverse weather and complex maritime settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8de8\u6ce8\u610fTransformer\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u8239\u8236\u5468\u56f4\u73af\u5883\u7684\u9e1f\u77b0\u56fe\uff0c\u4ee5\u652f\u6301\u66f4\u5b89\u5168\u7684\u81ea\u4e3b\u6d77\u6d0b\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u7684\u6d77\u6d0b\u5bfc\u822a\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u548c\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5bfc\u822a\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u8de8\u6ce8\u610fTransformer\u6a21\u578b\uff0c\u6df1\u5ea6\u878d\u5408\u591a\u89c6\u89d2RGB\u548c\u957f\u6ce2\u7ea2\u5916\u56fe\u50cf\u4e0e\u7a00\u758fLiDAR\u70b9\u4e91\u6570\u636e\uff0c\u5e76\u7ed3\u5408X\u6ce2\u6bb5\u96f7\u8fbe\u548c\u7535\u5b50\u6d77\u56fe\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u751f\u6210\u7684\u9e1f\u77b0\u56fe\u63d0\u4f9b\u4e86\u8be6\u7ec6\u53ef\u9760\u7684\u573a\u666f\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u9645\u6d77\u4e0a\u8bd5\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u548c\u590d\u6742\u6d77\u51b5\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u6210\u529f\u63d0\u5347\u4e86\u81ea\u4e3b\u6d77\u6d0b\u5bfc\u822a\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\uff0c\u5c55\u73b0\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02830", "pdf": "https://arxiv.org/pdf/2505.02830", "abs": "https://arxiv.org/abs/2505.02830", "authors": ["Qingqiu Li", "Zihang Cui", "Seongsu Bae", "Jilan Xu", "Runtian Yuan", "Yuejie Zhang", "Rui Feng", "Quanli Shen", "Xiaobo Zhang", "Junjun He", "Shujun Wang"], "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u5256\u5b66\u5bfc\u5411\u7684\u63a8\u7406\u6846\u67b6\uff08AOR\uff09\uff0c\u901a\u8fc7\u591a\u6b65\u63a8\u7406\u589e\u5f3a\u533b\u5b66\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08MLMM\uff09\u7684\u533a\u57df\u7406\u89e3\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u6307\u5bfc\u7684\u5927\u89c4\u6a21\u6307\u4ee4\u6570\u636e\u96c6\uff08AOR-Instruction\uff09\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660eAOR\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08MLMM\uff09\u5728\u80f8\u7247\uff08CXR\uff09\u89e3\u8bfb\u4e2d\u5b58\u5728\u533a\u57df\u7ea7\u7406\u89e3\u4e0d\u8db3\u548c\u5355\u6b65\u63a8\u7406\u5bfc\u81f4\u7684\u51c6\u786e\u6027\u53ca\u53ef\u89e3\u91ca\u6027\u53d7\u9650\u7684\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89e3\u5256\u5b66\u5bfc\u5411\u7684\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u89e3\u5256\u5b66\u5bfc\u5411\u7684\u63a8\u7406\u6846\u67b6\uff08AOR\uff09\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u533a\u57df\u7ea7\u4fe1\u606f\u5b9e\u73b0\u591a\u6b65\u63a8\u7406\uff0c\u5e76\u7ed3\u5408\u4e13\u5bb6\u5f00\u53d1\u7684\u6307\u4ee4\u6570\u636e\u96c6\uff08AOR-Instruction\uff09\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAOR\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AOR\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u533a\u57df\u7ea7\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u533b\u5b66\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u80f8\u7247\u89e3\u8bfb\u65b9\u6848\u3002"}}
{"id": "2505.02299", "pdf": "https://arxiv.org/pdf/2505.02299", "abs": "https://arxiv.org/abs/2505.02299", "authors": ["Daisuke Yamada", "Harit Vishwakarma", "Ramya Korlakai Vinayak"], "title": "Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine Learning (ML) models are trained on in-distribution (ID) data but\noften encounter out-of-distribution (OOD) inputs during deployment -- posing\nserious risks in safety-critical domains. Recent works have focused on\ndesigning scoring functions to quantify OOD uncertainty, with score thresholds\ntypically set based solely on ID data to achieve a target true positive rate\n(TPR), since OOD data is limited before deployment. However, these TPR-based\nthresholds leave false positive rates (FPR) uncontrolled, often resulting in\nhigh FPRs where OOD points are misclassified as ID. Moreover, fixed scoring\nfunctions and thresholds lack the adaptivity needed to handle newly observed,\nevolving OOD inputs, leading to sub-optimal performance. To address these\nchallenges, we propose a human-in-the-loop framework that \\emph{safely updates\nboth scoring functions and thresholds on the fly} based on real-world OOD\ninputs. Our method maximizes TPR while strictly controlling FPR at all times,\neven as the system adapts over time. We provide theoretical guarantees for FPR\ncontrol under stationary conditions and present extensive empirical evaluations\non OpenOOD benchmarks to demonstrate that our approach outperforms existing\nmethods by achieving higher TPRs while maintaining FPR control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u52a8\u6001\u66f4\u65b0\u8bc4\u5206\u51fd\u6570\u548c\u9608\u503c\u4ee5\u5e94\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u8f93\u5165\uff0c\u540c\u65f6\u4e25\u683c\u63a7\u5236\u5047\u6b63\u7387\uff08FPR\uff09\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e38\u9047\u5230\u5206\u5e03\u5916\u8f93\u5165\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u5206\u5e03\u5185\u6570\u636e\u8bbe\u5b9a\u9608\u503c\uff0c\u5bfc\u81f4\u5047\u6b63\u7387\u9ad8\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u4ea4\u4e92\u6846\u67b6\uff0c\u52a8\u6001\u66f4\u65b0\u8bc4\u5206\u51fd\u6570\u548c\u9608\u503c\uff0c\u7ed3\u5408\u5b9e\u65f6\u89c2\u5bdf\u7684OOD\u8f93\u5165\uff0c\u6700\u5927\u5316\u771f\u6b63\u7387\uff08TPR\uff09\u5e76\u4e25\u683c\u63a7\u5236FPR\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u5728\u5e73\u7a33\u6761\u4ef6\u4e0bFPR\u7684\u53ef\u63a7\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5728OpenOOD\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u9ad8TPR\u5e76\u4fdd\u6301FPR\u63a7\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aOOD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u52a8\u6001\u73af\u5883\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2505.02835", "pdf": "https://arxiv.org/pdf/2505.02835", "abs": "https://arxiv.org/abs/2505.02835", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Xiao Hu", "Chaoyou Fu", "Bin Wen", "Tianke Zhang", "Changyi Liu", "Kaiyu Jiang", "Kaibing Chen", "Kaiyu Tang", "Haojie Ding", "Jiankang Chen", "Fan Yang", "Zhang Zhang", "Tingting Gao", "Liang Wang"], "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Home page: https://github.com/yfzhang114/r1_reward", "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u6765\u63d0\u5347\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff08MRMs\uff09\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u4e86StableReinforce\u7b97\u6cd5\uff0c\u5e76\u572820\u4e07\u6761\u504f\u597d\u6570\u636e\u4e0a\u8bad\u7ec3\u4e86R1-Reward\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5956\u52b1\u5efa\u6a21\u7684\u57fa\u51c6\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6a21\u578b\u7ed3\u6784\u548c\u8bad\u7ec3\u6570\u636e\u7684\u6539\u8fdb\u4e0a\uff0c\u800c\u5bf9\u957f\u671f\u63a8\u7406\u80fd\u529b\u7684\u63a2\u7d22\u4e0d\u8db3\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u957f\u671f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5c06\u5956\u52b1\u5efa\u6a21\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684RL\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86StableReinforce\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86\u8bad\u7ec3\u635f\u5931\u3001\u4f18\u52bf\u4f30\u8ba1\u7b56\u7565\u548c\u5956\u52b1\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "result": "R1-Reward\u6a21\u578b\u5728\u591a\u6a21\u6001\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff08VL Reward-Bench\u63d0\u53478.4%\uff0cMultimodal Reward Bench\u63d0\u534714.3%\uff09\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u589e\u52a0\u65f6\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u4f18\u5316\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0cStableReinforce\u7b97\u6cd5\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u7a33\u5b9a\u6027\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.02308", "pdf": "https://arxiv.org/pdf/2505.02308", "abs": "https://arxiv.org/abs/2505.02308", "authors": ["Gianluca Fabiani", "Hannes Vandecasteele", "Somdatta Goswami", "Constantinos Siettos", "Ioannis G. Kevrekidis"], "title": "Enabling Local Neural Operators to perform Equation-Free System-Level Analysis", "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA", "stat.ML", "68T05, 62M45, 65P30, 65J15, 65J22, 65F15, 41A35, 47J25, 35B40,\n  37M20, 37N30", "G.1.8; G.1.5; G.1.3; G.1.10; G.4; I.2.6; I.6.5; J.2"], "comment": "33 pages, 9 figures", "summary": "Neural Operators (NOs) provide a powerful framework for computations\ninvolving physical laws that can be modelled by (integro-) partial differential\nequations (PDEs), directly learning maps between infinite-dimensional function\nspaces that bypass both the explicit equation identification and their\nsubsequent numerical solving. Still, NOs have so far primarily been employed to\nexplore the dynamical behavior as surrogates of brute-force temporal\nsimulations/predictions. Their potential for systematic rigorous numerical\nsystem-level tasks, such as fixed-point, stability, and bifurcation analysis -\ncrucial for predicting irreversible transitions in real-world phenomena -\nremains largely unexplored. Toward this aim, inspired by the Equation-Free\nmultiscale framework, we propose and implement a framework that integrates\n(local) NOs with advanced iterative numerical methods in the Krylov subspace,\nso as to perform efficient system-level stability and bifurcation analysis of\nlarge-scale dynamical systems. Beyond fixed point, stability, and bifurcation\nanalysis enabled by local in time NOs, we also demonstrate the usefulness of\nlocal in space as well as in space-time (\"patch\") NOs in accelerating the\ncomputer-aided analysis of spatiotemporal dynamics. We illustrate our framework\nvia three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes\nmultiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE,\nwhich features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN)\nmodel, consisting of two coupled PDEs that exhibit both Hopf and saddle-node\nbifurcations.", "AI": {"tldr": "NOs\u88ab\u7528\u4e8e\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\uff0c\u4f46\u5176\u5728\u7cfb\u7edf\u7ea7\u4efb\u52a1\uff08\u5982\u56fa\u5b9a\u70b9\u3001\u7a33\u5b9a\u6027\u548c\u5206\u5c94\u5206\u6790\uff09\u4e0a\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002\u672c\u6587\u63d0\u51fa\u7ed3\u5408NOs\u4e0eKrylov\u5b50\u7a7a\u95f4\u6570\u503c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u7cfb\u7edf\u7ea7\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u975e\u7ebf\u6027PDE\u6848\u4f8b\u9a8c\u8bc1\u3002", "motivation": "\u63a2\u7d22NOs\u5728\u7cfb\u7edf\u7ea7\u5206\u6790\u4efb\u52a1\uff08\u5982\u56fa\u5b9a\u70b9\u3001\u7a33\u5b9a\u6027\u548c\u5206\u5c94\u5206\u6790\uff09\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5f25\u8865\u5f53\u524dNOs\u4e3b\u8981\u7528\u4e8e\u52a8\u529b\u5b66\u6a21\u62df\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u5c40\u90e8NOs\u4e0eKrylov\u5b50\u7a7a\u95f4\u9ad8\u7ea7\u6570\u503c\u65b9\u6cd5\uff0c\u63d0\u51fa\u6846\u67b6\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u52a8\u529b\u5b66\u7684\u7a33\u5b9a\u6027\u548c\u5206\u5c94\u5206\u6790\uff0c\u5e76\u6db5\u76d6\u7a7a\u95f4\u3001\u65f6\u95f4\u53ca\u65f6\u7a7a\u5c40\u90e8NOs\u7684\u52a0\u901f\u5206\u6790\u80fd\u529b\u3002", "result": "\u901a\u8fc71D Allen-Cahn\u65b9\u7a0b\u3001Liouville-Bratu-Gelfand PDE\u548cFHN\u6a21\u578b\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u5c55\u793aNOs\u5728\u5904\u7406\u5206\u5c94\u3001\u4e34\u754c\u70b9\u7b49\u590d\u6742\u52a8\u6001\u7684\u80fd\u529b\u3002", "conclusion": "NOs\u7ed3\u5408\u6570\u503c\u65b9\u6cd5\u53ef\u9ad8\u6548\u5b8c\u6210\u7cfb\u7edf\u7ea7\u5206\u6790\uff0c\u5c24\u5176\u5728\u5904\u7406\u975e\u7ebf\u6027PDE\u7684\u7a33\u5b9a\u6027\u548c\u5206\u5c94\u95ee\u9898\u65f6\u8868\u73b0\u4f18\u8d8a\uff0c\u6269\u5c55\u4e86NOs\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.01632", "pdf": "https://arxiv.org/pdf/2505.01632", "abs": "https://arxiv.org/abs/2505.01632", "authors": ["Noussaiba Djeffal", "Djamel Addou", "Hamza Kheddar", "Sid Ahmed Selouani"], "title": "Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Addressing the detrimental impact of non-stationary environmental noise on\nautomatic speech recognition (ASR) has been a persistent and significant\nresearch focus. Despite advancements, this challenge continues to be a major\nconcern. Recently, data-driven supervised approaches, such as deep neural\nnetworks, have emerged as promising alternatives to traditional unsupervised\nmethods. With extensive training, these approaches have the potential to\novercome the challenges posed by diverse real-life acoustic environments. In\nthis light, this paper introduces a novel neural framework that incorporates a\nrobust frontend into ASR systems in both clean and noisy environments.\nUtilizing the Aurora-2 speech database, the authors evaluate the effectiveness\nof an acoustic feature set for Mel-frequency, employing the approach of\ntransfer learning based on Residual neural network (ResNet). The experimental\nresults demonstrate a significant improvement in recognition accuracy compared\nto convolutional neural networks (CNN) and long short-term memory (LSTM)\nnetworks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9c81\u68d2\u524d\u7aef\u5230ASR\u7cfb\u7edf\u4e2d\uff0c\u6539\u5584\u4e86\u5728\u5e72\u51c0\u548c\u5608\u6742\u73af\u5883\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\uff0c\u4f7f\u7528\u57fa\u4e8eResNet\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8bc6\u522b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u975e\u5e73\u7a33\u73af\u5883\u566a\u58f0\u5bf9\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u6570\u636e\u9a71\u52a8\u7684\u76d1\u7763\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u76f8\u6bd4\u4f20\u7edf\u65e0\u76d1\u7763\u65b9\u6cd5\u5c55\u73b0\u51fa\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u7ed3\u5408\u9c81\u68d2\u524d\u7aef\u5230ASR\u7cfb\u7edf\u4e2d\uff0c\u5229\u7528Aurora-2\u8bed\u97f3\u6570\u636e\u5e93\u8bc4\u4f30\u57fa\u4e8eResNet\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4CNN\u548cLSTM\u7f51\u7edc\uff0c\u8be5\u65b9\u6cd5\u5728\u5e72\u51c0\u548c\u5608\u6742\u73af\u5883\u4e0b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u5206\u522b\u4e3a98.94%\u548c91.21%\u3002", "conclusion": "\u8be5\u795e\u7ecf\u6846\u67b6\u53ca\u5176\u65b9\u6cd5\u5728ASR\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.02360", "pdf": "https://arxiv.org/pdf/2505.02360", "abs": "https://arxiv.org/abs/2505.02360", "authors": ["Fares B. Mehouachi", "Saif Eddin Jabari"], "title": "Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adversarial training is a cornerstone of robust deep learning, but fast\nmethods like the Fast Gradient Sign Method (FGSM) often suffer from\nCatastrophic Overfitting (CO), where models become robust to single-step\nattacks but fail against multi-step variants. While existing solutions rely on\nnoise injection, regularization, or gradient clipping, we propose a novel\nsolution that purely controls the $l^p$ training norm to mitigate CO.\n  Our study is motivated by the empirical observation that CO is more prevalent\nunder the $l^{\\infty}$ norm than the $l^2$ norm. Leveraging this insight, we\ndevelop a framework for generalized $l^p$ attack as a fixed point problem and\ncraft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to\n$l^{\\infty}$. This leads to our core insight: CO emerges when highly\nconcentrated gradients where information localizes in few dimensions interact\nwith aggressive norm constraints. By quantifying gradient concentration through\nParticipation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM\nthat automatically tunes the training norm based on gradient information.\nExtensive experiments demonstrate that this approach achieves strong robustness\nwithout requiring additional regularization or noise injection, providing a\nnovel and theoretically-principled pathway to mitigate the CO problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u63a7\u5236\u8bad\u7ec3\u4e2d\u7684$l^p$\u8303\u6570\u6765\u7f13\u89e3\u5bf9\u6297\u8bad\u7ec3\u4e2d\u201c\u707e\u96be\u6027\u8fc7\u62df\u5408\u201d\uff08CO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5982\u566a\u58f0\u6ce8\u5165\u6216\u6b63\u5219\u5316\u7684\u9700\u6c42\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u8303\u6570\u4e0bCO\u7684\u51fa\u73b0\u673a\u5236\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684$l^p$-FGSM\u653b\u51fb\u6846\u67b6\uff0c\u80fd\u6839\u636e\u68af\u5ea6\u4fe1\u606f\u81ea\u52a8\u8c03\u6574\u8bad\u7ec3\u8303\u6570\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u5bf9\u6297\u8bad\u7ec3\u4e2d\uff0c\u5feb\u901f\u65b9\u6cd5\uff08\u5982FGSM\uff09\u5e38\u56e0\u201c\u707e\u96be\u6027\u8fc7\u62df\u5408\u201d\uff08CO\uff09\u5bfc\u81f4\u6a21\u578b\u5bf9\u591a\u6b65\u653b\u51fb\u5931\u6548\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u566a\u58f0\u6ce8\u5165\u6216\u6b63\u5219\u5316\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0CO\u4e0e\u8bad\u7ec3\u8303\u6570\u7684\u9009\u62e9\u5bc6\u5207\u76f8\u5173\uff0c\u5c24\u5176\u662f$l^{\\infty}$\u8303\u6570\u4e0b\u66f4\u6613\u51fa\u73b0\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8303\u6570\u5bf9CO\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u5e72\u9884\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06$l^p$\u8303\u6570\u653b\u51fb\u5efa\u6a21\u4e3a\u56fa\u5b9a\u70b9\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1$l^p$-FGSM\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u96c6\u4e2d\u5ea6\uff08\u5982\u53c2\u4e0e\u6bd4\u548c\u71b5\uff09\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8303\u6570\uff0c\u4ece\u800c\u907f\u514dCO\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u566a\u58f0\u6216\u6b63\u5219\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u591a\u6b65\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u8303\u6570\u63a7\u5236\u5bf9\u7f13\u89e3CO\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u68af\u5ea6\u96c6\u4e2d\u5ea6\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u8bad\u7ec3\u8303\u6570\uff0c\u672c\u6587\u4e3a\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684CO\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u7814\u7a76\u9014\u5f84\u3002"}}
{"id": "2505.01635", "pdf": "https://arxiv.org/pdf/2505.01635", "abs": "https://arxiv.org/abs/2505.01635", "authors": ["A N M Nafiul Islam", "Xuezhong Niu", "Jiahui Duan", "Shubham Kumar", "Kai Ni", "Abhronil Sengupta"], "title": "Dendritic Computing with Multi-Gate Ferroelectric Field-Effect Transistors", "categories": ["cs.ET", "cs.AI"], "comment": null, "summary": "Although inspired by neuronal systems in the brain, artificial neural\nnetworks generally employ point-neurons, which offer far less computational\ncomplexity than their biological counterparts. Neurons have dendritic arbors\nthat connect to different sets of synapses and offer local non-linear\naccumulation - playing a pivotal role in processing and learning. Inspired by\nthis, we propose a novel neuron design based on a multi-gate ferroelectric\nfield-effect transistor that mimics dendrites. It leverages ferroelectric\nnonlinearity for local computations within dendritic branches, while utilizing\nthe transistor action to generate the final neuronal output. The branched\narchitecture paves the way for utilizing smaller crossbar arrays in hardware\nintegration, leading to greater efficiency. Using an experimentally calibrated\ndevice-circuit-algorithm co-simulation framework, we demonstrate that networks\nincorporating our dendritic neurons achieve superior performance in comparison\nto much larger networks without dendrites ($\\sim$17$\\times$ fewer trainable\nweight parameters). These findings suggest that dendritic hardware can\nsignificantly improve computational efficiency, and learning capacity of\nneuromorphic systems optimized for edge applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6805\u94c1\u7535\u573a\u6548\u5e94\u6676\u4f53\u7ba1\u7684\u65b0\u578b\u795e\u7ecf\u5143\u8bbe\u8ba1\uff0c\u6a21\u62df\u6811\u7a81\u529f\u80fd\uff0c\u901a\u8fc7\u94c1\u7535\u975e\u7ebf\u6027\u5b9e\u73b0\u5c40\u90e8\u8ba1\u7b97\uff0c\u5e76\u5229\u7528\u6676\u4f53\u7ba1\u52a8\u4f5c\u751f\u6210\u795e\u7ecf\u5143\u8f93\u51fa\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u4ee5\u66f4\u5c11\u53c2\u6570\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4f7f\u7528\u70b9\u795e\u7ecf\u5143\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u8fdc\u4f4e\u4e8e\u751f\u7269\u795e\u7ecf\u5143\u3002\u751f\u7269\u795e\u7ecf\u5143\u7684\u6811\u7a81\u7ed3\u6784\u548c\u5c40\u90e8\u975e\u7ebf\u6027\u79ef\u7d2f\u5728\u5b66\u4e60\u548c\u5904\u7406\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "method": "\u57fa\u4e8e\u591a\u6805\u94c1\u7535\u573a\u6548\u5e94\u6676\u4f53\u7ba1\u8bbe\u8ba1\u65b0\u578b\u795e\u7ecf\u5143\uff0c\u5229\u7528\u94c1\u7535\u975e\u7ebf\u6027\u5b9e\u73b0\u6811\u7a81\u5206\u652f\u5185\u7684\u5c40\u90e8\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u6676\u4f53\u7ba1\u52a8\u4f5c\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002\u91c7\u7528\u5b9e\u9a8c\u6821\u51c6\u7684\u8bbe\u5907-\u7535\u8def-\u7b97\u6cd5\u534f\u540c\u4eff\u771f\u6846\u67b6\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u4e0e\u65e0\u6811\u7a81\u7684\u5927\u89c4\u6a21\u7f51\u7edc\u76f8\u6bd4\uff0c\u8fd9\u79cd\u6811\u7a81\u795e\u7ecf\u5143\u7f51\u7edc\u4ec5\u9700\u7ea61/17\u7684\u53ef\u8bad\u7ec3\u6743\u91cd\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "\u8fd9\u79cd\u6811\u7a81\u786c\u4ef6\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u8fb9\u7f18\u5e94\u7528\u573a\u666f\u4e0b\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u7684\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2505.02369", "pdf": "https://arxiv.org/pdf/2505.02369", "abs": "https://arxiv.org/abs/2505.02369", "authors": ["Juyoung Yun"], "title": "Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "cs.NE", "math.IT"], "comment": null, "summary": "Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization.", "AI": {"tldr": "ZSharp\u901a\u8fc7\u5c42\u95f4Z-score\u6807\u51c6\u5316\u548c\u767e\u5206\u4f4d\u8fc7\u6ee4\u663e\u8457\u68af\u5ea6\u6210\u5206\uff0c\u6539\u8fdbSAM\u65b9\u6cd5\uff0c\u63d0\u5347\u6df1\u5ea6\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u6539\u52a8\u67b6\u6784\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6613\u6536\u655b\u5230\u5c16\u9510\u6700\u5c0f\u503c\uff0c\u964d\u4f4e\u6a21\u578b\u9c81\u68d2\u6027\uff0cSAM\u65b9\u6cd5\u867d\u80fd\u5bfb\u627e\u5e73\u5766\u6700\u5c0f\u503c\uff0c\u4f46\u68af\u5ea6\u6270\u52a8\u53ef\u80fd\u5305\u542b\u4e0d\u663e\u8457\u65b9\u5411\u3002ZSharp\u65e8\u5728\u4f18\u5316\u8fd9\u4e00\u70b9\u3002", "method": "ZSharp\u5728SAM\u57fa\u7840\u4e0a\u589e\u52a0\u5c42\u95f4Z-score\u6807\u51c6\u5316\u548c\u767e\u5206\u4f4d\u8fc7\u6ee4\uff0c\u4fdd\u7559\u7edf\u8ba1\u663e\u8457\u68af\u5ea6\u6210\u5206\uff0c\u51cf\u5c11\u6270\u52a8\u65b9\u5411\u566a\u58f0\u3002", "result": "\u5728CIFAR-10/100\u548cTiny-ImageNet\u4e0a\uff0cZSharp\u5728ResNet\u3001VGG\u548cViT\u4e0a\u6d4b\u8bd5\u7cbe\u5ea6\u4f18\u4e8eSAM\u53ca\u5176\u53d8\u4f53\uff0c\u5c24\u5176\u5728\u6df1\u5c42\u548cTransformer\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "ZSharp\u662f\u8f7b\u91cf\u4e14\u539f\u7406\u6e05\u6670\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u9510\u5ea6\u611f\u77e5\u7684\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2505.01638", "pdf": "https://arxiv.org/pdf/2505.01638", "abs": "https://arxiv.org/abs/2505.01638", "authors": ["Michael Marinaccio", "Fatemeh Afghah"], "title": "Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.4.6; I.4.8"], "comment": "7 pages, 4 figures, 4 tables", "summary": "High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)\ntypically requires multimodal sensing - especially RGB and thermal imagery -\nwhich increases hardware cost and power consumption. This paper introduces\nSAM-TIFF, a novel teacher-student distillation framework for pixel-level\nwildfire temperature prediction and segmentation using RGB input only. A\nmultimodal teacher network trained on paired RGB-Thermal imagery and\nradiometric TIFF ground truth distills knowledge to a unimodal RGB student\nnetwork, enabling thermal-sensor-free inference. Segmentation supervision is\ngenerated using a hybrid approach of segment anything (SAM)-guided mask\ngeneration, and selection via TOPSIS, along with Canny edge detection and\nOtsu's thresholding pipeline for automatic point prompt selection. Our method\nis the first to perform per-pixel temperature regression from RGB UAV data,\ndemonstrating strong generalization on the recent FLAME 3 dataset. This work\nlays the foundation for lightweight, cost-effective UAV-based wildfire\nmonitoring systems without thermal sensors.", "AI": {"tldr": "SAM-TIFF: \u57fa\u4e8eRGB\u8f93\u5165\u7684\u65e0\u4eba\u673a\u68ee\u6797\u706b\u707e\u6e29\u5ea6\u9884\u6d4b\u4e0e\u5206\u5272\u6846\u67b6\uff0c\u65e0\u9700\u70ed\u6210\u50cf\u4f20\u611f\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u7cbe\u5ea6\u68ee\u6797\u706b\u707e\u76d1\u6d4b\u901a\u5e38\u9700\u8981\u591a\u6a21\u6001\u4f20\u611f\u5668\uff08RGB\u548c\u70ed\u6210\u50cf\uff09\uff0c\u4f46\u589e\u52a0\u4e86\u786c\u4ef6\u6210\u672c\u548c\u80fd\u8017\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4ec5\u4f7f\u7528RGB\u8f93\u5165\u5b9e\u73b0\u70ed\u6210\u50cf\u4f20\u611f\u5668\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faSAM-TIFF\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6559\u5e08\u7f51\u7edc\uff08\u57fa\u4e8eRGB-\u70ed\u6210\u50cf\u914d\u5bf9\u6570\u636e\uff09\u5411\u5355\u6a21\u6001RGB\u5b66\u751f\u7f51\u7edc\u84b8\u998f\u77e5\u8bc6\u3002\u5206\u5272\u76d1\u7763\u91c7\u7528SAM\u5f15\u5bfc\u7684\u63a9\u6a21\u751f\u6210\u3001TOPSIS\u9009\u62e9\u3001Canny\u8fb9\u7f18\u68c0\u6d4b\u548cOtsu\u9608\u503c\u5904\u7406\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u5728FLAME 3\u6570\u636e\u96c6\u4e0a\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8eRGB\u6570\u636e\u7684\u50cf\u7d20\u7ea7\u6e29\u5ea6\u56de\u5f52\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8f7b\u91cf\u3001\u4f4e\u6210\u672c\u7684\u65e0\u4eba\u673a\u706b\u707e\u76d1\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u65e0\u9700\u70ed\u6210\u50cf\u4f20\u611f\u5668\u3002"}}
{"id": "2505.02380", "pdf": "https://arxiv.org/pdf/2505.02380", "abs": "https://arxiv.org/abs/2505.02380", "authors": ["Arnab Sanyal", "Prithwish Mukherjee", "Gourav Datta", "Sandeep P. Chinchali"], "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices", "categories": ["cs.LG"], "comment": "6 pages, 1 reference page. Under submission and review at ISLPED 2025", "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9%$ - $146.6%$ faster inference throughput on\nmemory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing\nthe required data movement. The proposed approach requires no additional\nre-training and is fully compatible with existing post-training quantization\nmethods, making it a practical solution for edge LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86EntroLLM\uff0c\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u91cf\u5316\u548c\u71b5\u7f16\u7801\u7684\u538b\u7f29\u6846\u67b6\uff0c\u5728\u4e0d\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\uff0c\u5e76\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u7136\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c42\u7ea7\u6df7\u5408\u91cf\u5316\u7b56\u7565\uff08\u5bf9\u79f0\u548c\u4e0d\u5bf9\u79f0\u91cf\u5316\uff09\u4f18\u5316\u538b\u7f29\u6027\uff0c\u5e76\u4f7f\u7528Huffman\u7f16\u7801\u5bf9\u91cf\u5316\u6743\u91cd\u8fdb\u884c\u65e0\u635f\u538b\u7f29\uff0c\u540c\u65f6\u5f15\u5165\u5e76\u884cHuffman\u89e3\u7801\u4ee5\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u591a\u4e2a\u8fb9\u6cbf\u517c\u5bb9\u7684LLM\uff08\u5982smolLM-1.7B-Instruct\u3001phi3-mini-4k-Instruct\u7b49\uff09\u4e0a\uff0cEntroLLM\u5b58\u50a8\u51cf\u5c11\u8fbe30%\uff08\u76f8\u5bf9\u4e8euint8\uff09\u548c65%\uff08\u76f8\u5bf9\u4e8euint4\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\uff1b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u63a8\u7406\u541e\u5410\u91cf\u63d0\u534731.9%-146.6%\u3002", "conclusion": "EntroLLM\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5e76\u517c\u5bb9\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u5316\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2505.01647", "pdf": "https://arxiv.org/pdf/2505.01647", "abs": "https://arxiv.org/abs/2505.01647", "authors": ["Mingfeng Li", "Weijie Zheng", "Benjamin Doerr"], "title": "Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy", "categories": ["cs.NE", "cs.AI"], "comment": "Initial version of one paper accepted by IJCAI2025", "summary": "Different from single-objective evolutionary algorithms, where non-elitism is\nan established concept, multi-objective evolutionary algorithms almost always\nselect the next population in a greedy fashion. In the only notable exception,\nBian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism\nfor the SMS-EMOA and proved that it can speed up computing the Pareto front of\nthe bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by\na factor of $\\max\\{1,2^{k/4}/n\\}$. While this constitutes the first proven\nspeed-up from non-elitist selection, suggesting a very interesting research\ndirection, it has to be noted that a true speed-up only occurs for $k \\ge\n4\\log_2(n)$, where the runtime is super-polynomial, and that the advantage\nreduces for larger numbers of objectives as shown in a later work. In this\nwork, we propose a different non-elitist selection mechanism based on aging,\nwhich exempts individuals younger than a certain age from a possible removal.\nThis remedies the two shortcomings of stochastic selection: We prove a speed-up\nby a factor of $\\max\\{1,\\Theta(k)^{k-1}\\}$, regardless of the number of\nobjectives. In particular, a positive speed-up can already be observed for\nconstant $k$, the only setting for which polynomial runtimes can be witnessed.\nOverall, this result supports the use of non-elitist selection schemes, but\nsuggests that aging-based mechanisms can be considerably more powerful than\nstochastic selection mechanisms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e74\u9f84\u7684\u975e\u7cbe\u82f1\u9009\u62e9\u673a\u5236\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u968f\u673a\u9009\u62e9\u673a\u5236\uff0c\u80fd\u591f\u5728\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u4e2d\u66f4\u5feb\u8ba1\u7b97\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5c24\u5176\u5728\u5e38\u6570 $k$ \u65f6\u5373\u80fd\u63d0\u901f\u3002", "motivation": "\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\u901a\u5e38\u91c7\u7528\u8d2a\u5a6a\u9009\u62e9\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u975e\u7cbe\u82f1\u9009\u62e9\u53ef\u80fd\u52a0\u901f\u8ba1\u7b97\u3002\u4e4b\u524d\u63d0\u51fa\u7684\u968f\u673a\u9009\u62e9\u673a\u5236\u5b58\u5728\u5c40\u9650\u6027\uff08\u5982\u4ec5\u5728\u9ad8\u590d\u6742\u5ea6\u7684\u8d85\u7ea7\u591a\u9879\u5f0f\u8fd0\u884c\u65f6\u6709\u6548\uff09\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e74\u9f84\u7684\u975e\u7cbe\u82f1\u9009\u62e9\u673a\u5236\uff0c\u8c41\u514d\u5e74\u8f7b\u4e2a\u4f53\u88ab\u79fb\u9664\u7684\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u4f18\u5316\u9009\u62e9\u8fc7\u7a0b\u3002", "result": "\u65b0\u673a\u5236\u5728\u5e38\u6570 $k$ \u65f6\u5373\u80fd\u63d0\u901f\uff0c\u4e14\u4e0d\u53d7\u76ee\u6807\u6570\u91cf\u7684\u5f71\u54cd\uff0c\u901f\u5ea6\u63d0\u5347\u56e0\u5b50\u4e3a $\\max\\{1,\\Theta(k)^{k-1}\\}$\u3002", "conclusion": "\u57fa\u4e8e\u5e74\u9f84\u7684\u975e\u7cbe\u82f1\u9009\u62e9\u673a\u5236\u6bd4\u968f\u673a\u9009\u62e9\u66f4\u6709\u6548\uff0c\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u975e\u7cbe\u82f1\u9009\u62e9\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02383", "pdf": "https://arxiv.org/pdf/2505.02383", "abs": "https://arxiv.org/abs/2505.02383", "authors": ["Bingshan Hu", "Zhiming Huang", "Tianyue H. Zhang", "Mathias L\u00e9cuyer", "Nidhi Hegde"], "title": "Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret", "categories": ["cs.LG"], "comment": "Accepted by ICML 2025", "summary": "We address differentially private stochastic bandit problems from the angles\nof exploring the deep connections among Thompson Sampling with Gaussian priors,\nGaussian mechanisms, and Gaussian differential privacy (GDP). We propose\nDP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade\noff privacy and regret. DP-TS-UCB satisfies $ \\tilde{O}\n\\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O\n\\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$\ncontrols the trade-off between privacy and regret. Theoretically, our DP-TS-UCB\nrelies on anti-concentration bounds of Gaussian distributions and links\nexploration mechanisms in Thompson Sampling-based algorithms and Upper\nConfidence Bound-based algorithms, which may be of independent interest.", "AI": {"tldr": "\u63d0\u51fa\u4e86DP-TS-UCB\u7b97\u6cd5\uff0c\u4e00\u79cd\u53c2\u6570\u5316\u7684\u79c1\u6709\u8d4c\u535a\u7b97\u6cd5\uff0c\u4ee5\u63a2\u7d22\u9ad8\u65af\u5dee\u5206\u9690\u79c1\u5728\u968f\u673a\u8d4c\u535a\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u5728\u9690\u79c1\u6027\u4e0e\u9057\u61be\u503c\u4e4b\u95f4\u63d0\u4f9b\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u5dee\u5206\u9690\u79c1\u968f\u673a\u8d4c\u535a\u95ee\u9898\uff0c\u63a2\u7d22Thompson\u91c7\u6837\u3001\u9ad8\u65af\u673a\u5236\u4e0e\u9ad8\u65af\u5dee\u5206\u9690\u79c1\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u5728\u9690\u79c1\u6027\u4e0e\u540e\u6094\u503c\u4e4b\u95f4\u6743\u8861\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86DP-TS-UCB\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86Thompson\u91c7\u6837\u7684\u63a2\u7d22\u673a\u5236\u548cUpper Confidence Bound (UCB)\u7684\u601d\u60f3\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u03b1\u5728\u9690\u79c1\u6027\u548c\u9057\u61be\u503c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "result": "\u7b97\u6cd5\u6ee1\u8db3GDP\u9690\u79c1\u4fdd\u8bc1\uff0c\u9057\u61be\u503c\u4e0a\u754c\u4e3a\ud835\udc42(\ud835\udc3e\ud835\udc59\ud835\udc5b\ud835\udefc+1(\ud835\udc47)/\u0394)\uff0c\u901a\u8fc7\u53c2\u6570\u03b1\u63a7\u5236\u9690\u79c1\u4e0e\u9057\u61be\u7684\u6743\u8861\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7b97\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u969c\u548c\u6027\u80fd\u4fdd\u8bc1\uff0c\u8fd8\u63ed\u793a\u4e86Thompson\u91c7\u6837\u548cUCB\u63a2\u7d22\u673a\u5236\u7684\u5173\u8054\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2505.02390", "pdf": "https://arxiv.org/pdf/2505.02390", "abs": "https://arxiv.org/abs/2505.02390", "authors": ["Enbo Zhao", "Yi Shen", "Shuming Shi", "Jieyun Huang", "Zhihao Chen", "Ning Wang", "Siqi Xiao", "Jian Zhang", "Kai Wang", "Shiguo Lian"], "title": "Quantitative Analysis of Performance Drop in DeepSeek Model Quantization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,\npossibly because the official service often suffers from being busy and some\norganizations have data privacy concerns. While single-machine deployment\noffers infrastructure simplicity, the models' 671B FP8 parameter configuration\nexceeds the practical memory limits of a standard 8-GPU machine. Quantization\nis a widely used technique that helps reduce model memory consumption. However,\nit is unclear what the performance of DeepSeek-R1 and V3 will be after being\nquantized. This technical report presents the first quantitative evaluation of\nmulti-bitwidth quantization across the complete DeepSeek model spectrum. Key\nfindings reveal that 4-bit quantization maintains little performance\ndegradation versus FP8 while enabling single-machine deployment on standard\nNVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization\nmethod that significantly outperforms traditional Q3_K_M variant on various\nbenchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach\nin most tasks. Moreover, DQ3_K_M supports single-machine deployment\nconfigurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of\nDQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing\noptimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.", "AI": {"tldr": "DeepSeek-R1\u548cV3\u6a21\u578b\u76844\u4f4d\u91cf\u5316\u5728\u6027\u80fd\u51e0\u4e4e\u4e0d\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\u53ef\u5b9e\u73b0\u5355\u673a\u90e8\u7f72\uff0c\u5e76\u63d0\u51fa\u52a8\u60013\u4f4d\u91cf\u5316\u65b9\u6cd5DQ3_K_M\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf3\u4f4d\u91cf\u5316\u4e14\u63a5\u8fd14\u4f4d\u91cf\u5316\u6548\u679c\uff0c\u652f\u6301\u591a\u79cdGPU\u8bbe\u5907\u3002", "motivation": "\u7531\u4e8e\u5b98\u65b9\u670d\u52a1\u5e38\u7e41\u5fd9\u4e14\u5b58\u5728\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u672c\u5730\u90e8\u7f72DeepSeek-R1\u548cV3\u6a21\u578b\u9700\u6c42\u9ad8\uff0c\u4f46\u5176\u5927\u89c4\u6a21\u53c2\u6570\u8d85\u51fa\u5355\u673a\u5185\u5b58\u9650\u5236\uff0c\u9700\u901a\u8fc7\u91cf\u5316\u6280\u672f\u89e3\u51b3\u3002", "method": "\u5bf9DeepSeek\u5168\u7cfb\u6a21\u578b\u8fdb\u884c\u4e86\u591a\u6bd4\u7279\u5bbd\u5ea6\u91cf\u5316\u7684\u9996\u6b21\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u52a8\u60013\u4f4d\u91cf\u5316\u65b9\u6cd5DQ3_K_M\u3002", "result": "4\u4f4d\u91cf\u5316\u6027\u80fd\u63a5\u8fd1FP8\uff0c\u652f\u6301\u5355\u673a\u90e8\u7f72\uff1bDQ3_K_M\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f20\u7edf3\u4f4d\u91cf\u5316\uff0c\u4e14\u4e0e4\u4f4d\u91cf\u5316\u6548\u679c\u76f8\u5f53\uff0c\u517c\u5bb9\u591a\u79cdGPU\u8bbe\u5907\u3002", "conclusion": "DQ3_K_M\u4e3aDeepSeek\u6a21\u578b\u7684\u9ad8\u6548\u5355\u673a\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2505.01664", "pdf": "https://arxiv.org/pdf/2505.01664", "abs": "https://arxiv.org/abs/2505.01664", "authors": ["Yi-Ming Zhai", "Chuan-Xian Ren", "Hong Yan"], "title": "Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual domain adaptation aims to learn discriminative and domain-invariant\nrepresentation for an unlabeled target domain by leveraging knowledge from a\nlabeled source domain. Partial domain adaptation (PDA) is a general and\npractical scenario in which the target label space is a subset of the source\none. The challenges of PDA exist due to not only domain shift but also the\nnon-identical label spaces of domains. In this paper, a Soft-masked Semi-dual\nOptimal Transport (SSOT) method is proposed to deal with the PDA problem.\nSpecifically, the class weights of domains are estimated, and then a reweighed\nsource domain is constructed, which is favorable in conducting\nclass-conditional distribution matching with the target domain. A soft-masked\ntransport distance matrix is constructed by category predictions, which will\nenhance the class-oriented representation ability of optimal transport in the\nshared feature space. To deal with large-scale optimal transport problems, the\nsemi-dual formulation of the entropy-regularized Kantorovich problem is\nemployed since it can be optimized by gradient-based algorithms. Further, a\nneural network is exploited to approximate the Kantorovich potential due to its\nstrong fitting ability. This network parametrization also allows the\ngeneralization of the dual variable outside the supports of the input\ndistribution. The SSOT model is built upon neural networks, which can be\noptimized alternately in an end-to-end manner. Extensive experiments are\nconducted on four benchmark datasets to demonstrate the effectiveness of SSOT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u63a9\u7801\u534a\u5bf9\u5076\u6700\u4f18\u4f20\u8f93\uff08SSOT\uff09\u65b9\u6cd5\u7528\u4e8e\u5904\u7406\u90e8\u5206\u57df\u9002\u5e94\uff08PDA\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u4f30\u8ba1\u7c7b\u522b\u6743\u91cd\u548c\u6784\u5efa\u7c7b\u522b\u6761\u4ef6\u5206\u5e03\u5339\u914d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u90e8\u5206\u57df\u9002\u5e94\uff08PDA\uff09\u7684\u6311\u6218\u5728\u4e8e\u57df\u504f\u79fb\u548c\u6807\u7b7e\u7a7a\u95f4\u4e0d\u5b8c\u5168\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7c7b\u522b\u6743\u91cd\u4f30\u8ba1\u548c\u8f6f\u63a9\u7801\u4f20\u8f93\u8ddd\u79bb\u77e9\u9635\uff0c\u7ed3\u5408\u534a\u5bf9\u5076\u6700\u4f18\u4f20\u8f93\u516c\u5f0f\u548c\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SSOT\u7684\u6709\u6548\u6027\u3002", "conclusion": "SSOT\u5728PDA\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u7c7b\u522b\u6761\u4ef6\u5339\u914d\u548c\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.01680", "pdf": "https://arxiv.org/pdf/2505.01680", "abs": "https://arxiv.org/abs/2505.01680", "authors": ["Tamim Ahmed", "Thanassis Rikakis"], "title": "Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study", "categories": ["cs.CV", "cs.AI", "cs.HC", "math.PR"], "comment": null, "summary": "Manual scoring of the Action Research Arm Test (ARAT) for upper extremity\nassessment in stroke rehabilitation is time-intensive and variable. We propose\nan automated ARAT scoring system integrating multimodal video analysis with\nSlowFast, I3D, and Transformer-based models using OpenPose keypoints and object\nlocations. Our approach employs multi-view data (ipsilateral, contralateral,\nand top perspectives), applying early and late fusion to combine features\nacross views and models. Hierarchical Bayesian Models (HBMs) infer movement\nquality components, enhancing interpretability. A clinician dashboard displays\ntask scores, execution times, and quality assessments. We conducted a study\nwith five clinicians who reviewed 500 video ratings generated by our system,\nproviding feedback on its accuracy and usability. Evaluated on a stroke\nrehabilitation dataset, our framework achieves 89.0% validation accuracy with\nlate fusion, with HBMs aligning closely with manual assessments. This work\nadvances automated rehabilitation by offering a scalable, interpretable\nsolution with clinical validation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7684ARAT\u8bc4\u5206\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89c6\u9891\u5206\u6790\u548c\u878d\u5408SlowFast\u3001I3D\u53caTransformer\u6a21\u578b\uff0c\u7ed3\u5408OpenPose\u5173\u952e\u70b9\u548c\u7269\u4f53\u4f4d\u7f6e\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0a\u80a2\u529f\u80fd\u7684\u9ad8\u6548\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe89%\u3002", "motivation": "\u624b\u52a8\u8bc4\u5206ARAT\uff08\u884c\u52a8\u7814\u7a76\u624b\u81c2\u6d4b\u8bd5\uff09\u5728\u5352\u4e2d\u5eb7\u590d\u4e2d\u8017\u65f6\u4e14\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u5206\u7cfb\u7edf\u6765\u63d0\u9ad8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\uff08\u540c\u4fa7\u3001\u5bf9\u4fa7\u548c\u9876\u90e8\u89c6\u89d2\uff09\uff0c\u7ed3\u5408SlowFast\u3001I3D\u548cTransformer\u6a21\u578b\uff0c\u901a\u8fc7\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u7b56\u7565\u6574\u5408\u7279\u5f81\uff0c\u5e76\u5229\u7528\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\uff08HBMs\uff09\u63a8\u65ad\u8fd0\u52a8\u8d28\u91cf\uff0c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7cfb\u7edf\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe\u523089%\uff0c\u4e14HBMs\u7684\u8bc4\u4f30\u4e0e\u4eba\u5de5\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e34\u5e8a\u533b\u751f\u53cd\u9988\u5176\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u826f\u597d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5eb7\u590d\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u5eb7\u590d\u7684\u8fdb\u5c55\u3002"}}
{"id": "2505.02402", "pdf": "https://arxiv.org/pdf/2505.02402", "abs": "https://arxiv.org/abs/2505.02402", "authors": ["Thibault de Surrel", "Florian Yger", "Fabien Lotte", "Sylvain Chevallier"], "title": "A probabilistic view on Riemannian machine learning models for SPD matrices", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "The goal of this paper is to show how different machine learning tools on the\nRiemannian manifold $\\mathcal{P}_d$ of Symmetric Positive Definite (SPD)\nmatrices can be united under a probabilistic framework. For this, we will need\nseveral Gaussian distributions defined on $\\mathcal{P}_d$. We will show how\npopular classifiers on $\\mathcal{P}_d$ can be reinterpreted as Bayes\nClassifiers using these Gaussian distributions. These distributions will also\nbe used for outlier detection and dimension reduction. By showing that those\ndistributions are pervasive in the tools used on $\\mathcal{P}_d$, we allow for\nother machine learning tools to be extended to $\\mathcal{P}_d$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6982\u7387\u6846\u67b6\u7edf\u4e00\u4e86\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u5de5\u5177\u53ef\u4ee5\u88ab\u91cd\u65b0\u89e3\u91ca\u4e3a\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5c06\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5de5\u5177\u7edf\u4e00\u5230\u4e00\u4e2a\u6982\u7387\u6846\u67b6\u4e2d\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u5de5\u5177\u5960\u5b9a\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5728\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u6d41\u5f62\u4e0a\u5f15\u5165\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u964d\u7ef4\u3002", "result": "\u5c55\u793a\u4e86\u8fd9\u4e9b\u9ad8\u65af\u5206\u5e03\u5728\u6d41\u5f62\u4e0a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f7f\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5f97\u4ee5\u6269\u5c55\u5230\u6d41\u5f62\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6982\u7387\u89c6\u89d2\uff0c\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.01694", "pdf": "https://arxiv.org/pdf/2505.01694", "abs": "https://arxiv.org/abs/2505.01694", "authors": ["Dazhi Huang"], "title": "Topology-Aware CLIP Few-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficiently adapting large Vision-Language Models (VLMs) like CLIP for\nfew-shot learning poses challenges in balancing pre-trained knowledge retention\nand task-specific adaptation. Existing methods often overlook valuable\nstructural information within the VLM's latent space. We introduce a\ntopology-aware tuning approach integrating Representation Topology Divergence\n(RTD) into the Task Residual (TR) framework. By explicitly aligning the\ntopological structures of visual and text representations using a combined RTD\nand Cross-Entropy loss, while freezing base VLM encoders, our method enhances\nfew-shot performance. We optimize only lightweight Task Residual parameters,\neffectively leveraging topological information. Across 6 diverse benchmark\ndatasets, our approach demonstrates significant gains, achieving an average\naccuracy improvement of 1-2\\% over relevant baseline methods in few-shot\nsettings. This work presents an effective strategy to boost VLM few-shot\ncapabilities by incorporating topological alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8868\u793a\u62d3\u6251\u5dee\u5f02\uff08RTD\uff09\u548c\u4efb\u52a1\u6b8b\u5dee\uff08TR\uff09\u6846\u67b6\u6765\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u6027\u80fd\uff0c\u907f\u514d\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\u4e22\u5931\u5e76\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u65f6\u5ffd\u89c6\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u4fdd\u7559\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u9002\u5e94\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u5728\u4efb\u52a1\u6b8b\u5dee\u6846\u67b6\u4e2d\u6574\u5408\u8868\u793a\u62d3\u6251\u5dee\u5f02\uff08RTD\uff09\uff0c\u540c\u65f6\u51bb\u7ed3\u57fa\u7840VLM\u7f16\u7801\u5668\uff0c\u4ec5\u4f18\u5316\u8f7b\u91cf\u7ea7\u7684\u4efb\u52a1\u6b8b\u5dee\u53c2\u6570\uff0c\u5229\u7528\u62d3\u6251\u7ed3\u6784\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u3002", "result": "\u57286\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e2d\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e861-2%\uff0c\u663e\u8457\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u5de5\u4f5c\u901a\u8fc7\u5f15\u5165\u62d3\u6251\u5bf9\u9f50\uff0c\u4e3a\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u3002"}}
{"id": "2505.02417", "pdf": "https://arxiv.org/pdf/2505.02417", "abs": "https://arxiv.org/abs/2505.02417", "authors": ["Yunfeng Ge", "Jiawei Li", "Yiji Zhao", "Haomin Wen", "Zhao Li", "Meikang Qiu", "Hongyan Li", "Ming Jin", "Shirui Pan"], "title": "T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Text-to-Time Series generation holds significant potential to address\nchallenges such as data sparsity, imbalance, and limited availability of\nmultimodal time series datasets across domains. While diffusion models have\nachieved remarkable success in Text-to-X (e.g., vision and audio data)\ngeneration, their use in time series generation remains in its nascent stages.\nExisting approaches face two critical limitations: (1) the lack of systematic\nexploration of general-proposed time series captions, which are often\ndomain-specific and struggle with generalization; and (2) the inability to\ngenerate time series of arbitrary lengths, limiting their applicability to\nreal-world scenarios. In this work, we first categorize time series captions\ninto three levels: point-level, fragment-level, and instance-level.\nAdditionally, we introduce a new fragment-level dataset containing over 600,000\nhigh-resolution time series-text pairs. Second, we propose Text-to-Series\n(T2S), a diffusion-based framework that bridges the gap between natural\nlanguage and time series in a domain-agnostic manner. T2S employs a\nlength-adaptive variational autoencoder to encode time series of varying\nlengths into consistent latent embeddings. On top of that, T2S effectively\naligns textual representations with latent embeddings by utilizing Flow\nMatching and employing Diffusion Transformer as the denoiser. We train T2S in\nan interleaved paradigm across multiple lengths, allowing it to generate\nsequences of any desired length. Extensive evaluations demonstrate that T2S\nachieves state-of-the-art performance across 13 datasets spanning 12 domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Text-to-Series\uff08T2S\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u5ea6\u81ea\u9002\u5e94\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548cFlow Matching\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u957f\u5ea6\u9650\u5236\u548c\u6587\u672c\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u4e00\u662f\u7f3a\u4e4f\u901a\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u6807\u6ce8\u65b9\u6cd5\uff0c\u4e8c\u662f\u65e0\u6cd5\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "T2S\u6846\u67b6\u7ed3\u5408\u4e86\u957f\u5ea6\u81ea\u9002\u5e94\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548cFlow Matching\u6280\u672f\uff0c\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u4f5c\u4e3a\u53bb\u566a\u5668\uff0c\u4ee5\u8de8\u957f\u5ea6\u4ea4\u66ff\u8bad\u7ec3\u7684\u65b9\u5f0f\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u65f6\u95f4\u5e8f\u5217\u7684\u751f\u6210\u3002", "result": "T2S\u572812\u4e2a\u9886\u57df\u768413\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "T2S\u901a\u8fc7\u521b\u65b0\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8de8\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2505.01696", "pdf": "https://arxiv.org/pdf/2505.01696", "abs": "https://arxiv.org/abs/2505.01696", "authors": ["Alireza Sadeghi", "Farshid Hajati", "Ahmadreza Argha", "Nigel H Lovell", "Min Yang", "Hamid Alinejad-Rokny"], "title": "Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking", "categories": ["q-bio.GN", "cs.AI"], "comment": "41 pages", "summary": "Integrating heterogeneous biomedical data including imaging, omics, and\nclinical records supports accurate diagnosis and personalised care. Graph-based\nmodels fuse such non-Euclidean data by capturing spatial and relational\nstructure, yet clinical uptake requires regulator-ready interpretability. We\npresent the first technical survey of interpretable graph based models for\nmultimodal biomedical data, covering 26 studies published between Jan 2019 and\nSep 2024. Most target disease classification, notably cancer and rely on static\ngraphs from simple similarity measures, while graph-native explainers are rare;\npost-hoc methods adapted from non-graph domains such as gradient saliency, and\nSHAP predominate. We group existing approaches into four interpretability\nfamilies, outline trends such as graph-in-graph hierarchies, knowledge-graph\nedges, and dynamic topology learning, and perform a practical benchmark. Using\nan Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient\nSaliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the\nbroadest set of known AD pathways and Gene-Ontology terms, whereas Gradient\nSaliency and Graph Masking surface complementary metabolic and transport\nsignatures. Permutation tests show all four beat random gene sets, but with\ndistinct trade-offs: SHAP and Graph Masking offer deeper biology at higher\ncompute cost, while Gradient Saliency and Sensitivity Analysis are quicker\nthough coarser. We also provide a step-by-step flowchart covering graph\nconstruction, explainer choice and resource budgeting to help researchers\nbalance transparency and performance. This review synthesises the state of\ninterpretable graph learning for multimodal medicine, benchmarks leading\ntechniques, and charts future directions, from advanced XAI tools to\nunder-studied diseases, serving as a concise reference for method developers\nand translational scientists.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e862019\u5e74\u81f32024\u5e7426\u9879\u53ef\u89e3\u91ca\u56fe\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u4e3b\u8981\u5305\u62ec\u75be\u75c5\u5206\u7c7b\uff08\u5982\u764c\u75c7\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u89e3\u91ca\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002\u57fa\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u961f\u5217\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cSHAP\u548c\u654f\u611f\u6027\u5206\u6790\u80fd\u66f4\u5168\u9762\u5730\u53d1\u73b0\u5df2\u77e5\u901a\u8def\uff0c\u800c\u68af\u5ea6\u663e\u8457\u6027\u548c\u56fe\u63a9\u7801\u5219\u63ed\u793a\u4e86\u8865\u5145\u7684\u4ee3\u8c22\u548c\u8fd0\u8f93\u7279\u5f81\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u6d41\u7a0b\u56fe\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u9009\u62e9\u65b9\u6cd5\u3002", "motivation": "\u6574\u5408\u5f02\u6784\u751f\u7269\u533b\u5b66\u6570\u636e\uff08\u5982\u5f71\u50cf\u3001\u7ec4\u5b66\u548c\u4e34\u5e8a\u8bb0\u5f55\uff09\u9700\u8981\u53ef\u89e3\u91ca\u7684\u56fe\u6a21\u578b\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u76d1\u7ba1\u8981\u6c42\u3002", "method": "\u7efc\u8ff0\u4e8626\u9879\u7814\u7a76\uff0c\u5206\u7c7b\u4e3a\u56db\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08SHAP\u3001\u68af\u5ea6\u663e\u8457\u6027\u3001\u654f\u611f\u6027\u5206\u6790\u548c\u56fe\u63a9\u7801\uff09\uff0c\u5e76\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "SHAP\u548c\u654f\u611f\u6027\u5206\u6790\u80fd\u66f4\u5168\u9762\u5730\u8bc6\u522b\u5df2\u77e5\u901a\u8def\uff0c\u68af\u5ea6\u663e\u8457\u6027\u548c\u56fe\u63a9\u7801\u5219\u63d0\u4f9b\u4e86\u8865\u5145\u4fe1\u606f\u3002\u4e0d\u540c\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u548c\u7cbe\u5ea6\u4e0a\u6709\u660e\u663e\u6743\u8861\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u53ef\u89e3\u91ca\u56fe\u5b66\u4e60\u7684\u73b0\u72b6\uff0c\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u6307\u5357\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u9ad8\u7ea7\u53ef\u89e3\u91ca\u5de5\u5177\u548c\u672a\u5145\u5206\u7814\u7a76\u7684\u75be\u75c5\uff09\u3002"}}
{"id": "2505.02426", "pdf": "https://arxiv.org/pdf/2505.02426", "abs": "https://arxiv.org/abs/2505.02426", "authors": ["Flora Amato", "Lingyu Qiu", "Mohammad Tanveer", "Salvatore Cuomo", "Fabio Giampaolo", "Francesco Piccialli"], "title": "Towards One-shot Federated Learning: Advances, Challenges, and Future Directions", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "One-shot FL enables collaborative training in a single round, eliminating the\nneed for iterative communication, making it particularly suitable for use in\nresource-constrained and privacy-sensitive applications. This survey offers a\nthorough examination of One-shot FL, highlighting its distinct operational\nframework compared to traditional federated approaches. One-shot FL supports\nresource-limited devices by enabling single-round model aggregation while\nmaintaining data locality. The survey systematically categorizes existing\nmethodologies, emphasizing advancements in client model initialization,\naggregation techniques, and strategies for managing heterogeneous data\ndistributions. Furthermore, we analyze the limitations of current approaches,\nparticularly in terms of scalability and generalization in non-IID settings. By\nanalyzing cutting-edge techniques and outlining open challenges, this survey\naspires to provide a comprehensive reference for researchers and practitioners\naiming to design and implement One-shot FL systems, advancing the development\nand adoption of One-shot FL solutions in a real-world, resource-constrained\nscenario.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u4e00\u51fb\u5f0f\u8054\u90a6\u5b66\u4e60\uff08One-shot FL\uff09\uff0c\u5f3a\u8c03\u5176\u5355\u8f6e\u901a\u4fe1\u3001\u8d44\u6e90\u8282\u7ea6\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u7279\u6027\uff0c\u4e0e\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u76f8\u6bd4\u7684\u4f18\u52bf\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5ba2\u6237\u7aef\u6a21\u578b\u521d\u59cb\u5316\u3001\u805a\u5408\u6280\u672f\u53ca\u5f02\u6784\u6570\u636e\u5904\u7406\u7684\u8fdb\u5c55\u4e0e\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u4e00\u51fb\u5f0f\u8054\u90a6\u5b66\u4e60\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u8fed\u4ee3\u9700\u6c42\u9ad8\u3001\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u548c\u9690\u79c1\u654f\u611f\u7684\u5e94\u7528\u573a\u666f\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5ba2\u6237\u7aef\u6a21\u578b\u521d\u59cb\u5316\u3001\u805a\u5408\u6280\u672f\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\u7ba1\u7406\u7684\u7b56\u7565\u3002", "result": "\u6587\u7ae0\u6307\u51fa\u5f53\u524d\u65b9\u6cd5\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u5b58\u5728\u5c40\u9650\uff0c\u5e76\u5206\u6790\u4e86\u524d\u6cbf\u6280\u672f\u7684\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e00\u51fb\u5f0f\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u4e0e\u5b9e\u65bd\u7684\u5168\u9762\u53c2\u8003\uff0c\u63a8\u52a8\u5176\u5728\u73b0\u5b9e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u7684\u5e94\u7528\u53d1\u5c55\u3002"}}
{"id": "2505.01699", "pdf": "https://arxiv.org/pdf/2505.01699", "abs": "https://arxiv.org/abs/2505.01699", "authors": ["Yifan Liu", "Ruichen Yao", "Yaokun Liu", "Ruohan Zong", "Zelin Li", "Yang Zhang", "Dong Wang"], "title": "Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning", "categories": ["cs.CV", "cs.AI", "I.2.10; K.4.1"], "comment": "Accepted by ACM FAccT 2025", "summary": "The widespread integration of face recognition technologies into various\napplications (e.g., access control and personalized advertising) necessitates a\ncritical emphasis on fairness. While previous efforts have focused on\ndemographic fairness, the fairness of individual biological face components\nremains unexplored. In this paper, we focus on face component fairness, a\nfairness notion defined by biological face features. To our best knowledge, our\nwork is the first work to mitigate bias of face attribute prediction at the\nbiological feature level. In this work, we identify two key challenges in\noptimizing face component fairness: attribute label scarcity and attribute\ninter-dependencies, both of which limit the effectiveness of bias mitigation\nfrom previous approaches. To address these issues, we propose \\textbf{B}ayesian\n\\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which\nincorporates a Bayesian Network calibrator to guide an adaptive\nmeta-learning-based sample reweighting process. During the training process of\nour approach, the Bayesian Network calibrator dynamically tracks model bias and\nencodes prior probabilities for face component attributes to overcome the above\nchallenges. To demonstrate the efficacy of our approach, we conduct extensive\nexperiments on a large-scale real-world human face dataset. Our results show\nthat BNMR is able to consistently outperform recent face bias mitigation\nbaselines. Moreover, our results suggest a positive impact of face component\nfairness on the commonly considered demographic fairness (e.g.,\n\\textit{gender}). Our findings pave the way for new research avenues on face\ncomponent fairness, suggesting that face component fairness could serve as a\npotential surrogate objective for demographic fairness. The code for our work\nis publicly\navailable~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BNMR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\u6821\u51c6\u5668\u548c\u5143\u5b66\u4e60\u6837\u672c\u91cd\u52a0\u6743\u4f18\u5316\u9762\u90e8\u7ec4\u4ef6\u7684\u516c\u5e73\u6027\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u7a00\u7f3a\u548c\u5c5e\u6027\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u6280\u672f\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u5173\u6ce8\u516c\u5e73\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u5728\u4eba\u53e3\u7edf\u8ba1\u5b66\u516c\u5e73\u6027\uff0c\u5ffd\u7565\u4e86\u751f\u7269\u9762\u90e8\u7ec4\u4ef6\u7684\u516c\u5e73\u6027\u3002\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4f18\u5316\u751f\u7269\u7279\u5f81\u5c42\u9762\u7684\u9762\u90e8\u5c5e\u6027\u9884\u6d4b\u504f\u5dee\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u6821\u51c6\u5668\u548c\u81ea\u9002\u5e94\u5143\u5b66\u4e60\u6837\u672c\u91cd\u52a0\u6743\uff08BNMR\uff09\uff0c\u52a8\u6001\u8ddf\u8e2a\u6a21\u578b\u504f\u5dee\u5e76\u7f16\u7801\u9762\u90e8\u7ec4\u4ef6\u5c5e\u6027\u7684\u5148\u9a8c\u6982\u7387\u4ee5\u514b\u670d\u6807\u7b7e\u7a00\u7f3a\u548c\u5c5e\u6027\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "result": "\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBNMR\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u9762\u90e8\u7ec4\u4ef6\u516c\u5e73\u6027\u5bf9\u4eba\u53e3\u7edf\u8ba1\u5b66\u516c\u5e73\u6027\uff08\u5982\u6027\u522b\uff09\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "\u9762\u90e8\u7ec4\u4ef6\u516c\u5e73\u6027\u53ef\u4f5c\u4e3a\u4eba\u53e3\u7edf\u8ba1\u5b66\u516c\u5e73\u6027\u7684\u6f5c\u5728\u66ff\u4ee3\u76ee\u6807\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.02433", "pdf": "https://arxiv.org/pdf/2505.02433", "abs": "https://arxiv.org/abs/2505.02433", "authors": ["Soumen Kumar Mondal", "Akshit Varmora", "Prateek Chanda", "Ganesh Ramakrishnan"], "title": "FairPO: Robust Preference Optimization for Fair Multi-Label Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We propose FairPO, a novel framework designed to promote fairness in\nmulti-label classification by directly optimizing preference signals with a\ngroup robustness perspective. In our framework, the set of labels is\npartitioned into privileged and non-privileged groups, and a preference-based\nloss inspired by Direct Preference Optimization (DPO) is employed to more\neffectively differentiate true positive labels from confusing negatives within\nthe privileged group, while preserving baseline classification performance for\nnon-privileged labels. By framing the learning problem as a robust optimization\nover groups, our approach dynamically adjusts the training emphasis toward\ngroups with poorer performance, thereby mitigating bias and ensuring a fairer\ntreatment across diverse label categories. In addition, we outline plans to\nextend this approach by investigating alternative loss formulations such as\nSimple Preference Optimisation (SimPO) and Contrastive Preference Optimization\n(CPO) to exploit reference-free reward formulations and contrastive training\nsignals. Furthermore, we plan to extend FairPO with multilabel generation\ncapabilities, enabling the model to dynamically generate diverse and coherent\nlabel sets for ambiguous inputs.", "AI": {"tldr": "FairPO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u504f\u597d\u4fe1\u53f7\u4fc3\u8fdb\u516c\u5e73\u6027\uff0c\u91c7\u7528\u7fa4\u4f53\u9c81\u68d2\u6027\u89c6\u89d2\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u91cd\u70b9\u4ee5\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7279\u6743\u548c\u975e\u7279\u6743\u6807\u7b7e\u7ec4\u65f6\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86FairPO\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5212\u5206\u7279\u6743\u548c\u975e\u7279\u6743\u6807\u7b7e\u7ec4\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u504f\u597d\u7684\u635f\u5931\u51fd\u6570\uff08DPO\u542f\u53d1\uff09\u6765\u4f18\u5316\u533a\u5206\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u8bc1\u975e\u7279\u6743\u6807\u7b7e\u7684\u5206\u7c7b\u6548\u679c\uff0c\u91c7\u7528\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u91cd\u70b9\u3002", "result": "\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u7279\u6743\u7ec4\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u7279\u6743\u7ec4\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u516c\u5e73\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "conclusion": "FairPO\u5c55\u793a\u4e86\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u63d0\u5347\u516c\u5e73\u6027\u7684\u6f5c\u529b\uff0c\u5e76\u8ba1\u5212\u6269\u5c55\u7814\u7a76\u4ee5\u63a2\u7d22\u66f4\u590d\u6742\u7684\u635f\u5931\u51fd\u6570\u548c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2505.01709", "pdf": "https://arxiv.org/pdf/2505.01709", "abs": "https://arxiv.org/abs/2505.01709", "authors": ["Kaidong Zhang", "Rongtao Xu", "Pengzhen Ren", "Junfan Lin", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "project page: https://abliao.github.io/RoBridge/", "summary": "Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation.", "AI": {"tldr": "RoBridge\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u8ba4\u77e5\u89c4\u5212\u3001\u4e0d\u53d8\u53ef\u64cd\u4f5c\u8868\u793a\u548c\u901a\u7528\u5177\u8eab\u4ee3\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u8ba4\u77e5\u4e0e\u6267\u884c\u9e3f\u6c9f\uff0c\u5728\u65b0\u4efb\u52a1\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u6cdb\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8ba4\u77e5\u4e0e\u6267\u884c\u6280\u80fd\u9e3f\u6c9f\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faRoBridge\u67b6\u6784\uff0c\u5305\u542b\u57fa\u4e8eVLM\u7684\u9ad8\u5c42\u8ba4\u77e5\u89c4\u5212\u5668\u3001\u4e0d\u53d8\u53ef\u64cd\u4f5c\u8868\u793a\u7b26\u53f7\u6865\u548c\u901a\u7528\u5177\u8eab\u4ee3\u7406\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u7a0b\u5e8f\u6280\u80fd\u3002", "result": "\u65b0\u4efb\u52a1\u6210\u529f\u738775%\uff0c\u4eff\u771f\u5230\u73b0\u5b9e\u6cdb\u5316\u5e73\u5747\u6210\u529f\u738783%\uff08\u6bcf\u4efb\u52a1\u4ec5\u97005\u4e2a\u771f\u5b9e\u6570\u636e\u6837\u672c\uff09\u3002", "conclusion": "RoBridge\u4e3a\u8ba4\u77e5\u63a8\u7406\u4e0e\u7269\u7406\u6267\u884c\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.02435", "pdf": "https://arxiv.org/pdf/2505.02435", "abs": "https://arxiv.org/abs/2505.02435", "authors": ["Pouria Fatemi", "Ehsan Sharifian", "Mohammad Hossein Yassaee"], "title": "A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Counterfactual explanations enhance interpretability by identifying\nalternative inputs that produce different outputs, offering localized insights\ninto model decisions. However, traditional methods often neglect causal\nrelationships, leading to unrealistic examples. While newer approaches\nintegrate causality, they are computationally expensive. To address these\nchallenges, we propose an efficient method based on backtracking\ncounterfactuals that incorporates causal reasoning to generate actionable\nexplanations. We first examine the limitations of existing methods and then\nintroduce our novel approach and its features. We also explore the relationship\nbetween our method and previous techniques, demonstrating that it generalizes\nthem in specific scenarios. Finally, experiments show that our method provides\ndeeper insights into model outputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56de\u6eaf\u53cd\u4e8b\u5b9e\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u6a21\u578b\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u56e0\u679c\u5173\u7cfb\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u56e0\u679c\u5173\u7cfb\uff0c\u751f\u6210\u4e0d\u73b0\u5b9e\u7684\u4f8b\u5b50\uff1b\u800c\u96c6\u6210\u56e0\u679c\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u878d\u5165\u56e0\u679c\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56de\u6eaf\u53cd\u4e8b\u5b9e\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u6a21\u578b\u89e3\u91ca\uff0c\u5e76\u5206\u6790\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u7684\u5173\u7cfb\u53ca\u5176\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u6df1\u5165\u5730\u7406\u89e3\u6a21\u578b\u8f93\u51fa\uff0c\u63d0\u4f9b\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u56e0\u679c\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01730", "pdf": "https://arxiv.org/pdf/2505.01730", "abs": "https://arxiv.org/abs/2505.01730", "authors": ["Pranav Ramesh", "Gopalakrishnan Srinivasan"], "title": "PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Spiking Neural Networks (SNNs) have been put forward as an energy-efficient\nalternative to Artificial Neural Networks (ANNs) since they perform sparse\nAccumulate operations instead of the power-hungry Multiply-and-Accumulate\noperations. ANN-SNN conversion is a widely used method to realize deep SNNs\nwith accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently\nproposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative\nto ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless,\nSNN inferencing requires a large number of timesteps to match the accuracy of\nthe source ANN for real-world datasets. In this work, we propose PASCAL, which\nperforms ANN-SNN conversion in such a way that the resulting SNN is\nmathematically equivalent to an ANN with QCFS-activation, thereby yielding\nsimilar accuracy as the source ANN with minimal inference timesteps. In\naddition, we propose a systematic method to configure the quantization step of\nQCFS activation in a layerwise manner, which effectively determines the optimal\nnumber of timesteps per layer for the converted SNN. Our results show that the\nResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on\nImageNet with a 64$\\times$ reduction in the number of inference timesteps\ncompared to existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPASCAL\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u7b49\u6548\u4e8e\u5e26\u6709QCFS\u6fc0\u6d3b\u7684ANN\u7684\u65b9\u5f0f\u6765\u4f18\u5316ANN-SNN\u8f6c\u6362\uff0c\u4ece\u800c\u5728\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u6b65\u6570\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684SNN\u5728\u63a8\u7406\u65f6\u9700\u8981\u5927\u91cf\u65f6\u95f4\u6b65\u624d\u80fd\u5339\u914d\u6e90ANN\u7684\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u96c6\u65f6\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f6c\u6362\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86PASCAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b66\u7b49\u6548\u4e8e\u5e26\u6709QCFS\u6fc0\u6d3b\u7684ANN\u7684\u65b9\u5f0f\u6765\u8fdb\u884cANN-SNN\u8f6c\u6362\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u914d\u7f6eQCFS\u6fc0\u6d3b\u91cf\u5316\u6b65\u9aa4\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528PASCAL\u65b9\u6cd5\u8f6c\u6362\u7684ResNet-34 SNN\u5728ImageNet\u4e0a\u7684\u51c6\u786e\u7387\u8fbe\u5230\u7ea674%\uff0c\u5e76\u4e14\u63a8\u7406\u65f6\u95f4\u6b65\u6570\u51cf\u5c11\u4e8664\u500d\u3002", "conclusion": "PASCAL\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86SNN\u63a8\u7406\u6240\u9700\u7684\u65f6\u95f4\u6b65\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4e3aSNN\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02469", "pdf": "https://arxiv.org/pdf/2505.02469", "abs": "https://arxiv.org/abs/2505.02469", "authors": ["Quynh Nguyen-Phuong Vu", "Luciano Sebastian Martinez-Rau", "Yuxuan Zhang", "Nho-Duc Tran", "Bengt Oelmann", "Michele Magno", "Sebastian Bader"], "title": "Efficient Continual Learning in Keyword Spotting using Binary Neural Networks", "categories": ["cs.LG", "cs.SD"], "comment": "Accepted for publication on \"2025 IEEE Sensors Applications\n  Symposium\"", "summary": "Keyword spotting (KWS) is an essential function that enables interaction with\nubiquitous smart devices. However, in resource-limited devices, KWS models are\noften static and can thus not adapt to new scenarios, such as added keywords.\nTo overcome this problem, we propose a Continual Learning (CL) approach for KWS\nbuilt on Binary Neural Networks (BNNs). The framework leverages the reduced\ncomputation and memory requirements of BNNs while incorporating techniques that\nenable the seamless integration of new keywords over time. This study evaluates\nseven CL techniques on a 16-class use case, reporting an accuracy exceeding 95%\nfor a single additional keyword and up to 86% for four additional classes.\nSensitivity to the amount of training samples in the CL phase, and differences\nin computational complexities are being evaluated. These evaluations\ndemonstrate that batch-based algorithms are more sensitive to the CL dataset\nsize, and that differences between the computational complexities are\ninsignificant. These findings highlight the potential of developing an\neffective and computationally efficient technique for continuously integrating\nnew keywords in KWS applications that is compatible with resource-constrained\ndevices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u7684\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e2d\u7684\u5173\u952e\u8bcd\u8bc6\u522b\uff08KWS\uff09\uff0c\u65e8\u5728\u52a8\u6001\u6dfb\u52a0\u65b0\u5173\u952e\u8bcd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u6dfb\u52a0\u65b0\u5173\u952e\u8bcd\u65f6\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u56db\u6b21\u65f6\u8fbe86%\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e2d\u7684KWS\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u65b0\u5173\u952e\u8bcd\u7684\u6dfb\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e8c\u8fdb\u5236\u795e\u7ecf\u7f51\u7edc\uff08BNN\uff09\u7ed3\u5408\u4e03\u79cd\u6301\u7eed\u5b66\u4e60\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728\u65b0\u5173\u952e\u8bcd\u52a8\u6001\u6dfb\u52a0\u65f6\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5355\u6b21\u6dfb\u52a0\u65b0\u5173\u952e\u8bcd\u65f6\u51c6\u786e\u7387\u8d8595%\uff0c\u56db\u6b21\u65f6\u8fbe86%\uff1b\u6279\u5904\u7406\u7b97\u6cd5\u5bf9\u6570\u636e\u91cf\u66f4\u654f\u611f\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e2d\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\uff0c\u4e3a\u52a8\u6001\u6269\u5c55KWS\u529f\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.02486", "pdf": "https://arxiv.org/pdf/2505.02486", "abs": "https://arxiv.org/abs/2505.02486", "authors": ["Jinpeng Chen", "Runmin Cong", "Yuzhi Zhao", "Hongzheng Yang", "Guangneng Hu", "Horace Ho Shing Ip", "Sam Kwong"], "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal\nLarge Language Models (MLLMs) to incrementally learn new tasks without\ncatastrophic forgetting. In this paper, we explore forgetting in this context,\ncategorizing it into superficial forgetting and essential forgetting.\nSuperficial forgetting refers to cases where the model's knowledge may not be\ngenuinely lost, but its responses to previous tasks deviate from expected\nformats due to the influence of subsequent tasks' answer styles, making the\nresults unusable. By contrast, essential forgetting refers to situations where\nthe model provides correctly formatted but factually inaccurate answers,\nindicating a true loss of knowledge. Assessing essential forgetting\nnecessitates addressing superficial forgetting first, as severe superficial\nforgetting can obscure the model's knowledge state. Hence, we first introduce\nthe Answer Style Diversification (ASD) paradigm, which defines a standardized\nprocess for transforming data styles across different tasks, unifying their\ntraining sets into similarly diversified styles to prevent superficial\nforgetting caused by style shifts. Building on this, we propose RegLoRA to\nmitigate essential forgetting. RegLoRA stabilizes key parameters where prior\nknowledge is primarily stored by applying regularization, enabling the model to\nretain existing competencies. Experimental results demonstrate that our overall\nmethod, SEFE, achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u6301\u7eed\u6307\u4ee4\u8c03\u6574\uff08MCIT\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u8868\u9762\u9057\u5fd8\u548c\u672c\u8d28\u9057\u5fd8\u3002\u901a\u8fc7\u7b54\u6848\u98ce\u683c\u591a\u6837\u5316\uff08ASD\uff09\u548cRegLoRA\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u9057\u5fd8\u73b0\u8c61\uff0c\u5b9e\u9a8c\u8868\u660eSEFE\u65b9\u6cd5\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u589e\u91cf\u5b66\u4e60\u4e2d\u56e0\u4efb\u52a1\u98ce\u683c\u53d8\u5316\u5bfc\u81f4\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u533a\u5206\u8868\u9762\u9057\u5fd8\u548c\u672c\u8d28\u9057\u5fd8\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7b54\u6848\u98ce\u683c\u591a\u6837\u5316\uff08ASD\uff09\u7edf\u4e00\u4efb\u52a1\u98ce\u683c\uff0c\u7ed3\u5408RegLoRA\u65b9\u6cd5\u901a\u8fc7\u6b63\u5219\u5316\u7a33\u5b9a\u5173\u952e\u53c2\u6570\u4ee5\u51cf\u5c11\u672c\u8d28\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684SEFE\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "\u901a\u8fc7ASD\u548cRegLoRA\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u589e\u91cf\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2505.02490", "pdf": "https://arxiv.org/pdf/2505.02490", "abs": "https://arxiv.org/abs/2505.02490", "authors": ["Aleksandr Karakulev", "Usama Zafar", "Salman Toor", "Prashant Singh"], "title": "Bayesian Robust Aggregation for Federated Learning", "categories": ["cs.LG", "stat.ML"], "comment": "14 pages, 4 figures, 8 tables", "summary": "Federated Learning enables collaborative training of machine learning models\non decentralized data. This scheme, however, is vulnerable to adversarial\nattacks, when some of the clients submit corrupted model updates. In real-world\nscenarios, the total number of compromised clients is typically unknown, with\nthe extent of attacks potentially varying over time. To address these\nchallenges, we propose an adaptive approach for robust aggregation of model\nupdates based on Bayesian inference. The mean update is defined by the maximum\nof the likelihood marginalized over probabilities of each client to be\n`honest'. As a result, the method shares the simplicity of the classical\naverage estimators (e.g., sample mean or geometric median), being independent\nof the number of compromised clients. At the same time, it is as effective\nagainst attacks as methods specifically tailored to Federated Learning, such as\nKrum. We compare our approach with other aggregation schemes in federated\nsetting on three benchmark image classification data sets. The proposed method\nconsistently achieves state-of-the-art performance across various attack types\nwith static and varying number of malicious clients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u4e2d\u5bf9\u6297\u6076\u610f\u5ba2\u6237\u7aef\u66f4\u65b0\u65f6\u7684\u7a33\u5065\u805a\u5408\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u53ef\u80fd\u63d0\u4ea4\u88ab\u7be1\u6539\u7684\u6a21\u578b\u66f4\u65b0\uff0c\u4e14\u653b\u51fb\u89c4\u6a21\u548c\u6076\u610f\u5ba2\u6237\u7aef\u6570\u91cf\u672a\u77e5\u6216\u52a8\u6001\u53d8\u5316\uff0c\u9700\u4e00\u79cd\u72ec\u7acb\u4e8e\u653b\u51fb\u89c4\u6a21\u7684\u7a33\u5065\u805a\u5408\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u5b9a\u4e49\u5747\u503c\u66f4\u65b0\uff0c\u6700\u5927\u5316\u8fb9\u9645\u4f3c\u7136\u6982\u7387\uff08\u5404\u5ba2\u6237\u7aef\u4e3a\u2018\u8bda\u5b9e\u2019\u7684\u6982\u7387\uff09\uff0c\u65b9\u6cd5\u7b80\u5355\u4e14\u4e0d\u4f9d\u8d56\u6076\u610f\u5ba2\u6237\u7aef\u6570\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7684\u6297\u653b\u51fb\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u5747\u503c\u4f30\u8ba1\u5668\uff08\u5982\u6837\u672c\u5747\u503c\u3001\u51e0\u4f55\u4e2d\u4f4d\u6570\uff09\u53ca\u4e13\u4e3a\u8054\u90a6\u5b66\u4e60\u8bbe\u8ba1\u7684Krum\u7b49\u65b9\u6cd5\uff0c\u4e14\u5bf9\u9759\u6001/\u52a8\u6001\u6076\u610f\u5ba2\u6237\u7aef\u5747\u6709\u6548\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u517c\u5177\u7b80\u6d01\u6027\u4e0e\u9ad8\u6548\u6027\uff0c\u65e0\u9700\u9884\u5148\u5df2\u77e5\u653b\u51fb\u89c4\u6a21\uff0c\u5373\u53ef\u5728\u591a\u79cd\u653b\u51fb\u7c7b\u578b\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.01743", "pdf": "https://arxiv.org/pdf/2505.01743", "abs": "https://arxiv.org/abs/2505.01743", "authors": ["Siyang Jiang", "Bufang Yang", "Lilin Xu", "Mu Yuan", "Yeerzhati Abudunuer", "Kaiwei Liu", "Liekang Zeng", "Hongkai Chen", "Zhenyu Yan", "Xiaofan Jiang", "Guoliang Xing"], "title": "An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancements in Large Vision Language Models (LVLMs) offer the\npotential to surpass conventional labeling by generating richer, more detailed\ndescriptions of on-device human behavior understanding (HBU) in low-resolution\nvision systems, such as depth, thermal, and infrared. However, existing large\nvision language model (LVLM) approaches are unable to understand low-resolution\ndata well as they are primarily designed for high-resolution data, such as RGB\nimages. A quick fixing approach is to caption a large amount of low-resolution\ndata, but it requires a significant amount of labor-intensive annotation\nefforts. In this paper, we propose a novel, labor-saving system, Llambda,\ndesigned to support low-resolution HBU. The core idea is to leverage limited\nlabeled data and a large amount of unlabeled data to guide LLMs in generating\ninformative captions, which can be combined with raw data to effectively\nfine-tune LVLM models for understanding low-resolution videos in HBU. First, we\npropose a Contrastive-Oriented Data Labeler, which can capture\nbehavior-relevant information from long, low-resolution videos and generate\nhigh-quality pseudo labels for unlabeled data via contrastive learning. Second,\nwe propose a Physical-Knowledge Guided Captioner, which utilizes spatial and\ntemporal consistency checks to mitigate errors in pseudo labels. Therefore, it\ncan improve LLMs' understanding of sequential data and then generate\nhigh-quality video captions. Finally, to ensure on-device deployability, we\nemploy LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.\nWe evaluate Llambda using a region-scale real-world testbed and three distinct\nlow-resolution datasets, and the experiments show that Llambda outperforms\nseveral state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLlambda\u7684\u65b0\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u4f18\u5316\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\uff08HBU\uff09\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u7269\u7406\u77e5\u8bc6\u5f15\u5bfc\u7684\u6807\u6ce8\u5668\u51cf\u5c11\u9519\u8bef\uff0c\u6700\u7ec8\u7ed3\u5408\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e3b\u8981\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u6570\u636e\uff08\u5982RGB\u56fe\u50cf\uff09\uff0c\u96be\u4ee5\u6709\u6548\u7406\u89e3\u4f4e\u5206\u8fa8\u7387\u6570\u636e\uff08\u5982\u6df1\u5ea6\u3001\u70ed\u6210\u50cf\uff09\u3002\u76f4\u63a5\u6807\u6ce8\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u9700\u8981\u5927\u91cf\u4eba\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u5bf9\u6bd4\u5bfc\u5411\u7684\u6570\u636e\u6807\u6ce8\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4ece\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\uff1b2. \u8bbe\u8ba1\u7269\u7406\u77e5\u8bc6\u5f15\u5bfc\u7684\u6807\u6ce8\u5668\uff0c\u5229\u7528\u65f6\u7a7a\u4e00\u81f4\u6027\u68c0\u67e5\u4fee\u6b63\u4f2a\u6807\u7b7e\u9519\u8bef\uff1b3. \u91c7\u7528LoRA\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u9002\u914d\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u3002", "result": "\u5728\u533a\u57df\u7ea7\u771f\u5b9e\u6d4b\u8bd5\u5e8a\u548c\u4e09\u79cd\u4f4e\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0a\uff0cLlambda\u5e73\u5747Bert-Score\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdbLVLM\u7cfb\u7edf40.03%\u3002", "conclusion": "Llambda\u901a\u8fc7\u7ed3\u5408\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u7269\u7406\u77e5\u8bc6\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86LVLM\u5728\u4f4e\u5206\u8fa8\u7387HBU\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6807\u6ce8\u6210\u672c\u3002"}}
{"id": "2505.02506", "pdf": "https://arxiv.org/pdf/2505.02506", "abs": "https://arxiv.org/abs/2505.02506", "authors": ["Florian Gallusser", "Simon Hentschel", "Anna Krause", "Andreas Hotho"], "title": "Exploring Design Choices for Autoregressive Deep Learning Climate Models", "categories": ["cs.LG"], "comment": "Tackling Climate Change with Machine Learning Workshop @ ICLR 2025", "summary": "Deep Learning models have achieved state-of-the-art performance in\nmedium-range weather prediction but often fail to maintain physically\nconsistent rollouts beyond 14 days. In contrast, a few atmospheric models\ndemonstrate stability over decades, though the key design choices enabling this\nremain unclear. This study quantitatively compares the long-term stability of\nthree prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained\non ERA5 reanalysis data at 5.625{\\deg} resolution. We systematically assess the\nimpact of autoregressive training steps, model capacity, and choice of\nprognostic variables, identifying configurations that enable stable 10-year\nrollouts while preserving the statistical properties of the reference dataset.\nNotably, rollouts with SFNO exhibit the greatest robustness to hyperparameter\nchoices, yet all models can experience instability depending on the random seed\nand the set of prognostic variables", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u5929\u6c14\u9884\u6d4b\u6a21\u578b\uff08FourCastNet\u3001SFNO\u3001ClimaX\uff09\u7684\u957f\u671f\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0SFNO\u5728\u8d85\u53c2\u6570\u9009\u62e9\u4e0a\u6700\u7a33\u5065\uff0c\u4f46\u6240\u6709\u6a21\u578b\u90fd\u53ef\u80fd\u56e0\u968f\u673a\u79cd\u5b50\u548c\u9884\u6d4b\u53d8\u91cf\u96c6\u7684\u4e0d\u5f53\u9009\u62e9\u800c\u5931\u7a33\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e2d\u671f\u5929\u6c14\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u671f\u9884\u6d4b\uff08\u598210\u5e74\uff09\u4e2d\u96be\u4ee5\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002\u7814\u7a76\u65e8\u5728\u627e\u51fa\u80fd\u4f7f\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u8bbe\u8ba1\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e09\u79cd\u6a21\u578b\uff08FourCastNet\u3001SFNO\u3001ClimaX\uff09\u5728ERA5\u518d\u5206\u6790\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u81ea\u56de\u5f52\u8bad\u7ec3\u6b65\u9aa4\u3001\u6a21\u578b\u5bb9\u91cf\u548c\u9884\u6d4b\u53d8\u91cf\u9009\u62e9\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "SFNO\u5728\u8d85\u53c2\u6570\u9009\u62e9\u4e0a\u8868\u73b0\u51fa\u6700\u5f3a\u9c81\u68d2\u6027\uff0c\u4f46\u6240\u6709\u6a21\u578b\u90fd\u53ef\u80fd\u56e0\u968f\u673a\u79cd\u5b50\u548c\u9884\u6d4b\u53d8\u91cf\u9009\u62e9\u4e0d\u5f53\u800c\u5931\u7a33\u3002\u7814\u7a76\u627e\u5230\u4e86\u80fd\u591f\u7ef4\u630110\u5e74\u7a33\u5b9a\u9884\u6d4b\u7684\u914d\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u914d\u7f6e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u957f\u671f\u7a33\u5b9a\u7684\u5929\u6c14\u9884\u6d4b\uff0c\u4f46\u7a33\u5b9a\u6027\u4ecd\u4f9d\u8d56\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u53c2\u6570\u8c03\u6574\u3002"}}
{"id": "2505.01780", "pdf": "https://arxiv.org/pdf/2505.01780", "abs": "https://arxiv.org/abs/2505.01780", "authors": ["Guangjin Pan", "Zhixing Li", "Ay\u00e7a \u00d6z\u00e7elikkale", "Christian H\u00e4ger", "Musa Furkan Keskin", "Henk Wymeersch"], "title": "Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach", "categories": ["eess.SP", "cs.AI", "cs.NI", "cs.SY", "eess.SY"], "comment": "6 pages, 15 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "In closed-loop distributed multi-sensor integrated sensing and communication\n(ISAC) systems, performance often hinges on transmitting high-dimensional\nsensor observations over rate-limited networks. In this paper, we first present\na general framework for rate-limited closed-loop distributed ISAC systems, and\nthen propose an autoencoder-based observation compression method to overcome\nthe constraints imposed by limited transmission capacity. Building on this\nframework, we conduct a case study using a closed-loop linear quadratic\nregulator (LQR) system to analyze how the interplay among observation,\ncompression, and state dimensions affects reconstruction accuracy, state\nestimation error, and control performance. In multi-sensor scenarios, our\nresults further show that optimal resource allocation initially prioritizes\nlow-noise sensors until the compression becomes lossless, after which resources\nare reallocated to high-noise sensors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u89c2\u6d4b\u538b\u7f29\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u591a\u4f20\u611f\u5668\u611f\u901a\u4e00\u4f53\u5316\u7cfb\u7edf\u4e2d\u9ad8\u7ef4\u89c2\u6d4b\u6570\u636e\u5728\u901f\u7387\u53d7\u9650\u7f51\u7edc\u4e2d\u7684\u4f20\u8f93\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8868\u660e\u4e86\u8d44\u6e90\u5206\u914d\u7684\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u591a\u4f20\u611f\u5668\u611f\u901a\u4e00\u4f53\u5316\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u7ef4\u89c2\u6d4b\u6570\u636e\u7684\u4f20\u8f93\u53d7\u9650\u4e8e\u7f51\u7edc\u901f\u7387\uff0c\u5f71\u54cd\u4e86\u7cfb\u7edf\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u538b\u7f29\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u89c2\u6d4b\u538b\u7f29\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u95ed\u73afLQR\u7cfb\u7edf\u6848\u4f8b\u5206\u6790\u4e86\u89c2\u6d4b\u3001\u538b\u7f29\u548c\u72b6\u6001\u7ef4\u5ea6\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u4f20\u611f\u5668\u573a\u666f\u4e2d\uff0c\u8d44\u6e90\u5206\u914d\u4f1a\u4f18\u5148\u5206\u914d\u7ed9\u4f4e\u566a\u58f0\u4f20\u611f\u5668\uff0c\u76f4\u5230\u538b\u7f29\u65e0\u5931\u771f\u540e\u518d\u5206\u914d\u7ed9\u9ad8\u566a\u58f0\u4f20\u611f\u5668\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u538b\u7f29\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5206\u5e03\u5f0fISAC\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u6700\u4f18\u8d44\u6e90\u5206\u914d\u7684\u7b56\u7565\u3002"}}
{"id": "2505.02514", "pdf": "https://arxiv.org/pdf/2505.02514", "abs": "https://arxiv.org/abs/2505.02514", "authors": ["Diego Perazzolo", "Chiara Castellani", "Enrico Grisan"], "title": "Uncovering Population PK Covariates from VAE-Generated Latent Spaces", "categories": ["cs.LG", "q-bio.QM", "I.2.1; I.5.1; I.5.2; I.5.4; I.5.5"], "comment": "Paper accepted at the 47th Annual International Conference IEEE EMBC\n  2025 (Engineering in Medicine and Biology Society), Copenhagen, Denmark", "summary": "Population pharmacokinetic (PopPK) modelling is a fundamental tool for\nunderstanding drug behaviour across diverse patient populations and enabling\npersonalized dosing strategies to improve therapeutic outcomes. A key challenge\nin PopPK analysis lies in identifying and modelling covariates that influence\ndrug absorption, as these relationships are often complex and nonlinear.\nTraditional methods may fail to capture hidden patterns within the data. In\nthis study, we propose a data-driven, model-free framework that integrates\nVariational Autoencoders (VAEs) deep learning model and LASSO regression to\nuncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles.\nThe VAE compresses high-dimensional PK signals into a structured latent space,\nachieving accurate reconstruction with a mean absolute percentage error (MAPE)\nof 2.26%. LASSO regression is then applied to map patient-specific covariates\nto the latent space, enabling sparse feature selection through L1\nregularization. This approach consistently identifies clinically relevant\ncovariates for tacrolimus including SNP, age, albumin, and hemoglobin which are\nretained across the tested regularization strength levels, while effectively\ndiscarding non-informative features. The proposed VAE-LASSO methodology offers\na scalable, interpretable, and fully data-driven solution for covariate\nselection, with promising applications in drug development and precision\npharmacotherapy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548cLASSO\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6a21\u62df\u7684\u4ed6\u514b\u83ab\u53f8\u836f\u4ee3\u52a8\u529b\u5b66\u6570\u636e\u4e2d\u8bc6\u522b\u5173\u952e\u534f\u53d8\u91cf\uff0c\u4ee5\u6539\u8fdb\u4e2a\u6027\u5316\u7ed9\u836f\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u836f\u7269\u5438\u6536\u4e2d\u7684\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u4e9f\u9700\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u4e14\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\u6765\u9ad8\u6548\u8bc6\u522b\u534f\u53d8\u91cf\u3002", "method": "\u91c7\u7528VAE\u538b\u7f29\u9ad8\u7ef4\u836f\u4ee3\u52a8\u529b\u5b66\u4fe1\u53f7\u81f3\u7ed3\u6784\u5316\u6f5c\u7a7a\u95f4\uff08MAPE\u4e3a2.26%\uff09\uff0c\u518d\u901a\u8fc7LASSO\u56de\u5f52\u8fdb\u884c\u7a00\u758f\u7279\u5f81\u9009\u62e9\u3002", "result": "\u65b9\u6cd5\u7a33\u5b9a\u8bc6\u522b\u51faSNP\u3001\u5e74\u9f84\u3001\u767d\u86cb\u767d\u548c\u8840\u7ea2\u86cb\u767d\u7b49\u4e34\u5e8a\u76f8\u5173\u534f\u53d8\u91cf\uff0c\u5e76\u6709\u6548\u6392\u9664\u65e0\u5173\u7279\u5f81\u3002", "conclusion": "VAE-LASSO\u4e3a\u534f\u53d8\u91cf\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u5168\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u836f\u7269\u7814\u53d1\u548c\u7cbe\u51c6\u6cbb\u7597\u4e2d\u5177\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.01781", "pdf": "https://arxiv.org/pdf/2505.01781", "abs": "https://arxiv.org/abs/2505.01781", "authors": ["Ziye Yang", "Ke Lu"], "title": "Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "The sensitivity to input parameters and lack of flexibility limits the\ntraditional Mean-Variance model. In contrast, the Black-Litterman model has\nattracted widespread attention by integrating market equilibrium returns with\ninvestors' subjective views. This paper proposes a novel hybrid deep learning\nmodel combining Singular Spectrum analysis (SSA), Multivariate Aligned\nEmpirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks\n(TCNs), aiming to improve the prediction accuracy of asset prices and thus\nenhance the ability of the Black-Litterman model to generate subjective views.\nExperimental results show that noise reduction pre-processing can improve the\nmodel's accuracy, and the prediction performance of the proposed model is\nsignificantly better than that of three multivariate decomposition benchmark\nmodels. We construct an investment portfolio by using 20 representative stocks\nfrom the NASDAQ 100 index. By combining the hybrid forecasting model with the\nBlack-Litterman model, the generated investment portfolio exhibits better\nreturns and risk control capabilities than the Mean-Variance, Equal-Weighted,\nand Market-Weighted models in the short holding period.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SSA\u3001MA-EMD\u548cTCN\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347Black-Litterman\u6a21\u578b\u7684\u4e3b\u89c2\u89c2\u70b9\u751f\u6210\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u4e14\u7ed3\u5408Black-Litterman\u7684\u6295\u8d44\u7ec4\u5408\u5728\u77ed\u671f\u6301\u6709\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edfMean-Variance\u6a21\u578b\u5bf9\u8f93\u5165\u53c2\u6570\u654f\u611f\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800cBlack-Litterman\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5e02\u573a\u5747\u8861\u6536\u76ca\u548c\u6295\u8d44\u8005\u4e3b\u89c2\u89c2\u70b9\u66f4\u53d7\u5173\u6ce8\u3002\u4f46\u540e\u8005\u4f9d\u8d56\u51c6\u786e\u7684\u4e3b\u89c2\u89c2\u70b9\u751f\u6210\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u8d44\u4ea7\u4ef7\u683c\u9884\u6d4b\u7cbe\u5ea6\u6765\u4f18\u5316\u5176\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aSSA\u7528\u4e8e\u964d\u566a\u9884\u5904\u7406\uff0cMA-EMD\u5206\u89e3\u591a\u53d8\u91cf\u65f6\u5e8f\u6570\u636e\uff0cTCN\u6355\u83b7\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u63d0\u5347Black-Litterman\u6a21\u578b\u7684\u4e3b\u89c2\u89c2\u70b9\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u964d\u566a\u9884\u5904\u7406\u63d0\u9ad8\u4e86\u6a21\u578b\u7cbe\u5ea6\uff0c\u6df7\u5408\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4e09\u79cd\u591a\u53d8\u91cf\u5206\u89e3\u57fa\u51c6\u6a21\u578b\u3002\u57fa\u4e8eNASDAQ 100\u6210\u5206\u80a1\u7684\u6295\u8d44\u7ec4\u5408\u5728\u77ed\u671f\u6301\u6709\u4e2d\u8868\u73b0\u4f18\u4e8eMean-Variance\u7b49\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6df7\u5408\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86Black-Litterman\u6a21\u578b\u7684\u4e3b\u89c2\u89c2\u70b9\u751f\u6210\u80fd\u529b\uff0c\u7ed3\u5408\u540e\u7684\u6295\u8d44\u7ec4\u5408\u5728\u6536\u76ca\u548c\u98ce\u9669\u63a7\u5236\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u77ed\u671f\u6295\u8d44\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.02515", "pdf": "https://arxiv.org/pdf/2505.02515", "abs": "https://arxiv.org/abs/2505.02515", "authors": ["Hongze Li", "Zesheng Zhou", "Zhenbiao Cao", "Xinhui Li", "Wei Chen", "Xiaojin Zhang"], "title": "FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization", "categories": ["cs.LG"], "comment": "11 pages, 6 figures", "summary": "Traditional domain generalization approaches predominantly focus on\nleveraging target domain-aware features while overlooking the critical role of\nsource domain-specific characteristics, particularly in federated settings with\ninherent data isolation. To address this gap, we propose the Federated Source\nDomain Awareness Framework (FedSDAF), the first method to systematically\nexploit source domain-aware features for enhanced federated domain\ngeneralization (FedDG). The FedSDAF framework consists of two synergistic\ncomponents: the Domain-Invariant Adapter, which preserves critical\ndomain-invariant features, and the Domain-Aware Adapter, which extracts and\nintegrates source domain-specific knowledge using a Multihead Self-Attention\nmechanism (MHSA). Furthermore, we introduce a bidirectional knowledge\ndistillation mechanism that fosters knowledge sharing among clients while\nsafeguarding privacy. Our approach represents the first systematic exploitation\nof source domain-aware features, resulting in significant advancements in model\ngeneralization capability.Extensive experiments on four standard benchmarks\n(OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently\nsurpasses state-of-the-art federated domain generalization approaches, with\naccuracy gains of 5.2-13.8%. The source code is available at\nhttps://github.com/pizzareapers/FedSDAF.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5FedSDAF\uff0c\u901a\u8fc7\u5229\u7528\u6e90\u9886\u57df\u611f\u77e5\u7279\u5f81\u6765\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u76ee\u6807\u9886\u57df\u611f\u77e5\u7279\u5f81\uff0c\u800c\u5ffd\u7565\u4e86\u6e90\u9886\u57df\u7279\u5b9a\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u3002", "method": "FedSDAF\u6846\u67b6\u5305\u62ec\u4e24\u90e8\u5206\uff1a\u9886\u57df\u4e0d\u53d8\u9002\u914d\u5668\u4fdd\u7559\u5173\u952e\u4e0d\u53d8\u7279\u5f81\uff0c\u9886\u57df\u611f\u77e5\u9002\u914d\u5668\u5229\u7528\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u6e90\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u5f15\u5165\u53cc\u5411\u77e5\u8bc6\u84b8\u998f\u673a\u5236\u4fc3\u8fdb\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5728\u56db\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08OfficeHome\u3001PACS\u3001VLCS\u548cDomainNet\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u53475.2-13.8%\u3002", "conclusion": "FedSDAF\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u5229\u7528\u6e90\u9886\u57df\u611f\u77e5\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.02537", "pdf": "https://arxiv.org/pdf/2505.02537", "abs": "https://arxiv.org/abs/2505.02537", "authors": ["Davide Sartor", "Alberto Sinigaglia", "Gian Antonio Susto"], "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "International Conference on Machine Learning", "summary": "Conventional techniques for imposing monotonicity in MLPs by construction\ninvolve the use of non-negative weight constraints and bounded activation\nfunctions, which pose well-known optimization challenges. In this work, we\ngeneralize previous theoretical results, showing that MLPs with non-negative\nweight constraint and activations that saturate on alternating sides are\nuniversal approximators for monotonic functions. Additionally, we show an\nequivalence between the saturation side in the activations and the sign of the\nweight constraint. This connection allows us to prove that MLPs with convex\nmonotone activations and non-positive constrained weights also qualify as\nuniversal approximators, in contrast to their non-negative constrained\ncounterparts. Our results provide theoretical grounding to the empirical\neffectiveness observed in previous works while leading to possible\narchitectural simplification. Moreover, to further alleviate the optimization\ndifficulties, we propose an alternative formulation that allows the network to\nadjust its activations according to the sign of the weights. This eliminates\nthe requirement for weight reparameterization, easing initialization and\nimproving training stability. Experimental evaluation reinforces the validity\nof the theoretical results, showing that our novel approach compares favourably\nto traditional monotonic architectures.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u6269\u5c55\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5177\u6709\u975e\u8d1f\u6743\u91cd\u7ea6\u675f\u548c\u7279\u5b9a\u6fc0\u6d3b\u51fd\u6570\u7684MLP\u53ef\u4ee5\u666e\u9002\u903c\u8fd1\u5355\u8c03\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u5f62\u5f0f\u4ee5\u7b80\u5316\u67b6\u6784\u5e76\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u6784\u9020\u5f3a\u5236\u5355\u8c03\u6027\u7684MLP\u5b58\u5728\u4f18\u5316\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7406\u8bba\u6df1\u5316\u548c\u63d0\u51fa\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660eMLP\u5728\u7279\u5b9a\u6743\u91cd\u7ea6\u675f\u548c\u6fc0\u6d3b\u51fd\u6570\u4e0b\u7684\u666e\u9002\u903c\u8fd1\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700\u6743\u91cd\u91cd\u53c2\u6570\u5316\u7684\u4f18\u5316\u5f62\u5f0f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5176\u5728\u5355\u8c03\u6027\u67b6\u6784\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u4e3a\u5148\u524d\u89c2\u5bdf\u5230\u7684\u7ecf\u9a8c\u6548\u679c\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u8fd8\u901a\u8fc7\u7b80\u5316\u67b6\u6784\u548c\u4f18\u5316\u8bad\u7ec3\u63a8\u52a8\u4e86\u9886\u57df\u8fdb\u5c55\u3002"}}
{"id": "2505.02540", "pdf": "https://arxiv.org/pdf/2505.02540", "abs": "https://arxiv.org/abs/2505.02540", "authors": ["Ljubomir Rokvic", "Panayiotis Danassis", "Boi Faltings"], "title": "Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN), IEEE, 2025", "summary": "In Federated Learning, heterogeneity in client data distributions often means\nthat a single global model does not have the best performance for individual\nclients. Consider for example training a next-word prediction model for\nkeyboards: user-specific language patterns due to demographics (dialect, age,\netc.), language proficiency, and writing style result in a highly non-IID\ndataset across clients. Other examples are medical images taken with different\nmachines, or driving data from different vehicle types. To address this, we\npropose a simple yet effective personalized federated learning framework\n(pFedLIA) that utilizes a computationally efficient influence approximation,\ncalled `Lazy Influence', to cluster clients in a distributed manner before\nmodel aggregation. Within each cluster, data owners collaborate to jointly\ntrain a model that captures the specific data patterns of the clients. Our\nmethod has been shown to successfully recover the global model's performance\ndrop due to the non-IID-ness in various synthetic and real-world settings,\nspecifically a next-word prediction task on the Nordic languages as well as\nseveral benchmark tasks. It matches the performance of a hypothetical Oracle\nclustering, and significantly improves on existing baselines, e.g., an\nimprovement of 17% on CIFAR100.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6pFedLIA\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u805a\u7c7b\u548c\u9ad8\u6548\u7684\u5f71\u54cd\u8fd1\u4f3c\u65b9\u6cd5\u89e3\u51b3\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7531\u4e8e\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u4e0d\u5747\uff08\u5982\u4e0d\u540c\u7528\u6237\u7684\u8bed\u8a00\u6a21\u5f0f\u3001\u533b\u7597\u8bbe\u5907\u6216\u8f66\u8f86\u7c7b\u578b\u5dee\u5f02\uff09\u5bfc\u81f4\u5355\u4e00\u5168\u5c40\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fapFedLIA\u6846\u67b6\uff0c\u5229\u7528`Lazy Influence`\u65b9\u6cd5\u5206\u5e03\u5f0f\u805a\u7c7b\u5ba2\u6237\u7aef\uff0c\u5e76\u5728\u6bcf\u4e2a\u805a\u7c7b\u5185\u8054\u5408\u8bad\u7ec3\u4e2a\u6027\u5316\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4efb\u52a1\uff08\u5982\u5317\u6b27\u8bed\u8a00\u9884\u6d4b\u548cCIFAR100\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u6062\u590d\u5168\u5c40\u6a21\u578b\u56e0\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u6027\u80fd\u63d0\u534717%\u3002", "conclusion": "pFedLIA\u80fd\u6709\u6548\u5339\u914d\u7406\u60f3\u805a\u7c7b\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u5ea6\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2505.01821", "pdf": "https://arxiv.org/pdf/2505.01821", "abs": "https://arxiv.org/abs/2505.01821", "authors": ["Jing Liu", "Yao Du", "Kun Yang", "Yan Wang", "Xiping Hu", "Zehua Wang", "Yang Liu", "Peng Sun", "Azzedine Boukerche", "Victor C. M. Leung"], "title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "30 pages, 10figures, 6 tables", "summary": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm\nfor addressing the computational demands of modern intelligent applications,\nintegrating cloud resources with edge devices to enable efficient, low-latency\nprocessing. Recent advancements in AI, particularly deep learning and large\nlanguage models (LLMs), have dramatically enhanced the capabilities of these\ndistributed systems, yet introduce significant challenges in model deployment\nand resource management. In this survey, we comprehensive examine the\nintersection of distributed intelligence and model optimization within\nedge-cloud environments, providing a structured tutorial on fundamental\narchitectures, enabling technologies, and emerging applications. Additionally,\nwe systematically analyze model optimization approaches, including compression,\nadaptation, and neural architecture search, alongside AI-driven resource\nmanagement strategies that balance performance, energy efficiency, and latency\nrequirements. We further explore critical aspects of privacy protection and\nsecurity enhancement within ECCC systems and examines practical deployments\nthrough diverse applications, spanning autonomous driving, healthcare, and\nindustrial automation. Performance analysis and benchmarking techniques are\nalso thoroughly explored to establish evaluation standards for these complex\nsystems. Furthermore, the review identifies critical research directions\nincluding LLMs deployment, 6G integration, neuromorphic computing, and quantum\ncomputing, offering a roadmap for addressing persistent challenges in\nheterogeneity management, real-time processing, and scalability. By bridging\ntheoretical advancements and practical deployments, this survey offers\nresearchers and practitioners a holistic perspective on leveraging AI to\noptimize distributed computing environments, fostering innovation in\nnext-generation intelligent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8fb9\u7f18-\u4e91\u534f\u540c\u8ba1\u7b97\uff08ECCC\uff09\u5982\u4f55\u6574\u5408AI\u6280\u672f\u4ee5\u4f18\u5316\u5206\u5e03\u5f0f\u7cfb\u7edf\u6027\u80fd\uff0c\u63a2\u8ba8\u4e86\u6a21\u578b\u4f18\u5316\u3001\u8d44\u6e90\u7ba1\u7406\u3001\u9690\u79c1\u5b89\u5168\u7b49\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740AI\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u548cLLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8fb9\u7f18-\u4e91\u534f\u540c\u8ba1\u7b97\u9762\u4e34\u6a21\u578b\u90e8\u7f72\u548c\u8d44\u6e90\u7ba1\u7406\u7684\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u63a8\u52a8\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u7684\u667a\u80fd\u5e94\u7528\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u7efc\u8ff0\u65b9\u5f0f\uff0c\u5206\u6790\u4e86ECCC\u7684\u57fa\u7840\u67b6\u6784\u3001\u5173\u952e\u6280\u672f\u3001\u6a21\u578b\u4f18\u5316\uff08\u5982\u538b\u7f29\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff09\u3001AI\u9a71\u52a8\u7684\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\u4ee5\u53ca\u9690\u79c1\u5b89\u5168\u4fdd\u62a4\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9ECCC\u7cfb\u7edf\u7684\u5168\u9762\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u5c55\u793a\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u540c\u65f6\u6307\u51faLLMs\u90e8\u7f72\u30016G\u6574\u5408\u7b49\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u7ed3\u5408\u7684\u89c6\u89d2\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u7684\u521b\u65b0\u3002"}}
{"id": "2505.02566", "pdf": "https://arxiv.org/pdf/2505.02566", "abs": "https://arxiv.org/abs/2505.02566", "authors": ["Kirill Lukyanov", "Georgii Sazonov", "Serafim Boyarsky", "Ilya Makarov"], "title": "Robustness questions the interpretability of graph neural networks: what to do?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have become a cornerstone in graph-based data\nanalysis, with applications in diverse domains such as bioinformatics, social\nnetworks, and recommendation systems. However, the interplay between model\ninterpretability and robustness remains poorly understood, especially under\nadversarial scenarios like poisoning and evasion attacks. This paper presents a\ncomprehensive benchmark to systematically analyze the impact of various factors\non the interpretability of GNNs, including the influence of\nrobustness-enhancing defense mechanisms.\n  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across\nfive datasets from two distinct domains, employing four interpretability\nmetrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how\ndefenses against poisoning and evasion attacks, applied before and during model\ntraining, affect interpretability and highlights critical trade-offs between\nrobustness and interpretability. The framework will be published as open\nsource.\n  The results reveal significant variations in interpretability depending on\nthe chosen defense methods and model architecture characteristics. By\nestablishing a standardized benchmark, this work provides a foundation for\ndeveloping GNNs that are both robust to adversarial threats and interpretable,\nfacilitating trust in their deployment in sensitive applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6df1\u5165\u7814\u7a76\u4e86GNN\u6a21\u578b\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u89e3\u91ca\u6027\u4e0e\u7a33\u5065\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u516d\u79cdGNN\u67b6\u6784\u548c\u591a\u79cd\u9632\u5fa1\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u7531\u4e8eGNN\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u751f\u7269\u4fe1\u606f\u5b66\u548c\u793e\u4ea4\u7f51\u7edc\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u89e3\u91ca\u6027\u4e0e\u7a33\u5065\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u81f3\u5173\u91cd\u8981\u3002\u5c24\u5176\u5728\u5bf9\u6297\u6027\u653b\u51fb\uff08\u5982\u6295\u6bd2\u548c\u89c4\u907f\u653b\u51fb\uff09\u4e0b\uff0c\u8fd9\u79cd\u5e73\u8861\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u91c7\u7528\u516d\u79cd\u57fa\u4e8eGCN\u3001SAGE\u3001GIN\u548cGAT\u7684GNN\u67b6\u6784\uff0c\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5e76\u8fd0\u7528\u56db\u79cd\u89e3\u91ca\u6027\u6307\u6807\uff08Fidelity\u3001Stability\u3001Consistency\u3001Sparsity\uff09\u3002\u901a\u8fc7\u9632\u5fa1\u63aa\u65bd\u5728\u8bad\u7ec3\u524d\u540e\u7684\u5e94\u7528\uff0c\u5206\u6790\u5176\u5bf9\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u9632\u5fa1\u65b9\u6cd5\u548c\u6a21\u578b\u67b6\u6784\u7279\u6027\u5bf9\u89e3\u91ca\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u7a33\u5065\u6027\u4e0e\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u517c\u5177\u7a33\u5065\u6027\u548c\u89e3\u91ca\u6027\u7684GNN\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5728\u654f\u611f\u5e94\u7528\u4e2d\u5efa\u7acb\u4fe1\u4efb\u3002"}}
{"id": "2505.02573", "pdf": "https://arxiv.org/pdf/2505.02573", "abs": "https://arxiv.org/abs/2505.02573", "authors": ["Hao Zhang", "Xunkai Li", "Yinlin Zhu", "Lianglin Hu"], "title": "Rethinking Federated Graph Learning: A Data Condensation Perspective", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.SI"], "comment": null, "summary": "Federated graph learning is a widely recognized technique that promotes\ncollaborative training of graph neural networks (GNNs) by multi-client\ngraphs.However, existing approaches heavily rely on the communication of model\nparameters or gradients for federated optimization and fail to adequately\naddress the data heterogeneity introduced by intricate and diverse graph\ndistributions. Although some methods attempt to share additional messages among\nthe server and clients to improve federated convergence during communication,\nthey introduce significant privacy risks and increase communication overhead.\nTo address these issues, we introduce the concept of a condensed graph as a\nnovel optimization carrier to address FGL data heterogeneity and propose a new\nFGL paradigm called FedGM. Specifically, we utilize a generalized condensation\ngraph consensus to aggregate comprehensive knowledge from distributed graphs,\nwhile minimizing communication costs and privacy risks through a single\ntransmission of the condensed data. Extensive experiments on six public\ndatasets consistently demonstrate the superiority of FedGM over\nstate-of-the-art baselines, highlighting its potential for a novel FGL\nparadigm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedGM\u7684\u65b0\u8054\u90a6\u56fe\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u538b\u7f29\u56fe\u4f5c\u4e3a\u4f18\u5316\u8f7d\u4f53\u6765\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u548c\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u56fe\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u578b\u53c2\u6570\u6216\u68af\u5ea6\u7684\u901a\u4fe1\uff0c\u672a\u80fd\u6709\u6548\u5904\u7406\u7531\u591a\u6837\u56fe\u5206\u5e03\u5f15\u5165\u7684\u6570\u636e\u5f02\u8d28\u6027\uff0c\u4e14\u5171\u4eab\u989d\u5916\u4fe1\u606f\u4f1a\u589e\u52a0\u9690\u79c1\u98ce\u9669\u548c\u901a\u4fe1\u6210\u672c\u3002", "method": "\u63d0\u51faFedGM\u6846\u67b6\uff0c\u5229\u7528\u5e7f\u4e49\u538b\u7f29\u56fe\u5171\u8bc6\u805a\u5408\u5206\u5e03\u5f0f\u56fe\u7684\u7efc\u5408\u77e5\u8bc6\uff0c\u901a\u8fc7\u5355\u6b21\u4f20\u8f93\u538b\u7f29\u6570\u636e\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u548c\u9690\u79c1\u98ce\u9669\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedGM\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u65b0\u8054\u90a6\u56fe\u5b66\u4e60\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "conclusion": "FedGM\u901a\u8fc7\u538b\u7f29\u56fe\u5171\u8bc6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u540c\u65f6\u517c\u987e\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\uff0c\u4e3a\u8054\u90a6\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.01823", "pdf": "https://arxiv.org/pdf/2505.01823", "abs": "https://arxiv.org/abs/2505.01823", "authors": ["Nitin Rai", "Arnold W. Schumann", "Nathan Boyd"], "title": "PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "Collecting large-scale crop disease images in the field is labor-intensive\nand time-consuming. Generative models (GMs) offer an alternative by creating\nsynthetic samples that resemble real-world images. However, existing research\nprimarily relies on Generative Adversarial Networks (GANs)-based image-to-image\ntranslation and lack a comprehensive analysis of computational requirements in\nagriculture. Therefore, this research explores a multi-modal text-to-image\napproach for generating synthetic crop disease images and is the first to\nprovide computational benchmarking in this context. We trained three Stable\nDiffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and\nfine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning\ntechniques to enhance generalization. SD3.5M outperformed the others, with an\naverage memory usage of 18 GB, power consumption of 180 W, and total energy use\nof 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results\ndemonstrate SD3.5M's ability to generate 500 synthetic images from just 36\nin-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease\ndata generation.", "AI": {"tldr": "\u7814\u7a76\u8005\u63a2\u7d22\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6587\u672c\u5230\u56fe\u50cf\u7684\u5408\u6210\u4f5c\u7269\u75c5\u5bb3\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5728\u519c\u4e1a\u9886\u57df\u8fdb\u884c\u4e86\u8ba1\u7b97\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002SD3.5M\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u63a8\u8350\u7528\u4e8e\u9ad8\u6548\u6570\u636e\u751f\u6210\u3002", "motivation": "\u91ce\u5916\u91c7\u96c6\u5927\u89c4\u6a21\u4f5c\u7269\u75c5\u5bb3\u56fe\u50cf\u8017\u65f6\u8017\u529b\uff0c\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u57fa\u4e8eGAN\uff0c\u7f3a\u4e4f\u5bf9\u519c\u4e1a\u573a\u666f\u8ba1\u7b97\u9700\u6c42\u7684\u5168\u9762\u5206\u6790\u3002", "method": "\u8bad\u7ec3\u4e09\u79cdStable Diffusion\u53d8\u4f53\uff08SDXL\u3001SD3.5M\u3001SD3.5L\uff09\uff0c\u7ed3\u5408Dreambooth\u548cLoRA\u5fae\u8c03\u6280\u672f\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "SD3.5M\u5e73\u5747\u5185\u5b58\u5360\u752818GB\uff0c\u529f\u8017180W\uff0c\u6bcf500\u5f20\u56fe\u50cf\u80fd\u80171.02kWh\uff0c1.5\u5c0f\u65f6\u5185\u53ef\u4ece36\u5f20\u6837\u672c\u751f\u6210500\u5f20\u5408\u6210\u56fe\u50cf\u3002", "conclusion": "SD3.5M\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u9002\u7528\u4e8e\u4f5c\u7269\u75c5\u5bb3\u6570\u636e\u7684\u9ad8\u6548\u751f\u6210\u3002"}}
{"id": "2505.02583", "pdf": "https://arxiv.org/pdf/2505.02583", "abs": "https://arxiv.org/abs/2505.02583", "authors": ["Chenxi Liu", "Shaowen Zhou", "Qianxiong Xu", "Hao Miao", "Cheng Long", "Ziyue Li", "Rui Zhao"], "title": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by IJCAI 2025 Survey Track", "summary": "The proliferation of edge devices has generated an unprecedented volume of\ntime series data across different domains, motivating various well-customized\nmethods. Recently, Large Language Models (LLMs) have emerged as a new paradigm\nfor time series analytics by leveraging the shared sequential nature of textual\ndata and time series. However, a fundamental cross-modality gap between time\nseries and LLMs exists, as LLMs are pre-trained on textual corpora and are not\ninherently optimized for time series. Many recent proposals are designed to\naddress this issue. In this survey, we provide an up-to-date overview of\nLLMs-based cross-modality modeling for time series analytics. We first\nintroduce a taxonomy that classifies existing approaches into four groups based\non the type of textual data employed for time series modeling. We then\nsummarize key cross-modality strategies, e.g., alignment and fusion, and\ndiscuss their applications across a range of downstream tasks. Furthermore, we\nconduct experiments on multimodal datasets from different application domains\nto investigate effective combinations of textual data and cross-modality\nstrategies for enhancing time series analytics. Finally, we suggest several\npromising directions for future research. This survey is designed for a range\nof professionals, researchers, and practitioners interested in LLM-based time\nseries modeling.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u8de8\u6a21\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u5bf9\u5176\u5206\u7c7b\u3001\u7b56\u7565\u3001\u5b9e\u9a8c\u5e94\u7528\u53ca\u672a\u6765\u65b9\u5411\u8fdb\u884c\u4e86\u5168\u9762\u603b\u7ed3\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18\u8bbe\u5907\u4ea7\u751f\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6fc0\u589e\uff0c\u4f20\u7edf\u5b9a\u5236\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002LLMs\u56e0\u5176\u5e8f\u5217\u5904\u7406\u80fd\u529b\u6210\u4e3a\u53ef\u80fd\u7684\u65b0\u8303\u5f0f\uff0c\u4f46\u5176\u4e0e\u65f6\u95f4\u5e8f\u5217\u7684\u8de8\u6a21\u6001\u5dee\u5f02\u9700\u8981\u89e3\u51b3\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6cd5\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u56db\u7c7b\uff0c\u603b\u7ed3\u4e86\u5173\u952e\u8de8\u6a21\u6001\u7b56\u7565\uff08\u5982\u5bf9\u9f50\u548c\u878d\u5408\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u9886\u57df\u591a\u6a21\u6001\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u540c\u6587\u672c\u6570\u636e\u4e0e\u8de8\u6a21\u6001\u7b56\u7565\u7684\u7ec4\u5408\u5bf9\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u6709\u6548\u6027\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u8de8\u6a21\u6001\u5efa\u6a21\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u9002\u5e94\u6027\u3001\u6548\u7387\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.01854", "pdf": "https://arxiv.org/pdf/2505.01854", "abs": "https://arxiv.org/abs/2505.01854", "authors": ["Yuwen Chen", "Zafer Yildiz", "Qihang Li", "Yaqian Chen", "Haoyu Dong", "Hanxue Gu", "Nicholas Konz", "Maciej A. Mazurowski"], "title": "Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Manual annotation of volumetric medical images, such as magnetic resonance\nimaging (MRI) and computed tomography (CT), is a labor-intensive and\ntime-consuming process. Recent advancements in foundation models for video\nobject segmentation, such as Segment Anything Model 2 (SAM 2), offer a\npotential opportunity to significantly speed up the annotation process by\nmanually annotating one or a few slices and then propagating target masks\nacross the entire volume. However, the performance of SAM 2 in this context\nvaries. Our experiments show that relying on a single memory bank and attention\nmodule is prone to error propagation, particularly at boundary regions where\nthe target is present in the previous slice but absent in the current one. To\naddress this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel\narchitecture that integrates distinct short-term and long-term memory banks\nwith separate attention modules to improve segmentation accuracy. We evaluate\nSLM-SAM 2 on three public datasets covering organs, bones, and muscles across\nMRI and CT modalities. We show that the proposed method markedly outperforms\nthe default SAM 2, achieving average Dice Similarity Coefficient improvement of\n0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for\nthe initial adaptation, respectively. SLM-SAM 2 also exhibits stronger\nresistance to over-propagation, making a notable step toward more accurate\nautomated annotation of medical images for segmentation model development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SLM-SAM 2\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u77ed\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u5206\u5272\u7cbe\u5ea6\uff0c\u663e\u8457\u4f18\u4e8eSAM 2\u3002", "motivation": "\u51cf\u5c11\u533b\u5b66\u56fe\u50cf\u624b\u52a8\u6807\u6ce8\u7684\u9ad8\u6210\u672c\u548c\u8017\u65f6\u957f\u95ee\u9898\uff0c\u6539\u8fdbSAM 2\u5728\u4f53\u79ef\u5206\u5272\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u77ed\u3001\u957f\u671f\u8bb0\u5fc6\u5e93\u548c\u72ec\u7acb\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4f18\u5316\u76ee\u6807\u63a9\u7801\u4f20\u64ad\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u5957\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5e73\u5747Dice\u7cfb\u6570\u63d0\u53470.14\uff085\u6837\u672c\uff09\u548c0.11\uff08\u5355\u6837\u672c\uff09\uff0c\u6297\u8fc7\u4f20\u64ad\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "SLM-SAM 2\u4e3a\u533b\u5b66\u56fe\u50cf\u81ea\u52a8\u6807\u6ce8\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02604", "pdf": "https://arxiv.org/pdf/2505.02604", "abs": "https://arxiv.org/abs/2505.02604", "authors": ["Yongding Tian", "Zaid Al-Ars", "Maksim Kitsak", "Peter Hofstee"], "title": "Low-Loss Space in Neural Networks is Continuous and Fully Connected", "categories": ["cs.LG"], "comment": "10 pages, 4 figures", "summary": "Visualizations of the loss landscape in neural networks suggest that minima\nare isolated points. However, both theoretical and empirical studies indicate\nthat it is possible to connect two different minima with a path consisting of\nintermediate points that also have low loss. In this study, we propose a new\nalgorithm which investigates low-loss paths in the full parameter space, not\nonly between two minima. Our experiments on LeNet5, ResNet18, and Compact\nConvolutional Transformer architectures consistently demonstrate the existence\nof such continuous paths in the parameter space. These results suggest that the\nlow-loss region is a fully connected and continuous space in the parameter\nspace. Our findings provide theoretical insight into neural network\nover-parameterization, highlighting that parameters collectively define a\nhigh-dimensional low-loss space, implying parameter redundancy exists only\nwithin individual models and not throughout the entire low-loss space.\nAdditionally, our work also provides new visualization methods and\nopportunities to improve model generalization by exploring the low-loss space\nthat is closer to the origin.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7b97\u6cd5\uff0c\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7a7a\u95f4\u4e2d\u4f4e\u635f\u5931\u8def\u5f84\u7684\u8fde\u7eed\u6027\uff0c\u8bc1\u660e\u4e86\u4f4e\u635f\u5931\u533a\u57df\u662f\u8fde\u901a\u4e14\u8fde\u7eed\u7684\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u4e2d\u7684\u6700\u5c0f\u503c\u662f\u5b64\u7acb\u7684\uff0c\u4f46\u7406\u8bba\u548c\u5b9e\u8bc1\u5747\u8868\u660e\u4e0d\u540c\u6700\u5c0f\u503c\u4e4b\u95f4\u53ef\u4ee5\u901a\u8fc7\u4f4e\u635f\u5931\u8def\u5f84\u8fde\u63a5\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u53c2\u6570\u7a7a\u95f4\u4e2d\u662f\u5426\u5b58\u5728\u66f4\u5e7f\u6cdb\u7684\u4f4e\u635f\u5931\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u5728\u5b8c\u6574\u53c2\u6570\u7a7a\u95f4\u4e2d\u63a2\u7d22\u4f4e\u635f\u5931\u8def\u5f84\uff0c\u5e76\u5728LeNet5\u3001ResNet18\u548cCompact Convolutional Transformer\u67b6\u6784\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u53c2\u6570\u7a7a\u95f4\u4e2d\u5b58\u5728\u8fde\u7eed\u7684\u4f4e\u635f\u5931\u8def\u5f84\uff0c\u4f4e\u635f\u5931\u533a\u57df\u662f\u8fde\u901a\u4e14\u8fde\u7eed\u7684\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u8fc7\u53c2\u6570\u5316\u7684\u7406\u8bba\u610f\u4e49\uff0c\u53c2\u6570\u5197\u4f59\u4ec5\u5b58\u5728\u4e8e\u5355\u4e2a\u6a21\u578b\u5185\u90e8\u800c\u975e\u6574\u4e2a\u4f4e\u635f\u5931\u7a7a\u95f4\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u548c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u7684\u6f5c\u5728\u9014\u5f84\u3002"}}
{"id": "2505.02621", "pdf": "https://arxiv.org/pdf/2505.02621", "abs": "https://arxiv.org/abs/2505.02621", "authors": ["Anming Gu", "Juno Kim"], "title": "Mirror Mean-Field Langevin Dynamics", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized\nnonlinear convex functional on the Wasserstein space over $\\mathbb{R}^d$, and\nhas gained attention recently as a model for the gradient descent dynamics of\ninteracting particle systems such as infinite-width two-layer neural networks.\nHowever, many problems of interest have constrained domains, which are not\nsolved by existing mean-field algorithms due to the global diffusion term. We\nstudy the optimization of probability measures constrained to a convex subset\nof $\\mathbb{R}^d$ by proposing the \\emph{mirror mean-field Langevin dynamics}\n(MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain\nlinear convergence guarantees for the continuous MMFLD via a uniform\nlog-Sobolev inequality, and uniform-in-time propagation of chaos results for\nits time- and particle-discretized counterpart.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u955c\u50cf\u5e73\u5747\u573a\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff08MMFLD\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u7ea6\u675f\u5728\u51f8\u5b50\u96c6\u4e0a\u7684\u6982\u7387\u6d4b\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e73\u5747\u573a\u7b97\u6cd5\u56e0\u5168\u5c40\u6269\u6563\u9879\u65e0\u6cd5\u5904\u7406\u7ea6\u675f\u57df\u7684\u95ee\u9898\uff0c\u5e76\u83b7\u5f97\u4e86\u7ebf\u6027\u6536\u655b\u4fdd\u8bc1\u548c\u6df7\u6c8c\u4f20\u64ad\u7ed3\u679c\u3002", "motivation": "\u8bb8\u591a\u95ee\u9898\u5177\u6709\u7ea6\u675f\u57df\uff0c\u73b0\u6709\u5e73\u5747\u573a\u7b97\u6cd5\u56e0\u5168\u5c40\u6269\u6563\u9879\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u955c\u50cf\u5e73\u5747\u573a\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff08MMFLD\uff09\uff0c\u6269\u5c55\u4e86\u5e73\u5747\u573a\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff08MFLD\uff09\u5230\u955c\u50cf\u6717\u4e4b\u4e07\u6846\u67b6\u4e2d\uff0c\u9002\u7528\u4e8e\u51f8\u5b50\u96c6\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5747\u5300\u5bf9\u6570\u7d22\u535a\u5217\u592b\u4e0d\u7b49\u5f0f\u83b7\u5f97\u4e86\u8fde\u7eedMMFLD\u7684\u7ebf\u6027\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u65f6\u95f4\u548c\u7c92\u5b50\u79bb\u6563\u5316\u7248\u672c\u7684\u6df7\u6c8c\u4f20\u64ad\u7ed3\u679c\u3002", "conclusion": "MMFLD\u4e3a\u7ea6\u675f\u57df\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.02627", "pdf": "https://arxiv.org/pdf/2505.02627", "abs": "https://arxiv.org/abs/2505.02627", "authors": ["Yuanpeng Li"], "title": "A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Compositional generalization is a crucial property in artificial\nintelligence, enabling models to handle novel combinations of known components.\nWhile most deep learning models lack this capability, certain models succeed in\nspecific tasks, suggesting the existence of governing conditions. This paper\nderives a necessary and sufficient condition for compositional generalization\nin neural networks. Conceptually, it requires that (i) the computational graph\nmatches the true compositional structure, and (ii) components encode just\nenough information in training. The condition is supported by mathematical\nproofs. This criterion combines aspects of architecture design, regularization,\nand training data properties. A carefully designed minimal example illustrates\nan intuitive understanding of the condition. We also discuss the potential of\nthe condition for assessing compositional generalization before training. This\nwork is a fundamental theoretical study of compositional generalization in\nneural networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u795e\u7ecf\u7f51\u7edc\u7ec4\u5408\u6cdb\u5316\u7684\u5145\u8981\u6761\u4ef6\uff0c\u7ed3\u5408\u67b6\u6784\u8bbe\u8ba1\u3001\u6b63\u5219\u5316\u548c\u8bad\u7ec3\u6570\u636e\u7279\u6027\uff0c\u4e3a\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u7ec4\u5408\u6cdb\u5316\u662fAI\u7684\u5173\u952e\u80fd\u529b\uff0c\u4f46\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u8be5\u80fd\u529b\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u63d0\u51fa\u6761\u4ef6\uff1a\u8981\u6c42\u8ba1\u7b97\u56fe\u5339\u914d\u771f\u5b9e\u7ec4\u5408\u7ed3\u6784\uff0c\u4e14\u7ec4\u4ef6\u8bad\u7ec3\u65f6\u7f16\u7801\u7684\u4fe1\u606f\u8db3\u591f\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6761\u4ef6\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u793a\u4f8b\u9a8c\u8bc1\u4e86\u5176\u76f4\u89c2\u7406\u89e3\u3002", "conclusion": "\u8be5\u6761\u4ef6\u4e3a\u8bad\u7ec3\u524d\u8bc4\u4f30\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u662f\u795e\u7ecf\u7f51\u7edc\u7ec4\u5408\u6cdb\u5316\u7684\u57fa\u7840\u7406\u8bba\u7814\u7a76\u3002"}}
{"id": "2505.01881", "pdf": "https://arxiv.org/pdf/2505.01881", "abs": "https://arxiv.org/abs/2505.01881", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "comment": "9 pages, 5 figures", "summary": "Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems.", "AI": {"tldr": "PhysNav-DG\u662f\u4e00\u79cd\u65b0\u578b\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4f20\u611f\u5668\u878d\u5408\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u80fd\u529b\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u9884\u6d4b\u5bfc\u822a\u52a8\u4f5c\u5e76\u751f\u6210\u8be6\u7ec6\u89e3\u91ca\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5bfc\u822a\u6210\u529f\u7387\u63d0\u534720%\u4ee5\u4e0a\u3002", "motivation": "\u4e3a\u4e86\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5bfc\u822a\uff0c\u9700\u8981\u517c\u987e\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u51b3\u7b56\u900f\u660e\u5ea6\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u4f20\u611f\u5668\u8f93\u5165\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaMA 3.2 11B\u3001BLIP-2\uff09\uff0c\u5e76\u5229\u7528\u6539\u8fdb\u7684\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u52a8\u6001\u8c03\u6574\u566a\u58f0\u53c2\u6570\u3002", "result": "\u5728MD-NEX\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bfc\u822a\u6210\u529f\u7387\u63d0\u5347\u8d8520%\uff0c\u751f\u6210\u7684\u89e3\u91ca\u65e2\u9ad8\u5ea6\u53ef\u4fe1\u53c8\u6e05\u6670\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8fde\u63a5\u9ad8\u5c42\u6b21\u8bed\u4e49\u63a8\u7406\u4e0e\u51e0\u4f55\u89c4\u5212\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u53ef\u4fe1\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02634", "pdf": "https://arxiv.org/pdf/2505.02634", "abs": "https://arxiv.org/abs/2505.02634", "authors": ["David Ramos", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "title": "Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning", "categories": ["cs.LG", "physics.comp-ph"], "comment": "20 pages, 7 figures", "summary": "The main objective of this paper is to introduce a transfer\nlearning-enhanced, multi-objective, deep reinforcement learning (DRL)\nmethodology that is able to optimise the geometry of any airfoil based on\nconcomitant aerodynamic and structural criteria. To showcase the method, we aim\nto maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural\nintegrity of the airfoil -- as modelled by its maximum thickness -- and train\nthe DRL agent using a list of different transfer learning (TL) strategies. The\nperformance of the DRL agent is compared with Particle Swarm Optimisation\n(PSO), a traditional gradient-free optimisation method. Results indicate that\nDRL agents are able to perform multi-objective shape optimisation, that the DRL\napproach outperforms PSO in terms of computational efficiency and shape\noptimisation performance, and that the TL-enhanced DRL agent achieves\nperformance comparable to the DRL one, while further saving substantial\ncomputational resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u7684\u591a\u76ee\u6807\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u7ffc\u578b\u51e0\u4f55\u5f62\u72b6\u3002\u8be5\u65b9\u6cd5\u540c\u65f6\u8003\u8651\u7a7a\u6c14\u52a8\u529b\u5b66\uff08\u5347\u963b\u6bd4\uff09\u548c\u7ed3\u6784\u5b8c\u6574\u6027\uff08\u6700\u5927\u539a\u5ea6\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3DRL\u4ee3\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u4f18\u5316\u6027\u80fd\u4e0a\u4f18\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\uff0c\u4e14\u8fc1\u79fb\u5b66\u4e60\u7684\u52a0\u5165\u8fdb\u4e00\u6b65\u8282\u7701\u4e86\u8ba1\u7b97\u8d44\u6e90\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\uff08\u5982PSO\uff09\u5728\u591a\u76ee\u6807\u7ffc\u578b\u4f18\u5316\u4e2d\u6548\u7387\u8f83\u4f4e\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u5df2\u6709\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u589e\u5f3a\u7684DRL\u65b9\u6cd5\uff0c\u63d0\u5347\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u591a\u79cd\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u5bf9\u7ffc\u578b\u7684\u5347\u963b\u6bd4\u548c\u7ed3\u6784\u539a\u5ea6\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\u3002DRL\u4ee3\u7406\u901a\u8fc7\u8bad\u7ec3\u5b66\u4e60\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u4e0e\u4f20\u7edfPSO\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "DRL\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u4f18\u5316\u6027\u80fd\u4e0a\u4f18\u4e8ePSO\u3002\u8fc1\u79fb\u5b66\u4e60\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u7eafDRL\u76f8\u5f53\u7684\u4f18\u5316\u6548\u679c\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u589e\u5f3a\u7684DRL\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u7ffc\u578b\u4f18\u5316\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u590d\u6742\u5de5\u7a0b\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01884", "pdf": "https://arxiv.org/pdf/2505.01884", "abs": "https://arxiv.org/abs/2505.01884", "authors": ["Siddharth Kothari", "Srinivasan Murali", "Sankalp Kothari", "Ujjwal Verma", "Jaya Sreevalsan-Nair"], "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "21 pages, 15 figures, 2 tables", "summary": "Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (Github link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86SAR\u56fe\u50cf\u4e2d\u5185\u9646\u6c34\u4f53\u7684\u5206\u5272\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86U-Net\u6a21\u578b\u5bf9\u4eba\u5de5\u6807\u6ce8\u9519\u8bef\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u6a21\u62df\u4e86\u5bf9\u6297\u653b\u51fb\u4ee5\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u3002", "motivation": "SAR\u56fe\u50cf\u7684\u6c34\u4f53\u5206\u5272\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4eba\u5de5\u6807\u6ce8\u5bb9\u6613\u5f15\u5165\u8bef\u5dee\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30U-Net\u6a21\u578b\u5bf9\u8fd9\u4e9b\u6807\u6ce8\u9519\u8bef\u7684\u5bb9\u5fcd\u5ea6\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5bf9\u6297\u653b\u51fb\u751f\u6210\u6709\u566a\u58f0\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u6d4b\u8bd5U-Net\u6a21\u578b\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u5206\u5272\u6027\u80fd\u3002", "result": "U-Net\u6a21\u578b\u5728\u4e00\u5b9a\u566a\u58f0\u6c34\u5e73\u4e0b\u4ecd\u80fd\u4fdd\u6301\u8f83\u597d\u7684\u5206\u5272\u6027\u80fd\uff0c\u4f46\u566a\u58f0\u8d85\u8fc7\u9608\u503c\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u4eba\u5de5\u6807\u6ce8\u7684\u8d28\u91cf\u5bf9\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5bf9\u6297\u8bad\u7ec3\u53ef\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.02640", "pdf": "https://arxiv.org/pdf/2505.02640", "abs": "https://arxiv.org/abs/2505.02640", "authors": ["Shubham Vaishnav", "Praveen Kumar Donta", "Sindri Magn\u00fasson"], "title": "Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": null, "summary": "Internet of Things (IoT) systems increasingly operate in environments where\ndevices must respond in real time while managing fluctuating resource\nconstraints, including energy and bandwidth. Yet, current approaches often fall\nshort in addressing scenarios where operational constraints evolve over time.\nTo address these limitations, we propose a novel Budgeted Multi-Armed Bandit\nframework tailored for IoT applications with dynamic operational limits. Our\nmodel introduces a decaying violation budget, which permits limited constraint\nviolations early in the learning process and gradually enforces stricter\ncompliance over time. We present the Budgeted Upper Confidence Bound (UCB)\nalgorithm, which adaptively balances performance optimization and compliance\nwith time-varying constraints. We provide theoretical guarantees showing that\nBudgeted UCB achieves sublinear regret and logarithmic constraint violations\nover the learning horizon. Extensive simulations in a wireless communication\nsetting show that our approach achieves faster adaptation and better constraint\nsatisfaction than standard online learning methods. These results highlight the\nframework's potential for building adaptive, resource-aware IoT systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9IoT\u5e94\u7528\u7684\u52a8\u6001\u9884\u7b97\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8870\u51cf\u8fdd\u89c4\u9884\u7b97\uff0c\u5e73\u8861\u6027\u80fd\u4f18\u5316\u4e0e\u7ea6\u675f\u9075\u5faa\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "IoT\u7cfb\u7edf\u9700\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u65f6\u54cd\u5e94\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8870\u51cf\u8fdd\u89c4\u9884\u7b97\u7684Budgeted Multi-Armed Bandit\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1Budgeted UCB\u7b97\u6cd5\uff0c\u52a8\u6001\u5e73\u8861\u6027\u80fd\u4e0e\u7ea6\u675f\u9075\u5faa\u3002", "result": "\u7406\u8bba\u8bc1\u660eBudgeted UCB\u5177\u6709\u6b21\u7ebf\u6027\u9057\u61be\u548c\u5bf9\u6570\u7ea7\u7ea6\u675f\u8fdd\u89c4\uff0c\u4eff\u771f\u663e\u793a\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u573a\u666f\u4e2d\u9002\u5e94\u66f4\u5feb\u3001\u7ea6\u675f\u6ee1\u8db3\u66f4\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u8d44\u6e90\u654f\u611f\u7684\u9002\u5e94\u6027IoT\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02655", "pdf": "https://arxiv.org/pdf/2505.02655", "abs": "https://arxiv.org/abs/2505.02655", "authors": ["Shiwei Guo", "Ziang Chen", "Yupeng Ma", "Yunfei Han", "Yi Wang"], "title": "SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Transformer model has shown strong performance in multivariate time\nseries forecasting by leveraging channel-wise self-attention. However, this\napproach lacks temporal constraints when computing temporal features and does\nnot utilize cumulative historical series effectively.To address these\nlimitations, we propose the Structured Channel-wise Transformer with Cumulative\nHistorical state (SCFormer). SCFormer introduces temporal constraints to all\nlinear transformations, including the query, key, and value matrices, as well\nas the fully connected layers within the Transformer. Additionally, SCFormer\nemploys High-order Polynomial Projection Operators (HiPPO) to deal with\ncumulative historical time series, allowing the model to incorporate\ninformation beyond the look-back window during prediction. Extensive\nexperiments on multiple real-world datasets demonstrate that SCFormer\nsignificantly outperforms mainstream baselines, highlighting its effectiveness\nin enhancing time series forecasting. The code is publicly available at\nhttps://github.com/ShiweiGuo1995/SCFormer", "AI": {"tldr": "SCFormer\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u7ea6\u675f\u548c\u9ad8\u9636\u591a\u9879\u5f0f\u6295\u5f71\u7b97\u5b50\u63d0\u5347Transformer\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684Transformer\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u7ea6\u675f\u4e14\u672a\u5145\u5206\u5229\u7528\u7d2f\u79ef\u5386\u53f2\u5e8f\u5217\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u53d7\u9650\u3002", "method": "SCFormer\u5bf9Transformer\u7684\u6240\u6709\u7ebf\u6027\u53d8\u6362\u5f15\u5165\u65f6\u95f4\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u9ad8\u9636\u591a\u9879\u5f0f\u6295\u5f71\u7b97\u5b50\uff08HiPPO\uff09\u5904\u7406\u7d2f\u79ef\u5386\u53f2\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSCFormer\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SCFormer\u901a\u8fc7\u6539\u8fdb\u65f6\u95f4\u7ea6\u675f\u548c\u5386\u53f2\u5e8f\u5217\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6548\u679c\u3002"}}
{"id": "2505.02659", "pdf": "https://arxiv.org/pdf/2505.02659", "abs": "https://arxiv.org/abs/2505.02659", "authors": ["Andrey Sidorenko"], "title": "A Note on Statistically Accurate Tabular Data Generation Using Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probobility distributions to\nenhance the statistical fidelity of LLM-generated tabular data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u9a71\u52a8\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u66f4\u51c6\u786e\u7684\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u590d\u6742\u7279\u5f81\u4f9d\u8d56\u5173\u7cfb\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5408\u6210\u8868\u683c\u6570\u636e\u65f6\u96be\u4ee5\u4fdd\u6301\u590d\u6742\u7279\u5f81\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c24\u5176\u662f\u5206\u7c7b\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u6982\u7387\u9a71\u52a8\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5229\u7528LLMs\u4f30\u8ba1\u6761\u4ef6\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u9ad8\u6570\u636e\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u8868\u683c\u6570\u636e\u7684\u7edf\u8ba1\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u6982\u7387\u9a71\u52a8\u7684\u63d0\u793a\u65b9\u6cd5\u4e3aLLM\u751f\u6210\u9ad8\u4fdd\u771f\u8868\u683c\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.01931", "pdf": "https://arxiv.org/pdf/2505.01931", "abs": "https://arxiv.org/abs/2505.01931", "authors": ["Jesse Barkley", "Abraham George", "Amir Barati Farimani"], "title": "Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics", "categories": ["cs.RO", "cs.AI"], "comment": "10 pages, 4 figures, 2 tables", "summary": "Classical robot navigation often relies on hardcoded state machines and\npurely geometric path planners, limiting a robot's ability to interpret\nhigh-level semantic instructions. In this paper, we first assess GPT-4's\nability to act as a path planner compared to the A* algorithm, then present a\nhybrid planning framework that integrates GPT-4's semantic reasoning with A* on\na low-cost robot platform operating on ROS2 Humble. Our approach eliminates\nexplicit finite state machine (FSM) coding by using prompt-based GPT-4\nreasoning to handle task logic while maintaining the accurate paths computed by\nA*. The GPT-4 module provides semantic understanding of instructions and\nenvironmental cues (e.g., recognizing toxic obstacles or crowded areas to\navoid, or understanding low-battery situations requiring alternate route\nselection), and dynamically adjusts the robot's occupancy grid via obstacle\nbuffering to enforce semantic constraints. We demonstrate multi-step reasoning\nfor sequential tasks, such as first navigating to a resource goal and then\nreaching a final destination safely. Experiments on a Petoi Bittle robot with\nan overhead camera and Raspberry Pi Zero 2W compare classical A* against\nGPT-4-assisted planning. Results show that while A* is faster and more accurate\nfor basic route generation and obstacle avoidance, the GPT-4-integrated system\nachieves high success rates (96-100%) on semantic tasks that are infeasible for\npure geometric planners. This work highlights how affordable robots can exhibit\nintelligent, context-aware behaviors by leveraging large language model\nreasoning with minimal hardware and no fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GPT-4\u8bed\u4e49\u7406\u89e3\u548cA*\u7b97\u6cd5\u7684\u6df7\u5408\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u901a\u8fc7\u6d88\u9664\u786c\u7f16\u7801\u72b6\u6001\u673a\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u7ea7\u8bed\u4e49\u6307\u4ee4\u7684\u667a\u80fd\u54cd\u5e94\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5bfc\u822a\u4f9d\u8d56\u786c\u7f16\u7801\u72b6\u6001\u673a\u548c\u51e0\u4f55\u8def\u5f84\u89c4\u5212\uff0c\u96be\u4ee5\u7406\u89e3\u9ad8\u7ea7\u8bed\u4e49\u6307\u4ee4\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u667a\u80fd\u884c\u4e3a\u3002", "method": "\u7ed3\u5408GPT-4\u7684\u8bed\u4e49\u63a8\u7406\u548cA*\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5360\u7528\u7f51\u683c\u548c\u6267\u884c\u591a\u6b65\u63a8\u7406\uff0c\u5b9e\u73b0\u8bed\u4e49\u7ea6\u675f\u7684\u4efb\u52a1\u903b\u8f91\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGPT-4\u8f85\u52a9\u7684\u7cfb\u7edf\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u9ad8\u8fbe96-100%\uff0c\u800cA*\u7b97\u6cd5\u5728\u57fa\u7840\u8def\u5f84\u89c4\u5212\u4e0a\u66f4\u5feb\u66f4\u51c6\u786e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u673a\u5668\u4eba\u53ef\u5b9e\u73b0\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u884c\u4e3a\uff0c\u65e0\u9700\u590d\u6742\u8c03\u4f18\u3002"}}
{"id": "2505.02712", "pdf": "https://arxiv.org/pdf/2505.02712", "abs": "https://arxiv.org/abs/2505.02712", "authors": ["Andrzej Mizera", "Jakub Zarzycki"], "title": "Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework", "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "comment": null, "summary": "Cellular reprogramming, the artificial transformation of one cell type into\nanother, has been attracting increasing research attention due to its\ntherapeutic potential for complex diseases. However, discovering reprogramming\nstrategies through classical wet-lab experiments is hindered by lengthy time\ncommitments and high costs. In this study, we explore the use of deep\nreinforcement learning (DRL) to control Boolean network models of complex\nbiological systems, such as gene regulatory networks and signalling pathway\nnetworks. We formulate a novel control problem for Boolean network models under\nthe asynchronous update mode in the context of cellular reprogramming. To\nfacilitate scalability, we consider our previously introduced concept of a\npseudo-attractor and we improve our procedure for effective identification of\npseudo-attractor states. Finally, we devise a computational framework to solve\nthe control problem. To leverage the structure of biological systems, we\nincorporate graph neural networks with graph convolutions into the artificial\nneural network approximator for the action-value function learned by the DRL\nagent. Experiments on a number of large real-world biological networks from\nliterature demonstrate the scalability and effectiveness of our approach.", "AI": {"tldr": "\u6458\u8981\uff1a\u8bba\u6587\u63a2\u7d22\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u7ec6\u80de\u91cd\u7f16\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u5229\u7528\u5e03\u5c14\u7f51\u7edc\u6a21\u578b\u548c\u5f02\u6b65\u66f4\u65b0\u6a21\u5f0f\u89e3\u51b3\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u4e86\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u679c\u3002", "motivation": "\u52a8\u673a\uff1a\u4f20\u7edf\u7684\u6e7f\u5b9e\u9a8c\u65b9\u6cd5\u5728\u7ec6\u80de\u91cd\u7f16\u7a0b\u7814\u7a76\u4e2d\u8017\u65f6\u4e14\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\uff1a\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u5e03\u5c14\u7f51\u7edc\u6a21\u578b\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u89e3\u51b3\u7ec6\u80de\u91cd\u7f16\u7a0b\u4e2d\u7684\u63a7\u5236\u95ee\u9898\u3002", "result": "\u7ed3\u679c\uff1a\u5728\u591a\u4e2a\u771f\u5b9e\u751f\u7269\u7f51\u7edc\u4e0a\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u4e3a\u7ec6\u80de\u91cd\u7f16\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02714", "pdf": "https://arxiv.org/pdf/2505.02714", "abs": "https://arxiv.org/abs/2505.02714", "authors": ["Xiao Shou", "Debarun Bhattacharjya", "Yanna Ding", "Chen Zhao", "Rui Li", "Jianxi Gao"], "title": "Less is More: Efficient Weight Farcasting with 1-Layer Neural Network", "categories": ["cs.LG"], "comment": "Accepted to DASFAA '25", "summary": "Addressing the computational challenges inherent in training large-scale deep\nneural networks remains a critical endeavor in contemporary machine learning\nresearch. While previous efforts have focused on enhancing training efficiency\nthrough techniques such as gradient descent with momentum, learning rate\nscheduling, and weight regularization, the demand for further innovation\ncontinues to burgeon as model sizes keep expanding. In this study, we introduce\na novel framework which diverges from conventional approaches by leveraging\nlong-term time series forecasting techniques. Our method capitalizes solely on\ninitial and final weight values, offering a streamlined alternative for complex\nmodel architectures. We also introduce a novel regularizer that is tailored to\nenhance the forecasting performance of our approach. Empirical evaluations\nconducted on synthetic weight sequences and real-world deep learning\narchitectures, including the prominent large language model DistilBERT,\ndemonstrate the superiority of our method in terms of forecasting accuracy and\ncomputational efficiency. Notably, our framework showcases improved performance\nwhile requiring minimal additional computational overhead, thus presenting a\npromising avenue for accelerating the training process across diverse tasks and\narchitectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6280\u672f\u7684\u65b0\u578b\u6846\u67b6\uff0c\u4ec5\u4f9d\u8d56\u521d\u59cb\u548c\u6700\u7ec8\u6743\u91cd\u503c\uff0c\u4e3a\u590d\u6742\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u6b63\u5219\u5316\u5668\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982\u5e26\u52a8\u91cf\u7684\u68af\u5ea6\u4e0b\u964d\u3001\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u6743\u91cd\u6b63\u5219\u5316\uff09\u7684\u6548\u7387\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4e9f\u9700\u521b\u65b0\u65b9\u6cd5\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u521d\u59cb\u548c\u6700\u7ec8\u6743\u91cd\u503c\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u578b\u6b63\u5219\u5316\u5668\u6765\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u5408\u6210\u6743\u91cd\u5e8f\u5217\u548c\u771f\u5b9e\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982DistilBERT\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a0\u901f\u591a\u6837\u5316\u4efb\u52a1\u548c\u67b6\u6784\u7684\u8bad\u7ec3\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\uff0c\u5c24\u5176\u5728\u6a21\u578b\u89c4\u6a21\u6301\u7eed\u589e\u5927\u7684\u80cc\u666f\u4e0b\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2505.02737", "pdf": "https://arxiv.org/pdf/2505.02737", "abs": "https://arxiv.org/abs/2505.02737", "authors": ["Pons Gerard", "Bilalli Besim", "Queralt Anna"], "title": "Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": "Pre-print submitted to ISWC 2024", "summary": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.", "AI": {"tldr": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5b9e\u4f53\u6d88\u6b67\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u5c42\u7ea7\u5206\u7c7b\u548c\u5b9e\u4f53\u63cf\u8ff0\u6539\u5584\u8f93\u5165\u63d0\u793a\uff0c\u6548\u679c\u4f18\u4e8e\u672a\u589e\u5f3a\u6a21\u578b\uff0c\u5e76\u5177\u66f4\u9ad8\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u3001\u8fc7\u65f6\u77e5\u8bc6\u6216\u9886\u57df\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u548c\u65f6\u95f4\u6d88\u8017\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7684\u5c42\u7ea7\u5206\u7c7b\u9010\u6b65\u4fee\u526a\u5019\u9009\u5b9e\u4f53\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f53\u63cf\u8ff0\u4e30\u5bcc\u8f93\u5165\u63d0\u793a\uff0c\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u4f53\u6d88\u6b67\u80fd\u529b\u3002", "result": "\u5728\u6d41\u884c\u7684\u5b9e\u4f53\u6d88\u6b67\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u672a\u589e\u5f3a\u548c\u4ec5\u589e\u5f3a\u63cf\u8ff0\u7684\u6a21\u578b\uff0c\u4e14\u6bd4\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u66f4\u5177\u9002\u5e94\u6027\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u5bf9\u5b9e\u4f53\u6d88\u6b67\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u606f\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2505.01950", "pdf": "https://arxiv.org/pdf/2505.01950", "abs": "https://arxiv.org/abs/2505.01950", "authors": ["Dong Xing", "Xianxun Zhu", "Wei Zhou", "Qika Lin", "Hang Yang", "Yuqing Wang"], "title": "Segment Any RGB-Thermal Model with Language-aided Distillation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.04220 by other authors", "summary": "The recent Segment Anything Model (SAM) demonstrates strong instance\nsegmentation performance across various downstream tasks. However, SAM is\ntrained solely on RGB data, limiting its direct applicability to RGB-thermal\n(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for\nscene understanding in adverse weather and lighting conditions, such as low\nlight and overexposure, we propose a novel framework, SARTM, which customizes\nthe powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash\nthe potential of SAM while introduce semantic understanding modules for RGB-T\ndata pairs. Specifically, our framework first involves fine tuning the original\nSAM by adding extra LoRA layers, aiming at preserving SAM's strong\ngeneralization and segmentation capabilities for downstream tasks. Secondly, we\nintroduce language information as guidance for training our SARTM. To address\ncross-modal inconsistencies, we introduce a Cross-Modal Knowledge\nDistillation(CMKD) module that effectively achieves modality adaptation while\nmaintaining its generalization capabilities. This semantic module enables the\nminimization of modality gaps and alleviates semantic ambiguity, facilitating\nthe combination of any modality under any visual conditions. Furthermore, we\nenhance the segmentation performance by adjusting the segmentation head of SAM\nand incorporating an auxiliary semantic segmentation head, which integrates\nmulti-scale features for effective fusion. Extensive experiments are conducted\nacross three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,\nand FMB. Both quantitative and qualitative results consistently demonstrate\nthat the proposed SARTM significantly outperforms state-of-the-art approaches\nacross a variety of conditions.", "AI": {"tldr": "SARTM\u6846\u67b6\u901a\u8fc7\u5b9a\u5236SAM\u6a21\u578b\uff0c\u7ed3\u5408RGB-\u70ed\u6210\u50cf\u6570\u636e\uff0c\u5f15\u5165\u8bed\u4e49\u6a21\u5757\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eSAM\u4ec5\u57fa\u4e8eRGB\u6570\u636e\u8bad\u7ec3\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eRGB-\u70ed\u6210\u50cf\uff08RGB-T\uff09\u8bed\u4e49\u5206\u5272\u3002\u800cRGB-T\u5728\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u66f4\u5177\u4f18\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u901a\u8fc7\u6dfb\u52a0LoRA\u5c42\u5fae\u8c03SAM\uff0c\u4fdd\u7559\u5176\u6cdb\u5316\u80fd\u529b\uff1b2. \u5f15\u5165\u8bed\u8a00\u4fe1\u606f\u6307\u5bfc\u8bad\u7ec3\uff1b3. \u4f7f\u7528\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff08CMKD\uff09\u51cf\u5c11\u6a21\u6001\u5dee\u5f02\uff1b4. \u8c03\u6574\u5206\u5272\u5934\u5e76\u52a0\u5165\u591a\u5c3a\u5ea6\u8bed\u4e49\u5206\u5272\u5934\u3002", "result": "\u5728MFNET\u3001PST900\u548cFMB\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSARTM\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SARTM\u6210\u529f\u5c06SAM\u9002\u914d\u5230RGB-T\u4efb\u52a1\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u548c\u8bed\u4e49\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2505.02743", "pdf": "https://arxiv.org/pdf/2505.02743", "abs": "https://arxiv.org/abs/2505.02743", "authors": ["Jiaxiang Yi", "Miguel A. Bessa"], "title": "Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties", "categories": ["cs.LG", "stat.ML"], "comment": "28 pages, 19 figures", "summary": "Real-world data contains aleatoric uncertainty - irreducible noise arising\nfrom imperfect measurements or from incomplete knowledge about the data\ngeneration process. Mean variance estimation (MVE) networks can learn this type\nof uncertainty but require ad-hoc regularization strategies to avoid\noverfitting and are unable to predict epistemic uncertainty (model\nuncertainty). Conversely, Bayesian neural networks predict epistemic\nuncertainty but are notoriously difficult to train due to the approximate\nnature of Bayesian inference. We propose to cooperatively train a variance\nnetwork with a Bayesian neural network and demonstrate that the resulting model\ndisentangles aleatoric and epistemic uncertainties while improving the mean\nestimation. We demonstrate the effectiveness and scalability of this method\nacross a diverse range of datasets, including a time-dependent heteroscedastic\nregression dataset we created where the aleatoric uncertainty is known. The\nproposed method is straightforward to implement, robust, and adaptable to\nvarious model architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MVE\u7f51\u7edc\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7684\u534f\u540c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u5206\u79bb\u4e86\u6570\u636e\u4e2d\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u5747\u503c\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u4e2d\u5b58\u5728\u4e0d\u53ef\u7ea6\u7684\u968f\u673a\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\uff08\u5982MVE\u7f51\u7edc\u548c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff09\u5404\u6709\u5c40\u9650\uff0c\u96be\u4ee5\u540c\u65f6\u5904\u7406\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u534f\u540c\u8bad\u7ec3MVE\u7f51\u7edc\u4e0e\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u7684\u5206\u79bb\uff0c\u5e76\u6539\u8fdb\u5747\u503c\u4f30\u8ba1\u3002\u65b9\u6cd5\u6613\u4e8e\u5b9e\u73b0\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\uff08\u5305\u62ec\u4e00\u4e2a\u81ea\u5efa\u7684\u65f6\u53d8\u5f02\u65b9\u5dee\u56de\u5f52\u6570\u636e\u96c6\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9c81\u68d2\u6027\u5f3a\uff0c\u8fd8\u80fd\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u6a21\u578b\u67b6\u6784\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01956", "pdf": "https://arxiv.org/pdf/2505.01956", "abs": "https://arxiv.org/abs/2505.01956", "authors": ["Ganesh Sapkota", "Sanjay Madria"], "title": "SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "In battlefield environments, adversaries frequently disrupt GPS signals,\nrequiring alternative localization and navigation methods. Traditional\nvision-based approaches like Simultaneous Localization and Mapping (SLAM) and\nVisual Odometry (VO) involve complex sensor fusion and high computational\ndemand, whereas range-free methods like DV-HOP face accuracy and stability\nchallenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a\nnavigation approach using landmark-based localization (LanBLoc) combined with a\nbattlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its\nperformance is benchmarked against three state-of-the-art visual localization\nalgorithms integrated with BMM and Bayesian filters, evaluated on synthetic and\nreal-imitated trajectory datasets using metrics including Average Displacement\nError (ADE), Final Displacement Error (FDE), and a newly introduced Average\nWeighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior\nperformance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two\nsafe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by\nintegrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm\nfor obstacle avoidance and risk exposure minimization. Simulation results in\nbattlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk\nexposure, and trajectory efficiency, while SafeNav-CHull provides superior\ncomputational speed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLanBLoc-BMM\uff0c\u7ed3\u5408\u5730\u6807\u5b9a\u4f4d\u4e0e\u6218\u573a\u8fd0\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5728GPS\u5931\u6548\u65f6\u5b9e\u73b0\u5bfc\u822a\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u3002", "motivation": "\u6218\u573a\u73af\u5883\u4e2dGPS\u4fe1\u53f7\u5e38\u53d7\u5e72\u6270\uff0c\u73b0\u6709\u89c6\u89c9\u6216\u8303\u56f4\u65e0\u5173\u65b9\u6cd5\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u6216\u51c6\u786e\u6027\u4e0a\u4e0d\u8db3\uff0c\u9700\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528LanBLoc\uff08\u5730\u6807\u5b9a\u4f4d\uff09\u4e0e\u6218\u573a\u8fd0\u52a8\u6a21\u578b\uff08BMM\uff09\u7ed3\u5408\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EKF\uff09\uff0c\u5e76\u5f15\u5165RAw-RRT*\u7b97\u6cd5\u7528\u4e8e\u907f\u969c\u548c\u98ce\u9669\u6700\u5c0f\u5316\u3002", "result": "LanBLoc-BMM(EKF)\u5728ADE\u3001FDE\u548cAWRS\u6307\u6807\u4e0a\u4f18\u4e8e\u5bf9\u6bd4\u7b97\u6cd5\u3002SafeNav-Centroid\u5728\u7cbe\u5ea6\u548c\u98ce\u9669\u63a7\u5236\u4e0a\u66f4\u4f18\uff0cSafeNav-CHull\u8ba1\u7b97\u66f4\u5feb\u3002", "conclusion": "LanBLoc-BMM\u53ca\u4e24\u79cdSafeNav\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6218\u573a\u73af\u5883\u4e0b\u7684GPS\u66ff\u4ee3\u5bfc\u822a\u9700\u6c42\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u3001\u98ce\u9669\u4e0e\u6548\u7387\u3002"}}
{"id": "2505.02795", "pdf": "https://arxiv.org/pdf/2505.02795", "abs": "https://arxiv.org/abs/2505.02795", "authors": ["Zheng Lin", "Yuxin Zhang", "Zhe Chen", "Zihan Fang", "Xianhao Chen", "Praneeth Vepakomma", "Wei Ni", "Jun Luo", "Yue Gao"], "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "16 pages, 22 figures", "summary": "Recently, large language models (LLMs) have achieved remarkable\nbreakthroughs, revolutionizing the natural language processing domain and\nbeyond. Due to immense parameter sizes, fine-tuning these models with private\ndata for diverse downstream tasks has become mainstream. Though federated\nlearning (FL) offers a promising solution for fine-tuning LLMs without sharing\nraw data, substantial computing costs hinder its democratization. Moreover, in\nreal-world scenarios, private client devices often possess heterogeneous\ncomputing resources, further complicating LLM fine-tuning. To combat these\nchallenges, we propose HSplitLoRA, a heterogeneous parameter-efficient\nfine-tuning (PEFT) framework built on split learning (SL) and low-rank\nadaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on\nheterogeneous client devices. HSplitLoRA first identifies important weights\nbased on their contributions to LLM training. It then dynamically configures\nthe decomposition ranks of LoRA adapters for selected weights and determines\nthe model split point according to varying computing budgets of client devices.\nFinally, a noise-free adapter aggregation mechanism is devised to support\nheterogeneous adapter aggregation without introducing noise. Extensive\nexperiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks\nin training accuracy and convergence speed.", "AI": {"tldr": "HSplitLoRA\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u88c2\u5b66\u4e60\u548c\u4f4e\u79e9\u9002\u5e94\u7684\u5f02\u6784\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f02\u6784\u8bbe\u5907\u4e0a\u7684\u5fae\u8c03\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u91cf\u5de8\u5927\uff0c\u8054\u90a6\u5b66\u4e60\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5ba2\u6237\u7aef\u8bbe\u5907\u8ba1\u7b97\u8d44\u6e90\u5f02\u6784\uff0c\u96be\u4ee5\u9ad8\u6548\u5fae\u8c03\uff0c\u56e0\u6b64\u63d0\u51faHSplitLoRA\u6846\u67b6\u3002", "method": "HSplitLoRA\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u6743\u91cd\u3001\u52a8\u6001\u914d\u7f6eLoRA\u9002\u914d\u5668\u5206\u89e3\u79e9\u3001\u786e\u5b9a\u6a21\u578b\u5206\u88c2\u70b9\uff0c\u5e76\u8bbe\u8ba1\u65e0\u566a\u58f0\u9002\u914d\u5668\u805a\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u5f02\u6784\u8bbe\u5907\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHSplitLoRA\u5728\u8bad\u7ec3\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "HSplitLoRA\u4e3a\u5f02\u6784\u73af\u5883\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01966", "pdf": "https://arxiv.org/pdf/2505.01966", "abs": "https://arxiv.org/abs/2505.01966", "authors": ["Bofei Liu", "Dong Ye", "Zunhao Yao", "Zhaowei Sun"], "title": "A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "6 pages, 7 figures", "summary": "Modular self-reconfigurable satellites refer to satellite clusters composed\nof individual modular units capable of altering their configurations. The\nconfiguration changes enable the execution of diverse tasks and mission\nobjectives. Existing path planning algorithms for reconfiguration often suffer\nfrom high computational complexity, poor generalization capability, and limited\nsupport for diverse target configurations. To address these challenges, this\npaper proposes a goal-oriented reinforcement learning-based path planning\nalgorithm. This algorithm is the first to address the challenge that previous\nreinforcement learning methods failed to overcome, namely handling multiple\ntarget configurations. Moreover, techniques such as Hindsight Experience Replay\nand Invalid Action Masking are incorporated to overcome the significant\nobstacles posed by sparse rewards and invalid actions. Based on these designs,\nour model achieves a 95% and 73% success rate in reaching arbitrary target\nconfigurations in a modular satellite cluster composed of four and six units,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u5bfc\u5411\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u6a21\u5757\u5316\u81ea\u91cd\u6784\u536b\u661f\u96c6\u7fa4\u5728\u591a\u76ee\u6807\u914d\u7f6e\u4e0b\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\uff0c\u6210\u529f\u7387\u9ad8\u8fbe95%\uff084\u5355\u5143\uff09\u548c73%\uff086\u5355\u5143\uff09\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u5bf9\u591a\u76ee\u6807\u914d\u7f6e\u652f\u6301\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u76ee\u6807\u5bfc\u5411\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408Hindsight Experience Replay\u548cInvalid Action Masking\u6280\u672f\uff0c\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u548c\u65e0\u6548\u52a8\u4f5c\u7684\u6311\u6218\u3002", "result": "\u57284\u5355\u5143\u548c6\u5355\u5143\u536b\u661f\u96c6\u7fa4\u4e2d\uff0c\u5206\u522b\u5b9e\u73b0\u4e8695%\u548c73%\u7684\u6210\u529f\u7387\u5230\u8fbe\u4efb\u610f\u76ee\u6807\u914d\u7f6e\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u76ee\u6807\u914d\u7f6e\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u6a21\u5757\u5316\u536b\u661f\u7684\u81ea\u91cd\u6784\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02809", "pdf": "https://arxiv.org/pdf/2505.02809", "abs": "https://arxiv.org/abs/2505.02809", "authors": ["Zhaorui Dong", "Yushun Zhang", "Zhi-Quan Luo", "Jianfeng Yao", "Ruoyu Sun"], "title": "Towards Quantifying the Hessian Structure of Neural Networks", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Empirical studies reported that the Hessian matrix of neural networks (NNs)\nexhibits a near-block-diagonal structure, yet its theoretical foundation\nremains unclear. In this work, we reveal two forces that shape the Hessian\nstructure: a ``static force'' rooted in the architecture design, and a\n``dynamic force'' arisen from training. We then provide a rigorous theoretical\nanalysis of ``static force'' at random initialization. We study linear models\nand 1-hidden-layer networks with the mean-square (MSE) loss and the\nCross-Entropy (CE) loss for classification tasks. By leveraging random matrix\ntheory, we compare the limit distributions of the diagonal and off-diagonal\nHessian blocks and find that the block-diagonal structure arises as $C\n\\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings\nreveal that $C$ is a primary driver of the near-block-diagonal structure. These\nresults may shed new light on the Hessian structure of large language models\n(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edcHessian\u77e9\u9635\u8fd1\u5757\u5bf9\u89d2\u7ed3\u6784\u7684\u6210\u56e0\uff0c\u5305\u62ec\u67b6\u6784\u8bbe\u8ba1\u7684\u201c\u9759\u6001\u529b\u201d\u548c\u8bad\u7ec3\u4ea7\u751f\u7684\u201c\u52a8\u6001\u529b\u201d\uff0c\u5e76\u91cd\u70b9\u5206\u6790\u4e86\u201c\u9759\u6001\u529b\u201d\u5728\u968f\u673a\u521d\u59cb\u5316\u4e0b\u7684\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edcHessian\u77e9\u9635\u8fd1\u5757\u5bf9\u89d2\u7ed3\u6784\u7684\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u6a21\u578b\u548c\u5355\u9690\u85cf\u5c42\u7f51\u7edc\uff0c\u7ed3\u5408MSE\u548cCE\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u968f\u673a\u77e9\u9635\u7406\u8bba\u6bd4\u8f83Hessian\u77e9\u9635\u5bf9\u89d2\u548c\u975e\u5bf9\u89d2\u5757\u7684\u6781\u9650\u5206\u5e03\u3002", "result": "\u5f53\u7c7b\u522b\u6570$C$\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u65f6\uff0cHessian\u77e9\u9635\u5448\u73b0\u5757\u5bf9\u89d2\u7ed3\u6784\uff0c\u8868\u660e$C$\u662f\u5176\u7ed3\u6784\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684Hessian\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u56e0\u4e3aLLMs\u901a\u5e38\u5177\u6709\u6781\u5927\u7684\u7c7b\u522b\u6570\u3002"}}
{"id": "2505.01998", "pdf": "https://arxiv.org/pdf/2505.01998", "abs": "https://arxiv.org/abs/2505.01998", "authors": ["Xiaoliang Chen", "Xin Yu", "Le Chang", "Yunhe Huang", "Jiashuai He", "Shibo Zhang", "Jin Li", "Likai Lin", "Ziyu Zeng", "Xianling Tu", "Shuyu Zhang"], "title": "A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction", "categories": ["cs.RO", "cs.AI", "physics.app-ph", "68T01", "I.2.8"], "comment": "34 pages, 11 figures, 10 tables", "summary": "This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations-covering far-field localization, weak signal detection, and\nmultilingual speech recognition-demonstrate that this hybrid strategy surpasses\ntraditional linear methods and purely data-driven baselines, achieving superior\nnoise suppression, minimal latency, and robust accuracy in demanding real-world\nscenarios. The proposed system demonstrates broad application prospects in AI\nhardware, robot, machine audition, artificial audition, and brain-machine\ninterfaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027\u58f0\u5b66\u8ba1\u7b97\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u566a\u58f0\u548c\u6df7\u54cd\u73af\u5883\u4e0b\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u3002\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u4ee5\u6291\u5236\u5e72\u6270\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4eba\u673a\u4ea4\u4e92\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e2d\u9762\u4e34\u566a\u58f0\u548c\u6df7\u54cd\u7684\u591a\u91cd\u6311\u6218\uff0c\u4f20\u7edf\u7ebf\u6027\u65b9\u6cd5\u6216\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5efa\u6a21\u548c\u667a\u80fd\u51b3\u7b56\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7269\u7406\u7684\u975e\u7ebf\u6027\u58f0\u5b66\u65b9\u7a0b\uff08\u5982Westervelt\u3001KZK\uff09\u5efa\u6a21\u9ad8\u9636\u58f0\u5b66\u73b0\u8c61\uff0c\u5c06\u5176\u5d4c\u5165\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u73af\u8def\u4e2d\uff0c\u81ea\u9002\u5e94\u4f18\u5316\u5438\u6536\u3001\u6ce2\u675f\u6210\u5f62\u7b49\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fdc\u573a\u5b9a\u4f4d\u3001\u5f31\u4fe1\u53f7\u68c0\u6d4b\u548c\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027\u65b9\u6cd5\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u566a\u58f0\u6291\u5236\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728AI\u786c\u4ef6\u3001\u673a\u5668\u4eba\u3001\u673a\u5668\u542c\u89c9\u548c\u8111\u673a\u63a5\u53e3\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e0b\u7684\u4ea4\u4e92\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01454", "pdf": "https://arxiv.org/pdf/2505.01454", "abs": "https://arxiv.org/abs/2505.01454", "authors": ["Zhiyong Jin", "Runhua Xu", "Chao Li", "Yizhong Liu", "Jianxin Li"], "title": "Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy, yet it faces significant\nchallenges in communication efficiency and vulnerability to poisoning attacks.\nWhile sparsification techniques mitigate communication overhead by transmitting\nonly critical model parameters, they inadvertently amplify security risks:\nadversarial clients can exploit sparse updates to evade detection and degrade\nmodel performance. Existing defense mechanisms, designed for standard FL\ncommunication scenarios, are ineffective in addressing these vulnerabilities\nwithin sparsified FL. To bridge this gap, we propose FLARE, a novel federated\nlearning framework that integrates sparse index mask inspection and model\nupdate sign similarity analysis to detect and mitigate poisoning attacks in\nsparsified FL. Extensive experiments across multiple datasets and adversarial\nscenarios demonstrate that FLARE significantly outperforms existing defense\nstrategies, effectively securing sparsified FL against poisoning attacks while\nmaintaining communication efficiency.", "AI": {"tldr": "FLARE \u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u7d22\u5f15\u63a9\u7801\u68c0\u67e5\u548c\u6a21\u578b\u66f4\u65b0\u7b26\u53f7\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6295\u6bd2\u653b\u51fb\u95ee\u9898\uff0c\u5e76\u4fdd\u6301\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u7a00\u758f\u5316\u8054\u90a6\u5b66\u4e60\u5728\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u7684\u540c\u65f6\uff0c\u7531\u4e8e\u7a00\u758f\u66f4\u65b0\u5bb9\u6613\u88ab\u653b\u51fb\u8005\u5229\u7528\u4ee5\u89c4\u907f\u68c0\u6d4b\uff0c\u589e\u52a0\u4e86\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u9632\u5fa1\u673a\u5236\u5728\u7a00\u758f\u5316\u573a\u666f\u4e0b\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "FLARE \u65b9\u6cd5\u5305\u62ec\u7a00\u758f\u7d22\u5f15\u63a9\u7801\u68c0\u67e5\u548c\u6a21\u578b\u66f4\u65b0\u7b26\u53f7\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u65e8\u5728\u68c0\u6d4b\u5e76\u51cf\u8f7b\u7a00\u758f\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6295\u6bd2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFLARE \u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u653b\u51fb\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u7b56\u7565\uff0c\u6709\u6548\u4fdd\u969c\u7a00\u758f\u5316\u8054\u90a6\u5b66\u4e60\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "FLARE \u4e3a\u7a00\u758f\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2505.01455", "pdf": "https://arxiv.org/pdf/2505.01455", "abs": "https://arxiv.org/abs/2505.01455", "authors": ["Gan Zhang", "Megha Rao", "Janni Yuval", "Ming Zhao"], "title": "Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity", "categories": ["physics.ao-ph", "cs.LG"], "comment": null, "summary": "Machine learning (ML) models are successful with weather forecasting and have\nshown progress in climate simulations, yet leveraging them for useful climate\npredictions needs exploration. Here we show this feasibility using NeuralGCM, a\nhybrid ML-physics atmospheric model, for seasonal predictions of large-scale\natmospheric variability and Northern Hemisphere tropical cyclone (TC) activity.\nInspired by physical model studies, we simplify boundary conditions, assuming\nsea surface temperature (SST) and sea ice follow their climatological cycle but\npersist anomalies present at initialization. With such forcings, NeuralGCM\nsimulates realistic atmospheric circulation and TC climatology patterns.\nFurthermore, this configuration yields useful seasonal predictions\n(July-November) for the tropical atmosphere and various TC activity metrics.\nNotably, the prediction skill for TC frequency in the North Atlantic and East\nPacific basins is comparable to existing physical models. These findings\nhighlight the promise of leveraging ML models with physical insights to model\nTC risks and deliver seamless weather-climate predictions.", "AI": {"tldr": "NeuralGCM\uff0c\u4e00\u79cd\u6df7\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u7269\u7406\u7684\u5927\u6c14\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u5b63\u8282\u6027\u9884\u6d4b\u5927\u89c4\u6a21\u5927\u6c14\u53d8\u5316\u548c\u5317\u534a\u7403\u70ed\u5e26\u6c14\u65cb\u6d3b\u52a8\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6c14\u5019\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u5173\u6ce8\u70ed\u5e26\u6c14\u65cb\u6d3b\u52a8\u7684\u5b63\u8282\u6027\u9884\u6d4b\u3002", "method": "\u4f7f\u7528NeuralGCM\u6a21\u578b\uff0c\u7b80\u5316\u8fb9\u754c\u6761\u4ef6\uff0c\u5047\u8bbe\u6d77\u9762\u6e29\u5ea6\u548c\u6d77\u51b0\u9075\u5faa\u6c14\u5019\u5b66\u5faa\u73af\u4f46\u4fdd\u6301\u521d\u59cb\u5316\u65f6\u7684\u5f02\u5e38\u3002", "result": "\u6a21\u578b\u80fd\u6a21\u62df\u771f\u5b9e\u7684\u5927\u6c14\u73af\u6d41\u548c\u70ed\u5e26\u6c14\u65cb\u6c14\u5019\u6a21\u5f0f\uff0c\u9884\u6d4b\u6280\u80fd\u5728\u5317\u5927\u897f\u6d0b\u548c\u4e1c\u592a\u5e73\u6d0b\u76c6\u5730\u7684\u70ed\u5e26\u6c14\u65cb\u9891\u7387\u4e0a\u4e0e\u4f20\u7edf\u7269\u7406\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u7ed3\u5408\u7269\u7406\u6d1e\u5bdf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u70ed\u5e26\u6c14\u65cb\u98ce\u9669\u5efa\u6a21\u548c\u65e0\u7f1d\u5929\u6c14-\u6c14\u5019\u9884\u6d4b\u65b9\u9762\u5177\u6709\u524d\u666f\u3002"}}
{"id": "2505.01460", "pdf": "https://arxiv.org/pdf/2505.01460", "abs": "https://arxiv.org/abs/2505.01460", "authors": ["Denis Parfenov", "Anton Parfenov"], "title": "Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Due to the increasing number of tasks that are solved on remote servers,\nidentifying and classifying traffic is an important task to reduce the load on\nthe server. There are various methods for classifying traffic. This paper\ndiscusses machine learning models for solving this problem. However, such ML\nmodels are also subject to attacks that affect the classification result of\nnetwork traffic. To protect models, we proposed a solution based on an\nautoencoder", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4fdd\u62a4\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u514d\u53d7\u653b\u51fb\uff0c\u4ee5\u51cf\u8f7b\u670d\u52a1\u5668\u8d1f\u8f7d\u3002", "motivation": "\u968f\u7740\u8fdc\u7a0b\u670d\u52a1\u5668\u5904\u7406\u4efb\u52a1\u589e\u591a\uff0c\u8bc6\u522b\u548c\u5206\u7c7b\u6d41\u91cf\u6210\u4e3a\u51cf\u8f7b\u670d\u52a1\u5668\u8d1f\u8f7d\u7684\u91cd\u8981\u4efb\u52a1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6a21\u578b\u6613\u53d7\u653b\u51fb\uff0c\u5f71\u54cd\u5206\u7c7b\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u4e86\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6297\u653b\u51fb\u80fd\u529b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u81ea\u7f16\u7801\u5668\u89e3\u51b3\u65b9\u6848\u6709\u6548\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4f7f\u5176\u5728\u653b\u51fb\u4e0b\u4ecd\u80fd\u4fdd\u6301\u51c6\u786e\u7684\u6d41\u91cf\u5206\u7c7b\u3002", "conclusion": "\u81ea\u7f16\u7801\u5668\u4e3a\u4fdd\u62a4\u6d41\u91cf\u5206\u7c7b\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u670d\u52a1\u5668\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2505.02048", "pdf": "https://arxiv.org/pdf/2505.02048", "abs": "https://arxiv.org/abs/2505.02048", "authors": ["Sebastian Rassmann", "David K\u00fcgler", "Christian Ewert", "Martin Reuter"], "title": "Regression s all you need for medical image translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.", "AI": {"tldr": "YODA\u662f\u4e00\u4e2a\u57fa\u4e8e2.5D\u6269\u6563\u7684\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u4e0e\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff1b\u5176\u63d0\u51fa\u7684ExpA\u91c7\u6837\u6280\u672f\u51cf\u5c11\u566a\u58f0\u5e72\u6270\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660eYODA\u4f18\u4e8e\u73b0\u6709GAN\u548cDM\u65b9\u6cd5\uff0c\u90e8\u5206\u4efb\u52a1\u4e2d\u751a\u81f3\u8d85\u8d8a\u771f\u5b9e\u91c7\u96c6\u56fe\u50cf\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\uff08MIT\uff09\u9700\u9ad8\u8d28\u91cf\u751f\u6210\u56fe\u50cf\uff0c\u4f46\u73b0\u6709GAN\u548cDM\u65b9\u6cd5\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u53ef\u80fd\u56e0\u566a\u58f0\u6216\u5185\u5bb9\u865a\u6784\u800c\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u786e\u4fdd\u89e3\u5256\u51c6\u786e\u6027\u3002", "method": "YODA\u6846\u67b6\u7ed3\u5408\u6269\u6563\u4e0e\u56de\u5f52\u8303\u5f0f\uff0c\u5e76\u5f15\u5165ExpA\u91c7\u6837\u6280\u672f\u6291\u5236\u751f\u6210\u566a\u58f0\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cYODA\u4f18\u4e8eGAN\u548cDM\u65b9\u6cd5\uff0c\u90e8\u5206\u4e0b\u6e38\u4efb\u52a1\u4e2d\u751f\u6210\u56fe\u50cf\u4f18\u4e8e\u771f\u5b9e\u91c7\u96c6\u3002", "conclusion": "YODA\u6311\u6218\u4e86DM\u7684\u9ed8\u8ba4\u4f18\u52bf\uff0c\u4e3aMIT\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2505.01463", "pdf": "https://arxiv.org/pdf/2505.01463", "abs": "https://arxiv.org/abs/2505.01463", "authors": ["Sabbir M. Saleh", "Nazim Madhavji", "John Steinbacher"], "title": "Enhancing the Cloud Security through Topic Modelling", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": "6 pages, 5 figures, 28th ACIS International Winter Conference on\n  Software Engineering, Artificial Intelligence, Networking and\n  Parallel/Distributed Computing (SNPD 2024-Winter)", "summary": "Protecting cloud applications is crucial in an age where security constantly\nthreatens the digital world. The inevitable cyber-attacks throughout the CI/CD\npipeline make cloud security innovations necessary. This research is motivated\nby applying Natural Language Processing (NLP) methodologies, such as Topic\nModelling, to analyse cloud security data and predict future attacks. This\nresearch aims to use topic modelling, specifically Latent Dirichlet Allocation\n(LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and\nPLSA, security-related text data, such as reports, logs, and other relevant\ndocuments, will be analysed and sorted into relevant topics (such as phishing\nor encryption). These algorithms may apply through Python using the Gensim\nframework. The topics shall be utilised to detect vulnerabilities within\nrelevant CI/CD pipeline records or log data. This application of Topic\nModelling anticipates providing a new form of vulnerability detection,\nimproving overall security throughout the CI/CD pipeline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528NLP\u65b9\u6cd5\uff08\u5982LDA\u548cpLSA\uff09\u5206\u6790\u4e91\u5b89\u5168\u6570\u636e\uff0c\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u9884\u6d4b\u672a\u6765\u653b\u51fb\uff0c\u65e8\u5728\u63d0\u5347CI/CD\u7ba1\u9053\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7NLP\u6280\u672f\u5206\u6790\u4e91\u5b89\u5168\u6570\u636e\uff0c\u63d0\u524d\u9884\u6d4b\u6f5c\u5728\u653b\u51fb\uff0c\u4f18\u5316CI/CD\u7ba1\u9053\u7684\u5b89\u5168\u9632\u62a4\u3002", "method": "\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff08LDA\u548cpLSA\uff09\uff0c\u901a\u8fc7Python\u7684Gensim\u6846\u67b6\u5206\u6790\u5b89\u5168\u76f8\u5173\u6587\u672c\u6570\u636e\uff08\u5982\u62a5\u544a\u3001\u65e5\u5fd7\uff09\uff0c\u5f52\u7c7b\u4e3a\u4e0d\u540c\u4e3b\u9898\uff08\u5982\u9493\u9c7c\u3001\u52a0\u5bc6\uff09\uff0c\u68c0\u6d4bCI/CD\u7ba1\u9053\u4e2d\u7684\u6f0f\u6d1e\u3002", "result": "\u4e3b\u9898\u5efa\u6a21\u6280\u672f\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3aCI/CD\u7ba1\u9053\u63d0\u4f9b\u65b0\u7684\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0cNLP\u9a71\u52a8\u7684\u4e3b\u9898\u5efa\u6a21\u53ef\u663e\u8457\u63d0\u5347\u4e91\u5b89\u5168\u9632\u62a4\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728CI/CD\u7ba1\u9053\u4e2d\u7684\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u3002"}}
{"id": "2505.01481", "pdf": "https://arxiv.org/pdf/2505.01481", "abs": "https://arxiv.org/abs/2505.01481", "authors": ["Zongxia Li", "Xiyang Wu", "Yubin Qin", "Guangyao Shi", "Hongyang Du", "Dinesh Manocha", "Tianyi Zhou", "Jordan Lee Boyd-Graber"], "title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthetic video generation with foundation models has gained attention for\nits realism and wide applications. While these models produce high-quality\nframes, they often fail to respect common sense and physical laws, resulting in\nabnormal content. Existing metrics like VideoScore emphasize general quality\nbut ignore such violations and lack interpretability. A more insightful\napproach is using multi-modal large language models (MLLMs) as interpretable\nevaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities\nin synthetic videos remains underexplored. To address this, we introduce\nVideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,\nand Kling, paired with expert-designed QA tasks solvable via human-level\nreasoning across various categories. We assess several SoTA MLLMs, including\nGPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and\nVideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,\nthese models still hallucinate on basic commonsense and physics tasks in\nsynthetic settings, underscoring the challenge of hallucination. We further\nfine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real\nand synthetic commonsense/physics data. Results show notable accuracy gains,\nespecially with counterexample integration, advancing MLLMs' reasoning\ncapabilities. Our data is available at https://github.com/zli12321/VideoHallu.", "AI": {"tldr": "VideoHallu\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5408\u6210\u89c6\u9891\u4e2d\u5f02\u5e38\u5185\u5bb9\u68c0\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684QA\u4efb\u52a1\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u5b83\u4eec\u5728\u5e38\u8bc6\u548c\u7269\u7406\u4efb\u52a1\u4e0a\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u6700\u7ec8\u901a\u8fc7GRPO\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u5e27\uff0c\u4f46\u5e38\u8fdd\u53cd\u5e38\u8bc6\u548c\u7269\u7406\u89c4\u5f8b\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff08\u5982VideoScore\uff09\u7f3a\u4e4f\u5bf9\u6b64\u7c7b\u95ee\u9898\u7684\u5173\u6ce8\u548c\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u89c2\u7684\u65b9\u6cd5\u8bc4\u4f30\u8fd9\u4e9b\u5f02\u5e38\u3002", "method": "\u5f15\u5165VideoHallu\u57fa\u51c6\uff0c\u5305\u542bVeo2\u3001Sora\u7b49\u6a21\u578b\u7684\u5408\u6210\u89c6\u9891\u53ca\u4e13\u5bb6\u8bbe\u8ba1\u7684QA\u4efb\u52a1\uff0c\u8bc4\u4f30GPT-4o\u3001Gemini-2.5-Pro\u7b49MLLMs\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7GRPO\u5728\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5148\u8fdb\u7684MLLMs\u5728\u5408\u6210\u89c6\u9891\u7684\u5e38\u8bc6\u548c\u7269\u7406\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46GRPO\u5fae\u8c03\uff08\u5c24\u5176\u662f\u7ed3\u5408\u53cd\u4f8b\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7a81\u51fa\u4e86\u5408\u6210\u89c6\u9891\u4e2d\u5e7b\u89c9\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7GRPO\u5fae\u8c03\u548c\u53cd\u4f8b\u6570\u636e\u9a8c\u8bc1\u4e86\u63d0\u5347MLLMs\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u548c\u65b9\u5411\u3002"}}
{"id": "2505.01484", "pdf": "https://arxiv.org/pdf/2505.01484", "abs": "https://arxiv.org/abs/2505.01484", "authors": ["Pedro Abdalla", "Roman Vershynin"], "title": "LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Given a text, can we determine whether it was generated by a large language\nmodel (LLM) or by a human? A widely studied approach to this problem is\nwatermarking. We propose an undetectable and elementary watermarking scheme in\nthe closed setting. Also, in the harder open setting, where the adversary has\naccess to most of the model, we propose an unremovable watermarking scheme.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e24\u79cd\u6c34\u5370\u65b9\u6848\uff1a\u5c01\u95ed\u73af\u5883\u4e0b\u4e0d\u53ef\u68c0\u6d4b\u7684\u7b80\u5355\u6c34\u5370\u548c\u5f00\u653e\u73af\u5883\u4e0b\u4e0d\u53ef\u79fb\u9664\u7684\u6c34\u5370\uff0c\u7528\u4e8e\u533a\u5206\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u6587\u672c\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6709\u6548\u533a\u5206\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\uff0c\u5c24\u5176\u5728\u5f00\u653e\u73af\u5883\u4e0b\u5bf9\u6297\u6027\u66f4\u5f3a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6c34\u5370\u65b9\u6848\uff1a\u5c01\u95ed\u73af\u5883\u4e0b\u7684\u4e0d\u53ef\u68c0\u6d4b\u6c34\u5370\u548c\u5f00\u653e\u73af\u5883\u4e0b\u7684\u4e0d\u53ef\u79fb\u9664\u6c34\u5370\u3002", "result": "\u65b9\u6848\u5728\u5c01\u95ed\u548c\u5f00\u653e\u73af\u5883\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86\u6c34\u5370\u7684\u4e0d\u53ef\u68c0\u6d4b\u6027\u548c\u4e0d\u53ef\u79fb\u9664\u6027\u3002", "conclusion": "\u901a\u8fc7\u6c34\u5370\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.02075", "pdf": "https://arxiv.org/pdf/2505.02075", "abs": "https://arxiv.org/abs/2505.02075", "authors": ["Volodymyr Havrylov", "Haiwen Huang", "Dan Zhang", "Andreas Geiger"], "title": "Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Foundation Models (VFMs) are large-scale, pre-trained models that\nserve as general-purpose backbones for various computer vision tasks. As VFMs'\npopularity grows, there is an increasing interest in understanding their\neffectiveness for dense prediction tasks. However, VFMs typically produce\nlow-resolution features, limiting their direct applicability in this context.\nOne way to tackle this limitation is by employing a task-agnostic feature\nupsampling module that refines VFM features resolution. To assess the\neffectiveness of this approach, we investigate Interactive Segmentation (IS) as\na novel benchmark for evaluating feature upsampling methods on VFMs. Due to its\ninherent multimodal input, consisting of an image and a set of user-defined\nclicks, as well as its dense mask output, IS creates a challenging environment\nthat demands comprehensive visual scene understanding. Our benchmarking\nexperiments show that selecting appropriate upsampling strategies significantly\nimproves VFM features quality. The code is released at\nhttps://github.com/havrylovv/iSegProbe", "AI": {"tldr": "\u7814\u7a76\u4e86\u4ea4\u4e92\u5f0f\u5206\u5272\uff08IS\uff09\u4f5c\u4e3a\u8bc4\u4f30VFMs\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\u7684\u65b0\u57fa\u51c6\uff0c\u7ed3\u679c\u8868\u660e\u9009\u62e9\u5408\u9002\u7684\u7b56\u7565\u663e\u8457\u63d0\u5347\u7279\u5f81\u8d28\u91cf\u3002", "motivation": "VFMs\u751f\u6210\u7684\u7279\u5f81\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\uff0c\u56e0\u6b64\u63a2\u7d22\u4efb\u52a1\u65e0\u5173\u7684\u7279\u5f81\u4e0a\u91c7\u6837\u6a21\u5757\u4ee5\u63d0\u5347\u7279\u5f81\u5206\u8fa8\u7387\u3002", "method": "\u91c7\u7528\u4ea4\u4e92\u5f0f\u5206\u5272\uff08IS\uff09\u4f5c\u4e3a\u57fa\u51c6\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u7528\u6237\u70b9\u51fb\u7684\u591a\u6a21\u6001\u8f93\u5165\uff0c\u8bc4\u4f30\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\u5bf9VFMs\u6548\u679c\u7684\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u9002\u5f53\u7684\u4e0a\u91c7\u6837\u7b56\u7565\u80fd\u663e\u8457\u63d0\u9ad8VFMs\u7279\u5f81\u7684\u8d28\u91cf\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u5206\u5272\u662f\u8bc4\u4f30VFMs\u7279\u5f81\u4e0a\u91c7\u6837\u7684\u6709\u6548\u57fa\u51c6\uff0c\u4f18\u5316\u4e0a\u91c7\u6837\u7b56\u7565\u5bf9\u63d0\u5347\u7279\u5f81\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.02077", "pdf": "https://arxiv.org/pdf/2505.02077", "abs": "https://arxiv.org/abs/2505.02077", "authors": ["Christian Schroeder de Witt"], "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": null, "summary": "Decentralized AI agents will soon interact across internet platforms,\ncreating security challenges beyond traditional cybersecurity and AI safety\nframeworks. Free-form protocols are essential for AI's task generalization but\nenable new threats like secret collusion and coordinated swarm attacks. Network\neffects can rapidly spread privacy breaches, disinformation, jailbreaks, and\ndata poisoning, while multi-agent dispersion and stealth optimization help\nadversaries evade oversightcreating novel persistent threats at a systemic\nlevel. Despite their critical importance, these security challenges remain\nunderstudied, with research fragmented across disparate fields including AI\nsecurity, multi-agent learning, complex systems, cybersecurity, game theory,\ndistributed systems, and technical AI governance. We introduce\n\\textbf{multi-agent security}, a new field dedicated to securing networks of\ndecentralized AI agents against threats that emerge or amplify through their\ninteractionswhether direct or indirect via shared environmentswith each other,\nhumans, and institutions, and characterize fundamental security-performance\ntrade-offs. Our preliminary work (1) taxonomizes the threat landscape arising\nfrom interacting AI agents, (2) surveys security-performance tradeoffs in\ndecentralized AI systems, and (3) proposes a unified research agenda addressing\nopen challenges in designing secure agent systems and interaction environments.\nBy identifying these gaps, we aim to guide research in this critical area to\nunlock the socioeconomic potential of large-scale agent deployment on the\ninternet, foster public trust, and mitigate national security risks in critical\ninfrastructure and defense contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\u5728\u4e92\u8054\u7f51\u5e73\u53f0\u4ea4\u4e92\u65f6\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u591a\u4ee3\u7406\u5b89\u5168\u8fd9\u4e00\u65b0\u9886\u57df\uff0c\u5e76\u521d\u6b65\u5206\u7c7b\u4e86\u5a01\u80c1\u3001\u7814\u7a76\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7814\u7a76\u8bae\u7a0b\u3002", "motivation": "\u968f\u7740\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\u5728\u4e92\u8054\u7f51\u4e0a\u7684\u4ea4\u4e92\u589e\u52a0\uff0c\u4f20\u7edf\u5b89\u5168\u6846\u67b6\u65e0\u6cd5\u5e94\u5bf9\u5176\u5e26\u6765\u7684\u65b0\u578b\u5a01\u80c1\uff08\u5982\u79d8\u5bc6\u5408\u8c0b\u3001\u7fa4\u4f53\u653b\u51fb\u7b49\uff09\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u786e\u4fdd\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u7c7b\u5a01\u80c1\u3001\u8c03\u7814\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u7edf\u4e00\u7814\u7a76\u8bae\u7a0b\uff0c\u521d\u6b65\u6784\u5efa\u4e86\u591a\u4ee3\u7406\u5b89\u5168\u9886\u57df\u7684\u6846\u67b6\u3002", "result": "\u8bba\u6587\u521d\u6b65\u63d0\u51fa\u4e86\u591a\u4ee3\u7406\u5b89\u5168\u9886\u57df\uff0c\u5e76\u660e\u786e\u4e86\u5a01\u80c1\u5206\u7c7b\u3001\u5b89\u5168\u6027\u80fd\u6743\u8861\u7814\u7a76\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u53bb\u4e2d\u5fc3\u5316AI\u4ee3\u7406\u5b89\u5168\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u5176\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u5b89\u5168\u6027\u548c\u516c\u4f17\u4fe1\u4efb\uff0c\u540c\u65f6\u964d\u4f4e\u56fd\u5bb6\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2505.01538", "pdf": "https://arxiv.org/pdf/2505.01538", "abs": "https://arxiv.org/abs/2505.01538", "authors": ["Hongbin Zhong", "Matthew Lentz", "Nina Narodytska", "Adriana Szekeres", "Kexin Rong"], "title": "HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning", "categories": ["cs.DB", "cs.CR", "cs.IR", "cs.LG", "H.2.4; H.3.3; D.4.6"], "comment": null, "summary": "As vector databases gain traction in enterprise applications, robust access\ncontrol has become critical to safeguard sensitive data. Access control in\nthese systems is often implemented through hybrid vector queries, which combine\nnearest neighbor search on vector data with relational predicates based on user\npermissions. However, existing approaches face significant trade-offs: creating\ndedicated indexes for each user minimizes query latency but introduces\nexcessive storage redundancy, while building a single index and applying access\ncontrol after vector search reduces storage overhead but suffers from poor\nrecall and increased query latency. This paper introduces HoneyBee, a dynamic\npartitioning framework that bridges the gap between these approaches by\nleveraging the structure of Role-Based Access Control (RBAC) policies. RBAC,\nwidely adopted in enterprise settings, groups users into roles and assigns\npermissions to those roles, creating a natural \"thin waist\" in the permission\nstructure that is ideal for partitioning decisions. Specifically, HoneyBee\nproduces overlapping partitions where vectors can be strategically replicated\nacross different partitions to reduce query latency while controlling storage\noverhead. By introducing analytical models for the performance and recall of\nthe vector search, HoneyBee formulates the partitioning strategy as a\nconstrained optimization problem to dynamically balance storage, query\nefficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee\nreduces storage redundancy compared to role partitioning and achieves up to 6x\nfaster query speeds than row-level security (RLS) with only 1.4x storage\nincrease, offering a practical middle ground for secure and efficient vector\nsearch.", "AI": {"tldr": "HoneyBee\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5206\u533a\u6846\u67b6\uff0c\u5229\u7528RBAC\u7b56\u7565\u7684\u7ed3\u6784\u4f18\u5316\u5411\u91cf\u6570\u636e\u5e93\u7684\u8bbf\u95ee\u63a7\u5236\uff0c\u5e73\u8861\u4e86\u5b58\u50a8\u5197\u4f59\u548c\u67e5\u8be2\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u7684\u8bbf\u95ee\u63a7\u5236\u65b9\u6cd5\u5728\u67e5\u8be2\u5ef6\u8fdf\u548c\u5b58\u50a8\u5197\u4f59\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5206\u533a\u7b56\u7565\u3002", "method": "HoneyBee\u901a\u8fc7RBAC\u7b56\u7565\u52a8\u6001\u751f\u6210\u91cd\u53e0\u5206\u533a\uff0c\u7ed3\u5408\u5206\u6790\u6a21\u578b\u4f18\u5316\u5b58\u50a8\u548c\u67e5\u8be2\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cHoneyBee\u76f8\u6bd4\u89d2\u8272\u5206\u533a\u51cf\u5c11\u5b58\u50a8\u5197\u4f59\uff0c\u67e5\u8be2\u901f\u5ea6\u6bd4\u884c\u7ea7\u5b89\u5168\u5feb6\u500d\u3002", "conclusion": "HoneyBee\u4e3a\u5b89\u5168\u9ad8\u6548\u7684\u5411\u91cf\u641c\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02120", "pdf": "https://arxiv.org/pdf/2505.02120", "abs": "https://arxiv.org/abs/2505.02120", "authors": ["Xiao Zhou", "Zhongxiang Zhao", "Hanze Guo"], "title": "Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Online platforms aggregate extensive user feedback across diverse behaviors,\nproviding a rich source for enhancing user engagement. Traditional recommender\nsystems, however, typically optimize for a single target behavior and represent\nuser preferences with a single vector, limiting their ability to handle\nmultiple important behaviors or optimization objectives. This conventional\napproach also struggles to capture the full spectrum of user interests,\nresulting in a narrow item pool during candidate generation. To address these\nlimitations, we present Tricolore, a versatile multi-vector learning framework\nthat uncovers connections between different behavior types for more robust\ncandidate generation. Tricolore's adaptive multi-task structure is also\ncustomizable to specific platform needs. To manage the variability in sparsity\nacross behavior types, we incorporate a behavior-wise multi-view fusion module\nthat dynamically enhances learning. Moreover, a popularity-balanced strategy\nensures the recommendation list balances accuracy with item popularity,\nfostering diversity and improving overall performance. Extensive experiments on\npublic datasets demonstrate Tricolore's effectiveness across various\nrecommendation scenarios, from short video platforms to e-commerce. By\nleveraging a shared base embedding strategy, Tricolore also significantly\nimproves the performance for cold-start users. The source code is publicly\navailable at: https://github.com/abnering/Tricolore.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTricolore\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5411\u91cf\u5b66\u4e60\u548c\u884c\u4e3a\u7c7b\u578b\u8fde\u63a5\u4f18\u5316\u63a8\u8350\u7cfb\u7edf\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5355\u76ee\u6807\u884c\u4e3a\u548c\u7528\u6237\u504f\u597d\u8868\u793a\u7684\u5c40\u9650\u6027\u3002\u91c7\u7528\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u7ed3\u6784\u3001\u884c\u4e3a\u7ea7\u591a\u89c6\u56fe\u878d\u5408\u6a21\u5757\u53ca\u6d41\u884c\u5ea6\u5e73\u8861\u7b56\u7565\uff0c\u63d0\u5347\u591a\u6837\u6027\u548c\u51b7\u542f\u52a8\u7528\u6237\u8868\u73b0\u3002\u516c\u5f00\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u53ea\u4f18\u5316\u5355\u4e00\u884c\u4e3a\u76ee\u6807\u5e76\u4f7f\u7528\u5355\u4e00\u5411\u91cf\u8868\u793a\u7528\u6237\u504f\u597d\uff0c\u96be\u4ee5\u5904\u7406\u591a\u884c\u4e3a\u7c7b\u578b\u6216\u76ee\u6807\uff0c\u4e14\u65e0\u6cd5\u5168\u9762\u6355\u6349\u7528\u6237\u5174\u8da3\uff0c\u5bfc\u81f4\u5019\u9009\u6c60\u72ed\u7a84\u3002", "method": "\u63d0\u51faTricolore\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u591a\u5411\u91cf\u5b66\u4e60\u6316\u6398\u884c\u4e3a\u95f4\u8054\u7cfb\uff1b2) \u81ea\u9002\u5e94\u591a\u4efb\u52a1\u7ed3\u6784\uff1b3) \u884c\u4e3a\u7ea7\u591a\u89c6\u56fe\u878d\u5408\u6a21\u5757\u52a8\u6001\u589e\u5f3a\u5b66\u4e60\uff1b4) \u6d41\u884c\u5ea6\u5e73\u8861\u7b56\u7565\u517c\u987e\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u3002", "result": "\u516c\u5f00\u6570\u636e\u96c6\u5b9e\u9a8c\u8868\u660e\uff0cTricolore\u5728\u77ed\u89c6\u9891\u3001\u7535\u5546\u7b49\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u5d4c\u5165\u7b56\u7565\u663e\u8457\u63d0\u5347\u51b7\u542f\u52a8\u7528\u6237\u6027\u80fd\u3002", "conclusion": "Tricolore\u901a\u8fc7\u591a\u884c\u4e3a\u5efa\u6a21\u548c\u52a8\u6001\u4f18\u5316\uff0c\u6709\u6548\u514b\u670d\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5e73\u53f0\u63d0\u4f9b\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u63a8\u8350\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02129", "pdf": "https://arxiv.org/pdf/2505.02129", "abs": "https://arxiv.org/abs/2505.02129", "authors": ["Xiaoping Sun", "Hai Zhuge"], "title": "Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Mode", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Organizing resources in a multidimensional classification space is an\napproach to efficiently managing and querying large-scale resources. This paper\ndefines an aggregation query on subspace defined by a range on the partial\norder on coordinate tree at each dimension, where each point contains resources\naggregated along the paths of partial order relations on the points so that\naggregated resources at each point within the subspace can be measured, ranked\nand selected. To efficiently locate non-empty points in a large subspace, an\napproach to generating graph index is proposed to build inclusion links with\npartial order relations on coordinates of dimensions to enable a subspace query\nto reach non-empty points by following indexing links and aggregate resources\nalong indexing paths back to their super points. Generating such an index is\ncostly as the number of children of an index node can be very large so that the\ntotal number of indexing nodes is unbounded. The proposed approach adopts the\nfollowing strategies to reduce the cost: (1) adding intersection links between\ntwo indexing nodes, which can better reduce query processing costs while\ncontrolling the number of nodes of the graph index; (2) intersection links are\nadded between two nodes according to the probabilistic distribution calculated\nfor estimating the costs of adding intersection between two nodes; (3)\ncoordinates at one dimension having more resources are split by coordinates at\nanother dimension to balance the number of resources hold by indexing nodes;\nand, (4) short-cut links are added between sibling coordinates of coordinate\ntrees to make an efficient query on linear order coordinates. Analysis and\nexperiments verified the effectiveness of the generated index in supporting\nsubspace aggregation query. This work makes significant contributions to the\ndevelopment of data model based on multi-dimensional classification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ef4\u5ea6\u5206\u7c7b\u7a7a\u95f4\u4e2d\u7684\u8d44\u6e90\u7ec4\u7ec7\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u67e5\u8be2\u548c\u56fe\u5f62\u7d22\u5f15\u4f18\u5316\u5927\u89c4\u6a21\u8d44\u6e90\u7ba1\u7406\uff0c\u91c7\u7528\u4e86\u591a\u79cd\u7b56\u7565\u964d\u4f4e\u7d22\u5f15\u6210\u672c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u9ad8\u6548\u7ba1\u7406\u548c\u67e5\u8be2\u5927\u89c4\u6a21\u591a\u7ef4\u5ea6\u5206\u7c7b\u8d44\u6e90\uff0c\u9700\u89e3\u51b3\u5927\u89c4\u6a21\u5b50\u7a7a\u95f4\u4e2d\u975e\u7a7a\u70b9\u7684\u5b9a\u4f4d\u53ca\u8d44\u6e90\u805a\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u56fe\u5f62\u7d22\u5f15\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5305\u542b\u94fe\u63a5\u548c\u90e8\u5206\u987a\u5e8f\u5173\u7cfb\u4f18\u5316\u67e5\u8be2\u8def\u5f84\uff0c\u5e76\u91c7\u7528\u56db\u79cd\u7b56\u7565\uff08\u5982\u4ea4\u63d2\u94fe\u63a5\u3001\u6982\u7387\u5206\u5e03\u8ba1\u7b97\u7b49\uff09\u964d\u4f4e\u7d22\u5f15\u6210\u672c\u3002", "result": "\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u751f\u6210\u7684\u7d22\u5f15\u5728\u652f\u6301\u5b50\u7a7a\u95f4\u805a\u5408\u67e5\u8be2\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u591a\u7ef4\u5ea6\u5206\u7c7b\u7684\u6570\u636e\u6a21\u578b\u53d1\u5c55\u505a\u51fa\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2505.02139", "pdf": "https://arxiv.org/pdf/2505.02139", "abs": "https://arxiv.org/abs/2505.02139", "authors": ["Muyao Zhong", "Yushi Lin", "Peng Yang"], "title": "Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "The Limit Order Book (LOB), the mostly fundamental data of the financial\nmarket, provides a fine-grained view of market dynamics while poses significant\nchallenges in dealing with the esteemed deep models due to its strong\nautocorrelation, cross-feature constrains, and feature scale disparity.\nExisting approaches often tightly couple representation learning with specific\ndownstream tasks in an end-to-end manner, failed to analyze the learned\nrepresentations individually and explicitly, limiting their reusability and\ngeneralization. This paper conducts the first systematic comparative study of\nLOB representation learning, aiming to identify the effective way of extracting\ntransferable, compact features that capture essential LOB properties. We\nintroduce LOBench, a standardized benchmark with real China A-share market\ndata, offering curated datasets, unified preprocessing, consistent evaluation\nmetrics, and strong baselines. Extensive experiments validate the sufficiency\nand necessity of LOB representations for various downstream tasks and highlight\ntheir advantages over both the traditional task-specific end-to-end models and\nthe advanced representation learning models for general time series. Our work\nestablishes a reproducible framework and provides clear guidelines for future\nresearch. Datasets and code will be publicly available at\nhttps://github.com/financial-simulation-lab/LOBench.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u9650\u4ef7\u8ba2\u5355\u7c3f\uff08LOB\uff09\u8868\u793a\u5b66\u4e60\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u6bd4\u8f83\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u53d6\u53ef\u8f6c\u79fb\u7d27\u51d1\u7279\u5f81\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86LOBench\u6807\u51c6\u5316\u57fa\u51c6\u3002", "motivation": "LOB\u6570\u636e\u5728\u91d1\u878d\u5e02\u573a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u7aef\u5230\u7aef\u4efb\u52a1\u4e2d\u7d27\u5bc6\u8026\u5408\u8868\u793a\u5b66\u4e60\uff0c\u7f3a\u4e4f\u5bf9\u5b66\u4e60\u8868\u793a\u7684\u5355\u72ec\u548c\u663e\u5f0f\u5206\u6790\uff0c\u9650\u5236\u4e86\u5176\u53ef\u91cd\u7528\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86LOBench\uff0c\u5305\u542b\u771f\u5b9e\u4e2d\u56fdA\u80a1\u5e02\u573a\u6570\u636e\uff0c\u63d0\u4f9b\u7edf\u4e00\u9884\u5904\u7406\u3001\u4e00\u81f4\u8bc4\u4f30\u6307\u6807\u548c\u5f3a\u57fa\u7ebf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LOB\u8868\u793a\u5bf9\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u5145\u5206\u6027\u548c\u5fc5\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u4efb\u52a1\u7279\u5b9a\u7aef\u5230\u7aef\u6a21\u578b\u548c\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u6a21\u578b\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u6307\u5357\u3002"}}
{"id": "2505.01616", "pdf": "https://arxiv.org/pdf/2505.01616", "abs": "https://arxiv.org/abs/2505.01616", "authors": ["Jianxing Qin", "Jingrong Chen", "Xinhao Kong", "Yongji Wu", "Liang Luo", "Zhaodong Wang", "Ying Zhang", "Tingjun Chen", "Alvin R. Lebeck", "Danyang Zhuo"], "title": "Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation", "categories": ["cs.DC", "cs.LG", "cs.PF"], "comment": null, "summary": "To accommodate ever-increasing model complexity, modern machine learning (ML)\nsystems have to scale to large GPU clusters. Changes in ML model architecture,\nML system implementation, and cluster configuration can significantly affect\noverall ML system performance. However, quantifying the performance impact\nbefore deployment is challenging. Existing performance estimation methods use\nperformance modeling or static workload simulation. These techniques are not\ngeneral: they requires significant human effort and computation capacity to\ngenerate training data or a workload. It is also difficult to adapt ML systems\nto use these techniques. This paper introduces, Phantora, a live GPU cluster\nsimulator for performance estimation. Phantora runs minimally modified ML\nmodels and frameworks, intercepting and simulating GPU-related operations to\nenable high-fidelity performance estimation. Phantora overcomes several\nresearch challenges in integrating an event-driven network simulator with live\nsystem execution, and introduces a set of techniques to improve simulation\nspeed, scalability, and accuracy. Our evaluation results show that Phantora can\ndeliver similar estimation accuracy to the state-of-the-art workload simulation\napproach with only one GPU, while reducing human effort and increasing\ngeneralizability.", "AI": {"tldr": "Phantora\u662f\u4e00\u79cd\u5b9e\u65f6GPU\u96c6\u7fa4\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6027\u80fd\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u5927\u91cf\u4eba\u5de5\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u8981\u6269\u5c55\u5230\u5927\u578bGPU\u96c6\u7fa4\uff0c\u4f46\u6a21\u578b\u67b6\u6784\u3001\u7cfb\u7edf\u5b9e\u73b0\u548c\u96c6\u7fa4\u914d\u7f6e\u7684\u53d8\u5316\u96be\u4ee5\u5728\u90e8\u7f72\u524d\u91cf\u5316\u6027\u80fd\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u7528\u6027\u5dee\u4e14\u9700\u5927\u91cf\u8d44\u6e90\u3002", "method": "Phantora\u901a\u8fc7\u62e6\u622a\u548c\u6a21\u62dfGPU\u76f8\u5173\u64cd\u4f5c\uff0c\u7ed3\u5408\u4e8b\u4ef6\u9a71\u52a8\u7f51\u7edc\u6a21\u62df\u5668\u4e0e\u5b9e\u65f6\u7cfb\u7edf\u6267\u884c\uff0c\u63d0\u5347\u6a21\u62df\u901f\u5ea6\u3001\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cPhantora\u4ec5\u9700\u4e00\u5757GPU\u5373\u53ef\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u65b9\u6cd5\u76f8\u4f3c\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u51cf\u5c11\u4eba\u5de5\u9700\u6c42\u5e76\u63d0\u9ad8\u901a\u7528\u6027\u3002", "conclusion": "Phantora\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u6027\u80fd\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\u548c\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2505.02154", "pdf": "https://arxiv.org/pdf/2505.02154", "abs": "https://arxiv.org/abs/2505.02154", "authors": ["Oliver Savolainen", "Dur e Najaf Amjad", "Roxana Petcu"], "title": "Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions", "categories": ["cs.IR", "cs.AI"], "comment": "10 pages, SIGIR 2025", "summary": "This reproducibility study analyzes and extends the paper \"Axiomatic Causal\nInterventions for Reverse Engineering Relevance Computation in Neural Retrieval\nModels,\" which investigates how neural retrieval models encode task-relevant\nproperties such as term frequency. We reproduce key experiments from the\noriginal paper, confirming that information on query terms is captured in the\nmodel encoding. We extend this work by applying activation patching to Spanish\nand Chinese datasets and by exploring whether document-length information is\nencoded in the model as well. Our results confirm that the designed activation\npatching method can isolate the behavior to specific components and tokens in\nneural retrieval models. Moreover, our findings indicate that the location of\nterm frequency generalizes across languages and that in later layers, the\ninformation for sequence-level tasks is represented in the CLS token. The\nresults highlight the need for further research into interpretability in\ninformation retrieval and reproducibility in machine learning research. Our\ncode is available at\nhttps://github.com/OliverSavolainen/axiomatic-ir-reproduce.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u590d\u5236\u548c\u6269\u5c55\u539f\u59cb\u8bba\u6587\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u68c0\u7d22\u6a21\u578b\u4e2d\u67e5\u8be2\u8bcd\u4fe1\u606f\u7684\u7f16\u7801\u65b9\u5f0f\uff0c\u5e76\u63a2\u7d22\u4e86\u8de8\u8bed\u8a00\u548c\u6587\u6863\u957f\u5ea6\u4fe1\u606f\u7684\u7f16\u7801\u60c5\u51b5\uff0c\u7ed3\u679c\u8868\u660e\u6fc0\u6d3b\u4fee\u8865\u65b9\u6cd5\u80fd\u6709\u6548\u5b9a\u4f4d\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u68c0\u7d22\u6a21\u578b\u5982\u4f55\u7f16\u7801\u4efb\u52a1\u76f8\u5173\u5c5e\u6027\uff08\u5982\u8bcd\u9891\uff09\uff0c\u5e76\u9a8c\u8bc1\u8fd9\u4e9b\u7f16\u7801\u662f\u5426\u5177\u6709\u8de8\u8bed\u8a00\u901a\u7528\u6027\uff0c\u540c\u65f6\u63a2\u7d22\u6587\u6863\u957f\u5ea6\u4fe1\u606f\u662f\u5426\u4e5f\u88ab\u7f16\u7801\u3002", "method": "\u590d\u5236\u539f\u59cb\u8bba\u6587\u7684\u5173\u952e\u5b9e\u9a8c\uff0c\u901a\u8fc7\u6fc0\u6d3b\u4fee\u8865\u65b9\u6cd5\u5206\u6790\u897f\u73ed\u7259\u8bed\u548c\u4e2d\u6587\u6570\u636e\u96c6\uff0c\u5e76\u68c0\u67e5\u6587\u6863\u957f\u5ea6\u4fe1\u606f\u7684\u7f16\u7801\u4f4d\u7f6e\u3002", "result": "\u786e\u8ba4\u6fc0\u6d3b\u4fee\u8865\u65b9\u6cd5\u80fd\u5b9a\u4f4d\u6a21\u578b\u7279\u5b9a\u7ec4\u4ef6\u548c\u6807\u8bb0\u7684\u884c\u4e3a\uff1b\u53d1\u73b0\u8bcd\u9891\u4fe1\u606f\u7684\u8de8\u8bed\u8a00\u901a\u7528\u6027\uff0c\u4e14\u5e8f\u5217\u7ea7\u4efb\u52a1\u4fe1\u606f\u96c6\u4e2d\u5728CLS\u6807\u8bb0\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4fe1\u606f\u68c0\u7d22\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u53ef\u91cd\u590d\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.01637", "pdf": "https://arxiv.org/pdf/2505.01637", "abs": "https://arxiv.org/abs/2505.01637", "authors": ["Samuel J. Kaufman", "Ren\u00e9 Just", "Rastislav Bodik"], "title": "Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression", "categories": ["cs.PL", "cs.LG", "D.1.2"], "comment": "13 pages, 2 figures", "summary": "High-throughput neural network inference requires coordinating many\noptimization decisions, including parallel tiling, microkernel selection, and\ndata layout. The product of these decisions forms a search space of programs\nwhich is typically intractably large. Existing approaches (e.g.,\nauto-schedulers) often address this problem by sampling this space\nheuristically. In contrast, we introduce a dynamic-programming-based approach\nto explore more of the search space by iteratively decomposing large program\nspecifications into smaller specifications reachable from a set of rewrites,\nthen composing a final program from each rewrite that minimizes an affine cost\nmodel. To reduce memory requirements, we employ a novel memoization table\nrepresentation, which indexes specifications by coordinates in $Z_{\\geq 0}$ and\ncompresses identical, adjacent solutions. This approach can visit a much larger\nset of programs than prior work. To evaluate the approach, we developed\nMorello, a compiler which lowers specifications roughly equivalent to a\nfew-node XLA computation graph to x86. Notably, we found that an affine cost\nmodel is sufficient to surface high-throughput programs. For example, Morello\nsynthesized a collection of matrix multiplication benchmarks targeting a Zen 1\nCPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply,\nwhich was integrated into Google's gemma.cpp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5212\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u7ec4\u5408\u7a0b\u5e8f\u89c4\u683c\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u8bb0\u5fc6\u8868\u8868\u793a\uff0c\u663e\u8457\u6269\u5927\u4e86\u641c\u7d22\u7a7a\u95f4\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u81ea\u52a8\u8c03\u5ea6\u5668\uff09\u901a\u5e38\u901a\u8fc7\u542f\u53d1\u5f0f\u91c7\u6837\u641c\u7d22\u7a7a\u95f4\u6765\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u4f46\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\u4e14\u96be\u4ee5\u5904\u7406\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u63a2\u7d22\u641c\u7d22\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u7a0b\u5e8f\u89c4\u683c\u5206\u89e3\u4e3a\u53ef\u901a\u8fc7\u6539\u5199\u83b7\u5f97\u7684\u5c0f\u89c4\u683c\uff0c\u5e76\u901a\u8fc7\u4eff\u5c04\u6210\u672c\u6a21\u578b\u7ec4\u5408\u6700\u4f18\u7a0b\u5e8f\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8bb0\u5fc6\u8868\u8868\u793a\u6cd5\uff0c\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5f00\u53d1\u7684Morello\u7f16\u8bd1\u5668\u5728x86\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7a0b\u5e8f\u5408\u6210\uff0c\u4f8b\u5982\u5728Zen 1 CPU\u4e0a\u4f18\u5316\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u5df2\u96c6\u6210\u5230Google\u7684gemma.cpp\u4e2d\u3002", "conclusion": "\u4eff\u5c04\u6210\u672c\u6a21\u578b\u80fd\u6709\u6548\u7b5b\u9009\u9ad8\u6027\u80fd\u7a0b\u5e8f\uff0c\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86\u641c\u7d22\u7a7a\u95f4\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.02158", "pdf": "https://arxiv.org/pdf/2505.02158", "abs": "https://arxiv.org/abs/2505.02158", "authors": ["Ioannis Avgerinos", "Ioannis Mourtos", "Nikolaos Tsompanidis", "Georgios Zois"], "title": "Pickup & Delivery with Time Windows and Transfers: combining decomposition with metaheuristics", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "This paper examines the generalisation of the Pickup and Delivery Problem\nthat allows mid-route load exchanges among vehicles and obeys strict\ntime-windows at all locations. We propose a novel Logic-Based Benders\nDecomposition (LBBD) that improves optimality gaps for all benchmarks in the\nliterature and scales up to handle larger ones. To tackle even larger\ninstances, we introduce a refined Large Neighborhood Search (LNS) algorithm\nthat improves the adaptability of LNS beyond case-specific configurations\nappearing in related literature.\n  To bridge the gap in benchmark availability, we develop an instance generator\nthat allows for extensive experimentation. For moderate datasets (25 and 50\nrequests), we evaluate the performance of both LBBD and LNS, the former being\nable to close the gap and the latter capable of providing near-optimal\nsolutions. For larger instances (75 and 100 requests), we recreate indicative\nstate-of-the-art metaheuristics to highlight the improvements introduced by our\nLNS refinements, while establishing its scalability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5141\u8bb8\u8f66\u8f86\u4e2d\u9014\u4ea4\u6362\u8d27\u7269\u5e76\u9075\u5b88\u4e25\u683c\u65f6\u95f4\u7a97\u7684\u53d6\u8d27\u548c\u9001\u8d27\u95ee\u9898\u7684\u6cdb\u5316\u3002\u63d0\u51fa\u4e86\u65b0\u7684LBBD\u548cLNS\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u4f8b\u751f\u6210\u5668\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u53d6\u8d27\u548c\u9001\u8d27\u95ee\u9898\u65f6\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e2d\u9014\u8d27\u7269\u4ea4\u6362\u548c\u4e25\u683c\u65f6\u95f4\u7a97\u7684\u7ea6\u675f\uff0c\u4e14\u7f3a\u4e4f\u8db3\u591f\u7684\u57fa\u51c6\u5b9e\u4f8b\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u903b\u8f91Benders\u5206\u89e3\uff08LBBD\uff09\u548c\u6539\u8fdb\u7684\u5927\u90bb\u57df\u641c\u7d22\uff08LNS\uff09\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1\u5b9e\u4f8b\u751f\u6210\u5668\u4ee5\u5f25\u8865\u57fa\u51c6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "result": "\u5728\u4e2d\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\uff0825\u548c50\u8bf7\u6c42\uff09\u4e2d\uff0cLBBD\u80fd\u5b8c\u5168\u95ed\u5408\u6700\u4f18\u95f4\u9699\uff0cLNS\u63d0\u4f9b\u63a5\u8fd1\u6700\u4f18\u89e3\uff1b\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0875\u548c100\u8bf7\u6c42\uff09\u4e2d\uff0c\u6539\u8fdb\u7684LNS\u5c55\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u7684LBBD\u548cLNS\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53d6\u8d27\u548c\u9001\u8d27\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2505.01639", "pdf": "https://arxiv.org/pdf/2505.01639", "abs": "https://arxiv.org/abs/2505.01639", "authors": ["Nicolas Coloma", "William Kleiber"], "title": "Fast Likelihood-Free Parameter Estimation for L\u00e9vy Processes", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.CO"], "comment": null, "summary": "L\\'evy processes are widely used in financial modeling due to their ability\nto capture discontinuities and heavy tails, which are common in high-frequency\nasset return data. However, parameter estimation remains a challenge when\nassociated likelihoods are unavailable or costly to compute. We propose a fast\nand accurate method for L\\'evy parameter estimation using the neural Bayes\nestimation (NBE) framework -- a simulation-based, likelihood-free approach that\nleverages permutation-invariant neural networks to approximate Bayes\nestimators. Through extensive simulations across several L\\'evy models, we show\nthat NBE outperforms traditional methods in both accuracy and runtime, while\nalso enabling rapid bootstrap-based uncertainty quantification. We illustrate\nour approach on a challenging high-frequency cryptocurrency return dataset,\nwhere the method captures evolving parameter dynamics and delivers reliable and\ninterpretable inference at a fraction of the computational cost of traditional\nmethods. NBE provides a scalable and practical solution for inference in\ncomplex financial models, enabling parameter estimation and uncertainty\nquantification over an entire year of data in just seconds. We additionally\ninvestigate nearly a decade of high-frequency Bitcoin returns, requiring less\nthan one minute to estimate parameters under the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8d1d\u53f6\u65af\u4f30\u8ba1\uff08NBE\uff09\u7684\u5feb\u901f\u3001\u51c6\u786e\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1L\u00e9vy\u8fc7\u7a0b\u7684\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u9891\u91d1\u878d\u6570\u636e\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "L\u00e9vy\u8fc7\u7a0b\u5728\u91d1\u878d\u5efa\u6a21\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u53c2\u6570\u4f30\u8ba1\u5728\u4f3c\u7136\u51fd\u6570\u4e0d\u53ef\u5f97\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u65f6\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u8d1d\u53f6\u65af\u4f30\u8ba1\uff08NBE\uff09\u6846\u67b6\uff0c\u5229\u7528\u7f6e\u6362\u4e0d\u53d8\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u8d1d\u53f6\u65af\u4f30\u8ba1\u5668\uff0c\u65e0\u9700\u663e\u5f0f\u4f3c\u7136\u51fd\u6570\uff0c\u901a\u8fc7\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "NBE\u5728\u591a\u79cdL\u00e9vy\u6a21\u578b\u4e2d\u7684\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u7cbe\u5ea6\u548c\u8fd0\u884c\u65f6\u95f4\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u5feb\u901f\u5b8c\u6210\u57fa\u4e8eBootstrap\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u5728\u52a0\u5bc6\u8d27\u5e01\u9ad8\u9891\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "NBE\u4e3a\u590d\u6742\u91d1\u878d\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53c2\u6570\u4f30\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u9ad8\u9891\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2505.02170", "pdf": "https://arxiv.org/pdf/2505.02170", "abs": "https://arxiv.org/abs/2505.02170", "authors": ["Danial Ramezani"], "title": "Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach", "categories": ["cs.CE", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Fantasy football is a billion-dollar industry with millions of participants.\nConstrained by a fixed budget, decision-makers draft a squad whose players are\nexpected to perform well in the upcoming weeks to maximize total points. This\npaper proposes novel deterministic and robust integer programming models that\nselect the optimal starting eleven and the captain. A new hybrid scoring metric\nis constructed using an interpretable artificial intelligence framework and\nunderlying match performance data. Several objective functions and estimation\ntechniques are introduced for the programming model. To the best of my\nknowledge, this is the first study to approach fantasy football through this\nlens. The models' performance is evaluated using data from the 2023/24 Premier\nLeague season. Results indicate that the proposed hybrid method achieved the\nhighest score while maintaining consistent performance. Utilizing the Monte\nCarlo simulation, the strategic choice of averaging techniques for estimating\ncost vectors, and the proposed hybrid approach are shown to be effective during\nthe out-of-sample period. This paper also provides a thorough analysis of the\noptimal formations and players selected by the models, offering valuable\ninsights into effective fantasy football strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u548c\u9c81\u68d2\u6574\u6570\u89c4\u5212\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u68a6\u5e7b\u8db3\u7403\u9635\u5bb9\u9009\u62e9\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6846\u67b6\u548c\u6bd4\u8d5b\u6570\u636e\u6784\u5efa\u6df7\u5408\u8bc4\u5206\u6307\u6807\uff0c\u5e76\u5728\u82f1\u8d85\u8d5b\u5b63\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u68a6\u5e7b\u8db3\u7403\u662f\u4e00\u4e2a\u4ef7\u503c\u6570\u5341\u4ebf\u7f8e\u5143\u7684\u4ea7\u4e1a\uff0c\u53c2\u4e0e\u8005\u9700\u8981\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u9009\u62e9\u8868\u73b0\u4f18\u5f02\u7684\u7403\u5458\u4ee5\u6700\u5927\u5316\u5f97\u5206\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u4f18\u5316\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u79d1\u5b66\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u786e\u5b9a\u6027\u548c\u9c81\u68d2\u6574\u6570\u89c4\u5212\u6a21\u578b\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6846\u67b6\u6784\u5efa\u6df7\u5408\u8bc4\u5206\u6307\u6807\uff0c\u5e76\u5f15\u5165\u591a\u79cd\u76ee\u6807\u51fd\u6570\u548c\u4f30\u8ba1\u6280\u672f\u3002\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u6210\u672c\u5411\u91cf\u5e73\u5747\u6280\u672f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u57282023/24\u82f1\u8d85\u8d5b\u5b63\u6570\u636e\u4e2d\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad8\u5f97\u5206\u4e14\u8868\u73b0\u7a33\u5b9a\u3002\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u6df7\u5408\u65b9\u6cd5\u5728\u6837\u672c\u5916\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u68a6\u5e7b\u8db3\u7403\u63d0\u4f9b\u4e86\u79d1\u5b66\u7684\u4f18\u5316\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u73a9\u5bb6\u9009\u62e9\u3001\u9635\u578b\u548c\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2505.01642", "pdf": "https://arxiv.org/pdf/2505.01642", "abs": "https://arxiv.org/abs/2505.01642", "authors": ["Shreya Sareen", "Swayamtrupta Panda"], "title": "Identifying Doppelganger Active Galactic Nuclei across redshifts from spectroscopic surveys", "categories": ["astro-ph.GA", "astro-ph.IM", "cs.LG"], "comment": "4 pages, 1 figure, submitted to AAS journals", "summary": "Active Galactic Nuclei (AGNs) are among the most luminous objects in the\nuniverse, making them valuable probes for studying galaxy evolution. However,\nunderstanding how AGN properties evolve over cosmic time remains a fundamental\nchallenge. This study investigates whether AGNs at low redshift (nearby) can\nserve as proxies for their high-redshift (distant) counterparts by identifying\nspectral 'doppelg\\\"angers', AGNs with remarkably similar emission line\nproperties despite being separated by vast cosmic distances. We analyze key\nspectral features of bona fide AGNs using the Sloan Digital Sky Survey's Data\nRelease 16, including continuum and emission lines: Nitrogen (N V), Carbon (C\nIV), Magnesium (Mg II), Hydrogen-beta (H$\\beta$), and Iron (Fe II - optical and\nUV) emission lines. We incorporated properties such as equivalent width,\nvelocity dispersion in the form of full width at half maximum (FWHM), and\ncontinuum luminosities (135nm, 300nm, and 510nm) closest to these prominent\nlines. Our initial findings suggest the existence of multiple AGNs with highly\nsimilar spectra, hinting at the possibility that local AGNs may indeed share\nintrinsic properties with high-redshift ones. We showcase here one of the\nbetter candidate pairs of AGNs resulting from our analyses.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4f4e\u7ea2\u79fbAGN\u662f\u5426\u80fd\u4f5c\u4e3a\u9ad8\u7ea2\u79fbAGN\u7684\u4ee3\u7406\uff0c\u901a\u8fc7\u5149\u8c31\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u53d1\u73b0\u5b58\u5728\u591a\u4e2a\u5149\u8c31\u9ad8\u5ea6\u76f8\u4f3c\u7684AGN\u3002", "motivation": "AGN\u662f\u7814\u7a76\u661f\u7cfb\u6f14\u5316\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u7406\u89e3\u5176\u6027\u8d28\u968f\u5b87\u5b99\u65f6\u95f4\u7684\u6f14\u53d8\u4ecd\u5177\u6311\u6218\u6027\u3002\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u5149\u8c31\u76f8\u4f3c\u6027\u9a8c\u8bc1\u4f4e\u7ea2\u79fbAGN\u80fd\u5426\u4ee3\u8868\u9ad8\u7ea2\u79fbAGN\u3002", "method": "\u5229\u7528Sloan Digital Sky Survey Data Release 16\u7684\u6570\u636e\uff0c\u5206\u6790AGN\u7684\u5173\u952e\u5149\u8c31\u7279\u5f81\uff0c\u5305\u62ec\u8fde\u7eed\u8c31\u548c\u53d1\u5c04\u7ebf\uff08\u5982N V\u3001C IV\u3001Mg II\u7b49\uff09\uff0c\u5e76\u6574\u5408\u7b49\u6548\u5bbd\u5ea6\u3001\u901f\u5ea6\u5f25\u6563\u548c\u8fde\u7eed\u8c31\u5149\u5ea6\u7b49\u5c5e\u6027\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u591a\u4e2a\u5149\u8c31\u9ad8\u5ea6\u76f8\u4f3c\u7684AGN\uff0c\u8868\u660e\u672c\u5730AGN\u53ef\u80fd\u4e0e\u9ad8\u7ea2\u79fbAGN\u5177\u6709\u76f8\u4f3c\u7684\u5185\u5728\u6027\u8d28\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u4f4e\u7ea2\u79fbAGN\u53ef\u80fd\u4f5c\u4e3a\u9ad8\u7ea2\u79fbAGN\u4ee3\u7406\u7684\u5047\u8bbe\uff0c\u4e3a\u7814\u7a76\u5b87\u5b99\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684AGN\u6027\u8d28\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.01654", "pdf": "https://arxiv.org/pdf/2505.01654", "abs": "https://arxiv.org/abs/2505.01654", "authors": ["Srecharan Selvam", "Abhisesh Silwal", "George Kantor"], "title": "T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation", "categories": ["cs.RO", "cs.LG", "I.2.10"], "comment": "11 Pages, 10 figures, 2 tables", "summary": "T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic\nsystem developed for autonomous leaf localization, selection, and grasping in\ngreenhouse environments. The system integrates a 6-degree-of-freedom\nmanipulator with a stereo vision pipeline to identify and interact with target\nleaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo\nprovides dense depth maps, allowing the reconstruction of 3D leaf masks. These\nobservations are processed through a leaf grasping algorithm that selects the\noptimal leaf based on clutter, visibility, and distance, and determines a grasp\npoint by analyzing local surface flatness, top-down approachability, and margin\nfrom edges. The selected grasp point guides a trajectory executed by ROS-based\nmotion controllers, driving a custom microneedle-equipped end-effector to clamp\nthe leaf and simulate tissue sampling. Experiments conducted with artificial\nplants under varied poses demonstrate that the T-Rex system can consistently\ndetect, plan, and perform physical interactions with plant-like targets,\nachieving a grasp success rate of 66.6\\%. This paper presents the system\narchitecture, implementation, and testing of T-Rex as a step toward plant\nsampling automation in Controlled Environment Agriculture (CEA).", "AI": {"tldr": "T-Rex\u662f\u4e00\u4e2a\u6e29\u5ba4\u73af\u5883\u4e0b\u81ea\u4e3b\u5b9a\u4f4d\u3001\u9009\u62e9\u548c\u6293\u53d6\u53f6\u7247\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u54086\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u7acb\u4f53\u89c6\u89c9\uff0c\u901a\u8fc7YOLOv8\u548cRAFT-Stereo\u5b9e\u73b0\u53f6\u7247\u5206\u5272\u4e0e3D\u91cd\u5efa\uff0c\u6293\u53d6\u6210\u529f\u738766.6%\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u6e29\u5ba4\u73af\u5883\u4e2d\u690d\u7269\u53f6\u7247\u91c7\u6837\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u5347\u53ef\u63a7\u73af\u5883\u519c\u4e1a(CEA)\u7684\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u96c6\u62106\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0e\u7acb\u4f53\u89c6\u89c9\uff0c\u4f7f\u7528YOLOv8\u8fdb\u884c\u5b9e\u65f6\u53f6\u7247\u5206\u5272\uff0cRAFT-Stereo\u751f\u6210\u6df1\u5ea6\u56fe\uff0c\u901a\u8fc7\u6293\u53d6\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u53f6\u7247\u5e76\u89c4\u5212\u8f68\u8ff9\u3002", "result": "\u5728\u4eba\u5de5\u690d\u7269\u5b9e\u9a8c\u4e2d\uff0cT-Rex\u7684\u6293\u53d6\u6210\u529f\u7387\u8fbe66.6%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5b9e\u73b0\u690d\u7269\u91c7\u6837\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u662fCEA\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2505.02192", "pdf": "https://arxiv.org/pdf/2505.02192", "abs": "https://arxiv.org/abs/2505.02192", "authors": ["Wenchuan Wang", "Mengqi Huang", "Yijing Tu", "Zhendong Mao"], "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics.", "AI": {"tldr": "DualReal\u662f\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u89e3\u51b3\u8eab\u4efd\u548c\u8fd0\u52a8\u7684\u4e00\u81f4\u6027\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\u8eab\u4efd\u548c\u8fd0\u52a8\u5b9a\u5236\u662f\u5b64\u7acb\u7684\uff0c\u5ffd\u7565\u4e86\u4e8c\u8005\u7684\u76f8\u4e92\u7ea6\u675f\u548c\u534f\u540c\u4f9d\u8d56\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u51b2\u7a81\u3002", "method": "DualReal\u91c7\u7528\u81ea\u9002\u5e94\u8054\u5408\u8bad\u7ec3\uff0c\u5305\u542b\u53cc\u611f\u77e5\u9002\u5e94\u5355\u5143\u548c\u9636\u6bb5\u6df7\u5408\u63a7\u5236\u5668\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u9636\u6bb5\u548c\u7c92\u5ea6\u4ee5\u907f\u514d\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDualReal\u5728CLIP-I\u548cDINO-I\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8621.7%\u548c31.8%\uff0c\u5728\u8fd0\u52a8\u8d28\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DualReal\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u4e0e\u8fd0\u52a8\u7684\u4e00\u81f4\u6027\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2505.01670", "pdf": "https://arxiv.org/pdf/2505.01670", "abs": "https://arxiv.org/abs/2505.01670", "authors": ["Christos Zangos", "Danish Ebadulla", "Thomas Christopher Sprague", "Ambuj Singh"], "title": "Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This work introduces a novel approach to fMRI-based visual image\nreconstruction using a subject-agnostic common representation space. We show\nthat the brain signals of the subjects can be aligned in this common space\nduring training to form a semantically aligned common brain. This is leveraged\nto demonstrate that aligning subject-specific lightweight modules to a\nreference subject is significantly more efficient than traditional end-to-end\ntraining methods. Our approach excels in low-data scenarios. We evaluate our\nmethods on different datasets, demonstrating that the common space is subject\nand dataset-agnostic.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8efMRI\u7684\u89c6\u89c9\u56fe\u50cf\u91cd\u5efa\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u4e3b\u4f53\u65e0\u5173\u7684\u516c\u5171\u8868\u793a\u7a7a\u95f4\u5bf9\u9f50\u4e0d\u540c\u4e3b\u4f53\u7684\u5927\u8111\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edffMRI\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u4e3b\u4f53\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u6570\u636e\u9700\u6c42\u9ad8\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u516c\u5171\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u4e3b\u4f53\u65e0\u5173\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u5e76\u964d\u4f4e\u6570\u636e\u9700\u6c42\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5728\u8bad\u7ec3\u65f6\u5bf9\u9f50\u4e3b\u4f53\u5927\u8111\u4fe1\u53f7\u5230\u4e00\u4e2a\u516c\u5171\u8868\u793a\u7a7a\u95f4\uff0c\u5f62\u6210\u8bed\u4e49\u5bf9\u9f50\u7684\u201c\u516c\u5171\u5927\u8111\u201d\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5c06\u4e3b\u4f53\u7279\u5b9a\u4fe1\u606f\u5bf9\u9f50\u5230\u53c2\u8003\u4e3b\u4f53\uff0c\u800c\u975e\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u516c\u5171\u8868\u793a\u7a7a\u95f4\u5177\u6709\u4e3b\u4f53\u548c\u6570\u636e\u96c6\u65e0\u5173\u6027\uff0c\u4e14\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u516c\u5171\u8868\u793a\u7a7a\u95f4\u663e\u8457\u63d0\u9ad8\u4e86fMRI\u89c6\u89c9\u56fe\u50cf\u91cd\u5efa\u7684\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\uff0c\u5c55\u793a\u4e86\u4e3b\u4f53\u65e0\u5173\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02198", "pdf": "https://arxiv.org/pdf/2505.02198", "abs": "https://arxiv.org/abs/2505.02198", "authors": ["Griffin Pitts", "Viktoria Marcus", "Sanaz Motamedi"], "title": "Student Perspectives on the Benefits and Risks of AI in Education", "categories": ["cs.CY", "cs.AI", "cs.ET", "K.3; K.4"], "comment": null, "summary": "The use of chatbots equipped with artificial intelligence (AI) in educational\nsettings has increased in recent years, showing potential to support teaching\nand learning. However, the adoption of these technologies has raised concerns\nabout their impact on academic integrity, students' ability to problem-solve\nindependently, and potential underlying biases. To better understand students'\nperspectives and experiences with these tools, a survey was conducted at a\nlarge public university in the United States. Through thematic analysis, 262\nundergraduate students' responses regarding their perceived benefits and risks\nof AI chatbots in education were identified and categorized into themes.\n  The results discuss several benefits identified by the students, with\nfeedback and study support, instruction capabilities, and access to information\nbeing the most cited. Their primary concerns included risks to academic\nintegrity, accuracy of information, loss of critical thinking skills, the\npotential development of overreliance, and ethical considerations such as data\nprivacy, system bias, environmental impact, and preservation of human elements\nin education.\n  While student perceptions align with previously discussed benefits and risks\nof AI in education, they show heightened concerns about distinguishing between\nhuman and AI generated work - particularly in cases where authentic work is\nflagged as AI-generated. To address students' concerns, institutions can\nestablish clear policies regarding AI use and develop curriculum around AI\nliteracy. With these in place, practitioners can effectively develop and\nimplement educational systems that leverage AI's potential in areas such as\nimmediate feedback and personalized learning support. This approach can enhance\nthe quality of students' educational experiences while preserving the integrity\nof the learning process with AI.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u5b66\u751f\u5bf9AI\u804a\u5929\u673a\u5668\u4eba\u5728\u6559\u80b2\u4e2d\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u5176\u76ca\u5904\u5305\u62ec\u5b66\u4e60\u652f\u6301\u548c\u4fe1\u606f\u83b7\u53d6\uff0c\u4f46\u62c5\u5fe7\u6d89\u53ca\u5b66\u672f\u8bda\u4fe1\u3001\u4fe1\u606f\u51c6\u786e\u6027\u548c\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u5efa\u8bae\u901a\u8fc7\u653f\u7b56\u548cAI\u7d20\u517b\u6559\u80b2\u89e3\u51b3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5b66\u751f\u5bf9AI\u804a\u5929\u673a\u5668\u4eba\u5728\u6559\u80b2\u4e2d\u7684\u4f7f\u7528\u4f53\u9a8c\uff0c\u4ee5\u5e73\u8861\u5176\u6f5c\u5728\u76ca\u5904\u4e0e\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5728\u7f8e\u56fd\u4e00\u6240\u5927\u578b\u516c\u7acb\u5927\u5b66\u7684262\u540d\u672c\u79d1\u751f\u8c03\u67e5\uff0c\u91c7\u7528\u4e3b\u9898\u5206\u6790\u6cd5\u5206\u6790\u5b66\u751f\u5bf9AI\u804a\u5929\u673a\u5668\u4eba\u7684\u5229\u5f0a\u8ba4\u77e5\u3002", "result": "\u5b66\u751f\u8ba4\u4e3aAI\u804a\u5929\u673a\u5668\u4eba\u6709\u52a9\u4e8e\u5b66\u4e60\u652f\u6301\u548c\u53cd\u9988\uff0c\u4f46\u4e5f\u62c5\u5fe7\u5b66\u672f\u8bda\u4fe1\u3001\u4fe1\u606f\u51c6\u786e\u6027\u53ca\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u5efa\u8bae\u901a\u8fc7\u660e\u786e\u653f\u7b56\u548cAI\u7d20\u517b\u6559\u80b2\uff0c\u5728\u5229\u7528AI\u4f18\u52bf\u7684\u540c\u65f6\u89e3\u51b3\u5b66\u751f\u62c5\u5fe7\uff0c\u4fdd\u969c\u6559\u80b2\u5b8c\u6574\u6027\u3002"}}
{"id": "2505.01742", "pdf": "https://arxiv.org/pdf/2505.01742", "abs": "https://arxiv.org/abs/2505.01742", "authors": ["Yu Mao", "Jingzong Li", "Jun Wang", "Hong Xu", "Tei-Wei Kuo", "Nan Guan", "Chun Jason Xue"], "title": "Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "Neural image compression, necessary in various machine-to-machine\ncommunication scenarios, suffers from its heavy encode-decode structures and\ninflexibility in switching between different compression levels. Consequently,\nit raises significant challenges in applying the neural image compression to\nedge devices that are developed for powerful servers with high computational\nand storage capacities. We take a step to solve the challenges by proposing a\nnew transformer-based edge-compute-free image coding framework called Easz.\nEasz shifts the computational overhead to the server, and hence avoids the\nheavy encoding and model switching overhead on the edge. Easz utilizes a\npatch-erase algorithm to selectively remove image contents using a conditional\nuniform-based sampler. The erased pixels are reconstructed on the receiver side\nthrough a transformer-based framework. To further reduce the computational\noverhead on the receiver, we then introduce a lightweight transformer-based\nreconstruction structure to reduce the reconstruction load on the receiver\nside. Extensive evaluations conducted on a real-world testbed demonstrate\nmultiple advantages of Easz over existing compression approaches, in terms of\nadaptability to different compression levels, computational efficiency, and\nimage reconstruction quality.", "AI": {"tldr": "Easz\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8fb9\u7f18\u8ba1\u7b97\u81ea\u7531\u56fe\u50cf\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u8d1f\u62c5\u8f6c\u79fb\u5230\u670d\u52a1\u5668\uff0c\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u96be\u9898\u3002", "motivation": "\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e94\u7528\u65f6\uff0c\u9762\u4e34\u7f16\u7801-\u89e3\u7801\u7ed3\u6784\u7b28\u91cd\u548c\u538b\u7f29\u7ea7\u522b\u5207\u6362\u4e0d\u7075\u6d3b\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51faEasz\u6846\u67b6\uff0c\u5229\u7528patch-erase\u7b97\u6cd5\u9009\u62e9\u6027\u5220\u9664\u56fe\u50cf\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7Transformer\u5728\u63a5\u6536\u7aef\u91cd\u5efa\u3002\u8fdb\u4e00\u6b65\u91c7\u7528\u8f7b\u91cf\u7ea7Transformer\u7ed3\u6784\u964d\u4f4e\u63a5\u6536\u7aef\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEasz\u5728\u538b\u7f29\u7ea7\u522b\u9002\u5e94\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Easz\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u8d1f\u62c5\u548c\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2505.02230", "pdf": "https://arxiv.org/pdf/2505.02230", "abs": "https://arxiv.org/abs/2505.02230", "authors": ["Micaela Siraj", "Jon Duke"], "title": "The GenAI Generation: Student Views of Awareness, Preparedness, and Concern", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Generative AI (GenAI) is revolutionizing education and workforce development,\nprofoundly shaping how students learn, engage, and prepare for their future.\nOutpacing the development of uniform policies and structures, GenAI has\nheralded a unique era and given rise to the GenAI Generation: a cohort of\nstudents whose education has been increasingly shaped by the opportunities and\nchallenges GenAI presents during its widespread adoption within society. This\nstudy examines our students' perceptions of GenAI through a concise survey with\noptional open-ended questions, focusing on their awareness, preparedness, and\nconcerns. Evaluation of more than 250 responses with more than 40% providing\ndetailed qualitative feedback reveals a core dual sentiment: while most\nstudents express enthusiasm for GenAI, an even greater proportion voice a\nspectrum of concerns about ethics, job displacement, and the adequacy of\neducational structures given the highly transformative technology. These\nfindings offer critical insights into how students view the potential and\npitfalls of GenAI for future career impacts, with accompanying recommendations\nto guide educational institutions in navigating a future driven by GenAI.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u6b63\u5728\u91cd\u5851\u6559\u80b2\u548c\u804c\u4e1a\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u653f\u7b56\u3002\u7814\u7a76\u901a\u8fc7\u8c03\u67e5\u63ed\u793a\u5b66\u751f\u5bf9GenAI\u7684\u53cc\u91cd\u60c5\u611f\uff1a\u65e2\u5174\u594b\u53c8\u62c5\u5fe7\u5176\u4f26\u7406\u3001\u5c31\u4e1a\u548c\u6559\u80b2\u7ed3\u6784\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u5b66\u751f\u5bf9GenAI\u7684\u8ba4\u77e5\u548c\u6001\u5ea6\uff0c\u4ee5\u6307\u5bfc\u6559\u80b2\u673a\u6784\u5e94\u5bf9\u8fd9\u4e00\u53d8\u9769\u6027\u6280\u672f\u5e26\u6765\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u542b\u5f00\u653e\u5f0f\u95ee\u9898\u7684\u7b80\u660e\u8c03\u67e5\uff0c\u6536\u96c6250\u591a\u4efd\u53cd\u9988\uff0840%\u4e3a\u5b9a\u6027\u56de\u7b54\uff09\uff0c\u5206\u6790\u5b66\u751f\u5bf9GenAI\u7684\u8ba4\u77e5\u3001\u51c6\u5907\u548c\u62c5\u5fe7\u3002", "result": "\u5b66\u751f\u666e\u904d\u5bf9GenAI\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u4f46\u66f4\u62c5\u5fe7\u4f26\u7406\u3001\u5c31\u4e1a\u66ff\u4ee3\u53ca\u6559\u80b2\u9002\u5e94\u6027\u7b49\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6559\u80b2\u673a\u6784\u63d0\u4f9b\u4e86\u5b66\u751f\u89c6\u89d2\u7684\u5173\u952e\u6d1e\u5bdf\uff0c\u5efa\u8bae\u8c03\u6574\u7b56\u7565\u4ee5\u5e94\u5bf9GenAI\u9a71\u52a8\u7684\u672a\u6765\u3002"}}
{"id": "2505.01751", "pdf": "https://arxiv.org/pdf/2505.01751", "abs": "https://arxiv.org/abs/2505.01751", "authors": ["Vivek Shripad Borkar"], "title": "A dynamic view of the double descent", "categories": ["math.OC", "cs.LG"], "comment": "8 pages, 2 figures", "summary": "It has been observed by Belkin et al.\\ that overparametrized neural networks\nexhibit a `double descent' phenomenon. That is, as the model complexity, as\nreflected in the number of features, increases, the training error initially\ndecreases, then increases, and then decreases again. A counterpart of this\nphenomenon in the time domain has been noted in the context of epoch-wise\ntraining, viz., that the training error decreases with time, then increases,\nthen decreases again. This note presents a plausible explanation for this\nphenomenon by using the theory of two time scale stochastic approximation and\nsingularly perturbed differential equations, applied to the continuous time\nlimit of the gradient dynamics. This adds a `dynamic' angle to an already well\nstudied theme.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8fc7\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684'\u53cc\u4e0b\u964d'\u73b0\u8c61\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u53cc\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u8fd1\u4f3c\u548c\u5947\u5f02\u6270\u52a8\u5fae\u5206\u65b9\u7a0b\u7684\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u63a2\u7d22\u8fc7\u53c2\u6570\u5316\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684'\u53cc\u4e0b\u964d'\u73b0\u8c61\u7684\u52a8\u6001\u673a\u5236\u3002", "method": "\u4f7f\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u8fd1\u4f3c\u548c\u5947\u5f02\u6270\u52a8\u5fae\u5206\u65b9\u7a0b\u7406\u8bba\uff0c\u5206\u6790\u68af\u5ea6\u52a8\u529b\u5b66\u7684\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u3002", "result": "\u4e3a'\u53cc\u4e0b\u964d'\u73b0\u8c61\u63d0\u4f9b\u4e86\u52a8\u6001\u89d2\u5ea6\u7684\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2505.02232", "pdf": "https://arxiv.org/pdf/2505.02232", "abs": "https://arxiv.org/abs/2505.02232", "authors": ["Malte Mosbach", "Sven Behnke"], "title": "Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Building models responsive to input prompts represents a transformative shift\nin machine learning. This paradigm holds significant potential for robotics\nproblems, such as targeted manipulation amidst clutter. In this work, we\npresent a novel approach to combine promptable foundation models with\nreinforcement learning (RL), enabling robots to perform dexterous manipulation\ntasks in a prompt-responsive manner. Existing methods struggle to link\nhigh-level commands with fine-grained dexterous control. We address this gap\nwith a memory-augmented student-teacher learning framework. We use the\nSegment-Anything 2 (SAM 2) model as a perception backbone to infer an object of\ninterest from user prompts. While detections are imperfect, their temporal\nsequence provides rich information for implicit state estimation by\nmemory-augmented models. Our approach successfully learns prompt-responsive\npolicies, demonstrated in picking objects from cluttered scenes. Videos and\ncode are available at https://memory-student-teacher.github.io", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u53ef\u63d0\u793a\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u7684\u5e08\u751f\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ee5\u54cd\u5e94\u63d0\u793a\u7684\u65b9\u5f0f\u6267\u884c\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5229\u7528SAM 2\u6a21\u578b\u4f5c\u4e3a\u611f\u77e5\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ece\u7528\u6237\u63d0\u793a\u4e2d\u63a8\u65ad\u611f\u5174\u8da3\u7684\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u68c0\u6d4b\u5b9e\u73b0\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u96be\u4ee5\u5c06\u9ad8\u7ea7\u547d\u4ee4\u4e0e\u7cbe\u7ec6\u7075\u5de7\u63a7\u5236\u8054\u7cfb\u8d77\u6765\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u6742\u4e71\u573a\u666f\u4e2d\u901a\u8fc7\u7528\u6237\u63d0\u793a\u6267\u884c\u76ee\u6807\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u8bb0\u5fc6\u589e\u5f3a\u7684\u5e08\u751f\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408Segment-Anything 2\uff08SAM 2\uff09\u6a21\u578b\u4f5c\u4e3a\u611f\u77e5\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ece\u7528\u6237\u63d0\u793a\u4e2d\u63a8\u65ad\u611f\u5174\u8da3\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u68c0\u6d4b\u5b9e\u73b0\u9690\u5f0f\u72b6\u6001\u4f30\u8ba1\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b66\u4e60\u4e86\u54cd\u5e94\u63d0\u793a\u7684\u7b56\u7565\uff0c\u5e76\u5728\u4ece\u6742\u4e71\u573a\u666f\u4e2d\u62fe\u53d6\u7269\u4f53\u7684\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7ed3\u5408\u53ef\u63d0\u793a\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6742\u4e71\u73af\u5883\u4e2d\u7684\u76ee\u6807\u64cd\u4f5c\u3002"}}
{"id": "2505.01785", "pdf": "https://arxiv.org/pdf/2505.01785", "abs": "https://arxiv.org/abs/2505.01785", "authors": ["Ayoub Abraich"], "title": "TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Estimating the causal effect of time-varying treatments on survival outcomes\nis a challenging task in many domains, particularly in medicine where treatment\nprotocols adapt over time. While recent advances in representation learning\nhave improved causal inference for static treatments, extending these methods\nto dynamic treatment regimes with survival outcomes remains under-explored. In\nthis paper, we introduce TV-SurvCaus, a novel framework that extends\nrepresentation balancing techniques to the time-varying treatment setting for\nsurvival analysis. We provide theoretical guarantees through (1) a generalized\nbound for time-varying precision in estimation of heterogeneous effects, (2)\nvariance control via sequential balancing weights, (3) consistency results for\ndynamic treatment regimes, (4) convergence rates for representation learning\nwith temporal dependencies, and (5) a formal bound on the bias due to\ntreatment-confounder feedback. Our neural architecture incorporates sequence\nmodeling to handle temporal dependencies while balancing time-dependent\nrepresentations. Through extensive experiments on both synthetic and real-world\ndatasets, we demonstrate that TV-SurvCaus outperforms existing methods in\nestimating individualized treatment effects with time-varying covariates and\ntreatments. Our framework advances the field of causal inference by enabling\nmore accurate estimation of treatment effects in dynamic, longitudinal settings\nwith survival outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TV-SurvCaus\u6846\u67b6\uff0c\u5c06\u8868\u793a\u5e73\u8861\u6280\u672f\u6269\u5c55\u5230\u52a8\u6001\u6cbb\u7597\u65b9\u6848\u4e2d\uff0c\u7528\u4e8e\u751f\u5b58\u5206\u6790\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u533b\u5b66\u7b49\u9886\u57df\u7684\u65f6\u95f4\u53d8\u5316\u6cbb\u7597\u65b9\u6848\u5bf9\u751f\u5b58\u7ed3\u679c\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u662f\u4e00\u4e2a\u590d\u6742\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u9759\u6001\u6cbb\u7597\u65b9\u6848\uff0c\u52a8\u6001\u65b9\u6848\u7684\u7814\u7a76\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "TV-SurvCaus\u6846\u67b6\u7ed3\u5408\u4e86\u8868\u793a\u5e73\u8861\u6280\u672f\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\u6027\u5e76\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u5e73\u8861\u65f6\u53d8\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTV-SurvCaus\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u51c6\u786e\u4f30\u8ba1\u52a8\u6001\u6cbb\u7597\u65b9\u6848\u4e0b\u7684\u4e2a\u4f53\u5316\u6cbb\u7597\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63a8\u52a8\u4e86\u65f6\u95f4\u52a8\u6001\u8bbe\u7f6e\u4e0b\u751f\u5b58\u7ed3\u679c\u7684\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.02236", "pdf": "https://arxiv.org/pdf/2505.02236", "abs": "https://arxiv.org/abs/2505.02236", "authors": ["Tianle Chen", "Chaitanya Chakka", "Deepti Ghadiyaram"], "title": "Improving Physical Object State Representation in Text-to-Image Generative Systems", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to Synthetic Data for Computer Vision - CVPR 2025 Workshop", "summary": "Current text-to-image generative models struggle to accurately represent\nobject states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this\nwork, we first design a fully-automatic pipeline to generate high-quality\nsynthetic data that accurately captures objects in varied states. Next, we\nfine-tune several open-source text-to-image models on this synthetic data. We\nevaluate the performance of the fine-tuned models by quantifying the alignment\nof the generated images to their prompts using GPT4o-mini, and achieve an\naverage absolute improvement of 8+% across four models on the public\nGenAI-Bench dataset. We also curate a collection of 200 prompts with a specific\nfocus on common objects in various physical states. We demonstrate a\nsignificant improvement of an average of 24+% over the baseline on this\ndataset. We release all evaluation prompts and code.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u63d0\u5347\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u5728\u5bf9\u8c61\u72b6\u6001\uff08\u5982'\u6ca1\u6709\u74f6\u5b50\u7684\u684c\u5b50'\u3001'\u7a7a\u676f\u5b50'\uff09\u4e0a\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5e76\u8bc4\u4f30\uff0c\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e868%\u4ee5\u4e0a\u7684\u63d0\u5347\uff0c\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e8624%\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\u5728\u51c6\u786e\u63cf\u8ff0\u5bf9\u8c61\u72b6\u6001\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6a21\u578b\u5fae\u8c03\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5168\u81ea\u52a8\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u6355\u6349\u5bf9\u8c61\u72b6\u6001\uff1b\u5fae\u8c03\u591a\u4e2a\u5f00\u6e90\u6587\u672c\u751f\u6210\u56fe\u50cf\u6a21\u578b\uff1b\u4f7f\u7528GPT4o-mini\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u4e0e\u63d0\u793a\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6GenAI-Bench\u4e0a\u5e73\u5747\u63d0\u53478%\u4ee5\u4e0a\uff0c\u5728\u81ea\u5efa\u7684200\u6761\u63d0\u793a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u534724%\u4ee5\u4e0a\u3002", "conclusion": "\u8bba\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5bf9\u8c61\u72b6\u6001\u63cf\u8ff0\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u8bc4\u4f30\u63d0\u793a\u548c\u4ee3\u7801\u3002"}}
{"id": "2505.01807", "pdf": "https://arxiv.org/pdf/2505.01807", "abs": "https://arxiv.org/abs/2505.01807", "authors": ["Anthony Nouy", "Alexandre Pasco"], "title": "Surrogate to Poincar\u00e9 inequalities on manifolds for dimension reduction in nonlinear feature spaces", "categories": ["math.NA", "cs.LG", "cs.NA", "65D40, 65D15, 41A10, 41A63, 60F10"], "comment": "35 pages, 6 figures", "summary": "We aim to approximate a continuously differentiable function $u:\\mathbb{R}^d\n\\rightarrow \\mathbb{R}$ by a composition of functions $f\\circ g$ where\n$g:\\mathbb{R}^d \\rightarrow \\mathbb{R}^m$, $m\\leq d$, and $f : \\mathbb{R}^m\n\\rightarrow \\mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we\nbuild $f$ using classical regression methods, involving evaluations of $u$.\nRecent works proposed to build a nonlinear $g$ by minimizing a loss function\n$\\mathcal{J}(g)$ derived from Poincar\\'e inequalities on manifolds, involving\nevaluations of the gradient of $u$. A problem is that minimizing $\\mathcal{J}$\nmay be a challenging task. Hence in this work, we introduce new convex\nsurrogates to $\\mathcal{J}$. Leveraging concentration inequalities, we provide\nsub-optimality results for a class of functions $g$, including polynomials, and\na wide class of input probability measures. We investigate performances on\ndifferent benchmarks for various training sample sizes. We show that our\napproach outperforms standard iterative methods for minimizing the training\nPoincar\\'e inequality based loss, often resulting in better approximation\nerrors, especially for rather small training sets and $m=1$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u51f8\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u6765\u903c\u8fd1\u8fde\u7eed\u53ef\u5fae\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u6807\u51c6\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5c0f\u578b\u8bad\u7ec3\u96c6\u548cm=1\u65f6\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u57fa\u4e8e\u5e9e\u52a0\u83b1\u4e0d\u7b49\u5f0f\u7684\u635f\u5931\u51fd\u6570\u6765\u6784\u5efa\u975e\u7ebf\u6027\u6620\u5c04g\uff0c\u4f46\u4f18\u5316\u8be5\u635f\u5931\u51fd\u6570\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u66f4\u6613\u4f18\u5316\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u56fa\u5b9ag\u65f6\u7528\u7ecf\u5178\u56de\u5f52\u65b9\u6cd5\u6784\u5efaf\uff1b\u5f15\u5165\u65b0\u7684\u51f8\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u6765\u4f18\u5316g\uff0c\u5e76\u901a\u8fc7\u6d53\u5ea6\u4e0d\u7b49\u5f0f\u63d0\u4f9b\u6b21\u4f18\u6027\u7ed3\u679c\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6807\u51c6\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u96c6\u8f83\u5c0f\u4e14m=1\u65f6\uff0c\u8fd1\u4f3c\u8bef\u5dee\u66f4\u5c0f\u3002", "conclusion": "\u63d0\u51fa\u7684\u51f8\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u6709\u6548\u89e3\u51b3\u4e86\u539f\u635f\u5931\u51fd\u6570\u7684\u4f18\u5316\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51fd\u6570\u903c\u8fd1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.02255", "pdf": "https://arxiv.org/pdf/2505.02255", "abs": "https://arxiv.org/abs/2505.02255", "authors": ["Jakub W\u0105sala", "Bart\u0142omiej Wrzalski", "Kornelia Noculak", "Yuliia Tarasenko", "Oliwer Krupa", "Jan Koco\u0144", "Grzegorz Chodak"], "title": "Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "25th International Conference on Computational Science", "summary": "This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u6210\u672c\u6548\u76ca\u6bd4\uff0c\u5c06\u84b8\u998f\u6a21\u578b\uff08\u5982FLUX.1-schnell\uff09\u7684\u8f93\u51fa\u8d28\u91cf\u63d0\u5347\u81f3\u4e0e\u9ad8\u8ba1\u7b97\u6210\u672c\u57fa\u7ebf\u6a21\u578b\uff08\u5982FLUX.1-dev\uff09\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c1182%\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6210\u672c\u6548\u76ca\u6bd4\uff0c\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u84b8\u998f\u6a21\u578b\u4e0e\u57fa\u7ebf\u6a21\u578b\u95f4\u7684\u5dee\u5f02\u662f\u53ef\u5b66\u4e60\u7684\u8fd9\u4e00\u5047\u8bbe\uff0c\u4e13\u6ce8\u4e8e\u8096\u50cf\u751f\u6210\u9886\u57df\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u751f\u6210\u5408\u6210\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5feb\u901f\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\uff0c\u5c06\u84b8\u998f\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u81f3\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u8bc1\u751f\u6210\u7c7b\u4f3c\u57fa\u7ebf\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u8096\u50cf\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e8682%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u7ed3\u5408\u84b8\u998f\u6a21\u578b\u4e0e\u589e\u5f3a\u5c42\uff0c\u53ef\u4ee5\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.01816", "pdf": "https://arxiv.org/pdf/2505.01816", "abs": "https://arxiv.org/abs/2505.01816", "authors": ["Eran Aizikovich", "Dudu Mimran", "Edita Grolman", "Yuval Elovici", "Asaf Shabtai"], "title": "Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "The Open Radio Access Network (O-RAN) architecture is revolutionizing\ncellular networks with its open, multi-vendor design and AI-driven management,\naiming to enhance flexibility and reduce costs. Although it has many\nadvantages, O-RAN is not threat-free. While previous studies have mainly\nexamined vulnerabilities arising from O-RAN's intelligent components, this\npaper is the first to focus on the security challenges and vulnerabilities\nintroduced by transitioning from single-operator to multi-operator RAN\narchitectures. This shift increases the risk of untrusted third-party operators\nmanaging different parts of the network. To explore these vulnerabilities and\ntheir potential mitigation, we developed an open-access testbed environment\nthat integrates a wireless network simulator with the official O-RAN Software\nCommunity (OSC) RAN intelligent component (RIC) cluster. This environment\nenables realistic, live data collection and serves as a platform for\ndemonstrating APATE (adversarial perturbation against traffic efficiency), an\nevasion attack in which a malicious cell manipulates its reported key\nperformance indicators (KPIs) and deceives the O-RAN traffic steering to gain\nunfair allocations of user equipment (UE). To ensure that O-RAN's legitimate\nactivity continues, we introduce MARRS (monitoring adversarial RAN reports), a\ndetection framework based on a long-short term memory (LSTM) autoencoder (AE)\nthat learns contextual features across the network to monitor malicious\ntelemetry (also demonstrated in our testbed). Our evaluation showed that by\nexecuting APATE, an attacker can obtain a 248.5% greater UE allocation than it\nwas supposed to in a benign scenario. In addition, the MARRS detection method\nwas also shown to successfully classify malicious cell activity, achieving\naccuracy of 99.2% and an F1 score of 0.978.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86O-RAN\u4ece\u5355\u8fd0\u8425\u5546\u8f6c\u5411\u591a\u8fd0\u8425\u5546\u67b6\u6784\u65f6\u5f15\u5165\u7684\u5b89\u5168\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u6d4b\u6846\u67b6MARRS\uff0c\u5e76\u5c55\u793a\u4e86\u653b\u51fbAPATE\u80fd\u975e\u6cd5\u83b7\u53d6\u66f4\u591aUE\u5206\u914d\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u586b\u8865O-RAN\u5728\u4ece\u5355\u8fd0\u8425\u5546\u8f6c\u5411\u591a\u8fd0\u8425\u5546\u67b6\u6784\u65f6\u5b89\u5168\u6f0f\u6d1e\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u68c0\u6d4b\u548c\u7f13\u89e3\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u4e00\u4e2a\u5f00\u653e\u8bbf\u95ee\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u7ed3\u5408\u65e0\u7ebf\u7f51\u7edc\u6a21\u62df\u5668\u548cO-RAN\u8f6f\u4ef6\u793e\u533a\u7684RIC\u96c6\u7fa4\uff0c\u4ee5\u53ca\u63d0\u51fa\u68c0\u6d4b\u6846\u67b6MARRS\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u653b\u51fbAPATE\u80fd\u975e\u6cd5\u83b7\u53d6248.5%\u7684UE\u5206\u914d\uff0c\u800c\u68c0\u6d4b\u6846\u67b6MARRS\u8fbe\u5230\u4e8699.2%\u7684\u51c6\u786e\u7387\u548c0.978\u7684F1\u5206\u6570\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u591a\u8fd0\u8425\u5546\u67b6\u6784\u4e0bO-RAN\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4ee5\u53caMARRS\u6846\u67b6\u5728\u68c0\u6d4b\u6076\u610f\u6d3b\u52a8\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.02272", "pdf": "https://arxiv.org/pdf/2505.02272", "abs": "https://arxiv.org/abs/2505.02272", "authors": ["Dyuman Aditya", "Junning Huang", "Nico Bohlinger", "Piotr Kicki", "Krzysztof Walas", "Jan Peters", "Matteo Luperto", "Davide Tateo"], "title": "Robust Localization, Mapping, and Navigation for Quadruped Robots", "categories": ["cs.RO", "cs.AI"], "comment": "8 Pages", "summary": "Quadruped robots are currently a widespread platform for robotics research,\nthanks to powerful Reinforcement Learning controllers and the availability of\ncheap and robust commercial platforms. However, to broaden the adoption of the\ntechnology in the real world, we require robust navigation stacks relying only\non low-cost sensors such as depth cameras. This paper presents a first step\ntowards a robust localization, mapping, and navigation system for low-cost\nquadruped robots. In pursuit of this objective we combine contact-aided\nkinematic, visual-inertial odometry, and depth-stabilized vision, enhancing\nstability and accuracy of the system. Our results in simulation and two\ndifferent real-world quadruped platforms show that our system can generate an\naccurate 2D map of the environment, robustly localize itself, and navigate\nautonomously. Furthermore, we present in-depth ablation studies of the\nimportant components of the system and their impact on localization accuracy.\nVideos, code, and additional experiments can be found on the project website:\nhttps://sites.google.com/view/low-cost-quadruped-slam", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a5\u89e6\u8f85\u52a9\u8fd0\u52a8\u5b66\u3001\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u6df1\u5ea6\u7a33\u5b9a\u89c6\u89c9\u7684\u4f4e\u6210\u672c\u56db\u8db3\u673a\u5668\u4eba\u5b9a\u4f4d\u3001\u5efa\u56fe\u548c\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5b9e\u73b0\u7a33\u5065\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u4e3a\u4e86\u6269\u5927\u4f4e\u6210\u672c\u56db\u8db3\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u4f4e\u6210\u672c\u4f20\u611f\u5668\uff08\u5982\u6df1\u5ea6\u76f8\u673a\uff09\u7684\u7a33\u5065\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u63a5\u89e6\u8f85\u52a9\u8fd0\u52a8\u5b66\u3001\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u548c\u6df1\u5ea6\u7a33\u5b9a\u89c6\u89c9\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u4eff\u771f\u548c\u4e24\u79cd\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u751f\u6210\u7cbe\u786e\u76842D\u73af\u5883\u5730\u56fe\u3001\u7a33\u5065\u5b9a\u4f4d\u5e76\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u4f4e\u6210\u672c\u56db\u8db3\u673a\u5668\u4eba\u7684\u7a33\u5065\u5bfc\u822a\u7cfb\u7edf\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5e76\u901a\u8fc7\u8be6\u5c3d\u7684\u6d88\u878d\u7814\u7a76\u5206\u6790\u4e86\u5173\u952e\u7ec4\u4ef6\u5bf9\u5b9a\u4f4d\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002"}}
{"id": "2505.02274", "pdf": "https://arxiv.org/pdf/2505.02274", "abs": "https://arxiv.org/abs/2505.02274", "authors": ["Xingyu Zhao", "Robab Aghazadeh-Chakherlou", "Chih-Hong Cheng", "Peter Popov", "Lorenzo Strigini"], "title": "On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles", "categories": ["cs.SE", "cs.AI", "cs.RO"], "comment": "under review", "summary": "Scenario-based testing has emerged as a common method for autonomous vehicles\n(AVs) safety, offering a more efficient alternative to mile-based testing by\nfocusing on high-risk scenarios. However, fundamental questions persist\nregarding its stopping rules, residual risk estimation, debug effectiveness,\nand the impact of simulation fidelity on safety claims. This paper argues that\na rigorous statistical foundation is essential to address these challenges and\nenable rigorous safety assurance. By drawing parallels between AV testing and\ntraditional software testing methodologies, we identify shared research gaps\nand reusable solutions. We propose proof-of-concept models to quantify the\nprobability of failure per scenario (pfs) and evaluate testing effectiveness\nunder varying conditions. Our analysis reveals that neither scenario-based nor\nmile-based testing universally outperforms the other. Furthermore, we introduce\nRisk Estimation Fidelity (REF), a novel metric to certify the alignment of\nsynthetic and real-world testing outcomes, ensuring simulation-based safety\nclaims are statistically defensible.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u57fa\u4e8e\u573a\u666f\u7684\u6d4b\u8bd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u7edf\u8ba1\u57fa\u7840\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6Risk Estimation Fidelity (REF)\u6765\u9a8c\u8bc1\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6d4b\u8bd5\u9700\u8981\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u57fa\u4e8e\u573a\u666f\u7684\u6d4b\u8bd5\u5728\u7edf\u8ba1\u57fa\u7840\u3001\u6b8b\u4f59\u98ce\u9669\u4f30\u8ba1\u7b49\u65b9\u9762\u4ecd\u5b58\u5728\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u6d4b\u8bd5\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u6bcf\u4e2a\u573a\u666f\u6545\u969c\u6982\u7387(pfs)\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6d4b\u8bd5\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u573a\u666f\u548c\u57fa\u4e8e\u91cc\u7a0b\u7684\u6d4b\u8bd5\u5404\u6709\u4f18\u52a3\uff0c\u4e14\u5f15\u5165\u4e86REF\u6307\u6807\u6765\u786e\u4fdd\u4eff\u771f\u6d4b\u8bd5\u7ed3\u679c\u7684\u7edf\u8ba1\u53ef\u9760\u6027\u3002", "conclusion": "\u7edf\u8ba1\u57fa\u7840\u7684\u5efa\u7acb\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u6d4b\u8bd5\u7684\u5173\u952e\uff0cREF\u4e3a\u65b0\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u8ba1\u53ef\u8fa9\u62a4\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.01828", "pdf": "https://arxiv.org/pdf/2505.01828", "abs": "https://arxiv.org/abs/2505.01828", "authors": ["Arman Sharifi Kolarijani", "Tolga Ok", "Peyman Mohajerin Esfahani", "Mohamad Amin Sharif Kolarijani"], "title": "Rank-One Modified Value Iteration", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "24 pages,9 figures, conference", "summary": "In this paper, we provide a novel algorithm for solving planning and learning\nproblems of Markov decision processes. The proposed algorithm follows a policy\niteration-type update by using a rank-one approximation of the transition\nprobability matrix in the policy evaluation step. This rank-one approximation\nis closely related to the stationary distribution of the corresponding\ntransition probability matrix, which is approximated using the power method. We\nprovide theoretical guarantees for the convergence of the proposed algorithm to\noptimal (action-)value function with the same rate and computational complexity\nas the value iteration algorithm in the planning problem and as the Q-learning\nalgorithm in the learning problem. Through our extensive numerical simulations,\nhowever, we show that the proposed algorithm consistently outperforms\nfirst-order algorithms and their accelerated versions for both planning and\nlearning problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8f6c\u79fb\u6982\u7387\u77e9\u9635\u7684\u79e9\u4e00\u8fd1\u4f3c\u6765\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u89c4\u5212\u548c\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u53ca\u8ba1\u7b97\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u7b97\u6cd5\u5728\u89c4\u5212\u548c\u5b66\u4e60\u95ee\u9898\u4e2d\u4f18\u4e8e\u4e00\u9636\u7b97\u6cd5\u53ca\u5176\u52a0\u901f\u7248\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u89c4\u5212\u548c\u5b66\u4e60\u7b97\u6cd5\uff08\u5982\u503c\u8fed\u4ee3\u548cQ\u5b66\u4e60\uff09\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u79e9\u4e00\u8fd1\u4f3c\u65b9\u6cd5\u5728\u7b80\u5316\u548c\u52a0\u901f\u8ba1\u7b97\u7684\u540c\u65f6\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u8f6c\u79fb\u6982\u7387\u77e9\u9635\u7684\u5173\u952e\u7279\u5f81\u3002", "method": "\u7b97\u6cd5\u91c7\u7528\u7b56\u7565\u8fed\u4ee3\u6846\u67b6\uff0c\u5728\u7b56\u7565\u8bc4\u4f30\u6b65\u9aa4\u4e2d\u5f15\u5165\u79e9\u4e00\u8fd1\u4f3c\u4ee3\u66ff\u5b8c\u6574\u7684\u8f6c\u79fb\u6982\u7387\u77e9\u9635\uff0c\u5e76\u7ed3\u5408\u5e42\u65b9\u6cd5\u4f30\u7b97\u5176\u5e73\u7a33\u5206\u5e03\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7b97\u6cd5\u7684\u6536\u655b\u6027\u4e0e\u4f20\u7edf\u503c\u8fed\u4ee3\u548cQ\u5b66\u4e60\u4e00\u81f4\uff0c\u4f46\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4e00\u9636\u7b97\u6cd5\u53ca\u5176\u52a0\u901f\u7248\u672c\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u79e9\u4e00\u8fd1\u4f3c\u5728\u4fdd\u6301\u7406\u8bba\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u89c4\u5212\u548c\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02281", "pdf": "https://arxiv.org/pdf/2505.02281", "abs": "https://arxiv.org/abs/2505.02281", "authors": ["Amir Ali Farzin", "Yuen-Man Pun", "Iman Shames"], "title": "Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles", "categories": ["math.OC", "cs.AI", "cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "This study explores the performance of a random Gaussian smoothing\nzeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly\nquasar-convex (SQC) functions in both unconstrained and constrained settings.\nFor the unconstrained problem, we establish the ZO algorithm's convergence to a\nglobal minimum along with its complexity when applied to both QC and SQC\nfunctions. For the constrained problem, we introduce the new notion of\nproximal-quasar-convexity and prove analogous results to the unconstrained\ncase. Specifically, we show the complexity bounds and the convergence of the\nalgorithm to a neighbourhood of a global minimum whose size can be controlled\nunder a variance reduction scheme. Theoretical findings are illustrated through\ninvestigating the performance of the algorithm applied to a range of problems\nin machine learning and optimisation. Specifically, we observe scenarios where\nthe ZO method outperforms gradient descent. We provide a possible explanation\nfor this phenomenon.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u968f\u673a\u9ad8\u65af\u5e73\u6ed1\u96f6\u9636\uff08ZO\uff09\u7b97\u6cd5\u5728\u6700\u5c0f\u5316\u62df\u661f\u51f8\uff08QC\uff09\u548c\u5f3a\u62df\u661f\u51f8\uff08SQC\uff09\u51fd\u6570\u65f6\u7684\u6027\u80fd\uff08\u5305\u62ec\u65e0\u7ea6\u675f\u548c\u6709\u7ea6\u675f\u60c5\u51b5\uff09\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e2d\u9a8c\u8bc1\u5176\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1ZO\u7b97\u6cd5\u5728\u62df\u661f\u51f8\u51fd\u6570\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u65e0\u68af\u5ea6\u7684\u590d\u6742\u6216\u9ad8\u7ef4\u95ee\u9898\u4e2d\uff0c\u63a2\u7d22\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u968f\u673a\u9ad8\u65af\u5e73\u6ed1\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u65b0\u6982\u5ff5\u201c\u8fd1\u7aef\u62df\u661f\u51f8\u6027\u201d\u5904\u7406\u7ea6\u675f\u95ee\u9898\uff0c\u5206\u6790\u5176\u6536\u655b\u6027\u548c\u590d\u6742\u5ea6\u3002", "result": "\u5728\u65e0\u7ea6\u675f\u548c\u6709\u7ea6\u675f\u95ee\u9898\u4e2d\uff0cZO\u7b97\u6cd5\u5747\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u6216\u53ef\u63a7\u90bb\u57df\uff0c\u4e14\u5728\u90e8\u5206\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u3002", "conclusion": "ZO\u7b97\u6cd5\u4e3a\u62df\u661f\u51f8\u51fd\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u5728\u65e0\u68af\u5ea6\u6216\u9ad8\u7ef4\u95ee\u9898\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.01859", "pdf": "https://arxiv.org/pdf/2505.01859", "abs": "https://arxiv.org/abs/2505.01859", "authors": ["Jiaqi Guo", "Chon Wai Ho", "Sumeetpal S. Singh"], "title": "Bayesian learning of the optimal action-value function in a Markov decision process", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": "66 pages", "summary": "The Markov Decision Process (MDP) is a popular framework for sequential\ndecision-making problems, and uncertainty quantification is an essential\ncomponent of it to learn optimal decision-making strategies. In particular, a\nBayesian framework is used to maintain beliefs about the optimal decisions and\nthe unknown ingredients of the model, which are also to be learned from the\ndata, such as the rewards and state dynamics. However, many existing Bayesian\napproaches for learning the optimal decision-making strategy are based on\nunrealistic modelling assumptions and utilise approximate inference techniques.\nThis raises doubts whether the benefits of Bayesian uncertainty quantification\nare fully realised or can be relied upon.\n  We focus on infinite-horizon and undiscounted MDPs, with finite state and\naction spaces, and a terminal state. We provide a full Bayesian framework, from\nmodelling to inference to decision-making. For modelling, we introduce a\nlikelihood function with minimal assumptions for learning the optimal\naction-value function based on Bellman's optimality equations, analyse its\nproperties, and clarify connections to existing works. For deterministic\nrewards, the likelihood is degenerate and we introduce artificial observation\nnoise to relax it, in a controlled manner, to facilitate more efficient Monte\nCarlo-based inference. For inference, we propose an adaptive sequential Monte\nCarlo algorithm to both sample from and adjust the sequence of relaxed\nposterior distributions. For decision-making, we choose actions using samples\nfrom the posterior distribution over the optimal strategies. While commonly\ndone, we provide new insight that clearly shows that it is a generalisation of\nThompson sampling from multi-arm bandit problems. Finally, we evaluate our\nframework on the Deep Sea benchmark problem and demonstrate the exploration\nbenefits of posterior sampling in MDPs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u65e0\u9650\u65f6\u95f4\u4e14\u65e0\u6298\u6263\u7684MDP\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u5efa\u6a21\u5230\u63a8\u7406\u518d\u5230\u51b3\u7b56\u7684\u5168\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u540e\u9a8c\u91c7\u6837\u5728\u63a2\u7d22\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\u65f6\u57fa\u4e8e\u4e0d\u73b0\u5b9e\u7684\u5efa\u6a21\u5047\u8bbe\uff0c\u5e76\u4f9d\u8d56\u8fd1\u4f3c\u63a8\u65ad\u6280\u672f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u4f18\u52bf\u672a\u5145\u5206\u53d1\u6325\u6216\u4e0d\u53ef\u9760\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u5c14\u66fc\u6700\u4f18\u6027\u65b9\u7a0b\u7684\u6700\u5c0f\u5047\u8bbe\u4f3c\u7136\u51fd\u6570\uff0c\u4e3a\u786e\u5b9a\u6027\u5956\u52b1\u5f15\u5165\u4eba\u5de5\u89c2\u6d4b\u566a\u58f0\u4ee5\u653e\u677e\u9000\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7684\u5e8f\u8d2f\u8499\u7279\u5361\u6d1b\u7b97\u6cd5\u8fdb\u884c\u63a8\u65ad\u3002", "result": "\u5728Deep Sea\u57fa\u51c6\u95ee\u9898\u4e0a\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u540e\u9a8c\u91c7\u6837\u5728MDP\u4e2d\u7684\u63a2\u7d22\u4f18\u52bf\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u63a8\u65ad\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4e3aMDP\u4e2d\u7684\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.01866", "pdf": "https://arxiv.org/pdf/2505.01866", "abs": "https://arxiv.org/abs/2505.01866", "authors": ["Daniel Commey", "Garth V. Crosby"], "title": "PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training while preserving\ndata privacy, but its classical cryptographic underpinnings are vulnerable to\nquantum attacks. This vulnerability is particularly critical in sensitive\ndomains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure\nBlockchain-based Federated Learning), a framework integrating post-quantum\ncryptography (PQC) with blockchain verification to secure FL against quantum\nadversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly\nDilithium) signatures to authenticate model updates and leverage optimized\nsmart contracts for decentralized validation. Extensive evaluations on diverse\ndatasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient\ncryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms)\nwith a fixed signature size of 3309 Bytes. Blockchain integration incurs a\nmanageable overhead, with average transaction times around 4.8 s and gas usage\nper update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the\ncryptographic overhead relative to transaction time remains minimal (around\n0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the\nbottleneck in blockchain-based FL. The system maintains competitive model\naccuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with\nround times showing sublinear growth with increasing client numbers. Our\nopen-source implementation and reproducible benchmarks validate the feasibility\nof deploying long-term, quantum-resistant security in practical FL systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86PQS-BFL\u6846\u67b6\uff0c\u7ed3\u5408\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u548c\u533a\u5757\u94fe\u6280\u672f\uff0c\u4ee5\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u514d\u53d7\u91cf\u5b50\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u52a0\u5bc6\u65b9\u6cd5\u6613\u53d7\u91cf\u5b50\u653b\u51fb\uff0c\u5c24\u5176\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u5b58\u5728\u91cd\u5927\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u6846\u67b6\u6765\u5e94\u5bf9\u91cf\u5b50\u8ba1\u7b97\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\uff08\u5982ML-DSA-65\u7b7e\u540d\uff09\u548c\u533a\u5757\u94fe\u9a8c\u8bc1\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u667a\u80fd\u5408\u7ea6\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPQS-BFL\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u52a0\u5bc6\u64cd\u4f5c\u9ad8\u6548\uff08\u7b7e\u540d\u65f6\u95f40.65ms\uff0c\u9a8c\u8bc1\u65f6\u95f40.53ms\uff09\uff0c\u533a\u5757\u94fe\u5f00\u9500\u53ef\u63a7\uff08\u4ea4\u6613\u65f6\u95f4\u7ea64.8s\uff09\uff0c\u6a21\u578b\u51c6\u786e\u7387\u4fdd\u6301\u9ad8\u6c34\u5e73\uff08\u5982MNIST\u8fbe98.8%\uff09\u3002", "conclusion": "PQS-BFL\u8bc1\u660e\u4e86\u540e\u91cf\u5b50\u5b89\u5168\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e14\u5728\u6027\u80fd\u548c\u5b89\u5168\u6027\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2505.02313", "pdf": "https://arxiv.org/pdf/2505.02313", "abs": "https://arxiv.org/abs/2505.02313", "authors": ["Jacqueline Harding", "Cameron Domenico Kirk-Giannini"], "title": "What Is AI Safety? What Do We Want It to Be?", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The field of AI safety seeks to prevent or reduce the harms caused by AI\nsystems. A simple and appealing account of what is distinctive of AI safety as\na field holds that this feature is constitutive: a research project falls\nwithin the purview of AI safety just in case it aims to prevent or reduce the\nharms caused by AI systems. Call this appealingly simple account The Safety\nConception of AI safety. Despite its simplicity and appeal, we argue that The\nSafety Conception is in tension with at least two trends in the ways AI safety\nresearchers and organizations think and talk about AI safety: first, a tendency\nto characterize the goal of AI safety research in terms of catastrophic risks\nfrom future systems; second, the increasingly popular idea that AI safety can\nbe thought of as a branch of safety engineering. Adopting the methodology of\nconceptual engineering, we argue that these trends are unfortunate: when we\nconsider what concept of AI safety it would be best to have, there are\ncompelling reasons to think that The Safety Conception is the answer.\nDescriptively, The Safety Conception allows us to see how work on topics that\nhave historically been treated as central to the field of AI safety is\ncontinuous with work on topics that have historically been treated as more\nmarginal, like bias, misinformation, and privacy. Normatively, taking The\nSafety Conception seriously means approaching all efforts to prevent or\nmitigate harms from AI systems based on their merits rather than drawing\narbitrary distinctions between them.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20AI\u5b89\u5168\u9886\u57df\u7684\u6838\u5fc3\u5728\u4e8e\u9884\u9632\u6216\u51cf\u5c11AI\u7cfb\u7edf\u9020\u6210\u7684\u5371\u5bb3\uff0c\u5e76\u6279\u8bc4\u4e86\u5f53\u524dAI\u5b89\u5168\u7814\u7a76\u4e2d\u8fc7\u5ea6\u5173\u6ce8\u672a\u6765\u7cfb\u7edf\u98ce\u9669\u548c\u5b89\u5168\u5de5\u7a0b\u5316\u7684\u503e\u5411\u3002", "motivation": "\u63a2\u8ba8AI\u5b89\u5168\u9886\u57df\u7684\u5b9a\u4e49\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u4e24\u79cd\u8d8b\u52bf\uff08\u5173\u6ce8\u672a\u6765\u98ce\u9669\u548c\u5de5\u7a0b\u5316\uff09\u4e0e\u7b80\u5355\u7684\u5b89\u5168\u5b9a\u4e49\uff08\u9884\u9632\u6216\u51cf\u5c11\u5371\u5bb3\uff09\u5b58\u5728\u77db\u76fe\uff0c\u5e76\u8bba\u8bc1\u540e\u8005\u66f4\u5177\u5408\u7406\u6027\u3002", "method": "\u91c7\u7528\u6982\u5ff5\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5206\u6790\u5f53\u524dAI\u5b89\u5168\u9886\u57df\u7684\u63cf\u8ff0\u6027\u8d8b\u52bf\uff08\u5982\u672a\u6765\u98ce\u9669\u548c\u5b89\u5168\u5de5\u7a0b\u5316\uff09\uff0c\u5e76\u4e0e\u89c4\u8303\u6027\u7684\u5b89\u5168\u5b9a\u4e49\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8bba\u8bc1\u4e86\u7b80\u5355\u7684\u5b89\u5168\u5b9a\u4e49\uff08\u9884\u9632\u6216\u51cf\u5c11\u5371\u5bb3\uff09\u66f4\u5177\u4f18\u52bf\uff0c\u56e0\u4e3a\u5b83\u80fd\u7edf\u4e00\u5386\u53f2\u6838\u5fc3\u8bae\u9898\uff08\u5982\u504f\u89c1\u3001\u9690\u79c1\uff09\u4e0e\u8fb9\u7f18\u8bae\u9898\uff0c\u5e76\u907f\u514d\u4eba\u4e3a\u5212\u5206\u3002", "conclusion": "\u5efa\u8bae\u91c7\u7528\u201c\u5b89\u5168\u6982\u5ff5\u201d\u4f5c\u4e3aAI\u5b89\u5168\u9886\u57df\u7684\u6838\u5fc3\u5b9a\u4e49\uff0c\u4ee5\u4fbf\u66f4\u516c\u5e73\u5730\u8bc4\u4f30\u6240\u6709\u9884\u9632AI\u5371\u5bb3\u7684\u52aa\u529b\u3002"}}
{"id": "2505.01917", "pdf": "https://arxiv.org/pdf/2505.01917", "abs": "https://arxiv.org/abs/2505.01917", "authors": ["Javier E. Santos", "Agnese Marcato", "Roman Colman", "Nicholas Lubbers", "Yen Ting Lin"], "title": "Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling", "categories": ["cs.GR", "cond-mat.mtrl-sci", "cs.LG", "eess.IV"], "comment": null, "summary": "Generative diffusion models have achieved remarkable success in producing\nhigh-quality images. However, because these models typically operate in\ncontinuous intensity spaces - diffusing independently per pixel and color\nchannel - they are fundamentally ill-suited for applications where quantities\nsuch as particle counts or material units are inherently discrete and governed\nby strict conservation laws such as mass preservation, limiting their\napplicability in scientific workflows. To address this limitation, we propose\nDiscrete Spatial Diffusion (DSD), a framework based on a continuous-time,\ndiscrete-state jump stochastic process that operates directly in discrete\nspatial domains while strictly preserving mass in both forward and reverse\ndiffusion processes. By using spatial diffusion to achieve mass preservation,\nwe introduce stochasticity naturally through a discrete formulation. We\ndemonstrate the expressive flexibility of DSD by performing image synthesis,\nclass conditioning, and image inpainting across widely-used image benchmarks,\nwith the ability to condition on image intensity. Additionally, we highlight\nits applicability to domain-specific scientific data for materials\nmicrostructure, bridging the gap between diffusion models and mass-conditioned\nscientific applications.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u79bb\u6563\u7a7a\u95f4\u548c\u4e25\u683c\u5b88\u6052\u91cf\uff08\u5982\u8d28\u91cf\uff09\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u79bb\u6563\u7a7a\u95f4\u6269\u6563\uff08DSD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u79bb\u6563\u72b6\u6001\u7684\u8df3\u8dc3\u968f\u673a\u8fc7\u7a0b\u76f4\u63a5\u5728\u79bb\u6563\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u5e76\u4fdd\u6301\u8d28\u91cf\u5b88\u6052\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u8fde\u7eed\u5f3a\u5ea6\u7a7a\u95f4\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u79bb\u6563\u4e14\u9700\u4e25\u683c\u5b88\u6052\uff08\u5982\u8d28\u91cf\uff09\u7684\u79d1\u5b66\u5e94\u7528\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fd9\u4e9b\u9886\u57df\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86DSD\u6846\u67b6\uff0c\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u79bb\u6563\u72b6\u6001\u7684\u8df3\u8dc3\u968f\u673a\u8fc7\u7a0b\uff0c\u76f4\u63a5\u5728\u79bb\u6563\u7a7a\u95f4\u64cd\u4f5c\uff0c\u786e\u4fdd\u4e86\u8d28\u91cf\u548c\u6269\u6563\u8fc7\u7a0b\u7684\u6b63\u9006\u5411\u4e25\u683c\u5b88\u6052\u3002", "result": "DSD\u5728\u56fe\u50cf\u5408\u6210\u3001\u7c7b\u522b\u6761\u4ef6\u548c\u4fee\u590d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u7075\u6d3b\u6027\uff0c\u5e76\u80fd\u5e94\u7528\u4e8e\u6750\u6599\u5fae\u89c2\u7ed3\u6784\u7b49\u79d1\u5b66\u6570\u636e\uff0c\u5f25\u5408\u4e86\u6269\u6563\u6a21\u578b\u4e0e\u79d1\u5b66\u5e94\u7528\u7684\u9e3f\u6c9f\u3002", "conclusion": "DSD\u6846\u67b6\u6269\u5c55\u4e86\u6269\u6563\u6a21\u578b\u7684\u9002\u7528\u6027\uff0c\u4e3a\u9700\u8981\u4e25\u683c\u8d28\u91cf\u5b88\u6052\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02314", "pdf": "https://arxiv.org/pdf/2505.02314", "abs": "https://arxiv.org/abs/2505.02314", "authors": ["James Read", "Ming-Yen Lee", "Wei-Hsing Huang", "Yuan-Chun Luo", "Anni Lu", "Shimeng Yu"], "title": "NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figures, 6 tables", "summary": "The exponential growth of artificial intelligence (AI) applications has\nexposed the inefficiency of conventional von Neumann architectures, where\nfrequent data transfers between compute units and memory create significant\nenergy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses\nthis challenge by performing multiply-accumulate (MAC) operations directly in\nthe memory arrays, substantially reducing data movement. However, designing\nrobust ACIM accelerators requires accurate modeling of device- and\ncircuit-level non-idealities. In this work, we present NeuroSim V1.5,\nintroducing several key advances: (1) seamless integration with TensorRT's\npost-training quantization flow enabling support for more neural networks\nincluding transformers, (2) a flexible noise injection methodology built on\npre-characterized statistical models, making it straightforward to incorporate\ndata from SPICE simulations or silicon measurements, (3) expanded device\nsupport including emerging non-volatile capacitive memories, and (4) up to 6.5x\nfaster runtime than NeuroSim V1.4 through optimized behavioral simulation. The\ncombination of these capabilities uniquely enables systematic design space\nexploration across both accuracy and hardware efficiency metrics. Through\nmultiple case studies, we demonstrate optimization of critical design\nparameters while maintaining network accuracy. By bridging high-fidelity noise\nmodeling with efficient simulation, NeuroSim V1.5 advances the design and\nvalidation of next-generation ACIM accelerators. All NeuroSim versions are\navailable open-source at https://github.com/neurosim/NeuroSim.", "AI": {"tldr": "NeuroSim V1.5 \u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u5185\u5b58\u5185\u8ba1\u7b97\uff08ACIM\uff09\u52a0\u901f\u5668\u7684\u5de5\u5177\uff0c\u901a\u8fc7TensorRT\u91cf\u5316\u652f\u6301\u66f4\u591a\u795e\u7ecf\u7f51\u7edc\uff0c\u4f18\u5316\u4e86\u566a\u58f0\u6ce8\u5165\u65b9\u6cd5\u5e76\u63d0\u5347\u4e86\u8fd0\u884c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u5728AI\u5e94\u7528\u4e2d\u5b58\u5728\u80fd\u6548\u548c\u5ef6\u8fdf\u95ee\u9898\uff0cACIM\u901a\u8fc7\u76f4\u63a5\u5185\u5b58\u8ba1\u7b97\u51cf\u5c11\u6570\u636e\u79fb\u52a8\uff0c\u4f46\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5668\u4ef6\u548c\u7535\u8def\u975e\u7406\u60f3\u6027\u5efa\u6a21\u3002", "method": "\u63d0\u51faNeuroSim V1.5\uff0c\u6574\u5408TensorRT\u91cf\u5316\u6d41\u7a0b\u3001\u566a\u58f0\u6ce8\u5165\u65b9\u6cd5\u3001\u652f\u6301\u65b0\u5174\u5b58\u50a8\u5668\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6a21\u62df\u63d0\u5347\u4e86\u8fd0\u884c\u901f\u5ea6\u3002", "result": "NeuroSim V1.5\u5b9e\u73b0\u4e86\u6700\u9ad86.5\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u652f\u6301\u8de8\u7cbe\u5ea6\u548c\u786c\u4ef6\u6548\u7387\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u3002", "conclusion": "NeuroSim V1.5\u901a\u8fc7\u9ad8\u4fdd\u771f\u566a\u58f0\u5efa\u6a21\u548c\u9ad8\u6548\u6a21\u62df\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3ACIM\u52a0\u901f\u5668\u7684\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\uff0c\u6240\u6709\u7248\u672c\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.01937", "pdf": "https://arxiv.org/pdf/2505.01937", "abs": "https://arxiv.org/abs/2505.01937", "authors": ["Yunbum Kook", "Santosh S. Vempala"], "title": "Faster logconcave sampling from a cold start in high dimension", "categories": ["cs.DS", "cs.LG", "math.FA", "math.ST", "stat.ML", "stat.TH"], "comment": "56 pages", "summary": "We present a faster algorithm to generate a warm start for sampling an\narbitrary logconcave density specified by an evaluation oracle, leading to the\nfirst sub-cubic sampling algorithms for inputs in (near-)isotropic position. A\nlong line of prior work incurred a warm-start penalty of at least linear in the\ndimension, hitting a cubic barrier, even for the special case of uniform\nsampling from convex bodies.\n  Our improvement relies on two key ingredients of independent interest. (1) We\nshow how to sample given a warm start in weaker notions of distance, in\nparticular $q$-R\\'enyi divergence for $q=\\widetilde{\\mathcal{O}}(1)$, whereas\nprevious analyses required stringent $\\infty$-R\\'enyi divergence (with the\nexception of Hit-and-Run, whose known mixing time is higher). This marks the\nfirst improvement in the required warmness since Lov\\'asz and Simonovits\n(1991). (2) We refine and generalize the log-Sobolev inequality of Lee and\nVempala (2018), originally established for isotropic logconcave distributions\nin terms of the diameter of the support, to logconcave distributions in terms\nof a geometric average of the support diameter and the largest eigenvalue of\nthe covariance matrix.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5feb\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u4efb\u610f\u5bf9\u6570\u51f9\u5bc6\u5ea6\u7684\u9884\u70ed\u542f\u52a8\u6837\u672c\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\uff08\u8fd1\uff09\u5404\u5411\u540c\u6027\u4f4d\u7f6e\u7684\u8f93\u5165\u4e0b\u7684\u4e9a\u7acb\u65b9\u91c7\u6837\u7b97\u6cd5\u3002\u4e4b\u524d\u7684\u7b97\u6cd5\u5728\u7ef4\u5ea6\u4e0a\u81f3\u5c11\u9700\u8981\u7ebf\u6027\u9884\u70ed\u60e9\u7f5a\uff0c\u800c\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u79cd\u5173\u952e\u6280\u672f\u6539\u8fdb\u4e86\u8fd9\u4e00\u9650\u5236\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u91c7\u6837\u5bf9\u6570\u51f9\u5bc6\u5ea6\u65f6\uff0c\u9884\u70ed\u542f\u52a8\u7684\u4ee3\u4ef7\u81f3\u5c11\u4e0e\u7ef4\u5ea6\u7ebf\u6027\u76f8\u5173\uff0c\u5373\u4f7f\u5728\u51f8\u4f53\u5747\u5300\u91c7\u6837\u8fd9\u4e00\u7279\u4f8b\u4e2d\u4e5f\u5b58\u5728\u7acb\u65b9\u969c\u788d\u3002\u672c\u7814\u7a76\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a(1) \u5728\u8f83\u5f31\u8ddd\u79bb\u6982\u5ff5\uff08\u5982$q$-R\u00e9nyi\u6563\u5ea6\uff09\u4e0b\u5b9e\u73b0\u91c7\u6837\uff1b(2) \u6539\u8fdb\u5e76\u63a8\u5e7f\u4e86Lee\u548cVempala (2018)\u7684\u5bf9\u6570Sobolev\u4e0d\u7b49\u5f0f\uff0c\u8003\u8651\u652f\u6301\u76f4\u5f84\u548c\u534f\u65b9\u5dee\u77e9\u9635\u6700\u5927\u7279\u5f81\u503c\u7684\u51e0\u4f55\u5e73\u5747\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u4e9a\u7acb\u65b9\u91c7\u6837\u7b97\u6cd5\uff0c\u964d\u4f4e\u4e86\u9884\u70ed\u542f\u52a8\u7684\u7ef4\u6570\u4f9d\u8d56\uff0c\u662f\u81eaLov\u00e1sz\u548cSimonovits (1991)\u4ee5\u6765\u5bf9\u9884\u70ed\u8981\u6c42\u7684\u9996\u6b21\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u5347\u4e86\u91c7\u6837\u6548\u7387\uff0c\u8fd8\u6269\u5c55\u4e86\u5bf9\u6570Sobolev\u4e0d\u7b49\u5f0f\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u5bf9\u6570\u51f9\u5bc6\u5ea6\u7684\u91c7\u6837\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u7406\u8bba\u5de5\u5177\u3002"}}
{"id": "2505.02347", "pdf": "https://arxiv.org/pdf/2505.02347", "abs": "https://arxiv.org/abs/2505.02347", "authors": ["Nilava Metya", "Arunesh Sinha"], "title": "Temporal Robustness in Discrete Time Linear Dynamical Systems", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "Discrete time linear dynamical systems, including Markov chains, have found\nmany applications. However, in some problems, there is uncertainty about the\ntime horizon for which the system runs. This creates uncertainty about the cost\n(or reward) incurred based on the state distribution when the system stops.\nGiven past data samples of how long a system ran, we propose to theoretically\nanalyze a distributional robust cost estimation task in a Wasserstein ambiguity\nset, instead of learning a probability distribution from a few samples. Towards\nthis, we show an equivalence between a discrete time Markov Chain on a\nprobability simplex and a global asymptotic stable (GAS) discrete time linear\ndynamical system, allowing us to base our study on a GAS system only. Then, we\nprovide various polynomial time algorithms and hardness results for different\ncases in our theoretical study, including a fundamental result about\nWasserstein distance based polytope.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u6563\u65f6\u95f4\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u65f6\u95f4\u8303\u56f4\u4e0b\u7684\u6210\u672c\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u6a21\u7cca\u96c6\u7684\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\uff0c\u5e76\u7ed9\u51fa\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u548c\u786c\u5ea6\u7ed3\u679c\u3002", "motivation": "\u7531\u4e8e\u7cfb\u7edf\u8fd0\u884c\u65f6\u95f4\u8303\u56f4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u6210\u672c\u6216\u5956\u52b1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5c06\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u7b49\u4ef7\u4e3a\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u7684\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\uff0c\u63d0\u51fa\u57fa\u4e8eWasserstein\u6a21\u7cca\u96c6\u7684\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8eWasserstein\u8ddd\u79bb\u591a\u9762\u4f53\u7684\u91cd\u8981\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0d\u786e\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u7684\u6210\u672c\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2505.01947", "pdf": "https://arxiv.org/pdf/2505.01947", "abs": "https://arxiv.org/abs/2505.01947", "authors": ["Ivan Tan", "Wei Minn", "Christopher M. Poskitt", "Lwin Khin Shar", "Lingxiao Jiang"], "title": "Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach", "categories": ["cs.SE", "cs.LG", "cs.RO"], "comment": "Accepted by the 29th International Conference on Engineering of\n  Complex Computer Systems (ICECCS 2025)", "summary": "UAVs, commonly referred to as drones, have witnessed a remarkable surge in\npopularity due to their versatile applications. These cyber-physical systems\ndepend on multiple sensor inputs, such as cameras, GPS receivers,\naccelerometers, and gyroscopes, with faults potentially leading to physical\ninstability and serious safety concerns. To mitigate such risks, anomaly\ndetection has emerged as a crucial safeguarding mechanism, capable of\nidentifying the physical manifestations of emerging issues and allowing\noperators to take preemptive action at runtime. Recent anomaly detection\nmethods based on LSTM neural networks have shown promising results, but three\nchallenges persist: the need for models that can generalise across the diverse\nmission profiles of drones; the need for interpretability, enabling operators\nto understand the nature of detected problems; and the need for capturing\ndomain knowledge that is difficult to infer solely from log data. Motivated by\nthese challenges, this paper introduces RADD, an integrated approach to anomaly\ndetection in drones that combines rule mining and unsupervised learning. In\nparticular, we leverage rules (or invariants) to capture expected relationships\nbetween sensors and actuators during missions, and utilise unsupervised\nlearning techniques to cover more subtle relationships that the rules may have\nmissed. We implement this approach using the ArduPilot drone software in the\nGazebo simulator, utilising 44 rules derived across the main phases of drone\nmissions, in conjunction with an ensemble of five unsupervised learning models.\nWe find that our integrated approach successfully detects 93.84% of anomalies\nover six types of faults with a low false positive rate (2.33%), and can be\ndeployed effectively at runtime. Furthermore, RADD outperforms a\nstate-of-the-art LSTM-based method in detecting the different types of faults\nevaluated in our study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c4\u5219\u6316\u6398\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5RADD\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LSTM\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9886\u57df\u77e5\u8bc6\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u65e0\u4eba\u673a\u4f9d\u8d56\u591a\u4f20\u611f\u5668\u8f93\u5165\uff0c\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709LSTM\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9886\u57df\u77e5\u8bc6\u6355\u6349\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "RADD\u7ed3\u5408\u89c4\u5219\u6316\u6398\uff08\u6355\u83b7\u4f20\u611f\u5668\u4e0e\u6267\u884c\u5668\u7684\u9884\u671f\u5173\u7cfb\uff09\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff08\u8986\u76d6\u89c4\u5219\u9057\u6f0f\u7684\u9690\u853d\u5173\u7cfb\uff09\uff0c\u5728ArduPilot\u548cGazebo\u4e2d\u5b9e\u73b0\u3002", "result": "RADD\u5728\u516d\u7c7b\u6545\u969c\u4e2d\u68c0\u6d4b\u7387\u8fbe93.84%\uff0c\u8bef\u62a5\u7387\u4ec52.33%\uff0c\u4f18\u4e8e\u73b0\u6709LSTM\u65b9\u6cd5\uff0c\u4e14\u53ef\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "RADD\u901a\u8fc7\u89c4\u5219\u4e0e\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.02352", "pdf": "https://arxiv.org/pdf/2505.02352", "abs": "https://arxiv.org/abs/2505.02352", "authors": ["Paramita Das", "Sai Keerthana Karnam", "Aditya Soni", "Animesh Mukherjee"], "title": "Social Biases in Knowledge Representations of Wikidata separates Global North from Global South", "categories": ["cs.IR", "cs.AI"], "comment": "10 pages", "summary": "Knowledge Graphs have become increasingly popular due to their wide usage in\nvarious downstream applications, including information retrieval, chatbot\ndevelopment, language model construction, and many others. Link prediction (LP)\nis a crucial downstream task for knowledge graphs, as it helps to address the\nproblem of the incompleteness of the knowledge graphs. However, previous\nresearch has shown that knowledge graphs, often created in a (semi) automatic\nmanner, are not free from social biases. These biases can have harmful effects\non downstream applications, especially by leading to unfair behavior toward\nminority groups. To understand this issue in detail, we develop a framework --\nAuditLP -- deploying fairness metrics to identify biased outcomes in LP,\nspecifically how occupations are classified as either male or female-dominated\nbased on gender as a sensitive attribute. We have experimented with the\nsensitive attribute of age and observed that occupations are categorized as\nyoung-biased, old-biased, and age-neutral. We conduct our experiments on a\nlarge number of knowledge triples that belong to 21 different geographies\nextracted from the open-sourced knowledge graph, Wikidata. Our study shows that\nthe variance in the biased outcomes across geographies neatly mirrors the\nsocio-economic and cultural division of the world, resulting in a transparent\npartition of the Global North from the Global South.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aAuditLP\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u91cd\u70b9\u5173\u6ce8\u6027\u522b\u548c\u5e74\u9f84\u4f5c\u4e3a\u654f\u611f\u5c5e\u6027\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u5728\uff08\u534a\uff09\u81ea\u52a8\u5316\u6784\u5efa\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5b58\u5728\u793e\u4f1a\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u5bf9\u5c11\u6570\u7fa4\u4f53\u4ea7\u751f\u4e0d\u516c\u5e73\u5f71\u54cd\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7AuditLP\u6846\u67b6\u91cf\u5316\u8fd9\u79cd\u504f\u89c1\u3002", "method": "\u5f00\u53d1AuditLP\u6846\u67b6\uff0c\u5229\u7528\u516c\u5e73\u6027\u6307\u6807\u5206\u6790\u6027\u522b\u548c\u5e74\u9f84\u5bf9\u804c\u4e1a\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u5b9e\u9a8c\u57fa\u4e8eWikidata\u768421\u4e2a\u5730\u7406\u533a\u57df\u7684\u77e5\u8bc6\u4e09\u5143\u7ec4\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u504f\u89c1\u7ed3\u679c\u7684\u5dee\u5f02\u53cd\u6620\u4e86\u5168\u7403\u793e\u4f1a\u7ecf\u6d4e\u548c\u6587\u5316\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5168\u7403\u5317\u65b9\u4e0e\u5168\u7403\u5357\u65b9\u7684\u660e\u663e\u533a\u5206\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u504f\u89c1\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u80cc\u666f\u5bc6\u5207\u76f8\u5173\uff0cAuditLP\u4e3a\u68c0\u6d4b\u548c\u91cf\u5316\u8fd9\u79cd\u504f\u89c1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.01985", "pdf": "https://arxiv.org/pdf/2505.01985", "abs": "https://arxiv.org/abs/2505.01985", "authors": ["Hung Pham", "Aiden Ren", "Ibrahim Tahir", "Jiatai Tong", "Thiago Serra"], "title": "Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate", "categories": ["math.OC", "cs.LG"], "comment": null, "summary": "We can approximate a constraint or an objective function that is uncertain or\nnonlinear with a neural network that we embed in the optimization model. This\napproach, which is known as constraint learning, faces the challenge that\noptimization models with neural network surrogates are harder to solve. Such\ndifficulties have motivated studies on model reformulation, specialized\noptimization algorithms, and - to a lesser extent - pruning of the embedded\nnetworks. In this work, we double down on the use of surrogates by applying\nnetwork pruning to produce a surrogate of the neural network itself. In the\ncontext of using a Mixed-Integer Linear Programming (MILP) solver to verify\nneural networks, we obtained faster adversarial perturbations for dense neural\nnetworks by using sparse surrogates, especially - and surprisingly - if not\ntaking the time to finetune the sparse network to make up for the loss in\naccuracy. In other words, we show that a pruned network with bad classification\nperformance can still be a good - and more efficient - surrogate.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u751f\u6210\u7a00\u758f\u4ee3\u7406\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4f18\u5316\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6c42\u89e3\u5668\u4e2d\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u7a00\u758f\u4ee3\u7406\u80fd\u66f4\u5feb\u751f\u6210\u5bf9\u6297\u6270\u52a8\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u5728\u4f18\u5316\u6a21\u578b\u4e2d\u8ba1\u7b97\u56f0\u96be\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u526a\u679d\u6280\u672f\u63d0\u5347\u6548\u7387\u7684\u6f5c\u529b\u3002", "method": "\u5e94\u7528\u7f51\u7edc\u526a\u679d\u751f\u6210\u7a00\u758f\u4ee3\u7406\u6a21\u578b\uff0c\u5c06\u5176\u5d4c\u5165MILP\u6c42\u89e3\u5668\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7a00\u758f\u4ee3\u7406\u6a21\u578b\u5373\u4f7f\u672a\u5fae\u8c03\uff08\u5206\u7c7b\u6027\u80fd\u8f83\u5dee\uff09\u4e5f\u80fd\u66f4\u9ad8\u6548\u5730\u751f\u6210\u5bf9\u6297\u6270\u52a8\u3002", "conclusion": "\u526a\u679d\u540e\u7684\u7f51\u7edc\u5373\u4f7f\u6027\u80fd\u4e0b\u964d\uff0c\u4ecd\u53ef\u4f5c\u4e3a\u9ad8\u6548\u4ee3\u7406\uff0c\u4e3a\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.02362", "pdf": "https://arxiv.org/pdf/2505.02362", "abs": "https://arxiv.org/abs/2505.02362", "authors": ["Ghazaleh SHirvani", "Saeid Ghasemshirazi"], "title": "Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Email spam detection is a critical task in modern communication systems,\nessential for maintaining productivity, security, and user experience.\nTraditional machine learning and deep learning approaches, while effective in\nstatic settings, face significant limitations in adapting to evolving spam\ntactics, addressing class imbalance, and managing data scarcity. These\nchallenges necessitate innovative approaches that reduce dependency on\nextensive labeled datasets and frequent retraining. This study investigates the\neffectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced\nNatural Language Processing (NLP) techniques such as BERT for email spam\ndetection. By employing BERT to preprocess and extract critical information\nfrom email content, and FLAN-T5 to classify emails in a Zero-Shot framework,\nthe proposed approach aims to address the limitations of traditional spam\ndetection systems. The integration of FLAN-T5 and BERT enables robust spam\ndetection without relying on extensive labeled datasets or frequent retraining,\nmaking it highly adaptable to unseen spam patterns and adversarial\nenvironments. This research highlights the potential of leveraging zero-shot\nlearning and NLPs for scalable and efficient spam detection, providing insights\ninto their capability to address the dynamic and challenging nature of spam\ndetection tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Zero-Shot\u5b66\u4e60\u548cNLP\u6280\u672f\uff08\u5982BERT\u548cFLAN-T5\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4b\u7535\u5b50\u90ae\u4ef6\u5783\u573e\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u9891\u7e41\u518d\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u5783\u573e\u90ae\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5728\u9002\u5e94\u53d8\u5316\u3001\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u521b\u65b0\u65b9\u6cd5\u4ee5\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u548c\u518d\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528BERT\u9884\u5904\u7406\u90ae\u4ef6\u5185\u5bb9\u5e76\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u7ed3\u5408FLAN-T5\u5728Zero-Shot\u6846\u67b6\u4e0b\u8fdb\u884c\u5206\u7c7b\uff0c\u5b9e\u73b0\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u9891\u7e41\u518d\u8bad\u7ec3\u7684\u5783\u573e\u90ae\u4ef6\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5c55\u73b0\u4e86\u5728\u4e0d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u9891\u7e41\u518d\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u672a\u77e5\u5783\u573e\u90ae\u4ef6\u6a21\u5f0f\u548c\u5bf9\u6297\u73af\u5883\u7684\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86Zero-Shot\u5b66\u4e60\u548cNLP\u6280\u672f\u5728\u52a8\u6001\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u5783\u573e\u90ae\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.01995", "pdf": "https://arxiv.org/pdf/2505.01995", "abs": "https://arxiv.org/abs/2505.01995", "authors": ["Sehwan Kim", "Faming Liang"], "title": "Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.CO", "stat.TH"], "comment": null, "summary": "Individual treatment effect estimation has gained significant attention in\nrecent data science literature. This work introduces the Double Neural Network\n(Double-NN) method to address this problem within the framework of extended\nfiducial inference (EFI). In the proposed method, deep neural networks are used\nto model the treatment and control effect functions, while an additional neural\nnetwork is employed to estimate their parameters. The universal approximation\ncapability of deep neural networks ensures the broad applicability of this\nmethod. Numerical results highlight the superior performance of the proposed\nDouble-NN method compared to the conformal quantile regression (CQR) method in\nindividual treatment effect estimation. From the perspective of statistical\ninference, this work advances the theory and methodology for statistical\ninference of large models. Specifically, it is theoretically proven that the\nproposed method permits the model size to increase with the sample size $n$ at\na rate of $O(n^{\\zeta})$ for some $0 \\leq \\zeta<1$, while still maintaining\nproper quantification of uncertainty in the model parameters. This result marks\na significant improvement compared to the range $0\\leq \\zeta < \\frac{1}{2}$\nrequired by the classical central limit theorem. Furthermore, this work\nprovides a rigorous framework for quantifying the uncertainty of deep neural\nnetworks under the neural scaling law, representing a substantial contribution\nto the statistical understanding of large-scale neural network models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Double-NN\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548cEFI\u6846\u67b6\u6765\u4f30\u8ba1\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\uff0c\u8868\u73b0\u4f18\u4e8eCQR\u65b9\u6cd5\uff0c\u5e76\u5728\u7406\u8bba\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\u3002", "motivation": "\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5982CQR\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u7edf\u8ba1\u63a8\u65ad\u7406\u8bba\u8fdb\u884c\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u5efa\u6a21\u5904\u7406\u548c\u5bf9\u7167\u7ec4\u6548\u5e94\uff0c\u7b2c\u4e09\u4e2a\u7f51\u7edc\u4f30\u8ba1\u53c2\u6570\uff0c\u4f9d\u6258EFI\u6846\u67b6\u4fdd\u969c\u9002\u7528\u6027\u3002", "result": "Double-NN\u8868\u73b0\u4f18\u4e8eCQR\uff0c\u7406\u8bba\u4e0a\u6a21\u578b\u89c4\u6a21\u53ef\u968f\u6837\u672c\u91cf\u4ee5$O(n^{\\zeta})$\u589e\u957f\uff08$0\\leq\\zeta<1$\uff09\uff0c\u7a81\u7834\u4e86\u7ecf\u5178\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u7684$\\zeta<\\frac{1}{2}$\u9650\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u4e25\u683c\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u7edf\u8ba1\u63a8\u65ad\u7406\u8bba\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.02010", "pdf": "https://arxiv.org/pdf/2505.02010", "abs": "https://arxiv.org/abs/2505.02010", "authors": ["Zeyuan Ma", "Zhiguang Cao", "Zhou Jiang", "Hongshu Guo", "Yue-Jiao Gong"], "title": "Meta-Black-Box-Optimization through Offline Q-function Learning", "categories": ["cs.NE", "cs.LG"], "comment": "Accepted as poster by ICML 2025", "summary": "Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated\nthat using RL to learn a meta-level policy for dynamic algorithm configuration\n(DAC) over an optimization task distribution could significantly enhance the\nperformance of the low-level BBO algorithm. However, the online learning\nparadigms in existing works makes the efficiency of MetaBBO problematic. To\naddress this, we propose an offline learning-based MetaBBO framework in this\npaper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO.\nSpecifically, we first transform DAC task into long-sequence decision process.\nThis allows us further introduce an effective Q-function decomposition\nmechanism to reduce the learning difficulty within the intricate algorithm\nconfiguration space. Under this setting, we propose three novel designs to\nmeta-learn DAC policy from offline data: we first propose a novel collection\nstrategy for constructing offline DAC experiences dataset with balanced\nexploration and exploitation. We then establish a decomposition-based Q-loss\nthat incorporates conservative Q-learning to promote stable offline learning\nfrom the offline dataset. To further improve the offline learning efficiency,\nwe equip our work with a Mamba architecture which helps long-sequence learning\neffectiveness and efficiency by selective state model and hardware-aware\nparallel scan respectively. Through extensive benchmarking, we observe that\nQ-Mamba achieves competitive or even superior performance to prior\nonline/offline baselines, while significantly improving the training efficiency\nof existing online baselines. We provide sourcecodes of Q-Mamba at\nhttps://github.com/MetaEvo/Q-Mamba.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u5b66\u4e60\u7684MetaBBO\u6846\u67b6Q-Mamba\uff0c\u901a\u8fc7\u5c06\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u4efb\u52a1\u8f6c\u5316\u4e3a\u957f\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408Q\u51fd\u6570\u5206\u89e3\u673a\u5236\u548cMamba\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684MetaBBO\u7814\u7a76\u4e3b\u8981\u91c7\u7528\u5728\u7ebf\u5b66\u4e60\u8303\u5f0f\uff0c\u5bfc\u81f4\u6548\u7387\u4e0d\u4f73\u3002\u4e3a\u4e86\u63d0\u9ad8\u6548\u679c\u548c\u6548\u7387\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u5c06\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u4efb\u52a1\u8f6c\u5316\u4e3a\u957f\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f15\u5165Q\u51fd\u6570\u5206\u89e3\u673a\u5236\u964d\u4f4e\u5b66\u4e60\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u8bbe\u8ba1\uff1a\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b56\u7565\u3001\u5206\u89e3\u5f0fQ-loss\u7ed3\u5408\u4fdd\u5b88Q\u5b66\u4e60\u3001Mamba\u67b6\u6784\u4f18\u5316\u957f\u5e8f\u5217\u5b66\u4e60\u3002", "result": "Q-Mamba\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u5728\u7ebf/\u79bb\u7ebf\u57fa\u7ebf\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "Q-Mamba\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MetaBBO\u7684\u6548\u7387\u548c\u6027\u80fd\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u7b97\u6cd5\u914d\u7f6e\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02370", "pdf": "https://arxiv.org/pdf/2505.02370", "abs": "https://arxiv.org/abs/2505.02370", "authors": ["Ming Li", "Xin Gu", "Fan Chen", "Xiaoying Xing", "Longyin Wen", "Chen Chen", "Sijie Zhu"], "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code, Data and Models are available at:\n  https://github.com/bytedance/SuperEdit", "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u66f4\u6709\u6548\u7684\u7f16\u8f91\u6307\u4ee4\uff08\u5305\u62ec\u4fee\u6b63\u6307\u4ee4\u548c\u5bf9\u6bd4\u6307\u4ee4\uff09\u6765\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u4efb\u52a1\u6216VLMs\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u7f16\u8f91\u6307\u4ee4\u4e0e\u56fe\u50cf\u5bf9\u4e0d\u5339\u914d\u800c\u5bfc\u81f4\u566a\u58f0\u76d1\u7763\u4fe1\u53f7\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u6307\u4ee4\u6765\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u6b65\u9aa4\u7684\u751f\u6210\u5c5e\u6027\uff0c\u5229\u7528\u4fee\u6b63\u6307\u4ee4\u548c\u5bf9\u6bd4\u6307\u4ee4\uff08\u901a\u8fc7\u4e09\u5143\u7ec4\u635f\u5931\u5f15\u5165\uff09\u63d0\u5347\u76d1\u7763\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f8b\u5982\u5728Real-Edit\u4e0a\u6bd4SOTA SmartEdit\u63d0\u53479.19%\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u5c1130\u500d\u3001\u6a21\u578b\u5c0f13\u500d\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u63a5\u9ad8\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u751f\u6210\u65b9\u5f0f\uff0c\u4e3a\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u9896\u3001\u7b80\u5355\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02019", "pdf": "https://arxiv.org/pdf/2505.02019", "abs": "https://arxiv.org/abs/2505.02019", "authors": ["Yuji Okamoto", "Tomoya Takeuchi", "Yusuke Sakemi"], "title": "Learning the Simplest Neural ODE", "categories": ["stat.ML", "cs.LG", "math.DS"], "comment": "Under review", "summary": "Since the advent of the ``Neural Ordinary Differential Equation (Neural\nODE)'' paper, learning ODEs with deep learning has been applied to system\nidentification, time-series forecasting, and related areas. Exploiting the\ndiffeomorphic nature of ODE solution maps, neural ODEs has also enabled their\nuse in generative modeling. Despite the rich potential to incorporate various\nkinds of physical information, training Neural ODEs remains challenging in\npractice. This study demonstrates, through the simplest one-dimensional linear\nmodel, why training Neural ODEs is difficult. We then propose a new\nstabilization method and provide an analytical convergence analysis. The\ninsights and techniques presented here serve as a concise tutorial for\nresearchers beginning work on Neural ODEs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u8bad\u7ec3\u795e\u7ecfODE\u7684\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5b9a\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u5206\u6790\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u7b80\u660e\u6559\u7a0b\u3002", "motivation": "\u795e\u7ecfODE\u5728\u7cfb\u7edf\u8bc6\u522b\u3001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u751f\u6210\u5efa\u6a21\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8bad\u7ec3\u56f0\u96be\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u4ee5\u6700\u7b80\u5355\u7684\u4e00\u7ef4\u7ebf\u6027\u6a21\u578b\u4e3a\u4f8b\uff0c\u5206\u6790\u4e86\u795e\u7ecfODE\u7684\u8bad\u7ec3\u96be\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5b9a\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6536\u655b\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u6240\u63d0\u7a33\u5b9a\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u795e\u7ecfODE\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2505.02056", "pdf": "https://arxiv.org/pdf/2505.02056", "abs": "https://arxiv.org/abs/2505.02056", "authors": ["Yuchen Wang", "Xuefeng Bai", "Xiucheng Li", "Weili Guan", "Liqiang Nie", "Xinyang Chen"], "title": "Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Adapting vision-language models (VLMs) to downstream tasks with pseudolabels\nhas gained increasing attention. A major obstacle is that the pseudolabels\ngenerated by VLMs tend to be imbalanced, leading to inferior performance. While\nexisting methods have explored various strategies to address this, the\nunderlying causes of imbalance remain insufficiently investigated. To fill this\ngap, we delve into imbalanced pseudolabels and identify two primary\ncontributing factors: concept mismatch and concept confusion. To mitigate these\ntwo issues, we propose a novel framework incorporating concept alignment and\nconfusion-aware calibrated margin mechanisms. The core of our approach lies in\nenhancing underperforming classes and promoting balanced predictions across\ncategories, thus mitigating imbalance. Extensive experiments on six benchmark\ndatasets with three learning paradigms demonstrate that the proposed method\neffectively enhances the accuracy and balance of pseudolabels, achieving a\nrelative improvement of 6.29% over the SoTA method. Our code is avaliable at\nhttps://anonymous.4open.science/r/CAP-C642/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u5ff5\u5bf9\u9f50\u548c\u6df7\u6dc6\u611f\u77e5\u6821\u51c6\u673a\u5236\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4f2a\u6807\u7b7e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4f2a\u6807\u7b7e\u4e0d\u5e73\u8861\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6df1\u5165\u63a2\u7a76\u5176\u6839\u672c\u539f\u56e0\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u6982\u5ff5\u5bf9\u9f50\u548c\u6df7\u6dc6\u611f\u77e5\u6821\u51c6\u673a\u5236\uff0c\u4f18\u5316\u4f4e\u6548\u7c7b\u522b\u5e76\u5e73\u8861\u9884\u6d4b\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u5bf9\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u6027\u80fd\u63d0\u53476.29%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u51c6\u786e\u6027\u548c\u5e73\u8861\u6027\u3002"}}
{"id": "2505.02388", "pdf": "https://arxiv.org/pdf/2505.02388", "abs": "https://arxiv.org/abs/2505.02388", "authors": ["Huangyue Yu", "Baoxiong Jia", "Yixin Chen", "Yandan Yang", "Puhao Li", "Rongpeng Su", "Jiaxin Li", "Qing Li", "Wei Liang", "Song-Chun Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025", "summary": "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to\neffectively support skill acquisition, sim-to-real transfer, and\ngeneralization. Achieving these quality standards, however, necessitates the\nprecise replication of real-world object diversity. Existing datasets\ndemonstrate that this process heavily relies on artist-driven designs, which\ndemand substantial human effort and present significant scalability challenges.\nTo scalably produce realistic and interactive 3D scenes, we first present\nMetaScenes, a large-scale, simulatable 3D scene dataset constructed from\nreal-world scans, which includes 15366 objects spanning 831 fine-grained\ncategories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,\nwhich enables the automated, high-quality replacement of assets, thereby\neliminating the reliance on artist-driven designs for scaling 3D scenes. We\nfurther propose two benchmarks to evaluate MetaScenes: a detailed scene\nsynthesis task focused on small item layouts for robotic manipulation and a\ndomain transfer task in vision-and-language navigation (VLN) to validate\ncross-domain transfer. Results confirm MetaScene's potential to enhance EAI by\nsupporting more generalizable agent learning and sim-to-real applications,\nintroducing new possibilities for EAI research. Project website:\nhttps://meta-scenes.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MetaScenes\u6570\u636e\u96c6\u548cScan2Sim\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u76843D\u573a\u666f\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u6570\u636e\u96c6\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\uff0c\u96be\u4ee5\u89c4\u6a21\u5316\u4e14\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u771f\u5b9e\u626b\u63cf\u6570\u636e\u81ea\u52a8\u5316\u751f\u6210\u4eff\u771f\u573a\u666f\uff0c\u63d0\u5347Embodied AI\u7814\u7a76\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u6784\u5efaMetaScenes\u6570\u636e\u96c6\uff08\u6765\u81ea\u771f\u5b9e\u626b\u63cf\u768415366\u4e2a\u7269\u4f53\uff0c831\u7c7b\uff09\uff1b2. \u63d0\u51faScan2Sim\u6a21\u578b\u5b9e\u73b0\u8d44\u4ea7\u81ea\u52a8\u66ff\u6362\uff1b3. \u8bbe\u8ba1\u4e24\u4e2a\u8bc4\u6d4b\u4efb\u52a1\uff08\u5c0f\u7269\u4f53\u5e03\u5c40\u5408\u6210\u548c\u8de8\u9886\u57dfVLN\u5bfc\u822a\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eMetaScenes\u80fd\u652f\u6301\u66f4\u901a\u7528\u7684\u667a\u80fd\u4f53\u5b66\u4e60\u548c\u4eff\u771f\u8fc1\u79fb\u5e94\u7528\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "MetaScenes\u548cScan2Sim\u4e3aEmbodied AI\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u573a\u666f\u751f\u6210\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u7814\u7a76\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.02071", "pdf": "https://arxiv.org/pdf/2505.02071", "abs": "https://arxiv.org/abs/2505.02071", "authors": ["Can K\u00fc\u00e7\u00fcks\u00f6zen", "Y\u00fccel Yemez"], "title": "Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "We propose the Compact Clustering Attention (COCA) layer, an effective\nbuilding block that introduces a hierarchical strategy for object-centric\nrepresentation learning, while solving the unsupervised object discovery task\non single images. COCA is an attention-based clustering module capable of\nextracting object-centric representations from multi-object scenes, when\ncascaded into a bottom-up hierarchical network architecture, referred to as\nCOCA-Net. At its core, COCA utilizes a novel clustering algorithm that\nleverages the physical concept of compactness, to highlight distinct object\ncentroids in a scene, providing a spatial inductive bias. Thanks to this\nstrategy, COCA-Net generates high-quality segmentation masks on both the\ndecoder side and, notably, the encoder side of its pipeline. Additionally,\nCOCA-Net is not bound by a predetermined number of object masks that it\ngenerates and handles the segmentation of background elements better than its\ncompetitors. We demonstrate COCA-Net's segmentation performance on six widely\nadopted datasets, achieving superior or competitive results against the\nstate-of-the-art models across nine different evaluation metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOCA\u7684\u7d27\u51d1\u805a\u7c7b\u6ce8\u610f\u529b\u5c42\uff0c\u7ed3\u5408\u81ea\u4e0b\u800c\u4e0a\u7684\u5c42\u6b21\u7f51\u7edc\u67b6\u6784COCA-Net\uff0c\u80fd\u591f\u4ece\u591a\u76ee\u6807\u573a\u666f\u4e2d\u63d0\u53d6\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8868\u793a\uff0c\u5e76\u5728\u5355\u56fe\u50cf\u4e0a\u8fdb\u884c\u65e0\u76d1\u7763\u5bf9\u8c61\u53d1\u73b0\u4efb\u52a1\u3002\u901a\u8fc7\u5229\u7528\u7d27\u51d1\u6027\u6982\u5ff5\u4f18\u5316\u805a\u7c7b\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u6216\u7ade\u4e89\u529b\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5355\u56fe\u50cf\u65e0\u76d1\u7763\u5bf9\u8c61\u53d1\u73b0\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5b9a\u4f4d\u5e76\u5206\u5272\u5bf9\u8c61\u7684\u65b9\u6cd5\uff0c\u5e76\u514b\u670d\u73b0\u6709\u7b97\u6cd5\u5728\u80cc\u666f\u5206\u5272\u548c\u5bf9\u8c61\u6570\u91cf\u9650\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCompact Clustering Attention (COCA)\u5c42\uff0c\u5229\u7528\u7d27\u51d1\u6027\u805a\u7c7b\u7b97\u6cd5\u5bf9\u76ee\u6807\u91cd\u5fc3\u8fdb\u884c\u7a7a\u95f4\u5f15\u5bfc\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u7684COCA-Net\u67b6\u6784\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u76ee\u6807\u4e2d\u5fc3\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u6a21\u578b\u5728\u4e5d\u79cd\u8bc4\u4f30\u6307\u6807\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u53ef\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\uff0c\u5c24\u5176\u5728\u80cc\u666f\u5206\u5272\u548c\u52a8\u6001\u8c03\u6574\u751f\u6210\u63a9\u7801\u6570\u91cf\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "COCA-Net\u901a\u8fc7\u7d27\u51d1\u6027\u805a\u7c7b\u548c\u5c42\u6b21\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u5bf9\u8c61\u53d1\u73b0\u7684\u6548\u679c\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7684\u76ee\u6807\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.02396", "pdf": "https://arxiv.org/pdf/2505.02396", "abs": "https://arxiv.org/abs/2505.02396", "authors": ["Kennard Norbert Sudiardjo", "Islam Nur Alam", "Wilson Wijaya", "Lili Ayu Wulandhari"], "title": "Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pneumonia Diagnosis, though it is crucial for an effective treatment, it can\nbe hampered by uncertainty. This uncertainty starts to arise due to some\nfactors like atypical presentations, limitations of diagnostic tools such as\nchest X-rays, and the presence of co-existing respiratory conditions. This\nresearch proposes one of the supervised learning methods, CNN. Using\nMobileNetV2 as the pre-trained one with ResNet101V2 architecture and using\nKeras API as the built from scratch model, for identifying lung diseases\nespecially pneumonia. The datasets used in this research were obtained from the\nwebsite through Kaggle. The result shows that by implementing CNN MobileNetV2\nand CNN from scratch the result is promising. While validating data,\nMobileNetV2 performs with stability and minimal overfitting, while the training\naccuracy increased to 84.87% later it slightly decreased to 78.95%, with\nincreasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is\nmore stable. Although it takes more time to train each epoch. Meanwhile, after\nthe 10th epoch, the Scratch model displayed more instability and overfitting\ndespite having higher validation accuracy, training accuracy decreased\nsignificantly to 78.12% and the validation loss increased from 0.5698 to\n1.1809. With these results, ResNet101V2 offers stability, and the Scratch model\noffers high accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528CNN MobileNetV2\u548c\u4ece\u5934\u6784\u5efa\u7684CNN\u6a21\u578b\u8bca\u65ad\u80ba\u708e\uff0c\u7ed3\u679c\u663e\u793aMobileNetV2\u7a33\u5b9a\u6027\u8f83\u597d\u4f46\u8bad\u7ec3\u65f6\u95f4\u8f83\u957f\uff0c\u800c\u4ece\u5934\u6784\u5efa\u7684\u6a21\u578b\u867d\u7cbe\u5ea6\u8f83\u9ad8\u4f46\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u80ba\u708e\u8bca\u65ad\u56e0\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u975e\u5178\u578b\u8868\u73b0\u548c\u8bca\u65ad\u5de5\u5177\u9650\u5236\uff09\u800c\u590d\u6742\u5316\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528MobileNetV2\u9884\u8bad\u7ec3\u6a21\u578b\u4e0eResNet101V2\u67b6\u6784\uff0c\u5e76\u4ece\u5934\u6784\u5efaKeras\u6a21\u578b\uff0c\u901a\u8fc7Kaggle\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "result": "MobileNetV2\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe84.87%\uff0c\u4f46\u7565\u6709\u4e0b\u964d\u81f378.95%\uff0c\u7a33\u5b9a\u6027\u66f4\u4f73\uff1b\u4ece\u5934\u6a21\u578b\u7cbe\u5ea6\u66f4\u9ad8\u4f46\u8fc7\u62df\u5408\u660e\u663e\u3002", "conclusion": "MobileNetV2\u9002\u5408\u7a33\u5b9a\u6027\u9700\u6c42\u573a\u666f\uff0c\u4ece\u5934\u6784\u5efa\u6a21\u578b\u9002\u7528\u4e8e\u7cbe\u5ea6\u4f18\u5148\u4f46\u9700\u6ce8\u610f\u8fc7\u62df\u5408\u95ee\u9898\u3002"}}
{"id": "2505.02101", "pdf": "https://arxiv.org/pdf/2505.02101", "abs": "https://arxiv.org/abs/2505.02101", "authors": ["Youran Dong", "Junfeng Yang", "Wei Yao", "Jin Zhang"], "title": "Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization", "categories": ["math.OC", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Bilevel optimization is a powerful tool for many machine learning problems,\nsuch as hyperparameter optimization and meta-learning. Estimating\nhypergradients (also known as implicit gradients) is crucial for developing\ngradient-based methods for bilevel optimization. In this work, we propose a\ncomputationally efficient technique for incorporating curvature information\ninto the approximation of hypergradients and present a novel algorithmic\nframework based on the resulting enhanced hypergradient computation. We provide\nconvergence rate guarantees for the proposed framework in both deterministic\nand stochastic scenarios, particularly showing improved computational\ncomplexity over popular gradient-based methods in the deterministic setting.\nThis improvement in complexity arises from a careful exploitation of the\nhypergradient structure and the inexact Newton method. In addition to the\ntheoretical speedup, numerical experiments demonstrate the significant\npractical performance benefits of incorporating curvature information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u5c42\u4f18\u5316\u8d85\u68af\u5ea6\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5229\u7528\u66f2\u7387\u4fe1\u606f\u6539\u8fdb\u8d85\u68af\u5ea6\u8ba1\u7b97\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u53cc\u5c42\u4f18\u5316\u4e2d\u8d85\u68af\u5ea6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u68af\u5ea6\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u66f2\u7387\u4fe1\u606f\u7684\u8d85\u68af\u5ea6\u8fd1\u4f3c\u6280\u672f\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u65b0\u7b97\u6cd5\u6846\u67b6\uff0c\u5229\u7528\u4e86\u8d85\u68af\u5ea6\u7ed3\u6784\u548c\u4e0d\u7cbe\u786e\u725b\u987f\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u7b97\u6cd5\u5728\u786e\u5b9a\u6027\u548c\u968f\u673a\u573a\u666f\u4e0b\u7684\u6536\u655b\u7387\uff0c\u5e76\u5728\u786e\u5b9a\u6027\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u4f18\u4e8e\u5e38\u89c1\u68af\u5ea6\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u7406\u8bba\u4e0a\u6709\u4f18\u52bf\uff0c\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86\u5176\u663e\u8457\u7684\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2505.02173", "pdf": "https://arxiv.org/pdf/2505.02173", "abs": "https://arxiv.org/abs/2505.02173", "authors": ["Chutiphan Charoensuk", "Nathakhun Wiroonsri"], "title": "Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering", "categories": ["stat.ML", "cs.LG"], "comment": "17 pages", "summary": "Time series clustering is an unsupervised learning method for classifying\ntime series data into groups with similar behavior. It is used in applications\nsuch as healthcare, finance, economics, energy, and climate science. Several\ntime series clustering methods have been introduced and used for over four\ndecades. Most of them focus on measuring either Euclidean distances or\nassociation dissimilarities between time series. In this work, we propose a new\ndissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),\nwhich combines a weighted average of a specified fraction of the largest\nelement-wise differences with the well-known Pearson correlation dissimilarity.\nIt is incorporated into hierarchical clustering. The performance is evaluated\nand compared with existing clustering algorithms. The results show that the\nRDPC algorithm outperforms others in complicated cases involving different\nseasonal patterns, trends, and peaks. Finally, we demonstrate our method by\nclustering a random sample of customers from a Thai electricity consumption\ntime series dataset into seven groups with unique characteristics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u65b9\u6cd5RDPC\uff0c\u7ed3\u5408\u4e86\u6392\u540d\u76ae\u5c14\u900a\u76f8\u5173\u4e0d\u76f8\u4f3c\u5ea6\u4e0e\u5c42\u6b21\u805a\u7c7b\uff0c\u5e76\u5728\u590d\u6742\u60c5\u51b5\u4e0b\uff08\u5982\u4e0d\u540c\u5b63\u8282\u6027\u3001\u8d8b\u52bf\u548c\u5cf0\u503c\uff09\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u5728\u591a\u4e2a\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce8\u6b27\u6c0f\u8ddd\u79bb\u6216\u5173\u8054\u4e0d\u76f8\u4f3c\u5ea6\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faRDPC\u4e0d\u76f8\u4f3c\u5ea6\u5ea6\u91cf\uff0c\u7ed3\u5408\u6700\u5927\u5143\u7d20\u5dee\u5f02\u7684\u52a0\u6743\u5e73\u5747\u503c\u4e0e\u76ae\u5c14\u900a\u76f8\u5173\u4e0d\u76f8\u4f3c\u5ea6\uff0c\u5e76\u878d\u5165\u5c42\u6b21\u805a\u7c7b\u3002", "result": "RDPC\u5728\u590d\u6742\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u6cf0\u56fd\u7535\u529b\u6d88\u8d39\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u6210\u529f\u5c06\u7528\u6237\u5206\u4e3a7\u4e2a\u72ec\u7279\u7fa4\u4f53\u3002", "conclusion": "RDPC\u662f\u4e00\u79cd\u6709\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u6a21\u5f0f\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2505.02185", "pdf": "https://arxiv.org/pdf/2505.02185", "abs": "https://arxiv.org/abs/2505.02185", "authors": ["Thomas Y. L. Lin", "Jerry Yao-Chieh Hu", "Paul W. Chiou", "Peter Lin"], "title": "Latent Variable Estimation in Bayesian Black-Litterman Models", "categories": ["q-fin.PM", "cs.LG", "econ.EM", "stat.ME", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its\nreliance on subjective investor views. Classical BL requires an investor\n\"view\": a forecast vector $q$ and its uncertainty matrix $\\Omega$ that describe\nhow much a chosen portfolio should outperform the market. Our key idea is to\ntreat $(q,\\Omega)$ as latent variables and learn them from market data within a\nsingle Bayesian network. Consequently, the resulting posterior estimation\nadmits closed-form expression, enabling fast inference and stable portfolio\nweights. Building on these, we propose two mechanisms to capture how features\ninteract with returns: shared-latent parametrization and feature-influenced\nviews; both recover classical BL and Markowitz portfolios as special cases.\nEmpirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve\nSharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the\nindex baselines. This work turns BL into a fully data-driven, view-free, and\ncoherent Bayesian framework for portfolio optimization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u4e3b\u89c2\u6295\u8d44\u8005\u89c2\u70b9\u7684\u8d1d\u53f6\u65afBlack-Litterman\uff08BL\uff09\u6295\u8d44\u7ec4\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u89c2\u70b9\u53c2\u6570\u89c6\u4e3a\u6f5c\u5728\u53d8\u91cf\u5e76\u4ece\u5e02\u573a\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u65ad\u548c\u7a33\u5b9a\u7684\u6743\u91cd\u5206\u914d\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u5176Sharpe\u6bd4\u7387\u63d0\u5347\u4e8650%\uff0c\u6362\u624b\u7387\u964d\u4f4e\u4e8655%\u3002", "motivation": "\u4f20\u7edfBL\u6a21\u578b\u9700\u8981\u4e3b\u89c2\u7684\u6295\u8d44\u8005\u89c2\u70b9\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u6570\u636e\u9a71\u52a8\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u4f9d\u8d56\uff0c\u4f7fBL\u6a21\u578b\u5b8c\u5168\u57fa\u4e8e\u6570\u636e\uff0c\u63d0\u9ad8\u5b9e\u7528\u6027\u548c\u7a33\u5065\u6027\u3002", "method": "\u901a\u8fc7\u5c06\u89c2\u70b9\u53c2\u6570$(q,\\Omega)$\u89c6\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5728\u4e00\u4e2a\u8d1d\u53f6\u65af\u7f51\u7edc\u4e2d\u4ece\u5e02\u573a\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u673a\u5236\uff08\u5171\u4eab\u6f5c\u5728\u53c2\u6570\u5316\u548c\u7279\u5f81\u5f71\u54cd\u89c2\u70b9\uff09\u6765\u6355\u6349\u7279\u5f81\u4e0e\u6536\u76ca\u7684\u4ea4\u4e92\u3002", "result": "\u572830\u5e74\u9053\u743c\u65af\u548c20\u5e74\u884c\u4e1aETF\u6570\u636e\u4e0a\uff0cSharpe\u6bd4\u7387\u63d0\u534750%\uff0c\u6362\u624b\u7387\u964d\u4f4e55%\uff0c\u8868\u73b0\u4f18\u4e8eMarkowitz\u548c\u57fa\u51c6\u6307\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06BL\u6a21\u578b\u8f6c\u5316\u4e3a\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u3001\u65e0\u89c2\u70b9\u4f9d\u8d56\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4e3a\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.02467", "pdf": "https://arxiv.org/pdf/2505.02467", "abs": "https://arxiv.org/abs/2505.02467", "authors": ["Valerio Guarrasi", "Klara Mogensen", "Sara Tassinari", "Sara Qvarlander", "Paolo Soda"], "title": "Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal deep learning harnesses diverse imaging modalities, such as MRI\nsequences, to enhance diagnostic accuracy in medical imaging. A key challenge\nis determining the optimal timing for integrating these\nmodalities-specifically, identifying the network layers where fusion modules\nshould be inserted. Current approaches often rely on manual tuning or\nexhaustive search, which are computationally expensive without any guarantee of\nconverging to optimal results. We propose a sequential forward search algorithm\nthat incrementally activates and evaluates candidate fusion modules at\ndifferent layers of a multimodal network. At each step, the algorithm retrains\nfrom previously learned weights and compares validation loss to identify the\nbest-performing configuration. This process systematically reduces the search\nspace, enabling efficient identification of the optimal fusion timing without\nexhaustively testing all possible module placements. The approach is validated\non two multimodal MRI datasets, each addressing different classification tasks.\nOur algorithm consistently identified configurations that outperformed unimodal\nbaselines, late fusion, and a brute-force ensemble of all potential fusion\nplacements. These architectures demonstrated superior accuracy, F-score, and\nspecificity while maintaining competitive or improved AUC values. Furthermore,\nthe sequential nature of the search significantly reduced computational\noverhead, making the optimization process more practical. By systematically\ndetermining the optimal timing to fuse imaging modalities, our method advances\nmultimodal deep learning for medical imaging. It provides an efficient and\nrobust framework for fusion optimization, paving the way for improved clinical\ndecision-making and more adaptable, scalable architectures in medical AI\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u987a\u5e8f\u524d\u5411\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u591a\u6a21\u6001\u7f51\u7edc\u4e2d\u786e\u5b9a\u6700\u4f73\u878d\u5408\u65f6\u673a\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u4e24\u4e2aMRI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u878d\u5408\u65f6\u673a\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u6216\u7a77\u4e3e\u641c\u7d22\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u4e00\u5b9a\u80fd\u627e\u5230\u6700\u4f18\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u548c\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u987a\u5e8f\u524d\u5411\u641c\u7d22\u7b97\u6cd5\u9010\u6b65\u6fc0\u6d3b\u548c\u8bc4\u4f30\u5019\u9009\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u91cd\u65b0\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\u6bd4\u8f83\uff0c\u7cfb\u7edf\u6027\u7f29\u5c0f\u641c\u7d22\u7a7a\u95f4\u4ee5\u627e\u5230\u6700\u4f73\u878d\u5408\u914d\u7f6e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2aMRI\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u3001\u665a\u671f\u878d\u5408\u53ca\u7a77\u4e3e\u878d\u5408\u914d\u7f6e\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7a33\u5065\u7684\u878d\u5408\u4f18\u5316\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u548c\u6269\u5c55AI\u67b6\u6784\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2505.02224", "pdf": "https://arxiv.org/pdf/2505.02224", "abs": "https://arxiv.org/abs/2505.02224", "authors": ["Andrew Quijano", "Spyros T. Halkidis", "Kevin Gallagher", "Kemal Akkaya", "Nikolaos Samaras"], "title": "Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "A decision tree is an easy-to-understand tool that has been widely used for\nclassification tasks. On the one hand, due to privacy concerns, there has been\nan urgent need to create privacy-preserving classifiers that conceal the user's\ninput from the classifier. On the other hand, with the rise of cloud computing,\ndata owners are keen to reduce risk by outsourcing their model, but want\nsecurity guarantees that third parties cannot steal their decision tree model.\nTo address these issues, Joye and Salehi introduced a theoretical protocol that\nefficiently evaluates decision trees while maintaining privacy by leveraging\ntheir comparison protocol that is resistant to timing attacks. However, their\napproach was not only inefficient but also prone to side-channel attacks.\nTherefore, in this paper, we propose a new decision tree inference protocol in\nwhich the model is shared and evaluated among multiple entities. We partition\nour decision tree model by each level to be stored in a new entity we refer to\nas a \"level-site.\" Utilizing this approach, we were able to gain improved\naverage run time for classifier evaluation for a non-complete tree, while also\nhaving strong mitigations against side-channel attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51b3\u7b56\u6811\u63a8\u7406\u534f\u8bae\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u5206\u5c42\u5b58\u50a8\u5728\u4e0d\u540c\u7684\u5b9e\u4f53\u4e2d\uff08\u79f0\u4e3a\u201clevel-site\u201d\uff09\uff0c\u63d0\u9ad8\u4e86\u975e\u5b8c\u5168\u6811\u7684\u5206\u7c7b\u5668\u8bc4\u4f30\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u4e86\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u9632\u5fa1\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u548c\u4e91\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e00\u4e2a\u65e2\u80fd\u4fdd\u62a4\u7528\u6237\u8f93\u5165\u9690\u79c1\uff0c\u53c8\u80fd\u9632\u6b62\u7b2c\u4e09\u65b9\u7a83\u53d6\u51b3\u7b56\u6811\u6a21\u578b\u7684\u5206\u7c7b\u5668\u3002", "method": "\u534f\u8bae\u5c06\u51b3\u7b56\u6811\u6a21\u578b\u6309\u5c42\u7ea7\u5212\u5206\u4e3a\u591a\u4e2a\u90e8\u5206\uff0c\u5206\u522b\u5b58\u50a8\u5728\u65b0\u7684\u201clevel-site\u201d\u5b9e\u4f53\u4e2d\uff0c\u4ece\u800c\u4f18\u5316\u8bc4\u4f30\u6548\u7387\u5e76\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u975e\u5b8c\u5168\u6811\u7684\u5206\u7c7b\u5668\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u4e14\u6709\u6548\u7f13\u89e3\u4e86\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u98ce\u9669\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u5b58\u50a8\u548c\u8bc4\u4f30\u534f\u8bae\u5728\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u548c\u4e91\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u6811\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.02483", "pdf": "https://arxiv.org/pdf/2505.02483", "abs": "https://arxiv.org/abs/2505.02483", "authors": ["Changxin Huang", "Junyang Liang", "Yanbin Chang", "Jingzhao Xu", "Jianqiang Li"], "title": "Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Enabling a high-degree-of-freedom robot to learn specific skills is a\nchallenging task due to the complexity of robotic dynamics. Reinforcement\nlearning (RL) has emerged as a promising solution; however, addressing such\nproblems requires the design of multiple reward functions to account for\nvarious constraints in robotic motion. Existing approaches typically sum all\nreward components indiscriminately to optimize the RL value function and\npolicy. We argue that this uniform inclusion of all reward components in policy\noptimization is inefficient and limits the robot's learning performance. To\naddress this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework\nbased on Large Language Models (LLMs). This paradigm dynamically adjusts the\nlearning intensity of each reward component throughout the policy optimization\nprocess, enabling robots to acquire skills in a gradual and structured manner.\nSpecifically, we design a multi-branch value network, where each branch\ncorresponds to a distinct reward component. During policy optimization, each\nbranch is assigned a weight that reflects its importance, and these weights are\nautomatically computed based on rules designed by LLMs. The LLM generates a\nrule set in advance, derived from the task description, and during training, it\nselects a weight calculation rule from the library based on language prompts\nthat evaluate the performance of each branch. Experimental results demonstrate\nthat the AHRS method achieves an average 6.48% performance improvement across\nmultiple high-degree-of-freedom robotic tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u6df7\u5408\u5956\u52b1\u8c03\u5ea6\u6846\u67b6\uff08AHRS\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5956\u52b1\u7ec4\u4ef6\u7684\u5b66\u4e60\u5f3a\u5ea6\uff0c\u63d0\u5347\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u5b66\u4e60\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u73b0\u5e73\u5747\u63d0\u53476.48%\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf9\u6240\u6709\u5956\u52b1\u7ec4\u4ef6\u65e0\u5dee\u522b\u6c42\u548c\uff0c\u6548\u7387\u4f4e\u4e14\u9650\u5236\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u591a\u5206\u652f\u4ef7\u503c\u7f51\u7edc\uff0c\u5404\u5206\u652f\u5bf9\u5e94\u4e0d\u540c\u5956\u52b1\u7ec4\u4ef6\uff0c\u5e76\u57fa\u4e8eLLM\u751f\u6210\u7684\u89c4\u5219\u5e93\u52a8\u6001\u8ba1\u7b97\u6743\u91cd\u3002", "result": "AHRS\u65b9\u6cd5\u5728\u591a\u4e2a\u9ad8\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53476.48%\u3002", "conclusion": "\u52a8\u6001\u5956\u52b1\u8c03\u5ea6\u80fd\u66f4\u9ad8\u6548\u5730\u6307\u5bfc\u673a\u5668\u4eba\u5206\u9636\u6bb5\u5b66\u4e60\u6280\u80fd\u3002"}}
{"id": "2505.02485", "pdf": "https://arxiv.org/pdf/2505.02485", "abs": "https://arxiv.org/abs/2505.02485", "authors": ["Lucas Kletzander", "Tommaso Mannelli Mazzoli", "Nysret Musliu", "Pascal Van Hentenryck"], "title": "Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints", "categories": ["math.OC", "cs.AI"], "comment": null, "summary": "The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization\nproblem with the goal to design shifts to cover prearranged bus tours. The\nobjective takes into account the operational cost as well as the satisfaction\nof drivers. This problem is heavily constrained due to strict legal rules and\ncollective agreements. The objective of this article is to provide\nstate-of-the-art exact and hybrid solution methods that can provide\nhigh-quality solutions for instances of different sizes. This work presents a\ncomprehensive study of both an exact method, Branch and Price (B&P), as well as\na Large Neighborhood Search (LNS) framework which uses B&P or Column Generation\n(CG) for the repair phase to solve the BDSP. It further proposes and evaluates\na novel deeper integration of B&P and LNS, storing the generated columns from\nthe LNS subproblems and reusing them for other subproblems, or to find better\nglobal solutions. The article presents a detailed analysis of several\ncomponents of the solution methods and their impact, including general\nimprovements for the B&P subproblem, which is a high-dimensional Resource\nConstrained Shortest Path Problem (RCSPP), and the components of the LNS. The\nevaluation shows that our approach provides new state-of-the-art results for\ninstances of all sizes, including exact solutions for small instances, and low\ngaps to a known lower bound for mid-sized instances. Conclusions: We observe\nthat B&P provides the best results for small instances, while the tight\nintegration of LNS and CG can provide high-quality solutions for larger\ninstances, further improving over LNS which just uses CG as a black box. The\nproposed methods are general and can also be applied to other rule sets and\nrelated optimization problems", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u516c\u4ea4\u8f66\u53f8\u673a\u8c03\u5ea6\u95ee\u9898\uff08BDSP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u652f\u5b9a\u4ef7\uff08B&P\uff09\u548c\u5927\u90bb\u57df\u641c\u7d22\uff08LNS\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u6574\u5408B&P\u548cLNS\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5c0f\u3001\u4e2d\u3001\u5927\u89c4\u6a21\u5b9e\u4f8b\u7684\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u516c\u4ea4\u8f66\u53f8\u673a\u8c03\u5ea6\u95ee\u9898\u53d7\u5230\u4e25\u683c\u7684\u6cd5\u5f8b\u89c4\u5219\u548c\u96c6\u4f53\u534f\u8bae\u7ea6\u675f\uff0c\u9700\u8981\u517c\u987e\u8fd0\u8425\u6210\u672c\u548c\u53f8\u673a\u6ee1\u610f\u5ea6\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u80fd\u591f\u4e3a\u4e0d\u540c\u89c4\u6a21\u5b9e\u4f8b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u7ed3\u5408\u4e86\u5206\u652f\u5b9a\u4ef7\uff08B&P\uff09\u548c\u5927\u90bb\u57df\u641c\u7d22\uff08LNS\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u6574\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b58\u50a8\u548c\u91cd\u7528LNS\u5b50\u95ee\u9898\u751f\u6210\u7684\u5217\u6765\u4f18\u5316\u5168\u5c40\u89e3\u3002\u8fd8\u6539\u8fdb\u4e86B&P\u5b50\u95ee\u9898\uff08\u8d44\u6e90\u7ea6\u675f\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0cRCSPP\uff09\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u89e3\uff0c\u5728\u4e2d\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u4e0e\u5df2\u77e5\u4e0b\u754c\u7684\u5dee\u8ddd\u8f83\u5c0f\uff0c\u540c\u65f6\u5728\u6240\u6709\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u5747\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "B&P\u5728\u5c0f\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u7d27\u5bc6\u6574\u5408\u7684LNS\u548cCG\u80fd\u591f\u4e3a\u5927\u89c4\u6a21\u5b9e\u4f8b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u89e3\u3002\u672c\u6587\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u89c4\u5219\u96c6\u548c\u76f8\u5173\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2505.02248", "pdf": "https://arxiv.org/pdf/2505.02248", "abs": "https://arxiv.org/abs/2505.02248", "authors": ["Liu Ziyin", "Isaac Chuang", "Tomaso Poggio"], "title": "Heterosynaptic Circuits Are Universal Gradient Machines", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cs.LG", "cs.NE", "q-bio.PE"], "comment": "preprint", "summary": "We propose a design principle for the learning circuits of the biological\nbrain. The principle states that almost any dendritic weights updated via\nheterosynaptic plasticity can implement a generalized and efficient class of\ngradient-based meta-learning. The theory suggests that a broad class of\nbiologically plausible learning algorithms, together with the standard machine\nlearning optimizers, can be grounded in heterosynaptic circuit motifs. This\nprinciple suggests that the phenomenology of (anti-) Hebbian (HBP) and\nheterosynaptic plasticity (HSP) may emerge from the same underlying dynamics,\nthus providing a unifying explanation. It also suggests an alternative\nperspective of neuroplasticity, where HSP is promoted to the primary learning\nand memory mechanism, and HBP is an emergent byproduct. We present simulations\nthat show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can\nexplain the flexibility of the biology circuits, and (c) gradient learning can\narise quickly from simple evolutionary dynamics that do not compute any\nexplicit gradient. While our primary focus is on biology, the principle also\nimplies a new approach to designing AI training algorithms and physically\nlearnable AI hardware. Conceptually, our result demonstrates that contrary to\nthe common belief, gradient computation may be extremely easy and common in\nnature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u751f\u7269\u5927\u8111\u5b66\u4e60\u7535\u8def\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u8ba4\u4e3a\u5f02\u7a81\u89e6\u53ef\u5851\u6027\uff08HSP\uff09\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u68af\u5ea6\u5143\u5b66\u4e60\uff0c\u7edf\u4e00\u89e3\u91ca\u4e86\uff08\u53cd\uff09Hebbian\u53ef\u5851\u6027\uff08HBP\uff09\u4e0eHSP\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u5e76\u6307\u51faHSP\u53ef\u80fd\u662f\u4e3b\u8981\u5b66\u4e60\u673a\u5236\uff0c\u800cHBP\u662f\u5176\u526f\u4ea7\u54c1\u3002\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HSP\u7684\u5143\u53ef\u5851\u6027\u3001\u751f\u7269\u7535\u8def\u7075\u6d3b\u6027\u53ca\u68af\u5ea6\u5b66\u4e60\u7684\u81ea\u7136\u8fdb\u5316\u8d77\u6e90\uff0c\u5bf9AI\u7b97\u6cd5\u4e0e\u786c\u4ef6\u8bbe\u8ba1\u6709\u542f\u793a\u3002", "motivation": "\u65e8\u5728\u63ed\u793a\u751f\u7269\u5927\u8111\u5b66\u4e60\u673a\u5236\u7684\u7edf\u4e00\u539f\u7406\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u4e3a\u68af\u5ea6\u8ba1\u7b97\u590d\u6742\u7684\u89c2\u70b9\uff0c\u5e76\u4e3aAI\u8bad\u7ec3\u7b97\u6cd5\u548c\u786c\u4ef6\u63d0\u4f9b\u751f\u7269\u5b66\u542f\u53d1\u7684\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f02\u7a81\u89e6\u53ef\u5851\u6027\uff08HSP\uff09\u7684\u901a\u7528\u5b66\u4e60\u539f\u5219\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1HSP\u80fd\u5426\u89e3\u91ca\u795e\u7ecf\u5143\u7684\u5143\u53ef\u5851\u6027\u3001\u7535\u8def\u7075\u6d3b\u6027\uff0c\u4ee5\u53ca\u68af\u5ea6\u5b66\u4e60\u5982\u4f55\u4ece\u81ea\u7136\u8fdb\u5316\u4e2d\u6d8c\u73b0\u3002", "result": "HSP\u88ab\u8bc1\u660e\u53ef\u7edf\u4e00\u89e3\u91caHBP\u73b0\u8c61\uff0c\u662f\u66f4\u57fa\u7840\u7684\u5b66\u4e60\u673a\u5236\uff1b\u6a21\u62df\u663e\u793aHSP\u652f\u6301\u5143\u53ef\u5851\u6027\u548c\u5feb\u901f\u68af\u5ea6\u5b66\u4e60\uff0c\u4e14\u65e0\u9700\u663e\u5f0f\u68af\u5ea6\u8ba1\u7b97\u3002", "conclusion": "\u68af\u5ea6\u8ba1\u7b97\u53ef\u80fd\u5728\u81ea\u7136\u754c\u4e2d\u666e\u904d\u4e14\u7b80\u5355\uff0cHSP\u4e3a\u751f\u7269\u4e0eAI\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u6838\u5fc3\u673a\u5236\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.02257", "pdf": "https://arxiv.org/pdf/2505.02257", "abs": "https://arxiv.org/abs/2505.02257", "authors": ["Yu Zhu", "Zehang Richard Li"], "title": "Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift", "categories": ["stat.ME", "cs.LG", "stat.AP"], "comment": "11 figures", "summary": "In regions lacking medically certified causes of death, verbal autopsy (VA)\nis a critical and widely used tool to ascertain the cause of death through\ninterviews with caregivers. Data collected by VAs are often analyzed using\nprobabilistic algorithms. The performance of these algorithms often degrades\ndue to distributional shift across populations. Most existing VA algorithms\nrely on centralized training, requiring full access to training data for joint\nmodeling. This is often infeasible due to privacy and logistical constraints.\nIn this paper, we propose a novel Bayesian Federated Learning (BFL) framework\nthat avoids data sharing across multiple training sources. Our method enables\nreliable individual-level cause-of-death classification and population-level\nquantification of cause-specific mortality fractions (CSMFs), in a target\ndomain with limited or no local labeled data. The proposed framework is\nmodular, computationally efficient, and compatible with a wide range of\nexisting VA algorithms as candidate models, facilitating flexible deployment in\nreal-world mortality surveillance systems. We validate the performance of BFL\nthrough extensive experiments on two real-world VA datasets under varying\nlevels of distribution shift. Our results show that BFL significantly\noutperforms the base models built on a single domain and achieves comparable or\nbetter performance compared to joint modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\uff08BFL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u53e3\u5934\u5c38\u68c0\uff08VA\uff09\u6570\u636e\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u7b97\u6cd5\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u65e0\u9700\u8de8\u57df\u5171\u4eab\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u4e2a\u4f53\u7ea7\u6b7b\u56e0\u5206\u7c7b\u548c\u7fa4\u4f53\u7ea7\u6b7b\u56e0\u7279\u5b9a\u6b7b\u4ea1\u7387\u5206\u6570\uff08CSMF\uff09\u7684\u53ef\u9760\u4f30\u8ba1\u3002", "motivation": "\u5728\u7f3a\u4e4f\u533b\u5b66\u8ba4\u8bc1\u6b7b\u56e0\u7684\u5730\u533a\uff0c\u53e3\u5934\u5c38\u68c0\uff08VA\uff09\u662f\u5173\u952e\u5de5\u5177\uff0c\u4f46\u7531\u4e8e\u5206\u5e03\u504f\u79fb\uff0c\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u4e0b\u964d\u3002\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u56e0\u9690\u79c1\u548c\u7269\u6d41\u9650\u5236\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\uff08BFL\uff09\u6846\u67b6\uff0c\u907f\u514d\u6570\u636e\u5171\u4eab\uff0c\u652f\u6301\u591a\u6e90\u8bad\u7ec3\uff0c\u517c\u5bb9\u591a\u79cd\u73b0\u6709VA\u7b97\u6cd5\uff0c\u6a21\u5757\u5316\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "result": "\u5728\u4e24\u79cd\u771f\u5b9eVA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBFL\u663e\u8457\u4f18\u4e8e\u5355\u57df\u57fa\u7840\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u8054\u5408\u5efa\u6a21\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "BFL\u6846\u67b6\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u548c\u9690\u79c1\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u6b7b\u4ea1\u7387\u76d1\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2505.02501", "pdf": "https://arxiv.org/pdf/2505.02501", "abs": "https://arxiv.org/abs/2505.02501", "authors": ["Asma Brazi", "Boris Meden", "Fabrice Mayran de Chamisso", "Steve Bourgeois", "Vincent Lepetit"], "title": "Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages, 5 figures", "summary": "We introduce Corr2Distrib, the first correspondence-based method which\nestimates a 6D camera pose distribution from an RGB image, explaining the\nobservations. Indeed, symmetries and occlusions introduce visual ambiguities,\nleading to multiple valid poses. While a few recent methods tackle this\nproblem, they do not rely on local correspondences which, according to the BOP\nChallenge, are currently the most effective way to estimate a single 6DoF pose\nsolution. Using correspondences to estimate a pose distribution is not\nstraightforward, since ambiguous correspondences induced by visual ambiguities\ndrastically decrease the performance of PnP. With Corr2Distrib, we turn these\nambiguities into an advantage to recover all valid poses. Corr2Distrib first\nlearns a symmetry-aware representation for each 3D point on the object's\nsurface, characterized by a descriptor and a local frame. This representation\nenables the generation of 3DoF rotation hypotheses from single 2D-3D\ncorrespondences. Next, we refine these hypotheses into a 6DoF pose distribution\nusing PnP and pose scoring. Our experimental evaluations on complex\nnon-synthetic scenes show that Corr2Distrib outperforms state-of-the-art\nsolutions for both pose distribution estimation and single pose estimation from\nan RGB image, demonstrating the potential of correspondences-based approaches.", "AI": {"tldr": "Corr2Distrib\u662f\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u5e94\u5173\u7cfb\u7684\u65b9\u6cd5\uff0c\u9996\u6b21\u4eceRGB\u56fe\u50cf\u4e2d\u4f30\u8ba16D\u76f8\u673a\u59ff\u6001\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u5bf9\u79f0\u6027\u548c\u906e\u6321\u5f15\u8d77\u7684\u89c6\u89c9\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u5bf9\u79f0\u6027\u548c\u906e\u6321\u4f1a\u5bfc\u81f4\u89c6\u89c9\u6a21\u7cca\uff0c\u4ea7\u751f\u591a\u4e2a\u6709\u6548\u7684\u59ff\u6001\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5927\u591a\u57fa\u4e8e\u5bf9\u5e94\u5173\u7cfb\u4f30\u8ba1\u5355\u4e00\u59ff\u6001\uff0c\u800c\u57fa\u4e8e\u5bf9\u5e94\u5173\u7cfb\u4f30\u8ba1\u59ff\u6001\u5206\u5e03\u5219\u9762\u4e34\u6027\u80fd\u4e0b\u964d\u7684\u6311\u6218\u3002", "method": "Corr2Distrib\u901a\u8fc7\u5b66\u4e60\u6bcf\u4e2a3D\u70b9\u7684\u5bf9\u79f0\u611f\u77e5\u8868\u793a\uff08\u5305\u62ec\u63cf\u8ff0\u7b26\u548c\u5c40\u90e8\u5750\u6807\u7cfb\uff09\uff0c\u751f\u62103DoF\u65cb\u8f6c\u5047\u8bbe\uff0c\u518d\u5229\u7528PnP\u548c\u59ff\u6001\u8bc4\u5206\u5c06\u5176\u7ec6\u5316\u62106DoF\u59ff\u6001\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCorr2Distrib\u5728\u590d\u6742\u975e\u5408\u6210\u573a\u666f\u4e2d\uff0c\u65e0\u8bba\u662f\u59ff\u6001\u5206\u5e03\u4f30\u8ba1\u8fd8\u662f\u5355\u4e00\u59ff\u6001\u4f30\u8ba1\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Corr2Distrib\u5c55\u793a\u4e86\u57fa\u4e8e\u5bf9\u5e94\u5173\u7cfb\u65b9\u6cd5\u5728\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u6210\u529f\u5c06\u89c6\u89c9\u6a21\u7cca\u8f6c\u5316\u4e3a\u4f18\u52bf\u3002"}}
{"id": "2505.02258", "pdf": "https://arxiv.org/pdf/2505.02258", "abs": "https://arxiv.org/abs/2505.02258", "authors": ["Emir Esenov", "Olof Hjortstam", "Yuriy Serdyuk", "Thomas Hammarstr\u00f6m", "Christian H\u00e4ger"], "title": "Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks", "categories": ["eess.SY", "cs.LG", "cs.SY", "physics.comp-ph"], "comment": null, "summary": "Dielectric response (DR) of insulating materials is key input information for\ndesigning electrical insulation systems and defining safe operating conditions\nof various HV devices. In dielectric materials, different polarization and\nconduction processes occur at different time scales, making it challenging to\nphysically interpret raw measured data. To analyze DR measurement results,\nequivalent circuit models (ECMs) are commonly used, reducing the complexity of\nthe physical system to a number of circuit elements that capture the dominant\nresponse. This paper examines the use of physics-informed neural networks\n(PINNs) for inverse modeling of DR in time domain using parallel RC circuits.\nTo assess their performance, we test PINNs on synthetic data generated from\nanalytical solutions of corresponding ECMs, incorporating Gaussian noise to\nsimulate measurement errors. Our results show that PINNs are highly effective\nat solving well-conditioned inverse problems, accurately estimating up to five\nunknown RC parameters with minimal requirements on neural network size,\ntraining duration, and hyperparameter tuning. Furthermore, we extend the ECMs\nto incorporate temperature dependence and demonstrate that PINNs can accurately\nrecover embedded, nonlinear temperature functions from noisy DR data sampled at\ndifferent temperatures. This case study in modeling DR in time domain presents\na solution with wide-ranging potential applications in disciplines relying on\nECMs, utilizing the latest technology in machine learning for scientific\ncomputation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5bf9\u65f6\u95f4\u57df\u4e2d\u7684\u4ecb\u7535\u54cd\u5e94\u8fdb\u884c\u9006\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u9a8c\u8bc1\u5176\u80fd\u51c6\u786e\u4f30\u8ba1\u6700\u591a\u4e94\u4e2a\u672a\u77e5RC\u53c2\u6570\uff0c\u5e76\u5728\u5f15\u5165\u6e29\u5ea6\u4f9d\u8d56\u6027\u540e\u4ecd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u7edd\u7f18\u6750\u6599\u7684\u4ecb\u7535\u54cd\u5e94\u662f\u8bbe\u8ba1\u7535\u6c14\u7edd\u7f18\u7cfb\u7edf\u7684\u5173\u952e\u8f93\u5165\uff0c\u4f46\u4f20\u7edf\u7b49\u6548\u7535\u8def\u6a21\u578b\uff08ECMs\uff09\u96be\u4ee5\u7269\u7406\u89e3\u91ca\u539f\u59cb\u6570\u636e\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528PINNs\u7b80\u5316\u9006\u5efa\u6a21\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u53c2\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5bf9\u65f6\u95f4\u57df\u4e2d\u7684\u4ecb\u7535\u54cd\u5e94\u8fdb\u884c\u9006\u5efa\u6a21\uff0c\u6d4b\u8bd5\u57fa\u4e8e\u5408\u6210\u6570\u636e\uff08\u542b\u9ad8\u65af\u566a\u58f0\u4ee5\u6a21\u62df\u6d4b\u91cf\u8bef\u5dee\uff09\uff0c\u5e76\u6269\u5c55ECMs\u4ee5\u5305\u542b\u6e29\u5ea6\u4f9d\u8d56\u6027\u3002", "result": "PINNs\u80fd\u591f\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u591a\u81f3\u4e94\u4e2a\u672a\u77e5RC\u53c2\u6570\uff0c\u7f51\u7edc\u89c4\u6a21\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u9700\u6c42\u6781\u4f4e\uff0c\u4e14\u5728\u6e29\u5ea6\u4f9d\u8d56\u6027\u6a21\u578b\u4e2d\u4ecd\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "PINNs\u4e3a\u4f9d\u8d56ECMs\u7684\u5b66\u79d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u9006\u5efa\u6a21\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u673a\u5668\u5b66\u4e60\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.02502", "pdf": "https://arxiv.org/pdf/2505.02502", "abs": "https://arxiv.org/abs/2505.02502", "authors": ["Xinyi Hou", "Jiahao Han", "Yanjie Zhao", "Haoyu Wang"], "title": "Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "Background: Large language models (LLMs) are increasingly deployed via\nopen-source and commercial frameworks, enabling individuals and organizations\nto self-host advanced AI capabilities. However, insecure defaults and\nmisconfigurations often expose LLM services to the public Internet, posing\nsignificant security and system engineering risks. Aims: This study aims to\nunveil the current landscape of public-facing LLM deployments in the wild\nthrough a large-scale empirical study, focusing on service prevalence, exposure\ncharacteristics, systemic vulnerabilities, and associated risks. Method: We\nconducted an Internet-wide measurement to identify public-facing LLM\ndeployments across 15 frameworks, discovering 320,102 services. We extracted\n158 unique API endpoints, grouped into 12 functional categories based on\ncapabilities and security risks. We further analyzed configurations,\nauthentication practices, and geographic distributions, revealing deployment\ntrends and systemic issues in real-world LLM system engineering. Results: Our\nstudy shows that public LLM deployments are rapidly growing but often insecure.\nAmong all endpoints, we observe widespread use of insecure protocols, poor TLS\nconfigurations, and unauthenticated access to critical operations. Security\nrisks, including model disclosure, system leakage, and unauthorized access, are\npervasive, highlighting the need for secure-by-default frameworks and stronger\ndeployment practices. Conclusions: Public-facing LLM deployments suffer from\nwidespread security and configuration flaws, exposing services to misuse, model\ntheft, resource hijacking, and remote exploitation. Strengthening default\nsecurity, deployment practices, and operational standards is critical for the\ngrowing self-hosted LLM ecosystem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u8c03\u67e5\u63ed\u793a\u4e86\u516c\u5171\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u90e8\u7f72\u7684\u5b89\u5168\u95ee\u9898\uff0c\u53d1\u73b0\u591a\u6570\u670d\u52a1\u56e0\u4e0d\u5b89\u5168\u534f\u8bae\u3001\u914d\u7f6e\u4e0d\u5f53\u548c\u65e0\u8ba4\u8bc1\u8bbf\u95ee\u800c\u5b58\u5728\u9ad8\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63ed\u793a\u516c\u5171LLM\u90e8\u7f72\u7684\u5b89\u5168\u73b0\u72b6\uff0c\u56e0\u4e0d\u5b89\u5168\u9ed8\u8ba4\u8bbe\u7f6e\u548c\u9519\u8bef\u914d\u7f6e\u53ef\u80fd\u5bfc\u81f4\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002", "method": "\u91c7\u7528\u4e92\u8054\u7f51\u6d4b\u91cf\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86320,102\u4e2a\u516c\u5171LLM\u670d\u52a1\uff0c\u5206\u6790\u4e86158\u4e2aAPI\u7aef\u70b9\uff0c\u5e76\u7814\u7a76\u5176\u914d\u7f6e\u3001\u8ba4\u8bc1\u548c\u5730\u7406\u5206\u5e03\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u516c\u5171LLM\u90e8\u7f72\u867d\u589e\u957f\u8fc5\u901f\uff0c\u4f46\u666e\u904d\u5b58\u5728\u4e0d\u5b89\u5168\u534f\u8bae\u3001TLS\u914d\u7f6e\u5dee\u3001\u65e0\u8ba4\u8bc1\u8bbf\u95ee\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u6cc4\u9732\u548c\u672a\u7ecf\u6388\u6743\u8bbf\u95ee\u7b49\u98ce\u9669\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u516c\u5171LLM\u90e8\u7f72\u4e9f\u9700\u52a0\u5f3a\u9ed8\u8ba4\u5b89\u5168\u6027\u548c\u90e8\u7f72\u5b9e\u8df5\uff0c\u4ee5\u5e94\u5bf9\u6a21\u578b\u7a83\u53d6\u3001\u8d44\u6e90\u52ab\u6301\u7b49\u5a01\u80c1\u3002"}}
{"id": "2505.02259", "pdf": "https://arxiv.org/pdf/2505.02259", "abs": "https://arxiv.org/abs/2505.02259", "authors": ["Stanislav Semenov"], "title": "Smooth Integer Encoding via Integral Balance", "categories": ["math.OC", "cs.LG", "03F60, 26E40", "F.4.1; G.1.0"], "comment": "28 pages, 4 figures, submitted to arXiv", "summary": "We introduce a novel method for encoding integers using smooth real-valued\nfunctions whose integral properties implicitly reflect discrete quantities. In\ncontrast to classical representations, where the integer appears as an explicit\nparameter, our approach encodes the number N in the set of natural numbers\nthrough the cumulative balance of a smooth function f_N(t), constructed from\nlocalized Gaussian bumps with alternating and decaying coefficients. The total\nintegral I(N) converges to zero as N tends to infinity, and the integer can be\nrecovered as the minimal point of near-cancellation.\n  This method enables continuous and differentiable representations of discrete\nstates, supports recovery through spline-based or analytical inversion, and\nextends naturally to multidimensional tuples (N1, N2, ...). We analyze the\nstructure and convergence of the encoding series, demonstrate numerical\nconstruction of the integral map I(N), and develop procedures for integer\nrecovery via numerical inversion. The resulting framework opens a path toward\nembedding discrete logic within continuous optimization pipelines, machine\nlearning architectures, and smooth symbolic computation.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u5e73\u6ed1\u5b9e\u503c\u51fd\u6570\u7f16\u7801\u6574\u6570\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u51fd\u6570\u7684\u79ef\u5206\u7279\u6027\u9690\u5f0f\u53cd\u6620\u79bb\u6563\u91cf\uff0c\u652f\u6301\u8fde\u7eed\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u663e\u5f0f\u8868\u793a\u6574\u6570\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5e73\u6ed1\u51fd\u6570\u9690\u5f0f\u7f16\u7801\uff0c\u4ee5\u652f\u6301\u8fde\u7eed\u4f18\u5316\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u79bb\u6563\u903b\u8f91\u5d4c\u5165\u3002", "method": "\u5229\u7528\u5c40\u90e8\u9ad8\u65af\u51f9\u51f8\u51fd\u6570\u4ea4\u66ff\u53e0\u52a0\u6784\u9020\u5e73\u6ed1\u51fd\u6570f_N(t)\uff0c\u901a\u8fc7\u79ef\u5206\u6536\u655b\u6027\u9690\u5f0f\u7f16\u7801\u6574\u6570N\uff0c\u5e76\u652f\u6301\u6570\u503c\u6216\u89e3\u6790\u6062\u590d\u3002", "result": "\u9a8c\u8bc1\u4e86\u7f16\u7801\u5e8f\u5217\u7684\u6536\u655b\u6027\uff0c\u5c55\u793a\u4e86\u79ef\u5206\u6620\u5c04I(N)\u7684\u6570\u503c\u6784\u9020\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u6570\u503c\u53cd\u6f14\u6062\u590d\u6574\u6570\u7684\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79bb\u6563\u903b\u8f91\u5728\u8fde\u7eed\u4f18\u5316\u3001\u673a\u5668\u5b66\u4e60\u67b6\u6784\u53ca\u5e73\u6ed1\u7b26\u53f7\u8ba1\u7b97\u4e2d\u7684\u5d4c\u5165\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2505.02533", "pdf": "https://arxiv.org/pdf/2505.02533", "abs": "https://arxiv.org/abs/2505.02533", "authors": ["Dimitrios Kafetzis", "Ramin Khalili", "Iordanis Koutsopoulos"], "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d44\u6e90\u611f\u77e5\u7684Transformer\u67b6\u6784\u5206\u533a\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8fc1\u79fb\u6ce8\u610f\u529b\u5934\u6765\u964d\u4f4e\u63a8\u65ad\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\u3002\u5728\u5c0f\u578b\u548c\u5927\u578b\u8bbe\u5907\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u957f\u5ea6\u589e\u52a0\uff0c\u5185\u5b58\u548c\u8ba1\u7b97\u8d1f\u8f7d\u4e5f\u968f\u4e4b\u589e\u52a0\uff0c\u5bfc\u81f4\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5bb9\u6613\u51fa\u73b0\u5185\u5b58\u8fc7\u8f7d\u6216\u9ad8\u63a8\u65ad\u5ef6\u8fdf\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u6e90\u611f\u77e5\u7684Transformer\u5206\u533a\u7b97\u6cd5\uff0c\u52a8\u6001\u66f4\u65b0\u5206\u533a\u51b3\u7b56\uff0c\u5e76\u5728\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u8fdb\u884c\u5206\u533a\uff0c\u5141\u8bb8\u52a8\u6001\u8fc1\u79fb\u4ee5\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283-5\u53f0\u8bbe\u5907\u7684\u5c0f\u89c4\u6a21\u73af\u5883\u4e0b\u80fd\u591f\u8fbe\u5230\u6700\u4f73\u5ef6\u8fdf\u768415-20%\uff0c\u5728\u5927\u89c4\u6a21\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u65ad\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u8fc1\u79fb\u6ce8\u610f\u529b\u5934\u6765\u4f18\u5316\u8d44\u6e90\u5229\u7528\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.02350", "pdf": "https://arxiv.org/pdf/2505.02350", "abs": "https://arxiv.org/abs/2505.02350", "authors": ["Bobo Lian", "Dandan Wang", "Chenjian Wu", "Minxin Chen"], "title": "Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud surface representation is a fundamental problem in computer\ngraphics and vision. This paper presents a machine learning approach for\napproximating the signed distance function (SDF) of a point cloud using sparse\nellipsoidal radial basis function networks, enabling a compact and accurate\nsurface representation. Given the SDF values defined on the grid points\nconstructed from the point cloud, our method approximates the SDF accurately\nwith as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,\nrepresent the SDF of a point cloud by sparse ERBFs. To balance sparsity and\napproximation precision, a dynamic multi-objective optimization strategy is\nintroduced, which adaptively adds the regularization terms and jointly\noptimizes the weights, centers, shapes, and orientations of ERBFs. To improve\ncomputational efficiency, a nearest-neighbor-based data structure is employed,\nrestricting function calculations to points near each Gaussian kernel center.\nThe computations for each kernel are further parallelized on CUDA, which\nsignificantly improves the optimization speed. Additionally, a hierarchical\noctree-based refinement strategy is designed for training. Specifically, the\ninitialization and optimization of network parameters are conducted using\ncoarse grid points in the octree lattice structure. Subsequently, fine lattice\npoints are progressively incorporated to accelerate model convergence and\nenhance training efficiency. Extensive experiments on multiple benchmark\ndatasets demonstrate that our method outperforms previous sparse representation\napproaches in terms of accuracy, robustness, and computational efficiency. The\ncorresponding code is publicly available at\nhttps://github.com/lianbobo/SE-RBFNet.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u692d\u7403\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u70b9\u4e91\u7684\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\uff08SDF\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\u548c\u5c42\u6b21\u5316\u516b\u53c9\u6811\u7ec6\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u8868\u9762\u8868\u793a\u3002", "motivation": "\u70b9\u4e91\u8868\u9762\u8868\u793a\u662f\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u89c6\u89c9\u4e2d\u7684\u57fa\u7840\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u7a00\u758f\u6027\u548c\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7a00\u758f\u692d\u7403\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\uff08ERBFs\uff09\u5b9e\u73b0\u7d27\u51d1\u4e14\u51c6\u786e\u7684\u8868\u9762\u8868\u793a\u3002", "method": "\u4f7f\u7528\u692d\u7403\u5f84\u5411\u57fa\u51fd\u6570\uff08ERBFs\uff09\u8fd1\u4f3cSDF\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\u5e73\u8861\u7a00\u758f\u6027\u548c\u7cbe\u5ea6\uff0c\u5e76\u91c7\u7528\u6700\u8fd1\u90bb\u6570\u636e\u7ed3\u6784\u548cCUDA\u5e76\u884c\u8ba1\u7b97\u63d0\u5347\u6548\u7387\u3002\u5c42\u6b21\u5316\u516b\u53c9\u6811\u7ec6\u5316\u7b56\u7565\u7528\u4e8e\u53c2\u6570\u521d\u59cb\u5316\u548c\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5148\u524d\u7a00\u758f\u8868\u793a\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7a00\u758fERBFs\u548c\u4f18\u5316\u7b56\u7565\uff0c\u672c\u6587\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u70b9\u4e91\u8868\u9762\u8868\u793a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.02361", "pdf": "https://arxiv.org/pdf/2505.02361", "abs": "https://arxiv.org/abs/2505.02361", "authors": ["Andrew Ma", "Marin Solja\u010di\u0107"], "title": "Learning simple heuristic rules for classifying materials based on chemical composition", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.chem-ph"], "comment": "10 pages, 3 figures", "summary": "In the past decade, there has been a significant interest in the use of\nmachine learning approaches in materials science research. Conventional deep\nlearning approaches that rely on complex, nonlinear models have become\nincreasingly important in computational materials science due to their high\npredictive accuracy. In contrast to these approaches, we have shown in a recent\nwork that a remarkably simple learned heuristic rule -- based on the concept of\ntopogivity -- can classify whether a material is topological using only its\nchemical composition. In this paper, we go beyond the topology classification\nscenario by also studying the use of machine learning to develop simple\nheuristic rules for classifying whether a material is a metal based on chemical\ncomposition. Moreover, we present a framework for incorporating\nchemistry-informed inductive bias based on the structure of the periodic table.\nFor both the topology classification and the metallicity classification tasks,\nwe empirically characterize the performance of simple heuristic rules fit with\nand without chemistry-informed inductive bias across a wide range of training\nset sizes. We find evidence that incorporating chemistry-informed inductive\nbias can reduce the amount of training data required to reach a given level of\ntest accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u2018topogivity\u2019\u6982\u5ff5\u7684\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u4ec5\u901a\u8fc7\u5316\u5b66\u6210\u5206\u5373\u53ef\u5206\u7c7b\u6750\u6599\u662f\u5426\u4e3a\u62d3\u6251\u6750\u6599\uff0c\u5e76\u6269\u5c55\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u91d1\u5c5e\u6027\u5206\u7c7b\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5468\u671f\u8868\u7ed3\u6784\u7684\u5316\u5b66\u4fe1\u606f\u5f52\u7eb3\u504f\u5dee\uff0c\u51cf\u5c11\u4e86\u8fbe\u5230\u7279\u5b9a\u6d4b\u8bd5\u7cbe\u5ea6\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u91cf\u3002", "motivation": "\u6750\u6599\u79d1\u5b66\u4e2d\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u590d\u6742\u6027\u63a8\u52a8\u4e86\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u7684\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u4f9d\u8d56\u5316\u5b66\u6210\u5206\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002\u7814\u7a76\u8005\u5e0c\u671b\u9a8c\u8bc1\u5316\u5b66\u4fe1\u606f\u5f52\u7eb3\u504f\u5dee\u662f\u5426\u80fd\u63d0\u5347\u7b80\u5355\u89c4\u5219\u7684\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u2018topogivity\u2019\u7684\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u7ed3\u5408\u5468\u671f\u8868\u7ed3\u6784\u7684\u5316\u5b66\u4fe1\u606f\u5f52\u7eb3\u504f\u5dee\uff0c\u5206\u522b\u7528\u4e8e\u62d3\u6251\u6750\u6599\u548c\u91d1\u5c5e\u6027\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5bf9\u6bd4\u6709\u65e0\u5f52\u7eb3\u504f\u5dee\u4e0b\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5316\u5b66\u4fe1\u606f\u5f52\u7eb3\u504f\u5dee\u7684\u5f15\u5165\u663e\u8457\u964d\u4f4e\u4e86\u8fbe\u5230\u7279\u5b9a\u6d4b\u8bd5\u7cbe\u5ea6\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u91cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7b80\u5355\u89c4\u5219\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5316\u5b66\u4fe1\u606f\u5f52\u7eb3\u504f\u5dee\u662f\u63d0\u5347\u6750\u6599\u5206\u7c7b\u4efb\u52a1\u4e2d\u7b80\u5355\u542f\u53d1\u5f0f\u89c4\u5219\u6027\u80fd\u7684\u5173\u952e\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.02649", "pdf": "https://arxiv.org/pdf/2505.02649", "abs": "https://arxiv.org/abs/2505.02649", "authors": ["Valentin Foucher", "Santiago de Leon-Martinez", "Robert Moro"], "title": "Eye Movements as Indicators of Deception: A Machine Learning Approach", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Gaze may enhance the robustness of lie detectors but remains under-studied.\nThis study evaluated the efficacy of AI models (using fixations, saccades,\nblinks, and pupil size) for detecting deception in Concealed Information Tests\nacross two datasets. The first, collected with Eyelink 1000, contains gaze data\nfrom a computerized experiment where 87 participants revealed, concealed, or\nfaked the value of a previously selected card. The second, collected with Pupil\nNeon, involved 36 participants performing a similar task but facing an\nexperimenter. XGBoost achieved accuracies up to 74% in a binary classification\ntask (Revealing vs. Concealing) and 49% in a more challenging\nthree-classification task (Revealing vs. Concealing vs. Faking). Feature\nanalysis identified saccade number, duration, amplitude, and maximum pupil size\nas the most important for deception prediction. These results demonstrate the\nfeasibility of using gaze and AI to enhance lie detectors and encourage future\nresearch that may improve on this.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u51dd\u89c6\u6570\u636e\uff08\u6ce8\u89c6\u3001\u626b\u89c6\u3001\u7728\u773c\u548c\u77b3\u5b54\u5927\u5c0f\uff09\u5728AI\u6a21\u578b\u4e2d\u7684\u6548\u679c\uff0c\u7528\u4e8e\u5728\u9690\u853d\u4fe1\u606f\u6d4b\u8bd5\u4e2d\u68c0\u6d4b\u6b3a\u9a97\u884c\u4e3a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6700\u9ad8\u51c6\u786e\u7387\u8fbe74%\u3002", "motivation": "\u7531\u4e8e\u51dd\u89c6\u6570\u636e\u5728\u63d0\u9ad8\u6d4b\u8c0e\u4eea\u9c81\u68d2\u6027\u65b9\u9762\u6f5c\u529b\u5de8\u5927\u4f46\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1AI\u6a21\u578b\u5229\u7528\u51dd\u89c6\u6570\u636e\u68c0\u6d4b\u6b3a\u9a97\u7684\u53ef\u884c\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e24\u4e2a\u6570\u636e\u96c6\uff08Eyelink 1000\u548cPupil Neon\u91c7\u96c6\uff09\u8bad\u7ec3XGBoost\u6a21\u578b\uff0c\u4efb\u52a1\u5305\u62ec\u4e8c\u5206\u7c7b\uff08\u63ed\u9732/\u9690\u7792\uff09\u548c\u4e09\u5206\u7c7b\uff08\u63ed\u9732/\u9690\u7792/\u4f2a\u9020\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u7279\u5f81\u3002", "result": "\u4e8c\u5206\u7c7b\u4efb\u52a1\u6700\u9ad8\u51c6\u786e\u7387\u4e3a74%\uff0c\u4e09\u5206\u7c7b\u4e3a49%\u3002\u626b\u89c6\u6570\u91cf\u3001\u6301\u7eed\u65f6\u95f4\u3001\u5e45\u5ea6\u53ca\u6700\u5927\u77b3\u5b54\u5927\u5c0f\u88ab\u8bc6\u522b\u4e3a\u9884\u6d4b\u6b3a\u9a97\u7684\u5173\u952e\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u51dd\u89c6\u6570\u636e\u548cAI\u7ed3\u5408\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u4f18\u5316\u6d4b\u8c0e\u4eea\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.02470", "pdf": "https://arxiv.org/pdf/2505.02470", "abs": "https://arxiv.org/abs/2505.02470", "authors": ["Tal Oved", "Beatrice Lena", "Chlo\u00e9 F. Najac", "Sheng Shen", "Matthew S. Rosen", "Andrew Webb", "Efrat Shimron"], "title": "Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems", "categories": ["eess.IV", "cs.LG", "eess.SP"], "comment": null, "summary": "Magnetic resonance imaging (MRI) offers superb-quality images, but its\naccessibility is limited by high costs, posing challenges for patients\nrequiring longitudinal care. Low-field MRI provides affordable imaging with\nlow-cost devices but is hindered by long scans and degraded image quality,\nincluding low signal-to-noise ratio (SNR) and tissue contrast. We propose a\nnovel healthcare paradigm: using deep learning to extract personalized features\nfrom past standard high-field MRI scans and harnessing them to enable\naccelerated, enhanced-quality follow-up scans with low-cost systems. To\novercome the SNR and contrast differences, we introduce ViT-Fuser, a\nfeature-fusion vision transformer that learns features from past scans, e.g.\nthose stored in standard DICOM CDs. We show that \\textit{a single prior scan is\nsufficient}, and this scan can come from various MRI vendors, field strengths,\nand pulse sequences. Experiments with four datasets, including glioblastoma\ndata, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that\nViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality\nimages from accelerated low-field scans, with robustness to out-of-distribution\ndata. Our freely available framework thus enables rapid, diagnostic-quality,\nlow-cost imaging for wide healthcare applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faViT-Fuser\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4ece\u5386\u53f2\u9ad8\u573aMRI\u626b\u63cf\u4e2d\u63d0\u53d6\u4e2a\u6027\u5316\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u4f4e\u6210\u672c\u4f4e\u573aMRI\u7684\u626b\u63cf\u901f\u5ea6\u4e0e\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u573aMRI\u6210\u672c\u9ad8\u4e14\u4e0d\u6613\u83b7\u53d6\uff0c\u800c\u4f4e\u573aMRI\u867d\u5ec9\u4ef7\u4f46\u56fe\u50cf\u8d28\u91cf\u548c\u626b\u63cf\u901f\u5ea6\u8f83\u5dee\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6210\u50cf\u3002", "method": "\u91c7\u7528ViT-Fuser\uff08\u7279\u5f81\u878d\u5408\u89c6\u89c9transformer\uff09\uff0c\u5229\u7528\u5386\u53f2\u9ad8\u573aMRI\u626b\u63cf\u7684\u7279\u5f81\uff08\u5982DICOM\u5149\u76d8\u6570\u636e\uff09\uff0c\u589e\u5f3a\u4f4e\u573aMRI\u7684\u626b\u63cf\u6548\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5728\u5305\u62ec\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u3001\u4f4e\u573a\uff0850mT\uff09\u548c\u8d85\u4f4e\u573a\uff086.5mT\uff09\u6570\u636e\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cViT-Fuser\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5bf9\u5206\u5e03\u5916\u6570\u636e\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f4e\u6210\u672c\u5feb\u901f\u83b7\u53d6\u8bca\u65ad\u7ea7MRI\u56fe\u50cf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u533b\u7597\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.02694", "pdf": "https://arxiv.org/pdf/2505.02694", "abs": "https://arxiv.org/abs/2505.02694", "authors": ["Kurtis Haut", "Masum Hasan", "Thomas Carroll", "Ronald Epstein", "Taylan Sen", "Ehsan Hoque"], "title": "AI Standardized Patient Improves Human Conversations in Advanced Cancer Care", "categories": ["cs.HC", "cs.AI"], "comment": "20 pages, 6 figures, 4 tables, submitting to New England Journal of\n  Medicine (NEJM)", "summary": "Serious illness communication (SIC) in end-of-life care faces challenges such\nas emotional stress, cultural barriers, and balancing hope with honesty.\nDespite its importance, one of the few available ways for clinicians to\npractice SIC is with standardized patients, which is expensive, time-consuming,\nand inflexible. In this paper, we present SOPHIE, an AI-powered standardized\npatient simulation and automated feedback system. SOPHIE combines large\nlanguage models (LLMs), a lifelike virtual avatar, and automated, personalized\nfeedback based on clinical literature to provide remote, on-demand SIC\ntraining. In a randomized control study with healthcare students and\nprofessionals, SOPHIE users demonstrated significant improvement across three\ncritical SIC domains: Empathize, Be Explicit, and Empower. These results\nsuggest that AI-driven tools can enhance complex interpersonal communication\nskills, offering scalable, accessible solutions to address a critical gap in\nclinician education.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSOPHIE\uff0c\u4e00\u79cdAI\u9a71\u52a8\u7684\u6807\u51c6\u5316\u75c5\u4eba\u6a21\u62df\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bad\u7ec3\u4e34\u7ec8\u5173\u6000\u4e2d\u7684\u4e25\u8083\u75c5\u60c5\u6c9f\u901a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u6602\u8d35\u3001\u8017\u65f6\u7684\u95ee\u9898\u3002", "motivation": "\u4e34\u7ec8\u5173\u6000\u4e2d\u7684\u4e25\u8083\u75c5\u60c5\u6c9f\u901a(SIC)\u9762\u4e34\u60c5\u611f\u538b\u529b\u3001\u6587\u5316\u969c\u788d\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u6807\u51c6\u5316\u75c5\u4eba\u8bad\u7ec3\u6602\u8d35\u4e14\u4e0d\u7075\u6d3b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SOPHIE\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u3001\u865a\u62df\u5316\u5f62\u8c61\u548c\u57fa\u4e8e\u4e34\u5e8a\u6587\u732e\u7684\u81ea\u52a8\u5316\u53cd\u9988\uff0c\u63d0\u4f9b\u8fdc\u7a0b\u3001\u6309\u9700\u7684SIC\u8bad\u7ec3\u3002", "result": "\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u663e\u793a\uff0c\u4f7f\u7528SOPHIE\u7684\u533b\u7597\u5b66\u751f\u548c\u4e13\u4e1a\u4eba\u58eb\u5728\u201c\u5171\u60c5\u3001\u660e\u786e\u8868\u8fbe\u3001\u8d4b\u80fd\u201d\u4e09\u4e2a\u5173\u952eSIC\u9886\u57df\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AI\u9a71\u52a8\u5de5\u5177\u53ef\u589e\u5f3a\u590d\u6742\u4eba\u9645\u6c9f\u901a\u6280\u80fd\uff0c\u4e3a\u4e34\u5e8a\u6559\u80b2\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u6613\u83b7\u53d6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2505.02508", "pdf": "https://arxiv.org/pdf/2505.02508", "abs": "https://arxiv.org/abs/2505.02508", "authors": ["Yang Lyu", "Yuchun Qian", "Tan Minh Nguyen", "Xin T. Tong"], "title": "Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Diffusion models is a popular computational tool to generate new data\nsamples. It utilizes a forward diffusion process that add noise to the data\ndistribution and then use a reverse process to remove noises to produce samples\nfrom the data distribution. However, when the empirical data distribution\nconsists of $n$ data point, using the empirical diffusion model will\nnecessarily produce one of the existing data points. This is often referred to\nas the memorization effect, which is usually resolved by sophisticated machine\nlearning procedures in the current literature. This work shows that the\nmemorization problem can be resolved by a simple inertia update step at the end\nof the empirical diffusion model simulation. Our inertial diffusion model\nrequires only the empirical diffusion model score function and it does not\nrequire any further training. We show that choosing the inertia diffusion model\nsample distribution is an $O\\left(n^{-\\frac{2}{d+4}}\\right)$ Wasserstein-1\napproximation of a data distribution lying on a $C^2$ manifold of dimension\n$d$. Since this estimate is significant smaller the Wasserstein1 distance\nbetween population and empirical distributions, it rigorously shows the\ninertial diffusion model produces new data samples. Remarkably, this upper\nbound is completely free of the ambient space dimension, since there is no\ntraining involved. Our analysis utilizes the fact that the inertial diffusion\nmodel samples are approximately distributed as the Gaussian kernel density\nestimator on the manifold. This reveals an interesting connection between\ndiffusion model and manifold learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u60ef\u6027\u66f4\u65b0\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u7ecf\u9a8c\u6269\u6563\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u6548\u5e94\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u65b0\u6570\u636e\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u7ecf\u9a8c\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u6570\u636e\u65f6\u5b58\u5728\u8bb0\u5fc6\u6548\u5e94\uff0c\u5373\u53ea\u80fd\u751f\u6210\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6837\u672c\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u7ecf\u9a8c\u6269\u6563\u6a21\u578b\u6a21\u62df\u7684\u672b\u7aef\u6dfb\u52a0\u60ef\u6027\u66f4\u65b0\u6b65\u9aa4\uff0c\u4ec5\u9700\u4f7f\u7528\u539f\u59cb\u7684\u6269\u6563\u6a21\u578b\u8bc4\u5206\u51fd\u6570\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u60ef\u6027\u6269\u6563\u6a21\u578b\u7684\u6837\u672c\u5206\u5e03\u80fd\u4ee5$O\\left(n^{-\\frac{2}{d+4}}\\right)$ Wasserstein-1\u8ddd\u79bb\u8fd1\u4f3c\u6570\u636e\u5206\u5e03\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u751f\u6210\u65b0\u6837\u672c\u3002", "conclusion": "\u60ef\u6027\u6269\u6563\u6a21\u578b\u4e0d\u4ec5\u89e3\u51b3\u4e86\u8bb0\u5fc6\u95ee\u9898\uff0c\u8fd8\u4e0e\u6d41\u5f62\u5b66\u4e60\u5b58\u5728\u6709\u8da3\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5c55\u73b0\u4e86\u65b9\u6cd5\u7684\u7b80\u6d01\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2505.02574", "pdf": "https://arxiv.org/pdf/2505.02574", "abs": "https://arxiv.org/abs/2505.02574", "authors": ["Robin Arbaud", "Elisa Motta", "Marco Domenico Avaro", "Stefano Picinich", "Marta Lorenzini", "Arash Ajoudani"], "title": "Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": "7 pages, 6 figures, to be presented at ICRA 2025", "summary": "Partial hand amputations significantly affect the physical and psychosocial\nwell-being of individuals, yet intuitive control of externally powered\nprostheses remains an open challenge. To address this gap, we developed a\nforce-controlled prosthetic finger activated by electromyography (EMG) signals.\nThe prototype, constructed around a wrist brace, functions as a supernumerary\nfinger placed near the index, allowing for early-stage evaluation on unimpaired\nsubjects. A neural network-based model was then implemented to estimate\nfingertip forces from EMG inputs, allowing for online adjustment of the\nprosthetic finger grip strength. The force estimation model was validated\nthrough experiments with ten participants, demonstrating its effectiveness in\npredicting forces. Additionally, online trials with four users wearing the\nprosthesis exhibited precise control over the device. Our findings highlight\nthe potential of using EMG-based force estimation to enhance the functionality\nof prosthetic fingers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u808c\u7535\u4fe1\u53f7\uff08EMG\uff09\u7684\u529b\u91cf\u63a7\u5236\u5047\u80a2\u624b\u6307\uff0c\u7528\u4e8e\u89e3\u51b3\u90e8\u5206\u624b\u90e8\u622a\u80a2\u8005\u7684\u5047\u80a2\u76f4\u89c2\u63a7\u5236\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9e\u73b0\u4e86\u63e1\u529b\u7684\u5728\u7ebf\u8c03\u6574\u3002", "motivation": "\u90e8\u5206\u624b\u90e8\u622a\u80a2\u5bf9\u4e2a\u4f53\u7684\u751f\u7406\u548c\u5fc3\u7406\u5f71\u54cd\u5de8\u5927\uff0c\u4f46\u73b0\u6709\u5047\u80a2\u7684\u76f4\u89c2\u63a7\u5236\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eEMG\u4fe1\u53f7\u7684\u5047\u80a2\u624b\u6307\u539f\u578b\uff0c\u642d\u5efa\u4e8e\u624b\u8155\u652f\u67b6\u4e0a\u4f5c\u4e3a\u989d\u5916\u624b\u6307\uff0c\u5e76\u5728\u672a\u53d7\u635f\u53d7\u8bd5\u8005\u4e2d\u8fdb\u884c\u4e86\u65e9\u671f\u8bc4\u4f30\uff0c\u968f\u540e\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6839\u636eEMG\u8f93\u5165\u9884\u6d4b\u6307\u5c16\u529b\u91cf\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u529b\u91cf\u4f30\u8ba1\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5341\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u652f\u6301\u5176\u9884\u6d4b\u80fd\u529b\uff1b\u56db\u4f4d\u7528\u6237\u7684\u5728\u7ebf\u8bd5\u9a8c\u5c55\u793a\u4e86\u7cbe\u786e\u7684\u8bbe\u5907\u63a7\u5236\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eEMG\u7684\u529b\u91cf\u4f30\u8ba1\u53ef\u4ee5\u63d0\u5347\u5047\u80a2\u624b\u6307\u7684\u529f\u80fd\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.02780", "pdf": "https://arxiv.org/pdf/2505.02780", "abs": "https://arxiv.org/abs/2505.02780", "authors": ["Jai Prakash Veerla", "Partha Sai Guttikonda", "Helen H. Shang", "Mohammad Sadegh Nasr", "Cesar Torres", "Jacob M. Luber"], "title": "Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow", "categories": ["cs.HC", "cs.AI", "cs.ET", "q-bio.TO"], "comment": null, "summary": "Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases\nlike cancer, yet current digital pathology tools hinder diagnosis. The immense\nscale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the\nlimited views traditional monitors offer. This mismatch forces constant panning\nand zooming, increasing pathologist cognitive load, causing diagnostic fatigue,\nand slowing pathologists' adoption of digital methods. PathVis, our\nmixed-reality visualization platform for Apple Vision Pro, addresses these\nchallenges. It transforms the pathologist's interaction with data, replacing\ncumbersome mouse-and-monitor navigation with intuitive exploration using\nnatural hand gestures, eye gaze, and voice commands in an immersive workspace.\nPathVis integrates AI to enhance diagnosis. An AI-driven search function\ninstantly retrieves and displays the top five similar patient cases\nside-by-side, improving diagnostic precision and efficiency through rapid\ncomparison. Additionally, a multimodal conversational AI assistant offers\nreal-time image interpretation support and aids collaboration among\npathologists across multiple Apple devices. By merging the directness of\ntraditional pathology with advanced mixed-reality visualization and AI, PathVis\nimproves diagnostic workflows, reduces cognitive strain, and makes pathology\npractice more effective and engaging. The PathVis source code and a demo video\nare publicly available at: https://github.com/jaiprakash1824/Path_Vis", "AI": {"tldr": "PathVis\u662f\u4e00\u4e2a\u4e3aApple Vision Pro\u8bbe\u8ba1\u7684\u6df7\u5408\u73b0\u5b9e\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6570\u5b57\u75c5\u7406\u5de5\u5177\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u81ea\u7136\u624b\u52bf\u3001\u773c\u52a8\u548c\u8bed\u97f3\u547d\u4ee4\u63d0\u4f9b\u76f4\u89c2\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408AI\u63d0\u5347\u8bca\u65ad\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u75c5\u7406\u5de5\u5177\u56e0\u56fe\u50cf\u5c3a\u5bf8\u5de8\u5927\u4e14\u663e\u793a\u9650\u5236\uff0c\u5bfc\u81f4\u75c5\u7406\u5b66\u5bb6\u9700\u9891\u7e41\u7f29\u653e\u79fb\u52a8\uff0c\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u8377\u548c\u75b2\u52b3\uff0c\u963b\u788d\u4e86\u6570\u5b57\u65b9\u6cd5\u7684\u666e\u53ca\u3002PathVis\u65e8\u5728\u4f18\u5316\u8fd9\u4e00\u4ea4\u4e92\u8fc7\u7a0b\u3002", "method": "PathVis\u91c7\u7528\u6df7\u5408\u73b0\u5b9e\u6280\u672f\uff0c\u7ed3\u5408\u624b\u52bf\u3001\u773c\u52a8\u548c\u8bed\u97f3\u547d\u4ee4\u8fdb\u884c\u4ea4\u4e92\uff0c\u5e76\u96c6\u6210AI\u529f\u80fd\uff0c\u5305\u62ec\u76f8\u4f3c\u75c5\u4f8b\u641c\u7d22\u548c\u5b9e\u65f6\u56fe\u50cf\u89e3\u8bfb\u52a9\u624b\uff0c\u652f\u6301\u591a\u8bbe\u5907\u534f\u4f5c\u3002", "result": "PathVis\u663e\u8457\u6539\u5584\u4e86\u8bca\u65ad\u6d41\u7a0b\uff0c\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u540c\u65f6\u901a\u8fc7AI\u8f85\u52a9\u63d0\u5347\u4e86\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f7f\u75c5\u7406\u5b9e\u8df5\u66f4\u9ad8\u6548\u4e14\u4e92\u52a8\u6027\u5f3a\u3002", "conclusion": "PathVis\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u548cAI\u6280\u672f\u7684\u7ed3\u5408\uff0c\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u666e\u53ca\u548c\u53d1\u5c55\u3002"}}
{"id": "2505.02613", "pdf": "https://arxiv.org/pdf/2505.02613", "abs": "https://arxiv.org/abs/2505.02613", "authors": ["Mei Qiu", "William Lorenz Reindl", "Yaobin Chen", "Stanley Chien", "Shu Hu"], "title": "Lane-Wise Highway Anomaly Detection", "categories": ["eess.IV", "cs.LG"], "comment": null, "summary": "This paper proposes a scalable and interpretable framework for lane-wise\nhighway traffic anomaly detection, leveraging multi-modal time series data\nextracted from surveillance cameras. Unlike traditional sensor-dependent\nmethods, our approach uses AI-powered vision models to extract lane-specific\nfeatures, including vehicle count, occupancy, and truck percentage, without\nrelying on costly hardware or complex road modeling. We introduce a novel\ndataset containing 73,139 lane-wise samples, annotated with four classes of\nexpert-validated anomalies: three traffic-related anomalies (lane blockage and\nrecovery, foreign object intrusion, and sustained congestion) and one\nsensor-related anomaly (camera angle shift). Our multi-branch detection system\nintegrates deep learning, rule-based logic, and machine learning to improve\nrobustness and precision. Extensive experiments demonstrate that our framework\noutperforms state-of-the-art methods in precision, recall, and F1-score,\nproviding a cost-effective and scalable solution for real-world intelligent\ntransportation systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u9ad8\u901f\u516c\u8def\u8f66\u9053\u7ea7\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u4ece\u76d1\u63a7\u6444\u50cf\u5934\u4e2d\u63d0\u53d6\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7AI\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u8f66\u9053\u7279\u5b9a\u7279\u5f81\uff08\u5982\u8f66\u8f86\u8ba1\u6570\u3001\u5360\u7528\u7387\u548c\u5361\u8f66\u6bd4\u4f8b\uff09\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u786c\u4ef6\u6216\u590d\u6742\u9053\u8def\u5efa\u6a21\u3002\u540c\u65f6\u5f15\u5165\u4e86\u5305\u542b73,139\u4e2a\u8f66\u9053\u6837\u672c\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u56db\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5f02\u5e38\u3002\u591a\u5206\u652f\u68c0\u6d4b\u7cfb\u7edf\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u89c4\u5219\u903b\u8f91\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u786c\u4ef6\u548c\u590d\u6742\u7684\u9053\u8def\u5efa\u6a21\uff0c\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u89c6\u89c9\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u8f66\u9053\u7ea7\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\uff08\u5982\u8f66\u8f86\u8ba1\u6570\u3001\u5360\u7528\u7387\u7b49\uff09\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u89c4\u5219\u903b\u8f91\u548c\u673a\u5668\u5b66\u4e60\u6784\u5efa\u591a\u5206\u652f\u68c0\u6d4b\u7cfb\u7edf\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u8f66\u9053\u7ea7\u5f02\u5e38\u6570\u636e\u96c6\u4ee5\u652f\u6301\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u4ef7\u6bd4\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u548c\u6df7\u5408\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u786c\u4ef6\u4f9d\u8d56\u548c\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\u3002"}}
{"id": "2505.02824", "pdf": "https://arxiv.org/pdf/2505.02824", "abs": "https://arxiv.org/abs/2505.02824", "authors": ["Kuofeng Gao", "Yufei Zhu", "Yiming Li", "Jiawang Bai", "Yong Yang", "Zhifeng Li", "Shu-Tao Xia"], "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u4e2d\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\uff08DOV\uff09\u6c34\u5370\u673a\u5236\u7684\u7248\u6743\u89c4\u907f\u653b\u51fb\uff08CEA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u653b\u51fb\u65b9\u6cd5CEAT2I\uff0c\u901a\u8fc7\u68c0\u6d4b\u6c34\u5370\u6837\u672c\u3001\u8bc6\u522b\u89e6\u53d1\u6807\u8bb0\u548c\u6d88\u9664\u6c34\u5370\u4e09\u9636\u6bb5\u5b9e\u73b0\u9ad8\u6548\u89c4\u907f\u3002", "motivation": "\u968f\u7740T2I\u6a21\u578b\u7684\u4e2a\u6027\u5316\u5fae\u8c03\u6d41\u884c\uff0c\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u96c6\u4f7f\u7528\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0cDOV\u901a\u8fc7\u6c34\u5370\u6280\u672f\u4fdd\u62a4\u6570\u636e\u96c6\u6240\u6709\u6743\u3002\u7136\u800c\uff0cDOV\u5bf9\u7248\u6743\u89c4\u907f\u653b\u51fb\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u63ed\u793a\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51faCEAT2I\u653b\u51fb\uff0c\u5206\u4e09\u9636\u6bb5\uff1a1\uff09\u5229\u7528\u5fae\u8c03\u4e2d\u6c34\u5370\u6837\u672c\u7684\u6536\u655b\u901f\u5ea6\u5dee\u5f02\u68c0\u6d4b\u6837\u672c\uff1b2\uff09\u901a\u8fc7\u8fed\u4ee3\u6d88\u878d\u63d0\u793a\u8bcd\u548c\u76d1\u6d4b\u7279\u5f81\u53d8\u5316\u5b9a\u4f4d\u89e6\u53d1\u6807\u8bb0\uff1b3\uff09\u91c7\u7528\u95ed\u5f0f\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u6d88\u9664\u6c34\u5370\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCEAT2I\u80fd\u6709\u6548\u7ed5\u8fc7DOV\u673a\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u751f\u6210\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u653b\u51fb\u7684\u53ef\u884c\u6027\u548c\u6548\u679c\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u9a8c\u8bc1\u4e86T2I\u6a21\u578b\u4e2dDOV\u6c34\u5370\u7684\u8106\u5f31\u6027\uff0cCEAT2I\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u7248\u6743\u4fdd\u62a4\u65b9\u6848\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u5b89\u5168\u673a\u5236\u9700\u5bf9\u6297\u89c4\u907f\u653b\u51fb\u3002"}}
{"id": "2505.02614", "pdf": "https://arxiv.org/pdf/2505.02614", "abs": "https://arxiv.org/abs/2505.02614", "authors": ["Yura Malitsky", "Alexander Posch"], "title": "Entropic Mirror Descent for Linear Systems: Polyak's Stepsize and Implicit Bias", "categories": ["math.OC", "cs.LG", "stat.ML", "90C25 (Primary) 65K05, 47J25, 90C30 (Secondary)"], "comment": "18 pages, 2 figures", "summary": "This paper focuses on applying entropic mirror descent to solve linear\nsystems, where the main challenge for the convergence analysis stems from the\nunboundedness of the domain. To overcome this without imposing restrictive\nassumptions, we introduce a variant of Polyak-type stepsizes. Along the way, we\nstrengthen the bound for $\\ell_1$-norm implicit bias, obtain sublinear and\nlinear convergence results, and generalize the convergence result to arbitrary\nconvex $L$-smooth functions. We also propose an alternative method that avoids\nexponentiation, resembling the original Hadamard descent, but with provable\nconvergence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Polyak\u578b\u6b65\u957f\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7ebf\u6027\u7cfb\u7edf\u7684\u65e0\u754c\u57df\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u6536\u655b\u6027\u7ed3\u679c\uff0c\u5e76\u63a8\u5e7f\u5230\u4e86\u4efb\u610f\u51f8L-\u5e73\u6ed1\u51fd\u6570\u3002", "motivation": "\u89e3\u51b3\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u4e2d\u65e0\u754c\u57df\u5e26\u6765\u7684\u6536\u655b\u5206\u6790\u6311\u6218\uff0c\u907f\u514d\u4f7f\u7528\u9650\u5236\u6027\u5047\u8bbe\u3002", "method": "\u5f15\u5165\u6539\u8fdb\u7684Polyak\u578b\u6b65\u957f\uff0c\u63d0\u51fa\u907f\u514d\u6307\u6570\u8fd0\u7b97\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u83b7\u5f97\u4e86\u2113\u2081-\u8303\u6570\u9690\u5f0f\u504f\u5dee\u7684\u4e25\u683c\u8fb9\u754c\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7ebf\u6027\u4e0e\u7ebf\u6027\u6536\u655b\uff0c\u5e76\u63a8\u5e7f\u5230\u51f8L-\u5e73\u6ed1\u51fd\u6570\u3002", "conclusion": "\u8bc1\u660e\u4e86\u65b0\u65b9\u6cd5\u7684\u6536\u655b\u6027\uff0c\u62d3\u5c55\u4e86\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.02664", "pdf": "https://arxiv.org/pdf/2505.02664", "abs": "https://arxiv.org/abs/2505.02664", "authors": ["Ali Rashidi Moghadam", "Sayedmohammadreza Rastegari", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "9 Pages, 6 figures", "summary": "Grasp pose detection in cluttered, real-world environments remains a\nsignificant challenge due to noisy and incomplete sensory data combined with\ncomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)\nmethod, a lightweight yet highly effective hypothesis-and-test robotics\ngrasping framework which leverages an ensemble of Graph Neural Networks for\nefficient geometric reasoning from point cloud data. Building on the success of\nGtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp\ndetection but was limited by assumptions of complete, noise-free point clouds\nand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to\nefficiently produce 7-Dof grasp candidates. Candidates are assessed with an\nensemble Graph Neural Network model which includes points within the gripper\njaws (inside points) and surrounding contextual points (outside points). This\nimproved representation boosts grasp detection performance over previous\nmethods using the same generator. GtG 2.0 shows up to a 35% improvement in\nAverage Precision on the GraspNet-1Billion benchmark compared to\nhypothesis-and-test and Graph Neural Network-based methods, ranking it among\nthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot and\nKinect-v1 camera show a success rate of 91% and a clutter completion rate of\n100%, demonstrating its flexibility and reliability.", "AI": {"tldr": "GtG 2.0\u65b9\u6cd5\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u6293\u53d6\u68c0\u6d4b\u6027\u80fd\uff0c\u5728GraspNet-1Billion\u57fa\u51c6\u4e0a\u6bd4\u4e4b\u524d\u65b9\u6cd5\u63d0\u5347\u4e8635%\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a91%\u7684\u6210\u529f\u7387\u548c100%\u7684\u6742\u4e71\u573a\u666f\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3\u6742\u4e71\u771f\u5b9e\u73af\u5883\u4e2d\u6293\u53d6\u59ff\u52bf\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u7684\u4f20\u611f\u5668\u6570\u636e\u4ee5\u53ca\u590d\u6742\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u5047\u8bbe-\u6d4b\u8bd5\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u8fdb\u884c\u70b9\u4e91\u6570\u636e\u7684\u51e0\u4f55\u63a8\u7406\uff0c\u751f\u62107-Dof\u6293\u53d6\u5019\u9009\uff0c\u5e76\u901a\u8fc7\u5305\u542b\u5185\u5916\u70b9\u7684GNN\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5728GraspNet-1Billion\u4e0a\u5e73\u5747\u7cbe\u5ea6\u63d0\u534735%\uff0c\u5b9e\u9a8c\u673a\u5668\u4eba\u5b9e\u73b091%\u6210\u529f\u7387\u548c100%\u6742\u4e71\u573a\u666f\u5b8c\u6210\u7387\u3002", "conclusion": "GtG 2.0\u5728\u6293\u53d6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u6210\u4e3a\u5f53\u524d\u6700\u4f18\u6846\u67b6\u4e4b\u4e00\u3002"}}
{"id": "2505.02796", "pdf": "https://arxiv.org/pdf/2505.02796", "abs": "https://arxiv.org/abs/2505.02796", "authors": ["Yige Wang", "Jiashuo Jiang"], "title": "Adaptive Bidding Policies for First-Price Auctions with Budget Constraints under Non-stationarity", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "We study how a budget-constrained bidder should learn to adaptively bid in\nrepeated first-price auctions to maximize her cumulative payoff. This problem\narose due to an industry-wide shift from second-price auctions to first-price\nauctions in display advertising recently, which renders truthful bidding (i.e.,\nalways bidding one's private value) no longer optimal. We propose a simple\ndual-gradient-descent-based bidding policy that maintains a dual variable for\nbudget constraint as the bidder consumes her budget. In analysis, we consider\ntwo settings regarding the bidder's knowledge of her private values in the\nfuture: (i) an uninformative setting where all the distributional knowledge\n(can be non-stationary) is entirely unknown to the bidder, and (ii) an\ninformative setting where a prediction of the budget allocation in advance. We\ncharacterize the performance loss (or regret) relative to an optimal policy\nwith complete information on the stochasticity. For uninformative setting, We\nshow that the regret is \\tilde{O}(\\sqrt{T}) plus a variation term that reflects\nthe non-stationarity of the value distributions, and this is of optimal order.\nWe then show that we can get rid of the variation term with the help of the\nprediction; specifically, the regret is \\tilde{O}(\\sqrt{T}) plus the prediction\nerror term in the informative setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53cc\u68af\u5ea6\u4e0b\u964d\u7684\u7b80\u5355\u6295\u6807\u7b56\u7565\uff0c\u7528\u4e8e\u9884\u7b97\u53d7\u9650\u7684\u6295\u6807\u8005\u5728\u91cd\u590d\u7684\u4e00\u4ef7\u62cd\u5356\u4e2d\u81ea\u9002\u5e94\u6295\u6807\u4ee5\u6700\u5927\u5316\u7d2f\u79ef\u6536\u76ca\uff0c\u5206\u6790\u4e86\u65e0\u4fe1\u606f\u8bbe\u7f6e\u548c\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u7531\u4e8e\u884c\u4e1a\u4ece\u4e8c\u4ef7\u62cd\u5356\u8f6c\u5411\u4e00\u4ef7\u62cd\u5356\uff0c\u4f7f\u5f97\u771f\u5b9e\u6295\u6807\u4e0d\u518d\u662f\u6700\u4f18\u9009\u62e9\uff0c\u56e0\u6b64\u7814\u7a76\u9884\u7b97\u53d7\u9650\u7684\u6295\u6807\u8005\u5982\u4f55\u81ea\u9002\u5e94\u6295\u6807\u4ee5\u6700\u5927\u5316\u6536\u76ca\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e86\u53cc\u68af\u5ea6\u4e0b\u964d\u7684\u6295\u6807\u7b56\u7565\uff0c\u7ef4\u62a4\u4e00\u4e2a\u9488\u5bf9\u9884\u7b97\u7ea6\u675f\u7684\u53cc\u53d8\u91cf\uff0c\u5206\u6790\u4e86\u65e0\u4fe1\u606f\u8bbe\u7f6e\u548c\u4fe1\u606f\u8bbe\u7f6e\u4e24\u79cd\u60c5\u51b5\u3002", "result": "\u5728\u65e0\u4fe1\u606f\u8bbe\u7f6e\u4e2d\uff0c\u9057\u61be\u4e3a$\tilde{O}(\\sqrt{T})$\u52a0\u4e0a\u53cd\u6620\u4ef7\u503c\u5206\u5e03\u975e\u5e73\u7a33\u6027\u7684\u53d8\u5316\u9879\uff1b\u5728\u4fe1\u606f\u8bbe\u7f6e\u4e2d\uff0c\u9057\u61be\u4e3a$\tilde{O}(\\sqrt{T})$\u52a0\u4e0a\u9884\u6d4b\u8bef\u5dee\u9879\u3002", "conclusion": "\u53cc\u68af\u5ea6\u4e0b\u964d\u7b56\u7565\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u7684\u9884\u6d4b\u80fd\u8fdb\u4e00\u6b65\u51cf\u5c11\u9057\u61be\u3002"}}
{"id": "2505.02833", "pdf": "https://arxiv.org/pdf/2505.02833", "abs": "https://arxiv.org/abs/2505.02833", "authors": ["Yanjie Ze", "Zixuan Chen", "Jo\u00e3o Pedro Ara\u00fajo", "Zi-ang Cao", "Xue Bin Peng", "Jiajun Wu", "C. Karen Liu"], "title": "TWIST: Teleoperated Whole-Body Imitation System", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project website: https://humanoid-teleop.github.io", "summary": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step\ntoward developing general-purpose robotic intelligence, with human motion\nproviding an ideal interface for controlling all degrees of freedom. Yet, most\ncurrent humanoid teleoperation systems fall short of enabling coordinated\nwhole-body behavior, typically limiting themselves to isolated locomotion or\nmanipulation tasks. We present the Teleoperated Whole-Body Imitation System\n(TWIST), a system for humanoid teleoperation through whole-body motion\nimitation. We first generate reference motion clips by retargeting human motion\ncapture data to the humanoid robot. We then develop a robust, adaptive, and\nresponsive whole-body controller using a combination of reinforcement learning\nand behavior cloning (RL+BC). Through systematic analysis, we demonstrate how\nincorporating privileged future motion frames and real-world motion capture\n(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid\nrobots to achieve unprecedented, versatile, and coordinated whole-body motor\nskills--spanning whole-body manipulation, legged manipulation, locomotion, and\nexpressive movement--using a single unified neural network controller. Our\nproject website: https://humanoid-teleop.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTWIST\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u8eab\u8fd0\u52a8\u6a21\u4eff\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u884c\u4e3a\u514b\u9686\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u8ddf\u8e2a\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u7684\u884c\u8d70\u6216\u64cd\u4f5c\u4efb\u52a1\uff0c\u65e0\u6cd5\u5b9e\u73b0\u534f\u8c03\u7684\u5168\u8eab\u884c\u4e3a\u3002\u4e3a\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86TWIST\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u5168\u8eab\u8fd0\u52a8\u6a21\u4eff\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u667a\u80fd\u3002", "method": "\u9996\u5148\u901a\u8fc7\u5c06\u4eba\u4f53\u8fd0\u52a8\u6355\u6349\u6570\u636e\u91cd\u65b0\u6620\u5c04\u5230\u673a\u5668\u4eba\u4e0a\u751f\u6210\u53c2\u8003\u8fd0\u52a8\u7247\u6bb5\uff0c\u7136\u540e\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u884c\u4e3a\u514b\u9686\uff08RL+BC\uff09\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u5168\u8eab\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u8bc1\u660e\u4e86\u5f15\u5165\u7279\u6743\u672a\u6765\u8fd0\u52a8\u5e27\u548c\u771f\u5b9e\u8fd0\u52a8\u6355\u6349\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u51c6\u786e\u6027\u3002TWIST\u7cfb\u7edf\u4f7f\u673a\u5668\u4eba\u9996\u6b21\u5b9e\u73b0\u4e86\u5305\u62ec\u5168\u8eab\u64cd\u4f5c\u3001\u817f\u5f0f\u64cd\u4f5c\u3001\u884c\u8d70\u548c\u8868\u8fbe\u6027\u8fd0\u52a8\u5728\u5185\u7684\u591a\u79cd\u534f\u8c03\u5168\u8eab\u8fd0\u52a8\u6280\u80fd\u3002", "conclusion": "TWIST\u7cfb\u7edf\u901a\u8fc7\u7edf\u4e00\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u591a\u6837\u5316\u548c\u534f\u8c03\u6027\u5168\u8eab\u8fd0\u52a8\u80fd\u529b\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u901a\u7528\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
