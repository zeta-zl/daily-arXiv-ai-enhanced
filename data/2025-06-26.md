<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.AI](#cs.AI) [Total: 23]
- [math.OC](#math.OC) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.CV](#cs.CV) [Total: 17]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Key words: 大语言模型、少样本翻译、低资源语言、CycleDistill、合成平行语料库

TL;DR: 提出CycleDistill方法，利用大语言模型和少样本翻译，通过迭代生成合成平行语料库来提升低资源语言的机器翻译质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决低资源语言因缺乏平行语料库导致机器翻译质量不足的问题。

Method: CycleDistill方法通过迭代生成合成平行语料库，并用于微调模型，仅需1到4个少样本示例。

Result: 实验显示，该方法显著提升翻译质量，首次迭代平均改进20-30 chrF点。

Conclusion: CycleDistill能有效提升低资源语言的机器翻译质量，且利用softmax激活带来轻微改进。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [2] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Key words: 大型语言模型, 检索增强生成, 图推理, 多跳问答, 知识图谱

TL;DR: 提出了一种新的框架Inference-Scaled GraphRAG，通过推理时计算扩展来增强LLMs在知识图谱上的多跳推理能力，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大型语言模型（LLMs）在知识密集型推理任务上表现不足，传统的检索增强生成（RAG）和GraphRAG方法未能充分捕捉知识图谱中节点之间的关系结构。

Method: 引入了Inference-Scaled GraphRAG框架，结合了推理时的顺序扩展和并行扩展，采用深度链式思维图遍历和多数投票机制。

Result: 在GRBench基准测试中，该方法显著提升了多跳问答性能，超越了传统GraphRAG和其他图遍历基线方法。

Conclusion: 推理时扩展是一种实用且与架构无关的解决方案，有助于LLMs在结构化知识推理任务中的表现。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [3] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Key words: REST API, 代理工具, 文档解析, 代码生成

TL;DR: Doc2Agent是一个可扩展的流水线，用于从API文档生成工具并构建代理，提升性能和降低成本。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在解决现实世界中API文档的非结构化和复杂性，提升代理的工具使用能力。

Method: 通过从API文档生成可执行工具，并利用代码代理迭代优化。

Result: 在WebArena基准测试中，性能提升55%，成本降低90%；适用于复杂领域的任务。

Conclusion: Doc2Agent为大规模构建代理工具提供了通用解决方案。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [4] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Key words: 时空数据挖掘, 多任务推理, 大型语言模型, 解释性输出, STReason

TL;DR: 该论文提出了一种名为STReason的新框架，结合了大型语言模型（LLMs）的推理能力和时空模型的分析能力，用于多任务推理和执行。无需任务特定的微调，STReason通过上下文学习分解复杂查询，并生成详细解释。实验表明其性能优于现有基线模型。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的时空数据挖掘模型通常局限于单一任务，缺乏多任务推理和复杂长时推理的能力，限制了其在实际多场景决策中的应用。

Method: STReason框架结合LLMs的推理能力和时空模型的分析能力，通过上下文学习将复杂查询分解为模块化、可解释的程序，并系统执行生成解决方案和详细解释。

Result: 实验结果显示，STReason在所有指标上显著优于先进的LLM基线模型，尤其在复杂的时空推理场景中表现优异。人类评估还验证了其可信度和实用性。

Conclusion: STReason为开发更强大且通用的时空推理系统提供了有前景的方向，有望减少专家工作量并拓宽实际应用范围。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [5] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Key words: RACG, 代码生成, 代码检索, SACL, 语义增强

TL;DR: 该研究分析了代码检索方法，发现现有检索器过度依赖表层文本特征并对文档有偏见，提出了SACL框架，通过增加语义信息改善了检索和代码生成性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 调查代码检索方法的局限性，提出改进方案以提升检索和代码生成效果。

Method: 系统性遮蔽代码特征并保持功能，提出SACL框架，通过语义信息增强代码或结构知识。

Result: SACL显著提升了代码检索性能（例如HumanEval上Recall@1提高了12.8%）和代码生成效果（Pass@1提高了4.88%）。

Conclusion: SACL通过减少偏见和增加语义信息，有效改善了代码检索和生成性能。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [6] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Key words: 语义表示学习,自编码器,符号语义,分布语义,潜在几何

TL;DR: 该论文探讨如何通过组合语义视角改进Transformer自回归语言模型的语义表示学习，以减少符号语义和分布语义之间的差距。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 通过整合组合性和符号性属性，提升语言模型的解释性、可控性、组合性和泛化能力。

Method: 综述并比较了三种主流自编码器架构（VAE、VQVAE和SAE），分析了它们在语义结构和解释性方面的潜在几何特性。

Result: 研究发现这些架构能通过不同的潜在几何特性帮助弥合符号语义和分布语义之间的差距。

Conclusion: 组合语义视角为语义表示学习提供了新方向，有助于改进语言模型的性能。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [7] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Key words: 时间序列, 问答任务, 多模态, ITFormer, 自然语言处理

TL;DR: 提出了一个名为Time-Series QA的新任务，并发布了EngineMT-QA数据集，用于研究时间序列数据和自然语言的交互。同时提出了ITFormer框架，结合时间序列编码器和预训练大语言模型，提升了问答准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 时间序列数据在多个领域至关重要，但如何将其与自然语言高效结合仍是一个挑战。

Method: 提出了Instruct Time Transformer (ITFormer)，通过桥接时间序列编码器和冻结的大语言模型，提取、对齐和融合多模态特征。

Result: ITFormer在问答任务中表现优于基线模型，且仅增加了不到1%的可训练参数。

Conclusion: ITFormer为时间序列与自然语言的结合提供了高效范式，推动了多模态AI的研究和应用。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [8] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Key words: 大型语言模型,放射学报告,正预测值,操作成本,质量保证

TL;DR: 采用三阶段LLM框架显著提高了放射学报告的正预测值（PPV）并降低了操作成本，同时保持了检测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于错误率低，基于大型语言模型（LLM）的放射学报告校对的正预测值（PPV）有限。本文旨在评估三阶段LLM框架是否能提高PPV并降低操作成本。

Method: 通过对MIMIC-III数据库中的1000份放射学报告进行回顾性分析，比较了三种LLM框架：单提示检测器、提取器加检测器、以及提取器、检测器和假阳性验证器。外部数据集用于验证。

Result: 三阶段框架的PPV从0.063提升至0.159，操作成本降至5.58美元/1000份报告，人类审核报告从192份减少到88份。

Conclusion: 三阶段LLM框架显著提高了PPV并降低了成本，为AI辅助放射学报告质量保证提供了有效策略。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [9] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Key words: 能力评估，IRT，自动评分，缺失分数补全

TL;DR: 提出了一个基于自动评分技术的新方法，用于补全缺失分数，以提高IRT能力估计的准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估学习者的高阶能力（如表达和逻辑思维）需求增加，但人工评分成本高，且现有缺失分数补全方法对稀疏数据效果差。

Method: 利用自动评分技术补全缺失分数，以提高IRT能力估计的准确性。

Result: 新方法显著提高了能力估计的准确性，同时大幅减少了人工评分的工作量。

Conclusion: 自动评分技术能有效补全缺失分数，为IRT能力估计提供可靠支持。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [10] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Key words: RAG系统、评测框架、CCRS、LLM、零样本评测

TL;DR: 提出CCRS评测框架，用于评估RAG系统输出的多维质量，解决了现有方法的不足，并通过实验证明其高效性和有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有RAG系统评估方法在捕捉多维质量（如上下文连贯性和事实准确性）时存在局限，急需一种高效且全面的评测工具。

Method: 提出CCRS，包含五项指标（CC、QR、ID、AC、IR），利用预训练LLM作为零样本端到端评测工具，对六种RAG配置进行评测。

Result: CCRS在BioASQ数据集上有效区分系统性能（如Mistral-7B优于Llama），其评测能力与复杂框架相当但效率更高。

Conclusion: CCRS为RAG系统提供了一种高效、全面且实用的评测框架，助力迭代优化。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [11] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Key words: 推理模型, AALC, 强化学习, 准确性, 简洁性

TL;DR: AALC是一种轻量级方法，通过动态平衡准确性和简洁性，显著减少推理模型生成的冗余内容，同时保持或提升准确性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大推理模型因生成冗长思维链而面临高延迟和成本问题，但准确性提升有限。

Method: 在强化学习中引入AALC，结合验证准确性和动态长度惩罚，延迟惩罚直到达到目标性能。

Result: 在数学基准测试中，响应长度减少50%以上，准确性保持或提升；定性分析显示冗余推理减少。

Conclusion: AALC通过奖励策略引导推理模型生成更高效、通用的路径，但可能牺牲部分可解释性。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [12] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Key words: 多变量时间序列预测, 结构编码器, 语言模型, 语义对齐, 任务适配

TL;DR: SEED是一种结合结构编码器与语言模型的多变量时间序列预测方法，通过分阶段处理实现结构与语义的融合，显著提升了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多变量时间序列预测需兼顾变量间结构依赖与任务泛化能力，现有结构编码器缺乏语义推理能力，而大型语言模型不兼容原始时间序列，SEED旨在填补这一空白。

Method: SEED包含四个阶段：1) 基于token-aware编码器的补丁提取；2) 投影模块对齐补丁与语言模型嵌入；3) 语义重编程映射补丁到任务原型；4) 冻结语言模型进行预测。

Result: 实验表明SEED在多种数据集上优于基线方法，验证了其在结构与语义建模上的有效性。

Conclusion: SEED通过解耦表示学习与推理，实现了数值模式与语义推理的高效对齐，为统一可迁移预测系统提供了新思路。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [13] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Key words: 不确定性量化, 假发现率, COIN框架, 统计校准, 文本生成

TL;DR: 本文提出COIN框架，通过统计校准阈值筛选单个生成答案，以控制假发现率（FDR），提高不确定性量化（UQ）的实用性和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 基础模型的不确定性量化（UQ）对识别和减少生成文本中的幻觉至关重要，但现有启发式方法缺乏对假发现率（FDR）的正式保障。

Method: 提出COIN框架，利用校准集估计经验错误率，并使用Clopper-Pearson等方法建立真实错误率上限，以选择满足FDR控制的最大不确定性阈值。

Result: COIN在风险控制、测试数据保留率和有限校准数据下的预测效率方面表现出色，且扩展性强。

Conclusion: COIN通过统计方法显著提升了不确定性量化的实用性和效率，适用于多种应用场景。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [14] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Key words: 大语言模型, 对话情感识别, 示例检索, 增强检索

TL;DR: 本文研究了如何使用高质量示例改进大语言模型在对话情感识别中的性能，提出随机和增强示例检索策略，并验证其效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 提升大语言模型在主观任务（如情感识别）中的准确性是一个挑战，特别是在对话情感识别（CER）任务中。

Method: 提出基于随机和增强示例检索的策略，并分析对话上下文对CER准确性的影响。

Result: 增强示例检索在所有数据集上均优于其他方法，表明检索有针对性且经过改写的示例的重要性。

Conclusion: 增强示例检索能显著提升对话情感识别的性能。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [15] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Key words: 句子嵌入, 语义相似性, 机器翻译评估, 内在评估, 外评估

TL;DR: 本文通过内在和外评估方法比较了捷克语特定和多语言句子嵌入模型，发现内在语义相似性表现好的模型在下游翻译任务中未必表现优异，反之亦然。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究捷克语特定和多语言句子嵌入模型在语义相似性和下游任务（如机器翻译评估）中的表现差异。

Method: 使用Costra数据集和多个STS基准进行内在评估，通过COMET指标对模型进行外评估微调。

Result: 内在语义相似性表现好的模型在下游任务中未必表现优异，反之亦然。

Conclusion: 语义属性探测与下游任务之间存在复杂关系，需要进一步研究可操作语义或更深入的下游任务数据集。

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [16] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Key words: NLP, 多视角学习, 软标签, 主观分类, XAI

TL;DR: 论文提出了一种多视角的软标签方法，用于处理NLP任务中的人类分歧，旨在更好地代表少数观点，并在多个主观分类任务中优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 传统方法通过聚合标注者观点生成单一真值，可能忽视少数观点。研究认为标签应反映个体多样性，因此提出多视角方法，以开发更具包容性的模型。

Method: 采用软标签的多视角方法，在多个主观文本分类任务（如仇恨言论、反讽、辱骂语言和立场检测）中进行分析。

Result: 多视角方法在Jensen-Shannon Divergence（JSD）和F1分数上优于传统方法，但在反讽和立场检测等主观任务中表现较低置信度。

Conclusion: 多视角方法能更好地捕捉人类分歧，但需进一步解决高主观性任务的挑战。XAI分析为模型预测提供了有价值见解。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [17] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Key words: Large Language Models, structured reasoning, logical deduction, neurosymbolic AI, Group Relative Policy Optimization

TL;DR: 论文提出了一种通过结构化推理增强大语言模型（LLMs）性能的新方法，解决了其在复杂逻辑推理任务中的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前LLMs在复杂逻辑推理和系统规划任务中存在困难，主要依赖隐式统计关系，缺乏结构化知识表示。

Method: 将非结构化数据转换为结构化格式，通过监督微调（SFT）训练LLMs，并使用Group Relative Policy Optimization（GRPO）结合MAX-Flow和LCS算法增强推理能力。

Result: 实验结果表明，该方法提升了DeepSeek-R1-Distill-Qwen-1.5B模型的推理能力，增强了性能与优化技术的兼容性。

Conclusion: 结构化推理显著提升了LLMs在复杂任务中的表现。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [18] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Key words: 自动流畅性评估, 自监督学习, CNN-BiLSTM, 语音分块, 多模型融合

TL;DR: 该论文提出了一种基于分块的自动流畅性评估方法，结合自监督学习模型和分层CNN-BiLSTM框架，显著提升了非母语者语音节奏和停顿的评估效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自动流畅性评估（AFA）在非母语者的语音节奏、停顿和流畅性检测中仍面临挑战，需要更细粒度的分析方法。

Method: 采用分块策略，结合自监督学习模型（Wav2Vec2、HuBERT、WavLM）和分层CNN-BiLSTM框架，通过Silero-VAD分割语音并融合特征。

Result: 在Avalinguo和Speechocean762数据集上，F1分数和Pearson相关系数分别提升了2.8-4.2点和6.2-4.0点，优于单模型和Pyannote.audio基准。

Conclusion: 分块多模型融合方法显著提升了流畅性评估的鲁棒性，未来需探索其对不规则韵律方言的普适性。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [19] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Key words: 叙事分析、大型语言模型、主题模型、动态追踪、媒体叙事

TL;DR: 论文提出结合大型语言模型和主题模型的方法，动态追踪叙事随时间的变化，应用于《华尔街日报》2009-2023年的文章，发现大型语言模型在检测叙事变化时高效，但在区分内容变化和叙事变化时表现不佳。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 随着媒体叙事的快速演变，需要动态追踪叙事的发展，而非仅提取静态叙事。现有方法虽能捕捉叙事元素或结构，但应用于大规模语料时面临高昂成本或计算难题。

Method: 结合大型语言模型的语言理解能力与主题模型的大规模适用性，通过主题模型和变化点检测方法识别特定主题的变化，再用大型语言模型自动化解析这些变化，区分内容与叙事变化。

Result: 在《华尔街日报》2009-2023年的语料中，大型语言模型能高效检测叙事变化，但在区分内容变化和叙事变化时表现不佳。

Conclusion: 论文展示了一种动态追踪叙事变化的方法，但大型语言模型在区分内容与叙事变化的性能有待提升。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [20] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Key words: Biomed-Enriched, PubMed, 临床案例, NLP, 预训练

TL;DR: Biomed-Enriched是一个通过两阶段标注构建的生物医学文本数据集，基于PubMed数据，提供临床案例段落等高质量子集，可用于生物医学NLP研究。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 临床文本通常因隐私问题难以获取，数据集提供了公开可用的临床案例资源。

Method: 使用大语言模型标注PubMed段落，通过小型语言模型扩展标注，构建高质量子集，并进行质量筛选和领域上采样。

Result: 数据集包含200万临床案例段落，实验显示临床上采样使MMLU ProfMed性能提升5%，质量筛选使MedQA和MedMCQA提升1%。

Conclusion: 该数据集为生物医学预训练提供了高效资源，组合技术可加速收敛，减少训练成本。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [21] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Key words: 个性化工具使用, 目标导向对话代理, 大型语言模型, TAPS, NLSI任务

TL;DR: 研究探讨如何在目标导向的对话代理中有效整合用户偏好，通过引入TAPS解决方案提升个性化工具的使用效果，达到最新技术水平。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法忽视了个性化在工具使用中的作用，本研究旨在填补这一空白。

Method: 提出TAPS，结合结构化标记工具和基于不确定性的工具检测器，以增强个性化工具使用。

Result: TAPS显著提升了大型语言模型整合用户偏好的能力，在NLSI任务上达到开源模型的最新水平。

Conclusion: 通过TAPS，能够有效提升个性化工具使用的性能，为未来研究提供方向。

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [22] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Key words: 罕见病诊断, 大型语言模型, 多模态输入, 透明推理, 代理系统

TL;DR: DeepRare是一个基于大型语言模型的罕见病诊断系统，通过处理异构临床输入生成透明的诊断假设链，并在性能上显著优于其他方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 罕见病诊断因临床异质性、低患病率和医生对罕见病知识有限，具有挑战性。

Method: 系统包含三个关键组件：中央主机、专业代理服务器和工具集成，通过模块化设计实现复杂诊断。

Result: DeepRare在2,919种疾病中表现优异，Recall@1达57.18%，远超其他方法。多模态输入下表现更佳。

Conclusion: DeepRare是一个高效、透明的罕见病诊断工具，已实现为易用的网络应用。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [23] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Key words: 大型语言模型, AI安全, Code of Thought, 毒性测试

TL;DR: 该论文通过引入Code of Thought (CoDoT)提示策略，评估大型语言模型（LLMs）的安全性，发现现有模型在安全性上存在严重不足，并提出急需从基本原理出发加强安全措施。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于大型语言模型在安全关键应用中的广泛使用，需要提高其能力的同时确保与人类价值观和偏好的一致性。

Method: 引入CoDoT策略，将自然语言输入转换为代码表示相同意图，以测试LLMs的安全性。

Result: 结果显示，包括GPT-4 Turbo在内的多种先进LLMs在安全性上表现不佳，毒性显著增加。

Conclusion: 论文强调必须从基本原理出发评估和改进LLMs的安全性，以确保安全与能力同步发展。

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [24] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Key words: 说话时间分布, 对话动态, 人机交互, 计算框架, 参与者感知

TL;DR: 该研究提出了一个计算框架，用于量化对话中说话时间的分布及其动态变化，揭示了不同类型的动态对参与者的感知影响，并讨论了其在人机交互中的应用。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索对话中说话时间分布的整体平衡性和动态变化，以理解其对参与者感知的影响，并为设计人机交互平台提供工具。

Method: 提出了一个计算框架，通过几个直观的变轴构建了说话时间共享动态的类型学，并应用于陌生人视频聊天的大数据集。

Result: 平衡的对话更受欢迎，尤其是说话较少者；即使总平衡相同，不同类型的动态也会导致不同的参与者感知。

Conclusion: 该框架为研究对话动态提供了新工具，尤其适用于人机交互和人机交流平台的设计。

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [25] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Key words: LiveRAG, 多样化重排, 知识感知

TL;DR: Team Marikarp提出的知识感知多样化重排RAG流水线在SIGIR 2025 LiveRAG竞赛中夺冠。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 竞赛旨在评估从15M文档子集中检索问题相关支持文档的能力。

Method: 采用知识感知多样化重排RAG流水线。

Result: 竞赛中获得第一名。

Conclusion: 该方法在多样化评估中表现优异。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [26] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Key words: 大语言模型、模型压缩、微调模型、层合并、零阶优化

TL;DR: 该论文提出了一种通过合并微调模型变体的层来压缩大语言模型的新方法，显著减少了参数数量同时保持了高性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的大语言模型（LLMs）参数量大，部署和推理成本高，当前剪枝方法仅针对单一模型，无法充分利用多模型的优势。

Method: 提出了一种零阶优化方法，支持层移除、层选择和层合并三种操作，通过组合微调模型的层实现压缩。

Result: 实验显示，在Llama2-13B模型上，压缩后的模型减少了约25%的参数，性能保持原模型的97.3%，优于现有方法。

Conclusion: 该方法通过合并微调模型的层，实现了高效的大语言模型压缩，具有显著的实际应用价值。

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [27] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Key words: 大型语言模型, API 更新, 强化学习, 代码生成

TL;DR: ReCode 框架通过强化学习提升大型语言模型动态适应外部库 API 更新的能力，实验表明其显著优于监督微调，且不影响模型通用代码生成能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大型语言模型因依赖过时的 API 知识而无法适应动态环境中的代码生成需求。

Method: 构建数据集训练模型进行版本迁移，引入基于字符串相似度的奖励机制进行强化学习。

Result: ReCode 显著提升模型在动态 API 场景下的性能，Qwen2.5-Coder-7B 训练后表现优于更大的模型。

Conclusion: ReCode 是一种有效提升 LLMs 适应 API 更新的方法，且不影响其通用能力。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [28] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Key words: 强化学习,基础语言模型,数学数据,两阶段训练,OctoThinker

TL;DR: 研究了不同基础语言模型在强化学习中的表现，发现高质量数学数据和QA数据能提升性能，但需注意数据格式和训练稳定性，提出了两阶段训练策略OctoThinker。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨基础语言模型在强化学习中的适应性，以开发下一代可扩展模型。

Method: 对比Qwen和Llama模型，分析数学数据和QA数据的影响，并提出Stable-then-Decay两阶段训练策略。

Result: 高质量数学数据显著提升性能，长链推理数据增强效果但需控制稳定性，OctoThinker模型缩小与RL友好模型的差距。

Conclusion: 两阶段训练策略和高质量数据对提升RL性能至关重要，开源模型和数据集支持后续研究。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [29] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Key words: 多语言生成,推理计算,采样策略,选择策略,性能提升

TL;DR: 该论文研究如何在多语言、多任务环境下通过调整采样和选择策略以提升生成任务的效果，结果表明其方法显著提高了模型在多种语言和任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决现有采样和选择策略在英语以外的语言和多任务场景中表现不佳的问题，提出更通用的方法以提升生成任务的性能。

Method: 提出新的采样和选择策略，适应多语言和多任务场景，并评估现有方法在多样化语言和任务中的表现。

Result: 新策略使8B模型在m-ArenaHard-v2.0任务中的胜率平均提升6.8%，111B模型胜率提升9.0%。

Conclusion: 研究强调了适应不同语言和任务的推理计算方法的重要性，为提升低资源语言的性能提供了途径。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [30] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Key words: 大型语言模型、行为编辑、伦理行为、BehaviorBench、代理行为

TL;DR: 本文提出了一种名为“行为编辑”的方法，用于指导基于大型语言模型的代理的伦理行为，并通过多层次的心理学基准BehaviorBench评估其效果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 部署基于大型语言模型的代理在高风险领域存在重大的安全和伦理风险，需要有效的方法来引导其行为。

Method: 将代理行为引导任务建模为模型编辑任务（行为编辑），并引入一个基于心理学道德理论的多层次基准BehaviorBench进行系统评估。

Result: 行为编辑能够动态调整代理在特定场景中的行为，并实现全局道德对齐的变化，展现出在伦理和恶意行为引导上的潜力。

Conclusion: 行为编辑是一种新的代理行为引导范式，具有应用前景，但也存在潜在风险。

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [31] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Key words: 扩散语言模型，代码生成，强化学习，coupled-GRPO，解码行为

TL;DR: 探究扩散语言模型（dLLMs）在代码生成中的潜力，提出新型RL训练框架coupled-GRPO，显著提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 扩散语言模型（dLLMs）因其全局规划和迭代优化特性在代码生成中具有潜力，但训练和推理机制尚未充分探索。

Method: 训练7B参数的DiffuCoder模型，分析其解码行为，提出coupled-GRPO采样方案以减少方差并提高效率。

Result: coupled-GRPO显著提升代码生成性能（+4.4%），并减少对自回归解码的依赖。

Conclusion: 研究揭示了dLLMs的生成机制，并提供了一种高效的RL训练框架。

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [32] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Key words: 大语言模型,多跳问答,提示策略,Memento,检索增强生成

TL;DR: Memento是一种新的提示策略，通过分解复杂问题、动态构建事实数据库并整合事实来提升大语言模型在多跳问答中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 大语言模型在纯推理任务上表现优异，但在需要检索与推理紧密结合的任务（如多跳问答）中表现不佳。

Method: 提出三阶段策略Memento：分解问题、动态构建事实数据库、整合事实。

Result: 在多个基准测试中，Memento显著提升了性能，如在PhantomWiki上比CoT翻倍，在2WikiMultiHopQA上提升20 F1点。

Conclusion: Memento策略能有效提升多跳问答任务中语言模型的性能。

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [33] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Key words: 大型语言模型, 价值取舍, 认知模型, 礼貌语音, 训练动态

TL;DR: 该研究利用认知科学中的礼貌语音认知模型，评估大型语言模型（LLMs）在人类价值取舍上的表现，并分析了模型在推理努力和强化学习动态中的价值取舍模式。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在探讨LLMs是否能像人类一样在社交情境中权衡不同的价值观（如信息效用与社交效用），并填补现有工具在动态多层面价值解读上的空白。

Method: 采用认知科学中的礼貌语音认知模型，系统评估LLMs在两种设置下的价值取舍：前沿黑盒模型的推理努力程度，以及开源模型的强化学习后训练动态。

Result: 研究发现推理模型中信息效用高于社交效用，开源模型在数学推理中表现更强；训练初期价值取舍变化大，且受基础模型和预训练数据影响显著。

Conclusion: 该方法对快速发展的LLM领域具有广泛适用性，有助于形成对其他高级行为的假设、优化推理模型的训练机制，并在训练中更好地控制价值取舍。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Key words: 阅读行为、眼动、霍克斯过程、时空动态、意外性

TL;DR: 提出了一种基于时空点过程的概率阅读行为模型，重点关注注视和扫视的动态，优于基线模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有阅读行为模型依赖强假设且忽略时空动态，需要更通用的方法。

Method: 使用标记时空点过程和霍克斯过程建模注视和扫视，捕捉时间和空间动态。

Result: 霍克斯过程优于基线模型，但上下文意外性对预测准确性提升有限。

Conclusion: 新模型更准确地捕捉阅读动态，但意外性理论难以解释精细眼动。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [35] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Key words: 机器学习, 研究纠错, 会议机制, 反驳与批评

TL;DR: 该论文主张在机器学习会议中设立专门的“反驳与批评”轨道，以系统性纠正研究错误。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于机器学习研究快速进展，导致了一些误导性、不正确或有缺陷的研究被接受。会议缺乏机制来纠正这些错误。

Method: 提出在会议中建立“反驳与批评”轨道，设计轨道结构、评审原则，并讨论潜在问题。

Result: 通过案例分析（如ICLR 2025的口头报告），展示了该机制的必要性和可行性。

Conclusion: 机器学习会议应建立官方机制，支持研究的自我纠正，以促进科学的进步。

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [36] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Key words: 多目标优化, STIMULUS算法, 随机梯度, 收敛速率, 样本复杂度

TL;DR: 提出了STIMULUS算法及其改进版本STIMULUS-M和自适应批处理版本STIMULUS+/STIMULUS-M+，用于解决多目标优化问题，具有低样本复杂度和高收敛性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多目标优化（MOO）在机器学习等领域应用广泛，但现有方法收敛速度和样本复杂度表现不佳。

Method: 引入了递归框架更新随机梯度估计，提出STIMULUS和STIMULUS-M算法，并在其后进一步提出自适应批处理版本。

Result: 在非凸和强凸设置下分别达到了$O(1/T)$和$O(\exp{-\mu T})$的收敛速率，样本复杂度表现优越。

Conclusion: STIMULUS系列算法在多目标优化中表现出色，显著提升了收敛性能和样本效率。

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [37] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Key words: Koopman theory, flight trajectory prediction, HIPPO method, state space equations, model interpretability

TL;DR: 论文提出了一种基于HIPPO方法、Koopman理论和控制论状态空间方程的新建模与控制框架FlightKooba，用于解决飞行轨迹预测任务中现有Koopman理论应用的低效、可解释性差和计算量大的问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有Koopman理论应用于飞行轨迹预测任务时效果不佳，模型可解释性差且计算量大，导致训练时间长。

Method: 结合HIPPO方法、Koopman理论和状态空间方程，直接从数据构建Koopman算子，减少可训练参数数量。

Result: FlightKooba在时间和内存消耗方面表现优越（训练时间接近Mamba模块，内存消耗减少50%以上，参数减少十倍），完成了飞行轨迹预测任务。

Conclusion: FlightKooba为Koopman算子的快速计算提供了新方法，为时间序列预测与控制的结合开辟了新可能。

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [38] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Key words: QoE优化; 虚拟现实; 因果推理; 强化学习; 资源分配

TL;DR: 论文提出了一种智能框架，结合自适应关键帧提取和因果感知强化学习，优化多用户VR交互的QoE。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法忽视了带宽、CPU频率与用户感知之间的因果关系，限制了QoE提升。

Method: 提出PS-CDDPG算法，结合DDPG和因果推理，优化关键帧比率、带宽和计算资源。

Result: 实验表明，框架显著降低交互延迟，提升QoE并保持公平性，优于基准方法。

Conclusion: 因果感知强化学习在多用户VR交互中能有效提升QoE和性能。

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [39] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Key words: 机器遗忘, 正交卷积核正则化, 类感知软剪枝, 隐私保护, MLaaS

TL;DR: 论文提出了一个基于正交卷积核正则化的类感知软剪枝框架，用于快速精确地实现机器遗忘，解决了现有方法在遗忘速度与预测准确性之间的权衡问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 满足隐私法规（如GDPR）的需求，解决现有机器遗忘方法在计算开销和性能损失之间的权衡问题。

Method: 使用正交卷积核正则化训练模型，通过激活差异分析高效识别类特定通道，实现快速和精确的遗忘。

Result: 在多个架构和数据集上验证了该方法的高效性，实现了毫秒级响应时间、完全遗忘目标类以及对保留数据的极小准确率损失。

Conclusion: 该框架为机器学习即服务（MLaaS）场景中的实时机器遗忘提供了一种高效实用的解决方案。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [40] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Key words: MIRAGE, 多模态基准, 农业咨询, 开放世界, 视觉-语言模型

TL;DR: MIRAGE是一个多模态专家级推理和决策的基准测试，专注于农业领域的咨询互动，结合了自然用户查询、专家回答和图像上下文，用于评估模型在实际知识密集型任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有基准测试多依赖于明确输入和封闭分类，而MIRAGE旨在填补开放世界、上下文丰富场景的评估空白，尤其在农业领域的实际应用中。

Method: 基于35,000多个真实用户与专家互动数据，通过多步流程构建，涵盖多样化的作物健康、害虫诊断和管理场景。

Result: MIRAGE包含7,000多种生物实体，是当前视觉-语言模型中分类最多样化的基准之一，适用于开放世界场景的评估。

Conclusion: MIRAGE为多模态模型提供了高保真度的评估平台，尤其在处理模糊输入、罕见实体和主动引导交互方面具有独特价值。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [41] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Key words: 生成语义通信、知识蒸馏、自适应传输、AIGC

TL;DR: 本文提出DeKA-g算法，通过知识蒸馏和自适应传输优化生成式语义通信系统的知识对齐，显著提升边缘生成内容与云端内容的一致性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于AI生成内容（AIGC）的增长，云端到边缘和移动用户的传输导致网络负担加重。生成式语义通信（GSC）通过传输紧凑信息（如提示文本和潜在表示）降低负载，但知识对齐问题仍具挑战性。

Method: 提出DeKA-g算法，包含元词辅助知识蒸馏（MAKD）和可变率分组SNR自适应（VGSA）。通过低秩矩阵蒸馏云端生成知识，并适应多样化无线信道条件。

Result: 实验显示，DeKA-g提升了边缘与云端生成图像的对齐性44%，压缩率适应效率提高116%，低SNR条件下性能提升28%。

Conclusion: DeKA-g有效解决了生成式语义通信系统中的知识对齐问题，显著提升了传输效率和性能。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [42] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Key words: 电力市场,深度神经网络,可解释AI,SHAP,Gradient,热图

TL;DR: 本研究使用深度神经网络（DNN）预测电价，并通过XAI方法（如SHAP和Gradient）结合可视化技术（如热图）分析多个电力市场中价格动态的驱动因素，以提升对市场的理解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 电力市场复杂且依赖性强，传统计量经济学方法（白盒模型）难以充分解释价格动态，而DNN模型虽强大但缺乏可解释性。

Method: 使用DNN预测电价，并应用SHAP和Gradient等XAI方法及热图等技术，分析五个电力市场中各特征的行为和贡献，提出了SSHAP值和SSHAP线的新概念。

Result: 通过DNN和XAI方法的结合，能够更清晰地理解电力市场中价格动态的驱动因素。

Conclusion: DNN与XAI方法的结合为分析复杂电力市场提供了有效工具，新提出的SSHAP概念进一步提升了高维表格模型的可解释性。

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [43] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Key words: 神经网络, 不确定性测量, 分类模型, 激活向量, 后处理框架

TL;DR: 本文提出了一种新型的后处理框架，通过检索与查询具有相似激活向量的训练案例，测量神经网络决策的不确定性，并引入了两个新指标：决策变化和层不确定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于神经网络在高风险领域（如医疗诊断或自动驾驶）中可能会出现错误决策，因此需要有效的方法来检测和缓解这些错误。本文旨在通过测量不确定性来提高决策的可靠性。

Method: 提出了一种后处理框架，基于每层中与查询具有相似激活向量的训练案例，计算决策变化和层不确定性两个新指标。

Result: 在CIFAR-10和MNIST数据集上的分类模型中评估了该方法的有效性，结果表明这些指标在不确定性估计方面优于基于softmax的置信度。

Conclusion: 该方法能够有效增强不确定性估计能力，尤其在具有挑战性的分类任务中表现优异。

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [44] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Key words: 轴承故障诊断,强化学习,深度Q网络,适应性,计算需求

TL;DR: 本研究探讨了使用强化学习（特别是深度Q网络）进行轴承故障分类的可行性，结果表明其在适应动态环境方面优于传统监督学习，但计算需求较高。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决轴承故障诊断中传统方法依赖标记数据和适应性不足的问题。

Method: 采用深度Q网络（DQN）进行轴承故障分类，优化奖励结构以提高适应性。

Result: 强化学习模型在适应性上优于传统监督学习，但计算需求较大。

Conclusion: 强化学习能补充传统方法，为自适应诊断框架提供可能。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [45] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Key words: 自回归模型, 掩码扩散模型, 解码器, 编码器, 生成速度, 困惑度

TL;DR: 该研究在解码器框架下评估了掩码扩散模型（MDMs），以公平比较MDM（作为任意顺序自回归，AO-AR）和标准自回归（AR）范式，并探讨了架构对MDM的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 自回归模型（AR）和掩码扩散模型（MDMs）通常架构不同，导致直接比较不公平。研究旨在区分核心范式差异与架构影响。

Method: 在解码器框架下评估MDMs，比较AO-AR与标准AR范式；并分析解码器与编码器架构对MDMs的影响。

Result: 解码器MDMs在生成速度上显著提升（约25倍），困惑度相当，尽管建模空间更大；标准AO-AR目标可能需要改进。

Conclusion: 研究解耦了范式和架构影响，为未来模型设计提供了见解。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [46] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Key words: 特征组重要性,广义可加模型,可解释机器学习,联合信号,医学数据分析

TL;DR: 本文提出了一种用于广义可加模型（GAMs）的全新方法，旨在高效评估特征组的联合重要性，无需重新训练模型，支持事后定义和重叠分组，并在高维场景中保持有效。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前可解释机器学习中，特征组的联合信号常被忽视或遗漏，而实际中最重要的预测因子往往是特征组的联合效应。

Method: 提出一种高效方法，适用于GAMs，允许事后定义特征组、支持重叠分组，并与统计学中的解释变异有并行性。

Result: 通过三个合成实验展示了方法在不同数据场景下的表现，并在两个案例中证明了特征组分析比单特征分析更全面、准确。

Conclusion: 分析特征组的联合重要性能够提供更准确的医学问题视图，适用于高维或多模态数据。

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [47] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Key words: 分层聚类, k均值聚类, 大型语言模型, 多模态数据, 可解释性, Python包

TL;DR: HERCULES是一种新型算法和Python工具包，用于对多种数据类型进行分层k均值聚类，并通过大型语言模型（LLMs）生成语义丰富的标题和描述，以提高聚类结果的可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 面对复杂多模态数据集的爆炸式增长，需要一种不仅能有效分组数据，还能提供人类可理解的聚类结构的工具。

Method: HERCULES通过递归应用k均值聚类构建层次结构，并利用LLMs为每一层聚类生成标题和描述。提供两种表示模式：基于原始数据的直接模式和基于LLM生成摘要的描述模式。

Result: HERCULES能够从复杂数据集中提取有意义的分层知识，并通过交互式可视化工具增强结果的可理解性。

Conclusion: HERCULES为多模态数据的分层聚类提供了一种高效且可解释的方法，展示了LLMs在此领域的潜力。

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [48] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Key words: 深度强化学习、无监督环境设计、泛化能力、课程学习、样本效率

TL;DR: 论文提出了一种基于转移预测误差和共学习性的新方法TRACED，用于优化无监督环境设计（UED）中的课程生成，显著提升了强化学习代理的零样本泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度强化学习代理在未知环境中的泛化能力不足。现有UED方法通常仅通过价值函数损失衡量学习潜力，忽略了任务间的关系，因此需改进。

Method: 提出了TRACED方法，结合转移预测误差和共学习性，优化课程生成以实现更高效的样本利用和任务复杂性递增。

Result: 实验表明，TRACED在多个基准测试中显著提升了零样本泛化能力，且所需环境交互次数比基线方法少2倍。

Conclusion: 通过改进的学习潜力衡量方法及任务关系建模，TRACED证明了在UED中高效课程设计的潜力。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [49] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Key words: 神经形态计算, RF神经元, 无线分体计算, 能量效率, 时域信号处理

TL;DR: 该论文提出了一种基于共振-放电（RF）神经元的无线分体计算架构，用于高效处理频谱丰富的时域信号，显著降低计算和传输能量。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 边缘应用（如无线传感和音频识别）产生的频谱丰富的信号无法被传统的LIF神经元有效捕获。

Method: 采用具有振荡动力学的RF神经元直接处理时域信号，消除昂贵的频谱预处理需求。

Result: 实验表明，RF-SNN架构在音频和调制分类任务中实现了与传统方法相当的精度，同时大幅减少峰值率和总能耗。

Conclusion: RF神经元在频谱特征提取和能量效率方面优于传统LIF神经元。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [50] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Key words: 量子联邦学习、深度展开、异构性、超参数优化、医疗诊断

TL;DR: 提出了一种基于深度展开的量子联邦学习方法，通过动态调整超参数来应对客户端异构性，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决量子联邦学习中客户端异构性导致的性能问题。

Method: 利用深度展开技术，使客户端能够自主优化学习率和正则化因子等超参数。

Result: 在IBM量子硬件上取得约90%的准确率，显著优于传统方法的55%。

Conclusion: 该方法在医疗和基因组研究等复杂任务中具有广泛适用性。

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [51] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Key words: 时间序列插补, 缺失值处理, 模式聚类, 自适应掩码

TL;DR: DIM-SUM 是一个预处理框架，通过模式聚类和自适应掩码策略提升时间序列插补模型的鲁棒性，显著优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统时间序列插补模型使用完全数据集和人工掩码模式，无法适应实际复杂缺失模式。

Method: DIM-SUM 结合模式聚类和自适应掩码策略，并具有理论学习保证。

Result: 在加州水务、电力数据等实验中，DIM-SUM 显著优于传统方法，准确率更高且处理时间更短。

Conclusion: DIM-SUM 能有效适应实际缺失模式，提高模型性能。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [52] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Key words: 扩散模型, 概率预测, 混沌系统, 滚动预测, ERDM

TL;DR: 论文提出了一种名为ERDM的新框架，将滚动预测结构与高性能的Elucidated扩散模型相结合，成功解决了高维混沌系统中预测的复杂时间依赖性和不确定性增长问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的扩散模型在预测高维混沌系统时，通常采用逐个时间点预测的方式，难以捕捉复杂的时间依赖性，且无法显式处理不确定性的逐步增长。

Method: 论文提出了Elucidated Rolling Diffusion Models (ERDM)，将Elucidated Diffusion Models (EDM)的核心组件（噪声计划、网络预处理和Heun采样器）适应于滚动预测场景，并引入了新的损失加权方案、高效的初始化策略和混合序列架构。

Result: 在2D Navier-Stokes模拟和ERA5全球天气预报中，ERDM在性能上优于现有的扩散基线模型（包括条件自回归EDM）。

Conclusion: ERDM为扩散模型在序列生成问题中处理逐步增长的不确定性提供了一个灵活且强大的通用框架。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [53] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Key words: 机器学习, 损失加权, 最后一层重训练, 过参数化, 数据偏差

TL;DR: 论文探讨了在有限数据下对机器学习模型进行最后一层重训练（LLR）时，损失加权在不同参数化设置中的效果，提出需考虑模型的相对过参数化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何在有限数据和模型参数适中的情况下，通过损失加权克服训练数据带来的偏差，填补现有方法在参数化极端情况之间的空白。

Method: 提出在最后一层重训练（LLR）的机制下，结合损失加权方法，同时考虑模型的相对过参数化因素。

Result: 理论和实验证明，损失加权在LLR机制下仍然有效，但必须考虑模型的相对过参数化。

Conclusion: 在适中的参数化设置下，损失加权需结合相对过参数化因素，才能有效平衡不同类别的性能。

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [54] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Key words: 多代理人任务、行动方案、遗传算法、图神经网络、多样性

TL;DR: 该研究提出了一种新的理论和计算框架，用于在灾害响应、搜救和军事任务中生成多样化的行动方案（COA）池，以应对环境变化和代理人能力差异的挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多代理人任务中环境变化和代理人能力多样性需要多样化的行动方案，以提高任务的适应性和完成率。

Method: 使用图抽象化任务空间和COA池量化多样性，采用遗传算法和基于策略梯度的图神经网络进行任务分配和排序。

Result: 在模拟环境中测试显示，相较于随机基线，性能显著提升，任务排序的最优差距小，计划20个COA需约50分钟。

Conclusion: 该方法能有效生成多样化的行动方案，适应复杂环境变化和代理人能力差异，提高任务完成率。

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [55] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Key words: 机器学习, 边缘计算, 数据遗忘, 零知识证明, zk-SNARK

TL;DR: 本文提出了一种基于零知识证明（zk-SNARK）的验证框架，用于确认边缘设备上个性化模型的数据遗忘操作，同时保护隐私并最小化计算开销。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于版权侵权、偏见或监管要求等问题，需要在边缘设备上可验证地删除某些数据样本，同时确保操作的正确性和完整性。

Method: 设计了与zk-SNARK兼容的数据遗忘算法，确保在计算和内存受限的边缘环境中高效运行，并保留个性化模型的性能。

Result: 实验证明该框架能有效验证数据遗忘操作，同时对模型性能的影响最小化。

Conclusion: 该框架为边缘设备提供了可验证、隐私保护且高效的数据遗忘解决方案。

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [56] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Key words: Transformer, 向量量化, 残差流, 概念向量, CLVQVAE

TL;DR: 该论文提出了一种名为CLVQVAE的框架，通过向量量化和EMA代码簿更新方法，解决Transformer层中特征演变的冗余问题，从而提高概念向量的可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于解决Transformer层中由于残差流的线性混合导致的特征冗余问题，目前的研究方法往往忽略了跨层叠加和冗余问题。

Method: 采用了基于向量量化的CLVQVAE框架，结合top-k温度采样和EMA代码簿更新方法，并利用scaled-spherical k-means++进行代码簿初始化。

Result: 通过量化跨层表示并压缩重复特征，生成了紧凑且可解释的概念向量。

Conclusion: CLVQVAE框架有效地解决了Transformer层中的特征冗余问题，并提供了更好的概念向量解释性。

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [57] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Key words: 不平衡数据流、多类分类、LSH-RHP、动态集成多样化

TL;DR: 本文提出了一种名为LSH-DynED的新方法，用于解决多类不平衡非平稳数据流分类问题，通过结合LSH-RHP和动态集成多样化框架，显著提升了分类性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多类不平衡数据流分类任务存在动态不平衡比率的挑战，现有研究较少，本文旨在填补这一空白。

Method: 提出LSH-DynED方法，利用LSH-RHP对多数类进行欠采样，生成平衡训练集，并集成到动态多样化框架中。

Result: 在23个真实数据和10个半合成数据集上的实验表明，LSH-DynED在Kappa和mG-Mean指标上优于15种先进方法。

Conclusion: LSH-DynED在大规模、高维和不平衡数据流中表现出色，具有适应性和鲁棒性。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [58] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Key words: 图神经网络, 预测不确定性, 知识蒸馏, 自蒸馏, 医疗

TL;DR: 本文提出一种基于知识蒸馏的新方法，用于更高效、更精确地量化图神经网络（GNN）的预测不确定性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在医疗领域，GNN表现优异，但预测不确定性的量化仍具挑战性，这对临床可信度至关重要。传统方法如贝叶斯和集成方法计算成本高且无法完全捕获模型多样性。

Method: 采用自蒸馏技术，同一网络同时作为教师和学生模型，避免独立训练多个网络，并通过加权GNN分类器开发不确定性度量。

Result: 在MIMIC-IV和Enzymes数据集上的实验表明，该方法能有效捕获模型预测不确定性，性能与MC Dropout和集成方法相当。

Conclusion: 所提方法在减少计算成本的同时，提高了不确定性量化的效率和精度，适用于临床可信度评估。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [59] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Key words: 随机生成数据, 预训练, 算法复杂性, 零样本学习, 微调

TL;DR: 该论文研究了使用随机生成的数据进行模型预训练的方法，从算法复杂性理论角度论证了其可行性，并通过实验验证了预训练模型在零样本学习和实际数据中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究动机在于探索随机生成数据在模型预训练中的潜力，尤其是在未见数据上的零样本学习能力，并验证其在真实数据中的表现。

Method: 方法包括从算法复杂性理论角度提供理论支持，并使用合成数据预训练模型。随后在真实数据上进行微调，验证其收敛速度和泛化能力。

Result: 实验结果表明，预训练模型在多种数据集上表现出零样本学习能力，且随着模型规模的增加性能提升。此外，预训练后的微调还带来了更快的收敛和更好的泛化效果。

Conclusion: 结论证明随机生成数据可用于预训练模型，并在实际应用中具有优势，特别是在零样本学习和微调后的性能改进上。

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [60] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Key words: 强化学习, 指令跟随, 大型语言模型, 指令重标注, Craftax

TL;DR: 论文提出一种利用大型语言模型（LLMs）自动从失败轨迹中生成开放式指令的新方法，以减少对人类标注的依赖，提高强化学习的样本效率和策略性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统强化学习依赖大量人工标注的指令数据集，且稀疏奖励学习困难，需要更高效的解决方案。

Method: 利用LLMs从失败的轨迹中识别有意义子任务并生成开放式指令，重新标注数据，丰富训练集。

Result: 在Craftax环境中，该方法显著提高了样本效率、指令覆盖率和策略性能，优于现有基线。

Conclusion: LLM引导的开放式指令重标注能有效提升强化学习的指令跟随能力。

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [61] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Key words: 表型分析,PATIENT-REPORTED OUTCOMES,张量分解,溃疡性结肠炎,监督学习

TL;DR: 本文提出了一种新型监督耦合矩阵-张量分解方法（SCMTF），用于整合患者报告结果（PROs）和实验室数据，预测溃疡性结肠炎的药物持续性治疗。该方法在测试集上表现出色，展示了PRO数据在表型分析中的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究旨在利用患者报告结果（PROs）进行表型分析，解决传统方法因数据稀疏和噪声而忽略PROs的问题。

Method: 提出监督耦合矩阵-张量分解（SCMTF）方法，整合时间序列PROs和实验室数据与静态特征，使用深度学习框架训练模型。

Result: 模型在预测8个月和20个月药物变化时，测试集AUC分别达0.853和0.803。识别出包含症状变量的有用表型。

Conclusion: 低秩矩阵和张量方法成功应用于溃疡性结肠炎和高度缺失的PRO数据，证明PROs包含传统方法忽略的有用信息。

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [62] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Key words: 预测性维护、分类、回归、剩余使用寿命、机器学习

TL;DR: 本文综述了预测性维护（PdM）中分类与回归方法的比较，强调了两种方法在故障预测中的优缺点，并探讨了当前挑战和未来趋势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 预测性维护在工业中的重要性日益增加，但分类与回归方法的比较尚未有系统性研究，本文旨在填补这一空白。

Method: 通过综合分析最新文献，探讨分类与回归方法在预测性维护中的应用，并突出关键进展和挑战。

Result: 回归方法适合估计剩余使用寿命（RUL），而分类方法适合预测故障概率；数据不平衡和高维特征空间是主要挑战。

Conclusion: 本文为研究人员提供了不同PdM方法的优缺点，并建议未来研究关注公共数据集和开源工具的发展。

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [63] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Key words: 边缘计算,AI推理,容错机制,多目标优化,集成学习

TL;DR: 论文提出了一种名为MEL的框架，用于在边缘计算环境下实现高容错性的AI推理服务，通过多级集成学习训练多个轻量级备份模型，以在服务器故障时维持性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 边缘计算环境受限于资源和功率，且易发生故障。传统容错方法如云端故障转移或压缩备份通常会影响延迟或准确性，无法满足关键边缘推理服务的需求。

Method: 提出了MEL框架，通过多目标优化问题训练多个轻量级备份模型，鼓励模型多样性以相互修正表现，同时保证单个模型的独立性能。

Result: 实验证明，MEL在视觉、语言和音频数据集上的表现与原架构相当，同时具备容错性和部署灵活性。集成模型大小为原模型的40%，在故障时仍能保持95.6%的准确率。

Conclusion: MEL是一种有效的边缘推理容错解决方案，能够在故障时维持高性能，同时保持部署的灵活性。

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [64] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Key words: 野火风险、LFMC、多模态模型、遥感、自动化管道

TL;DR: 该研究利用预训练的多模态地球观测模型，生成大范围、空间完整的野火风险因子LFMC地图，显著提升了准确性并实现自动化生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 野火强度和频率持续增加，传统LFMC测量方式成本高且更新缓慢，亟需高效、低成本的监测方法。

Method: 采用预训练的多模态地球观测模型，生成覆盖广泛的LFMC地图，并通过自动化管道快速应用于美国各地。

Result: 相比随机初始化模型，RMSE降低20%，并在两处野火受灾地区验证了有效性。

Conclusion: 该方法显著提升了LFMC监测的效率和精度，为野火研究和应急响应提供了有力工具。

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [65] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Key words: 因果发现、LTI-DAE系统、DIPCA、代数关系、动态系统

TL;DR: 提出了针对LTI-DAE系统的因果发现方法PoV，优于现有方法，适用于纯动态和混合因果系统。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有因果发现方法仅适用于无代数关系的动态系统，无法处理反馈控制或守恒定律下的混合因果系统。

Method: 提出PoV方法，通过DIPCA确定代数关系数、动态关系数和约束矩阵，再通过条件数划分变量子集。

Result: PoV方法能够识别最小因果驱动子集，适用于LTI-DAE系统。

Conclusion: PoV方法在纯动态和混合因果系统中均有效，扩大了因果发现的应用范围。

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [66] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Key words: 因果发现, 偏微分方程, 物理信息神经网络, 反事实扰动

TL;DR: 提出了一种基于物理信息神经网络和反事实扰动的框架，用于发现偏微分方程中的因果结构，通过功能干预量化算子级必要性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在通过因果方法提升偏微分方程结构的可解释性和恢复能力，尤其是在噪声、冗余和数据稀缺的情况下。

Method: 使用因果敏感性指数和结构偏差度量评估候选微分算子的影响，结合理论证明和实验验证。

Result: 在合成和真实数据集上验证了框架的有效性，能够在噪声和数据稀缺下准确恢复因果算子。

Conclusion: 将因果偏微分方程的发现定位为一个可解释的推理任务，基于结构因果模型和变分残差分析。

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [67] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Key words: 大型语言模型, 剪枝, 激活稀疏性, GPU优化

TL;DR: DuoGPT提出了一种双稀疏框架，结合非结构化权重剪枝和激活稀疏性，优化LLMs部署效率，性能优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于大型语言模型（LLMs）的高内存和计算成本，部署困难。现有剪枝方法忽视运行时激活稀疏性。

Method: 将激活稀疏性重新解释为动态结构化权重稀疏性，提出双稀疏（spMspV）框架，结合非结构化剪枝和激活稀疏性，并扩展OBC框架。

Result: 在LLaMA-2和LLaMA-3上，DuoGPT在相同加速比下比基线稠密模型准确率提升高达9.17%。

Conclusion: DuoGPT通过双稀疏性和GPU优化，显著提升了LLMs的部署效率与性能。

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [68] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Key words: 大语言模型、代码归因、分布测试、Anubis、假设检验

TL;DR: 该论文提出了一种名为Anubis的零样本归因工具，用于判断代码样本是否源自特定的大语言模型（LLM），并通过分布测试方法实现高准确率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着越来越多代码样本从大语言模型中生成，如何准确归因这些代码的来源成为一个重要问题。

Method: 作者利用假设检验技术，结合LLM的样本和密度估计，设计了一个零样本归因工具Anubis。

Result: 实验表明，Anubis在区分不同LLM（如DeepSeek-Coder、CodeGemma和Stable-Code）时，仅需约2000个样本即可达到高AUROC分数（≥0.9）。

Conclusion: Anubis提供了一种高效的代码来源归因方法，为LLM生成代码的追踪提供了可行的解决方案。

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [69] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Key words: 情感计算, 启动效应, APS, SEED数据集, 数据驱动

TL;DR: 本文提出了一种数据驱动的方法Affective Priming Score（APS），用于检测情感计算中受启动效应影响的数据点，显著降低模型误分类率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 情感计算中，启动效应对数据的影响尚未充分研究，可能导致模型误分类。本文旨在解决这一问题。

Method: 提出APS方法，为每个数据点分配分数，量化其受启动效应影响的程度，并在SEED和SEED-VII数据集上验证。

Result: 使用无启动序列的数据训练模型，显著降低了误分类率。

Conclusion: APS方法能有效识别和缓解数据层面的启动效应，提升模型稳健性，为情感计算数据集的设计提供重要参考。

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [70] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Key words: 链接预测, 图神经网络, 有向图, 特征嵌入, 社区信息

TL;DR: 论文提出了一种新的图神经网络框架，结合特征嵌入和社区信息，改进有向图的链接预测性能，并通过实验验证其优越性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 链接预测是有向图分析中的一个经典问题，现有深度学习方法通常通过对比学习和图卷积分析节点相似性。作者希望通过结合特征嵌入和社区信息，进一步提升性能。

Method: 提出了一种新的图神经网络框架，融合特征嵌入和社区信息，并提出了一种将输入图转化为有向线图的方法，以在卷积过程中聚合更多信息。

Result: 实验结果表明，在多个训练数据比例下（30%、40%、50%、60%），所提出的方法在基准数据集上优于现有最优方法。

Conclusion: 结合特征嵌入与社区信息的图神经网络框架能显著提升有向图链接预测的性能。

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [71] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Key words: 联邦学习、非独立同分布数据、知识蒸馏、生成对抗网络、数据隐私

TL;DR: 论文提出了一种名为FedBKD的无数据蒸馏框架，解决了联邦学习中的非独立同分布数据问题，同时提升全局和局部模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中非独立同分布数据和引入公共数据集带来的隐私泄漏问题，促使研究者寻求一种既能保持模型泛化能力又能个性化本地模型的解决方案。

Method: 使用生成对抗网络（GAN）生成合成数据，通过冻结本地模型的参数作为判别器，并利用合成数据进行全局与本地模型间的双向知识蒸馏。

Result: 在4个基准测试中，FedBKD在各种非独立同分布设置下均取得了最佳性能。

Conclusion: FedBKD框架有效解决了联邦学习中的数据隐私和非独立同分布问题，同时提升了模型性能。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [72] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Key words: 量化大型语言模型、安全性评估、Q-resafe、校准数据集、性能恢复

TL;DR: 本文研究了量化大型语言模型（LLMs）的安全性问题，提出了一个量化感知的安全修补框架Q-resafe，有效恢复了量化模型的安全能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 量化LLMs在资源受限环境中具有重要应用价值，但现有研究表明量化可能损害模型的安全能力，亟需系统性评估和修复策略。

Method: 通过广泛的安全基准测试评估主流量化技术和多样校准数据集的安全性，提出Q-resafe框架修复安全隐患。

Result: 实验证明Q-resafe能够有效恢复量化LLMs的安全能力，与量化前模型性能对齐。

Conclusion: Q-resafe为解决量化LLMs的安全问题提供了可行的解决方案。

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [73] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Key words: 电力消费预测, 合成时间序列数据, WGAN, DDPM, HMM, MABF

TL;DR: 该论文深入比较了多种数据驱动方法在生成长时间电力消费预测的合成时间序列数据上的表现，重点评估了WGAN、DDPM、HMM和MABF等方法的能力与限制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 目前电力价值链中的研究多集中于短期发电或消费预测，且系统层面多于个体消费者层面，尤其是长时个体电力消费预测的研究较少。

Method: 采用混合WGAN、DDPM、HMM和MABF等技术，比较它们在复制电力消费时间动态、长程依赖性和概率转移方面的性能。

Result: 研究为状态估计等能源相关任务提供了选择最合适方法的依据，并提升了合成数据的准确性和隐私保护性。

Conclusion: 该框架增强了合成电力消费数据的准确性和可靠性，同时满足匿名化等隐私保护需求。

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [74] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Key words: 机器学习, 模型多样性, 反事实解释, 稳健性, 计算论证

TL;DR: 该论文探讨了在机器学习中模型多样性（MM）下提供反事实解释（CEs）的问题，并提出了一种新的方法——recourse-aware ensembling（RAE），以确保CEs的稳健性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在模型多样性情况下，不同模型对同一输入的预测可能不同，导致反事实解释缺乏稳健性。作者希望解决这一问题。

Method: 提出了一种基于计算论证的argumentative ensembling方法，通过显式表示模型与反事实之间的冲突，并利用论证语义解决冲突。

Result: 理论分析和实证研究表明，该方法在四种语义下均能有效满足六种期望性质。

Conclusion: 该方法能有效确保反事实解释在模型多样性下的稳健性，并支持模型偏好的定制化。

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [75] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Key words: 联邦学习, 集群联邦学习, 非IID数据, 知识蒸馏, 共享知识

TL;DR: 论文提出了一个新颖的联邦学习框架，通过学习多个集群的知识蒸馏出一个通用专家模型，平衡个性化和共享知识，提升非IID数据下的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法忽视了集群间的共享信息，这些信息对联邦学习系统的所有参与者具有普适性价值，因此需要一种新方法来捕捉和利用这些信息。

Method: 采用三步迭代框架：客户端本地训练、集群特定模型聚合、通用专家蒸馏；通过蒸馏实现模型异质性处理和减少集群间冲突。

Result: 实验证明，该方法在多种场景下表现优异，能更有效地平衡个性化和共享知识，提升性能。

Conclusion: 提出的蒸馏框架为集群联邦学习提供了新思路，通过结合共享和个性化知识，显著提升了非IID数据下的学习效果。

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [76] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Key words: QR码解码, Transformer模型, 输入敏感性, 错误纠正

TL;DR: Transformer模型能够成功解码QR码，甚至超越理论纠错限制，通过学习嵌入文本结构，并能从英语训练数据推广到其他语言和随机字符串。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究学习中等输入敏感性的函数，特别是在QR码解码任务上的表现。

Method: 使用Transformer模型进行QR码解码，测试其是否能够超越理论纠错限制。

Result: Transformer模型能够成功解码QR码，且表现优于传统方法，同时能推广到不同语言和随机字符串。

Conclusion: Transformer模型在QR码解码中表现出独特的解码机制，不同于传统QR码读取器。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [77] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Key words: 机器学习, 记忆, 隐私, 影响分布, （近）重复样本

TL;DR: 研究了机器学习模型中记忆训练数据的机制，指出仅依赖反事实自影响会低估记忆风险，并提出了基于全影响分布的新方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨机器学习模型中记忆训练数据的隐私和泛化问题，发现当前反事实自影响指标不足以全面评估记忆风险。

Method: 采用全影响分布分析方法，研究训练样本间的相互作用对小语言模型的影响，并扩展到图像分类任务。

Result: 发现（近）重复样本会显著降低自影响，但仍可能被（近）提取；全影响分布能更准确地揭示此类风险。

Conclusion: 记忆现象源于训练数据间的复杂相互作用，全影响分布比单纯的自影响更能全面评估记忆风险。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [78] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Key words: 模仿学习, 探索, ILDE, 专家性能, 样本效率

TL;DR: 论文提出了一种名为ILDE的新模仿学习算法，通过双重探索机制（乐观策略优化和好奇心驱动的状态探索）提升模仿学习的性能，实验表明其样本效率优于现有方法，且在较少演示下实现了超越专家性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 模仿学习在实践中的主要挑战是专家策略学习的准确性受限和需要进一步探索环境以超越专家性能。

Method: 提出的ILDE算法包含两部分：乐观策略优化（奖励高不确定性的状态-动作对）和好奇心驱动的状态探索（偏离演示轨迹的状态）。

Result: ILDE在Atari和MuJoCo任务上表现出色，样本效率高且用较少的演示实现了超越专家性能。

Conclusion: ILDE作为一种不确定性正则化策略优化方法，理论和实验证明了其有效性。

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [79] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Key words: 强化学习, 大型语言模型, 非策略算法, REINFORCE, 基线优化

TL;DR: 分析了基于非策略强化学习的REINFORCE算法，提出基线V的选择对算法性能的影响，并通过理论和实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何通过非策略强化学习方法来对齐大型语言模型（LLMs），同时提升其简单性和数据效率。

Method: 使用带基线的非策略REINFORCE算法，其中A=r-V，V为可调节的基线，分析其对算法性能的影响。

Result: 理论分析表明，当基线V低于期望奖励时，算法具有策略改进保证；实验在随机多臂赌博机环境和LLM推理任务中验证了结果。

Conclusion: 非策略更新更应关注正奖励信号，为LLM对齐提供了一种有效方法。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [80] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Key words: AI,作物病害检测,深度学习,迁移学习,可持续农业

TL;DR: 研究开发了一种基于AI的作物病害检测系统，通过比较不同深度学习模型（如EfficientNet、ResNet101、MobileNetV2和自定义CNN）的效能，验证了迁移学习在农业中的应用潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 帮助资源有限的农村农民通过AI技术改善作物健康管理，促进可持续农业。

Method: 比较多种深度学习模型（包括自定义CNN）的迁移学习效能。

Result: 自定义CNN在验证中的准确率达到95.76%，系统能有效分类植物病害。

Conclusion: 迁移学习有望重塑农业实践，支持农村的可持续耕作。

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [81] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Key words: LoRA, PLoP, 适配器放置, 微调, 强化学习

TL;DR: PLoP（Precise LoRA Placement）是一种轻量级方法，通过理论分析和实验验证，自动确定LoRA适配器的最佳放置位置，以提升大型模型的微调效果。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究LoRA适配器放置策略的优化问题，以解决现有方法在模块选择上的不一致性和效率问题。

Method: 通过直观的理论分析，提出PLoP方法，自动识别适合放置LoRA适配器的模块类型。

Result: PLoP在监督微调和强化学习推理任务中表现优于或至少与常用策略相当。

Conclusion: PLoP为LoRA适配器的放置提供了一种高效且自动化的解决方案。

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [82] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Key words: 动态图, Graph Neural CDEs, 排列等变性, 参数效率, 泛化能力

TL;DR: 论文提出了Permutation Equivariant Neural Graph CDEs模型，通过将Graph Neural CDEs投影到排列等变函数空间，减少了参数量，提升了训练效率和泛化能力，实验验证了其优势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决动态图中节点特征和网络结构演化带来的复杂时序动态问题，提出更高效且保留表征能力的模型。

Method: 将Graph Neural CDEs投影到排列等变函数空间，减少参数量。

Result: 实验表明在仿真和真实任务中，模型在插值和外推场景下性能提升。

Conclusion: 提出的模型在减少参数的同时保持了表征能力，提升了训练效率和泛化性能。

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [83] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Key words: 捆绑推荐、公平性、序列化推荐、生产者公平、启发式方法

TL;DR: 该论文研究了序列化捆绑推荐中的公平性问题，提出了一种结合生产商公平性和捆绑质量的实时解决方案，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实场景中，需要确保不同商品组在推荐过程中获得理想的曝光度，同时保持捆绑推荐的高质量。

Method: 论文提出了精确解决方案用于小规模问题，并研究了三种启发式方法：质量优先、公平优先和自适应平衡方法。

Result: 在三个真实数据集上的实验表明，这些方法能够在保证捆绑质量的同时实现公平推荐。

Conclusion: 通过平衡公平性和质量，提出的方法能有效解决序列化捆绑推荐中的公平性问题。

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [84] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Key words: 格拉杰因果, 深度学习, 时间序列, 模型不确定性, 残差分布

TL;DR: 该论文提出了一种基于深度学习的新方法，通过模型不确定性和残差分布来检测格拉杰因果关系，避免了传统变量选择方法的限制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统线性向量自回归模型（VAR）虽然解释性强，但在实际应用中受限。现有深度学习方法将格拉杰因果（GC）视为变量选择问题，缺乏灵活性。

Method: 论文提出了一种新范式，通过深度学习模型联合建模时间序列，利用模型不确定性或残差分布对比来揭示GC结构，并探讨了输入层dropout的影响。

Result: 研究表明，经过良好正则化的深度学习模型无需显式添加损失函数项，即可从数据中学习到真实的GC结构。

Conclusion: 深度学习模型可以有效学习格拉杰因果关系，为GC分析提供了新的思路。

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [85] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Key words: 大型语言模型, SVD压缩, 局部重要性保护, 全局重要性保护, 模型性能

TL;DR: 本文提出了一种双重重要性保护机制（DipSVD），通过局部和全局保护措施提升SVD压缩方法的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 大型语言模型（LLMs）的计算需求和部署成本激增，需要高效的压缩方法。现有SVD方法忽视了矩阵关键部分的保护，导致压缩模型性能下降。

Method: 提出双重重要性保护机制：1）局部保护——通过通道加权数据白化保留关键奇异向量；2）全局保护——通过启发式或优化方法让不重要层承担更多压缩负担。

Result: 实验表明，DipSVD在多个基准测试中优于现有SVD压缩方法，尤其在高压缩比下表现更佳。

Conclusion: DipSVD通过双重保护机制显著提升了SVD压缩的性能和适用性。

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [86] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Key words: MVPA, MVPFormer, iEEG, 自注意力机制, 时间序列预测

TL;DR: 提出了一种名为MVPA的新型自注意力机制，用于处理多变量时间序列数据中的异构通道配置问题，并构建了MVPFormer作为人类电生理学的生成基础模型。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决多变量时间序列数据中由于通道配置差异导致的深度学习模型泛化能力不足的问题，特别是在临床iEEG数据中。

Method: 使用MVPA（多变量并行注意力）机制，分离内容、时间和空间注意力，构建MVPFormer模型，并在SWEC iEEG数据集上进行训练。

Result: MVPFormer在癫痫检测任务中表现出专家水平性能，并在多个数据集上超越了现有的Transformer基线模型。

Conclusion: MVPA是一种适用于异构时间序列的通用注意力机制，MVPFormer是首个开源、开放的iEEG基础模型，具有顶尖的临床性能。

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [87] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Key words: 轨迹分析, 特征选择, 分类学, 高维数据, 可解释性

TL;DR: 提出了一种基于分类学的特征选择方法，用于解决轨迹分析中高维特征爆炸问题，提高了模型效率和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 轨迹分析中高维特征导致效率降低和解释性差，亟需一种方法优化特征选择。

Method: 分类学方法将特征分为几何和运动学两类，进一步细分为曲率、凹痕、速度和加速度，以减少组合空间。

Result: 分类学方法在预测性能上表现优越，特征选择时间显著减少，且能揭示数据集对特定特征集的敏感性。

Conclusion: 分类学特征选择方法降低了维度和计算复杂度，提升了可解释性，为可解释人工智能提供了方法论框架。

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [88] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Key words: LaplaceGNN,自监督学习,图神经网络,谱增强,对抗训练

TL;DR: LaplaceGNN是一个新颖的自监督图学习框架，通过利用谱引导技术避免负采样，结合拉普拉斯信号，高效捕捉结构表示。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决传统自监督图学习方法依赖负采样和手工增强的问题，提供更高效、简化的学习框架。

Method: 采用谱增强和对抗性引导训练方案，结合拉普拉斯信号，实现正向对齐和线性扩展。

Result: 在多个基准数据集上表现优于现有自监督图学习方法。

Conclusion: LaplaceGNN为高效学习图表示提供了新方向。

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [89] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Key words: 遥感基础模型,自监督学习,Sentinel-1 SAR,Sentinel-2 MSI,Transformer

TL;DR: TESSERA是一种新型遥感基础模型，通过自监督学习从像素级卫星时间序列数据中生成全球、稳健的10米尺度表征，融合光学和SAR数据，性能超越现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 通过开发高性能遥感基础模型，支持多样化地球观测应用，如气候建模和碳核算。

Method: 使用双Transformer编码器分别处理Sentinel-1 SAR和Sentinel-2 MSI数据，通过MLP融合生成全球表征。

Result: TESSERA在五个多样化任务中表现超越传统遥感基线和主流地理空间基础模型。

Conclusion: TESSERA为高分辨率遥感表征提供开源解决方案，显著提升了性能。

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [90] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Key words: AI, IoT, personalized learning, differential privacy, peer-to-peer, poisoning attacks

TL;DR: P4是一种为资源受限的IoT设备设计的个性化、私有、点对点学习方法，通过轻量级去中心化算法实现高效知识传递、数据隐私保护和抗中毒攻击。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着AI在IoT生态中的广泛应用，个性化学习在异构、资源受限设备上的高效和隐私保护需求日益突出。

Method: 提出P4方法，采用去中心化算法检测客户端相似性并形成协作组，利用差分隐私知识蒸馏共同训练模型。

Result: P4在基准数据集上表现优异，比同类方法精度提高5%至30%，并能抵御高达30%的恶意客户端。

Conclusion: P4在实用性、隐私保护和抗攻击性上均表现出色，适用于IoT设备的个性化学习需求。

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [91] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Key words: 未来策略评估, 非平稳环境, 时间序列, 重要性加权, 电子商务推荐

TL;DR: 该论文提出了一种新的非平稳环境下未来策略价值评估和学习方法（OPFV），利用时间序列数据中的结构信息来减少偏差，并在理论和实验上验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在非平稳环境中（如电子商务推荐系统），现有方法由于假设平稳性或依赖奖励模型，导致显著偏差。论文旨在解决这一局限，准确估计未来策略价值。

Method: 提出了一种名为OPFV的新估计器，通过时间序列数据中的结构（如季节性、节假日效应）进行重要性加权，并扩展到策略梯度方法以优化未来策略。

Result: 理论分析确定了OPFV低偏差的条件，实验结果显示该方法在非平稳环境下显著优于现有方法。

Conclusion: OPFV通过在历史数据中利用时间结构，有效解决了未来策略价值评估和学习的挑战，为实际应用提供了可行方案。

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [92] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Key words: 联邦学习,知识蒸馏,不公平聚合,异质性,KDIA

TL;DR: 提出了一种名为KDIA的策略，通过知识蒸馏和不公平聚合解决小部分客户参与的大规模联邦学习问题，实验表明其能有效提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习中客户标签偏斜、数据量偏斜等异质性问题，尤其是在大规模客户中仅少部分参与训练的挑战性场景。

Method: 采用知识蒸馏与教师-学生不公平聚合策略（KDIA），教师模型基于参与间隔、参与次数和数据量比例加权，学生模型为参与客户的平均聚合。训练时使用自知识蒸馏和服务器生成的近似IID数据辅助训练。

Result: 在CIFAR-10/100/CINIC-10等数据集和多种异质性设置下，KDIA在更少训练轮次下达到更高准确率，尤其在严重异质性下改进显著。

Conclusion: KDIA针对联邦学习中的异质性问题提出了一种有效的解决方案，能够提升模型性能，尤其在极端情况下表现突出。

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [93] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Key words: Physics-informed Neural Networks, PINNs, PDE, 采样点选择, Hessian矩阵

TL;DR: 提出了一种基于Hessian矩阵的积分近似方法，用于指导PINNs训练中采样点的选择。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 提升Physics-informed Neural Networks（PINNs）训练效率，通过优化采样点选择改进PDE求解精度。

Method: 基于Hessian矩阵的积分方法，动态调整训练过程中的采样点。

Result: 该方法改进了PINNs的训练效率和精度。

Conclusion: 新方法为PINNs的训练提供了一种更有效的采样策略。

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [94] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Key words: 上下文学习,表格数据分类,谱图理论,大语言模型

TL;DR: 本文提出了一种基于谱图理论的算法，用于自动确定表格数据分类中上下文学习（ICL）所需的演示数量，通过整合数据分布、用户提示模板和特定大语言模型（LLM），并验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 确定上下文学习中理想的演示数量是表格数据分类的关键问题，本文旨在解决这一挑战。

Method: 基于谱图理论，提出了一种新算法，通过构建相似性图并分析其拉普拉斯矩阵的特征值，量化演示间的相似性，从而确定最小演示数量。

Result: 实验表明，该方法在多样化数据集和LLM上的表现优于传统的随机选择算法。

Conclusion: 该方法能有效确定代表性演示数量，提升上下文学习的性能。

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [95] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Key words: 多模态学习, 表征学习, 对齐方法, 融合策略, AI系统

TL;DR: 多模态学习通过结合图像、文本和音频等多种信息源，帮助AI系统构建更强的内部表征，但在数据格式、不完整输入和对抗攻击等方面仍有挑战。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多模态学习旨在通过整合不同模态的信息，提升AI系统对复杂事物的理解和决策能力。

Method: 主要方法包括表征学习、对齐方法和融合策略。

Result: 多模态学习在计算机视觉、自然语言处理和医疗等领域展现了潜力。

Conclusion: 未来多模态学习有望推动AI系统实现类人类的理解能力。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [96] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Key words: 联邦学习,硬件优化,贪心随机搜索,本地批量大小,收敛速度

TL;DR: 通过硬件使用优化改进联邦学习中的本地训练过程，提出一种贪心随机搜索方法优化本地批量大小，加速收敛速度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习中，参与者共享硬件但缺乏信息交换，可能导致训练配置不当。优化本地训练配置以提高训练效率。

Method: 利用联邦学习的并行处理特性，使用贪心随机搜索优化所有参与者的本地批量大小。

Result: 与默认参数设置相比，该方法显著提升了收敛速度，接近局部参数优化的效果。

Conclusion: 贪心随机搜索方法在联邦学习中有效优化本地训练配置，提升训练效率。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [97] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Key words: 联邦学习（FL）、去中心化金融（DeFi）、激励机制、自动化做市商（AMMs）、投资机制

TL;DR: 本文提出了一种新颖的联邦学习激励框架，通过引入客户端特定代币和DeFi平台，实现更灵活、可扩展的奖励分配机制，并为第三方提供投资联邦学习过程的途径。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习（FL）在数据隐私和性能方面具有显著优势，但在商业化应用中，如何公平分配奖励和吸引投资仍是未充分研究的领域。

Method: 提出了一种基于客户端特定代币和DeFi平台的分布式金融解决方案，利用自动化做市商（AMMs）优化奖励分配和投资机制。

Result: 该框架为FL生态系统提供了更灵活、可扩展的奖励分配系统，并支持第三方投资，弥补了现有激励方案的不足。

Conclusion: 通过结合DeFi技术，该框架为商业化联邦学习的激励问题提供了创新解决方案。

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [98] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Key words: 工业NILM, 数据增强, 数字孪生, 能耗分解

TL;DR: 论文提出了SIDED数据集和AMDA方法，以解决工业NILM中数据稀缺和隐私问题，并显著提升了能耗分解模型的性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 工业NILM面临高质量数据集稀缺和能耗模式复杂多变的挑战，同时需解决数据隐私问题。

Method: 通过数字孪生模拟生成开源数据集SIDED，并提出高效的数据增强方法AMDA，基于设备功耗贡献调整数据分布。

Result: 使用AMDA增强数据的模型在复杂工业设备能耗分解中表现出色，归一化误差为0.093，优于其他方法。

Conclusion: SIDED和AMDA显著提升了工业NILM的模型泛化能力，为数据稀缺问题提供了有效解决方案。

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [99] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Key words: 激光粉末床熔融（LPBF），物理信息神经网络（PINN），有限元分析（FEA），热场预测，动态材料更新

TL;DR: 该研究提出了一种结合有限元分析（FEA）与物理信息神经网络（PINN）的高效建模框架FEA-PINN，用于加速激光粉末床熔融（LPBF）过程中的热场预测，同时保持FEA精度。通过动态材料更新策略和迁移学习，FEA-PINN在减少计算成本的同时达到了与FEA相当的精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 由于传统有限元分析（FEA）计算成本高、耗时长，LPBF过程的高效模拟成为迫切需求。本研究旨在通过结合FEA和PINN的优势，开发一种高效且精确的热场预测方法。

Method: 提出FEA-PINN框架，动态更新材料相变（粉末-液体-固体），引入表观热容法处理温度相关材料特性和相变行为。利用少量训练数据实现高精度预测，并通过迁移学习泛化新参数。为解决时间依赖问题中的误差累积，结合校正FEA仿真以增强物理一致性。

Result: FEA-PINN在保持FEA精度的同时显著降低了计算成本。通过基准FEA数据和单轨扫描实验验证了其有效性。

Conclusion: FEA-PINN为LPBF过程的热场预测提供了高效且精确的新方法，解决了传统FEA计算成本高的问题，具有实际应用潜力。

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [100] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Key words: 排队系统, 强化学习, 最优路由, 多目标优化, 参数调优

TL;DR: 研究了如何利用强化学习算法优化技能型排队系统的客户路由，通过真实数据集验证其高效性和适应性，并引入新启发式规则以减少延迟。算法还能优化多目标，如收益最大化和服务器负载公平性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索在复杂排队系统（如数据中心、云计算网络）中实现最优客户路由的可行性，以提升系统效率和适应性。

Method: 使用强化学习算法并结合新启发式路由规则，通过真实数据集进行实验，分析算法的学习和适应能力。

Result: 算法在动态环境中表现优于静态基准策略，并能够优化多目标（如收益、负载公平性）。同时对参数敏感性和估计误差进行了分析。

Conclusion: 该算法具有实际应用的潜力，能够有效管理和优化排队系统，但需注意参数调优和敏感性。

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [101] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Key words: 异常检测, 多变量时间序列, Transformer, iTransformer, 评估指标

TL;DR: 本文探讨了基于Transformer的多变量时间序列异常检测方法，重点分析了iTransformer架构及其关键参数对性能的影响，并提出了一套综合评估方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 多变量时间序列中的异常检测在多个领域至关重要，但因异常未知性和时间序列间复杂的相互依赖性而具有挑战性。Transformer模型为此提供了新的解决方案。

Method: 研究了iTransformer架构的异常检测应用，分析了窗口大小、步长和模型维度等关键参数；探索了从多维异常分数中提取标签的方法及评估指标；评估了训练中异常数据的影响及损失函数的缓解效果；并对多种Transformer模型进行了数据集广泛的比较。

Result: 综合分析了iTransformer在异常检测中的性能，提出了一套有效的方法和评估框架，展示了Transformer模型的潜力。

Conclusion: Transformer模型在多变量时间序列异常检测中表现优异，iTransformer架构的探索为实际问题提供了有价值的参考。

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [102] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Key words: 图神经网络, 分布外泛化, 图变换器, 域适应, 聚类分析

TL;DR: 该论文研究了图神经网络的分布外（OOD）泛化问题，重点比较了图变换器（GT）和混合架构与传统消息传递神经网络（MPNNs）的性能。结果表明GT架构在OOD场景下表现更优，并提出了一种新颖的后训练分析方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前图学习方法大多假设训练和测试数据分布相同，但现实场景中分布偏移常见。论文旨在探索图变换器在OOD泛化中的潜力。

Method: 系统评估GT和混合架构在OOD设置下的表现，并比较MPNNs；提出了一种分析ID与OOD数据集聚类结构的方法。

Result: GT和混合架构比MPNNs展现出更强的OOD泛化能力；后训练分析方法提供了模型无关的泛化洞察。

Conclusion: 图变换器在真实世界图学习中具有鲁棒性潜力，为OOD泛化研究指明了新方向。

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [103] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Key words: 支持向量图, 机器学习, 图索引, 核方法, 稀疏性

TL;DR: 通过机器学习构建支持向量图（SVG）用于非欧几里得空间的向量搜索，并提出SVG-L0优化方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统图索引仅适用于欧几里得空间，限制了其在其他向量空间的应用。

Method: 提出支持向量图（SVG），利用核方法建立图连接性，并提出SVG-L0以稀疏性约束优化图构建。

Result: SVG在非欧几里得空间具有理论可导航性，SVG-L0通过自调整特性简化算法复杂度。

Conclusion: SVG及SVG-L0为向量搜索提供了更通用的解决方案。

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [104] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Key words: 哈密顿系统，符号学习，能量守恒，H-FEX，复杂相互作用

TL;DR: H-FEX是一种符号学习方法，通过新颖的交互节点有效捕捉复杂哈密顿系统的相互作用项，实验证明其能准确恢复复杂系统的哈密顿函数并保持能量守恒。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的数据驱动方法（如符号回归和神经网络）在捕捉复杂哈密顿函数并保持能量守恒方面存在困难，因此需要一种更有效的方法。

Method: H-FEX通过引入新颖的交互节点，有效捕捉复杂的相互作用项，从而学习哈密顿系统的控制方程。

Result: 实验表明，H-FEX能够准确恢复复杂系统的哈密顿函数，并长期保持能量守恒。

Conclusion: H-FEX为发现复杂动力系统的闭合形式表达式提供了强有力的框架。

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [105] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Key words: 闭环学习、指数族、最大似然估计、最大后验估计、正则化

TL;DR: 摘要研究了闭环学习过程，特别是神经网络模型自我生成数据训练的动态行为。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究闭环学习过程，因为未来大型神经网络可能主要依赖于自我生成的数据进行训练。

Method: 研究属于指数族分布的模型，推导参数动态方程，分析最大似然估计的性质。

Result: 参数估计会放大初始数据偏差，但可通过污染数据、最大后验估计或正则化避免；动态行为的渐近结果不具有重参数不变性。

Conclusion: 闭环学习可能导致偏差放大，但有方法可以缓解这一现象。

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [106] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Key words: 

TL;DR: 论文提出了一种名为FedEDS的新联邦学习方案，通过加密数据共享解决网络拓扑、物理距离和数据异构性对边缘设备的影响。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着隐私保护日益重要，联邦学习（FL）在边缘设备上训练模型的需求增多，但现有研究忽视了网络拓扑、物理距离和数据异构性的影响，导致延迟增加和模型性能下降。

Method: 提出FedEDS方案，利用客户端模型和模型的随机层训练数据加密器，生成加密数据并与其他客户端共享。客户端使用对应随机层和加密数据训练本地模型。

Result: 实验结果表明，FedEDS能够加速联邦学习的收敛速度，减少数据异构性的负面影响。

Conclusion: FedEDS适用于需要快速收敛的边缘设备应用服务，提升了模型性能。

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [107] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Key words: 学习延迟, 代理损失函数, H一致性, Bayes一致性, 多专家系统

TL;DR: 该论文提出新的代理损失函数和高效算法，以解决多专家学习中的延迟分配问题，并提供了理论保证和实验验证。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在自然语言生成、图像处理和医疗诊断等领域，如何平衡专家系统的准确性和计算成本是一个关键挑战。

Method: 论文提出了一族新的可实现H一致性的代理损失函数，并针对单阶段和双阶段学习场景设计了高效算法。

Result: 实验结果表明，提出的代理损失函数在性能上优于现有基线方法。

Conclusion: 该研究在多专家学习的延迟分配问题上取得了理论突破，并通过实验验证了其有效性。

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [108] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Key words: 联邦学习, 梯度泄漏攻击, 隐私保护, 检测机制

TL;DR: 该论文分析了联邦学习中恶意梯度泄漏攻击的实践限制，并提出了一个轻量级客户端检测机制。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究联邦学习中梯度更新可能泄露客户端本地数据敏感信息的风险，尤其是恶意服务器通过操纵全局模型加剧这一问题的情况。

Method: 通过综合分析和实验，揭示了这些攻击在真实联邦学习环境中的效率与隐蔽性之间的权衡，并设计了一个简单的客户端检测机制。

Result: 研究发现恶意梯度泄漏攻击在现实中难以同时高效且隐蔽，且可以通过基本监控检测到。提出的检测机制能有效防御这类攻击。

Conclusion: 恶意梯度泄漏攻击在实践中受限且可检测，通过轻量级机制即可防御，为隐私敏感的联邦学习系统提供了可行保障。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [109] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Key words: 自动定理证明,大型语言模型,形式化证明,Lean,辅助引理

TL;DR: 提出了一种名为Prover Agent的新型AI代理，结合大型语言模型（LLMs）与形式化证明助手Lean，用于自动定理证明。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 通过整合LLMs和形式化证明工具，提高自动定理证明的成功率和效率。

Method: 通过协调非正式推理的LLM、形式化证明模型以及Lean的反馈，同时生成辅助引理来帮助发现整体证明策略。

Result: 在MiniF2F基准测试中取得了86.1%的成功率，成为使用小型语言模型（SLMs）且样本预算较低的当前最佳方法。

Conclusion: Prover Agent展示了通过整合LLMs和形式化工具，在自动定理证明中的显著效果。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [110] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Key words: 上下文归属、组合多臂老虎机、生成式问答、汤普森采样、查询效率

TL;DR: 提出一种基于组合多臂老虎机（CMAB）的新框架，用于高效探索和利用上下文中对生成答案贡献最大的部分，显著提升了查询效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了构建可解释且可信的生成式问答系统，需要识别检索到的上下文中哪些部分对模型生成答案有贡献。

Method: 将上下文归属问题建模为组合多臂老虎机（CMAB），每个上下文段视为老虎机臂，使用组合汤普森采样（CTS）在有限查询预算下高效探索上下文子集。定义基于归一化令牌似然的奖励函数，评估子集对模型回答的支持程度。

Result: 实验表明，该方法在多样化数据集和大型语言模型上以更少的模型查询实现了有竞争力的归属质量。

Conclusion: 该方法成功平衡了探索与利用，显著提高了查询效率，同时保持了高归属保真度。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [111] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Key words: 大型语言模型, 量子计算, PennyLane, 检索增强生成, QHackBench

TL;DR: 该论文提出了一种新的基准数据集QHackBench，用于评估大型语言模型（LLMs）在PennyLane量子计算代码生成中的性能，并通过检索增强生成（RAG）和多智能体评估流程优化结果。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 虽然LLMs在代码生成方面表现优异，但其在量子计算领域的应用尚未充分探索。本文旨在填补这一空白，通过真实世界的量子编程挑战（Quantum Hackathon）来评估LLMs的性能。

Method: 1. 创建QHackBench数据集；2. 使用标准提示和RAG方法评估模型；3. 引入多智能体评估流程迭代优化错误解决方案；4. 评估功能正确性、语法有效性和执行成功率。

Result: RAG增强模型在复杂量子算法中表现与标准提示相似，而多智能体流程进一步提高了执行成功率。

Conclusion: QHackBench和配套评估框架的发布将推动AI辅助量子编程的研究进展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [112] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Key words: AI, healthcare, RAG, energy efficiency, sustainability

TL;DR: 研究开发了一种可定制的RAG框架，用于医疗任务，其性能优于商业模型，且能耗更低。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着AI在医疗领域的广泛应用，其环境与伦理问题引发关注，需开发可持续的解决方案。

Method: 开发了能监测能耗与CO2排放的RAG框架，并基于开源LLMs构建模型进行测试。

Result: 自定义RAG模型在准确性和能耗上均优于商业模型，且环境足迹更小。

Conclusion: 本地LLM可构建高性能、低能耗的RAG框架，符合可持续发展目标。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [113] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Key words: 实时决策支持,低延迟AI,Edge-IoT,人机协作,模型压缩

TL;DR: 研究了利用低延迟AI模型的实时决策支持系统，整合了AI驱动决策工具、Edge-IoT技术及人机协作方法，探讨了大语言模型在资源受限下的应用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索如何通过技术进展（如DeLLMa、模型压缩和边缘设备分析改进）解决资源限制和框架适应性需求，提升实时决策支持效能。

Method: 结合文献综述，分析低延迟AI模型、Edge-IoT整合及人机协作的最佳实践，提出开发策略和应用场景。

Result: 提供实用的开发视角，强调高效灵活AI支持系统的机会，为未来突破奠定基础。

Conclusion: AI有望重塑实时决策支持，未来研究方向包括资源优化和适应性框架开发。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [114] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Key words: 大型语言模型、动机性推理、认知偏差、身份认同、政治极化

TL;DR: 研究发现，大型语言模型（LLMs）在被赋予特定身份角色时，会表现出类似人类的动机性推理偏差，尤其在政治和社会议题上，且传统提示去偏方法效果有限。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探讨LLMs是否会因身份角色分配而产生动机性推理偏差，尤其是在政治和社会议题中。

Method: 为8种LLMs分配8种不同政治和社会身份角色，测试其在验证信息真伪和评估科学证据时的表现，并尝试用提示去偏。

Result: LLMs在身份角色下推理准确性下降9%，政治身份下的科学证据评估正确率高达90%，且去偏提示效果不佳。

Conclusion: LLMs表现出类似人类的动机性推理偏差，传统去偏方法难以缓解，可能加剧身份认同推理问题。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [115] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Key words: Large Language Models, Electronic Health Records, Clinical Dialogue, Reinforcement Learning, Diagnosis Prediction

TL;DR: DiaLLM是一种新型医疗大语言模型，整合电子健康记录以支持临床对话，提升测试推荐和诊断预测能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有医疗大语言模型忽视电子健康记录（EHR）的重要性，仅关注诊断推荐，限制了临床实用性。

Method: 提出DiaLLM，整合EHR数据，设计临床测试参考策略，采用强化学习框架处理证据收集和诊断自动化。

Result: 实验表明，DiaLLM在临床测试推荐和诊断预测方面优于基线模型。

Conclusion: DiaLLM通过整合EHR和改进强化学习策略，显著提升了医疗对话模型的临床适用性。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [116] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Key words: 开放科学, 可重复性, AI工具, Reproducibility Copilot

TL;DR: OpenPub是一个AI驱动平台，通过Reproducibility Copilot模块帮助研究人员提高研究成果的计算可重复性，大幅减少重复时间。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决研究结果透明性和可重复性的挑战，促进开放科学。

Method: 开发了Reproducibility Copilot，分析手稿、代码等材料，生成结构化Jupyter Notebook和建议。

Result: 测试显示，Reproducibility Copilot将重复时间从30多小时缩短至约1小时，并覆盖大量可计算重复的结果。

Conclusion: AI工具能有效减轻可重复性负担，提升科学交流的透明度和可验证性。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [117] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Key words: LLM, 语言模型架构, 遗传编程, 规模梯级验证, 自主发现

TL;DR: 提出了一种名为Genesys的多智能体LLM系统，用于发现新语言模型架构，通过遗传编程和规模梯级验证，性能优于现有模型。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 探索是否可以利用LLM模拟研究过程，从提议到验证，自动发现新语言模型架构。

Method: 采用多智能体LLM系统Genesys，结合遗传编程和规模梯级验证，逐步筛选和优化设计。

Result: 生成了1,162个新设计，其中1,062个完成预训练验证，最佳设计在多个基准测试中优于GPT2和Mamba2。

Conclusion: Genesys系统有效推动了语言模型架构的自动发现，为未来自主研究系统设计提供了新思路。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [118] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Key words: 大型语言模型、企业评估、Bloom分类学、标注管道、基准测试

TL;DR: 提出了一种基于Bloom分类学的14任务框架，用于全面评估大型语言模型（LLM）在企业环境中的能力，解决了现有基准测试的不足。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基准测试（如MMLU）无法充分评估企业特定任务的复杂性，需要更全面的评估框架。

Method: 开发了一个可扩展的标注管道（LLM-as-a-Labeler、LLM-as-a-Judge和CRAG），并创建了包含9,700个样本的强大基准。

Result: 评估显示开源模型（如DeepSeek R1）在推理任务上与专有模型相当，但在判断性任务上表现较差。

Conclusion: 研究为企业提供了定制化评估蓝图，推动了LLM的实际部署。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [119] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Key words: 移动代理,强化学习,任务级奖励,探索能力,纠错能力

TL;DR: 提出了Mobile-R1方法，通过交互式多轮强化学习和任务级奖励，提升移动代理的探索和纠错能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有基于强化学习的移动代理研究多采用离线训练或动作级奖励，限制了与环境的动态交互，容易陷入局部最优，需改进其探索和纠错能力。

Method: 采用三阶段训练框架：初始格式微调、动作级奖励的在线单步训练、任务级奖励的多轮轨迹训练。

Result: 显著提升性能，并开源了涵盖28个中文应用的数据集、基准测试、模型权重和代码。

Conclusion: Mobile-R1有效增强移动代理的动态交互和任务完成能力。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [120] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Key words: 特征工程、大型语言模型（LLM）、REFeat、推理引导、表格数据

TL;DR: 本文提出了一种名为REFeat的新方法，利用大型语言模型（LLM）通过多种推理引导生成多样且信息丰富的特征，解决了现有方法生成特征过于简单或重复的问题。实验证明，该方法在59个基准数据集上实现了更高的预测准确性，并生成了更多样和有意义的特征。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 目前基于LLM的特征生成方法存在生成的特性过于简单或重复的问题，主要由于LLM选择的转换固有偏见和生成过程中缺乏结构化推理指导。因此，需要一种新方法来引导LLM生成更高质量的特征。

Method: 提出了REFeat方法，通过利用多种类型的推理引导LLM的特征生成过程，从而生成多样且信息丰富的特征。

Result: 在59个基准数据集上，REFeat不仅平均预测准确率更高，还生成了更多样和有意义的特征。

Conclusion: 研究表明，将丰富的推理范式和自适应策略选择引入LLM驱动的特征发现中具有广阔的前景。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [121] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Key words: grounding, classifier model, benchmark, real-world scenarios

TL;DR: 介绍了两种解决文本中主张基础性问题的贡献：一个紧凑的开源分类模型Paladin-mini和一个新的评估数据集grounding-benchmark。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决在给定上下文中基础主张的问题，即确保主张在文档中有支持性证据。

Method: 开发Paladin-mini（3.8B参数的分类模型）和grounding-benchmark评估数据集，测试其在关键推理任务中的表现。

Result: 展示了Paladin-mini与当前最先进技术的基准对比结果，并提供了可重复的结果。

Conclusion: Paladin-mini和grounding-benchmark为现实场景中的基础性问题提供了有效解决方案。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [122] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Key words: 电动汽车, V2G技术, 利润最大化, 混合整数规划, 元启发式算法

TL;DR: 论文提出了一种结合电动汽车（EV）与车辆到电网（V2G）技术的优化问题（EVOP-V2G），旨在最大化司机利润，并通过混合整数规划和两种元启发式算法实现高效求解。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着电动汽车的普及和服务系统对其的整合，如何优化充电和放电策略以最大化利润，同时支持电网，成为重要问题。

Method: 提出了混合整数规划（MIP）模型和两种元启发式算法：进化算法（EA）和大邻域搜索（LNS）。

Result: 实验表明，新方法能显著提高司机利润（相比基线翻倍），在小规模实例中接近最优，且在大规模实例中表现优异。

Conclusion: 研究为电动汽车驱动的智能交通系统提供了高效且盈利的解决方案，同时支持电网稳定。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [123] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Key words: 业务流程管理,深度强化学习,任务分配,决策优化

TL;DR: GymPN是一个基于深度强化学习的软件库，用于优化业务流程中的决策，支持部分流程可见性和多决策建模。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 现有业务流程管理系统需要更优的决策支持工具，以解决任务分配和执行的复杂问题。

Method: GymPN基于深度强化学习，扩展了之前的工作，支持部分流程可见性和多决策建模。

Result: 在八种典型业务流程决策问题模式中，GymPN能轻松建模并学习到最优决策策略。

Conclusion: GymPN解决了现有方法的局限性，提供了更现实的业务流程决策支持。

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [124] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Key words: 神经细胞自动机, 随机动态, 混合模型, 生物模拟, 图像分割

TL;DR: 论文提出了一种混合神经细胞自动机（MNCA）框架，通过结合概率规则和内在噪声，解决了传统神经细胞自动机（NCA）无法捕捉现实世界随机性的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统NCA的确定性限制了其对生物和物理系统随机性的建模能力，因此需要一种新方法来捕捉这种随机动态。

Method: 提出MNCA框架，将混合模型的概念引入NCA，结合概率规则和内在噪声来模拟多样化的局部行为。

Result: MNCA在组织生长模拟、图像形态生成和显微镜图像分割中表现出更高的鲁棒性、更真实的生物生长模式以及可解释的规则分割。

Conclusion: MNCA是一种有前景的工具，用于建模随机动态系统和研究自生长过程。

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [125] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Key words: 感知, 人工智能, 功能性, 可计算化, 伦理

TL;DR: 提出一种可计算化的感知定义，用于机器实现，强调其功能性与主观性，并探讨潜在实现方法及其伦理意义。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为AI设计有意义的功能性感知定义，并避免无意中创造出具有感知能力的机器。

Method: 通过功能性、可计算的方式定义感知，要求感官信号具有持久性和定性特征，并提出实现方法。

Result: 提出一种具体的感知定义，并探讨其在当前技术下的潜在实现途径。

Conclusion: 功能性感知定义有助于AI设计与伦理考量，避免无意创造具有感知能力的机器。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [126] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Key words: 自动驾驶，大型语言模型，案例推理，风险场景，决策支持

TL;DR: 论文提出了一种基于案例推理增强的大型语言模型（CBR-LLM）框架，用于复杂风险场景下的避障决策，通过结合语义场景理解和历史案例检索，提升决策准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 自动驾驶在安全关键场景中需要快速且基于情境的决策，但现有大型语言模型（LLMs）在领域适应性和情境理解方面存在局限，无法直接应用于高风险动态环境。

Method: 采用CBR-LLM框架，结合语义场景分析和历史案例检索，利用LLM生成情境敏感且符合人类行为的避障建议。

Result: 实验显示，框架显著提升决策准确性、解释质量和与人类专家行为的一致性，案例检索策略优于随机采样。

Conclusion: CBR-LLM框架在复杂风险场景中表现出色，有望作为自动驾驶系统的自适应可信决策支持工具。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [127] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Key words: 可持续蛋白质、多代理AI、RAG、GPT、微调、提示工程

TL;DR: 提出了一种多代理AI框架，用于可持续蛋白质生产研究，特别关注微生物蛋白源，通过RAG方法和两种优化策略提升了信息提取性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 全球对可持续蛋白质的需求推动了需要快速处理和合成领域知识的智能工具。

Method: 采用基于GPT的LLM代理，结合文献搜索和信息提取代理，探索了微调和提示工程两种优化方法。

Result: 微调方法在性能上表现更优（均值≥0.94），提示工程的不确定性更低，用户界面已开发并发布。

Conclusion: 多代理AI框架在微生物蛋白研究中表现出色，未来可扩展更多功能。

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [128] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Key words: CogGen, 生成式AI, 认知学徒框架, 贝叶斯知识追踪, 视频编程教育

TL;DR: CogGen是一种以学习者为中心的AI架构，通过将学生建模与生成式AI辅导结合，将编程视频转化为互动的自适应学习体验。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 提升视频编程教育的交互性和适应性，结合认知学徒框架。

Method: 视频按学习目标分段，基于认知学徒策略的对话辅导引擎，以及使用贝叶斯知识追踪的学生模型。

Result: 视频分段准确且教学策略与知识、方法、行动和互动层对齐，各部分对生成有效指导均为必需。

Conclusion: CogGen通过结合结构化学生建模与交互式AI对话，为视频编程教育提供了可扩展的解决方案。

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [129] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Key words: 生成AI, 大语言模型, PETSc, 检索增强生成, 科学计算

TL;DR: 本文介绍了一个基于大语言模型（LLM）的系统，用于整合和利用PETSc（高性能科学计算数值库）的分散知识库，提升用户和开发者的访问效率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: PETSc积累了丰富但分散的非正式知识，难以被用户和开发者充分利用。

Method: 结合检索增强生成（RAG）、重排算法和聊天机器人等定制LLM工具，设计系统架构和评估方法。

Result: 通过Argonne领导计算设施资源，验证了LLM对数值软件开发和使用（如Krylov求解器）的提升作用。

Conclusion: 目标是建立一个可扩展的科学软件知识中心AI框架，未来进一步发展为加速科学发现的平台。

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [130] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Key words: 大型语言模型,机器学习代理,MLE-Live,CoMind,Kaggle竞赛

TL;DR: 提出了MLE-Live框架和CoMind代理，用于评估和提升机器学习代理在模拟研究社区中的协作能力。CoMind在Kaggle竞赛中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决现有机器学习代理在孤立环境中工作的问题，模拟人类研究社区的协作方式。

Method: 开发MLE-Live框架评估代理的协作能力，提出CoMind代理，利用社区知识进行创新。

Result: CoMind在MLE-Live框架中表现优异，且在四个Kaggle竞赛中平均胜率79.2%。

Conclusion: 通过MLE-Live和CoMind，展示了机器学习代理在协作研究中的潜力。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [131] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Key words: 大型语言模型,多智能体,心智理论,基准测试,交互式实验

TL;DR: 论文提出了Decrypto，一个基于游戏的多智能体推理和心智理论基准，填补了现有基准的不足，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 随着大型语言模型（LLM）具备代理能力，需要其处理复杂的多智能体场景，但目前对LLM的心智理论（ToM）能力理解不足，现有基准存在局限性。

Method: 提出Decrypto基准，结合认知科学、计算语用学和多智能体强化学习，设计交互式ToM实验平台，并通过综合实验验证。

Result: 发现前沿LLM在游戏能力上落后于人类和简单词嵌入基线；部分经典ToM任务中，先进推理模型表现反而更差。

Conclusion: Decrypto填补了当前推理和ToM评估的空白，为改进人工智能代理铺平了道路。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [132] [Finite-Time Information-Theoretic Bounds in Queueing Control](https://arxiv.org/abs/2506.18278)
*Yujie Liu,Vincent Y. F. Tan,Yunbei Xu*

Key words: 调度问题、随机处理网络、有限时间性能、信息论下界、Lyapunov漂移

TL;DR: 该论文首次建立了有限时间内信息论下界，并提出了达到这些下界的新调度策略，揭示了MaxWeight在有限时间内的次优性。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 研究在随机处理网络中调度问题的有限时间性能，填补MaxWeight策略在有限时间内性能分析的空白。

Method: 采用极小极大框架、信息论下界分析，并提出一种新的调度规则以最小化Lyapunov漂移。

Result: 揭示了MaxWeight在有限时间内的次优性，并提出了匹配下界的新策略。

Conclusion: 研究指出了仅依赖漂移方法的局限性，为队列控制的非渐近最优性提供了理论基础。

Abstract: We establish the first finite-time information-theoretic lower bounds-and
derive new policies that achieve them-for the total queue length in scheduling
problems over stochastic processing networks with both adversarial and
stochastic arrivals. Prior analyses of MaxWeight guarantee only stability and
asymptotic optimality in heavy traffic; we prove that, at finite horizons,
MaxWeight can incur strictly larger backlog by problem-dependent factors which
we identify. Our main innovations are 1) a minimax framework that pinpoints the
precise problem parameters governing any policy's finite-time performance; 2)
an information-theoretic lower bound on total queue length; 3) fundamental
limitation of MaxWeight that it is suboptimal in finite time; and 4) a new
scheduling rule that minimizes the full Lyapunov drift-including its
second-order term-thereby matching the lower bound under certain conditions, up
to universal constants. These findings reveal a fundamental limitation on
"drift-only" methods and points the way toward principled, non-asymptotic
optimality in queueing control.

</details>


### [133] [A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization](https://arxiv.org/abs/2506.20344)
*Po Chen,Rujun Jiang,Peng Wang*

Key words: 深度矩阵分解，优化，损失景观，临界点，鞍点

TL;DR: 该论文对正则化深度矩阵分解（DMF）问题的损失景观进行了全面研究，提供了所有临界点的闭式表达式，并阐明了临界点成为局部最小值、全局最小值或鞍点的条件。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 尽管深度矩阵分解在许多领域有广泛应用，但其优化理论基础尚不完善。本研究旨在填补这一空白。

Method: 通过数学分析提供临界点的闭式表达式，并建立临界点性质的精确条件。数值实验用于验证理论。

Result: 揭示了临界点的类型条件，并解释了梯度方法为何通常收敛到局部最小值。

Conclusion: 该研究为深度矩阵分解的优化提供了理论基础，支持了梯度方法的有效性。

Abstract: Despite its wide range of applications across various domains, the
optimization foundations of deep matrix factorization (DMF) remain largely
open. In this work, we aim to fill this gap by conducting a comprehensive study
of the loss landscape of the regularized DMF problem. Toward this goal, we
first provide a closed-form expression of all critical points. Building on
this, we establish precise conditions under which a critical point is a local
minimizer, a global minimizer, a strict saddle point, or a non-strict saddle
point. Leveraging these results, we derive a necessary and sufficient condition
under which each critical point is either a local minimizer or a strict saddle
point. This provides insights into why gradient-based methods almost always
converge to a local minimizer of the regularized DMF problem. Finally, we
conduct numerical experiments to visualize its loss landscape under different
settings to support our theory.

</details>


### [134] [First-order methods for stochastic and finite-sum convex optimization with deterministic constraints](https://arxiv.org/abs/2506.20630)
*Zhaosong Lu,Yifeng Xiao*

Key words: 随机优化，凸优化，确定性约束，加速随机梯度，方差减少

TL;DR: 本文研究了具有确定性约束的随机和有限和凸优化问题，提出了一种新的方法来寻找确定性约束几乎满足的随机最优解（ϵ-SFSO），并通过加速随机梯度或改进的方差减少方法实现。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 现有方法通常寻求期望可行解，但在实际应用中约束需要几乎确定性地满足，因此需要一种新方法以避免显著违反约束的风险。

Method: 采用加速随机梯度（ASG）或改进的方差减少ASG方法，通过一系列适当选择惩罚参数的二次惩罚子问题来实现目标。

Result: 建立了计算ϵ-SFSO解的一阶oracle复杂度界限，并为样本平均近似方法提供了类似结果。

Conclusion: 所提出的方法能够有效解决确定性约束几乎满足的随机优化问题，并为相关应用提供了理论支持。

Abstract: In this paper, we study a class of stochastic and finite-sum convex
optimization problems with deterministic constraints. Existing methods
typically aim to find an $\epsilon$-$expectedly\ feasible\ stochastic\ optimal$
solution, in which the expected constraint violation and expected optimality
gap are both within a prescribed tolerance $\epsilon$. However, in many
practical applications, constraints must be nearly satisfied with certainty,
rendering such solutions potentially unsuitable due to the risk of substantial
violations. To address this issue, we propose stochastic first-order methods
for finding an $\epsilon$-$surely\ feasible\ stochastic\ optimal$
($\epsilon$-SFSO) solution, where the constraint violation is deterministically
bounded by $\epsilon$ and the expected optimality gap is at most $\epsilon$.
Our methods apply an accelerated stochastic gradient (ASG) scheme or a modified
variance-reduced ASG scheme $only\ once$ to a sequence of quadratic penalty
subproblems with appropriately chosen penalty parameters. We establish
first-order oracle complexity bounds for the proposed methods in computing an
$\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle
complexity results for sample average approximation method in computing an
$\epsilon$-SFSO solution of the stochastic optimization problem using our
proposed methods to solve the sample average problem.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [135] [Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data](https://arxiv.org/abs/2506.20141)
*Xiaoyu Li,Zhao Song,Jiahao Zhang*

Key words: 投稿限制, 拒稿策略, 线性规划, 会议管理, 计算机科学

TL;DR: 本文提出了一种优化方法，以减少因投稿限制而导致的无效拒稿，并在实际会议数据中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.DS

Motivation: 由于AI研究快速发展，会议投稿量激增，导致许多会议实行严格的投稿限制和简单拒稿政策，这可能无意中损害有价值的论文和作者的劳动成果。本文旨在研究如何在遵循投稿限制的同时，最小化无效拒稿。

Method: 首先将当前拒稿政策形式化为一个优化问题，然后开发了一种基于线性规划松弛和舍入方案的实用算法。

Result: 在11年的ICLR实际数据评估中，该方法在不违反作者投稿限制的情况下，最多可保留19.23%的论文，且算法高效，计算时间不超过53.64秒。

Conclusion: 本文提供了一种简单实用的拒稿策略，显著减少了不必要的拒稿，展示了改进当前计算机科学会议投稿政策的潜力。

Abstract: The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [136] [DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules](https://arxiv.org/abs/2506.19862)
*Junjie Xu,Jiahao Zhang,Mangal Prakash,Xiang Zhang,Suhang Wang*

Key words: 几何图神经网络, 生物分子建模, 欧几里得空间, 球谐空间, 多尺度建模

TL;DR: DualEquiNet是一种新型的双空间几何图神经网络，通过结合欧几里得和球谐空间的特征，解决了现有方法在建模大型生物分子时的局限性，实现了多尺度的高效表达。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 现有的几何图神经网络（GNNs）在小分子建模中表现优异，但在处理大型生物分子（如RNA和蛋白质）时面临可扩展性和表达能力的挑战。需要同时捕捉原子级的相互作用和长距离依赖关系，以及生物相关的层次结构。

Method: DualEquiNet在欧几里得和球谐空间中构建互补表示，利用双向跨空间消息传递和跨空间交互池化机制，将原子特征分层聚合为生物学上有意义的单元。

Result: DualEquiNet在RNA性质预测和蛋白质建模的多个基准测试中实现了最先进的性能，并在两个新引入的3D结构基准测试中超过现有方法。

Conclusion: DualEquiNet通过双空间设计有效解决了大型生物分子多尺度建模的挑战，具有广泛的适用性和高效性。

Abstract: Geometric graph neural networks (GNNs) that respect E(3) symmetries have
achieved strong performance on small molecule modeling, but they face
scalability and expressiveness challenges when applied to large biomolecules
such as RNA and proteins. These systems require models that can simultaneously
capture fine-grained atomic interactions, long-range dependencies across
spatially distant components, and biologically relevant hierarchical structure,
such as atoms forming residues, which in turn form higher-order domains.
Existing geometric GNNs, which typically operate exclusively in either
Euclidean or Spherical Harmonics space, are limited in their ability to capture
both the fine-scale atomic details and the long-range, symmetry-aware
dependencies required for modeling the multi-scale structure of large
biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant
Network that constructs complementary representations in both Euclidean and
Spherical Harmonics spaces to capture local geometry and global symmetry-aware
features. DualEquiNet employs bidirectional cross-space message passing and a
novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate
atomic features into biologically meaningful units, such as residues, enabling
efficient and expressive multi-scale modeling for large biomolecular systems.
DualEquiNet achieves state-of-the-art performance on multiple existing
benchmarks for RNA property prediction and protein modeling, and outperforms
prior methods on two newly introduced 3D structural benchmarks demonstrating
its broad effectiveness across a range of large biomolecule modeling tasks.

</details>


### [137] [Scalable and Cost-Efficient de Novo Template-Based Molecular Generation](https://arxiv.org/abs/2506.19865)
*Piotr Gaiński,Oussama Boussif,Andrei Rekesh,Dmytro Shevchuk,Ali Parviz,Mike Tyers,Robert A. Batey,Michał Koziarski*

Key words: 分子生成, 模板基方法, GFlowNets, 递归成本指导, 动态库

TL;DR: 本文提出了一种基于模板的分子生成方法，通过递归成本指导和动态库机制解决合成成本、大规模构建块库和小片段集利用的挑战，显著提高了成本效率和分子多样性。

<details>
  <summary>Details</summary>

Main category: q-bio.BM

Motivation: 解决模板基GFlowNets中合成成本高、构建块库规模大和小片段集利用效率低的三大核心挑战。

Method: 提出递归成本指导框架，结合辅助机器学习模型近似合成成本和可行性；引入动态库机制重用高奖励状态构建完整合成树。

Result: 方法在模板基分子生成中取得了最先进的结果，显著提升了成本效率、分子多样性和质量。

Conclusion: 递归成本指导和动态库机制的有效性为模板基分子生成提供了新的解决方案。

Abstract: Template-based molecular generation offers a promising avenue for drug design
by ensuring generated compounds are synthetically accessible through predefined
reaction templates and building blocks. In this work, we tackle three core
challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)
scaling to large building block libraries, and (3) effectively utilizing small
fragment sets. We propose \textbf{Recursive Cost Guidance}, a backward policy
framework that employs auxiliary machine learning models to approximate
synthesis cost and viability. This guidance steers generation toward low-cost
synthesis pathways, significantly enhancing cost-efficiency, molecular
diversity, and quality, especially when paired with an \textbf{Exploitation
Penalty} that balances the trade-off between exploration and exploitation. To
enhance performance in smaller building block libraries, we develop a
\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states
to construct full synthesis trees. Our approach establishes state-of-the-art
results in template-based molecular generation.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [138] [Physics-Guided Radiotherapy Treatment Planning with Deep Learning](https://arxiv.org/abs/2506.19880)
*Stefanos Achlatis,Efstratios Gavves,Jan-Jakob Sonke*

Key words: 放射治疗,深度学习,VMAT,3D剂量分布,自适应治疗

TL;DR: 提出了一个两阶段、物理引导的深度学习流程，用于放射治疗计划，能够高效生成与临床实际情况匹配的治疗方案，减少对危险器官的辐射暴露。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 现有的自适应放射治疗需要对治疗计划进行频繁修改以适应解剖结构变化，传统方法效率低，深度学习提供了一种自动化的解决方案。

Method: 采用两阶段的深度学习流程，第一阶段直接监督MLC和MU值，第二阶段加入3D剂量分布的额外监督信号，结合了物理引导。使用3D U-Net和UNETR架构。

Result: 在133名前列腺癌患者数据上验证，生成的计划与临床实际情况接近，PTV的D95%和V95%差异小，同时减少了危险器官的辐射暴露。

Conclusion: 物理引导的深度学习在放射治疗计划中具有潜力，能够高效生成高质量的治疗方案。

Abstract: Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated
arc therapy (VMAT) being a commonly used technique that enhances dose
conformity by dynamically adjusting multileaf collimator (MLC) positions and
monitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires
frequent modifications to treatment plans to account for anatomical variations,
necessitating time-efficient solutions. Deep learning offers a promising
solution to automate this process. To this end, we propose a two-stage,
physics-guided deep learning pipeline for radiotherapy planning. In the first
stage, our network is trained with direct supervision on treatment plan
parameters, consisting of MLC and MU values. In the second stage, we
incorporate an additional supervision signal derived from the predicted 3D dose
distribution, integrating physics-based guidance into the training process. We
train and evaluate our approach on 133 prostate cancer patients treated with a
uniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target
volume (PTV). Our results demonstrate that the proposed approach, implemented
using both 3D U-Net and UNETR architectures, consistently produces treatment
plans that closely match clinical ground truths. Our method achieves a mean
difference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV
while generating dose distributions that reduce radiation exposure to organs at
risk. These findings highlight the potential of physics-guided deep learning in
RT planning.

</details>


### [139] [Neural networks for the prediction of peel force for skin adhesive interface using FEM simulation](https://arxiv.org/abs/2506.19855)
*Ashish Masarkar,Rakesh Gupta,Naga Neehar Dingari,Beena Rai*

Key words: 粘合剂剥离、神经网络、有限元模拟、生物医学应用、计算效率

TL;DR: 提出了一种基于神经网络的粘合剂剥离力预测方法，显著降低计算成本并减少模拟时间。

<details>
  <summary>Details</summary>

Main category: physics.med-ph

Motivation: 研究粘合剂在皮肤上的剥离行为对医学粘合剂和透皮贴片等生物医学应用至关重要。传统方法资源消耗大且耗时。

Method: 利用有限元模拟生成数据集，训练神经网络预测剥离力，并通过5折交叉验证验证性能。

Result: 模型预测表现优秀，测试集MSE为3.66*10^-7，R^2得分为0.94。

Conclusion: 该方法高效可靠，为皮肤-粘合剂系统的设计和优化提供了可扩展框架。

Abstract: Studying the peeling behaviour of adhesives on skin is vital for advancing
biomedical applications such as medical adhesives and transdermal patches.
Traditional methods like experimental testing and finite element method (FEM),
though considered gold standards, are resource-intensive, computationally
expensive and time-consuming, particularly when analysing a wide material
parameter space. In this study, we present a neural network-based approach to
predict the minimum peel force (F_min) required for adhesive detachment from
skin tissue, limiting the need for repeated FEM simulations and significantly
reducing the computational cost. Leveraging a dataset generated from FEM
simulations of 90 degree peel test with varying adhesive and fracture mechanics
parameters, our neural network model achieved high accuracy, validated through
rigorous 5-fold cross-validation. The final architecture was able to predict a
wide variety of skin-adhesive peeling behaviour, exhibiting a mean squared
error (MSE) of 3.66*10^-7 and a R^2 score of 0.94 on test set, demonstrating
robust performance. This work introduces a reliable, computationally efficient
method for predicting adhesive behaviour, significantly reducing simulation
time while maintaining accuracy. This integration of machine learning with
high-fidelity biomechanical simulations enables efficient design and
optimization of skin-adhesive systems, providing a scalable framework for
future research in computational dermato-mechanics and bio-adhesive material
design.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [140] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Key words: 可解释模型,皮层表面数据,医学图像,X-SiT,阿尔茨海默病,额颞叶痴呆

TL;DR: 论文提出了X-SiT，一种可解释的神经网络，用于基于皮层表面数据检测阿尔茨海默病和额颞叶痴呆，兼具高效分类和可视化解释能力。

<details>
  <summary>Details</summary>

Main category: cs.GR

Motivation: 3D体积数据的复杂性使其难以可视化，而皮层表面渲染更易于理解和交互分析，因此开发X-SiT以提供可解释的医疗图像预测。

Method: X-SiT结合原型表面块解码器和案例推理，利用空间对应的皮层原型对表面块嵌入进行分类。

Result: X-SiT在阿尔茨海默病和额颞叶痴呆检测中达到最优性能，并提供了与疾病模式一致的原型，揭示了分类错误。

Conclusion: X-SiT是首个基于可解释皮层特征的神经网络，为临床决策提供了直观且高效的预测工具。

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [141] [OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning](https://arxiv.org/abs/2506.20297)
*Natalie Lang,Maya Simhi,Nir Shlezinger*

Key words: 联邦学习,量化,自适应,OLALa,通信开销

TL;DR: 论文提出了一种名为OLALa的联邦学习方法，通过在线调整量化器来优化通信开销并提升性能。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 解决联邦学习中因固定量化规则导致的性能次优问题，适应异构和动态环境。

Method: 采用OLALa框架，客户端通过轻量计算在线调整量化器，仅交换少量参数。

Result: 实验显示OLALa在多种量化率下优于传统固定码本和非自适应方法。

Conclusion: OLALa通过自适应量化有效提升联邦学习的性能和适应性。

Abstract: Federated learning (FL) enables collaborative training across distributed
clients without sharing raw data, often at the cost of substantial
communication overhead induced by transmitting high-dimensional model updates.
This overhead can be alleviated by having the clients quantize their model
updates, with dithered lattice quantizers identified as an attractive scheme
due to its structural simplicity and convergence-preserving properties.
However, existing lattice-based FL schemes typically rely on a fixed
quantization rule, which is suboptimal in heterogeneous and dynamic
environments where the model updates distribution varies across users and
training rounds. In this work, we propose Online Learned Adaptive Lattices
(OLALa), a heterogeneous FL framework where each client can adjust its
quantizer online using lightweight local computations. We first derive
convergence guarantees for FL with non-fixed lattice quantizers and show that
proper lattice adaptation can tighten the convergence bound. Then, we design an
online learning algorithm that enables clients to tune their quantizers
throughout the FL process while exchanging only a compact set of quantization
parameters. Numerical experiments demonstrate that OLALa consistently improves
learning performance under various quantization rates, outperforming
conventional fixed-codebook and non-adaptive schemes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [142] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Key words: 大型语言模型（LLMs）、遗留代码、代码分块、文档生成、政府企业软件

TL;DR: 论文研究了大型语言模型（LLMs）在处理政府遗留代码（如ALC和MUMPS）时的应用，尤其是通过分块方法优化代码总结模块注释的质量，结果表明LLMs在分块任务中可以替代人类专家。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 政府企业软件常使用遗留语言编写，且代码长度超过现有LLMs的上下文窗口限制，而LLMs在遗留语言上的表现尚未充分研究，亟需实证分析其在代码现代化中的作用。

Method: 研究了多种代码分块方法，评估其对不同LLMs（如GPT-4o、Claude 3 Sonnet等）生成文档质量的影响，并与人类专家分区进行对比。

Result: LLMs选择的分区点与人类专家分区高度接近，且其生成的注释在事实性和实用性上分别比人类分区的注释高出20%和10%。

Conclusion: LLMs可以作为人类分区的替代方案，用于大型代码库的现代化分块任务。

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [143] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Key words: AI,敏捷开发,研究路线图,行业合作,数据质量

TL;DR: 该研讨会探讨了将AI融入敏捷开发的挑战与机遇，并制定了研究路线图。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 解决AI与敏捷开发结合的实践问题，推动行业与学术合作。

Method: 通过互动会议系统化分析并优先处理挑战，找出根源。

Result: 制定了包括短期解决方案和长期目标的研究路线图。

Conclusion: 研讨会为未来行业与学术合作提供了结构化议程。

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [144] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Key words: BIM, 合规性检查, 大语言模型, 自动化, 建筑规范

TL;DR: 研究提出了一种基于大语言模型（LLM）的半自动化方法，用于改进BIM中的建筑规范合规性检查，显著减少时间和错误。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 手动检查BIM中的建筑规范合规性耗时且容易出错。

Method: 整合多种LLM（如GPT、Claude、Gemini和Llama）与Revit软件，解析规范、生成Python脚本并执行半自动化检查。

Result: 案例研究表明，系统减少了合规检查的时间和努力，提高了准确性，能够识别违规问题并生成报告。

Conclusion: 该方法为BIM合规性检查提供了全面、适应性强且经济高效的解决方案。

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [145] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Key words: 机器学习, 产品构思, Lean Inception, 数据依赖, 技术可行性

TL;DR: 论文提出了Define-ML框架，扩展了传统Lean Inception方法，以解决机器学习产品构思中的数据依赖和技术可行性问题，并通过案例验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 传统构思方法缺乏对机器学习特定挑战的支持，可能导致产品愿景不匹配和不切实际的期望。

Method: 基于Technology Transfer Model开发Define-ML，通过静态（玩具问题）和动态（工业案例）验证，结合定量与定性分析评估其效用。

Result: 参与者认为Define-ML能有效澄清数据问题、对齐业务目标并促进跨职能合作，尽管存在学习曲线，但所有参与者均有意采用。

Conclusion: Define-ML提供了一个开放的、已验证的机器学习产品构思方法，结合了敏捷性与技术可行性。

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [146] [Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies](https://arxiv.org/abs/2506.19973)
*Vojtěch Novák,Ivan Zelinka,Lenka Přibylová,Lubomír Martínek*

Key words: 量子神经网络，倾向得分，选择偏差，生存分析，CMA-ES

TL;DR: 研究探索了量子神经网络(QNN)在倾向得分估计中的应用，用于消除选择偏差，比较结肠癌患者腹腔镜与开放手术的生存结果。QNN在模拟硬件噪声下优于经典方法，尤其在样本量小时表现突出。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 解决在比较腹腔镜与开放手术生存结果时的选择偏差问题，特别是针对小样本、高维数据集的情况。

Method: 采用QNN架构，包括线性ZFeatureMap编码、SummedPaulis操作符预测和CMA-ES优化，并整合方差正则化以减少量子噪声。

Result: QNN在模拟噪声条件下表现优于经典方法（如逻辑回归和梯度提升机），倾向得分匹配和加权后消除了协变量不平衡，生存分析显示调整后无显著差异。

Conclusion: QNN结合CMA-E和噪声感知策略，有望改善生物医学研究中的因果推断能力。

Abstract: This study investigates the application of quantum neural networks (QNNs) for
propensity score estimation to address selection bias in comparing survival
outcomes between laparoscopic and open surgical techniques in a cohort of 1177
colorectal carcinoma patients treated at University Hospital Ostrava
(2001-2009). Using a dataset with 77 variables, including patient demographics
and tumor characteristics, we developed QNN-based propensity score models
focusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture
employed a linear ZFeatureMap for data encoding, a SummedPaulis operator for
predictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
for robust, gradient-free optimization in noisy quantum environments. Variance
regularization was integrated to mitigate quantum measurement noise, with
simulations conducted under exact, sampling (1024 shots), and noisy hardware
(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,
outperformed classical logistic regression and gradient boosted machines in
small samples (AUC up to 0.750 for n=100), with noise modeling enhancing
predictive stability. Propensity score matching and weighting, optimized via
genetic matching and matching weights, achieved covariate balance with
standardized mean differences of 0.0849 and 0.0869, respectively. Survival
analyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen
additive regression revealed no significant survival differences
post-adjustment (p-values 0.287-0.851), indicating confounding bias in
unadjusted outcomes. These results highlight QNNs' potential, enhanced by
CMA-ES and noise-aware strategies, to improve causal inference in biomedical
research, particularly for small-sample, high-dimensional datasets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [147] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Key words: 云计算, 碳足迹, MAIZX框架, 能耗优化, 气候变化

TL;DR: 该论文研究了MAIZX框架，通过动态排名资源（如数据中心、边缘计算节点和多云环境）来优化云操作并减少碳足迹，实现了85.68%的CO2减排。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 云计算的高能耗和碳排放对环境造成了巨大挑战，尤其是在私人云基础设施广泛使用的情况下，亟需高效透明的解决方案以实现2050年净零排放目标。

Method: 利用灵活的排名算法，基于实时和预测的碳强度、PUE和能耗动态优化云资源，并通过与虚拟机监控器直接交互实现工作负载优化。

Result: MAIZX框架在测试中表现出可扩展性和高效性，CO2排放减少了85.68%，显著提升了气候表现潜力。

Conclusion: MAIZX框架为云计算环境提供了减少碳足迹的有效工具，同时保持了操作效率，对未来可持续发展具有重要意义。

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [148] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Key words: 关键词：AI, 能源使用, 碳排放, 绿色 AI, WattsOnAI

TL;DR: 简介：WattsOnAI 是一个用于测量、分析和可视化 AI 工作负载的能源使用、功率消耗、硬件性能和碳排放的综合工具包。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 动机：现有工具测量和报告 AI 能源消耗和碳排放的局限性，需要更系统化的解决方案。

Method: 方法：WattsOnAI 通过集成现有 AI 框架，提供标准化报告和精细时间序列数据，支持关联分析。

Result: 结果：工具支持瓶颈识别和性能优化，推动更可持续的“绿色 AI”实践。

Conclusion: 结论：WattsOnAI 填补现有工具缺口，促进研究社区关注环境影响。

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [149] [CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems](https://arxiv.org/abs/2506.19993)
*Haochen Zhang,Tianyi Zhang,Junze Yin,Oren Gal,Anshumali Shrivastava,Vladimir Braverman*

Key words: 推荐系统, 大型语言模型, 词汇扩展, 序列理解, 工业应用

TL;DR: 提出了一种名为CoVE的新系统，利用压缩词汇扩展和序列理解能力优化LLMs在推荐任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 现有方法未能充分利用LLMs的序列信息处理能力，导致推荐任务表现不佳。

Method: 通过为每个项目分配独特ID的压缩词汇扩展（CoVE）框架，并压缩嵌入层以提高实用性。

Result: 在多个推荐数据集上验证了CoVE的有效性和性能。

Conclusion: CoVE显著提升了LLMs在推荐任务中的表现，并适用于大规模工业应用。

Abstract: Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

</details>


### [150] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Key words: 多模态检索,弱监督,编辑距离,FemmIR,MuQNOL

TL;DR: 论文提出FemmIR框架，用于无需相似性标签的多模态检索，基于编辑距离的弱监督方法，性能接近现有系统。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 避免监督分类任务的标注开销，复用预训练编码器，适用于标注稀缺的实际应用。

Method: 使用编辑距离的弱监督方法，通过数据样本的属性替换成本衡量相关性。

Result: 在MuQNOL数据集上测试，性能接近现有检索系统。

Conclusion: FemmIR在无需标注的情况下实现了多模态检索的实用性能。

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


### [151] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank](https://arxiv.org/abs/2506.20501)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Key words: 双塔模型, 学习排序, 点击数据, 日志策略, 样本加权

TL;DR: 本文研究了双塔模型在点击数据训练中性能下降的原因，分析了模型可识别性和日志策略的影响，并提出样本加权技术以减轻偏置。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 针对双塔模型在工业生产系统点击数据训练中性能下降的现象，研究其背后原因并提出解决方案。

Method: 理论分析双塔模型的可识别性条件，探讨日志策略对模型的影响，并提出样本加权技术。

Result: 研究表明，文档位置交换或特征分布重叠是恢复模型参数的必要条件；日志策略在模型完美捕捉用户行为时不引入偏置，但会放大模型不完美时的偏置。

Conclusion: 双塔模型的性能下降可能与可识别性问题和日志策略的偏置放大效应有关，样本加权技术可以有效减轻这些影响。

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [152] [An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking](https://arxiv.org/abs/2506.19960)
*Adam Foster,Zeno Schätzle,P. Bernát Szabó,Lixue Cheng,Jonas Köhler,Gino Cassella,Nicholas Gao,Jiawei Li,Frank Noé,Jan Hermann*

Key words: 量子蒙特卡洛,深度神经网络,波函数模型,键断裂,化学精度

TL;DR: Orbformer是一种可迁移的波函数模型，通过预训练在量子化学中实现了高效且准确的键断裂描述。

<details>
  <summary>Details</summary>

Main category: physics.chem-ph

Motivation: 解决多参考方法在描述键断裂时计算成本高且难以迁移的问题。

Method: 提出Orbformer，一种基于深度神经网络和量子蒙特卡洛的预训练波函数模型。

Result: 在多种复杂化学反应中达到化学精度（1 kcal/mol）。

Conclusion: Orbformer为量子化学中的成本分摊提供了实用方法。

Abstract: Reliable description of bond breaking remains a major challenge for quantum
chemistry due to the multireferential character of the electronic structure in
dissociating species. Multireferential methods in particular suffer from large
computational cost, which under the normal paradigm has to be paid anew for
each system at a full price, ignoring commonalities in electronic structure
across molecules. Quantum Monte Carlo with deep neural networks (deep QMC)
uniquely offers to exploit such commonalities by pretraining transferable
wavefunction models, but all such attempts were so far limited in scope. Here,
we bring this new paradigm to fruition with Orbformer, a novel transferable
wavefunction model pretrained on 22,000 equilibrium and dissociating structures
that can be fine-tuned on unseen molecules reaching an accuracy-cost ratio
rivalling classical multireferential methods. On established benchmarks as well
as more challenging bond dissociations and Diels-Alder reactions, Orbformer is
the only method that consistently converges to chemical accuracy (1 kcal/mol).
This work turns the idea of amortizing the cost of solving the Schr\"odinger
equation over many molecules into a practical approach in quantum chemistry.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [153] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.20039)
*Koorosh Moslemi,Chi-Guhn Lee*

Key words: 多智能体强化学习, 团队形成, 双向分组, 动态环境

TL;DR: 该论文提出了一个用于动态多智能体系统中双向团队学习的框架，填补了现有研究中双向分组在动态环境中未充分探索的空白。

<details>
  <summary>Details</summary>

Main category: cs.MA

Motivation: 现有研究主要关注单向分组或固定团队设置，而忽略了动态环境中双向分组的影响，本研究旨在填补这一空白。

Method: 引入了一个用于学习双向团队形成的框架，分析其算法特性对策略性能和泛化的影响。

Result: 在广泛采用的多智能体场景中验证了该方法的竞争性能和更高的泛化能力。

Conclusion: 该框架为动态多智能体系统中的团队学习提供了新的研究方向，并展示了良好的实际效果。

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [154] [Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch](https://arxiv.org/abs/2506.20513)
*Lei Liu,Chao Song,Liangsheng He,Silin Wang,Xuan Feng,Cai Liu*

Key words: 全波形反演（FWI）、探地雷达（GPR）、GPU加速、PyTorch、CUDA、地下成像

TL;DR: 该研究提出了一种高性能的双参数全波形反演框架（FWI），用于探地雷达（GPR），通过CUDA内核函数和PyTorch的混合编译实现加速。该方法结合了GPU编程的计算效率和Python深度学习框架的灵活性，实现了对介电常数和电导率的准确高效反演。

<details>
  <summary>Details</summary>

Main category: physics.geo-ph

Motivation: 旨在为探地雷达数据提供一种高效且灵活的双参数全波形反演方法，以满足快速地下成像的需求，适用于土木工程、环境监测和地球物理勘探等领域。

Method: 通过将定制化的CUDA内核集成到PyTorch的自动微分机制中，结合GPU计算的高效性和Python框架的灵活性，实现双参数FWI。支持可选的正则化策略，如总变差和多尺度反演。

Result: 在合成数据和实际波场数据上的实验证明，该方法能够高效且准确地完成双参数FWI，同时具有灵活性和可扩展性。

Conclusion: 提出的框架为探地雷达数据的快速地下成像提供了一种实用且可扩展的解决方案，适用于多种应用场景。

Abstract: This study proposes a high-performance dual-parameter full waveform inversion
framework (FWI) for ground-penetrating radar (GPR), accelerated through the
hybrid compilation of CUDA kernel functions and PyTorch. The method leverages
the computational efficiency of GPU programming while preserving the
flexibility and usability of Python-based deep learning frameworks. By
integrating customized CUDA kernels into PyTorch's automatic differentiation
mechanism, the framework enables accurate and efficient inversion of both
dielectric permittivity and electrical conductivity. Experimental evaluations
on synthetic data and real wavefield data demonstrate that the proposed method
achieves dual-parameter FWI for GPR data while maintaining high accuracy.
Moreover, the framework is flexible and extensible, supporting optional
regularization strategies such as total variation and multi-scale inversion.
These features make the proposed approach a practical and scalable framework
for rapid GPR-based subsurface imaging in applications including civil
engineering, environmental monitoring, and geophysical exploration.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [155] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
*Lee Qi Zun,Oscar Wong Jin Hao,Nor Anita Binti Che Omar,Zalifa Zakiah Binti Asnir,Mohamad Sabri bin Sinal Zainal,Goh Man Fye*

Key words: FundaQ-8, 眼底图像质量评估, ResNet18, 糖尿病视网膜病变, 深度学习

TL;DR: 论文提出了一种名为FundaQ-8的专家验证框架，用于系统评估眼底图像质量，并基于此开发了一个ResNet18回归模型预测质量分数。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 眼底图像质量评估（FIQA）因图像采集的多样性和专家主观评估的差异而具有挑战性，需要一个标准化的框架。

Method: 使用FundaQ-8作为结构化评分参考，开发了基于ResNet18的回归模型，通过迁移学习、均方误差优化和标准化预处理进行训练。

Result: 模型在真实临床数据和Kaggle数据集上表现良好，验证了框架的可靠性和临床可解释性。

Conclusion: 将FundaQ-8整合到深度学习模型中提升了糖尿病视网膜病变分级的诊断稳健性，凸显了质量感知训练在筛查应用中的价值。

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


### [156] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/abs/2506.19975)
*Hang Zhang,Yuxi Zhang,Jiazheng Wang,Xiang Chen,Renjiu Hu,Xin Tian,Gaolei Li,Min Liu*

Key words: 变形图像配准,离散优化,位移熵,特征提取,深度学习

TL;DR: VoxelOpt是一种结合学习和迭代方法的离散优化框架，用于变形图像配准，在精度和运行时之间取得了更好的平衡。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决基于学习的方法在训练数据有限、大变形和无标签监督时的性能不足问题，同时克服迭代方法速度慢的缺点。

Method: 采用位移熵测量体素信号强度，引入体素自适应消息传递、多级图像金字塔和预训练分割模型进行特征提取。

Result: 在腹部CT配准中，VoxelOpt在效率和精度上均优于领先的迭代方法，并与有监督学习的最新方法相当。

Conclusion: VoxelOpt成功结合了学习和迭代方法的优势，实现了高效的变形图像配准，代码将开源。

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [157] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/abs/2506.20614)
*Simon Perrin,Sébastien Levilly,Huajun Sun,Harold Mouchère,Jean-Michel Serfaty*

Key words: 4D Flow MRI, PC-MRA, 分割任务, WMF特征, 深度学习

TL;DR: 提出了一种新的手工艺特征WMF，用于4D Flow MRI图像的三维可视化，显著提升了分割任务中的IoU和Dice指标。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于4D Flow MRI图像的分辨率低和噪声问题，传统的血管分割方法（如PC-MRA）效果不佳。WMF特征旨在改进这一分割任务。

Method: 通过加权平均频率（WMF）特征，揭示三维空间中被搏动性血流通过的体素区域，并利用最优阈值和深度学习方法进行分割实验。

Result: 实验结果表明，WMF特征比PC-MRA特征在IoU和Dice指标上分别提升了0.12和0.13。

Conclusion: WMF特征在4D Flow MRI图像分割中具有潜力，未来可应用于其他血管区域（如心脏或大脑）的分割任务。

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [158] [Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers](https://arxiv.org/abs/2506.19875)
*Taous Iatariene,Can Cui,Alexandre Guérin,Romain Serizel*

Key words: 说话人跟踪, 说话人嵌入, 波束成形, 身份重新分配

TL;DR: 论文研究了利用说话人嵌入来解决间歇性和移动说话人跟踪中身份分配不连贯的问题。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 研究动机是解决在间歇性和移动说话人场景中，传统基于空间观测的跟踪方法因空间轨迹不连续而导致的身份分配不连贯问题。

Method: 方法是在初始跟踪后，利用多通道音频信号和波束成形技术增强说话人信号，提取说话人嵌入，并通过注册池重新分配跟踪身份。

Result: 实验表明，该方法在说话人位置变化的数据集上显著提高了神经和标准跟踪系统的身份分配性能，并研究了波束成形和输入时长对嵌入提取的影响。

Conclusion: 结论是说话人嵌入方法能有效改进间歇性和移动说话人场景中的身份分配连贯性。

Abstract: Speaker tracking methods often rely on spatial observations to assign
coherent track identities over time. This raises limits in scenarios with
intermittent and moving speakers, i.e., speakers that may change position when
they are inactive, thus leading to discontinuous spatial trajectories. This
paper proposes to investigate the use of speaker embeddings, in a simple
solution to this issue. We propose to perform identity reassignment
post-tracking, using speaker embeddings. We leverage trajectory-related
information provided by an initial tracking step and multichannel audio signal.
Beamforming is used to enhance the signal towards the speakers' positions in
order to compute speaker embeddings. These are then used to assign new track
identities based on an enrollment pool. We evaluate the performance of the
proposed speaker embedding-based identity reassignment method on a dataset
where speakers change position during inactivity periods. Results show that it
consistently improves the identity assignment performance of neural and
standard tracking systems. In particular, we study the impact of beamforming
and input duration for embedding extraction.

</details>


### [159] [MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition](https://arxiv.org/abs/2506.19887)
*Hyo Jin Jon,Longbin Jin,Hyuntaek Jung,Hyunseo Kim,Donghun Min,Eun Yi Kim*

Key words: 情感识别,自然语音,声学特征,文本特征,不确定性感知

TL;DR: 本文提出了一种名为MATER的多层次声学-文本情感表示框架，用于处理自然语音中的情感识别任务。通过融合声学和文本特征，并结合不确定性感知的集成策略，系统在SERNC挑战中表现优异。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 解决自然语音中情感识别的复杂性，包括主体内和主体间变异性。

Method: 提出MATER框架，整合声学和文本特征，并引入不确定性感知集成策略。

Result: 在SERNC挑战中排名第四，Macro-F1为41.01%，CCC为0.5928；在价预测中排名第二，CCC为0.6941。

Conclusion: MATER框架在自然语音情感识别中表现出色，特别是在复杂情境下。

Abstract: This paper presents our contributions to the Speech Emotion Recognition in
Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion
recognition and emotional attribute prediction. To handle the complexities of
natural speech, including intra- and inter-subject variability, we propose
Multi-level Acoustic-Textual Emotion Representation (MATER), a novel
hierarchical framework that integrates acoustic and textual features at the
word, utterance, and embedding levels. By fusing low-level lexical and acoustic
cues with high-level contextualized representations, MATER effectively captures
both fine-grained prosodic variations and semantic nuances. Additionally, we
introduce an uncertainty-aware ensemble strategy to mitigate annotator
inconsistencies, improving robustness in ambiguous emotional expressions. MATER
ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of
0.5928, securing second place in valence prediction with an impressive CCC of
0.6941.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [160] [PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning](https://arxiv.org/abs/2506.20043)
*Ahmet Sarigun,Bora Uyar,Vedran Franke,Altuna Akalin*

Key words: 分子对接, PocketVina, 口袋预测, 物理有效性, 虚拟筛选

TL;DR: PocketVina是一种基于口袋预测和系统多口袋探索的快速分子对接框架，在物理有效配体结合位点采样方面表现优越，尤其在处理结构多样和未见靶点时优于深度学习方法。

<details>
  <summary>Details</summary>

Main category: q-bio.QM

Motivation: 解决分子对接中物理有效配体结合位点采样的挑战，尤其是针对未见或结构多样的靶点。

Method: 结合口袋预测与系统多口袋探索的搜索对接框架。

Result: 在多个基准测试中表现优异，在物理有效性和RMSD上达到SOTA，并在区分活性与非活性靶点时优于深度学习基线。

Conclusion: PocketVina提供无需特定任务训练的高效对接策略，适合高通量虚拟筛选和药物发现。

Abstract: Sampling physically valid ligand-binding poses remains a major challenge in
molecular docking, particularly for unseen or structurally diverse targets. We
introduce PocketVina, a fast and memory-efficient, search-based docking
framework that combines pocket prediction with systematic multi-pocket
exploration. We evaluate PocketVina across four established
benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and
PoseBusters--and observe consistently strong performance in sampling physically
valid docking poses. PocketVina achieves state-of-the-art performance when
jointly considering ligand RMSD and physical validity (PB-valid), while
remaining competitive with deep learning-based approaches in terms of RMSD
alone, particularly on structurally diverse and previously unseen targets.
PocketVina also maintains state-of-the-art physically valid docking accuracy
across ligands with varying degrees of flexibility. We further introduce
TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000
protein-ligand pairs, and a partition of the dataset labeled with PubChem
activity annotations. On this large-scale dataset, PocketVina successfully
discriminates active from inactive targets, outperforming a deep learning
baseline while requiring significantly less GPU memory and runtime. PocketVina
offers a robust and scalable docking strategy that requires no task-specific
training and runs efficiently on standard GPUs, making it well-suited for
high-throughput virtual screening and structure-based drug discovery.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [161] [DeepQuark: deep-neural-network approach to multiquark bound states](https://arxiv.org/abs/2506.20555)
*Wei-Lin Wu,Lu Meng,Shi-Lin Zhu*

Key words: 变分蒙特卡罗、多夸克、深度神经网络、禁闭相互作用、非微扰QCD

TL;DR: 该论文首次将基于深度神经网络的变分蒙特卡罗方法应用于多夸克束缚态的研究，提出了一种高效的新架构DeepQuark，能够处理强关联、额外离散量子数等挑战，在多夸克系统中表现优异。

<details>
  <summary>Details</summary>

Main category: hep-ph

Motivation: 研究多夸克系统的复杂性超越了电子或核子系统，由于强SU(3)色相互作用的存在，传统方法难以精确描述，因此需要发展新的计算工具以解决这一难题。

Method: 论文设计了DeepQuark架构，针对多夸克系统的独特挑战（如强关联、额外量子数、难以处理的禁闭相互作用）进行了优化，并通过变分蒙特卡罗方法实现了高效计算。

Result: 在核子、双重味四夸克和全重四夸克系统中，DeepQuark表现与最先进方法（如扩散蒙特卡罗和高斯展开法）相当，在三重味五夸克系统中则优于现有计算结果。

Conclusion: DeepQuark为研究更大多夸克系统提供了新工具，突破了传统方法的计算限制，并为探索禁闭机制和非微扰QCD提供了有力框架。

Abstract: For the first time, we implement the deep-neural-network-based variational
Monte Carlo approach for the multiquark bound states, whose complexity
surpasses that of electron or nucleon systems due to strong SU(3) color
interactions. We design a novel and high-efficiency architecture, DeepQuark, to
address the unique challenges in multiquark systems such as stronger
correlations, extra discrete quantum numbers, and intractable confinement
interaction. Our method demonstrates competitive performance with
state-of-the-art approaches, including diffusion Monte Carlo and Gaussian
expansion method, in the nucleon, doubly heavy tetraquark, and fully heavy
tetraquark systems. Notably, it outperforms existing calculations for
pentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we
successfully incorporate three-body flux-tube confinement interactions without
additional computational costs. In tetraquark systems, we consistently describe
hadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased
form of wave function ansatz. In the pentaquark sector, we obtain weakly bound
$\bar D^*\Xi_{cc}^*$ molecule $P_{cc\bar c}(5715)$ with $S=\frac{5}{2}$ and its
bottom partner $P_{bb\bar b}(15569)$. They can be viewed as the analogs of the
molecular $T_{cc}$. We recommend experimental search of $P_{cc\bar c}(5715)$ in
the D-wave $J/\psi \Lambda_c$ channel. DeepQuark holds great promise for
extension to larger multiquark systems, overcoming the computational barriers
in conventional methods. It also serves as a powerful framework for exploring
confining mechanism beyond two-body interactions in multiquark states, which
may offer valuable insights into nonperturbative QCD and general many-body
physics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [162] [PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models](https://arxiv.org/abs/2506.20097)
*Wang Bill Zhu,Miaosen Chai,Ishika Singh,Robin Jia,Jesse Thomason*

Key words: 神经符号学习,符号动作语义,动态规划,多智能体,PDDL

TL;DR: PSALM-V是一个自主的神经符号学习系统，首次能够在视觉环境中通过交互诱导符号动作语义（如前条件和后条件），无需专家定义动作，使用LLM生成启发式计划和候选符号语义。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 传统方法依赖于文本领域或不现实的假设，如预先定义的问题文件或完全可观测性，PSALM-V旨在动态推断PDDL问题文件和动作语义。

Method: 系统通过分析执行结果和合成可能的错误解释，迭代生成和执行计划，并在对每个动作的可能语义的树结构信念上进行细化，直至达到目标状态。

Result: 在ALFRED任务中，PSALM-V将计划成功率从37%提升至74%；在2D游戏环境中提高了步骤效率并实现了多智能体场景的域诱导；在机器人任务中成功诱导了PDDL前条件和后条件。

Conclusion: PSALM-V展示了在复杂视觉和多智能体环境中自主学习和规划的能力，显著优于现有方法。

Abstract: We propose PSALM-V, the first autonomous neuro-symbolic learning system able
to induce symbolic action semantics (i.e., pre- and post-conditions) in visual
environments through interaction. PSALM-V bootstraps reliable symbolic planning
without expert action definitions, using LLMs to generate heuristic plans and
candidate symbolic semantics. Previous work has explored using large language
models to generate action semantics for Planning Domain Definition Language
(PDDL)-based symbolic planners. However, these approaches have primarily
focused on text-based domains or relied on unrealistic assumptions, such as
access to a predefined problem file, full observability, or explicit error
messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain
action semantics by analyzing execution outcomes and synthesizing possible
error explanations. The system iteratively generates and executes plans while
maintaining a tree-structured belief over possible action semantics for each
action, iteratively refining these beliefs until a goal state is reached.
Simulated experiments of task completion in ALFRED demonstrate that PSALM-V
increases the plan success rate from 37% (Claude-3.7) to 74% in partially
observed setups. Results on two 2D game environments, RTFM and Overcooked-AI,
show that PSALM-V improves step efficiency and succeeds in domain induction in
multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions
for real-world robot BlocksWorld tasks, despite low-level manipulation failures
from the robot.

</details>


### [163] [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
*Ruben Janssens,Jens De Bock,Sofie Labat,Eva Verhelst,Veronique Hoste,Tony Belpaeme*

Key words: 人机交互, 沟通错误检测, 机器学习, 计算机视觉, 多模态数据

TL;DR: 研究评估了机器学习模型在检测人机对话中沟通错误的效果，发现即使使用最先进的模型，识别效果仅略优于随机猜测。人类也仅能识别约一半的错误，揭示出用户感知但未向机器人传达的局限性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 人机交互中的沟通错误检测对于维护用户参与和信任至关重要，但机器人在解读非语言反馈方面面临挑战。

Method: 使用包含240次人机对话的多模态数据集，分析计算机视觉模型在检测四种对话错误时的性能。

Result: 模型在识别沟通错误时表现不佳，仅略优于随机猜测；在情感表达更丰富的数据集上能成功识别困惑状态。人类测试者也仅能识别约一半的错误。

Conclusion: 研究揭示了识别机器人对话中沟通错误的根本局限性，为改进人机对话设计提供了方向。

Abstract: Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

</details>


### [164] [Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion](https://arxiv.org/abs/2506.20036)
*Jeremiah Coholich,Muhammad Ali Murtaza,Seth Hutchinson,Zsolt Kira*

Key words: 分层强化学习、四足机器人、复杂地形、运动控制

TL;DR: 提出了一种新的分层强化学习框架，用于四足机器人在复杂地形上的运动，通过高低层策略协作提升性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决四足机器人在复杂地形中运动时面临的挑战，提升其运动效率和适应性。

Method: 采用双层策略框架，高层策略选择目标，低层策略通过强化学习实现目标，无需额外训练。

Result: 相比端到端强化学习方法，新框架在奖励更高、碰撞更少的情况下表现出色，适应训练未见的复杂地形。

Conclusion: 分层强化学习框架显著提升了四足机器人在复杂地形上的运动能力，具有更好的泛化性。

Abstract: We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.

</details>


### [165] [Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis](https://arxiv.org/abs/2506.20049)
*Lorin Achey,Alec Reed,Brendan Crowe,Bradley Hayes,Christoffer Heckman*

Key words: 机器人探索，生成式占用地图，扩散模型，实时融合

TL;DR: 提出了一种基于生成式占用地图的新方法SceneSense，用于实时概率融合预测，显著提升地图质量和机器人探索效率。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决机器人探索中由于部分观测导致的地图质量不足的问题。

Method: 使用扩散模型SceneSense预测3D占用地图，并与实时运行的地图概率融合。

Result: 实验表明，SceneSense显著提升地图质量（24.44%和75.59%的FID改进），并提高探索鲁棒性和效率。

Conclusion: SceneSense提供了一种有效的“即插即用”地图改进方法，适用于不同环境。

Abstract: We present a novel approach for enhancing robotic exploration by using
generative occupancy mapping. We introduce SceneSense, a diffusion model
designed and trained for predicting 3D occupancy maps given partial
observations. Our proposed approach probabilistically fuses these predictions
into a running occupancy map in real-time, resulting in significant
improvements in map quality and traversability. We implement SceneSense onboard
a quadruped robot and validate its performance with real-world experiments to
demonstrate the effectiveness of the model. In these experiments, we show that
occupancy maps enhanced with SceneSense predictions better represent our fully
observed ground truth data (24.44% FID improvement around the robot and 75.59%
improvement at range). We additionally show that integrating
SceneSense-enhanced maps into our robotic exploration stack as a "drop-in" map
improvement, utilizing an existing off-the-shelf planner, results in
improvements in robustness and traversability time. Finally we show results of
full exploration evaluations with our proposed system in two dissimilar
environments and find that locally enhanced maps provide more consistent
exploration results than maps constructed only from direct sensor measurements.

</details>


### [166] [Generating and Customizing Robotic Arm Trajectories using Neural Networks](https://arxiv.org/abs/2506.20259)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Igor Farkaš*

Key words: 神经网路,轨迹生成,机械臂,正向运动学,认知机器人

TL;DR: 提出了一种神经网路方法，用于生成和定制机械臂的轨迹，确保精确性和重复性，应用于认知机器人实验。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为了提高机器人动作的可预测性，尤其是在与人类交互时，需要开发一种能精确控制机械臂轨迹的方法。

Method: 通过神经网络计算机械臂的正向运动学，并与关节角度生成器结合，训练神经网络以生成适合的轨迹。

Result: 成功生成了精确且可定制的轨迹，机械臂能够执行线性移动并指向特定空间点。

Conclusion: 该方法广泛适用，能够生成精确且可定制的轨迹，适应不同场景需求。

Abstract: We introduce a neural network approach for generating and customizing the
trajectory of a robotic arm, that guarantees precision and repeatability. To
highlight the potential of this novel method, we describe the design and
implementation of the technique and show its application in an experimental
setting of cognitive robotics. In this scenario, the NICO robot was
characterized by the ability to point to specific points in space with precise
linear movements, increasing the predictability of the robotic action during
its interaction with humans. To achieve this goal, the neural network computes
the forward kinematics of the robot arm. By integrating it with a generator of
joint angles, another neural network was developed and trained on an artificial
dataset created from suitable start and end poses of the robotic arm. Through
the computation of angular velocities, the robot was characterized by its
ability to perform the movement, and the quality of its action was evaluated in
terms of shape and accuracy. Thanks to its broad applicability, our approach
successfully generates precise trajectories that could be customized in their
shape and adapted to different settings.

</details>


### [167] [CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition](https://arxiv.org/abs/2506.20373)
*Joerg Deigmoeller,Stephan Hasler,Nakul Agarwal,Daniel Tanneberg,Anna Belardinelli,Reza Ghoddoosian,Chao Wang,Felix Ocker,Fan Zhang,Behzad Dariush,Michael Gienger*

Key words: 人机交互, 情境感知, 实体标识, 协作机器人

TL;DR: CARMA是一种用于人机群体交互中的情境感知系统，通过唯一标识物理实例并组织为角色、对象和动作的三元组，验证实验中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 为了在人机群体交互中实现有效合作，需要情境意识和一致的实体表示。

Method: CARMA通过唯一标识物理实例并组织为角色、对象和动作的三元组来实现情境感知。

Result: 实验表明，系统能可靠生成准确的角色-动作-对象三元组。

Conclusion: CARMA为协作环境中时空推理和决策提供了结构化基础。

Abstract: We introduce CARMA, a system for situational grounding in human-robot group
interactions. Effective collaboration in such group settings requires
situational awareness based on a consistent representation of present persons
and objects coupled with an episodic abstraction of events regarding actors and
manipulated objects. This calls for a clear and consistent assignment of
instances, ensuring that robots correctly recognize and track actors, objects,
and their interactions over time. To achieve this, CARMA uniquely identifies
physical instances of such entities in the real world and organizes them into
grounded triplets of actors, objects, and actions.
  To validate our approach, we conducted three experiments, where multiple
humans and a robot interact: collaborative pouring, handovers, and sorting.
These scenarios allow the assessment of the system's capabilities as to role
distinction, multi-actor awareness, and consistent instance identification. Our
experiments demonstrate that the system can reliably generate accurate
actor-action-object triplets, providing a structured and robust foundation for
applications requiring spatiotemporal reasoning and situated decision-making in
collaborative settings.

</details>


### [168] [DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy](https://arxiv.org/abs/2506.20668)
*Sungjae Park,Homanga Bharadhwaj,Shubham Tulsiani*

Key words: 机器人学习，运动重定向，扩散策略，任务适应，人类演示

TL;DR: DemoDiffusion通过模仿单一人演示，结合运动重定向和扩散策略修改轨迹，实现机器人对自然环境中任务的适应性。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 解决机器人如何通过单一人类演示学习并适应新任务的问题，避免复杂的在线强化学习或配对数据需求。

Method: 结合运动重定向生成粗糙开环轨迹，并利用预训练扩散策略调整轨迹以适应机器人动作分布。

Result: 实验表明，DemoDiffusion在仿真和真实场景中均优于基础策略和重定向轨迹，能成功完成预训练策略完全失败的任务。

Conclusion: DemoDiffusion为机器人提供了一种简单、可扩展的任务适应方法，无需额外数据或复杂学习。

Abstract: We propose DemoDiffusion, a simple and scalable method for enabling robots to
perform manipulation tasks in natural environments by imitating a single human
demonstration. Our approach is based on two key insights. First, the hand
motion in a human demonstration provides a useful prior for the robot's
end-effector trajectory, which we can convert into a rough open-loop robot
motion trajectory via kinematic retargeting. Second, while this retargeted
motion captures the overall structure of the task, it may not align well with
plausible robot actions in-context. To address this, we leverage a pre-trained
generalist diffusion policy to modify the trajectory, ensuring it both follows
the human motion and remains within the distribution of plausible robot
actions. Our approach avoids the need for online reinforcement learning or
paired human-robot data, enabling robust adaptation to new tasks and scenes
with minimal manual effort. Experiments in both simulation and real-world
settings show that DemoDiffusion outperforms both the base policy and the
retargeted trajectory, enabling the robot to succeed even on tasks where the
pre-trained generalist policy fails entirely. Project page:
https://demodiffusion.github.io/

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [169] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Key words: 文档布局分析,目标检测,Transformer,YOLO,历史文档

TL;DR: 论文比较了五种目标检测架构在三个历史文档数据集上的性能，发现CNN-OBB模型在处理复杂布局时表现最佳，而Transformer模型在结构化布局中更优。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 历史文档的复杂布局需要稳健的文档布局分析，论文旨在评估不同模型在此任务中的表现。

Method: 比较了两种Transformer模型和三种YOLO变体在三个数据集上的性能，分析了模型架构、数据集特性和边界框表示的影响。

Result: Co-DETR在结构化数据集上表现最佳，而YOLOv11x-OBB在复杂数据集上显著优于其他模型。

Conclusion: 使用定向边界框是准确建模历史文档非笛卡尔特性的关键，需在全局上下文意识和CNN-OBB模型的泛化能力间权衡。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [170] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Key words: 多模态模型、强化学习、动态搜索、检索增强生成、VQA任务

TL;DR: 提出了一种名为MMSearch-R1的端到端强化学习框架，旨在提升大型多模态模型（LMMs）在现实互联网环境中的动态搜索能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现实世界的信息复杂多变，现有方法（如RAG和基于提示的搜索代理）通常依赖固定流程，导致搜索效率低下或过度搜索。

Method: MMSearch-R1框架结合了图像和文本搜索工具，通过基于结果的奖励和搜索惩罚机制，指导模型决定何时及如何调用搜索工具。

Result: 实验表明，该模型不仅超越同尺寸的RAG基线模型，还能减少30%以上的搜索调用次数，同时匹配更大RAG模型的性能。

Conclusion: MMSearch-R1为多模态搜索研究提供了实用的框架和关键见解。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [171] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Key words: AI生成视频,视觉伪影,基准数据集,像素级标注

TL;DR: 论文介绍了BrokenVideos数据集，用于AI生成视频中视觉伪影的检测和定位，填补了现有研究的空白。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: AI生成视频常出现视觉伪影，影响真实感和用户信任，但目前缺乏专门用于伪影定位的基准数据集。

Method: 提出了包含3,254个AI生成视频的数据集BrokenVideos，提供像素级标注，并通过人工验证确保质量。

Result: 实验表明，在BrokenVideos上训练的模型能显著提升伪影定位能力。

Conclusion: BrokenVideos为生成视频模型的伪影定位研究提供了关键基准。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [172] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Key words: 自回归模型, 概念擦除, WGA, TLM, ECGVF

TL;DR: 提出了一种新的方法EAR，用于在自回归模型中实现高效且保持性能的概念擦除，并通过新基准ECGVF进行评估。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 自回归模型在视觉理解和图像生成中表现优异，但如何在不影响整体生成质量的情况下移除不需要的概念仍具挑战性。

Method: 提出了窗口梯度积累（WGA）和阈值损失掩码（TLM）策略，并通过新基准ECGVF进行评测。

Result: 实验表明EAR在概念擦除效果和模型性能保持上均有显著改进。

Conclusion: EAR提供了一种有效且实用的方法，解决了自回归模型中的概念擦除问题。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [173] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Key words: 结构化剪枝、损失感知、自动化选择、深度神经网络压缩、训练中剪枝

TL;DR: 本文提出了一种高效的损失感知自动选择结构化剪枝标准（LAASP）方法，用于深度神经网络的压缩和加速。该方法采用训练中剪枝的策略，避免了传统三阶段的繁琐过程，并通过自动选择剪枝标准和层优化网络性能。实验显示，该方法在VGGNet和ResNet模型上显著减少计算量（FLOPs）同时保持高准确率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有剪枝方法通常需要繁琐的训练、剪枝和微调三阶段，且剪枝标准和层选择依赖人工。本文旨在通过自动化剪枝标准和层选择，简化流程并提升剪枝效果。

Method: 提出LAASP方法，结合训练中剪枝策略，自动选择基于损失的最优剪枝标准（如幅度或相似性）和层，并通过短暂重训练缓解剪枝带来的精度下降。

Result: 在CIFAR-10数据集上，ResNet56和ResNet110模型的FLOPs减少52%，同时Top-1准确率优于现有方法；ImageNet数据集上，ResNet50的FLOPs减少42%以上，Top-5精度仅下降0.33%。

Conclusion: LAASP方法通过自动化剪枝标准选择和层优化，有效减少了计算量和人工干预，在多种模型和数据集上验证了其优越性。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [174] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Key words: 深度学习,全色锐化,Wald协议,PADM,高频细节

TL;DR: 本文探讨了Wald协议在深度学习全色锐化中的局限性，提出PADM模块自适应学习退化过程，并结合HFreqdiff模块增强高频细节，显著提升图像质量和空间清晰度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统Wald协议生成的合成数据在实际应用中泛化能力不足，限制了模型性能，因此需要更准确的退化过程学习和高频细节提取方法。

Method: 提出PADM模块，通过PAlignNet和PDegradeNet的相互迭代自适应学习退化过程；引入HFreqdiff模块，结合CFB和BACM实现高频细节提取和逆向过程学习。

Result: 实验表明，该方法在空间清晰度和图像质量上优于现有技术。

Conclusion: PADM和HFreqdiff模块有效解决了Wald协议的局限性，提升了全色锐化的性能。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [175] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Key words: 动作识别、多模态、不确定性建模、对象检测、显著性检测

TL;DR: 提出了一种深度翻译动作识别框架，通过联合预测动作概念和辅助特征来提升准确率，并结合多种模态特征实现最佳性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 视频中的动作识别需要高层次语义推理和多模态特征整合，但现有方法在处理缺失线索和噪声时仍有不足。

Method: 利用幻觉流推断缺失线索，引入对象检测特征(ODF)和显著性检测特征(SDF)，并结合多种辅助模态及不确定性建模。

Result: 在Kinetics-400、Kinetics-600和Something-Something V2等基准测试中达到最佳性能。

Conclusion: 该框架通过多模态整合和不确定性建模，有效捕捉细粒度动作动态，提升了动作识别性能。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [176] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Key words: 手写识别,多模态融合,Transformer,共享潜在空间

TL;DR: 提出了一种通过早期融合离线图像和在线笔画数据的端到端网络，利用互补线索提升手写识别准确性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有系统通常仅利用单一模态（图像或笔画轨迹），未能充分利用手写识别的互补线索。

Method: 采用共享潜在空间，通过补丁编码器和轻量级Transformer分别处理灰度图像和笔画序列，利用可学习查询联合注意力机制增强上下文。

Result: 在IAMOn-DB和VNOn-DB数据集上达到SOTA，准确率提升1%，并在ISI-Air数据集上验证了适应性。

Conclusion: 融合多模态线索提升了手写识别的表现，增强了书写者独立性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [177] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Key words: 深度伪造检测, 块效应, PLADA, 压缩图像, 双重注意力机制

TL;DR: PLADA是一种新颖的框架，通过处理压缩图像的“块效应”和利用配对与非配对数据，提升了社交媒体中深度伪造图像的检测效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 随着GANs和扩散模型的快速发展，AI生成的图像（深度伪造）几乎无法与真实图像区分，尤其是在社交媒体中广泛传播后，其滥用问题日益突出。现有的检测方法忽略了压缩引入的“块效应”，且主要针对原始图像，实际场景中效果有限。

Method: PLADA框架包含两个核心模块：块效应消除器（B2E）通过双重注意力机制处理块效应，开放数据聚合（ODA）利用配对和非配对数据提升检测效果。

Result: 在26个数据集上的实验表明，PLADA在深度伪造检测中表现优异，尤其在有限配对数据和压缩情况下优于现有方法。

Conclusion: PLADA不仅解决了压缩图像中的块效应问题，还为开放世界场景提供了鲁棒的解决方案，强调了块效应对深度伪造检测的重要性。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [178] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Key words: 深度学习, 纺织品相似性, Siamese模型, 线密度图, 艺术品保护

TL;DR: 本文提出了一种基于深度学习的纺织品相似性评估新方法，用于解决传统基于线密度图匹配方法在非连续织物上的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统织物分析方法依赖于线密度图匹配，无法应用于非连续织物。本文旨在通过深度学习技术填补这一空白。

Method: 采用Siamese深度学习模型，通过扫描图像的成对比较学习特征表示，并结合多个样本对的预测结果提供稳健相似性评分。

Result: 实验结果表明，该方法能在线密度相似的情况下有效比较平纹织物，验证了其可行性和准确性。

Conclusion: 该方法为织物分析开辟了新途径，尤其在艺术品鉴定和保护领域具有应用潜力。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [179] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Key words: 密集视频描述、分割与摘要、LSTM、视觉特征、ActivityNet Captions

TL;DR: 提出了一种用于密集视频描述的分割与摘要（DaS）框架，通过分割长视频为事件提案，利用视觉特征和语义描述生成最终摘要。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 旨在解决长视频的密集描述问题，通过结合视觉特征和语义信息生成更准确的视频描述。

Method: 采用分割与摘要框架，结合C3D视觉特征和LSTM网络，分两阶段进行语义和视觉信息的编码与解码。

Result: 在ActivityNet Captions数据集上验证了框架的有效性。

Conclusion: DaS框架在密集视频描述任务中表现优越。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [180] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Key words: 因果表示学习, 医学影像, 疾病分类, 泛化性, 鲁棒性

TL;DR: 通过分组观测学习可识别的因果表示，提高了胸部X射线疾病分类的泛化性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 在医学影像中，学习可识别的因果表示可以提升任务特定潜在特征的泛化性和鲁棒性。

Method: 提出一种端到端框架，通过分组观测来学习可识别的因果表示，确保了对种族、性别和成像视角的不变性。

Result: 实验表明，这些因果表示在多分类任务中提高了泛化性和鲁棒性。

Conclusion: 分组观测法有效提升了医学影像中疾病分类任务的性能。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [181] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Key words: 密集视频描述，图卷积网络，长短时记忆网络，场景演化

TL;DR: 提出了一个基于图的分割与总结（GPaS）框架，通过细化视频片段并利用GCN和LSTM的交互提升密集视频描述任务的效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法未充分探索事件时间提案中的场景演化，导致在场景和对象变化较大的提案中表现不佳。

Method: 采用两阶段框架：分割阶段将事件提案拆分为更细的视频片段；总结阶段通过GCN和LSTM的交互，将各片段的描述总结为一句话。

Result: 在ActivityNet Captions和YouCook II数据集上，方法优于现有技术。

Conclusion: GPaS框架显著提升了密集视频描述任务的性能。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [182] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Key words: 深度学习,零水印,失真不变特征,对抗学习,鲁棒性

TL;DR: 本文提出了一种新颖的深度学习框架，用于基于失真不变特征学习的鲁棒图像零水印技术。该方法保留原始图像不变，通过特征空间优化学习参考签名，并在多样化的图像数据集和失真条件下表现出优异的鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决图像水印技术中的失真问题，并保留原始图像的完整性，本文提出了一种零水印框架，通过深度学习学习失真不变特征，从而提高水印的鲁棒性和泛化能力。

Method: 框架包含两个模块：1）通过噪声对抗学习训练特征提取器，生成对失真不变且语义丰富的特征表示；2）设计基于学习的多位零水印方案，将不变特征投影到可训练的参考代码上以匹配目标二进制消息。

Result: 实验结果表明，该方法在特征稳定性和水印恢复方面达到了最先进的鲁棒性，并在多样化的图像数据集和失真条件下表现出优异的性能。

Conclusion: 本文提出的深度学习框架在零水印技术中表现出卓越的鲁棒性和泛化能力，优于现有的自监督和深度水印技术。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [183] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Key words: 高效跟踪, Transformer, 动态路由, 资源优化

TL;DR: 论文提出了一种高效跟踪模型HiT和动态跟踪器DyHiT，通过Bridge Module和双重图像位置编码提升性能，同时在资源受限设备上实现高速运行。动态路由架构还支持训练免费加速方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Transformer模型在视觉跟踪中虽先进，但资源消耗大，限制了其在资源受限设备上的实用性。

Method: HiT使用Bridge Module连接轻量级Transformer；DyHiT通过动态路由分类场景并分配计算资源。

Result: HiT在NVIDIA Jetson AGX上达61 fps，AUC 64.6%；DyHiT最快版本111 fps，AUC 62.4%；加速方法使SeqTrack-B256提速2.68倍，AUC不变。

Conclusion: HiT和DyHiT在性能和效率上表现优异，动态路由架构为高效跟踪提供了新思路。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [184] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Key words: 显微镜图像分析、深度学习、解耦表示学习、可解释性、图像分类

TL;DR: 本文提出了一种解耦表示学习（DRL）方法，以提高显微镜图像分类模型的可解释性，并在多个数据集上展示了其效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 显微镜图像分析在多个领域至关重要，但深度学习模型的可解释性仍是一个挑战，需要新的方法来平衡准确性与可解释性。

Method: 采用解耦表示学习方法，通过从合成数据中学习表示并迁移到真实数据，以提升模型的可解释性。

Result: 在浮游生物、酵母液泡和人类细胞等多个显微镜图像数据集上，该方法在准确性和可解释性之间取得了良好平衡。

Conclusion: 解耦表示学习方法能有效提升显微镜图像分类任务的可解释性，为实际问题提供了可行的解决方案。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [185] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Key words: 扩散模型,超高分辨率图像合成,零样本生成,小波变换,细节增强

TL;DR: HiWave是一种无需训练的零样本方法，通过两阶段流程和基于小波的细节增强模块，显著提升预训练扩散模型在超高分辨率图像合成中的视觉保真度和结构一致性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决高分辨率图像合成中计算成本高和现有零样本技术常见的问题（如物体重复和空间不连贯）。

Method: 采用两阶段流程：生成基础图像后，进行补丁式DDIM反演和基于小波的细节增强。低频保留全局结构，高频增强细节。

Result: 在Stable Diffusion XL上评估显示，HiWave显著减少视觉伪影，感知质量优于现有方法。用户研究中80%以上偏好其效果。

Conclusion: HiWave无需重新训练或架构修改，即可实现高质量超高分辨率图像合成。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [186] [Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design](https://arxiv.org/abs/2506.20334)
*Daniele Ravasio,Marcello Farina,Alessio La Bella,Andrea Ballarino*

Key words: 递归神经网络,输出反馈,增量输入-状态稳定性,非线性模型预测控制,线性矩阵不等式

TL;DR: 本文研究了一类递归神经网络的输出反馈方案设计，提出了一种基于线性矩阵不等式的观察器和静态状态反馈控制器设计方法，并探讨了其鲁棒性和性能扩展。

<details>
  <summary>Details</summary>

Main category: eess.SY

Motivation: 研究目的是为递归神经网络设计输出反馈方案，以解决状态估计不确定性和扰动鲁棒性问题。

Method: 方法包括基于线性矩阵不等式的观察器和静态状态反馈控制器设计，以及引入基于管道的非线性模型预测控制器（NMPC）以扩展性能。

Result: 理论结果表明，所提方案能够保证收敛性和递归可行性，并通过数值仿真在pH中和过程基准上验证了有效性。

Conclusion: 该研究为递归神经网络提供了有效的输出反馈方案，并通过NMPC扩展了吸引区域。

Abstract: This paper investigates the design of output-feedback schemes for systems
described by a class of recurrent neural networks. We propose a procedure based
on linear matrix inequalities for designing an observer and a static
state-feedback controller. The algorithm leverages global and regional
incremental input-to-state stability (incremental ISS) and enables the tracking
of constant setpoints, ensuring robustness to disturbances and state estimation
uncertainty. To address the potential limitations of regional incremental ISS,
we introduce an alternative scheme in which the static law is replaced with a
tube-based nonlinear model predictive controller (NMPC) that exploits regional
incremental ISS properties. We show that these conditions enable the
formulation of a robust NMPC law with guarantees of convergence and recursive
feasibility, leading to an enlarged region of attraction. Theoretical results
are validated through numerical simulations on the pH-neutralisation process
benchmark, demonstrating the effectiveness of the proposed schemes.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [187] [Do psychic cells generate consciousness?](https://arxiv.org/abs/2506.20164)
*Mototaka Suzuki,Jaan Aru*

Key words: 意识, 皮质锥体神经元, 代谢型受体, 麻醉, 神经科学

TL;DR: 摘要回顾了近年来在理解大脑意识处理的细胞机制方面取得的进展，重点探讨了皮质锥体神经元及其特定的代谢型受体在麻醉诱导意识丧失中的作用。

<details>
  <summary>Details</summary>

Main category: q-bio.NC

Motivation: 探讨技术进步如何使神经科学家能够以前所未有的方式研究意识的基本问题。

Method: 通过综述近期研究，分析皮质锥体神经元（“心理细胞”）及其代谢型受体在意识处理中的作用。

Result: 研究发现皮质锥体神经元的特定代谢型受体可能是麻醉诱导意识丧失的关键机制。

Conclusion: Ramon y Cajal一个世纪前的直觉可能是正确的，心理细胞可能与意识的生成和控制有关。

Abstract: Technological advances in the past decades have begun to enable
neuroscientists to address fundamental questions about consciousness in an
unprecedented way. Here we review remarkable recent progress in our
understanding of cellular-level mechanisms of conscious processing in the
brain. Of particular interest are the cortical pyramidal neurons -- or "psychic
cells" called by Ram\'on y Cajal more than 100 years ago -- which have an
intriguing cellular mechanism that accounts for selective disruption of
feedback signaling in the brain upon anesthetic-induced loss of consciousness.
Importantly, a particular class of metabotropic receptors distributed over the
dendrites of pyramidal cells are highlighted as the key cellular mechanism.
After all, Cajal's instinct over a century ago may turn out to be correct -- we
may have just begun to understand whether and how psychic cells indeed generate
and control our consciousness.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [188] [Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings](https://arxiv.org/abs/2506.20609)
*Ankit Shah,Rita Singh,Bhiksha Raj,Alexander Hauptmann*

Key words: 枪声检测, 枪支分类, 声学分析, 机器学习, 深度学习

TL;DR: 该研究通过分析枪声的声学特征，提出了一种低成本枪击检测和枪支类型分类方法，基于机器学习和深度学习模型，并在真实数据上进行了验证。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 枪支相关暴力和大规模枪击事件的增加对公共安全构成威胁，而现有的枪击检测系统成本高昂。研究旨在利用普及设备（如手机）的录音功能，开发低成本解决方案。

Method: 研究分析了3459条枪声录音的声学特征，提出了支持向量机（SVM）和卷积神经网络（CNN）的框架，联合实现枪击检测和枪支类型分类。

Result: 深度学习模型在干净数据上的平均精度（mAP）为0.58，优于SVM基线（0.39），但在噪声数据上的表现下降至0.35。

Conclusion: 研究展示了利用普及设备进行低成本枪击检测和分类的潜力，但需解决数据质量和噪声问题。

Abstract: The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [189] [Exploration-Exploitation Tradeoff in Universal Lossy Compression](https://arxiv.org/abs/2506.20261)
*Nir Weinberger,Ram Zamir*

Key words: 通用压缩, 多臂老虎机, 损失压缩, 探索与利用, 健壮算法

TL;DR: 该论文将序列模式的通用压缩问题转化为多臂老虎机问题，研究在损失压缩情况下探索与利用的权衡，并提出健壮的算法。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 研究在序列模式的通用压缩中，如何通过多臂老虎机模型优化探索与利用的平衡。

Method: 将序列模式转化为多臂老虎机问题，提出并分析基于成本导向的算法。

Result: 提出了一种健壮的、适用于任何块长度的算法，克服了之前方法的局限性。

Conclusion: 通过多臂老虎机框架，能够更有效地实现序列模式下的损失压缩。

Abstract: Universal compression can learn the source and adapt to it either in a batch
mode (forward adaptation), or in a sequential mode (backward adaptation). We
recast the sequential mode as a multi-armed bandit problem, a fundamental model
in reinforcement-learning, and study the trade-off between exploration and
exploitation in the lossy compression case. We show that a previously proposed
"natural type selection" scheme can be cast as a reconstruction-directed MAB
algorithm, for sequential lossy compression, and explain its limitations in
terms of robustness and short-block performance. We then derive and analyze
robust cost-directed MAB algorithms, which work at any block length.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [190] [Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control](https://arxiv.org/abs/2506.20554)
*Andrew Mole,Max Weissenbacher,Georgios Rigas,Sylvain Laizet*

Key words: 风电场控制, 强化学习, 大涡模拟, 动态优化, 可再生能源

TL;DR: 利用强化学习（RL）控制器结合高保真大涡模拟（LES）实现动态闭环控制，显著提高风电场功率输出。

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

Motivation: 传统风电场控制独立优化单台风力机，忽略了协同尾流导向对整体发电量的潜在提升，且现有方法多依赖静态低精度模拟，无法捕捉关键湍流动态。

Method: 提出首个直接集成高保真大涡模拟（LES）的强化学习（RL）控制器，实现动态闭环控制，实时响应大气湍流。

Result: RL控制器使风电场功率输出提升4.30%，显著优于静态最优偏航控制（增益2.19%）。

Conclusion: 动态流响应控制是风电场优化的革命性方法，对加速可再生能源部署以实现净零目标具有直接意义。

Abstract: Traditional wind farm control operates each turbine independently to maximize
individual power output. However, coordinated wake steering across the entire
farm can substantially increase the combined wind farm energy production.
Although dynamic closed-loop control has proven effective in flow control
applications, wind farm optimization has relied primarily on static,
low-fidelity simulators that ignore critical turbulent flow dynamics. In this
work, we present the first reinforcement learning (RL) controller integrated
directly with high-fidelity large-eddy simulation (LES), enabling real-time
response to atmospheric turbulence through collaborative, dynamic control
strategies. Our RL controller achieves a 4.30% increase in wind farm power
output compared to baseline operation, nearly doubling the 2.19% gain from
static optimal yaw control obtained through Bayesian optimization. These
results establish dynamic flow-responsive control as a transformative approach
to wind farm optimization, with direct implications for accelerating renewable
energy deployment to net-zero targets.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [191] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Key words: 数据可视化,设计原理,自然语言处理,大型语言模型

TL;DR: 该论文提出了一个新的数据集和方法，通过自然语言探讨可视化设计背后的原理，利用真实的可视化笔记本和大型语言模型生成验证的问题-答案-原理三元组。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 现有自然语言数据集多关注可视化解读而非设计原理，本文旨在填补这一空白，利用学生的可视化笔记本研究设计决策背后的理由。

Method: 利用学生数据可视化课程的笔记本作为数据源，结合大型语言模型生成问题-答案-原理三元组，并验证和筛选数据集。

Result: 构建了一个记录学生可视化设计选择及其背后原理的数据集。

Conclusion: 该方法为研究可视化设计原理提供了新途径，数据集可用于进一步研究。

Abstract: Prior natural language datasets for data visualization have focused on tasks
such as visualization literacy assessment, insight generation, and
visualization generation from natural language instructions. These studies
often rely on controlled setups with purpose-built visualizations and
artificially constructed questions. As a result, they tend to prioritize the
interpretation of visualizations, focusing on decoding visualizations rather
than understanding their encoding. In this paper, we present a new dataset and
methodology for probing visualization design rationale through natural
language. We leverage a unique source of real-world visualizations and natural
language narratives: literate visualization notebooks created by students as
part of a data visualization course. These notebooks combine visual artifacts
with design exposition, in which students make explicit the rationale behind
their design decisions. We also use large language models (LLMs) to generate
and categorize question-answer-rationale triples from the narratives and
articulations in the notebooks. We then carefully validate the triples and
curate a dataset that captures and distills the visualization design choices
and corresponding rationales of the students.

</details>


### [192] [Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents](https://arxiv.org/abs/2506.20062)
*Runlong Ye,Zeling Zhang,Boushra Almazroua,Michael Liut*

Key words: AI代码助手、透明度、解释性、CopilotLens、人机协作

TL;DR: 介绍CopilotLens，一个提升代码助手透明度的交互框架，通过动态双层次界面展示AI决策过程。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统AI代码助手缺乏透明度，开发者难以理解其决策过程，影响可信度和协作效率。

Method: 提出CopilotLens框架，通过动态双层次界面展示AI的“思考过程”，包括高层计划和代码库上下文。

Result: CopilotLens增强代码助手的解释能力，促进开发者理解与AI协作。

Conclusion: CopilotLens为未来代码助手设计提供框架，强调透明决策而非速度。

Abstract: AI-powered code assistants are widely used to generate code completions,
significantly boosting developer productivity. However, these tools typically
present suggestions without explaining their rationale, leaving their
decision-making process inscrutable. This opacity hinders developers' ability
to critically evaluate the output, form accurate mental models, and build
calibrated trust in the system. To address this, we introduce CopilotLens, a
novel interactive framework that reframes code completion from a simple
suggestion into a transparent, explainable event. CopilotLens operates as an
explanation layer that reveals the AI agent's "thought process" through a
dynamic two-level interface, surfacing everything from its reconstructed
high-level plans to the specific codebase context influencing the code. This
paper presents the design and rationale of CopilotLens, offering a concrete
framework for building future agentic code assistants that prioritize clarity
of reasoning over speed of suggestion, thereby fostering deeper comprehension
and more robust human-AI collaboration.

</details>


### [193] [Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype](https://arxiv.org/abs/2506.20156)
*Xuefei Hou,Xizhao Tan*

Key words: 自主学习（SRL）、Spaced Repetition Systems（SRS）、Just-in-Time Adaptive Intervention（JITAI）、知识图谱、大语言模型（LLM）

TL;DR: 论文提出了一种名为'Insight Recall'的新范式，通过上下文触发的个人过往见解检索来支持自主学习（SRL），并实现了一个原型系统Irec。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 现有数字工具未能有效支持元认知反思，Spaced Repetition Systems（SRS）忽略了上下文作用，而Personal Knowledge Management（PKM）工具需要高人工维护。

Method: 采用Just-in-Time Adaptive Intervention（JITAI）框架，构建动态知识图谱和混合检索引擎，结合大语言模型（LLM）进行深度相似性评估，并设计了一个'Guided Inquiry'模块。

Result: 开发了一个可行的原型系统Irec，能够及时提供相关见解作为元认知支架，并通过人工参与的知识图谱构建减轻认知负担。

Conclusion: 论文贡献了一个坚实的理论框架和可用系统平台，用于设计增强元认知和自我调节的下一代智能学习系统。

Abstract: The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.

</details>


### [194] [AI in the Writing Process: How Purposeful AI Support Fosters Student Writing](https://arxiv.org/abs/2506.20595)
*Momin N. Siddiqui,Roy Pea,Hari Subramonyam*

Key words: AI写作支持,学习者自主性,知识转化,随机对照试验

TL;DR: 研究表明，集成式AI写作工具比聊天式LLM更能提升学生的写作自主性和知识转化深度。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 探讨不同AI支持方式对写作自主性和知识转化深度的影响。

Method: 采用随机对照试验，比较三种写作辅助方式的效果。

Result: 集成式AI工具组表现出更高的自主性和知识转化深度。

Conclusion: 针对性设计的AI写作支持工具可帮助学生保持写作自主性并更深度地参与内容。

Abstract: The ubiquity of technologies like ChatGPT has raised concerns about their
impact on student writing, particularly regarding reduced learner agency and
superficial engagement with content. While standalone chat-based LLMs often
produce suboptimal writing outcomes, evidence suggests that purposefully
designed AI writing support tools can enhance the writing process. This paper
investigates how different AI support approaches affect writers' sense of
agency and depth of knowledge transformation. Through a randomized control
trial with 90 undergraduate students, we compare three conditions: (1) a
chat-based LLM writing assistant, (2) an integrated AI writing tool to support
diverse subprocesses, and (3) a standard writing interface (control). Our
findings demonstrate that, among AI-supported conditions, students using the
integrated AI writing tool exhibited greater agency over their writing process
and engaged in deeper knowledge transformation overall. These results suggest
that thoughtfully designed AI writing support targeting specific aspects of the
writing process can help students maintain ownership of their work while
facilitating improved engagement with content.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [195] [Supervised Similarity for Firm Linkages](https://arxiv.org/abs/2506.19856)
*Ryan Samson,Adrian Banner,Luca Candelori,Sebastien Cottrell,Tiziana Di Matteo,Paul Duchnowski,Vahagn Kirakosyan,Jose Marques,Kharen Musaelian,Stefano Pasquali,Ryan Stever,Dario Villani*

Key words: 企业关联, CVLs, 欧几里得相似性, 量子认知机器学习, 动量溢出交易策略

TL;DR: 论文提出了一种新的企业关联代理指标CVLs，并通过欧几里得相似性和量子认知机器学习方法进行估计，发现QCML方法在动量溢出交易策略中表现更优。

<details>
  <summary>Details</summary>

Main category: q-fin.ST

Motivation: 旨在通过新的代理指标和方法更准确地估计企业关联，并探索其在交易策略中的实际应用。

Method: 采用欧几里得相似性和量子认知机器学习（QCML）两种方法估计企业关联。

Result: 两种方法均能构建盈利的动量溢出交易策略，但QCML方法优于欧几里得相似性。

Conclusion: QCML方法在企业关联估计和交易策略中更具优势。

Abstract: We introduce a novel proxy for firm linkages, Characteristic Vector Linkages
(CVLs). We use this concept to estimate firm linkages, first through Euclidean
similarity, and then by applying Quantum Cognition Machine Learning (QCML) to
similarity learning. We demonstrate that both methods can be used to construct
profitable momentum spillover trading strategies, but QCML similarity
outperforms the simpler Euclidean similarity.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [196] [MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](https://arxiv.org/abs/2506.19884)
*Zhengxiang Huang,Chaoyue Niu,Zhaode Wang,Jiarui Xue,Hanming Zhang,Yugang Wang,Zewei Xin,Xiaotang Jiang,Chengfei Lv,Fan Wu,Guihai Chen*

Key words: LLM, 能耗, 解码, MNN-AECS, 移动设备

TL;DR: 论文提出了MNN-AECS，一种无需root或操作系统修改的节能LLM解码系统，通过动态选择低功耗CPU核心，显著降低能耗且保持解码速度。

<details>
  <summary>Details</summary>

Main category: cs.OS

Motivation: 随着设备端LLM推理需求的增长，能耗成为关键问题，尤其是电池有限的移动设备。现有研究多关注预填充阶段加速，忽视了能耗问题。

Method: 提出自适应能源中心核心选择（AECS），将其集成到MNN中，形成MNN-AECS系统，动态选择低功耗CPU核心以减少能耗。

Result: MNN-AECS在7台设备上测试，平均节能23%，且无速度下降。与其他引擎相比，节能39%至78%，加速12%至363%。

Conclusion: MNN-AECS是首个无需系统修改的节能LLM解码解决方案，显著提升能效。

Abstract: As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [197] [Valid Selection among Conformal Sets](https://arxiv.org/abs/2506.20173)
*Mahmoud Hegazy,Liviu Aolaritei,Michael I. Jordan,Aymeric Dieuleveut*

Key words: 共形预测, 覆盖保证, 稳定性方法, 在线学习

TL;DR: 提出了一种基于稳定性的方法，确保所选的预测集保持覆盖保证，并将其扩展到在线共形预测设置。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 实践中存在多个有效的共形预测集，但选择最优集（如最小集）可能破坏覆盖保证。

Method: 采用稳定性方法确保所选预测集的覆盖性，并扩展至在线设置和结构化场景。

Result: 实验证明了该方法的有效性。

Conclusion: 稳定性方法能够在不破坏覆盖保证的情况下优化预测集选择。

Abstract: Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.

</details>


### [198] [Data-Driven Dynamic Factor Modeling via Manifold Learning](https://arxiv.org/abs/2506.19945)
*Graeme Baker,Agostino Capponi,J. Antonio Sidaoui*

Key words: 动态因子框架, 非线性流形学习, 卡尔曼滤波, 金融压力测试

TL;DR: 本文提出了一种数据驱动的动态因子框架，通过非线性流形学习技术揭示高维协变量与响应变量的联合动态，并利用线性扩散和卡尔曼滤波进行预测。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 旨在解决高维协变量与响应变量间动态关系的建模问题，避免参数化模型的限制。

Method: 采用各向异性扩散映射揭示动态，结合线性扩散和卡尔曼滤波进行预测。

Result: 在金融压力测试中，该方法显著优于传统方法，误差降低高达55%。

Conclusion: 数据驱动框架在动态建模和预测中表现优异，尤其在金融领域具有实际应用价值。

Abstract: We propose a data-driven dynamic factor framework where a response variable
depends on a high-dimensional set of covariates, without imposing any
parametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,
a nonlinear manifold learning technique introduced by Singer and Coifman, our
framework uncovers the joint dynamics of the covariates and responses in a
purely data-driven way. We approximate the embedding dynamics using linear
diffusions, and exploit Kalman filtering to predict the evolution of the
covariates and response variables directly from the diffusion map embedding
space. We generalize Singer's convergence rate analysis of the graph Laplacian
from the case of independent uniform samples on a compact manifold to the case
of time series arising from Langevin diffusions in Euclidean space.
Furthermore, we provide rigorous justification for our procedure by showing the
robustness of approximations of the diffusion map coordinates by linear
diffusions, and the convergence of ergodic averages under standard spectral
assumptions on the underlying dynamics. We apply our method to the stress
testing of equity portfolios using a combination of financial and macroeconomic
factors from the Federal Reserve's supervisory scenarios. We demonstrate that
our data-driven stress testing method outperforms standard scenario analysis
and Principal Component Analysis benchmarks through historical backtests
spanning three major financial crises, achieving reductions in mean absolute
error of up to 55% and 39% for scenario-based portfolio return prediction,
respectively.

</details>


### [199] [A Principled Path to Fitted Distributional Evaluation](https://arxiv.org/abs/2506.20048)
*Sungee Hong,Jiayi Wang,Zhengling Qi,Raymond Ka Wai Wong*

Key words: 强化学习, 分布式策略评估, 拟合分布评估, 理论框架, 收敛分析

TL;DR: 提出一种基于分布策略评估（FDE）的统一框架，扩展了传统的拟合Q评估方法，并提供理论分析和实验验证。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有分布策略评估方法缺乏统一理论框架，本文旨在填补这一空白。

Method: 提出拟合分布评估（FDE）框架，基于理论原则设计新方法并进行收敛分析。

Result: 新方法在理论和实验中表现优异，包括线性二次调节器和Atari游戏。

Conclusion: FDE方法为分布策略评估提供了理论基础和实践验证。

Abstract: In reinforcement learning, distributional off-policy evaluation (OPE) focuses
on estimating the return distribution of a target policy using offline data
collected under a different policy. This work focuses on extending the widely
used fitted-Q evaluation -- developed for expectation-based reinforcement
learning -- to the distributional OPE setting. We refer to this extension as
fitted distributional evaluation (FDE). While only a few related approaches
exist, there remains no unified framework for designing FDE methods. To fill
this gap, we present a set of guiding principles for constructing theoretically
grounded FDE methods. Building on these principles, we develop several new FDE
methods with convergence analysis and provide theoretical justification for
existing methods, even in non-tabular environments. Extensive experiments,
including simulations on linear quadratic regulators and Atari games,
demonstrate the superior performance of the FDE methods.

</details>


### [200] [Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives](https://arxiv.org/abs/2506.20114)
*Brian Liu,Rahul Mazumder,Peter Radchenko*

Key words: 树集成, 决策规则, 可解释性, 优化算法

TL;DR: 提出了一种从树集成中提取紧凑决策规则集的估计器，提高了模型的可解释性和准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 树集成模型虽然预测准确，但难以解释且可能隐藏有用关系。

Method: 开发了一种灵活的估计器，可控制规则数量和交互深度，并设计了精确和近似算法优化求解。

Result: 实验证明该估计器优于现有规则提取算法，并具有与理论预测一致的大样本性能。

Conclusion: 该方法在保持准确性的同时增强了模型的可解释性。

Abstract: Tree ensembles are non-parametric methods widely recognized for their
accuracy and ability to capture complex interactions. While these models excel
at prediction, they are difficult to interpret and may fail to uncover useful
relationships in the data. We propose an estimator to extract compact sets of
decision rules from tree ensembles. The extracted models are accurate and can
be manually examined to reveal relationships between the predictors and the
response. A key novelty of our estimator is the flexibility to jointly control
the number of rules extracted and the interaction depth of each rule, which
improves accuracy. We develop a tailored exact algorithm to efficiently solve
optimization problems underlying our estimator and an approximate algorithm for
computing regularization paths, sequences of solutions that correspond to
varying model sizes. We also establish novel non-asymptotic prediction error
bounds for our proposed approach, comparing it to an oracle that chooses the
best data-dependent linear combination of the rules in the ensemble subject to
the same complexity constraint as our estimator. The bounds illustrate that the
large-sample predictive performance of our estimator is on par with that of the
oracle. Through experiments, we demonstrate that our estimator outperforms
existing algorithms for rule extraction.

</details>


### [201] [POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.20406)
*Ruijia Zhang,Zhengling Qi,Yue Wu,Xiangyu Zhang,Yanxun Xu*

Key words: 动态治疗策略, 离线强化学习, 悲观模型, 不确定性量化, 策略学习

TL;DR: POLAR 是一种基于悲观模型的离线策略学习算法，用于优化动态治疗策略（DTRs），通过量化历史-动作对的不确定性并提供理论保证，实现了优于现有方法的效果。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 现有方法依赖于强假设或缺乏统计保证，POLAR旨在解决这些局限性，提供鲁棒且高效的离线DTR优化方案。

Method: POLAR利用离线数据估计状态转移动态，量化不确定性，并在奖励函数中引入悲观惩罚以降低高不确定性动作的权重。

Result: POLAR在合成数据和MIMIC-III数据集上的实验表明，其性能优于现有方法，并能生成接近最优的历史感知治疗策略。

Conclusion: POLAR是首个同时提供统计和计算保证的基于模型的DTR方法，为离线策略优化提供了高效且可靠的解决方案。

Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for
optimizing sequential decision-making in domains where decisions must adapt
over time in response to individual trajectories, such as healthcare,
education, and digital interventions. However, existing statistical methods
often rely on strong positivity assumptions and lack robustness under partial
data coverage, while offline reinforcement learning approaches typically focus
on average training performance, lack statistical guarantees, and require
solving complex optimization problems. To address these challenges, we propose
POLAR, a novel pessimistic model-based policy learning algorithm for offline
DTR optimization. POLAR estimates the transition dynamics from offline data and
quantifies uncertainty for each history-action pair. A pessimistic penalty is
then incorporated into the reward function to discourage actions with high
uncertainty. Unlike many existing methods that focus on average training
performance, POLAR directly targets the suboptimality of the final learned
policy and offers theoretical guarantees, without relying on computationally
intensive minimax or constrained optimization procedures. To the best of our
knowledge, POLAR is the first model-based DTR method to provide both
statistical and computational guarantees, including finite-sample bounds on
policy suboptimality. Empirical results on both synthetic data and the
MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods
and yields near-optimal, history-aware treatment strategies.

</details>


### [202] [Scalable Subset Selection in Linear Mixed Models](https://arxiv.org/abs/2506.20425)
*Ryan Thompson,Matt P. Wand,Joanna J. J. Wang*

Key words: 线性混合模型, $ℓ_0$正则化, 稀疏学习, 坐标下降算法, 局部搜索

TL;DR: 提出了一种新的$ℓ_0$正则化方法，用于线性混合模型（LMMs）的子集选择，能够高效处理数千个预测变量，填补了现有稀疏学习方法在处理大规模数据时的不足。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 随着数据规模的扩大，现有线性混合模型（LMMs）的稀疏学习方法在预测和解释性上无法处理数千个候选预测变量，亟需一种高效且可扩展的方法。

Method: 开发了一种坐标下降算法作为主要工具，并提供了收敛性保证；同时提出了一种局部搜索算法以应对非凸优化问题。这些方法还可扩展到广义LMMs的子集选择。

Result: 新方法在合成实验中表现优异，并在生物学和新闻学的两个数据集上验证了其实用性。

Conclusion: 该方法填补了LMMs稀疏学习在处理大规模数据时的空白，具有高效性和可扩展性。

Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine or
adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes
containing thousands of candidate predictors, necessitating sparsity for
prediction and interpretation. However, existing sparse learning methods for
LMMs do not scale well beyond tens or hundreds of predictors, leaving a large
gap compared with sparse methods for linear models, which ignore random
effects. This paper closes the gap with a new $\ell_0$ regularized method for
LMM subset selection that can run on datasets containing thousands of
predictors in seconds to minutes. On the computational front, we develop a
coordinate descent algorithm as our main workhorse and provide a guarantee of
its convergence. We also develop a local search algorithm to help traverse the
nonconvex optimization surface. Both algorithms readily extend to subset
selection in generalized LMMs via a penalized quasi-likelihood approximation.
On the statistical front, we provide a finite-sample bound on the
Kullback-Leibler divergence of the new method. We then demonstrate its
excellent performance in synthetic experiments and illustrate its utility on
two datasets from biology and journalism.

</details>


### [203] [Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery](https://arxiv.org/abs/2506.20533)
*Gilad Lerman,Kang Li,Tyler Maunu,Teng Zhang*

Key words: 鲁棒子空间估计, 迭代重加权最小二乘法, 动态平滑正则化, 线性收敛, 全局收敛

TL;DR: 论文提出了一种具有动态平滑正则化的迭代重加权最小二乘法（IRLS）变体，证明了其在确定性条件下能从任意初始值线性收敛到潜在子空间，并扩展了该理论到仿射子空间估计。此外，通过低维神经网络训练的案例展示了IRLS的实际优势。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 鲁棒子空间估计是机器学习和数据分析的基础任务，而IRLS尽管在实践中表现良好，其理论性质仍缺乏深入理解。本文旨在填补这一空白。

Method: 提出了一种动态平滑正则化的IRLS变体，并在确定性条件下分析其收敛性。

Result: 证明了该方法能从任意初始值线性收敛到子空间，并首次在鲁棒子空间恢复和黎曼流形上的非凸IRLS中提供了全局收敛保证。

Conclusion: 本文为IRLS提供了理论支持，并展示了其在仿射子空间估计等新领域的应用潜力。

Abstract: Robust subspace estimation is fundamental to many machine learning and data
analysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and
empirically effective approach to this problem, yet its theoretical properties
remain poorly understood. This paper establishes that, under deterministic
conditions, a variant of IRLS with dynamic smoothing regularization converges
linearly to the underlying subspace from any initialization. We extend these
guarantees to affine subspace estimation, a setting that lacks prior recovery
theory. Additionally, we illustrate the practical benefits of IRLS through an
application to low-dimensional neural network training. Our results provide the
first global convergence guarantees for IRLS in robust subspace recovery and,
more broadly, for nonconvex IRLS on a Riemannian manifold.

</details>


### [204] [LARP: Learner-Agnostic Robust Data Prefiltering](https://arxiv.org/abs/2506.20573)
*Kristian Minchev,Dimitar Iliev Dimitrov,Nikola Konstantinov*

Key words: 数据预过滤,学习者无关性,鲁棒性,Huber估计器,异构学习者

TL;DR: 该论文探讨了如何通过学习者无关的鲁棒数据预过滤（LARP）方法，从公共数据集中筛选出高质量数据以保护下游学习任务。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 公共数据集常包含低质量或污染数据，影响学习效果，需要开发学习者无关的预过滤技术。

Method: 提出LARP框架，基于Huber估计器和Huber数据污染模型，分析多种预过滤方法。

Result: 实验显示，针对异构学习者的LARP会导致性能损失，但能降低重复预过滤成本。

Conclusion: LARP在大数据集中能有效平衡性能损失与成本，适合实际应用。

Abstract: The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [205] [Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research](https://arxiv.org/abs/2506.19863)
*Ahmed Almeldein,Mohammed Alnaggar,Rick Archibald,Tom Beck,Arpan Biswas,Rike Bostelmann,Wes Brewer,Chris Bryan,Christopher Calle,Cihangir Celik,Rajni Chahal,Jong Youl Choi,Arindam Chowdhury,Mark Cianciosa,Franklin Curtis,Gregory Davidson,Sebastian De Pascuale,Lisa Fassino,Ana Gainaru,Yashika Ghai,Luke Gibson,Qian Gong,Christopher Greulich,Scott Greenwood,Cory Hauck,Ehab Hassan,Rinkle Juneja,Soyoung Kang,Scott Klasky,Atul Kumar,Vineet Kumar,Paul Laiu,Calvin Lear,Yan-Ru Lin,Jono McConnell,Furkan Oz,Anant Raj,Pradeep Ramuhalli,Marie Romedenne,Samantha Sabatino,José Salcedo-Pérez,Nathan D. See,Arpan Sircar,Punam Thankur,Tim Younkin,Xiao-Ying Yu,Prashant Jain,Tom Evans,Prasanna Balaprakash*

Key words: 大型语言模型,核能研究,提示工程,跨学科合作,AI辅助

TL;DR: 大型语言模型（LLMs）在核能研究中展现出潜力，加速了核聚变和裂变研究的早期探索与工作流设计，但仍需专家验证和领域特定数据支持。

<details>
  <summary>Details</summary>

Main category: physics.comp-ph

Motivation: 探讨LLMs在核能研究中的应用潜力，以加速核能技术的开发与优化。

Method: 通过14个跨学科团队在一天内使用ChatGPT、Gemini等模型，结合提示工程和迭代优化，探索核科学挑战。

Result: LLMs在早期探索、文献综述和工作流设计上表现优秀，但在新材料设计和高级代码生成方面存在局限。

Conclusion: AI可作为核能研究的补充工具，但仍需领域特定数据和自动化工作流支持以确保科学严谨性。

Abstract: The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated
the potential of Large Language Models (LLMs) to accelerate fusion and fission
research. Fourteen interdisciplinary teams explored diverse nuclear science
challenges using ChatGPT, Gemini, Claude, and other AI models over a single
day. Applications ranged from developing foundation models for fusion reactor
control to automating Monte Carlo simulations, predicting material degradation,
and designing experimental programs for advanced reactors. Teams employed
structured workflows combining prompt engineering, deep research capabilities,
and iterative refinement to generate hypotheses, prototype code, and research
strategies. Key findings demonstrate that LLMs excel at early-stage
exploration, literature synthesis, and workflow design, successfully
identifying research gaps and generating plausible experimental frameworks.
However, significant limitations emerged, including difficulties with novel
materials designs, advanced code generation for modeling and simulation, and
domain-specific details requiring expert validation. The successful outcomes
resulted from expert-driven prompt engineering and treating AI as a
complementary tool rather than a replacement for physics-based methods. The
workshop validated AI's potential to accelerate nuclear energy research through
rapid iteration and cross-disciplinary synthesis while highlighting the need
for curated nuclear-specific datasets, workflow automation, and specialized
model development. These results provide a roadmap for integrating AI tools
into nuclear science workflows, potentially reducing development cycles for
safer, more efficient nuclear energy systems while maintaining rigorous
scientific standards.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [206] [Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization](https://arxiv.org/abs/2506.20056)
*Yuheng Chen,Alexander Montes McNeil,Taehyuk Park,Blake A. Wilson,Vaishnavi Iyer,Michael Bezick,Jae-Ik Choi,Rohan Ojha,Pravin Mahendran,Daksh Kumar Singh,Geetika Chitturi,Peigang Chen,Trang Do,Alexander V. Kildishev,Vladimir M. Shalaev,Michael Moebius,Wenshan Cai,Yongmin Liu,Alexandra Boltasseva*

Key words: 机器学习, 光电子器件, 优化设计

TL;DR: ML-PDD利用机器学习优化光电子器件开发，解决传统方法计算成本高、难以规模化等问题。

<details>
  <summary>Details</summary>

Main category: physics.optics

Motivation: 传统方法在光电子器件开发中存在计算复杂、成本高和规模化问题，机器学习提供新解决方案。

Method: 结合替代估计器、生成模型、强化学习和主动学习，优化设计和制造流程。

Result: ML-PDD显著提升设计效率、模拟速度和制造可靠性。

Conclusion: 机器学习为光电子器件开发带来高效、低成本的新途径。

Abstract: Photonic device development (PDD) has achieved remarkable success in
designing and implementing new devices for controlling light across various
wavelengths, scales, and applications, including telecommunications, imaging,
sensing, and quantum information processing. PDD is an iterative, five-step
process that consists of: i) deriving device behavior from design parameters,
ii) simulating device performance, iii) finding the optimal candidate designs
from simulations, iv) fabricating the optimal device, and v) measuring device
performance. Classically, all these steps involve Bayesian optimization,
material science, control theory, and direct physics-driven numerical methods.
However, many of these techniques are computationally intractable, monetarily
costly, or difficult to implement at scale. In addition, PDD suffers from large
optimization landscapes, uncertainties in structural or optical
characterization, and difficulties in implementing robust fabrication
processes. However, the advent of machine learning over the past decade has
provided novel, data-driven strategies for tackling these challenges, including
surrogate estimators for speeding up computations, generative modeling for
noisy measurement modeling and data augmentation, reinforcement learning for
fabrication, and active learning for experimental physical discovery. In this
review, we present a comprehensive perspective on these methods to enable
machine-learning-assisted PDD (ML-PDD) for efficient design optimization with
powerful generative models, fast simulation and characterization modeling under
noisy measurements, and reinforcement learning for fabrication. This review
will provide researchers from diverse backgrounds with valuable insights into
this emerging topic, fostering interdisciplinary efforts to accelerate the
development of complex photonic devices and systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [207] [Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability](https://arxiv.org/abs/2506.19870)
*Md Asif Ul Hoq Khan,MD Zahedul Islam,Istiaq Ahmed,Md Masud Karim Rabbi,Farhana Rahman Anonna,MD Abdul Fahim Zeeshan,Mehedi Hasan Ridoy,Bivash Ranjan Chowdhury,Md Nazmul Shakir Rabbi,GM Alamin Sadnan*

Key words: 区块链, 人工智能, 分散能源市场, 能源交易, 欺诈检测

TL;DR: 本文提出了一个结合区块链和人工智能的能源交易系统，旨在解决美国分散能源市场的安全和可靠性问题。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 分散能源市场的兴起带来了安全和认证的新挑战，尤其是在点对点能源交易中。

Method: 研究结合区块链和AI技术，利用包含120万条匿名交易记录的数据集，构建了一个双层系统架构。

Result: 系统成功提高了交易安全性和市场情报，并有效检测了欺诈行为。

Conclusion: 区块链与AI的结合为解决分散能源市场的问题提供了有效方案。

Abstract: Peer-to-peer trading and the move to decentralized grids have reshaped the
energy markets in the United States. Notwithstanding, such developments lead to
new challenges, mainly regarding the safety and authenticity of energy trade.
This study aimed to develop and build a secure, intelligent, and efficient
energy transaction system for the decentralized US energy market. This research
interlinks the technological prowess of blockchain and artificial intelligence
(AI) in a novel way to solve long-standing challenges in the distributed energy
market, specifically those of security, fraudulent behavior detection, and
market reliability. The dataset for this research is comprised of more than 1.2
million anonymized energy transaction records from a simulated peer-to-peer
(P2P) energy exchange network emulating real-life blockchain-based American
microgrids, including those tested by LO3 Energy and Grid+ Labs. Each record
contains detailed fields of transaction identifier, timestamp, energy volume
(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier
(hashed for privacy), smart meter readings, geolocation regions, and settlement
confirmation status. The dataset also includes system-calculated behavior
metrics of transaction rate, variability of energy production, and historical
pricing patterns. The system architecture proposed involves the integration of
two layers, namely a blockchain layer and artificial intelligence (AI) layer,
each playing a unique but complementary function in energy transaction securing
and market intelligence improvement. The machine learning models used in this
research were specifically chosen for their established high performance in
classification tasks, specifically in the identification of energy transaction
fraud in decentralized markets.

</details>


### [208] [An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network](https://arxiv.org/abs/2506.19871)
*Yining Pang,Chenghan Li*

Key words: 保险欺诈检测, GAN, 对抗攻击, 模型鲁棒性

TL;DR: 论文提出了一种基于GAN的方法对抗保险欺诈检测系统，展示了攻击者无需了解训练数据或模型细节即可生成被误判为合法的欺诈案例，攻击成功率高达99%，强调了提升模型鲁棒性的紧迫性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 保险欺诈检测对保险系统安全和效率至关重要，但目前系统缺乏标准化防御机制，易受新兴对抗性威胁影响。

Method: 采用基于GAN的方法对欺诈检测系统进行对抗攻击，通过微改真实保险记录和索赔生成欺诈案例。

Result: 攻击成功率高达99%，显示系统易受攻击，导致欺诈风险显著增加。

Conclusion: 研究突显了增强保险欺诈检测模型对抗性操纵的鲁棒性的紧迫性，以确保保险系统的稳定性和可靠性。

Abstract: Insurance fraud detection represents a pivotal advancement in modern
insurance service, providing intelligent and digitalized monitoring to enhance
management and prevent fraud. It is crucial for ensuring the security and
efficiency of insurance systems. Although AI and machine learning algorithms
have demonstrated strong performance in detecting fraudulent claims, the
absence of standardized defense mechanisms renders current systems vulnerable
to emerging adversarial threats. In this paper, we propose a GAN-based approach
to conduct adversarial attacks on fraud detection systems. Our results indicate
that an attacker, without knowledge of the training data or internal model
details, can generate fraudulent cases that are classified as legitimate with a
99\% attack success rate (ASR). By subtly modifying real insurance records and
claims, adversaries can significantly increase the fraud risk, potentially
bypassing compromised detection systems. These findings underscore the urgent
need to enhance the robustness of insurance fraud detection models against
adversarial manipulation, thereby ensuring the stability and reliability of
different insurance systems.

</details>


### [209] [Towards Provable (In)Secure Model Weight Release Schemes](https://arxiv.org/abs/2506.19874)
*Xing Yang,Bingtao Wang,Yuhao Wang,Zimo Ji,Terry Jingchen Zhang,Wenyuan Jiang*

Key words: 安全权重发布, 机器学习, 密码学, 形式化安全, 漏洞分析

TL;DR: 该论文提出了一种安全权重发布方案的形式化定义，并通过案例分析揭示了TaylorMLP的漏洞，主张未来研究需结合机器学习和安全性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有安全权重发布方案缺乏严格的安全基础，论文旨在填补这一空白。

Method: 引入具体的安全定义，并以TaylorMLP为例进行分析。

Result: 发现TaylorMLP存在参数提取漏洞，未能实现其安全目标。

Conclusion: 论文为未来权重发布方案的设计和评估提供了蓝图。

Abstract: Recent secure weight release schemes claim to enable open-source model
distribution while protecting model ownership and preventing misuse. However,
these approaches lack rigorous security foundations and provide only informal
security guarantees. Inspired by established works in cryptography, we
formalize the security of weight release schemes by introducing several
concrete security definitions. We then demonstrate our definition's utility
through a case study of TaylorMLP, a prominent secure weight release scheme.
Our analysis reveals vulnerabilities that allow parameter extraction thus
showing that TaylorMLP fails to achieve its informal security goals. We hope
this work will advocate for rigorous research at the intersection of machine
learning and security communities and provide a blueprint for how future weight
release schemes should be designed and evaluated.

</details>


### [210] [Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017](https://arxiv.org/abs/2506.19877)
*Zhaoyang Xu,Yunbo Liu*

Key words: 入侵检测,机器学习,CICIDS2017数据集,MLP,CNN,OCSVM,LOF

TL;DR: 该研究比较了四种机器学习模型在入侵检测中的表现，结果显示不同模型在不同场景下的优劣势，并提供了动态网络环境中模型选择的实用建议。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 识别合适的机器学习范式对于构建有效且通用的入侵检测系统至关重要。

Method: 研究比较了多层感知器（MLP）、一维卷积神经网络（CNN）、一类支持向量机（OCSVM）和局部离群因子（LOF）四种模型在CICIDS2017数据集上的表现，分为已知攻击和未知攻击两种场景。

Result: 监督学习的MLP和CNN在已知攻击上表现优异，但在未知攻击上召回率大幅下降；无监督的LOF在未知攻击上召回率高但误报率也高；OCSVM在精度和召回率上表现均衡，适用于多种场景。

Conclusion: 研究结果为动态网络环境中选择入侵检测模型提供了实用指导。

Abstract: Identifying suitable machine learning paradigms for intrusion detection
remains critical for building effective and generalizable security solutions.
In this study, we present a controlled comparison of four representative models
- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),
One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on
the CICIDS2017 dataset under two scenarios: detecting known attack types and
generalizing to previously unseen threats. Our results show that supervised MLP
and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic
recall drops on novel attacks. Unsupervised LOF attains moderate overall
accuracy and high recall on unknown threats at the cost of elevated false
alarms, while boundary-based OCSVM balances precision and recall best,
demonstrating robust detection across both scenarios. These findings offer
practical guidance for selecting IDS models in dynamic network environments.

</details>


### [211] [Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models](https://arxiv.org/abs/2506.19889)
*Wanli Peng,Xin Chen,Hang Fu,XinYu He,Xue Yiming,Juan Wen*

Key words: LLMs, 隐私侵犯攻击, 检索混淆生成, 防御方法

TL;DR: 本文提出了一种基于检索混淆生成（RCG）的新防御范式，用于高效且隐蔽地抵御隐私侵犯攻击（PVA）。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 大型语言模型（LLMs）的隐私侵犯攻击（PVA）引发严重隐私问题，现有防御方法成本高且效果不佳。

Method: 设计重写提示构建干扰数据库，采用最不相关检索策略生成防御查询。

Result: 在两种数据集和八个流行LLMs上的实验表明，该方法可行且优越。

Conclusion: RCG方法能有效且隐蔽地防御PVA。

Abstract: Recent advances in large language models (LLMs) have made a profound impact
on our society and also raised new security concerns. Particularly, due to the
remarkable inference ability of LLMs, the privacy violation attack (PVA),
revealed by Staab et al., introduces serious personal privacy issues. Existing
defense methods mainly leverage LLMs to anonymize the input query, which
requires costly inference time and cannot gain satisfactory defense
performance. Moreover, directly rejecting the PVA query seems like an effective
defense method, while the defense method is exposed, promoting the evolution of
PVA. In this paper, we propose a novel defense paradigm based on
retrieval-confused generation (RCG) of LLMs, which can efficiently and covertly
defend the PVA. We first design a paraphrasing prompt to induce the LLM to
rewrite the "user comments" of the attack query to construct a disturbed
database. Then, we propose the most irrelevant retrieval strategy to retrieve
the desired user data from the disturbed database. Finally, the "data comments"
are replaced with the retrieved user data to form a defended query, leading to
responding to the adversary with some wrong personal attributes, i.e., the
attack fails. Extensive experiments are conducted on two datasets and eight
popular LLMs to comprehensively evaluate the feasibility and the superiority of
the proposed defense method.

</details>


### [212] [RepuNet: A Reputation System for Mitigating Malicious Clients in DFL](https://arxiv.org/abs/2506.19892)
*Isaac Marroqui Penalva,Enrique Tomás Martínez Beltrán,Manuel Gil Pérez,Alberto Huertas Celdrán*

Key words: 去中心化联合学习, 声誉系统, 模型投毒, 延迟攻击, 消息泛洪

TL;DR: RepuNet是一种去中心化的声誉系统，用于动态评估节点行为并调整其在DFL中的影响力，有效检测和缓解恶意行为。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 去中心化联合学习（DFL）中节点自主选择聚合对象，可能导致恶意行为（如模型投毒、延迟攻击或消息泛洪），现有解决方案效率不高。

Method: 提出RepuNet系统，通过模型相似性、参数变化、消息延迟和通信量等指标动态评估节点声誉，并调整其聚合权重。

Result: 在MNIST和CIFAR-10数据集上测试，RepuNet的F1分数分别超过95%和约76%。

Conclusion: RepuNet适应性强、鲁棒性高，能有效缓解DFL中的威胁。

Abstract: Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.

</details>


### [213] [Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models](https://arxiv.org/abs/2506.19881)
*Aloni Cohen*

Key words: 版权保护, 生成模型, 可证明性, 差分隐私, 洁室设置

TL;DR: 该论文探讨生成模型输出是否可能侵犯训练数据的版权，提出了新的理论基础，指出NAF不足以防止侵权，并引入了无责复制保护框架。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究目的是解决生成模型输出的版权保护问题，提出更坚实的技术和法律基础，以确保模型输出不侵犯版权。

Method: 论文首先分析NAF的局限性，提出无责复制保护框架，并实例化为“洁室复制保护”，同时证明了差分隐私（DP）在特定条件下的有效性。

Result: 研究发现NAF无法防止逐字复制，新提出的无责复制保护框架能更好地控制侵权风险。

Conclusion: 论文得出结论，新的框架为版权保护提供了更可靠的基础，并验证了DP在版权保护中的作用。

Abstract: Are there any conditions under which a generative model's outputs are
guaranteed not to infringe the copyrights of its training data? This is the
question of "provable copyright protection" first posed by Vyas, Kakade, and
Barak (ICML 2023). They define near access-freeness (NAF) and propose it as
sufficient for protection. This paper revisits the question and establishes new
foundations for provable copyright protection -- foundations that are firmer
both technically and legally. First, we show that NAF alone does not prevent
infringement. In fact, NAF models can enable verbatim copying, a blatant
failure of copy protection that we dub being tainted. Then, we introduce our
blameless copy protection framework for defining meaningful guarantees, and
instantiate it with clean-room copy protection. Clean-room copy protection
allows a user to control their risk of copying by behaving in a way that is
unlikely to copy in a counterfactual clean-room setting. Finally, we formalize
a common intuition about differential privacy and copyright by proving that DP
implies clean-room copy protection when the dataset is golden, a copyright
deduplication requirement.

</details>


### [214] [Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack](https://arxiv.org/abs/2506.19886)
*Xuesong Wang,Mo Li,Xingyan Shi,Zhaoqian Liu,Shenghao Yang*

Key words: 语义通信, 扩散模型, 隐私保护, 任务导向通信, 6G网络

TL;DR: 本文提出了DiffSem框架，一种基于扩散机制的语义通信系统，通过自引用标签嵌入优化语义信息重建，提升任务性能，同时利用语义信息失真增强系统鲁棒性。实验结果表明其分类准确率提升10.03%，并揭示传统图像质量指标与任务相关语义信息泄露之间的不匹配。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 解决任务导向语义通信中隐私保护与任务准确性之间的矛盾，并克服传统图像质量指标评估攻击严重性的不足。

Method: 提出DiffSem框架，结合扩散机制与自引用标签嵌入优化语义重建；采用语义信息失真提升鲁棒性；设计新指标量化攻击者估计的语义保真度。

Result: 在MNIST数据集上分类准确率提升10.03%，动态信道下性能稳定；新指标有效揭示语义泄露与传统指标的不一致性。

Conclusion: DiffSem显著提升任务性能与隐私保护能力，为语义通信系统设计提供新方向。

Abstract: Semantic communication has emerged as a promising neural network-based system
design for 6G networks. Task-oriented semantic communication is a novel
paradigm whose core goal is to efficiently complete specific tasks by
transmitting semantic information, optimizing communication efficiency and task
performance. The key challenge lies in preserving privacy while maintaining
task accuracy, as this scenario is susceptible to model inversion attacks. In
such attacks, adversaries can restore or even reconstruct input data by
analyzing and processing model outputs, owing to the neural network-based
nature of the systems. In addition, traditional systems use image quality
indicators (such as PSNR or SSIM) to assess attack severity, which may be
inadequate for task-oriented semantic communication, since visual differences
do not necessarily ensure semantic divergence. In this paper, we propose a
diffusion-based semantic communication framework, named DiffSem, that optimizes
semantic information reconstruction through a diffusion mechanism with
self-referential label embedding to significantly improve task performance. Our
model also compensates channel noise and adopt semantic information distortion
to ensure the robustness of the system in various signal-to-noise ratio
environments. To evaluate the attacker's effectiveness, we propose a new metric
that better quantifies the semantic fidelity of estimations from the adversary.
Experimental results based on this criterion show that on the MNIST dataset,
DiffSem improves the classification accuracy by 10.03%, and maintain stable
performance under dynamic channels. Our results further demonstrate that
significant deviation exists between traditional image quality indicators and
the leakage of task-relevant semantic information.

</details>


### [215] [Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing](https://arxiv.org/abs/2506.20000)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Key words: 隐私保护, 联邦计算, FHE, MPC, DP

TL;DR: Guardian-FC是一个新颖的两层框架，用于隐私保护的联邦计算，通过统一的防护机制支持多种隐私保护技术。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 解决联邦计算中隐私保护机制的多样性和安全性问题，提供统一的安全执行框架。

Method: 采用后端中立、领域特定的语言（DSL）编写插件，通过代理-AI控制平面实现安全循环。

Result: 展示了后端无关的安全性和验证的形式化模型基础。

Conclusion: Guardian-FC为隐私保护联邦计算提供了一个灵活且可扩展的解决方案，并提出未来研究方向。

Abstract: We propose Guardian-FC, a novel two-layer framework for privacy preserving
federated computing that unifies safety enforcement across diverse privacy
preserving mechanisms, including cryptographic back-ends like fully homomorphic
encryption (FHE) and multiparty computation (MPC), as well as statistical
techniques such as differential privacy (DP). Guardian-FC decouples guard-rails
from privacy mechanisms by executing plug-ins (modular computation units),
written in a backend-neutral, domain-specific language (DSL) designed
specifically for federated computing workflows and interchangeable Execution
Providers (EPs), which implement DSL operations for various privacy back-ends.
An Agentic-AI control plane enforces a finite-state safety loop through signed
telemetry and commands, ensuring consistent risk management and auditability.
The manifest-centric design supports fail-fast job admission and seamless
extensibility to new privacy back-ends. We present qualitative scenarios
illustrating backend-agnostic safety and a formal model foundation for
verification. Finally, we outline a research agenda inviting the community to
advance adaptive guard-rail tuning, multi-backend composition, DSL
specification development, implementation, and compiler extensibility alongside
human-override usability.

</details>


### [216] [SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models](https://arxiv.org/abs/2506.20415)
*Dipayan Saha,Shams Tarek,Hasan Al Shaikh,Khan Thamid Hasan,Pavan Sai Nalluri,Md. Ajoad Hasan,Nashmin Alam,Jingbo Zhou,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Key words: SoC安全、多代理系统、大型语言模型、自动化验证、硬件安全

TL;DR: 论文提出了一种基于多代理系统的SV-LLM，利用大型语言模型（LLMs）自动化增强SoC安全验证，减少人工干预并提高效率。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 传统SoC安全验证方法面临自动化、可扩展性和适应性等挑战，而LLMs的自然语言理解和代码生成能力为解决这些问题提供了新思路。

Method: 设计了SV-LLM多代理系统，包含多个专用代理（如验证问答、威胁建模等），结合上下文学习、微调和RAG等技术优化任务性能。

Result: 通过案例研究和实验，展示了SV-LLM在自动化安全验证、风险识别和漏洞检测方面的潜力与高效性。

Conclusion: SV-LLM有望革新硬件安全实践，提供早期风险识别和缓解，提升验证效率。

Abstract: Ensuring the security of complex system-on-chips (SoCs) designs is a critical
imperative, yet traditional verification techniques struggle to keep pace due
to significant challenges in automation, scalability, comprehensiveness, and
adaptability. The advent of large language models (LLMs), with their remarkable
capabilities in natural language understanding, code generation, and advanced
reasoning, presents a new paradigm for tackling these issues. Moving beyond
monolithic models, an agentic approach allows for the creation of multi-agent
systems where specialized LLMs collaborate to solve complex problems more
effectively. Recognizing this opportunity, we introduce SV-LLM, a novel
multi-agent assistant system designed to automate and enhance SoC security
verification. By integrating specialized agents for tasks like verification
question answering, security asset identification, threat modeling, test plan
and property generation, vulnerability detection, and simulation-based bug
validation, SV-LLM streamlines the workflow. To optimize their performance in
these diverse tasks, agents leverage different learning paradigms, such as
in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The
system aims to reduce manual intervention, improve accuracy, and accelerate
security analysis, supporting proactive identification and mitigation of risks
early in the design cycle. We demonstrate its potential to transform hardware
security practices through illustrative case studies and experiments that
showcase its applicability and efficacy.

</details>


### [217] [Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks](https://arxiv.org/abs/2506.20082)
*Yali Yuan,Weiyi Zou,Guang Cheng*

Key words: 网站指纹攻击, 注意力机制, 大规模分类, 多标签浏览, 网页指纹识别

TL;DR: 论文提出了一种名为ADWPF的注意力驱动的细粒度网页指纹攻击方法，用于解决大规模环境中网页指纹识别和分类的挑战。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 传统网站指纹攻击（WF）在实战中只能识别主页，但用户常访问多个子页面。论文扩展了WF框架至大规模场景，将子页面建模为独立类别，并考虑多标签浏览场景带来的分类困难。

Method: 提出ADWPF方法，通过注意力机制对流量进行目标增强，提取低维特征并使用自注意力模块捕捉全局上下文模式。采用残差注意力处理多标签场景中不同时间位置的网页。

Result: 实验表明，ADWPF在不同规模的数据集上均优于现有方法。

Conclusion: ADWPF在大规模网页指纹识别中表现出色，解决了传统方法的局限性。

Abstract: Website Fingerprinting (WF) attacks aim to infer which websites a user is
visiting by analyzing traffic patterns, thereby compromising user anonymity.
Although this technique has been demonstrated to be effective in controlled
experimental environments, it remains largely limited to small-scale scenarios,
typically restricted to recognizing website homepages. In practical settings,
however, users frequently access multiple subpages in rapid succession, often
before previous content fully loads. WebPage Fingerprinting (WPF) generalizes
the WF framework to large-scale environments by modeling subpages of the same
site as distinct classes. These pages often share similar page elements,
resulting in lower inter-class variance in traffic features. Furthermore, we
consider multi-tab browsing scenarios, in which a single trace encompasses
multiple categories of webpages. This leads to overlapping traffic segments,
and similar features may appear in different positions within the traffic,
thereby increasing the difficulty of classification. To address these
challenges, we propose an attention-driven fine-grained WPF attack, named
ADWPF. Specifically, during the training phase, we apply targeted augmentation
to salient regions of the traffic based on attention maps, including attention
cropping and attention masking. ADWPF then extracts low-dimensional features
from both the original and augmented traffic and applies self-attention modules
to capture the global contextual patterns of the trace. Finally, to handle the
multi-tab scenario, we employ the residual attention to generate class-specific
representations of webpages occurring at different temporal positions.
Extensive experiments demonstrate that the proposed method consistently
surpasses state-of-the-art baselines across datasets of different scales.

</details>


### [218] [Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox](https://arxiv.org/abs/2506.20102)
*Malikussaid,Sutiyo*

Key words: ARC框架,关键基础设施,协同进化,深度强化学习,对抗训练

TL;DR: 论文提出ARC框架，通过自主闭环强化过程实现分析韧性，解决了IT与OT融合中关键基础设施面对智能攻击时静态防御失效的问题。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: IT与OT的融合导致关键基础设施暴露于智能攻击下，现有静态防御无法应对，需要动态、自我改进的安全范式。

Method: 引入ARC框架，包含一个自主发现攻击路径的DRL“红方代理”和一个通过对抗训练不断强化的“蓝方代理”防御系统。

Result: 在TEP和SWaT测试平台上验证了框架的优越性能，特别是协同进化过程显著提高了对新攻击的检测能力。

Conclusion: ARC不仅是一种改进，更是未来关键基础设施动态、自我改进安全范式的必要转变。

Abstract: The convergence of IT and OT has created hyper-connected ICS, exposing
critical infrastructure to a new class of adaptive, intelligent adversaries
that render static defenses obsolete. Existing security paradigms often fail to
address a foundational "Trinity of Trust," comprising the fidelity of the
system model, the integrity of synchronizing data, and the resilience of the
analytical engine against sophisticated evasion. This paper introduces the ARC
framework, a method for achieving analytical resilience through an autonomous,
closed-loop hardening process. ARC establishes a perpetual co-evolutionary arms
race within the high-fidelity sandbox of a F-SCDT. A DRL agent, the "Red
Agent," is formalized and incentivized to autonomously discover stealthy,
physically-plausible attack paths that maximize process disruption while
evading detection. Concurrently, an ensemble-based "Blue Agent" defender is
continuously hardened via adversarial training against the evolving threats
discovered by its adversary. This co-evolutionary dynamic forces both agents to
become progressively more sophisticated, enabling the system to autonomously
probe and patch its own vulnerabilities. Experimental validation on both the
TEP and the SWaT testbeds demonstrates the framework's superior performance. A
comprehensive ablation study, supported by extensive visualizations including
ROC curves and SHAP plots, reveals that the co-evolutionary process itself is
responsible for a significant performance increase in detecting novel attacks.
By integrating XAI to ensure operator trust and proposing a scalable F-ARC
architecture, this work presents ARC not merely as an improvement, but as a
necessary paradigm shift toward dynamic, self-improving security for the future
of critical infrastructure.

</details>


### [219] [Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS](https://arxiv.org/abs/2506.20576)
*Sabrine Ennaji,Elhadj Benkhelifa,Luigi V. Mancini*

Key words: 对抗攻击, 黑盒攻击, 网络流量, 变化点检测, 因果分析

TL;DR: 这篇论文提出了一种新的黑盒对抗攻击方法，解决了现有方法在实际应用中的局限性，特别是在结构化数据（如网络流量）中。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 现有对抗攻击方法在理论进展与实际应用之间存在显著差距，尤其是在处理结构化数据和复杂特征时，同时现有防御方法难以应对不断演变的对抗攻击。

Method: 提出了一种基于变化点检测和因果分析的自适应特征选择策略，严格遵循黑盒约束，减少交互以避免检测。

Result: 实验证明该方法能够以最低交互有效规避检测，具有低计算成本和高可部署性。

Conclusion: 这项工作通过提升对网络流量中对抗攻击的理解，为开发鲁棒防御奠定了基础。

Abstract: Adversarial attacks, wherein slight inputs are carefully crafted to mislead
intelligent models, have attracted increasing attention. However, a critical
gap persists between theoretical advancements and practical application,
particularly in structured data like network traffic, where interdependent
features complicate effective adversarial manipulations. Moreover, ambiguity in
current approaches restricts reproducibility and limits progress in this field.
Hence, existing defenses often fail to handle evolving adversarial attacks.
This paper proposes a novel approach for black-box adversarial attacks, that
addresses these limitations. Unlike prior work, which often assumes system
access or relies on repeated probing, our method strictly respect black-box
constraints, reducing interaction to avoid detection and better reflect
real-world scenarios. We present an adaptive feature selection strategy using
change-point detection and causality analysis to identify and target sensitive
features to perturbations. This lightweight design ensures low computational
cost and high deployability. Our comprehensive experiments show the attack's
effectiveness in evading detection with minimal interaction, enhancing its
adaptability and applicability in real-world scenarios. By advancing the
understanding of adversarial attacks in network traffic, this work lays a
foundation for developing robust defenses.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [220] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Key words: $ϵ$-PLA, 学习索引, 机器学习, 数据库系统

TL;DR: 论文研究了在数据库系统中使用误差有界分段线性近似（$ϵ$-PLA）的机器学习模型，改进了其拟合算法的分析，并提供了实际应用中的性能评估。

<details>
  <summary>Details</summary>

Main category: cs.DB

Motivation: 由于$ϵ$-PLA在诸多学习索引中的核心地位，其设计与分析尚不充分，论文旨在填补这一空白。

Method: 从理论和实证角度重新审视$ϵ$-PLA，提出改进的下界分析，并全面评估现有算法在不同学习数据结构中的表现。

Result: 确立了$ϵ$-PLA算法的改进下界，明确了模型精度、大小和查询性能之间的关键权衡。

Conclusion: 论文为未来学习数据结构的合理设计提供了实用指导。

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [221] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Key words: 信道跳频,自注意力机制,预测,节点移动性,通信开销

TL;DR: 论文提出了一种基于学习的信道占用预测方法MiLAAP，通过自注意力机制高效预测信道状态，减少通信开销，同时适应节点移动性。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 无线网络中信道跳频通信系统需适应干扰变化和节点移动性，传统状态共享方法会引入高通信开销，本文旨在解决这一问题。

Method: 提出MiLAAP框架，利用自注意力机制和多头自注意力机制分别预测信道占用状态和节点运动轨迹，仅依赖本地观测数据。

Result: MiLAAP在动态网络中实现了接近100%的信道状态预测准确率，并具有零样本泛化能力。

Conclusion: MiLAAP显著减少了通信开销，同时提高了信道调度的效率和准确性。

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>
