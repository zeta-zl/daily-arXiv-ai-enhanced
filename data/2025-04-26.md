<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 78]
- [cs.LG](#cs.LG) [Total: 140]
- [cs.AI](#cs.AI) [Total: 28]
- [physics.app-ph](#physics.app-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 4]
- [math.OC](#math.OC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 6]
- [physics.optics](#physics.optics) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CV](#cs.CV) [Total: 36]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.CE](#cs.CE) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 6]
- [eess.IV](#eess.IV) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SI](#cs.SI) [Total: 6]
- [stat.ML](#stat.ML) [Total: 8]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.LO](#cs.LO) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 4]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.DC](#cs.DC) [Total: 2]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
*Cong Qi,Hanzhang Fang,Tianxing Hu,Siqi Jiang,Wei Zhi*

Main category: cs.CL

TL;DR: GeneMamba is introduced as a scalable, efficient foundation model for single-cell transcriptomics, using Bi-Mamba architecture to address computational challenges of scRNA-seq with linear-time complexity.


<details>
  <summary>Details</summary>
Motivation: Existing transformer models in scRNA-seq face issues with quadratic complexity and poor long-range dependency handling. GeneMamba aims to provide a more efficient and biologically informed solution.

Method: GeneMamba leverages Bi-Mamba architecture for bidirectional gene context with linear-time complexity. It includes pretraining on 30M cells and biologically informed objectives like pathway-aware contrastive loss and rank-based gene encoding.

Result: GeneMamba shows strong performance in tasks like multi-batch integration, cell type annotation, and gene-gene correlation, outperforming transformer baselines.

Conclusion: GeneMamba is a practical, scalable alternative to transformer-based methods, enhancing large-scale single-cell data analysis with improved efficiency and biological relevance.

Abstract: Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of
cellular heterogeneity, but its complexity, which is marked by high
dimensionality, sparsity, and batch effects, which poses major computational
challenges. Transformer-based models have made significant advances in this
domain but are often limited by their quadratic complexity and suboptimal
handling of long-range dependencies. In this work, we introduce GeneMamba, a
scalable and efficient foundation model for single-cell transcriptomics built
on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba
captures bidirectional gene context with linear-time complexity, offering
substantial computational gains over transformer baselines. The model is
pretrained on nearly 30 million cells and incorporates biologically informed
objectives, including pathway-aware contrastive loss and rank-based gene
encoding. We evaluate GeneMamba across diverse tasks, including multi-batch
integration, cell type annotation, and gene-gene correlation, demonstrating
strong performance, interpretability, and robustness. These results position
GeneMamba as a practical and powerful alternative to transformer-based methods,
advancing the development of biologically grounded, scalable tools for
large-scale single-cell data analysis.

</details>


### [2] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
*Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Amit Agarwal*

Main category: cs.CL

TL;DR: 论文系统比较了BPE、SentencePiece和字符级分词方法在低资源印度语言NER任务中的表现，发现SentencePiece在跨语言零样本迁移中表现最佳，尤其在保留实体一致性方面优于BPE。


<details>
  <summary>Details</summary>
Motivation: 探索BPE等分词方法在低资源印度语言（如阿萨姆语、孟加拉语）和极低资源语言（如桑塔利语、曼尼普尔语）中NER任务的适用性，解决其形态复杂性带来的挑战。

Method: 使用IndicBERT模型，对比BPE、SentencePiece和字符级分词策略，评估其内部分词效率、OOV率和形态保留能力，以及外在下游任务（如微调和零样本跨语言迁移）性能。

Result: SentencePiece在低资源印度语言NER任务中表现最优，尤其在跨语言场景中能更好保留实体标签的连续性，而BPE因通用性不足在未见语言中表现较差。

Conclusion: SentencePiece因其对语言结构的更好保留能力，成为多语言低资源印度语言NER任务中更有效的分词策略。

Abstract: Tokenization is a critical component of Natural Language Processing (NLP),
especially for low resource languages, where subword segmentation influences
vocabulary structure and downstream task accuracy. Although Byte Pair Encoding
(BPE) is a standard tokenization method in multilingual language models, its
suitability for Named Entity Recognition (NER) in low resource Indic languages
remains underexplored due to its limitations in handling morphological
complexity. In this work, we systematically compare BPE, SentencePiece, and
Character Level tokenization strategies using IndicBERT for NER tasks in low
resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as
extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We
assess both intrinsic linguistic properties tokenization efficiency, out of
vocabulary (OOV) rates, and morphological preservation as well as extrinsic
downstream performance, including fine tuning and zero shot cross lingual
transfer.
  Our experiments show that SentencePiece is a consistently better performing
approach than BPE for NER in low resource Indic Languages, particularly in zero
shot cross lingual settings, as it better preserves entity consistency. While
BPE provides the most compact tokenization form, it is not capable of
generalization because it misclassifies or even fails to recognize entity
labels when tested on unseen languages. In contrast, SentencePiece constitutes
a better linguistic structural preservation model, benefiting extremely low
resource and morphologically rich Indic languages, such as Santali and
Manipuri, for superior entity recognition, as well as high generalization
across scripts, such as Sindhi, written in Arabic. The results point to
SentencePiece as the more effective tokenization strategy for NER within
multilingual and low resource Indic NLP applications.

</details>


### [3] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
*Luca Moroni,Giovanni Puccetti,Pere-Lluis Huguet Cabot,Andrei Stefan Bejgu,Edoardo Barba,Alessio Miaschi,Felice Dell'Orletta,Andrea Esuli,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文提出了一种名为SAVA的新方法，通过神经映射优化英文LLMs对意大利语的词汇适应，显著减少了token生成数量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大多数LLMs主要为英语设计，对其他语言的优化不足，导致编码效率低和推理速度慢。

Method: 采用SAVA方法，通过神经映射进行词汇替换，并优化词汇以减少参数数量。

Result: Mistral-7b-v0.1的token生成数量减少25%，Llama-3.1-8B参数减少10亿，性能通过有限的目标语言训练恢复。

Conclusion: SAVA能有效优化LLMs对非英语语言的适应，提升性能并减少资源消耗。

Abstract: The number of pretrained Large Language Models (LLMs) is increasing steadily,
though the majority are designed predominantly for the English language. While
state-of-the-art LLMs can handle other languages, due to language contamination
or some degree of multilingual pretraining data, they are not optimized for
non-English languages, leading to inefficient encoding (high token "fertility")
and slower inference speed. In this work, we thoroughly compare a variety of
vocabulary adaptation techniques for optimizing English LLMs for the Italian
language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a
novel method that leverages neural mapping for vocabulary substitution. SAVA
achieves competitive performance across multiple downstream tasks, enhancing
grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing
token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and
reducing the number of parameters by 1 billion. We show that, following the
adaptation of the vocabulary, these models can recover their performance with a
relatively limited stage of continual training on the target language. Finally,
we test the capabilities of the adapted models on various multi-choice and
generative tasks.

</details>


### [4] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 该研究发现大型语言模型（LLMs）在政治立场上的表现并非固定的左倾或右倾，而是具有话题特定的信念稳定性，强调了对真实世界应用进行话题特定可靠性评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 动机：探讨LLMs的政治立场是否反映真实的内部信念，还是仅仅是表面上的训练数据对齐。

Method: 方法：提出一个新框架，通过分析论证一致性和不确定性量化来评估信念深度，并在12个LLMs上对19项经济政策进行了测试。

Result: 结果：LLMs展现出话题特定的信念稳定性，左倾和右倾模型的响应一致性分别高达95%和89%，并能够有效区分表面对齐与真实信念。

Conclusion: 结论：挑战了LLMs具有稳定人类政治意识形态的假设，强调了话题特定评估在实际应用中的关键作用。

Abstract: Large Language Models (LLMs) are increasingly shaping political discourse,
yet their responses often display inconsistency when subjected to scrutiny.
While prior research has primarily categorized LLM outputs as left- or
right-leaning to assess their political stances, a critical question remains:
Do these responses reflect genuine internal beliefs or merely surface-level
alignment with training data? To address this, we propose a novel framework for
evaluating belief depth by analyzing (1) argumentative consistency and (2)
uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from
the Political Compass Test, challenging their belief stability with both
supportive and opposing arguments. Our analysis reveals that LLMs exhibit
topic-specific belief stability rather than a uniform ideological stance.
Notably, up to 95% of left-leaning models' responses and 89% of right-leaning
models' responses remain consistent under the challenge, enabling semantic
entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing
between surface-level alignment from genuine belief. These findings call into
question the assumption that LLMs maintain stable, human-like political
ideologies, emphasizing the importance of conducting topic-specific reliability
assessments for real-world applications.

</details>


### [5] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
*Arjun Subramonian,Vagrant Gautam,Preethi Seshadri,Dietrich Klakow,Kai-Wei Chang,Yizhou Sun*

Main category: cs.CL

TL;DR: 本文研究了不同LLM性别识别评估方法之间的一致性问题，发现概率评估和生成评估方法在实际应用中可能产生分歧，并且与人类评估存在差异，为未来的评估方法提供了改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨概率评估和生成评估方法在LLM性别识别任务中的一致性，验证这些方法是否具有收敛效度。

Method: 通过系统地元评估三种现有数据集，提出了一种方法将数据集转换为支持并行概率和生成评估的形式，并对6种模型进行了自动评估和2,400次人工评估。

Result: 研究发现不同评估方法间存在明显分歧（20.2%的实例冲突），且自动评估未能全面捕捉LLM的性别识别行为复杂性，与人类评估结果存在显著差异。

Conclusion: 研究强调了现有LLM评估方法的局限性，提供了改进建议，并质疑了更广泛的评估方法假设（即不同评估方法结果一致）。

Abstract: Numerous methods have been proposed to measure LLM misgendering, including
probability-based evaluations (e.g., automatically with templatic sentences)
and generation-based evaluations (e.g., with automatic heuristics or human
validation). However, it has gone unexamined whether these evaluation methods
have convergent validity, that is, whether their results align. Therefore, we
conduct a systematic meta-evaluation of these methods across three existing
datasets for LLM misgendering. We propose a method to transform each dataset to
enable parallel probability- and generation-based evaluation. Then, by
automatically evaluating a suite of 6 models from 3 families, we find that
these methods can disagree with each other at the instance, dataset, and model
levels, conflicting on 20.2% of evaluation instances. Finally, with a human
evaluation of 2400 LLM generations, we show that misgendering behaviour is
complex and goes far beyond pronouns, which automatic evaluations are not
currently designed to capture, suggesting essential disagreement with human
evaluations. Based on our findings, we provide recommendations for future
evaluations of LLM misgendering. Our results are also more widely relevant, as
they call into question broader methodological conventions in LLM evaluation,
which often assume that different evaluation methods agree.

</details>


### [6] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
*Rendi Chevi,Kentaro Inui,Thamar Solorio,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 大语言模型（LLM）的语言风格（如权威性、确定性或表达清晰度）显著影响用户偏好，但效果因用户群体和个体特质而异；但需警惕其可能加剧错误信息的风险。


<details>
  <summary>Details</summary>
Motivation: 探究除了信息准确性外，LLM的回应风格（如语言风格）如何影响用户偏好，并揭示其中潜在的利弊（如增强体验 vs. 增加错误信息风险）。

Method: 通过探索性和实验性用户研究，分析不同语言风格对偏好的影响，并考察用户个体特质的调节作用。

Result: 语言风格确实影响用户偏好，但具体影响方式因用户群体和个体特质而异；当前样本存在局限性（多样性不足、规模小）。

Conclusion: 需扩大样本以验证语言风格、个体特质与偏好的联合效应，并进一步探究其因果关系；初步结果需谨慎解读。

Abstract: What makes an interaction with the LLM more preferable for the user? While it
is intuitive to assume that information accuracy in the LLM's responses would
be one of the influential variables, recent studies have found that inaccurate
LLM's responses could still be preferable when they are perceived to be more
authoritative, certain, well-articulated, or simply verbose. These variables
interestingly fall under the broader category of language style, implying that
the style in the LLM's responses might meaningfully influence users'
preferences. This hypothesized dynamic could have double-edged consequences:
enhancing the overall user experience while simultaneously increasing their
susceptibility to risks such as LLM's misinformation or hallucinations. In this
short paper, we present our preliminary studies in exploring this subject.
Through a series of exploratory and experimental user studies, we found that
LLM's language style does indeed influence user's preferences, but how and
which language styles influence the preference varied across different user
populations, and more interestingly, moderated by the user's very own
individual traits. As a preliminary work, the findings in our studies should be
interpreted with caution, particularly given the limitations in our samples,
which still need wider demographic diversity and larger sample sizes. Our
future directions will first aim to address these limitations, which would
enable a more comprehensive joint effect analysis between the language style,
individual traits, and preferences, and further investigate the potential
causal relationship between and beyond these variables.

</details>


### [7] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
*Seunghyun Yoo*

Main category: cs.CL

TL;DR: 论文提出了一个交互式思维链（CoT）框架，通过模块化和用户可编辑的方式增强AI的透明度和责任感，以提升用户的批判性思维和认知参与。


<details>
  <summary>Details</summary>
Motivation: 由于短内容泛滥和AI快速普及，用户深度思考和批判性思维的能力下降，需要一种方法来提升AI的透明度和用户参与度。

Method: 提出了交互式CoT框架，分解推理过程为可检查、修改和重新执行的模块，并结合偏好学习的轻量级编辑-适应机制。

Result: 框架增强了AI的透明度和用户参与，支持多样化的认知风格和意图，同时通过元数据披露和隐私保护确保伦理透明。

Conclusion: 该框架为提升AI系统的批判性参与、责任感和包容性提供了设计原则和架构。

Abstract: Due to the proliferation of short-form content and the rapid adoption of AI,
opportunities for deep, reflective thinking have significantly diminished,
undermining users' critical thinking and reducing engagement with the reasoning
behind AI-generated outputs. To address this issue, we propose an Interactive
Chain-of-Thought (CoT) Framework that enhances human-centered explainability
and responsible AI usage by making the model's inference process transparent,
modular, and user-editable. The framework decomposes reasoning into clearly
defined blocks that users can inspect, modify, and re-execute, encouraging
active cognitive engagement rather than passive consumption. It further
integrates a lightweight edit-adaptation mechanism inspired by preference
learning, allowing the system to align with diverse cognitive styles and user
intentions. Ethical transparency is ensured through explicit metadata
disclosure, built-in bias checkpoint functionality, and privacy-preserving
safeguards. This work outlines the design principles and architecture necessary
to promote critical engagement, responsible interaction, and inclusive
adaptation in AI systems aimed at addressing complex societal challenges.

</details>


### [8] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
*Muskan Garg,Shaina Raza,Shebuti Rayana,Xingyi Liu,Sunghwan Sohn*

Main category: cs.CL

TL;DR: 这篇论文探讨了小型语言模型（SLMs）在资源有限的医疗环境中作为一种可扩展且临床可行的解决方案的潜力，提出了一个分类框架，并展示了SLMs在医疗领域的突破性进展和实验结果。


<details>
  <summary>Details</summary>
Motivation: 由于对数据隐私的日益关注和资源有限，大型语言模型（LLMs）在医疗应用中的使用受到限制。SLMs为资源受限的环境提供了一种可扩展且临床可行的解决方案。

Method: 论文提出了一个分类框架，从NLP任务、利益相关者角色和护理连续性三个维度分析模型。方法包括从头构建模型、通过提示和微调适应临床需求，以及通过压缩技术提高可访问性和可持续性。

Result: 论文展示了SLMs在医疗领域的突破性进展和广泛的实验结果，强调了其在医疗中的变革潜力，并提供了更新的资源库。

Conclusion: SLMs在资源受限的医疗环境中具有重要潜力，论文提出的分类框架和实验结果为医疗专业人士和研究人员提供了宝贵的资源，推动了该领域的未来研究和开发。

Abstract: Despite substantial progress in healthcare applications driven by large
language models (LLMs), growing concerns around data privacy, and limited
resources; the small language models (SLMs) offer a scalable and clinically
viable solution for efficient performance in resource-constrained environments
for next-generation healthcare informatics. Our comprehensive survey presents a
taxonomic framework to identify and categorize them for healthcare
professionals and informaticians. The timeline of healthcare SLM contributions
establishes a foundational framework for analyzing models across three
dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present
a taxonomic framework to identify the architectural foundations for building
models from scratch; adapting SLMs to clinical precision through prompting,
instruction fine-tuning, and reasoning; and accessibility and sustainability
through compression techniques. Our primary objective is to offer a
comprehensive survey for healthcare professionals, introducing recent
innovations in model optimization and equipping them with curated resources to
support future research and development in the field. Aiming to showcase the
groundbreaking advancements in SLMs for healthcare, we present a comprehensive
compilation of experimental results across widely studied NLP tasks in
healthcare to highlight the transformative potential of SLMs in healthcare. The
updated repository is available at Github

</details>


### [9] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
*Hannah Cyberey,David Evans*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）中的“审查”机制，提出了一种检测和控制模型输出中审查程度的方法，并揭示了“思想压制”这一额外的审查维度。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解大型语言模型如何通过拒绝有害请求来确保输出与模型控制者的偏好一致，即所谓的“审查”机制。

Method: 研究者使用表示工程技术和开放权重的安全调整模型，开发了一种检测和控制审查水平的方法，并通过分析DeepSeek-R1的推理模型发现了“思想压制”这一额外审查维度。

Result: 研究发现，通过一个拒绝-服从向量可以检测和控制模型的审查水平，并且可以通过应用该向量的负倍数来移除审查。

Conclusion: 该研究揭示了大型语言模型中审查和思想压制的机制，为理解和控制模型的输出提供了新的方法。

Abstract: Large language models (LLMs) have transformed the way we access information.
These models are often tuned to refuse to comply with requests that are
considered harmful and to produce responses that better align with the
preferences of those who control the models. To understand how this
"censorship" works. We use representation engineering techniques to study
open-weights safety-tuned models. We present a method for finding a
refusal--compliance vector that detects and controls the level of censorship in
model outputs. We also analyze recent reasoning LLMs, distilled from
DeepSeek-R1, and uncover an additional dimension of censorship through "thought
suppression". We show a similar approach can be used to find a vector that
suppresses the model's reasoning process, allowing us to remove censorship by
applying the negative multiples of this vector

</details>


### [10] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
*Chanhee Park,Hyeonseok Moon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 该论文介绍了MIRAGE数据集，专门用于评估检索增强生成（RAG）系统，包含7560个实例和37800个检索条目，并提出了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 由于RAG系统中检索与生成组件的复杂交互，现有评测基准不足，无法进行细致评估。

Method: 作者构建了MIRAGE数据集，包含大量实例和检索条目，并设计了新的指标，测试RAG系统的适应性。

Result: 通过多组实验，发现模型对齐的最优配置，并揭示了RAG系统的动态特性。

Conclusion: MIRAGE数据集和评估代码公开，为RAG研究提供了灵活评估工具。

Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective
method for enhancing the generative capabilities of Large Language Models
(LLMs) through the incorporation of external knowledge. However, the evaluation
of RAG systems remains a challenge, due to the intricate interplay between
retrieval and generation components. This limitation has resulted in a scarcity
of benchmarks that facilitate a detailed, component-specific assessment. In
this work, we present MIRAGE, a Question Answering dataset specifically
designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped
to a retrieval pool of 37,800 entries, enabling an efficient and precise
evaluation of both retrieval and generation tasks. We also introduce novel
evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions
such as noise vulnerability, context acceptability, context insensitivity, and
context misinterpretation. Through comprehensive experiments across various
retriever-LLM configurations, we provide new insights into the optimal
alignment of model pairs and the nuanced dynamics within RAG systems. The
dataset and evaluation code are publicly available, allowing for seamless
integration and customization in diverse research settings\footnote{The MIRAGE
code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


### [11] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
*Minju Seo,Jinheon Baek,Seongyun Lee,Sung Ju Hwang*

Main category: cs.CL

TL;DR: PaperCoder是一个多代理LLM框架，旨在将机器学习论文转化为功能性代码库，通过规划、分析和生成三个阶段实现，并在评估中表现出高效和高质量。


<details>
  <summary>Details</summary>
Motivation: 机器学习研究中代码实现的缺失导致重现结果和进一步研究耗时费力，而大语言模型（LLMs）在理解科学文献和生成高质量代码方面表现出色，这启发了PaperCoder的开发。

Method: PaperCoder通过三个阶段工作：规划（创建路线图、系统架构和配置文件）、分析（解读实现细节）和生成（生成模块化、依赖感知的代码），并利用专门设计的代理在流程中协作。

Result: 基于模型和人类评估（包括原作者评审），PaperCoder生成的代码实现质量高且忠实于原文，并在PaperBench基准测试中显著优于基线方法。

Conclusion: PaperCoder有效地解决了机器学习论文代码生成的问题，展示了其在生成高质量代码实现方面的潜力。

Abstract: Despite the rapid growth of machine learning research, corresponding code
implementations are often unavailable, making it slow and labor-intensive for
researchers to reproduce results and build upon prior work. In the meantime,
recent Large Language Models (LLMs) excel at understanding scientific documents
and generating high-quality code. Inspired by this, we introduce PaperCoder, a
multi-agent LLM framework that transforms machine learning papers into
functional code repositories. PaperCoder operates in three stages: planning,
where it constructs a high-level roadmap, designs the system architecture with
diagrams, identifies file dependencies, and generates configuration files;
analysis, which focuses on interpreting implementation-specific details; and
generation, where modular, dependency-aware code is produced. Moreover, each
phase is instantiated through a set of specialized agents designed to
collaborate effectively across the pipeline. We then evaluate PaperCoder on
generating code implementations from machine learning papers based on both
model-based and human evaluations, specifically from the original paper
authors, with author-released repositories as ground truth if available. Our
results demonstrate the effectiveness of PaperCoder in creating high-quality,
faithful implementations. Furthermore, it consistently shows strengths in the
recently released PaperBench benchmark, surpassing strong baselines by
substantial margins.

</details>


### [12] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
*Yangxinyu Xie,Bowen Jiang,Tanwi Mallick,Joshua David Bergerson,John K. Hutchison,Duane R. Verner,Jordan Branham,M. Ross Alexander,Robert B. Ross,Yan Feng,Leslie-Anne Levy,Weijie Su,Camillo J. Taylor*

Main category: cs.CL

TL;DR: 提出基于检索增强生成（RAG）的多代理LLM系统WildfireGPT，专为野火灾害决策支持设计，整合多种数据源，显著优于现有LLM方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽具变革性，但在需要专业知识的领域（如自然灾害）往往缺乏情境特异性信息，因此需开发针对性解决方案。

Method: 采用RAG框架的多代理LLM系统，集成自然灾害与极端天气预测数据、观测数据集及科学文献，以用户为中心设计，为利益相关者提供定制化风险分析。

Result: 通过10个专家主导的案例研究验证，WildfireGPT在决策支持方面显著优于现有基于LLM的解决方案。

Conclusion: WildfireGPT展示了RAG和多代理设计在提升LLM专业领域性能的有效性，为自然灾害应对提供了精准且情境相关的决策支持工具。

Abstract: Large language models (LLMs) are a transformational capability at the
frontier of artificial intelligence and machine learning that can support
decision-makers in addressing pressing societal challenges such as extreme
natural hazard events. As generalized models, LLMs often struggle to provide
context-specific information, particularly in areas requiring specialized
knowledge. In this work we propose a retrieval-augmented generation (RAG)-based
multi-agent LLM system to support analysis and decision-making in the context
of natural hazards and extreme weather events. As a proof of concept, we
present WildfireGPT, a specialized system focused on wildfire hazards. The
architecture employs a user-centered, multi-agent design to deliver tailored
risk insights across diverse stakeholder groups. By integrating natural hazard
and extreme weather projection data, observational datasets, and scientific
literature through an RAG framework, the system ensures both the accuracy and
contextual relevance of the information it provides. Evaluation across ten
expert-led case studies demonstrates that WildfireGPT significantly outperforms
existing LLM-based solutions for decision support.

</details>


### [13] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
*Kaidong Feng,Zhu Sun,Jie Yang,Hui Fang,Xinghua Qu,Wenyuan Liu*

Main category: cs.CL

TL;DR: 本文研究了知识蒸馏（KD）在捆绑生成中的应用，通过从大型教师模型向小型学生模型转移知识，以减少计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型（LLMs）的高计算成本，研究如何通过知识蒸馏提高捆绑生成的效率。

Method: 提出一个全面的KD框架，逐步提取知识、捕获不同量的知识，并利用互补的LLM适应技术。

Result: 实验表明知识格式、数量和利用方法共同影响LLM捆绑生成性能，证实了KD的潜力。

Conclusion: 知识蒸馏可以显著提升LLM捆绑生成的效率与效果。

Abstract: LLMs are increasingly explored for bundle generation, thanks to their
reasoning capabilities and knowledge. However, deploying large-scale LLMs
introduces significant efficiency challenges, primarily high computational
costs during fine-tuning and inference due to their massive parameterization.
Knowledge distillation (KD) offers a promising solution, transferring expertise
from large teacher models to compact student models. This study systematically
investigates knowledge distillation approaches for bundle generation, aiming to
minimize computational demands while preserving performance. We explore three
critical research questions: (1) how does the format of KD impact bundle
generation performance? (2) to what extent does the quantity of distilled
knowledge influence performance? and (3) how do different ways of utilizing the
distilled knowledge affect performance? We propose a comprehensive KD framework
that (i) progressively extracts knowledge (patterns, rules, deep thoughts);
(ii) captures varying quantities of distilled knowledge through different
strategies; and (iii) exploits complementary LLM adaptation techniques
(in-context learning, supervised fine-tuning, combination) to leverage
distilled knowledge in small student models for domain-specific adaptation and
enhanced efficiency. Extensive experiments provide valuable insights into how
knowledge format, quantity, and utilization methodologies collectively shape
LLM-based bundle generation performance, exhibiting KD's significant potential
for more efficient yet effective LLM-based bundle generation.

</details>


### [14] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
*Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Yongkang Huang,Yihan Shi,Xikun Zhang,Libiao Peng,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: CRDial框架通过多轮对话设计解决心理治疗的认知重构问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决由于临床医生短缺和污名化导致的心理治疗需求，通过人机交互实现有效的认知重构。

Method: 采用CRDial框架，设计多轮对话阶段（识别与重构），整合句子级支持策略，并引入多通道循环机制。

Result: 从LLM中提取大规模双语数据集Crisp，并训练Crispers模型，在多项评估中表现优越。

Conclusion: CRDial框架及Crispers模型在认知重构任务中展现出色效果，为人机交互心理治疗提供新方向。

Abstract: Cognitive Restructuring (CR) is a psychotherapeutic process aimed at
identifying and restructuring an individual's negative thoughts, arising from
mental health challenges, into more helpful and positive ones via multi-turn
dialogues. Clinician shortage and stigma urge the development of human-LLM
interactive psychotherapy for CR. Yet, existing efforts implement CR via simple
text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to
align with the psychotherapeutic process for effective CR. To address this gap,
we propose CRDial, a novel framework for CR, which creates multi-turn dialogues
with specifically designed identification and restructuring stages of negative
thoughts, integrates sentence-level supportive conversation strategies, and
adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we
distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from
LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and
14B scales. Extensive human studies show the superiority of Crispers in
pointwise, pairwise, and intervention evaluations.

</details>


### [15] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
*Ocheme Anthony Ekle,Biswarup Das*

Main category: cs.CL

TL;DR: 本研究开发了基于RNN和Transformer的迁移学习模型，用于低资源语言伊博语的英语翻译。通过结合RNN（LSTM和GRU）及注意力机制，使用预训练模型MarianNMT提升性能，最终BLEU分数提升了4.83，翻译准确率达到70%。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如伊博语）机器翻译性能不足的问题，利用现有技术提升翻译质量。

Method: 使用RNN（LSTM和GRU）结合注意力机制，并应用基于MarianNMT的迁移学习框架SimpleTransformers。

Result: RNN模型达到与现有基准相当的结果，迁移学习使BLEU分数提升4.83，翻译准确率达70%。

Conclusion: 结合RNN和迁移学习能有效提升低资源语言翻译性能，缩小性能差距。

Abstract: In this study, we develop Neural Machine Translation (NMT) and
Transformer-based transfer learning models for English-to-Igbo translation - a
low-resource African language spoken by over 40 million people across Nigeria
and West Africa. Our models are trained on a curated and benchmarked dataset
compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,
all verified by native language experts. We leverage Recurrent Neural Network
(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated
Recurrent Units (GRU), enhanced with attention mechanisms to improve
translation accuracy. To further enhance performance, we apply transfer
learning using MarianNMT pre-trained models within the SimpleTransformers
framework. Our RNN-based system achieves competitive results, closely matching
existing English-Igbo benchmarks. With transfer learning, we observe a
performance gain of +4.83 BLEU points, reaching an estimated translation
accuracy of 70%. These findings highlight the effectiveness of combining RNNs
with transfer learning to address the performance gap in low-resource language
translation tasks.

</details>


### [16] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
*Zhaolu Kang,Hongtian Cai,Xiangyang Ji,Jinzhe Li,Nanfei Gu*

Main category: cs.CL

TL;DR: JurisCTC模型针对法律领域无监督域自适应问题，通过对比学习实现跨法律领域的知识迁移，显著提升了法律判决预测任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决法律文本复杂性和标注数据稀缺性，探索跨法律领域的知识迁移，提升法律判决预测的准确性。

Method: 提出JurisCTC模型，利用对比学习方法区分不同法律领域的样本，实现跨领域的知识迁移。

Result: 在民事和刑事法律领域的判决预测任务中，JurisCTC分别达到76.59%和78.83%的峰值准确率。

Conclusion: JurisCTC在跨法律领域的知识迁移和判决预测任务中表现出色，为法律文本处理提供了新思路。

Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant
attention in the field of Natural Language Processing (NLP) owing to its
ability to enhance model generalization across diverse domains. However, its
application for knowledge transfer between distinct legal domains remains
largely unexplored. To address the challenges posed by lengthy and complex
legal texts and the limited availability of large-scale annotated datasets, we
propose JurisCTC, a novel model designed to improve the accuracy of Legal
Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC
facilitates effective knowledge transfer across various legal domains and
employs contrastive learning to distinguish samples from different domains.
Specifically, for the LJP task, we enable knowledge transfer between civil and
criminal law domains. Compared to other models and specific large language
models (LLMs), JurisCTC demonstrates notable advancements, achieving peak
accuracies of 76.59% and 78.83%, respectively.

</details>


### [17] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
*Xiuying Chen,Tairan Wang,Juexiao Zhou,Zirui Song,Xin Gao,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 该研究探讨了医学文本生成中的公平性问题，提出了一种算法以选择性优化表现不佳的群体，显著减少不同种族、性别和年龄组间的性能差异，同时保持整体性能。


<details>
  <summary>Details</summary>
Motivation: AI系统在医学应用中表现出色，但可能放大人类偏见，尤其在历史上服务不足的群体中表现欠佳。公平性在医学影像分类领域已有研究，但在文本生成领域尚属空白。

Method: 提出一种算法，选择性优化表现不佳的群体。选择规则不仅考虑词级准确性，还考虑病理准确性，确保整个过程可微分以有效训练模型。

Result: 评估显示，该算法将不同群体间的差异减少超过30%，文本生成准确性的相对变化通常控制在2%以内。

Conclusion: 该算法有效减少深度学习模型生成的偏见，提升医学文本生成的公平性和可靠性。

Abstract: Artificial intelligence (AI) systems, particularly those based on deep
learning models, have increasingly achieved expert-level performance in medical
applications. However, there is growing concern that such AI systems may
reflect and amplify human bias, and reduce the quality of their performance in
historically under-served populations. The fairness issue has attracted
considerable research interest in the medical imaging classification field, yet
it remains understudied in the text generation domain. In this study, we
investigate the fairness problem in text generation within the medical field
and observe significant performance discrepancies across different races,
sexes, and age groups, including intersectional groups, various model scales,
and different evaluation metrics. To mitigate this fairness issue, we propose
an algorithm that selectively optimizes those underperformed groups to reduce
bias. The selection rules take into account not only word-level accuracy but
also the pathology accuracy to the target reference, while ensuring that the
entire process remains fully differentiable for effective model training. Our
evaluations across multiple backbones, datasets, and modalities demonstrate
that our proposed algorithm enhances fairness in text generation without
compromising overall performance. Specifically, the disparities among various
groups across different metrics were diminished by more than 30% with our
algorithm, while the relative change in text generation accuracy was typically
within 2%. By reducing the bias generated by deep learning models, our proposed
approach can potentially alleviate concerns about the fairness and reliability
of text generation diagnosis in medical domain.
  Our code is publicly available to facilitate further research at
https://github.com/iriscxy/GenFair.

</details>


### [18] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
*Junyan Zhang,Shuliang Liu,Aiwei Liu,Yubo Gao,Jungang Li,Xiaojie Gu,Xuming Hu*

Main category: cs.CL

TL;DR: CoheMark是一种基于句子间连贯关系的高级句子级水印技术，通过模糊c均值聚类和句子选择标准，在保持文本质量的同时实现强水印效果。


<details>
  <summary>Details</summary>
Motivation: 现有句子级水印技术依赖随机分段或生成过程，可能限制合适句子的可用性并影响生成响应质量，因此需要一种在高质量文本与鲁棒水印检测之间平衡的方案。

Method: 采用模糊c均值聚类选择句子，并应用特定下一句选择标准，利用句子间连贯关系提升逻辑流畅性。

Result: 实验证明CoheMark在保持文本质量的同时，实现了强水印强度。

Conclusion: CoheMark通过连贯性驱动的句子选择，有效解决了水印技术与文本质量的平衡问题。

Abstract: Watermarking technology is a method used to trace the usage of content
generated by large language models. Sentence-level watermarking aids in
preserving the semantic integrity within individual sentences while maintaining
greater robustness. However, many existing sentence-level watermarking
techniques depend on arbitrary segmentation or generation processes to embed
watermarks, which can limit the availability of appropriate sentences. This
limitation, in turn, compromises the quality of the generated response. To
address the challenge of balancing high text quality with robust watermark
detection, we propose CoheMark, an advanced sentence-level watermarking
technique that exploits the cohesive relationships between sentences for better
logical fluency. The core methodology of CoheMark involves selecting sentences
through trained fuzzy c-means clustering and applying specific next sentence
selection criteria. Experimental evaluations demonstrate that CoheMark achieves
strong watermark strength while exerting minimal impact on text quality.

</details>


### [19] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
*Yulia Otmakhova,Hung Thinh Truong,Rahmad Mahendra,Zenan Zhai,Rongxin Zhu,Daniel Beck,Jey Han Lau*

Main category: cs.CL

TL;DR: FLUKE是一个任务无关的框架，通过系统化的最小化测试数据变体评估模型鲁棒性，覆盖语言多层级变化，结合LLM与人工验证生成修改。结果显示语言变体影响因任务而异，LLM整体更优但仍脆弱，尤其对否定修改。


<details>
  <summary>Details</summary>
Motivation: 旨在系统评估模型对不同语言变体的鲁棒性，弥补传统测试的不足，理解模型行为背后的脆弱性。

Method: 提出FLUKE框架，利用LLM生成从拼写到方言的多层级语言变体数据，并通过人工验证保证质量，在四大NLP任务上测试微调模型和LLM。

Result: 1) 语言变体的影响与任务强相关；2) LLM整体鲁棒性优于微调模型，但对特定变体仍敏感；3) 所有模型普遍对否定修改表现脆弱。

Conclusion: 系统性鲁棒性测试对理解模型行为至关重要，当前模型（包括LLM）仍需针对语言变体进一步优化。

Abstract: We present FLUKE (Framework for LingUistically-driven and tasK-agnostic
robustness Evaluation), a task-agnostic framework for assessing model
robustness through systematic minimal variations of test data. FLUKE introduces
controlled variations across linguistic levels - from orthography to dialect
and style varieties - and leverages large language models (LLMs) with human
validation to generate modifications. We demonstrate FLUKE's utility by
evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and
reveal that (1) the impact of linguistic variations is highly task-dependent,
with some tests being critical for certain tasks but irrelevant for others; (2)
while LLMs have better overall robustness compared to fine-tuned models, they
still exhibit significant brittleness to certain linguistic variations; (3) all
models show substantial vulnerability to negation modifications across most
tasks. These findings highlight the importance of systematic robustness testing
for understanding model behaviors.

</details>


### [20] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
*Zihan Wang,Lu Yuan,Zhengxuan Zhang,Qing Zhao*

Main category: cs.CL

TL;DR: 本文提出了双面共情框架（DAE），结合认知与情感共情，从创作者和读者角度分析虚假信息，并通过LLMs模拟读者反应，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息检测方法忽视人类共情在传播中的作用，DAE框架旨在填补这一空白。

Method: 采用双面共情框架（DAE），分析创作者的认知策略与情感诉求，并通过LLMs模拟读者的认知判断与情感反应。

Result: 在基准数据集上的实验显示，DAE优于现有方法，并在多模态虚假信息检测中提供了新范式。

Conclusion: DAE框架通过共情视角为虚假信息检测提供了更全面、以人为本的方法。

Abstract: In the digital era, social media has become a major conduit for information
dissemination, yet it also facilitates the rapid spread of misinformation.
Traditional misinformation detection methods primarily focus on surface-level
features, overlooking the crucial roles of human empathy in the propagation
process. To address this gap, we propose the Dual-Aspect Empathy Framework
(DAE), which integrates cognitive and emotional empathy to analyze
misinformation from both the creator and reader perspectives. By examining
creators' cognitive strategies and emotional appeals, as well as simulating
readers' cognitive judgments and emotional responses using Large Language
Models (LLMs), DAE offers a more comprehensive and human-centric approach to
misinformation detection. Moreover, we further introduce an empathy-aware
filtering mechanism to enhance response authenticity and diversity.
Experimental results on benchmark datasets demonstrate that DAE outperforms
existing methods, providing a novel paradigm for multimodal misinformation
detection.

</details>


### [21] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
*Chengguang Gan,Sunbowen Lee,Zhixi Cai,Yanbin Wei,Lei Zheng,Yunhao Liang,Shiwen Ni,Tatsunori Mori*

Main category: cs.CL

TL;DR: 该研究首次将互增强效应（MRE）扩展到多模态信息提取领域，提出多模态互增强效应（M-MRE）任务并构建数据集，通过提示格式适配器（PFA）验证了MRE在多模态场景中的可行性和通用性。


<details>
  <summary>Details</summary>
Motivation: 探索MRE在多模态领域的适用性，填补视觉与多模态任务中MRE研究的空白，以提升跨模态任务的性能。

Method: 提出多模态互增强效应（M-MRE）任务及配套数据集，设计兼容大型视觉语言模型（LVLMs）的提示格式适配器（PFA）。

Result: 实验表明MRE在多模态文本-图像理解场景中同样有效，实现了三个相关任务的互增强，验证其跨领域通用性。

Conclusion: MRE可推广至多模态领域，为跨模态任务联合建模提供新思路，PFA的高兼容性为未来研究奠定基础。

Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection
of information extraction and model interpretability. MRE aims to leverage the
mutual understanding between tasks of different granularities, enhancing the
performance of both coarse-grained and fine-grained tasks through joint
modeling. While MRE has been explored and validated in the textual domain, its
applicability to visual and multimodal domains remains unexplored. In this
work, we extend MRE to the multimodal information extraction domain for the
first time. Specifically, we introduce a new task: Multimodal Mutual
Reinforcement Effect (M-MRE), and construct a corresponding dataset to support
this task. To address the challenges posed by M-MRE, we further propose a
Prompt Format Adapter (PFA) that is fully compatible with various Large
Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can
also be observed in the M-MRE task, a multimodal text-image understanding
scenario. This provides strong evidence that MRE facilitates mutual gains
across three interrelated tasks, confirming its generalizability beyond the
textual domain.

</details>


### [22] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
*Jose G. Moreno,Jesus Lovon,M'Rick Robin-Charlet,Christine Damase-Michel,Lynda Tamine*

Main category: cs.CL

TL;DR: PatientDx是一个无需在患者数据上微调的LLM合并框架，旨在解决医疗领域的数据隐私问题，并在MIMIC-IV数据集上取得7%的AUROC提升。


<details>
  <summary>Details</summary>
Motivation: 医疗领域数据隐私问题突出，传统微调方法需大量敏感数据，PatientDx旨在通过模型合并技术避免数据泄露风险。

Method: 基于LLM合并技术，优化模块合并策略，通过数值推理模型调整超参数，无需在目标数据上训练模型。

Result: 在MIMIC-IV的死亡率预测任务中，AUROC提升7%，且在避免数据泄露的同时保持性能。

Conclusion: PatientDx提供了一种隐私保护的替代方案，性能不逊于传统微调方法，具有实际应用潜力。

Abstract: Fine-tuning of Large Language Models (LLMs) has become the default practice
for improving model performance on a given task. However, performance
improvement comes at the cost of training on vast amounts of annotated data
which could be sensitive leading to significant data privacy concerns. In
particular, the healthcare domain is one of the most sensitive domains exposed
to data privacy issues. In this paper, we present PatientDx, a framework of
model merging that allows the design of effective LLMs for health-predictive
tasks without requiring fine-tuning nor adaptation on patient data. Our
proposal is based on recently proposed techniques known as merging of LLMs and
aims to optimize a building block merging strategy. PatientDx uses a pivotal
model adapted to numerical reasoning and tunes hyperparameters on examples
based on a performance metric but without training of the LLM on these data.
Experiments using the mortality tasks of the MIMIC-IV dataset show improvements
up to 7% in terms of AUROC when compared to initial models. Additionally, we
confirm that when compared to fine-tuned models, our proposal is less prone to
data leak problems without hurting performance. Finally, we qualitatively show
the capabilities of our proposal through a case study. Our best model is
publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>


### [23] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
*Yongxuan Wu,Runyu Chen,Peiyu Liu,Hongjin Qian*

Main category: cs.CL

TL;DR: 摘要提出了一个针对长上下文理解的新基准，特别关注口语对话的高冗余性和信息密度不均问题，并评估了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的口语对话具有高冗余性和信息密度不均的特点，但现有基准未充分反映这些复杂性，限制了大型语言模型在实际场景中的应用。

Method: 构建了一个基于直播流的口语长文本数据集，设计了三类任务（检索依赖、推理依赖和混合），并评估了现有方法和提出的新基线。

Result: 现有方法在任务特定偏好上表现良好，但对高冗余输入表现不佳；提出的新基线在任务间表现更优。

Conclusion: 研究揭示了当前方法的局限性，提出改进长上下文理解的方向，并为开发实际系统提供了实用的基准。

Abstract: Long-context understanding poses significant challenges in natural language
processing, particularly for real-world dialogues characterized by speech-based
elements, high redundancy, and uneven information density. Although large
language models (LLMs) achieve impressive results on existing benchmarks, these
datasets fail to reflect the complexities of such texts, limiting their
applicability to practical scenarios. To bridge this gap, we construct the
first spoken long-text dataset, derived from live streams, designed to reflect
the redundancy-rich and conversational nature of real-world scenarios. We
construct tasks in three categories: retrieval-dependent, reasoning-dependent,
and hybrid. We then evaluate both popular LLMs and specialized methods to
assess their ability to understand long-contexts in these tasks. Our results
show that current methods exhibit strong task-specific preferences and perform
poorly on highly redundant inputs, with no single method consistently
outperforming others. We propose a new baseline that better handles redundancy
in spoken text and achieves strong performance across tasks. Our findings
highlight key limitations of current methods and suggest future directions for
improving long-context understanding. Finally, our benchmark fills a gap in
evaluating long-context spoken language understanding and provides a practical
foundation for developing real-world e-commerce systems. The code and benchmark
are available at https://github.com/Yarayx/livelongbench.

</details>


### [24] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
*Jihyun Lee,Yejin Jeon,Seungyeon Seo,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 论文提出了PicPersona-TOD数据集和Pictor模型，通过用户图像实现个性化对话，提升交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统生成通用、单调的回复，缺乏个性化和对用户个人属性的适应性。

Method: 结合用户图像的Persona数据集，利用第一印象、对话策略引导提示和外部知识减少幻觉。

Result: 个性化回复显著提升用户体验，Pictor模型在新领域也表现优异。

Conclusion: PicPersona-TOD和Pictor模型有效提升了对话系统的个性化和适应性。

Abstract: Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests
through natural language interactions, yet existing systems often produce
generic, monotonic responses that lack individuality and fail to adapt to
users' personal attributes. To address this, we introduce PicPersona-TOD, a
novel dataset that incorporates user images as part of the persona, enabling
personalized responses tailored to user-specific factors such as age or
emotional context. This is facilitated by first impressions, dialogue
policy-guided prompting, and the use of external knowledge to reduce
hallucinations. Human evaluations confirm that our dataset enhances user
experience, with personalized responses contributing to a more engaging
interaction. Additionally, we introduce a new NLG model, Pictor, which not only
personalizes responses, but also demonstrates robust performance across unseen
domains https://github.com/JihyunLee1/PicPersona.

</details>


### [25] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
*Anna Lieb,Maneesh Arora,Eni Mustafaraj*

Main category: cs.CL

TL;DR: 论文探讨了利用LLM生成的文本增强来提升主题建模的实用性和解释性，并通过政治学案例验证了GPT-4增强的高效性。


<details>
  <summary>Details</summary>
Motivation: 传统无监督学习方法（如主题建模）在解释性和针对特定领域研究问题的实用性上存在局限，作者希望通过LLM生成的文本增强来解决这些问题。

Method: 采用GPT-4生成的文本对主题建模输出进行增强，并通过政治学案例研究验证方法的有效性。

Result: 研究发现，GPT-4增强的主题建模能生成高度可解释的类别，可用于特定领域研究问题且需极少人工干预。

Conclusion: LLM生成的文本增强能显著提升主题建模的实用性和解释性，为社会科学研究提供高效工具。

Abstract: Unsupervised machine learning techniques, such as topic modeling and
clustering, are often used to identify latent patterns in unstructured text
data in fields such as political science and sociology. These methods overcome
common concerns about reproducibility and costliness involved in the
labor-intensive process of human qualitative analysis. However, two major
limitations of topic models are their interpretability and their practicality
for answering targeted, domain-specific social science research questions. In
this work, we investigate opportunities for using LLM-generated text
augmentation to improve the usefulness of topic modeling output. We use a
political science case study to evaluate our results in a domain-specific
application, and find that topic modeling using GPT-4 augmentations creates
highly interpretable categories that can be used to investigate domain-specific
research questions with minimal human guidance.

</details>


### [26] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
*Xin Yi,Shunfan Zhengc,Linlin Wanga,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: CDG-KD是一种通过对比解码和双向蒸馏的统一框架，能在未经授权的知识蒸馏中同时实现水印擦除和伪造攻击。


<details>
  <summary>Details</summary>
Motivation: 研究水印在未经授权知识蒸馏下的鲁棒性和不可伪造性，填补现有水印攻击方法的不足。

Method: 采用对比解码提取学生模型与弱水印参考输出的差异，通过双向蒸馏训练两种学生模型，分别实现水印擦除和伪造。

Result: 实验证明CDG-KD能有效攻击水印且保持模型整体性能。

Conclusion: 研究强调了开发鲁棒且不可伪造水印方案的必要性。

Abstract: Watermarking has emerged as a critical technique for combating misinformation
and protecting intellectual property in large language models (LLMs). A recent
discovery, termed watermark radioactivity, reveals that watermarks embedded in
teacher models can be inherited by student models through knowledge
distillation. On the positive side, this inheritance allows for the detection
of unauthorized knowledge distillation by identifying watermark traces in
student models. However, the robustness of watermarks against scrubbing attacks
and their unforgeability in the face of spoofing attacks under unauthorized
knowledge distillation remain largely unexplored. Existing watermark attack
methods either assume access to model internals or fail to simultaneously
support both scrubbing and spoofing attacks. In this work, we propose
Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified
framework that enables bidirectional attacks under unauthorized knowledge
distillation. Our approach employs contrastive decoding to extract corrupted or
amplified watermark texts via comparing outputs from the student model and
weakly watermarked references, followed by bidirectional distillation to train
new student models capable of watermark removal and watermark forgery,
respectively. Extensive experiments show that CDG-KD effectively performs
attacks while preserving the general performance of the distilled model. Our
findings underscore critical need for developing watermarking schemes that are
robust and unforgeable.

</details>


### [27] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
*Yejin Bang,Ziwei Ji,Alan Schelten,Anthony Hartshorn,Tara Fowler,Cheng Zhang,Nicola Cancedda,Pascale Fung*

Main category: cs.CL

TL;DR: 本文提出了一个全面的幻觉基准，通过区分外部与内部幻觉并引入动态测试集生成来应对LLM生成内容偏离的问题，旨在推动一致性研究与减少数据泄露。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容偏离（幻觉）问题，提升用户信任与生成AI系统的采用率。

Method: 通过新提出的外部幻觉任务和动态测试集生成，结合清晰分类法建立统一框架。

Result: 提出了一个避免数据泄露的稳健基准，并分析了现有基准的局限性与饱和问题。

Conclusion: 清晰的分类与动态基准有助于推动幻觉研究的进步，减少数据泄露风险。

Abstract: Large language models (LLMs) often generate responses that deviate from user
input or training data, a phenomenon known as "hallucination." These
hallucinations undermine user trust and hinder the adoption of generative AI
systems. Addressing hallucinations is essential for the advancement of LLMs.
This paper introduces a comprehensive hallucination benchmark, incorporating
both new extrinsic and existing intrinsic evaluation tasks, built upon clear
taxonomy of hallucination. A major challenge in benchmarking hallucinations is
the lack of a unified framework due to inconsistent definitions and
categorizations. We disentangle LLM hallucination from "factuality," proposing
a clear taxonomy that distinguishes between extrinsic and intrinsic
hallucinations, to promote consistency and facilitate research. Extrinsic
hallucinations, where the generated content is not consistent with the training
data, are increasingly important as LLMs evolve. Our benchmark includes dynamic
test set generation to mitigate data leakage and ensure robustness against such
leakage. We also analyze existing benchmarks, highlighting their limitations
and saturation. The work aims to: (1) establish a clear taxonomy of
hallucinations, (2) introduce new extrinsic hallucination tasks, with data that
can be dynamically regenerated to prevent saturation by leakage, (3) provide a
comprehensive analysis of existing benchmarks, distinguishing them from
factuality evaluations.

</details>


### [28] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
*Rei Higuchi,Ryotaro Kawata,Naoki Nishikawa,Kazusato Oko,Shoichiro Yamaguchi,Sosuke Kobayashi,Seiya Tokui,Kohei Hayashi,Daisuke Okanohara,Taiji Suzuki*

Main category: cs.CL

TL;DR: 研究探讨了在预训练数据前添加元数据（如URL、域名等）对提升语言模型性能的影响，发现该技术在不同下游任务中效果不一，取决于上下文是否足以推断潜在语义。


<details>
  <summary>Details</summary>
Motivation: 探索预训练阶段添加元数据是否能帮助模型更好地获取潜在语义，以及这种技术为何在某些任务中有效而在其他任务中无效。

Method: 通过人工生成的数据（基于概率上下文无关文法）分析模型行为，比较不同上下文长度下元数据的作用。

Result: 当上下文足够长可推断潜在语义时，元数据提升模型性能；反之若上下文信息不足，则会负面影响性能。

Conclusion: 元数据技术的效果依赖于下游任务提示中能否推断潜在语义，需根据具体任务上下文权衡使用。

Abstract: The ability to acquire latent semantics is one of the key properties that
determines the performance of language models. One convenient approach to
invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at
the beginning of texts in the pre-training data, making it easier for the model
to access latent semantics before observing the entire text. Previous studies
have reported that this technique actually improves the performance of trained
models in downstream tasks; however, this improvement has been observed only in
specific downstream tasks, without consistent enhancement in average next-token
prediction loss. To understand this phenomenon, we closely investigate how
prepending metadata during pre-training affects model performance by examining
its behavior using artificial data. Interestingly, we found that this approach
produces both positive and negative effects on the downstream tasks. We
demonstrate that the effectiveness of the approach depends on whether latent
semantics can be inferred from the downstream task's prompt. Specifically,
through investigations using data generated by probabilistic context-free
grammars, we show that training with metadata helps improve model's performance
when the given context is long enough to infer the latent semantics. In
contrast, the technique negatively impacts performance when the context lacks
the necessary information to make an accurate posterior inference.

</details>


### [29] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
*Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Yunjie Ji,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: 该论文构建了一个大规模、难度分级的推理数据集，并利用通过率和变异系数筛选高质量数据，显著提升了基础模型的推理能力，在AIME2024数学推理基准上达到79.2%的通过率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在基础训练过程和数据质量方面缺乏深入理解的问题。

Method: 构建分级推理数据集，利用通过率和变异系数筛选数据，调整学习率进行针对性训练。

Result: 模型在AIME2024数学推理基准上达到79.2%的通过率，接近最先进水平。

Conclusion: 通过数据质量筛选和训练优化，显著提升了基础模型的推理能力，并公开数据集以推动开源进展。

Abstract: Although large language models (LLMs) have recently achieved remarkable
performance on various complex reasoning benchmarks, the academic community
still lacks an in-depth understanding of base model training processes and data
quality. To address this, we construct a large-scale, difficulty-graded
reasoning dataset containing approximately 3.34 million unique queries of
varying difficulty levels and about 40 million distilled responses generated by
multiple models over several passes. Leveraging pass rate and Coefficient of
Variation (CV), we precisely select the most valuable training data to enhance
reasoning capability. Notably, we observe a training pattern shift, indicating
that reasoning-focused training based on base models requires higher learning
rates for effective training. Using this carefully selected data, we
significantly improve the reasoning capabilities of the base model, achieving a
pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This
result surpasses most current distilled models and closely approaches
state-of-the-art performance. We provide detailed descriptions of our data
processing, difficulty assessment, and training methodology, and have publicly
released all datasets and methods to promote rapid progress in open-source
long-reasoning LLMs. The dataset is available at:
https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [30] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
*Zhenkai Qin,Guifang Yang,Dongze Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RAGAT-Mind的多粒度建模方法，用于中文谣言检测，结合了多种深度学习技术，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着虚假信息在社交媒体上的泛滥，有效的谣言检测成为自然语言处理领域的重要挑战。

Method: 模型结合了TextCNN、双向GRU、多头自注意力机制和双向图卷积网络（BiGCN），用于提取不同层级的语言特征。

Result: 在Weibo1-Rumor数据集上，模型达到了99.2%的准确率和0.9919的宏观F1分数。

Conclusion: 实验验证了结合层次化语言特征和图语义结构的有效性，模型表现出良好的泛化能力和可解释性，具有实际应用价值。

Abstract: As false information continues to proliferate across social media platforms,
effective rumor detection has emerged as a pressing challenge in natural
language processing. This paper proposes RAGAT-Mind, a multi-granular modeling
approach for Chinese rumor detection, built upon the MindSpore deep learning
framework. The model integrates TextCNN for local semantic extraction,
bidirectional GRU for sequential context learning, Multi-Head Self-Attention
for global dependency focusing, and Bidirectional Graph Convolutional Networks
(BiGCN) for structural representation of word co-occurrence graphs. Experiments
on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior
classification performance, attaining 99.2% accuracy and a macro-F1 score of
0.9919. The results validate the effectiveness of combining hierarchical
linguistic features with graph-based semantic structures. Furthermore, the
model exhibits strong generalization and interpretability, highlighting its
practical value for real-world rumor detection applications.

</details>


### [31] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
*Samaneh Hosseini Moghaddam,Kelly Lyons,Cheryl Regehr,Vivek Goel,Kaitlyn Regehr*

Main category: cs.CL

TL;DR: 该论文提出了一个用于区分在线文本中辱骂语言关键特征的分类法，旨在帮助检测和缓解网络滥用行为。


<details>
  <summary>Details</summary>
Motivation: 网络辱骂语言的泛滥对个人和社区的健康与福祉构成威胁，需要有效的识别和缓解方法以及持续监控和早期干预机制。

Method: 通过系统化方法整合了18个现有多标签数据集的分类体系，开发了一个分层、多面的分类法，包含5个类别和17个维度。

Result: 最终的分类法能够从背景、目标、强度、直接性和辱骂主题等多个方面对网络辱骂语言进行分类。

Conclusion: 这一共享的理解有助于推动在线辱骂检测和缓解领域的协作与知识共享，加速相关研究的进展。

Abstract: The proliferation of abusive language in online communications has posed
significant risks to the health and wellbeing of individuals and communities.
The growing concern regarding online abuse and its consequences necessitates
methods for identifying and mitigating harmful content and facilitating
continuous monitoring, moderation, and early intervention. This paper presents
a taxonomy for distinguishing key characteristics of abusive language within
online text. Our approach uses a systematic method for taxonomy development,
integrating classification systems of 18 existing multi-label datasets to
capture key characteristics relevant to online abusive language classification.
The resulting taxonomy is hierarchical and faceted, comprising 5 categories and
17 dimensions. It classifies various facets of online abuse, including context,
target, intensity, directness, and theme of abuse. This shared understanding
can lead to more cohesive efforts, facilitate knowledge exchange, and
accelerate progress in the field of online abuse detection and mitigation among
researchers, policy makers, online platform owners, and other stakeholders.

</details>


### [32] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
*Zena Al-Khalili,Nick Howell,Dietrich Klakow*

Main category: cs.CL

TL;DR: 论文研究了代码辅助LLMs在数学推理任务中的表现，发现现有评估多关注执行正确性，而忽略了对生成程序的深入分析。通过手动和自动评估，揭示了LLMs在不同数学问题上的规则遵循能力差异，并强调需超越执行准确性的评估需求。


<details>
  <summary>Details</summary>
Motivation: 现有对代码辅助LLMs的评估局限于执行正确性，缺乏对生成程序的深入分析。本文旨在填补这一空白，探究LLMs在数学推理任务中对数学规则的遵循程度及其对性能的影响。

Method: 评估了五个不同LLMs在两个数学数据集上的生成程序，结合手动和自动分析，重点关注程序是否基于数学规则（grounding）及其与最终表现的关系。

Result: 结果显示：1）模型能力和题目难度影响规则遵循程度；2）闭源模型更有效利用数学规则，开源模型则表现不佳；3）在更难的MATH500数据集中，规则遵循程序比例减半，而未遵循的生成翻倍。

Conclusion: 研究强调了需要超越执行准确性的评估，以更深入理解代码辅助LLMs在数学领域的能力与局限。

Abstract: Assisting LLMs with code generation improved their performance on
mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is
generally restricted to execution correctness, lacking a rigorous evaluation of
their generated programs. In this work, we bridge this gap by conducting an
in-depth analysis of code-assisted LLMs' generated programs in response to math
reasoning tasks. Our evaluation focuses on the extent to which LLMs ground
their programs to math rules, and how that affects their end performance. For
this purpose, we assess the generations of five different LLMs, on two
different math datasets, both manually and automatically. Our results reveal
that the distribution of grounding depends on LLMs' capabilities and the
difficulty of math problems. Furthermore, mathematical grounding is more
effective for closed-source models, while open-source models fail to employ
math rules in their solutions correctly. On MATH500, the percentage of grounded
programs decreased to half, while the ungrounded generations doubled in
comparison to ASDiv grade-school problems. Our work highlights the need for
in-depth evaluation beyond execution accuracy metrics, toward a better
understanding of code-assisted LLMs' capabilities and limits in the math
domain.

</details>


### [33] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
*Yuanchang Ye,Weiyan Wen*

Main category: cs.CL

TL;DR: 该研究提出了一种基于Split Conformal Prediction（SCP）框架的方法，用于缓解大型视觉语言模型（LVLM）在视觉问答任务中的幻觉问题，通过动态阈值校准和跨模态一致性验证来量化不确定性。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态推理中表现出色，但其输出常包含高置信度的幻觉内容，这对安全关键应用构成风险。因此，需要一种模型无关的不确定性量化方法，以在用户定义的风险水平下提供统计保证。

Method: 该框架将数据分为校准集和测试集，通过非一致性分数构建预测集，并结合动态阈值校准和跨模态一致性验证。创新点包括严格控制边际覆盖率、动态调整预测集大小以及无需重新训练。

Result: 在ScienceQA和MMMU等基准测试中，该方法在所有α值下均实现了理论保证，且在不同校准-测试分割比例下表现稳健。

Conclusion: 该框架为多模态AI系统提供了一种可扩展的解决方案，用于幻觉检测和不确定性感知决策，弥合了理论可靠性与实际应用之间的差距。

Abstract: This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.

</details>


### [34] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
*Jared Fernandez,Clara Na,Vashisth Tiwari,Yonatan Bisk,Sasha Luccioni,Emma Strubell*

Main category: cs.CL

TL;DR: 这篇论文分析了大型语言模型（LLM）推理优化对能源消耗的影响，提出了一种建模方法，并通过实验证明优化措施可减少高达73%的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和普及度的增加，其计算和环境成本上升。现有研究多专注于理想环境下的延迟降低，忽略了实际推理工作负载对能源使用的影响。

Method: 引入了一种建模方法，通过输入-输出令牌分布和批量大小的分箱策略近似模拟实际LLM工作流程，并分析了软件框架、解码策略、GPU架构等多种因素。

Result: 研究发现推理优化的效果高度依赖于工作负载、软件堆栈和硬件加速器，基于FLOPs或理论GPU利用率的能源估计显著低估实际消耗。优化措施可减少高达73%的能源使用。

Conclusion: 研究结果为可持续LLM部署提供了基础，并为未来AI基础设施的能源高效设计策略提供了指导。

Abstract: As large language models (LLMs) scale in size and adoption, their
computational and environmental costs continue to rise. Prior benchmarking
efforts have primarily focused on latency reduction in idealized settings,
often overlooking the diverse real-world inference workloads that shape energy
use. In this work, we systematically analyze the energy implications of common
inference efficiency optimizations across diverse Natural Language Processing
(NLP) and generative Artificial Intelligence (AI) workloads, including
conversational AI and code generation. We introduce a modeling approach that
approximates real-world LLM workflows through a binning strategy for
input-output token distributions and batch size variations. Our empirical
analysis spans software frameworks, decoding strategies, GPU architectures,
online and offline serving settings, and model parallelism configurations. We
show that the effectiveness of inference optimizations is highly sensitive to
workload geometry, software stack, and hardware accelerators, demonstrating
that naive energy estimates based on FLOPs or theoretical GPU utilization
significantly underestimate real-world energy consumption. Our findings reveal
that the proper application of relevant inference efficiency optimizations can
reduce total energy use by up to 73% from unoptimized baselines. These insights
provide a foundation for sustainable LLM deployment and inform energy-efficient
design strategies for future AI infrastructure.

</details>


### [35] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
*Haru-Tada Sato,Fuka Matsuzaki,Jun-ichiro Takahashi*

Main category: cs.CL

TL;DR: 通过集成贝叶斯推断(EBI)，结合多个小型语言模型(SLM)的推理能力，达到与专有大型语言模型(LLM)相当的准确性。实验证明，即使在模型性能较低或语言不同的情况下，EBI也能显著提升整体效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用计算资源有限的SLM组合，达到与昂贵LLM相媲美的性能，同时研究如何有效整合性能较低的模型。

Method: 提出Ensemble Bayesian Inference (EBI)方法，基于贝叶斯估计整合多个SLM的输出，并验证其在多语言和多任务（能力评估、用户画像分析）中的表现。

Result: 实验结果显示，EBI能显著提升SLM集成性能，甚至纳入负提升模型也能改善整体表现，且在多语言任务中均有效。

Conclusion: EBI为资源有限的高性能AI系统开发提供了新思路，同时提升了低性能模型的利用价值。

Abstract: This study explores the potential of small language model(SLM) ensembles to
achieve accuracy comparable to proprietary large language models (LLMs). We
propose Ensemble Bayesian Inference (EBI), a novel approach that applies
Bayesian estimation to combine judgments from multiple SLMs, allowing them to
exceed the performance limitations of individual models. Our experiments on
diverse tasks(aptitude assessments and consumer profile analysis in both
Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze
cases where incorporating models with negative Lift values into ensembles
improves overall performance, and we examine the method's efficacy across
different languages. These findings suggest new possibilities for constructing
high-performance AI systems with limited computational resources and for
effectively utilizing models with individually lower performance. Building on
existing research on LLM performance evaluation, ensemble methods, and
open-source LLM utilization, we discuss the novelty and significance of our
approach.

</details>


### [36] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
*Cheng Wang,Yue Liu,Baolong Li,Duzhen Zhang,Zhongzhi Li,Junfeng Fang*

Main category: cs.CL

TL;DR: 本文综述了大型推理模型（LRMs）的安全性风险、攻击方式和防御策略，并提出了分类体系以便理解和改进其安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型推理模型在数学和编码等任务中的广泛应用，其安全漏洞和风险逐渐显现，影响了实际部署和应用。本文旨在系统梳理这些安全问题，为未来研究提供清晰框架。

Method: 通过对现有研究的综合分析，分类总结了LRMs的安全风险、攻击手段及防御措施，并构建了一个详细的分类体系。

Result: 提出了一个全面的分类体系，系统化地展现了LRMs当前的安全问题和应对策略。

Conclusion: 该研究为提升LRMs的安全性和可靠性提供了系统性的指导，有助于未来进一步优化和部署这些模型。

Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks
like mathematics and coding, leveraging their advanced reasoning capabilities.
Nevertheless, as these capabilities progress, significant concerns regarding
their vulnerabilities and safety have arisen, which can pose challenges to
their deployment and application in real-world settings. This paper presents a
comprehensive survey of LRMs, meticulously exploring and summarizing the newly
emerged safety risks, attacks, and defense strategies. By organizing these
elements into a detailed taxonomy, this work aims to offer a clear and
structured understanding of the current safety landscape of LRMs, facilitating
future research and development to enhance the security and reliability of
these powerful models.

</details>


### [37] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
*Vansh Gupta,Sankalan Pal Chowdhury,Vilém Zouhar,Donya Rooein,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 研究评估了主流大语言模型（LLM）在六种非英语语言（印地语、阿拉伯语、波斯语、泰卢固语、乌克兰语、捷克语）及英语教育任务中的表现，发现其性能与训练数据中语言资源的丰富程度相关，建议部署前验证模型在目标语言任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在非英语教育场景中的应用合理性，填补多语言表现评估的空白，为教育实践提供参考。

Method: 选择四种教育任务（误解识别、针对性反馈、互动辅导、翻译评分），在六种非英语语言及英语中测试主流LLM的性能。

Result: 模型表现与训练数据语言资源量成正比，低资源语言任务性能显著低于英语，但多数语言表现尚可。

Conclusion: 建议实际应用前验证LLM在目标语言任务中的性能，尤其是低资源语言场景。

Abstract: Large language models (LLMs) are increasingly being adopted in educational
settings. These applications expand beyond English, though current LLMs remain
primarily English-centric. In this work, we ascertain if their use in education
settings in non-English languages is warranted. We evaluated the performance of
popular LLMs on four educational tasks: identifying student misconceptions,
providing targeted feedback, interactive tutoring, and grading translations in
six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to
English. We find that the performance on these tasks somewhat corresponds to
the amount of language represented in training data, with lower-resource
languages having poorer task performance. Although the models perform
reasonably well in most languages, the frequent performance drop from English
is significant. Thus, we recommend that practitioners first verify that the LLM
works well in the target language for their educational task before deployment.

</details>


### [38] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula Allen-Meares,Eulalia Puig Abril,Olga Garcia,Carolyn Dickens,Andrew Boyd*

Main category: cs.CL

TL;DR: 比较两种对话助手（神经符号架构与基于ChatGPT）在帮助心衰患者查询食物含盐量时的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，评估其在医疗领域与传统架构的优劣显得尤为重要。

Method: 进行组内用户研究，对比两种系统的表现。

Result: 自研系统更准确、高效且简洁，而ChatGPT版本错误更少、需要更少的澄清。患者对二者无明显偏好。

Conclusion: 两种架构各有优劣，在医疗应用中需权衡选择。

Abstract: Conversational assistants are becoming more and more popular, including in
healthcare, partly because of the availability and capabilities of Large
Language Models. There is a need for controlled, probing evaluations with real
stakeholders which can highlight advantages and disadvantages of more
traditional architectures and those based on generative AI. We present a
within-group user study to compare two versions of a conversational assistant
that allows heart failure patients to ask about salt content in food. One
version of the system was developed in-house with a neurosymbolic architecture,
and one is based on ChatGPT. The evaluation shows that the in-house system is
more accurate, completes more tasks and is less verbose than the one based on
ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors
and requires fewer clarifications to complete the task. Patients show no
preference for one over the other.

</details>


### [39] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
*Piotr Nawrot,Robert Li,Renjie Huang,Sebastian Ruder,Kelly Marchisio,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: 稀疏注意力是扩展Transformer大模型长文本处理能力的有效方法，但需权衡效率与准确性。研究比较了不同模型规模、序列长度和稀疏度下的稀疏注意力方法，提出了新发现和稀疏注意力的定制化缩放规律。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏注意力在Transformer大模型中的应用潜力及其效率-准确性权衡的研究空白，探索其在长序列任务中的实际效果。

Method: 对不同模型规模、序列长度和稀疏度下的训练无关稀疏注意力方法进行比较实验，并设计基于自然语言的可控任务进行评估。

Result: 1）超长序列中，大且高稀疏模型优于小且密集模型；2）解码阶段可达到的稀疏度高于预填充阶段，且与模型规模相关；3）稀疏注意力并非通用方案，需任务特定策略；4）提出了稀疏注意力的新缩放规律。

Conclusion: 稀疏注意力能增强Transformer处理长序列的能力，但需针对性能敏感场景谨慎评估其取舍。

Abstract: Sparse attention offers a promising strategy to extend long-context
capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy
trade-offs, and systematic scaling studies remain unexplored. To address this
gap, we perform a careful comparison of training-free sparse attention methods
at varying model scales, sequence lengths, and sparsity levels on a diverse
collection of long-sequence tasks-including novel ones that rely on natural
language while remaining controllable and easy to evaluate. Based on our
experiments, we report a series of key findings: 1) an isoFLOPS analysis
reveals that for very long sequences, larger and highly sparse models are
preferable to smaller and dense ones. 2) The level of sparsity attainable while
statistically guaranteeing accuracy preservation is higher during decoding than
prefilling, and correlates with model size in the former. 3) There is no clear
strategy that performs best across tasks and phases, with different units of
sparsification or budget adaptivity needed for different scenarios. Even
moderate sparsity levels often result in significant performance degradation on
at least one task, highlighting that sparse attention is not a universal
solution. 4) We introduce and validate novel scaling laws specifically tailored
for sparse attention, providing evidence that our findings are likely to hold
true beyond our range of experiments. Through these insights, we demonstrate
that sparse attention is a key tool to enhance the capabilities of Transformer
LLMs for processing longer sequences, but requires careful evaluation of
trade-offs for performance-sensitive applications.

</details>


### [40] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
*Cong Qi,Hanzhang Fang,Tianxing Hu,Siqi Jiang,Wei Zhi*

Main category: cs.CL

TL;DR: GeneMamba是一个基于状态空间建模的高效单细胞转录组基础模型，通过Bi-Mamba架构实现线性复杂度，优于传统Transformer方法，并在多批次整合、细胞类型注释等任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据分析面临高维度、稀疏性和批次效应等挑战，现有Transformer模型因二次复杂度和长程依赖处理不佳而受限。

Method: 采用Bi-Mamba架构和状态空间建模，结合通路感知对比损失和基于排序的基因编码，模型在近3000万个细胞上进行了预训练。

Result: GeneMamba在多批次整合、细胞类型注释和基因-基因相关性任务中表现优异，兼具可解释性和鲁棒性。

Conclusion: GeneMamba为单细胞数据分析提供了高效且生物学基础扎实的解决方案，优于现有Transformer方法。

Abstract: Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of
cellular heterogeneity, but its complexity, which is marked by high
dimensionality, sparsity, and batch effects, which poses major computational
challenges. Transformer-based models have made significant advances in this
domain but are often limited by their quadratic complexity and suboptimal
handling of long-range dependencies. In this work, we introduce GeneMamba, a
scalable and efficient foundation model for single-cell transcriptomics built
on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba
captures bidirectional gene context with linear-time complexity, offering
substantial computational gains over transformer baselines. The model is
pretrained on nearly 30 million cells and incorporates biologically informed
objectives, including pathway-aware contrastive loss and rank-based gene
encoding. We evaluate GeneMamba across diverse tasks, including multi-batch
integration, cell type annotation, and gene-gene correlation, demonstrating
strong performance, interpretability, and robustness. These results position
GeneMamba as a practical and powerful alternative to transformer-based methods,
advancing the development of biologically grounded, scalable tools for
large-scale single-cell data analysis.

</details>


### [41] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
*Priyaranjan Pattnayak,Hitesh Laxmichand Patel,Amit Agarwal*

Main category: cs.CL

TL;DR: 研究比较了BPE、SentencePiece和字符级分词策略在低资源印度语言NER任务中的表现，发现SentencePiece在跨语言零样本设置中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨BPE在低资源印度语言NER任务中的适用性不足，尤其是处理形态复杂性时的局限性。

Method: 系统性比较BPE、SentencePiece和字符级分词策略在多种低资源印度语言（如阿萨姆语、孟加拉语）和极低资源语言（如桑塔利语）中的应用。

Result: SentencePiece在跨语言零样本设置中表现优于BPE，尤其在保持实体一致性和形态丰富性上更有效。

Conclusion: SentencePiece是多语言和低资源印度语言NER任务中更有效的分词策略。

Abstract: Tokenization is a critical component of Natural Language Processing (NLP),
especially for low resource languages, where subword segmentation influences
vocabulary structure and downstream task accuracy. Although Byte Pair Encoding
(BPE) is a standard tokenization method in multilingual language models, its
suitability for Named Entity Recognition (NER) in low resource Indic languages
remains underexplored due to its limitations in handling morphological
complexity. In this work, we systematically compare BPE, SentencePiece, and
Character Level tokenization strategies using IndicBERT for NER tasks in low
resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as
extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We
assess both intrinsic linguistic properties tokenization efficiency, out of
vocabulary (OOV) rates, and morphological preservation as well as extrinsic
downstream performance, including fine tuning and zero shot cross lingual
transfer.
  Our experiments show that SentencePiece is a consistently better performing
approach than BPE for NER in low resource Indic Languages, particularly in zero
shot cross lingual settings, as it better preserves entity consistency. While
BPE provides the most compact tokenization form, it is not capable of
generalization because it misclassifies or even fails to recognize entity
labels when tested on unseen languages. In contrast, SentencePiece constitutes
a better linguistic structural preservation model, benefiting extremely low
resource and morphologically rich Indic languages, such as Santali and
Manipuri, for superior entity recognition, as well as high generalization
across scripts, such as Sindhi, written in Arabic. The results point to
SentencePiece as the more effective tokenization strategy for NER within
multilingual and low resource Indic NLP applications.

</details>


### [42] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
*Luca Moroni,Giovanni Puccetti,Pere-Lluis Huguet Cabot,Andrei Stefan Bejgu,Edoardo Barba,Alessio Miaschi,Felice Dell'Orletta,Andrea Esuli,Roberto Navigli*

Main category: cs.CL

TL;DR: 论文提出了一种名为SAVA的新方法，通过词汇适应技术优化英文LLM以支持意大利语，显著降低了token数量并提升了模型效率。


<details>
  <summary>Details</summary>
Motivation: 当前大多数预训练大语言模型主要针对英语，对非英语语言支持不足，导致编码效率低和推理速度慢，因此需要优化这些模型以更好地支持其他语言。

Method: 提出SAVA方法，利用神经映射进行词汇替换，适配了Mistral-7b-v0.1和Llama-3.1-8B模型，并通过持续训练恢复性能。

Result: SAVA将token数量减少了25%，参数减少了10亿，并在多项下游任务中表现优异。

Conclusion: SAVA方法有效提升了非英语语言的处理效率，且通过有限的目标语言持续训练即可恢复模型性能。

Abstract: The number of pretrained Large Language Models (LLMs) is increasing steadily,
though the majority are designed predominantly for the English language. While
state-of-the-art LLMs can handle other languages, due to language contamination
or some degree of multilingual pretraining data, they are not optimized for
non-English languages, leading to inefficient encoding (high token "fertility")
and slower inference speed. In this work, we thoroughly compare a variety of
vocabulary adaptation techniques for optimizing English LLMs for the Italian
language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a
novel method that leverages neural mapping for vocabulary substitution. SAVA
achieves competitive performance across multiple downstream tasks, enhancing
grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing
token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and
reducing the number of parameters by 1 billion. We show that, following the
adaptation of the vocabulary, these models can recover their performance with a
relatively limited stage of continual training on the target language. Finally,
we test the capabilities of the adapted models on various multi-choice and
generative tasks.

</details>


### [43] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 论文提出了一个评估大型语言模型（LLM）政治信仰深度的新框架，通过分析论证一致性和不确定性量化，发现LLM的信仰稳定性是主题特定的，而非统一的意识形态立场。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLM的政治回应是否反映真实的内在信仰，还是仅仅与训练数据表面一致。

Method: 方法包括提出一个评估框架，分析12个LLM在19项经济政策上的回应，并通过支持和反对的论据测试其信仰稳定性。

Result: 结果显示，左倾和右倾模型的回应在挑战下分别有95%和89%保持一致性，语义熵能有效区分表面一致性和真实信仰（AUROC=0.78）。

Conclusion: 结论指出LLM并不具有稳定、类人的政治意识形态，强调在实际应用中需进行主题特定的可靠性评估。

Abstract: Large Language Models (LLMs) are increasingly shaping political discourse,
yet their responses often display inconsistency when subjected to scrutiny.
While prior research has primarily categorized LLM outputs as left- or
right-leaning to assess their political stances, a critical question remains:
Do these responses reflect genuine internal beliefs or merely surface-level
alignment with training data? To address this, we propose a novel framework for
evaluating belief depth by analyzing (1) argumentative consistency and (2)
uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from
the Political Compass Test, challenging their belief stability with both
supportive and opposing arguments. Our analysis reveals that LLMs exhibit
topic-specific belief stability rather than a uniform ideological stance.
Notably, up to 95% of left-leaning models' responses and 89% of right-leaning
models' responses remain consistent under the challenge, enabling semantic
entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing
between surface-level alignment from genuine belief. These findings call into
question the assumption that LLMs maintain stable, human-like political
ideologies, emphasizing the importance of conducting topic-specific reliability
assessments for real-world applications.

</details>


### [44] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
*Arjun Subramonian,Vagrant Gautam,Preethi Seshadri,Dietrich Klakow,Kai-Wei Chang,Yizhou Sun*

Main category: cs.CL

TL;DR: 该论文系统评估了现有衡量大语言模型（LLM）错误性别指代的方法是否具有一致性，发现概率评估与生成评估之间存在显著分歧，并指出自动评估无法全面捕捉人类判定的复杂行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于验证不同评估方法（概率与生成）对LLM错误性别指代的结果是否一致，填补了对这些方法收敛效力的研究空白。

Method: 通过将三个现有数据集转换为支持并行概率与生成评估的格式，系统评估了6个模型（来自3个模型家族），并结合2400次人工标注分析分歧点。

Result: 发现两种评估方法在实例、数据集和模型层面存在20.2%的分歧，且自动评估未能涵盖人类判定中的复杂行为（如超出代词范畴的错误）。

Conclusion: 结论指出当前自动评估设计存在局限，需改进以更贴近人类判断，同时对LLM评估中‘方法结果一致’的常规假设提出质疑。

Abstract: Numerous methods have been proposed to measure LLM misgendering, including
probability-based evaluations (e.g., automatically with templatic sentences)
and generation-based evaluations (e.g., with automatic heuristics or human
validation). However, it has gone unexamined whether these evaluation methods
have convergent validity, that is, whether their results align. Therefore, we
conduct a systematic meta-evaluation of these methods across three existing
datasets for LLM misgendering. We propose a method to transform each dataset to
enable parallel probability- and generation-based evaluation. Then, by
automatically evaluating a suite of 6 models from 3 families, we find that
these methods can disagree with each other at the instance, dataset, and model
levels, conflicting on 20.2% of evaluation instances. Finally, with a human
evaluation of 2400 LLM generations, we show that misgendering behaviour is
complex and goes far beyond pronouns, which automatic evaluations are not
currently designed to capture, suggesting essential disagreement with human
evaluations. Based on our findings, we provide recommendations for future
evaluations of LLM misgendering. Our results are also more widely relevant, as
they call into question broader methodological conventions in LLM evaluation,
which often assume that different evaluation methods agree.

</details>


### [45] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
*Rendi Chevi,Kentaro Inui,Thamar Solorio,Alham Fikri Aji*

Main category: cs.CL

TL;DR: LLM的语言风格（如权威性、确定性、表达清晰度或冗长度）可能影响用户偏好，但效果因用户个体差异而异。初步研究表明，虽然语言风格确实重要，但仍需更多样化和大规模的样本验证。


<details>
  <summary>Details</summary>
Motivation: 探索LLM交互中用户偏好的驱动因素，尤其是语言风格对偏好和潜在风险（如错误信息）的双重影响。

Method: 通过探索性和实验性用户研究，分析不同语言风格对用户偏好的影响，并结合个体特质进行调节分析。

Result: 语言风格确实影响用户偏好，但具体影响方式因用户群体和个体特质而异，样本局限性提示需进一步验证。

Conclusion: 语言风格是LLM交互体验的关键变量，未来需扩大样本并深入分析其与个体特质的联合效应及因果关系。

Abstract: What makes an interaction with the LLM more preferable for the user? While it
is intuitive to assume that information accuracy in the LLM's responses would
be one of the influential variables, recent studies have found that inaccurate
LLM's responses could still be preferable when they are perceived to be more
authoritative, certain, well-articulated, or simply verbose. These variables
interestingly fall under the broader category of language style, implying that
the style in the LLM's responses might meaningfully influence users'
preferences. This hypothesized dynamic could have double-edged consequences:
enhancing the overall user experience while simultaneously increasing their
susceptibility to risks such as LLM's misinformation or hallucinations. In this
short paper, we present our preliminary studies in exploring this subject.
Through a series of exploratory and experimental user studies, we found that
LLM's language style does indeed influence user's preferences, but how and
which language styles influence the preference varied across different user
populations, and more interestingly, moderated by the user's very own
individual traits. As a preliminary work, the findings in our studies should be
interpreted with caution, particularly given the limitations in our samples,
which still need wider demographic diversity and larger sample sizes. Our
future directions will first aim to address these limitations, which would
enable a more comprehensive joint effect analysis between the language style,
individual traits, and preferences, and further investigate the potential
causal relationship between and beyond these variables.

</details>


### [46] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
*Seunghyun Yoo*

Main category: cs.CL

TL;DR: Interactive Chain-of-Thought Framework提升AI解释性和负责使用，通过模块化和可编辑推理过程增强用户认知参与，支持多样认知风格。


<details>
  <summary>Details</summary>
Motivation: 解决短内容泛滥和AI快速普及导致用户深度思考减少的问题，提升批判性思维和对AI输出推理的理解。

Method: 提出交互式推理链框架，将推理分解为可检查、修改和重执行的模块，结合轻量级编辑适应机制和伦理透明度设计。

Result: 框架实现用户主动参与推理、适应多样需求，并通过元数据披露和隐私保护确保伦理合规。

Conclusion: 该设计促进AI系统中的批判性互动、负责使用及包容性适应，适用于复杂社会挑战场景。

Abstract: Due to the proliferation of short-form content and the rapid adoption of AI,
opportunities for deep, reflective thinking have significantly diminished,
undermining users' critical thinking and reducing engagement with the reasoning
behind AI-generated outputs. To address this issue, we propose an Interactive
Chain-of-Thought (CoT) Framework that enhances human-centered explainability
and responsible AI usage by making the model's inference process transparent,
modular, and user-editable. The framework decomposes reasoning into clearly
defined blocks that users can inspect, modify, and re-execute, encouraging
active cognitive engagement rather than passive consumption. It further
integrates a lightweight edit-adaptation mechanism inspired by preference
learning, allowing the system to align with diverse cognitive styles and user
intentions. Ethical transparency is ensured through explicit metadata
disclosure, built-in bias checkpoint functionality, and privacy-preserving
safeguards. This work outlines the design principles and architecture necessary
to promote critical engagement, responsible interaction, and inclusive
adaptation in AI systems aimed at addressing complex societal challenges.

</details>


### [47] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
*Muskan Garg,Shaina Raza,Shebuti Rayana,Xingyi Liu,Sunghwan Sohn*

Main category: cs.CL

TL;DR: 论文探讨了在医疗保健领域中小型语言模型（SLMs）的潜力，提出了分类框架，并展示了SLMs在资源受限环境中的高效表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医疗应用中有显著进展，但数据隐私和资源限制问题促使对小型语言模型（SLMs）的研究，以提供可扩展且临床可行的解决方案。

Method: 通过分类框架分析SLMs的三个维度：NLP任务、利益相关者角色和护理连续性，并探讨了构建模型的架构基础、通过提示和微调适应临床需求的技术。

Result: 展示了SLMs在多种NLP任务中的实验成果，强调了其在医疗领域的变革潜力，并提供了更新的资源库。

Conclusion: SLMs为资源受限的医疗环境提供了高效解决方案，未来研究可通过模型优化进一步推动该领域发展。

Abstract: Despite substantial progress in healthcare applications driven by large
language models (LLMs), growing concerns around data privacy, and limited
resources; the small language models (SLMs) offer a scalable and clinically
viable solution for efficient performance in resource-constrained environments
for next-generation healthcare informatics. Our comprehensive survey presents a
taxonomic framework to identify and categorize them for healthcare
professionals and informaticians. The timeline of healthcare SLM contributions
establishes a foundational framework for analyzing models across three
dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present
a taxonomic framework to identify the architectural foundations for building
models from scratch; adapting SLMs to clinical precision through prompting,
instruction fine-tuning, and reasoning; and accessibility and sustainability
through compression techniques. Our primary objective is to offer a
comprehensive survey for healthcare professionals, introducing recent
innovations in model optimization and equipping them with curated resources to
support future research and development in the field. Aiming to showcase the
groundbreaking advancements in SLMs for healthcare, we present a comprehensive
compilation of experimental results across widely studied NLP tasks in
healthcare to highlight the transformative potential of SLMs in healthcare. The
updated repository is available at Github

</details>


### [48] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
*Hannah Cyberey,David Evans*

Main category: cs.CL

TL;DR: 研究通过表征工程方法分析LLMs的"审查"机制，发现拒绝-服从向量并控制输出审查程度，同时揭示思维抑制现象。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs如何通过安全调优实现"审查"，并探究其机制。

Method: 使用表征工程技术识别拒绝-服从向量，分析思维抑制维度。

Result: 成功找到控制审查水平的向量，并通过负向量移除审查。

Conclusion: 表征工程可有效解码LLMs的审查机制，为模型透明度提供新方法。

Abstract: Large language models (LLMs) have transformed the way we access information.
These models are often tuned to refuse to comply with requests that are
considered harmful and to produce responses that better align with the
preferences of those who control the models. To understand how this
"censorship" works. We use representation engineering techniques to study
open-weights safety-tuned models. We present a method for finding a
refusal--compliance vector that detects and controls the level of censorship in
model outputs. We also analyze recent reasoning LLMs, distilled from
DeepSeek-R1, and uncover an additional dimension of censorship through "thought
suppression". We show a similar approach can be used to find a vector that
suppresses the model's reasoning process, allowing us to remove censorship by
applying the negative multiples of this vector

</details>


### [49] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
*Chanhee Park,Hyeonseok Moon,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 介绍了MIRAGE，一个专为RAG评估设计的数据集，包含7560个实例和37800个检索条目，用于精确评估检索和生成任务，并提出新的评估指标以衡量RAG适应性。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统评估的挑战，填补现有基准在组件特异性评估上的不足。

Method: 构建MIRAGE数据集，设计新评估指标（噪声脆弱性、上下文可接受性等），并进行多组实验分析不同retriever-LLM配置的效果。

Result: MIRAGE能高效评估RAG系统的检索和生成能力，实验揭示了模型对对齐的优化建议和RAG系统内部的细微动态。

Conclusion: MIRAGE为RAG系统提供了标准化、可定制的评估工具，有助于推进相关研究。

Abstract: Retrieval-Augmented Generation (RAG) has gained prominence as an effective
method for enhancing the generative capabilities of Large Language Models
(LLMs) through the incorporation of external knowledge. However, the evaluation
of RAG systems remains a challenge, due to the intricate interplay between
retrieval and generation components. This limitation has resulted in a scarcity
of benchmarks that facilitate a detailed, component-specific assessment. In
this work, we present MIRAGE, a Question Answering dataset specifically
designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped
to a retrieval pool of 37,800 entries, enabling an efficient and precise
evaluation of both retrieval and generation tasks. We also introduce novel
evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions
such as noise vulnerability, context acceptability, context insensitivity, and
context misinterpretation. Through comprehensive experiments across various
retriever-LLM configurations, we provide new insights into the optimal
alignment of model pairs and the nuanced dynamics within RAG systems. The
dataset and evaluation code are publicly available, allowing for seamless
integration and customization in diverse research settings\footnote{The MIRAGE
code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


### [50] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
*Minju Seo,Jinheon Baek,Seongyun Lee,Sung Ju Hwang*

Main category: cs.CL

TL;DR: PaperCoder是一个基于多智能体LLM的框架，旨在将机器学习论文转化为功能性代码仓库，通过规划、分析和生成三个阶段实现，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习研究中代码实现常缺失的问题，利用大语言模型的理解和代码生成能力，提高研究复现和后续开发的效率。

Method: PaperCoder采用三阶段流程：规划（架构设计与配置）、分析（实现细节解读）、生成（模块化代码生成），并由多个专用智能体协作完成。

Result: 在模型和人类评估中，PaperCoder生成的代码质量高、忠实于原文，并在PaperBench基准测试中显著优于基线方法。

Conclusion: PaperCoder通过多智能体LLM框架有效实现了论文到代码的转化，证明了其在提升研究复现效率方面的潜力。

Abstract: Despite the rapid growth of machine learning research, corresponding code
implementations are often unavailable, making it slow and labor-intensive for
researchers to reproduce results and build upon prior work. In the meantime,
recent Large Language Models (LLMs) excel at understanding scientific documents
and generating high-quality code. Inspired by this, we introduce PaperCoder, a
multi-agent LLM framework that transforms machine learning papers into
functional code repositories. PaperCoder operates in three stages: planning,
where it constructs a high-level roadmap, designs the system architecture with
diagrams, identifies file dependencies, and generates configuration files;
analysis, which focuses on interpreting implementation-specific details; and
generation, where modular, dependency-aware code is produced. Moreover, each
phase is instantiated through a set of specialized agents designed to
collaborate effectively across the pipeline. We then evaluate PaperCoder on
generating code implementations from machine learning papers based on both
model-based and human evaluations, specifically from the original paper
authors, with author-released repositories as ground truth if available. Our
results demonstrate the effectiveness of PaperCoder in creating high-quality,
faithful implementations. Furthermore, it consistently shows strengths in the
recently released PaperBench benchmark, surpassing strong baselines by
substantial margins.

</details>


### [51] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
*Yangxinyu Xie,Bowen Jiang,Tanwi Mallick,Joshua David Bergerson,John K. Hutchison,Duane R. Verner,Jordan Branham,M. Ross Alexander,Robert B. Ross,Yan Feng,Leslie-Anne Levy,Weijie Su,Camillo J. Taylor*

Main category: cs.CL

TL;DR: 该论文提出了一种基于检索增强生成（RAG）的多智能体LLM系统WildfireGPT，专注于野火灾害，通过整合多种数据源提升决策支持的准确性和情境相关性，显著优于现有LLM解决方案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽具变革性能力，但在需要专业知识的领域（如自然灾害）中常缺乏情境特定信息。为弥补这一缺陷，研究旨在设计一个能支持自然灾害决策的系统。

Method: 采用基于RAG的多智能体LLM架构，整合自然灾害与极端天气数据、观测数据集和科学文献，开发专注于野火的WildfireGPT系统，以用户为中心提供定制化风险洞察。

Result: 通过10个专家主导的案例研究验证，WildfireGPT在决策支持方面显著优于现有LLM方案，证明了其信息准确性和情境相关性的优势。

Conclusion: WildfireGPT展示了多智能体RAG-LLM系统在专业领域的潜力，为自然灾害决策提供了更精准、可靠的解决方案。

Abstract: Large language models (LLMs) are a transformational capability at the
frontier of artificial intelligence and machine learning that can support
decision-makers in addressing pressing societal challenges such as extreme
natural hazard events. As generalized models, LLMs often struggle to provide
context-specific information, particularly in areas requiring specialized
knowledge. In this work we propose a retrieval-augmented generation (RAG)-based
multi-agent LLM system to support analysis and decision-making in the context
of natural hazards and extreme weather events. As a proof of concept, we
present WildfireGPT, a specialized system focused on wildfire hazards. The
architecture employs a user-centered, multi-agent design to deliver tailored
risk insights across diverse stakeholder groups. By integrating natural hazard
and extreme weather projection data, observational datasets, and scientific
literature through an RAG framework, the system ensures both the accuracy and
contextual relevance of the information it provides. Evaluation across ten
expert-led case studies demonstrates that WildfireGPT significantly outperforms
existing LLM-based solutions for decision support.

</details>


### [52] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
*Kaidong Feng,Zhu Sun,Jie Yang,Hui Fang,Xinghua Qu,Wenyuan Liu*

Main category: cs.CL

TL;DR: 该论文研究了知识蒸馏在捆绑生成中的应用，探讨知识格式、数量及利用方法对性能的影响，并提出一个综合框架以提升效率。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型的计算成本高，知识蒸馏被用于将大模型的知识迁移到小模型，以降低计算需求同时保持性能。

Method: 提出一个综合知识蒸馏框架，逐步提取知识（模式、规则、深层思维），通过不同策略捕获不同数量的知识，并利用适应技术（上下文学习、监督微调等）在小模型中应用这些知识。

Result: 实验表明知识格式、数量及利用方法共同影响捆绑生成性能，证明知识蒸馏在高效LLM应用中的潜力。

Conclusion: 知识蒸馏能有效降低计算成本并保持性能，为LLM在捆绑生成中的应用提供了高效解决方案。

Abstract: LLMs are increasingly explored for bundle generation, thanks to their
reasoning capabilities and knowledge. However, deploying large-scale LLMs
introduces significant efficiency challenges, primarily high computational
costs during fine-tuning and inference due to their massive parameterization.
Knowledge distillation (KD) offers a promising solution, transferring expertise
from large teacher models to compact student models. This study systematically
investigates knowledge distillation approaches for bundle generation, aiming to
minimize computational demands while preserving performance. We explore three
critical research questions: (1) how does the format of KD impact bundle
generation performance? (2) to what extent does the quantity of distilled
knowledge influence performance? and (3) how do different ways of utilizing the
distilled knowledge affect performance? We propose a comprehensive KD framework
that (i) progressively extracts knowledge (patterns, rules, deep thoughts);
(ii) captures varying quantities of distilled knowledge through different
strategies; and (iii) exploits complementary LLM adaptation techniques
(in-context learning, supervised fine-tuning, combination) to leverage
distilled knowledge in small student models for domain-specific adaptation and
enhanced efficiency. Extensive experiments provide valuable insights into how
knowledge format, quantity, and utilization methodologies collectively shape
LLM-based bundle generation performance, exhibiting KD's significant potential
for more efficient yet effective LLM-based bundle generation.

</details>


### [53] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
*Jinfeng Zhou,Yuxuan Chen,Jianing Yin,Yongkang Huang,Yihan Shi,Xikun Zhang,Libiao Peng,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 论文提出CRDial框架，通过多轮对话实现认知重构（CR），克服了现有方法的局限性，并基于该框架构建了高质量的对话数据集Crisp，训练了高效的对话模型Crispers。


<details>
  <summary>Details</summary>
Motivation: 面对心理医生短缺和社交污名，需要开发人机交互的心理治疗工具。现有CR方法过于简化，无法有效模拟心理治疗过程，亟需改进。

Method: 提出CRDial框架，设计多轮对话的识别与重构阶段，整合支持性对话策略，采用多通道循环机制迭代优化CR。基于此框架，从LLM中蒸馏出双语数据集Crisp，并训练7B和14B规模的Crispers模型。

Result: 人工评估显示Crispers在点对点、成对和干预评估中表现优越。

Conclusion: CRDial为认知重构提供了有效的人机交互解决方案，Crispers模型在实用性和效果上具有显著优势，推动了心理治疗工具的发展。

Abstract: Cognitive Restructuring (CR) is a psychotherapeutic process aimed at
identifying and restructuring an individual's negative thoughts, arising from
mental health challenges, into more helpful and positive ones via multi-turn
dialogues. Clinician shortage and stigma urge the development of human-LLM
interactive psychotherapy for CR. Yet, existing efforts implement CR via simple
text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to
align with the psychotherapeutic process for effective CR. To address this gap,
we propose CRDial, a novel framework for CR, which creates multi-turn dialogues
with specifically designed identification and restructuring stages of negative
thoughts, integrates sentence-level supportive conversation strategies, and
adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we
distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from
LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and
14B scales. Extensive human studies show the superiority of Crispers in
pointwise, pairwise, and intervention evaluations.

</details>


### [54] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
*Ocheme Anthony Ekle,Biswarup Das*

Main category: cs.CL

TL;DR: 该研究开发了基于神经机器翻译（NMT）和Transformer的迁移学习模型，用于英语到伊博语的翻译，并取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决伊博语这种低资源非洲语言在机器翻译任务中的性能差距。

Method: 结合RNN架构（如LSTM和GRU）与注意力机制，并使用MarianNMT预训练模型进行迁移学习。

Result: RNN系统达到与现有基准相当的结果，迁移学习使BLEU分数提升4.83，翻译准确率估计达70%。

Conclusion: 结合RNN与迁移学习能有效提升低资源语言翻译任务的表现。

Abstract: In this study, we develop Neural Machine Translation (NMT) and
Transformer-based transfer learning models for English-to-Igbo translation - a
low-resource African language spoken by over 40 million people across Nigeria
and West Africa. Our models are trained on a curated and benchmarked dataset
compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,
all verified by native language experts. We leverage Recurrent Neural Network
(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated
Recurrent Units (GRU), enhanced with attention mechanisms to improve
translation accuracy. To further enhance performance, we apply transfer
learning using MarianNMT pre-trained models within the SimpleTransformers
framework. Our RNN-based system achieves competitive results, closely matching
existing English-Igbo benchmarks. With transfer learning, we observe a
performance gain of +4.83 BLEU points, reaching an estimated translation
accuracy of 70%. These findings highlight the effectiveness of combining RNNs
with transfer learning to address the performance gap in low-resource language
translation tasks.

</details>


### [55] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
*Zhaolu Kang,Hongtian Cai,Xiangyang Ji,Jinzhe Li,Nanfei Gu*

Main category: cs.CL

TL;DR: 论文提出JurisCTC模型，用于提升法律判决预测（LJP）任务的准确性，通过无监督域适应（UDA）和对比学习实现跨法律领域知识迁移，在民事和刑事领域取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前无监督域适应（UDA）在自然语言处理（NLP）中应用广泛，但在法律领域的跨领域知识迁移研究较少。法律文本长且复杂，标注数据集有限，因此需要一种有效方法提升法律判决预测的准确性。

Method: 提出JurisCTC模型，利用对比学习区分不同领域的样本，实现民事和刑事法律领域间的知识迁移。

Result: JurisCTC在LJP任务中表现优异，最高准确率分别达到76.59%和78.83%，优于现有模型和特定大语言模型（LLMs）。

Conclusion: JurisCTC通过跨领域知识迁移和对比学习，有效提升了法律判决预测的准确性，为法律领域的无监督域适应提供了新思路。

Abstract: In recent years, Unsupervised Domain Adaptation (UDA) has gained significant
attention in the field of Natural Language Processing (NLP) owing to its
ability to enhance model generalization across diverse domains. However, its
application for knowledge transfer between distinct legal domains remains
largely unexplored. To address the challenges posed by lengthy and complex
legal texts and the limited availability of large-scale annotated datasets, we
propose JurisCTC, a novel model designed to improve the accuracy of Legal
Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC
facilitates effective knowledge transfer across various legal domains and
employs contrastive learning to distinguish samples from different domains.
Specifically, for the LJP task, we enable knowledge transfer between civil and
criminal law domains. Compared to other models and specific large language
models (LLMs), JurisCTC demonstrates notable advancements, achieving peak
accuracies of 76.59% and 78.83%, respectively.

</details>


### [56] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
*Xiuying Chen,Tairan Wang,Juexiao Zhou,Zirui Song,Xin Gao,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 该论文研究了医疗领域中文本生成的公平性问题，发现AI模型在不同种族、性别和年龄组中存在性能差异，并提出了一种选择性优化算法来减少这些偏差，同时保持整体性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗领域的广泛应用，系统可能反映并放大人类偏见，尤其是在历史上服务不足的群体中。虽然医疗影像分类中的公平性问题已有较多研究，但文本生成领域的公平性问题仍较少被探索。

Method: 提出了一种算法，通过选择性优化表现不佳的群体来减少偏差。该算法综合考虑了词级准确率和病理准确性，并确保整个过程完全可微以进行有效训练。

Result: 在多种骨干模型、数据集和模态上的评估表明，该算法显著减少了不同群体间的性能差异（降幅超过30%），同时文本生成的准确性变化通常控制在2%以内。

Conclusion: 通过减少深度学习模型生成的偏见，该方法有望提升医疗文本生成诊断的公平性和可靠性。

Abstract: Artificial intelligence (AI) systems, particularly those based on deep
learning models, have increasingly achieved expert-level performance in medical
applications. However, there is growing concern that such AI systems may
reflect and amplify human bias, and reduce the quality of their performance in
historically under-served populations. The fairness issue has attracted
considerable research interest in the medical imaging classification field, yet
it remains understudied in the text generation domain. In this study, we
investigate the fairness problem in text generation within the medical field
and observe significant performance discrepancies across different races,
sexes, and age groups, including intersectional groups, various model scales,
and different evaluation metrics. To mitigate this fairness issue, we propose
an algorithm that selectively optimizes those underperformed groups to reduce
bias. The selection rules take into account not only word-level accuracy but
also the pathology accuracy to the target reference, while ensuring that the
entire process remains fully differentiable for effective model training. Our
evaluations across multiple backbones, datasets, and modalities demonstrate
that our proposed algorithm enhances fairness in text generation without
compromising overall performance. Specifically, the disparities among various
groups across different metrics were diminished by more than 30% with our
algorithm, while the relative change in text generation accuracy was typically
within 2%. By reducing the bias generated by deep learning models, our proposed
approach can potentially alleviate concerns about the fairness and reliability
of text generation diagnosis in medical domain.
  Our code is publicly available to facilitate further research at
https://github.com/iriscxy/GenFair.

</details>


### [57] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
*Junyan Zhang,Shuliang Liu,Aiwei Liu,Yubo Gao,Jungang Li,Xiaojie Gu,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoheMark的句子级水印技术，通过利用句子间的连贯关系来提升逻辑流畅性，同时保持水印检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有句子级水印技术依赖随机分割或生成过程，可能限制适用句子并影响生成内容质量，因此需平衡文本质量与水印检测鲁棒性。

Method: CoheMark采用训练后的模糊C均值聚类筛选句子，并结合特定的下一句选择标准，确保逻辑连贯性。

Result: 实验证明CoheMark在保持高水印强度的同时，对文本质量影响极小。

Conclusion: CoheMark有效解决了传统水印技术中质量与鲁棒性的矛盾，为模型生成内容的追踪提供了更佳方案。

Abstract: Watermarking technology is a method used to trace the usage of content
generated by large language models. Sentence-level watermarking aids in
preserving the semantic integrity within individual sentences while maintaining
greater robustness. However, many existing sentence-level watermarking
techniques depend on arbitrary segmentation or generation processes to embed
watermarks, which can limit the availability of appropriate sentences. This
limitation, in turn, compromises the quality of the generated response. To
address the challenge of balancing high text quality with robust watermark
detection, we propose CoheMark, an advanced sentence-level watermarking
technique that exploits the cohesive relationships between sentences for better
logical fluency. The core methodology of CoheMark involves selecting sentences
through trained fuzzy c-means clustering and applying specific next sentence
selection criteria. Experimental evaluations demonstrate that CoheMark achieves
strong watermark strength while exerting minimal impact on text quality.

</details>


### [58] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
*Yulia Otmakhova,Hung Thinh Truong,Rahmad Mahendra,Zenan Zhai,Rongxin Zhu,Daniel Beck,Jey Han Lau*

Main category: cs.CL

TL;DR: FLUKE是一个任务无关的框架，通过系统性地微调测试数据评估模型鲁棒性，揭示了语言变异对任务的依赖性、LLM的脆弱性以及否定修改对模型的普遍影响。


<details>
  <summary>Details</summary>
Motivation: 现有的模型鲁棒性评估方法缺乏系统性和任务无关性，FLUKE旨在填补这一空白。

Method: FLUKE通过控制语言层面的变体（从拼写到方言和风格）生成测试数据，并利用LLM和人工验证。

Result: 语言变异对任务影响各异，LLM整体鲁棒性较强但仍存在脆弱性，所有模型对否定修改普遍敏感。

Conclusion: 系统鲁棒性测试对理解模型行为至关重要，FLUKE提供了有效的评估工具。

Abstract: We present FLUKE (Framework for LingUistically-driven and tasK-agnostic
robustness Evaluation), a task-agnostic framework for assessing model
robustness through systematic minimal variations of test data. FLUKE introduces
controlled variations across linguistic levels - from orthography to dialect
and style varieties - and leverages large language models (LLMs) with human
validation to generate modifications. We demonstrate FLUKE's utility by
evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and
reveal that (1) the impact of linguistic variations is highly task-dependent,
with some tests being critical for certain tasks but irrelevant for others; (2)
while LLMs have better overall robustness compared to fine-tuned models, they
still exhibit significant brittleness to certain linguistic variations; (3) all
models show substantial vulnerability to negation modifications across most
tasks. These findings highlight the importance of systematic robustness testing
for understanding model behaviors.

</details>


### [59] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
*Zihan Wang,Lu Yuan,Zhengxuan Zhang,Qing Zhao*

Main category: cs.CL

TL;DR: 该论文提出了双方面共情框架(DAE)，结合认知和情感共情来从创建者和读者角度分析虚假信息，并通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息检测方法主要关注表面特征，忽略了人类共情在传播过程中的关键作用，因此需要一种更全面、以人为本的方法。

Method: 提出了双方面共情框架(DAE)，整合认知和情感共情，分析创建者的认知策略和情感诉求，并利用大语言模型模拟读者的认知判断和情感响应。还引入了共情感知过滤机制以提高响应真实性和多样性。

Result: 在基准数据集上的实验结果表明，DAE优于现有方法。

Conclusion: DAE为多模态虚假信息检测提供了一种新范式，结合了人类共情的深度分析。

Abstract: In the digital era, social media has become a major conduit for information
dissemination, yet it also facilitates the rapid spread of misinformation.
Traditional misinformation detection methods primarily focus on surface-level
features, overlooking the crucial roles of human empathy in the propagation
process. To address this gap, we propose the Dual-Aspect Empathy Framework
(DAE), which integrates cognitive and emotional empathy to analyze
misinformation from both the creator and reader perspectives. By examining
creators' cognitive strategies and emotional appeals, as well as simulating
readers' cognitive judgments and emotional responses using Large Language
Models (LLMs), DAE offers a more comprehensive and human-centric approach to
misinformation detection. Moreover, we further introduce an empathy-aware
filtering mechanism to enhance response authenticity and diversity.
Experimental results on benchmark datasets demonstrate that DAE outperforms
existing methods, providing a novel paradigm for multimodal misinformation
detection.

</details>


### [60] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
*Chengguang Gan,Sunbowen Lee,Zhixi Cai,Yanbin Wei,Lei Zheng,Yunhao Liang,Shiwen Ni,Tatsunori Mori*

Main category: cs.CL

TL;DR: 该论文首次将互增强效应（MRE）扩展到多模态信息提取领域，提出了多模态互增强效应（M-MRE）任务，并构建了相应数据集和Prompt Format Adapter（PFA）方法。实验证明MRE在多模态场景中同样有效，验证了其跨领域通用性。


<details>
  <summary>Details</summary>
Motivation: 尽管互增强效应（MRE）在文本领域已被探索和验证，但其在视觉和多模态领域的适用性尚未研究。本文旨在将MRE扩展至多模态信息提取领域，探索其在跨模态任务中的潜力。

Method: 提出多模态互增强效应（M-MRE）任务，并构建相应的数据集。为解决M-MRE的挑战，设计了完全兼容多种大型视觉语言模型（LVLMs）的Prompt Format Adapter（PFA）。

Result: 实验结果表明，在多模态文本-图像理解场景中，MRE效应仍然存在。跨三个相关任务的相互增益验证了MRE在非文本领域的通用性。

Conclusion: MRE不仅在文本领域有效，也能在多模态任务中实现互增强，为其跨领域应用提供了理论支持和实践验证。

Abstract: Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection
of information extraction and model interpretability. MRE aims to leverage the
mutual understanding between tasks of different granularities, enhancing the
performance of both coarse-grained and fine-grained tasks through joint
modeling. While MRE has been explored and validated in the textual domain, its
applicability to visual and multimodal domains remains unexplored. In this
work, we extend MRE to the multimodal information extraction domain for the
first time. Specifically, we introduce a new task: Multimodal Mutual
Reinforcement Effect (M-MRE), and construct a corresponding dataset to support
this task. To address the challenges posed by M-MRE, we further propose a
Prompt Format Adapter (PFA) that is fully compatible with various Large
Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can
also be observed in the M-MRE task, a multimodal text-image understanding
scenario. This provides strong evidence that MRE facilitates mutual gains
across three interrelated tasks, confirming its generalizability beyond the
textual domain.

</details>


### [61] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
*Jose G. Moreno,Jesus Lovon,M'Rick Robin-Charlet,Christine Damase-Michel,Lynda Tamine*

Main category: cs.CL

TL;DR: 提出了一个名为PatientDx的框架，通过合并预训练的大型语言模型（LLMs）来优化医疗预测任务，避免了敏感患者数据的微调需求，提升了数据隐私性。


<details>
  <summary>Details</summary>
Motivation: 当前的微调方法依赖大量敏感数据，尤其在医疗领域可能引发隐私问题。

Method: 基于模型合并技术，设计了优化策略，通过不训练LLM的方式调整超参数。

Result: 在MIMIC-IV数据集上，AUROC指标提升高达7%，且更不易发生数据泄露。

Conclusion: PatientDx既保护数据隐私，又保持了性能，适用于医疗领域。

Abstract: Fine-tuning of Large Language Models (LLMs) has become the default practice
for improving model performance on a given task. However, performance
improvement comes at the cost of training on vast amounts of annotated data
which could be sensitive leading to significant data privacy concerns. In
particular, the healthcare domain is one of the most sensitive domains exposed
to data privacy issues. In this paper, we present PatientDx, a framework of
model merging that allows the design of effective LLMs for health-predictive
tasks without requiring fine-tuning nor adaptation on patient data. Our
proposal is based on recently proposed techniques known as merging of LLMs and
aims to optimize a building block merging strategy. PatientDx uses a pivotal
model adapted to numerical reasoning and tunes hyperparameters on examples
based on a performance metric but without training of the LLM on these data.
Experiments using the mortality tasks of the MIMIC-IV dataset show improvements
up to 7% in terms of AUROC when compared to initial models. Additionally, we
confirm that when compared to fine-tuned models, our proposal is less prone to
data leak problems without hurting performance. Finally, we qualitatively show
the capabilities of our proposal through a case study. Our best model is
publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>


### [62] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
*Yongxuan Wu,Runyu Chen,Peiyu Liu,Hongjin Qian*

Main category: cs.CL

TL;DR: 论文构建了首个基于实时流媒体的长文本口语数据集，评估了现有大语言模型和专业方法在冗余和对话性强的长文本理解上的表现，发现现有方法存在任务偏好且无法处理高冗余输入，并提出了一种新基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言处理长文本理解基准未能反映真实对话的高冗余和信息密度不均的特点，限制了模型在实际场景中的应用。

Method: 构建了基于实时流媒体数据的长文本口语数据集，并设计了检索依赖、推理依赖和混合三类任务，评估了大语言模型和专业方法的性能。

Result: 现有方法在任务中存在明显偏好，对高冗余输入表现不佳；提出的新基线方法在跨任务中表现优异。

Conclusion: 研究揭示了当前方法的局限性，为长文本口语理解提供了新的评估基准和未来改进方向。

Abstract: Long-context understanding poses significant challenges in natural language
processing, particularly for real-world dialogues characterized by speech-based
elements, high redundancy, and uneven information density. Although large
language models (LLMs) achieve impressive results on existing benchmarks, these
datasets fail to reflect the complexities of such texts, limiting their
applicability to practical scenarios. To bridge this gap, we construct the
first spoken long-text dataset, derived from live streams, designed to reflect
the redundancy-rich and conversational nature of real-world scenarios. We
construct tasks in three categories: retrieval-dependent, reasoning-dependent,
and hybrid. We then evaluate both popular LLMs and specialized methods to
assess their ability to understand long-contexts in these tasks. Our results
show that current methods exhibit strong task-specific preferences and perform
poorly on highly redundant inputs, with no single method consistently
outperforming others. We propose a new baseline that better handles redundancy
in spoken text and achieves strong performance across tasks. Our findings
highlight key limitations of current methods and suggest future directions for
improving long-context understanding. Finally, our benchmark fills a gap in
evaluating long-context spoken language understanding and provides a practical
foundation for developing real-world e-commerce systems. The code and benchmark
are available at https://github.com/Yarayx/livelongbench.

</details>


### [63] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
*Jihyun Lee,Yejin Jeon,Seungyeon Seo,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 论文提出了PicPersona-TOD数据集和Pictor模型，通过用户图像实现个性化对话，提升任务导向对话系统的用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有的任务导向对话系统往往生成通用、单调的响应，缺乏个性化和对用户属性的适应能力。

Method: 引入PicPersona-TOD数据集，结合用户图像和外部知识，使用第一印象和对话策略提示生成个性化响应；提出Pictor模型，适应未见领域。

Result: 人类评估证实个性化响应提升了用户体验；Pictor模型在未见领域表现稳健。

Conclusion: PicPersona-TOD和Pictor模型为个性化任务导向对话提供了有效解决方案。

Abstract: Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests
through natural language interactions, yet existing systems often produce
generic, monotonic responses that lack individuality and fail to adapt to
users' personal attributes. To address this, we introduce PicPersona-TOD, a
novel dataset that incorporates user images as part of the persona, enabling
personalized responses tailored to user-specific factors such as age or
emotional context. This is facilitated by first impressions, dialogue
policy-guided prompting, and the use of external knowledge to reduce
hallucinations. Human evaluations confirm that our dataset enhances user
experience, with personalized responses contributing to a more engaging
interaction. Additionally, we introduce a new NLG model, Pictor, which not only
personalizes responses, but also demonstrates robust performance across unseen
domains https://github.com/JihyunLee1/PicPersona.

</details>


### [64] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
*Anna Lieb,Maneesh Arora,Eni Mustafaraj*

Main category: cs.CL

TL;DR: 论文探讨了利用LLM生成的文本增强来提升主题模型输出的实用性和可解释性，通过政治学案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型在可解释性和针对特定社会科学研究问题的实用性方面存在局限，研究旨在通过LLM增强文本改善这一问题。

Method: 使用GPT-4生成的文本增强技术，结合政治学案例研究，优化主题模型的输出。

Result: 实验表明，结合GPT-4增强的主题模型能生成高度可解释的类别，适用于特定领域研究，且需极少人工干预。

Conclusion: LLM增强显著提升了主题模型在社会科学研究中的实用性和可解释性，为领域特定问题提供了高效解决方案。

Abstract: Unsupervised machine learning techniques, such as topic modeling and
clustering, are often used to identify latent patterns in unstructured text
data in fields such as political science and sociology. These methods overcome
common concerns about reproducibility and costliness involved in the
labor-intensive process of human qualitative analysis. However, two major
limitations of topic models are their interpretability and their practicality
for answering targeted, domain-specific social science research questions. In
this work, we investigate opportunities for using LLM-generated text
augmentation to improve the usefulness of topic modeling output. We use a
political science case study to evaluate our results in a domain-specific
application, and find that topic modeling using GPT-4 augmentations creates
highly interpretable categories that can be used to investigate domain-specific
research questions with minimal human guidance.

</details>


### [65] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
*Xin Yi,Shunfan Zhengc,Linlin Wanga,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 该论文研究了LLMs中的水印继承问题，提出了一种名为CDG-KD的统一框架，支持双向攻击（擦除和伪造），以验证水印的鲁棒性和不可伪造性。


<details>
  <summary>Details</summary>
Motivation: 水印在LLMs中对于打击误导信息和保护知识产权至关重要，但现有方法无法同时支持擦除和伪造攻击，且水印在未经授权的知识蒸馏中的鲁棒性未被充分探索。

Method: 提出了CDG-KD框架，通过对比解码提取水印文本，再通过双向蒸馏训练学生模型以支持水印擦除和伪造。

Result: 实验表明CDG-KD能有效执行攻击，同时保持蒸馏模型的通用性能。

Conclusion: 研究强调了开发具有鲁棒性和不可伪造性的水印方案的紧迫性。

Abstract: Watermarking has emerged as a critical technique for combating misinformation
and protecting intellectual property in large language models (LLMs). A recent
discovery, termed watermark radioactivity, reveals that watermarks embedded in
teacher models can be inherited by student models through knowledge
distillation. On the positive side, this inheritance allows for the detection
of unauthorized knowledge distillation by identifying watermark traces in
student models. However, the robustness of watermarks against scrubbing attacks
and their unforgeability in the face of spoofing attacks under unauthorized
knowledge distillation remain largely unexplored. Existing watermark attack
methods either assume access to model internals or fail to simultaneously
support both scrubbing and spoofing attacks. In this work, we propose
Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified
framework that enables bidirectional attacks under unauthorized knowledge
distillation. Our approach employs contrastive decoding to extract corrupted or
amplified watermark texts via comparing outputs from the student model and
weakly watermarked references, followed by bidirectional distillation to train
new student models capable of watermark removal and watermark forgery,
respectively. Extensive experiments show that CDG-KD effectively performs
attacks while preserving the general performance of the distilled model. Our
findings underscore critical need for developing watermarking schemes that are
robust and unforgeable.

</details>


### [66] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
*Yejin Bang,Ziwei Ji,Alan Schelten,Anthony Hartshorn,Tara Fowler,Cheng Zhang,Nicola Cancedda,Pascale Fung*

Main category: cs.CL

TL;DR: 该论文提出了一个全面的幻觉基准，通过区分外在和内在幻觉，解决了现有研究中定义不一致的问题，并引入了动态测试集生成以防止数据泄漏，旨在推动LLM幻觉研究的标准化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）生成与用户输入或训练数据不一致的“幻觉”现象，损害了用户信任并阻碍了生成式AI系统的应用。为了解决这一问题，需要建立一个统一的幻觉评估框架。

Method: 论文引入了清晰的幻觉分类法，区分外在幻觉（生成内容与训练数据不一致）和内在幻觉，并开发了动态测试集生成技术以避免数据泄漏，同时分析了现有基准的局限性。

Result: 提出了一个全面的幻觉基准，包括新的外在幻觉任务和动态测试集生成方法，解决了数据泄漏和基准饱和问题，为幻觉研究提供了更一致的评估工具。

Conclusion: 该研究通过明确的分类法和动态测试集技术，推动了LLM幻觉研究的标准化，为未来改进LLMs的可靠性和一致性奠定了基础。

Abstract: Large language models (LLMs) often generate responses that deviate from user
input or training data, a phenomenon known as "hallucination." These
hallucinations undermine user trust and hinder the adoption of generative AI
systems. Addressing hallucinations is essential for the advancement of LLMs.
This paper introduces a comprehensive hallucination benchmark, incorporating
both new extrinsic and existing intrinsic evaluation tasks, built upon clear
taxonomy of hallucination. A major challenge in benchmarking hallucinations is
the lack of a unified framework due to inconsistent definitions and
categorizations. We disentangle LLM hallucination from "factuality," proposing
a clear taxonomy that distinguishes between extrinsic and intrinsic
hallucinations, to promote consistency and facilitate research. Extrinsic
hallucinations, where the generated content is not consistent with the training
data, are increasingly important as LLMs evolve. Our benchmark includes dynamic
test set generation to mitigate data leakage and ensure robustness against such
leakage. We also analyze existing benchmarks, highlighting their limitations
and saturation. The work aims to: (1) establish a clear taxonomy of
hallucinations, (2) introduce new extrinsic hallucination tasks, with data that
can be dynamically regenerated to prevent saturation by leakage, (3) provide a
comprehensive analysis of existing benchmarks, distinguishing them from
factuality evaluations.

</details>


### [67] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
*Rei Higuchi,Ryotaro Kawata,Naoki Nishikawa,Kazusato Oko,Shoichiro Yamaguchi,Sosuke Kobayashi,Seiya Tokui,Kohei Hayashi,Daisuke Okanohara,Taiji Suzuki*

Main category: cs.CL

TL;DR: 前置元数据（如URL、域名等）在预训练数据中可以提升语言模型在特定下游任务的表现，但其效果取决于上下文是否足够推断潜在语义：上下文充足时效果积极，反之则负面。


<details>
  <summary>Details</summary>
Motivation: 研究前置元数据如何影响语言模型的潜在语义学习能力，并解释为何其在某些下游任务中表现提升而在平均预测损失上无一致性改善。

Method: 使用人工生成的数据（基于概率上下文无关文法），分析模型在不同上下文长度下对潜在语义的推断能力。

Result: 前置元数据在上下文足够长时（能推断潜在语义）提升性能，但上下文不足时会导致性能下降。

Conclusion: 该技术的有效性高度依赖下游任务提示的上下文信息，需谨慎应用以避免负面效果。

Abstract: The ability to acquire latent semantics is one of the key properties that
determines the performance of language models. One convenient approach to
invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at
the beginning of texts in the pre-training data, making it easier for the model
to access latent semantics before observing the entire text. Previous studies
have reported that this technique actually improves the performance of trained
models in downstream tasks; however, this improvement has been observed only in
specific downstream tasks, without consistent enhancement in average next-token
prediction loss. To understand this phenomenon, we closely investigate how
prepending metadata during pre-training affects model performance by examining
its behavior using artificial data. Interestingly, we found that this approach
produces both positive and negative effects on the downstream tasks. We
demonstrate that the effectiveness of the approach depends on whether latent
semantics can be inferred from the downstream task's prompt. Specifically,
through investigations using data generated by probabilistic context-free
grammars, we show that training with metadata helps improve model's performance
when the given context is long enough to infer the latent semantics. In
contrast, the technique negatively impacts performance when the context lacks
the necessary information to make an accurate posterior inference.

</details>


### [68] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
*Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Yunjie Ji,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于难度分级的推理数据集构建方法，并通过精确筛选高质量数据显著提升了基础模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前大型语言模型在训练过程和数据质量上缺乏深入理解的问题。

Method: 构建大规模难度分级的推理数据集，利用通过率和变异系数筛选最有价值的数据，并调整学习率进行训练。

Result: 在AIME2024数学推理基准测试中，模型的通过率达到79.2%，超越了多数蒸馏模型并接近最先进水平。

Conclusion: 通过高质量数据和训练方法的优化，显著提升了基础模型的推理能力，并开源了数据集和方法以推动开源领域的进步。

Abstract: Although large language models (LLMs) have recently achieved remarkable
performance on various complex reasoning benchmarks, the academic community
still lacks an in-depth understanding of base model training processes and data
quality. To address this, we construct a large-scale, difficulty-graded
reasoning dataset containing approximately 3.34 million unique queries of
varying difficulty levels and about 40 million distilled responses generated by
multiple models over several passes. Leveraging pass rate and Coefficient of
Variation (CV), we precisely select the most valuable training data to enhance
reasoning capability. Notably, we observe a training pattern shift, indicating
that reasoning-focused training based on base models requires higher learning
rates for effective training. Using this carefully selected data, we
significantly improve the reasoning capabilities of the base model, achieving a
pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This
result surpasses most current distilled models and closely approaches
state-of-the-art performance. We provide detailed descriptions of our data
processing, difficulty assessment, and training methodology, and have publicly
released all datasets and methods to promote rapid progress in open-source
long-reasoning LLMs. The dataset is available at:
https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [69] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
*Zhenkai Qin,Guifang Yang,Dongze Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为RAGAT-Mind的多粒度建模方法，用于中文谣言检测，结合多种深度学习技术，在Weibo1-Rumor数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假信息的泛滥使得谣言检测成为自然语言处理领域的重要挑战，需要更高效的解决方案。

Method: 采用TextCNN提取局部语义，双向GRU学习序列上下文，多头自注意力机制聚焦全局依赖，双向图卷积网络（BiGCN）处理词共现图结构表示。

Result: 在Weibo1-Rumor数据集上达到99.2%的准确率和0.9919的宏F1分数，验证了方法的有效性。

Conclusion: 模型结合了层次化语言特征与图语义结构，具有强泛化能力和可解释性，适用于实际谣言检测场景。

Abstract: As false information continues to proliferate across social media platforms,
effective rumor detection has emerged as a pressing challenge in natural
language processing. This paper proposes RAGAT-Mind, a multi-granular modeling
approach for Chinese rumor detection, built upon the MindSpore deep learning
framework. The model integrates TextCNN for local semantic extraction,
bidirectional GRU for sequential context learning, Multi-Head Self-Attention
for global dependency focusing, and Bidirectional Graph Convolutional Networks
(BiGCN) for structural representation of word co-occurrence graphs. Experiments
on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior
classification performance, attaining 99.2% accuracy and a macro-F1 score of
0.9919. The results validate the effectiveness of combining hierarchical
linguistic features with graph-based semantic structures. Furthermore, the
model exhibits strong generalization and interpretability, highlighting its
practical value for real-world rumor detection applications.

</details>


### [70] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
*Samaneh Hosseini Moghaddam,Kelly Lyons,Cheryl Regehr,Vivek Goel,Kaitlyn Regehr*

Main category: cs.CL

TL;DR: 本文提出了一种用于区分在线文本中辱骂性语言关键特征的分类法，整合了18个多标签数据集的分类系统，构建了一个包含5个类别和17个维度的分层多面分类法。


<details>
  <summary>Details</summary>
Motivation: 在线辱骂性语言的泛滥对个人和社区的健康与福祉构成重大风险，迫切需要识别和减轻有害内容的方法，以促进持续监控、管理和早期干预。

Method: 采用系统化的分类法开发方法，整合了18个现有多标签数据集的分类系统，捕捉与在线辱骂性语言分类相关的关键特征。

Result: 最终构建了一个分层多面的分类法，包括5个类别和17个维度，涵盖了在线辱骂的多种方面，如背景、目标、强度、直接性和主题。

Conclusion: 这一共享理解可以促进更一致的努力，推动知识交流，并加速在线辱骂检测和缓解领域的研究者、政策制定者、平台所有者等各方进展。

Abstract: The proliferation of abusive language in online communications has posed
significant risks to the health and wellbeing of individuals and communities.
The growing concern regarding online abuse and its consequences necessitates
methods for identifying and mitigating harmful content and facilitating
continuous monitoring, moderation, and early intervention. This paper presents
a taxonomy for distinguishing key characteristics of abusive language within
online text. Our approach uses a systematic method for taxonomy development,
integrating classification systems of 18 existing multi-label datasets to
capture key characteristics relevant to online abusive language classification.
The resulting taxonomy is hierarchical and faceted, comprising 5 categories and
17 dimensions. It classifies various facets of online abuse, including context,
target, intensity, directness, and theme of abuse. This shared understanding
can lead to more cohesive efforts, facilitate knowledge exchange, and
accelerate progress in the field of online abuse detection and mitigation among
researchers, policy makers, online platform owners, and other stakeholders.

</details>


### [71] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
*Zena Al-Khalili,Nick Howell,Dietrich Klakow*

Main category: cs.CL

TL;DR: 通过分析代码辅助LLM在数学推理任务中生成的程序，揭示其是否基于数学规则，以及这对性能的影响，研究发现不同LLM和问题难度会影响规则遵守程度。


<details>
  <summary>Details</summary>
Motivation: 当前对代码辅助LLM的评估仅关注执行正确性，缺乏对其生成程序严谨性的深入分析，本研究旨在填补这一空白。

Method: 手动和自动评估五个不同LLM在两个数学数据集上的生成程序，重点关注程序是否基于数学规则。

Result: 发现规则遵守程度与LLM能力和问题难度相关，闭源模型表现更好，开源模型在MATH500上规则遵守率降至一半。

Conclusion: 强调需要超越执行准确性的深入评估，以更全面理解代码辅助LLM在数学领域的能力和局限。

Abstract: Assisting LLMs with code generation improved their performance on
mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is
generally restricted to execution correctness, lacking a rigorous evaluation of
their generated programs. In this work, we bridge this gap by conducting an
in-depth analysis of code-assisted LLMs' generated programs in response to math
reasoning tasks. Our evaluation focuses on the extent to which LLMs ground
their programs to math rules, and how that affects their end performance. For
this purpose, we assess the generations of five different LLMs, on two
different math datasets, both manually and automatically. Our results reveal
that the distribution of grounding depends on LLMs' capabilities and the
difficulty of math problems. Furthermore, mathematical grounding is more
effective for closed-source models, while open-source models fail to employ
math rules in their solutions correctly. On MATH500, the percentage of grounded
programs decreased to half, while the ungrounded generations doubled in
comparison to ASDiv grade-school problems. Our work highlights the need for
in-depth evaluation beyond execution accuracy metrics, toward a better
understanding of code-assisted LLMs' capabilities and limits in the math
domain.

</details>


### [72] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
*Yuanchang Ye,Weiyan Wen*

Main category: cs.CL

TL;DR: 该研究通过Split Conformal Prediction框架解决了大型视觉-语言模型在VQA任务中的幻觉问题，提出了一种模型无关的不确定性量化方法，动态调整预测集大小并确保误差率低于预设风险水平，验证了其在不同数据集和模型上的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在多模态推理中表现出色，但其输出可能存在高置信度的幻觉内容，这在安全关键应用中构成风险。因此，需要一种可靠的方法来检测和减少幻觉。

Method: 采用Split Conformal Prediction框架，结合动态阈值校准和跨模态一致性验证，通过非符合性分数构建预测集，动态调整预测集大小并确保边际覆盖率。

Result: 在多个基准测试和模型中，该方法确保了理论保证的严格误差控制，并在不同校准-测试分割比例下表现稳定。

Conclusion: 该框架为多模态AI系统提供了一种可扩展的解决方案，填补了理论可靠性与实际应用之间的差距，适用于健康和自动驾驶等安全敏感领域。

Abstract: This study addresses the critical challenge of hallucination mitigation in
Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks
through a Split Conformal Prediction (SCP) framework. While LVLMs excel in
multi-modal reasoning, their outputs often exhibit hallucinated content with
high confidence, posing risks in safety-critical applications. We propose a
model-agnostic uncertainty quantification method that integrates dynamic
threshold calibration and cross-modal consistency verification. By partitioning
data into calibration and test sets, the framework computes nonconformity
scores to construct prediction sets with statistical guarantees under
user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous
control of \textbf{marginal coverage} to ensure empirical error rates remain
strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes
inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of
prior distribution assumptions and retraining requirements. Evaluations on
benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces
theoretical guarantees across all $\alpha$ values. The framework achieves
stable performance across varying calibration-to-test split ratios,
underscoring its robustness for real-world deployment in healthcare, autonomous
systems, and other safety-sensitive domains. This work bridges the gap between
theoretical reliability and practical applicability in multi-modal AI systems,
offering a scalable solution for hallucination detection and uncertainty-aware
decision-making.

</details>


### [73] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
*Jared Fernandez,Clara Na,Vashisth Tiwari,Yonatan Bisk,Sasha Luccioni,Emma Strubell*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）推理效率优化的能源影响，提出了一种模型方法，通过输入输出令牌分布和批量大小变化近似真实工作负载，发现优化可减少高达73%的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和应用的扩大，其计算和环境成本上升，现有研究多关注理想化环境下的延迟减少，而忽略了多样化真实工作负载对能源的影响。

Method: 采用分箱策略对输入输出令牌分布和批量大小变化建模，分析软件框架、解码策略、GPU架构、服务设置和模型并行配置。

Result: 研究表明，推理优化的效果对工作负载几何形状、软件栈和硬件加速器高度敏感，优化可减少高达73%的能源消耗。

Conclusion: 研究成果为可持续LLM部署提供了基础，并指导未来AI基础设施的能源高效设计策略。

Abstract: As large language models (LLMs) scale in size and adoption, their
computational and environmental costs continue to rise. Prior benchmarking
efforts have primarily focused on latency reduction in idealized settings,
often overlooking the diverse real-world inference workloads that shape energy
use. In this work, we systematically analyze the energy implications of common
inference efficiency optimizations across diverse Natural Language Processing
(NLP) and generative Artificial Intelligence (AI) workloads, including
conversational AI and code generation. We introduce a modeling approach that
approximates real-world LLM workflows through a binning strategy for
input-output token distributions and batch size variations. Our empirical
analysis spans software frameworks, decoding strategies, GPU architectures,
online and offline serving settings, and model parallelism configurations. We
show that the effectiveness of inference optimizations is highly sensitive to
workload geometry, software stack, and hardware accelerators, demonstrating
that naive energy estimates based on FLOPs or theoretical GPU utilization
significantly underestimate real-world energy consumption. Our findings reveal
that the proper application of relevant inference efficiency optimizations can
reduce total energy use by up to 73% from unoptimized baselines. These insights
provide a foundation for sustainable LLM deployment and inform energy-efficient
design strategies for future AI infrastructure.

</details>


### [74] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
*Haru-Tada Sato,Fuka Matsuzaki,Jun-ichiro Takahashi*

Main category: cs.CL

TL;DR: 该研究提出了一种名为Ensemble Bayesian Inference (EBI)的新方法，通过贝叶斯估计结合多个小型语言模型(SLM)的输出，使其性能超越单个模型，接近大型语言模型(LLM)，尤其在多语言和资源受限场景下表现显著。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过整合多个小型语言模型(SLM)来克服其个体性能限制，实现与专有大型语言模型(LLM)相似的准确性，同时降低计算资源需求。

Method: 方法上提出了Ensemble Bayesian Inference (EBI)，利用贝叶斯估计综合多个SLM的预测，优化整体性能，并验证了在负面贡献模型和多语言任务中的有效性。

Result: 实验结果表明，EBI在多语言任务（如能力评估和消费者画像分析）中显著提升性能，并能有效利用性能较低的模型提升整体表现。

Conclusion: 结论指出，EBI为资源受限场景下的高性能AI系统构建提供了新思路，并展示了低性能模型在集成中的潜在价值，对开源LLM的利用具有重要意义。

Abstract: This study explores the potential of small language model(SLM) ensembles to
achieve accuracy comparable to proprietary large language models (LLMs). We
propose Ensemble Bayesian Inference (EBI), a novel approach that applies
Bayesian estimation to combine judgments from multiple SLMs, allowing them to
exceed the performance limitations of individual models. Our experiments on
diverse tasks(aptitude assessments and consumer profile analysis in both
Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze
cases where incorporating models with negative Lift values into ensembles
improves overall performance, and we examine the method's efficacy across
different languages. These findings suggest new possibilities for constructing
high-performance AI systems with limited computational resources and for
effectively utilizing models with individually lower performance. Building on
existing research on LLM performance evaluation, ensemble methods, and
open-source LLM utilization, we discuss the novelty and significance of our
approach.

</details>


### [75] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
*Cheng Wang,Yue Liu,Baolong Li,Duzhen Zhang,Zhongzhi Li,Junfeng Fang*

Main category: cs.CL

TL;DR: 本文总结了大型推理模型（LRMs）的安全风险、攻击方式和防御策略，并通过分类法为未来的研究和发展提供了清晰的结构化理解。


<details>
  <summary>Details</summary>
Motivation: 随着LRMs在数学和编程等任务中展现出强大的推理能力，其安全漏洞和风险也引起了广泛关注，这阻碍了其在实际应用中的部署。

Method: 论文通过全面的调查和分类法，系统地总结了LRMs的安全风险、攻击方式和防御策略。

Result: 研究提供了一个详细的安全风险分类法，帮助理解LRMs的安全现状。

Conclusion: 该研究为未来提升LRMs安全性和可靠性的研究奠定了基础。

Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks
like mathematics and coding, leveraging their advanced reasoning capabilities.
Nevertheless, as these capabilities progress, significant concerns regarding
their vulnerabilities and safety have arisen, which can pose challenges to
their deployment and application in real-world settings. This paper presents a
comprehensive survey of LRMs, meticulously exploring and summarizing the newly
emerged safety risks, attacks, and defense strategies. By organizing these
elements into a detailed taxonomy, this work aims to offer a clear and
structured understanding of the current safety landscape of LRMs, facilitating
future research and development to enhance the security and reliability of
these powerful models.

</details>


### [76] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
*Vansh Gupta,Sankalan Pal Chowdhury,Vilém Zouhar,Donya Rooein,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）在非英语教育任务中的表现，发现其性能与训练数据中语言的丰富程度相关，推荐部署前验证模型在目标语言中的效果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在非英语教育环境中的适用性，尤其是在资源较少的语言中是否能有效执行教育任务。

Method: 评估了多个流行LLM在六种非英语语言（如印地语、阿拉伯语等）中执行四项教育任务的表现，并与英语表现对比。

Result: 模型表现与训练数据中的语言资源量相关，非英语语言（尤其是低资源语言）任务性能明显低于英语。

Conclusion: 建议实际应用前验证LLM在目标语言中的任务表现，以确保教育部署的有效性和公平性。

Abstract: Large language models (LLMs) are increasingly being adopted in educational
settings. These applications expand beyond English, though current LLMs remain
primarily English-centric. In this work, we ascertain if their use in education
settings in non-English languages is warranted. We evaluated the performance of
popular LLMs on four educational tasks: identifying student misconceptions,
providing targeted feedback, interactive tutoring, and grading translations in
six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to
English. We find that the performance on these tasks somewhat corresponds to
the amount of language represented in training data, with lower-resource
languages having poorer task performance. Although the models perform
reasonably well in most languages, the frequent performance drop from English
is significant. Thus, we recommend that practitioners first verify that the LLM
works well in the target language for their educational task before deployment.

</details>


### [77] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula Allen-Meares,Eulalia Puig Abril,Olga Garcia,Carolyn Dickens,Andrew Boyd*

Main category: cs.CL

TL;DR: 文章比较了两种对话助手（自主研发的神经符号架构和基于ChatGPT的版本）在医疗健康领域的表现，发现前者更准确且简洁，后者错误较少且需澄清更少，但患者对两者无偏好。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，医疗健康领域的对话助手需求增加，但需通过实际评估对比传统架构与生成式AI的优缺点。

Method: 采用组内用户研究，对比自主研发的神经符号架构对话助手和基于ChatGPT的版本，评估两者在盐分问答任务中的表现。

Result: 自主研发系统更准确、任务完成率更高且回答更简洁；ChatGPT版本语音错误更少、需澄清次数更少，但患者对两者无明显偏好。

Conclusion: 传统架构与生成式AI各有优劣，应根据需求选择；患者无偏好表明两种方案在实用中均可接受。

Abstract: Conversational assistants are becoming more and more popular, including in
healthcare, partly because of the availability and capabilities of Large
Language Models. There is a need for controlled, probing evaluations with real
stakeholders which can highlight advantages and disadvantages of more
traditional architectures and those based on generative AI. We present a
within-group user study to compare two versions of a conversational assistant
that allows heart failure patients to ask about salt content in food. One
version of the system was developed in-house with a neurosymbolic architecture,
and one is based on ChatGPT. The evaluation shows that the in-house system is
more accurate, completes more tasks and is less verbose than the one based on
ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors
and requires fewer clarifications to complete the task. Patients show no
preference for one over the other.

</details>


### [78] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
*Piotr Nawrot,Robert Li,Renjie Huang,Sebastian Ruder,Kelly Marchisio,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: 这篇论文探讨了稀疏注意力在Transformer大语言模型（LLMs）中的潜力，通过系统实验分析了不同模型规模、序列长度和稀疏度下的效率-准确性权衡，总结出稀疏注意力是处理长序列的有效工具，但需根据具体任务谨慎选择策略。


<details>
  <summary>Details</summary>
Motivation: 稀疏注意力被认为是扩展Transformer LLMs长上下文能力的潜在方法，但其实际可行性、效率与准确性的权衡以及系统性扩展研究尚未充分探索，因此本文旨在填补这一空白。

Method: 通过对比无训练的稀疏注意力方法，研究不同模型规模、序列长度和稀疏度下的表现，并在多样化的长序列任务上进行评估，包括新的可控且易评估的自然语言任务。

Result: 研究发现：1) 对于极长序列，更大且高稀疏度的模型优于小且密集的模型；2) 解码阶段可达到的稀疏度高于预填充阶段，且与模型规模相关；3) 稀疏策略需因任务和阶段而异，未发现通用最优方法；4) 提出了针对稀疏注意力的新缩放规律。

Conclusion: 稀疏注意力是增强Transformer LLMs长序列处理能力的重要工具，但在性能敏感的应用中需仔细权衡其效果。

Abstract: Sparse attention offers a promising strategy to extend long-context
capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy
trade-offs, and systematic scaling studies remain unexplored. To address this
gap, we perform a careful comparison of training-free sparse attention methods
at varying model scales, sequence lengths, and sparsity levels on a diverse
collection of long-sequence tasks-including novel ones that rely on natural
language while remaining controllable and easy to evaluate. Based on our
experiments, we report a series of key findings: 1) an isoFLOPS analysis
reveals that for very long sequences, larger and highly sparse models are
preferable to smaller and dense ones. 2) The level of sparsity attainable while
statistically guaranteeing accuracy preservation is higher during decoding than
prefilling, and correlates with model size in the former. 3) There is no clear
strategy that performs best across tasks and phases, with different units of
sparsification or budget adaptivity needed for different scenarios. Even
moderate sparsity levels often result in significant performance degradation on
at least one task, highlighting that sparse attention is not a universal
solution. 4) We introduce and validate novel scaling laws specifically tailored
for sparse attention, providing evidence that our findings are likely to hold
true beyond our range of experiments. Through these insights, we demonstrate
that sparse attention is a key tool to enhance the capabilities of Transformer
LLMs for processing longer sequences, but requires careful evaluation of
trade-offs for performance-sensitive applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [A Novel Graph Transformer Framework for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.16961)
*Binon Teji,Swarup Roy*

Main category: cs.LG

TL;DR: 该论文提出了一种名为GT-GRN的图变换器模型，通过整合多种网络信息和嵌入技术，显著提升了基因调控网络（GRN）的推断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基因共表达数据推断GRN易受噪声影响且可能无法反映真实的生物学调控关系，现有方法在网络重建阶段面临诸多挑战，需要整合先验知识以提升推断效果。

Method: 采用自编码器嵌入原始基因表达数据，利用随机游走和BERT生成全局基因嵌入，结合位置编码，最终通过图变换器模型GT-GRN进行GRN推断。

Result: 实验表明GT-GRN显著优于现有方法，展示了更高的准确性和鲁棒性。

Conclusion: 整合多种嵌入技术和先验知识的GT-GRN模型为GRN推断提供了更优的解决方案。

Abstract: The inference of gene regulatory networks (GRNs) is a foundational stride
towards deciphering the fundamentals of complex biological systems. Inferring a
possible regulatory link between two genes can be formulated as a link
prediction problem. Inference of GRNs via gene coexpression profiling data may
not always reflect true biological interactions, as its susceptibility to noise
and misrepresenting true biological regulatory relationships. Most GRN
inference methods face several challenges in the network reconstruction phase.
Therefore, it is important to encode gene expression values, leverege the prior
knowledge gained from the available inferred network structures and positional
informations of the input network nodes towards inferring a better and more
confident GRN network reconstruction. In this paper, we explore the integration
of multiple inferred networks to enhance the inference of Gene Regulatory
Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene
expression patterns directly from raw data, preserving intricate biological
signals. Then, we embed the prior knowledge from GRN structures transforming
them into a text-like representation using random walks, which are then encoded
with a masked language model, BERT, to generate global embeddings for each gene
across all networks. Additionally, we embed the positional encodings of the
input gene networks to better identify the position of each unique gene within
the graph. These embeddings are integrated into graph transformer-based model,
termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the
topological structure of the ground truth network while incorporating the
enriched encoded information. Experimental results demonstrate that GT-GRN
significantly outperforms existing GRN inference methods, achieving superior
accuracy and highlighting the robustness of our approach.

</details>


### [80] [Backslash: Rate Constrained Optimized Training of Large Language Models](https://arxiv.org/abs/2504.16968)
*Jun Wu,Jiangtao Wen,Yuxing Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为Backslash的训练时压缩方法，通过速率-失真优化实现模型精度与复杂度的灵活权衡，显著减少参数冗余。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型（LLMs）参数压缩研究主要集中在训练后阶段，而训练时的压缩方法尚未充分探索。

Method: 采用速率-失真优化（RDO）的Rate-Constrained Training（Backslash）方法，在训练过程中动态调整参数冗余。

Result: 实验表明，Backslash能在多种架构和任务中减少60%-90%内存占用且无精度损失，并显著优于训练后压缩方法。此外，它还提升泛化性、抗剪枝鲁棒性，并加速边缘设备推理。

Conclusion: Backslash是一种高效且通用的训练时压缩方法，为模型优化提供了新方向。

Abstract: The rapid advancement of large-language models (LLMs) has driven extensive
research into parameter compression after training has been completed, yet
compression during the training phase remains largely unexplored. In this work,
we introduce Rate-Constrained Training (Backslash), a novel training-time
compression approach based on rate-distortion optimization (RDO). Backslash
enables a flexible trade-off between model accuracy and complexity,
significantly reducing parameter redundancy while preserving performance.
Experiments in various architectures and tasks demonstrate that Backslash can
reduce memory usage by 60\% - 90\% without accuracy loss and provides
significant compression gain compared to compression after training. Moreover,
Backslash proves to be highly versatile: it enhances generalization with small
Lagrange multipliers, improves model robustness to pruning (maintaining
accuracy even at 80\% pruning rates), and enables network simplification for
accelerated inference on edge devices.

</details>


### [81] [STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction](https://arxiv.org/abs/2504.16970)
*Yin Wang,Chunlin Gong,Xiang Wu,Hanleran Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于相空间重构和时空融合映射的数据驱动海表温度预测方法，相比传统方法在小样本下更高效准确。


<details>
  <summary>Details</summary>
Motivation: 现有海表温度预测方法如物理数值模拟高复杂，数据驱动方法需大样本且可解释性差，需改进。

Method: 利用相空间重构构建初始延迟吸引子对，设计时空融合映射（STFM）揭示其内在关联。

Result: 在小样本测试中表现出高预测精度，优于传统模型。

Conclusion: 该方法通过相空间重构高效捕捉海温动态，为数据驱动预测提供了新思路。

Abstract: The sea surface temperature (SST), a key environmental parameter, is crucial
to optimizing production planning, making its accurate prediction a vital
research topic. However, the inherent nonlinearity of the marine dynamic system
presents significant challenges. Current forecasting methods mainly include
physics-based numerical simulations and data-driven machine learning
approaches. The former, while describing SST evolution through differential
equations, suffers from high computational complexity and limited
applicability, whereas the latter, despite its computational benefits, requires
large datasets and faces interpretability challenges. This study presents a
prediction framework based solely on data-driven techniques. Using phase space
reconstruction, we construct initial-delay attractor pairs with a mathematical
homeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover
their intrinsic connections. Unlike conventional models, our method captures
SST dynamics efficiently through phase space reconstruction and achieves high
prediction accuracy with minimal training data in comparative tests

</details>


### [82] [Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications](https://arxiv.org/abs/2504.16972)
*Hossein Ahmadi,Sajjad Emdadi Mahdimahalleh,Arman Farahat,Banafsheh Saffari*

Main category: cs.LG

TL;DR: 这篇综述总结了自编码器和视觉变换器在无监督信号分析中的应用进展，涵盖架构、应用及趋势，探讨了它们在特征提取、异常检测和分类中的表现，同时指出了可解释性、扩展性和领域泛化等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信、雷达、生物医学工程和物联网等领域中未标记时间序列数据的快速增长，推动了无监督学习的进步。本文旨在综述自编码器和视觉变换器在信号分析中的最新应用。

Method: 通过分析自编码器和视觉变换器的架构及其应用，探讨它们在特征提取、异常检测和分类中的表现，并研究了混合架构和自监督学习的优势。

Result: 研究展示了这些模型在多样化信号类型（如心电图、雷达波形和物联网传感器数据）中的有效性，但也揭示了可解释性、扩展性和领域泛化等挑战。

Conclusion: 本文为开发鲁棒、自适应的信号智能模型提供了路线图，强调了方法创新与实际应用的结合。

Abstract: The rapid growth of unlabeled time-series data in domains such as wireless
communications, radar, biomedical engineering, and the Internet of Things (IoT)
has driven advancements in unsupervised learning. This review synthesizes
recent progress in applying autoencoders and vision transformers for
unsupervised signal analysis, focusing on their architectures, applications,
and emerging trends. We explore how these models enable feature extraction,
anomaly detection, and classification across diverse signal types, including
electrocardiograms, radar waveforms, and IoT sensor data. The review highlights
the strengths of hybrid architectures and self-supervised learning, while
identifying challenges in interpretability, scalability, and domain
generalization. By bridging methodological innovations and practical
applications, this work offers a roadmap for developing robust, adaptive models
for signal intelligence.

</details>


### [83] [Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980)
*Pratyush Maini,Sachin Goyal,Dylan Sam,Alex Robey,Yash Savani,Yiding Jiang,Andy Zou,Zacharcy C. Lipton,J. Zico Kolter*

Main category: cs.LG

TL;DR: 论文提出了一种数据中心的预训练框架，通过安全分类器、合成安全数据集和拒绝对话等方法来内置模型安全性，显著降低了有害内容生成率。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在预训练后难以移除有害内容的问题，研究致力于在预训练阶段就构建安全性。

Method: 采用安全分类器过滤数据、生成合成安全数据集、构建拒绝对话和教育材料数据集，并在预训练中注入有害标签标记。

Result: 安全预训练模型将攻击成功率从38.8%降至8.4%，且不影响标准安全基准性能。

Conclusion: 该框架有效提升了模型的安全性，验证了在预训练阶段内置安全性的可行性。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, the risk of generating harmful or toxic content remains a central
challenge. Post-hoc alignment methods are brittle: once unsafe patterns are
learned during pretraining, they are hard to remove. We present a data-centric
pretraining framework that builds safety into the model from the start. Our
contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled
examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset
to date (100B tokens) generated via recontextualization of harmful web data;
(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into
refusal dialogues and web-style educational material; (iv) Harmfulness-Tag
annotations injected during pretraining to flag unsafe content and steer away
inference from harmful generations; and (v) safety evaluations measuring base
model behavior before instruction tuning. Our safety-pretrained models reduce
attack success rates from 38.8% to 8.4% with no performance degradation on
standard LLM safety benchmarks.

</details>


### [84] [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
*Amin Karbasi,Omar Montasser,John Sous,Grigoris Velegkas*

Main category: cs.LG

TL;DR: 该论文探讨了自动检测大语言模型（LLM）幻觉的可行性，理论分析表明，仅使用目标语言的正确示例训练检测器时，检测是不可能的，但在加入专家标注的反馈后则变为可能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理论分析自动检测LLM幻觉是否可行，借鉴经典的语言识别框架，探讨其在LLM输出正确性检测中的应用。

Method: 采用理论框架，将幻觉检测与语言识别任务等价，并分析在不同训练数据（仅正确示例 vs. 加入专家标注反馈）下的可能性。

Result: 结果表明，仅用正确示例时幻觉检测本质不可行，但加入专家标注的负例后，检测对所有可数语言集合成为可能。

Conclusion: 结论强调专家标注反馈对训练可靠幻觉检测器的关键作用，支持如RLHF等反馈方法的重要性。

Abstract: Is automated hallucination detection possible? In this work, we introduce a
theoretical framework to analyze the feasibility of automatically detecting
hallucinations produced by large language models (LLMs). Inspired by the
classical Gold-Angluin framework for language identification and its recent
adaptation to language generation by Kleinberg and Mullainathan, we investigate
whether an algorithm, trained on examples drawn from an unknown target language
$K$ (selected from a countable collection) and given access to an LLM, can
reliably determine whether the LLM's outputs are correct or constitute
hallucinations.
  First, we establish an equivalence between hallucination detection and the
classical task of language identification. We prove that any hallucination
detection method can be converted into a language identification method, and
conversely, algorithms solving language identification can be adapted for
hallucination detection. Given the inherent difficulty of language
identification, this implies that hallucination detection is fundamentally
impossible for most language collections if the detector is trained using only
correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the
detector with both positive examples (correct statements) and negative examples
(explicitly labeled incorrect statements), dramatically changes this
conclusion. Under this enriched training regime, automated hallucination
detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in
training hallucination detectors and provide theoretical support for
feedback-based methods, such as reinforcement learning with human feedback
(RLHF), which have proven critical for reliable LLM deployment.

</details>


### [85] [Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU](https://arxiv.org/abs/2504.17028)
*Iman Khadir,Shane Stevenson,Henry Li,Kyle Krick,Abram Burrows,David Hall,Stan Posey,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 论文探讨了如何通过利用GPU和免费AI模型（如NVIDIA的FourCastNetv2）使大学研究小组能够普及AI驱动的全球天气预测模型，并分析了其可行性和挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）成本高昂且耗时，而AI模型如FourCastNetv2能显著降低时间和成本，但资源受限的大学研究小组在复现结果时面临挑战。因此，研究旨在探索如何利用现有技术资源实现AI天气预报的普及。

Method: 通过NVIDIA的FourCastNetv2 API生成预测，并利用NVIDIA硬件训练原版FourCastNet模型，同时分析数据管理、训练效率和模型验证的实践。

Result: 研究表明，尽管资源有限，大学研究小组仍能利用GPU和开源模型实现AI天气预报，但需解决资源分配和技术挑战。

Conclusion: 本文为大学研究小组和课程提供了一个初步指南，有助于推动AI天气预报在机器学习和气候科学领域的普及，从而促进数字经济学中的AI NWP民主化。

Abstract: This paper demonstrates the feasibility of democratizing AI-driven global
weather forecasting models among university research groups by leveraging
Graphics Processing Units (GPUs) and freely available AI models, such as
NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network
for weather prediction and is trained on a 73-channel subset of the European
Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset
at single levels and different pressure levels. Although the training
specifications for FourCastNetv2 are not released to the public, the training
documentation of the model's first generation, FourCastNet, is available to all
users. The training had 64 A100 GPUs and took 16 hours to complete. Although
NVIDIA's models offer significant reductions in both time and cost compared to
traditional Numerical Weather Prediction (NWP), reproducing published
forecasting results presents ongoing challenges for resource-constrained
university research groups with limited GPU availability. We demonstrate both
(i) leveraging FourCastNetv2 to create predictions through the designated
application programming interface (API) and (ii) utilizing NVIDIA hardware to
train the original FourCastNet model. Further, this paper demonstrates the
capabilities and limitations of NVIDIA A100's for resource-limited research
groups in universities. We also explore data management, training efficiency,
and model validation, highlighting the advantages and challenges of using
limited high-performance computing resources. Consequently, this paper and its
corresponding GitHub materials may serve as an initial guide for other
university research groups and courses related to machine learning, climate
science, and data science to develop research and education programs on AI
weather forecasting, and hence help democratize the AI NWP in the digital
economy.

</details>


### [86] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/abs/2504.17058)
*Rahul Vishwakarma*

Main category: cs.LG

TL;DR: 论文提出了一种称为Conformalized GAN（cGAN）的新框架，通过将保形预测方法集成到生成对抗网络中，为生成的合成数据提供无分布的不确定性量化，从而在高风险领域实现可靠应用。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽能生成高质量的合成样本，但缺乏与底层数据分布的严格统计保证，限制了其在需要鲁棒误差边界的关键领域的应用。

Method: 作者提出了一种结合保形预测方法（包括归纳保形预测、蒙德里安保形预测、交叉保形预测和Venn-Abers预测器）的新框架cGAN，以增强生成样本的校准性并保持传统GAN的生成能力。

Result: cGAN展示了增强的校准特性，同时通过严格的数学证明提供了有限样本有效性保证和渐近效率特性，为合成数据在高风险领域的应用提供了统计保证。

Conclusion: cGAN框架通过结合保形预测方法，为生成模型提供了统计上可验证的合成数据，显著提升了在高风险领域的适用性和可靠性。

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>


### [87] [Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks](https://arxiv.org/abs/2504.17065)
*Sahar Bagherkhani,Jackson Christopher Earls,Franco De Flaviis,Pierre Baldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的方法，利用卷积神经网络(CNNs)实现远场到近场(FF-NF)变换，旨在无需依赖显式解析变换即可从天线远场数据重建近场分布，并在训练和测试中取得了较低误差。


<details>
  <summary>Details</summary>
Motivation: 电磁场重建在诸多应用中至关重要，包括天线诊断、电磁干扰分析和系统建模。传统方法依赖复杂的解析变换，本研究的动机是利用深度学习简化这一过程并提升准确性。

Method: 采用卷积神经网络(CNNs)，基于配对的远场和近场数据进行训练，以均方误差(MSE)作为评估指标。

Result: 最佳模型的训练误差为0.0199，测试误差为0.3898，且视觉对比证实了模型能有效捕捉复杂的电磁场行为。

Conclusion: 研究表明深度学习在电磁场重建中具有潜力，能够高效且准确地完成远场到近场的变换任务。

Abstract: Electromagnetic field reconstruction is crucial in many applications,
including antenna diagnostics, electromagnetic interference analysis, and
system modeling. This paper presents a deep learning-based approach for
Far-Field to Near-Field (FF-NF) transformation using Convolutional Neural
Networks (CNNs). The goal is to reconstruct near-field distributions from the
far-field data of an antenna without relying on explicit analytical
transformations. The CNNs are trained on paired far-field and near-field data
and evaluated using mean squared error (MSE). The best model achieves a
training error of 0.0199 and a test error of 0.3898. Moreover, visual
comparisons between the predicted and true near-field distributions demonstrate
the model's effectiveness in capturing complex electromagnetic field behavior,
highlighting the potential of deep learning in electromagnetic field
reconstruction.

</details>


### [88] [Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching](https://arxiv.org/abs/2504.17066)
*Kewen Peng,Yicheng Yang,Hao Zhuo,Tim Menzies*

Main category: cs.LG

TL;DR: 本文探讨了训练和测试数据抽样方式对公平性指标可靠性的影响，并提出了一种名为FairMatch的后处理方法，利用倾向评分匹配来评估和减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，许多模型在多种公平性指标下仍存在不公平问题，且由于训练和测试数据通常来自同一总体，训练数据中的偏见可能延续到测试数据中，影响公平性评估的准确性。

Method: 提出FairMatch方法，通过倾向评分匹配在测试集中识别控制组和处理组配对，调整不同子组的决策阈值；对无法匹配的样本，使用公平性感知损失函数进行概率校准。

Result: 实验表明，FairMatch能精准定位测试数据中模型无偏见的子集，并在剩余数据上显著减少偏见，且不牺牲预测性能。

Conclusion: 倾向评分匹配为公平性评估和缓解提供了一种原则性方法，同时保持了模型的预测性能。

Abstract: Fairness-aware learning aims to mitigate discrimination against specific
protected social groups (e.g., those categorized by gender, ethnicity, age)
while minimizing predictive performance loss. Despite efforts to improve
fairness in machine learning, prior studies have shown that many models remain
unfair when measured against various fairness metrics. In this paper, we
examine whether the way training and testing data are sampled affects the
reliability of reported fairness metrics. Since training and test sets are
often randomly sampled from the same population, bias present in the training
data may still exist in the test data, potentially skewing fairness
assessments. To address this, we propose FairMatch, a post-processing method
that applies propensity score matching to evaluate and mitigate bias. FairMatch
identifies control and treatment pairs with similar propensity scores in the
test set and adjusts decision thresholds for different subgroups accordingly.
For samples that cannot be matched, we perform probabilistic calibration using
fairness-aware loss functions. Experimental results demonstrate that our
approach can (a) precisely locate subsets of the test data where the model is
unbiased, and (b) significantly reduce bias on the remaining data. Overall,
propensity score matching offers a principled way to improve both fairness
evaluation and mitigation, without sacrificing predictive performance.

</details>


### [89] [In-Context Learning can distort the relationship between sequence likelihoods and biological fitness](https://arxiv.org/abs/2504.17068)
*Pranav Kantroo,Günter P. Wagner,Benjamin B. Machta*

Main category: cs.LG

TL;DR: 论文指出语言模型在训练后会评估生物序列的合理性，但上下文学习可能扭曲序列适应度与似然评分的关系，尤其是重复模因的序列会获得异常高分。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型评估生物序列合理性时的潜在偏差，尤其是上下文学习如何影响序列适应度与模型评分的相关性。

Method: 使用基于掩码语言建模目标的蛋白质语言模型（特别是Transformer架构），分析重复模因序列的似然评分异常现象。

Result: 发现Transformer模型易受重复模因干扰，通过“查表”行为覆盖模型的先验知识，且在RNA反向互补模因中同样存在此现象。

Conclusion: 上下文学习可能导致语言模型对生物序列的评分失真，需警惕此类偏差在应用中的影响。

Abstract: Language models have emerged as powerful predictors of the viability of
biological sequences. During training these models learn the rules of the
grammar obeyed by sequences of amino acids or nucleotides. Once trained, these
models can take a sequence as input and produce a likelihood score as an
output; a higher likelihood implies adherence to the learned grammar and
correlates with experimental fitness measurements. Here we show that in-context
learning can distort the relationship between fitness and likelihood scores of
sequences. This phenomenon most prominently manifests as anomalously high
likelihood scores for sequences that contain repeated motifs. We use protein
language models with different architectures trained on the masked language
modeling objective for our experiments, and find transformer-based models to be
particularly vulnerable to this effect. This behavior is mediated by a look-up
operation where the model seeks the identity of the masked position by using
the other copy of the repeated motif as a reference. This retrieval behavior
can override the model's learned priors. This phenomenon persists for
imperfectly repeated sequences, and extends to other kinds of biologically
relevant features such as reversed complement motifs in RNA sequences that fold
into hairpin structures.

</details>


### [90] [Sparse Phased Array Optimization Using Deep Learning](https://arxiv.org/abs/2504.17073)
*David Lu,Lior Maman,Jackson Earls,Amir Boag,Pierre Baldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的稀疏相控阵优化方法，通过减少栅瓣来提升阵列设计效果，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏相控阵设计面临非凸问题和自由度多的挑战，传统方法难以高效优化。通过深度学习解决这些问题，提升阵列性能。

Method: 生成稀疏阵列配置，用神经网络近似非凸成本函数（主瓣与旁瓣能量比），结合梯度和惩罚机制优化元素坐标。

Result: 在10种初始成本最低的阵列上应用，平均改进552%，最高提升643%，显著降低了旁瓣水平。

Conclusion: 该方法为超精准波束成形和高效无线通信/雷达系统铺平了道路。

Abstract: Antenna arrays are widely used in wireless communication, radar systems,
radio astronomy, and military defense to enhance signal strength, directivity,
and interference suppression. We introduce a deep learning-based optimization
approach that enhances the design of sparse phased arrays by reducing grating
lobes. This approach begins by generating sparse array configurations to
address the non-convex challenges and extensive degrees of freedom inherent in
array design. We use neural networks to approximate the non-convex cost
function that estimates the energy ratio between the main and side lobes. This
differentiable approximation facilitates cost function minimization through
gradient descent, optimizing the antenna elements' coordinates and leading to
an improved layout. Additionally, we incorporate a tailored penalty mechanism
that includes various physical and design constraints into the optimization
process, enhancing its robustness and practical applicability. We demonstrate
the effectiveness of our method by applying it to the ten array configurations
with the lowest initial costs, achieving further cost reductions ranging from
411% to 643%, with an impressive average improvement of 552%. By significantly
reducing side lobe levels in antenna arrays, this breakthrough paves the way
for ultra-precise beamforming, enhanced interference mitigation, and
next-generation wireless and radar systems with unprecedented efficiency and
clarity.

</details>


### [91] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely,Otto Lamminpää,Steffen Mauceri,Sean M. R. Crowell,Christopher W. O'Dell,Gregory R. McGarragh*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散的方法，用于从卫星反射的太阳光谱中更快速、更灵活地估算温室气体浓度，解决了传统方法计算成本高和不确定性估计不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的最优估计算法（OE）在估算温室气体浓度时存在收敛问题和不确定性估计不准确的缺陷，而未来的卫星任务将产生更多数据，因此需开发更快速且可靠的算法。

Method: 论文提出了一种扩散方法，能够灵活地估算高斯或非高斯后验分布，相较现有方法显著提升了计算速度。

Result: 该方法被应用于NASA的轨道碳观测站-2光谱仪，表现出了优于当前技术的计算效率和准确性。

Conclusion: 这种新方法为实现近实时全球碳源和碳汇监测提供了可能，对气候政策制定具有重要意义。

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>


### [92] [A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices](https://arxiv.org/abs/2504.17079)
*Esam Mahdi,C. Martin-Barreiro,X. Cabezas*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer和GRU的深度学习混合模型，用于提高加密货币价格预测的准确性，实验表明其优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 结合Transformer捕捉长期模式和GRU处理短期序列的能力，提升加密货币价格预测的准确性。

Method: 使用历史数据（价格、交易量、恐惧与贪婪指数）训练混合模型，并与RBFN、GRNN、BiLSTM和BiGRU模型对比。

Result: 混合模型在MSE、RMSE、MAE和MAPE指标上表现最优，验证了其有效性。

Conclusion: 该模型为加密货币市场的实时决策提供了改进建议，支持混合深度学习模型在金融分析中的应用。

Abstract: In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.

</details>


### [93] [GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs](https://arxiv.org/abs/2504.17099)
*Martin Boeckling,Heiko Paulheim,Sarah Detzler*

Main category: cs.LG

TL;DR: 论文提出了一种改进的RDF2Vec方法，通过融入几何信息学习空间感知的实体嵌入，并在多个基准数据集上验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱中许多空间实体的几何信息未被充分利用，现有实体表示学习方法未考虑这些信息。

Method: 通过从地理节点扩散图并应用带空间权重的图游走改进RDF2Vec。

Result: 改进方法在多个基准数据集上优于非空间感知的RDF2Vec和GeoTransE。

Conclusion: 融入几何信息能显著提升实体嵌入学习的效果。

Abstract: Many knowledge graphs contain a substantial number of spatial entities, such
as cities, buildings, and natural landmarks. For many of these entities, exact
geometries are stored within the knowledge graphs. However, most existing
approaches for learning entity representations do not take these geometries
into account. In this paper, we introduce a variant of RDF2Vec that
incorporates geometric information to learn location-aware embeddings of
entities. Our approach expands different nodes by flooding the graph from
geographic nodes, ensuring that each reachable node is considered. Based on the
resulting flooded graph, we apply a modified version of RDF2Vec that biases
graph walks using spatial weights. Through evaluations on multiple benchmark
datasets, we demonstrate that our approach outperforms both non-location-aware
RDF2Vec and GeoTransE.

</details>


### [94] [Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks](https://arxiv.org/abs/2504.17109)
*Zhaobin Mo,Xiangyi Liao,Dominik A. Karbowski,Yanbing Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种结合时空图神经网络（ST-GNNs）和Shapley值的新方法，用于识别和解释交通崩溃的前兆，并在I-24数据上验证了道路拓扑和急刹车是主要因素。


<details>
  <summary>Details</summary>
Motivation: 理解和预测交通崩溃的前兆对提升道路安全和交通流量管理至关重要，但现有黑盒神经网络缺乏可解释性。

Method: 通过将Shapley解释方法扩展到时空场景，结合ST-GNNs，实现了预测与可解释原因之间的桥梁。

Result: 在I-24数据上的实验表明，道路拓扑和急刹车是引发交通崩溃的关键因素。

Conclusion: 该方法不仅提升了交通崩溃预测的准确性，还提供了可解释的分析，为实际交通管理提供了依据。

Abstract: Understanding and predicting the precursors of traffic breakdowns is critical
for improving road safety and traffic flow management. This paper presents a
novel approach combining spatiotemporal graph neural networks (ST-GNNs) with
Shapley values to identify and interpret traffic breakdown precursors. By
extending Shapley explanation methods to a spatiotemporal setting, our proposed
method bridges the gap between black-box neural network predictions and
interpretable causes. We demonstrate the method on the Interstate-24 data, and
identify that road topology and abrupt braking are major factors that lead to
traffic breakdowns.

</details>


### [95] [Scalable Permutation-Aware Modeling for Temporal Set Prediction](https://arxiv.org/abs/2504.17140)
*Ashish Ranjan,Ayush Agarwal,Shalin Barot,Sushant Kumar*

Main category: cs.LG

TL;DR: 提出了一种利用置换等变和置换不变变换的新框架，用于高效建模集合动态，显著降低训练和推理时间，同时在多个公共基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖复杂架构且计算开销大，难以扩展。本文旨在解决这一问题。

Method: 采用置换等变和置换不变变换的高效框架来建模集合动态。

Result: 在多个公共基准测试中，该方法表现与或优于最先进模型，同时显著降低计算时间。

Conclusion: 该方法实现了高效且可扩展的时间集合预测，具有显著优势。

Abstract: Temporal set prediction involves forecasting the elements that will appear in
the next set, given a sequence of prior sets, each containing a variable number
of elements. Existing methods often rely on intricate architectures with
substantial computational overhead, which hampers their scalability. In this
work, we introduce a novel and scalable framework that leverages
permutation-equivariant and permutation-invariant transformations to
efficiently model set dynamics. Our approach significantly reduces both
training and inference time while maintaining competitive performance.
Extensive experiments on multiple public benchmarks show that our method
achieves results on par with or superior to state-of-the-art models across
several evaluation metrics. These results underscore the effectiveness of our
model in enabling efficient and scalable temporal set prediction.

</details>


### [96] [OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection](https://arxiv.org/abs/2504.17160)
*Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: 该论文提出了一种新工具OUI（过拟合-欠拟合指标），用于在无需验证数据的情况下监控深度神经网络的训练动态，并指导权重衰减（WD）超参数的选择。实验证明，OUI能更快收敛，且其推荐区间与模型泛化性能高度相关。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖验证数据调参，耗时且不高效。OUI旨在通过实时监控训练动态，更早、更准确地识别过拟合或欠拟合，从而优化WD参数。

Method: 论文设计了OUI指标，并通过在DenseNet、EfficientNet和ResNet等模型及CIFAR-100、TinyImageNet和ImageNet-1K数据集上的实验验证其有效性。

Result: 实验显示，OUI能快速收敛，其推荐区间显著提高模型泛化性能，且优于传统指标（如损失或准确率）。

Conclusion: OUI为超参数调优提供了高效工具，减少了验证数据依赖，并在早期训练阶段即可识别最优WD值，提升了模型性能。

Abstract: We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for
monitoring the training dynamics of Deep Neural Networks (DNNs) and identifying
optimal regularization hyperparameters. Specifically, we validate that OUI can
effectively guide the selection of the Weight Decay (WD) hyperparameter by
indicating whether a model is overfitting or underfitting during training
without requiring validation data. Through experiments on DenseNet-BC-100 with
CIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,
we show that maintaining OUI within a prescribed interval correlates strongly
with improved generalization and validation scores. Notably, OUI converges
significantly faster than traditional metrics such as loss or accuracy,
enabling practitioners to identify optimal WD (hyperparameter) values within
the early stages of training. By leveraging OUI as a reliable indicator, we can
determine early in training whether the chosen WD value leads the model to
underfit the training data, overfit, or strike a well-balanced trade-off that
maximizes validation scores. This enables more precise WD tuning for optimal
performance on the tested datasets and DNNs. All code for reproducing these
experiments is available at https://github.com/AlbertoFdezHdez/OUI.

</details>


### [97] [A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation](https://arxiv.org/abs/2504.17196)
*Jiawen Hou,Hao Wu*

Main category: cs.LG

TL;DR: 论文提出了一种结合$L_2$-norm和smooth $L_1$-norm的时序感知交通速度数据填补方法（TATSI），通过SLF-NMU算法实现高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 交通速度数据常因传感器故障等原因缺失，现有基于张量分解的方法因依赖$L_2$-norm而鲁棒性不足。

Method: 提出TATSI方法，结合$L_2$-norm和${SL}_1$-norm构建损失函数，并采用SLF-NMU算法进行非负潜在因子分析。

Result: 在三个真实交通速度数据集上验证，TATSI能更精确捕捉时序模式，填补效果优于现有方法。

Conclusion: TATSI通过改进损失函数和算法设计，显著提升了交通速度数据填补的准确性和鲁棒性。

Abstract: In intelligent transportation systems (ITS), traffic management departments
rely on sensors, cameras, and GPS devices to collect real-time traffic data.
Traffic speed data is often incomplete due to sensor failures, data
transmission delays, or occlusions, resulting in missing speed data in certain
road segments. Currently, tensor decomposition based methods are extensively
utilized, they mostly rely on the $L_2$-norm to construct their learning
objectives, which leads to reduced robustness in the algorithms. To address
this, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which
combines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,
thereby achieving both high accuracy and robust performance in imputing missing
time-varying traffic speed data. TATSI adopts a single latent factor-dependent,
nonnegative, and multiplicative update (SLF-NMU) approach, which serves as an
efficient solver for performing nonnegative latent factor analysis (LFA) on a
tensor. Empirical studies on three real-world time-varying traffic speed
datasets demonstrate that, compared with state-of-the-art traffic speed
predictors, TATSI more precisely captures temporal patterns, thereby yielding
the most accurate imputations for missing traffic speed data.

</details>


### [98] [Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2504.17210)
*Junfei Wang,Darshana Upadhyay,Marzia Zaman,Pirathayini Srikantha*

Main category: cs.LG

TL;DR: 本文提出了一种基于去噪扩散概率模型（DDPMs）的物理信息生成框架，用于合成可行的电力潮流数据。该方法通过辅助训练和物理信息损失函数，确保生成数据既具有统计保真度，又符合电力系统可行性。在IEEE 14总线和30总线基准系统上的评估表明，该方法优于三种基线模型。


<details>
  <summary>Details</summary>
Motivation: 智能电网中许多数据驱动模块依赖于高质量电力潮流数据，但现实数据常因隐私和操作限制而有限。

Method: 采用基于DDPMs的物理信息生成框架，结合辅助训练和物理信息损失函数。

Result: 在IEEE 14总线和30总线系统上验证，生成数据在统计特征、多样性和可行性方面优于基线模型。

Conclusion: 该工作展示了将生成模型集成到数据驱动电力系统应用中的潜力。

Abstract: Many data-driven modules in smart grid rely on access to high-quality power
flow data; however, real-world data are often limited due to privacy and
operational constraints. This paper presents a physics-informed generative
framework based on Denoising Diffusion Probabilistic Models (DDPMs) for
synthesizing feasible power flow data. By incorporating auxiliary training and
physics-informed loss functions, the proposed method ensures that the generated
data exhibit both statistical fidelity and adherence to power system
feasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark
systems, demonstrating its ability to capture key distributional properties and
generalize to out-of-distribution scenarios. Comparative results show that the
proposed model outperforms three baseline models in terms of feasibility,
diversity, and accuracy of statistical features. This work highlights the
potential of integrating generative modelling into data-driven power system
applications.

</details>


### [99] [Enhancing Variational Autoencoders with Smooth Robust Latent Encoding](https://arxiv.org/abs/2504.17219)
*Hyomin Lee,Minseon Kim,Sangwon Jang,Jongheon Jeong,Sung Ju Hwang*

Main category: cs.LG

TL;DR: SRL-VAE是一种新型对抗训练框架，通过平滑潜在空间提升生成质量和鲁棒性，同时保持原始保真度，实验表明在图像重建、文本引导编辑及对抗攻击中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法通常专注于鲁棒性，但可能损害生成模型的保真度。本研究旨在打破这一假设，通过平滑潜在空间兼顾生成质量和鲁棒性。

Method: 提出SRL-VAE框架，利用对抗扰动平滑潜在空间，并通过原始表示正则化维持保真度，作为预训练VAE的后处理步骤。

Result: 实验证明SRL-VAE在图像重建、文本引导编辑及对抗攻击（如Nightshade攻击）中显著提升生成质量和鲁棒性。

Conclusion: 研究表明对抗训练可以同时增强生成模型的保真度和鲁棒性，为生成模型鲁棒性研究提供了新范式。

Abstract: Variational Autoencoders (VAEs) have played a key role in scaling up
diffusion-based generative models, as in Stable Diffusion, yet questions
regarding their robustness remain largely underexplored. Although adversarial
training has been an established technique for enhancing robustness in
predictive models, it has been overlooked for generative models due to concerns
about potential fidelity degradation by the nature of trade-offs between
performance and robustness. In this work, we challenge this presumption,
introducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training
framework that boosts both generation quality and robustness. In contrast to
conventional adversarial training, which focuses on robustness only, our
approach smooths the latent space via adversarial perturbations, promoting more
generalizable representations while regularizing with originality
representation to sustain original fidelity. Applied as a post-training step on
pre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal
computational overhead. Experiments show that SRL-VAE improves both generation
quality, in image reconstruction and text-guided image editing, and robustness,
against Nightshade attacks and image editing attacks. These results establish a
new paradigm, showing that adversarial training, once thought to be detrimental
to generative models, can instead enhance both fidelity and robustness.

</details>


### [100] [Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification](https://arxiv.org/abs/2504.17232)
*Nivedita M,Yasmeen Shajitha S*

Main category: cs.LG

TL;DR: 提出了一个整合机器学习框架，用于高级交通分析，结合时间序列预测、分类和计算机视觉技术，实现交通预测、事故严重性分类和图像分类，并在智能城市系统中部署。


<details>
  <summary>Details</summary>
Motivation: 旨在通过集成多种机器学习技术提升交通分析的准确性和实用性，支持智能城市系统中的实时监控和事故预防。

Method: 使用ARIMA(2,0,1)模型进行交通预测（MAE: 2.1），XGBoost分类器用于事故严重性分类（平衡数据上100%准确率），以及CNN用于交通图像分类（92%准确率）。

Result: 框架在多样化数据集上表现优于基线模型，并识别出事故严重性的关键影响因素（如天气和道路基础设施）。

Conclusion: 模块化设计支持智能城市系统部署，有助于实时监控、事故预防和资源优化，推动智能交通系统的发展。

Abstract: This study proposes an integrated machine learning framework for advanced
traffic analysis, combining time-series forecasting, classification, and
computer vision techniques. The system utilizes an ARIMA(2,0,1) model for
traffic prediction (MAE: 2.1), an XGBoost classifier for accident severity
classification (100% accuracy on balanced data), and a Convolutional Neural
Network (CNN) for traffic image classification (92% accuracy). Tested on
diverse datasets, the framework outperforms baseline models and identifies key
factors influencing accident severity, including weather and road
infrastructure. Its modular design supports deployment in smart city systems
for real-time monitoring, accident prevention, and resource optimization,
contributing to the evolution of intelligent transportation systems.

</details>


### [101] [NeuralGrok: Accelerate Grokking by Neural Gradient Transformation](https://arxiv.org/abs/2504.17243)
*Xinyu Zhou,Simin Fan,Martin Jaggi,Jie Fu*

Main category: cs.LG

TL;DR: 提出NeuralGrok，一种基于梯度的方法，通过学习最优梯度变换加速Transformer在算术任务中的泛化，并通过辅助模块和双层优化算法动态调整梯度，显著提升泛化速度和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究Grokking现象，即在长时间过拟合后实现泛化，探索加速Transformer在算术任务中泛化的方法。

Method: 提出NeuralGrok，结合辅助模块（如MLP块）和双层优化算法，动态调制梯度分量对泛化的贡献。

Result: 实验表明NeuralGrok显著加速泛化，提升训练稳定性，并通过新指标AGE验证其降低模型复杂度的效果。

Conclusion: NeuralGrok为Transformer的Grokking现象提供新见解，促进对泛化能力的深入理解。

Abstract: Grokking is proposed and widely studied as an intricate phenomenon in which
generalization is achieved after a long-lasting period of overfitting. In this
work, we propose NeuralGrok, a novel gradient-based approach that learns an
optimal gradient transformation to accelerate the generalization of
transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary
module (e.g., an MLP block) in conjunction with the base model. This module
dynamically modulates the influence of individual gradient components based on
their contribution to generalization, guided by a bilevel optimization
algorithm. Our extensive experiments demonstrate that NeuralGrok significantly
accelerates generalization, particularly in challenging arithmetic tasks. We
also show that NeuralGrok promotes a more stable training paradigm, constantly
reducing the model's complexity, while traditional regularization methods, such
as weight decay, can introduce substantial instability and impede
generalization. We further investigate the intrinsic model complexity
leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that
NeuralGrok effectively facilitates generalization by reducing the model
complexity. We offer valuable insights on the grokking phenomenon of
Transformer models, which encourages a deeper understanding of the fundamental
principles governing generalization ability.

</details>


### [102] [Targeted AMP generation through controlled diffusion with efficient embeddings](https://arxiv.org/abs/2504.17247)
*Diogo Soares,Leon Hetzel,Paulina Szymczak,Fabian Theis,Stephan Günnemann,Ewa Szczurek*

Main category: cs.LG

TL;DR: OmegAMP是一个基于扩散模型的深度学习框架，用于高效生成具有特定属性的抗菌肽（AMPs），显著提高了实验命中率并降低了假阳性率。


<details>
  <summary>Details</summary>
Motivation: 传统的抗菌肽发现方法实验命中率低且缺乏对肽属性的精确控制，OmegAMP旨在解决这些问题。

Method: 采用扩散生成模型、低维嵌入、精确控制机制和新型分类器框架，以实现多样性和数据分布一致性。

Result: OmegAMP在抗菌肽发现流程中表现出最先进的性能，显著提升了计算框架在对抗抗菌素耐药性中的潜力。

Conclusion: OmegAMP通过先进的可控生成和高效过滤机制推进了抗菌肽的发现，为对抗抗菌素耐药性提供了有力工具。

Abstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical
challenges such as low experimental hit rates as well as the need for nuanced
controllability and efficient modeling of peptide properties. To address these
challenges, we introduce OmegAMP, a framework that leverages a diffusion-based
generative model with efficient low-dimensional embeddings, precise
controllability mechanisms, and novel classifiers with drastically reduced
false positive rates for candidate filtering. OmegAMP enables the targeted
generation of AMPs with specific physicochemical properties, activity profiles,
and species-specific effectiveness. Moreover, it maximizes sample diversity
while ensuring faithfulness to the underlying data distribution during
generation. We demonstrate that OmegAMP achieves state-of-the-art performance
across all stages of the AMP discovery pipeline, significantly advancing the
potential of computational frameworks in combating antimicrobial resistance.

</details>


### [103] [Group Downsampling with Equivariant Anti-aliasing](https://arxiv.org/abs/2504.17258)
*Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 该论文研究了在等变网络（如G-CNNs）中通用下采样层的泛化问题，提出了一种在有限群上下采样信号（特征图）并抗锯齿的方法。通过算法选择子群并研究带限性，该方法在图像分类任务中提升了精度、更好地保持了等变性并减少了模型大小。


<details>
  <summary>Details</summary>
Motivation: 研究目的是泛化均匀下采样层以适用于群等变架构，旨在增加高维特征的感受野并减少模型的内存/计算量。

Method: 提出了一种算法来选择子群，并研究了带限性和抗锯齿方法，推广了基于经典采样理论的下采样概念。

Result: 实验表明，所提出的下采样操作在图像分类任务中提高了精度，更好地保持了等变性，并减少了模型大小。

Conclusion: 该方法成功地泛化了经典下采样操作，适用于群等变网络，并在实验中验证了其有效性。

Abstract: Downsampling layers are crucial building blocks in CNN architectures, which
help to increase the receptive field for learning high-level features and
reduce the amount of memory/computation in the model. In this work, we study
the generalization of the uniform downsampling layer for group equivariant
architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature
maps) on general finite groups with anti-aliasing. This involves the following:
(a) Given a finite group and a downsampling rate, we present an algorithm to
form a suitable choice of subgroup. (b) Given a group and a subgroup, we study
the notion of bandlimited-ness and propose how to perform anti-aliasing.
Notably, our method generalizes the notion of downsampling based on classical
sampling theory. When the signal is on a cyclic group, i.e., periodic, our
method recovers the standard downsampling of an ideal low-pass filter followed
by a subsampling operation. Finally, we conducted experiments on image
classification tasks demonstrating that the proposed downsampling operation
improves accuracy, better preserves equivariance, and reduces model size when
incorporated into G-equivariant networks

</details>


### [104] [Symbolic Representation for Any-to-Any Generative Tasks](https://arxiv.org/abs/2504.17261)
*Jiaqi Chen,Xiaoye Zhu,Yue Wang,Tianyang Liu,Xinhui Chen,Ying Chen,Chak Tou Leong,Yifei Ke,Joseph Liu,Yiwen Yuan,Julian McAuley,Li-jia Li*

Main category: cs.LG

TL;DR: 提出了一种符号化生成任务描述语言和推理引擎，通过结构化符号流程表示多模态任务，无需训练且高效灵活。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型依赖大规模训练和隐式神经表示，计算成本高且灵活性有限。本文旨在通过显式符号表示解决这些问题。

Method: 采用符号化表示（函数、参数、拓扑逻辑）和预训练语言模型，将自然语言指令直接映射为符号化工作流，无需任务特定调整。

Result: 在12种多模态生成任务中表现优异，性能匹配或超越现有统一模型，同时具备更高的效率、可编辑性和可中断性。

Conclusion: 符号化任务表示为生成AI能力的提升提供了成本低、可扩展的基础。

Abstract: We propose a symbolic generative task description language and a
corresponding inference engine capable of representing arbitrary multimodal
tasks as structured symbolic flows. Unlike conventional generative models that
rely on large-scale training and implicit neural representations to learn
cross-modal mappings, often at high computational cost and with limited
flexibility, our framework introduces an explicit symbolic representation
comprising three core primitives: functions, parameters, and topological logic.
Leveraging a pre-trained language model, our inference engine maps natural
language instructions directly to symbolic workflows in a training-free manner.
Our framework successfully performs over 12 diverse multimodal generative
tasks, demonstrating strong performance and flexibility without the need for
task-specific tuning. Experiments show that our method not only matches or
outperforms existing state-of-the-art unified models in content quality, but
also offers greater efficiency, editability, and interruptibility. We believe
that symbolic task representations provide a cost-effective and extensible
foundation for advancing the capabilities of generative AI.

</details>


### [105] [Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy](https://arxiv.org/abs/2504.17274)
*Siddharth Vishwanath,Jonathan Hehir*

Main category: cs.LG

TL;DR: 该论文研究了在图数据中如何通过$ε$-边局部差分隐私恢复潜在信息，提出了一种调整统计推断程序的方法，能够近乎最优地恢复潜在位置及其几何与拓扑信息。


<details>
  <summary>Details</summary>
Motivation: 在图数据中，保护用户关系的隐私至关重要，尤其是在差分隐私下，现有的方法难以同时保护隐私并有效恢复潜在信息。论文旨在解决这一问题，扩展了隐私保护下的社区检测到更丰富的模型和推断任务。

Method: 论文利用了标准的局部差分隐私机制导致的潜在位置的几何失真，通过调整统计推断程序来恢复潜在位置。这一方法适用于广义随机点积图模型。

Result: 研究结果表明，该方法可以实现潜在位置的一致恢复，并且在局部边差分隐私约束下接近极小极大最优。此外，该方法还能一致地恢复潜在位置的几何和拓扑信息，如持续性图所示。

Conclusion: 该论文不仅扩展了隐私保护下的图数据分析方法，还提供了在保护隐私的同时进行有效推断的理论保证，适用于更广泛的模型和任务。

Abstract: We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.

</details>


### [106] [HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks](https://arxiv.org/abs/2504.17276)
*Ke-Jia Chen,Wenhui Mu,Zheng Liu*

Main category: cs.LG

TL;DR: 论文提出HeRB方法，通过减轻异质性和传递同质性知识解决GNN中的结构不平衡问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: GNN在处理图数据时面临结构不平衡的挑战，且现有方法未考虑异质性（相连节点标签或特征不同），导致效果不佳。

Method: HeRB包含两部分：1）减少异质性的增强模块（降低类间边、增加类内边）；2）同质性知识转移机制（从头部节点向尾部节点传递信息）。

Result: 实验显示HeRB在2个同质性和6个异质性基准数据集上表现优异，消融研究验证了两部分的有效性。

Conclusion: HeRB通过处理异质性和传递同质性知识，有效解决了GNN的结构不平衡问题。

Abstract: Recent research has witnessed the remarkable progress of Graph Neural
Networks (GNNs) in the realm of graph data representation. However, GNNs still
encounter the challenge of structural imbalance. Prior solutions to this
problem did not take graph heterophily into account, namely that connected
nodes process distinct labels or features, thus resulting in a deficiency in
effectiveness. Upon verifying the impact of heterophily on solving the
structural imbalance problem, we propose to rectify the heterophily first and
then transfer homophilic knowledge. To the end, we devise a method named HeRB
(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two
innovative components: 1) A heterophily-lessening augmentation module which
serves to reduce inter-class edges and increase intra-class edges; 2) A
homophilic knowledge transfer mechanism to convey homophilic information from
head nodes to tail nodes. Experimental results demonstrate that HeRB achieves
superior performance on two homophilic and six heterophilic benchmark datasets,
and the ablation studies further validate the efficacy of two proposed
components.

</details>


### [107] [ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders](https://arxiv.org/abs/2504.17277)
*Zongliang Ji,Andre Carlos Kajdacsy-Balla Amaral,Anna Goldenberg,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ExOSITO的新方法，结合了离策略学习和特权信息，以确定ICU实验室测试的最佳订单集，减少了过度订购的负担，同时确保不遗漏关键测试。


<details>
  <summary>Details</summary>
Motivation: ICU中实验室测试的过度订购增加了临床负担和成本，因此需要一种智能方法来优化测试订单，同时确保关键信息的可用性。

Method: 结合离策略学习和特权信息，通过因果赌博问题训练，利用离线数据和临床奖励函数，开发了ExOSITO方法。

Result: ExOSITO减少了成本，未遗漏重要测试订单，表现优于医生策略和先前方法。

Conclusion: ExOSITO提供了一种可解释且高效的辅助工具，帮助临床医生优化实验室测试订单。

Abstract: Ordering a minimal subset of lab tests for patients in the intensive care
unit (ICU) can be challenging. Care teams must balance between ensuring the
availability of the right information and reducing the clinical burden and
costs associated with each lab test order. Most in-patient settings experience
frequent over-ordering of lab tests, but are now aiming to reduce this burden
on both hospital resources and the environment. This paper develops a novel
method that combines off-policy learning with privileged information to
identify the optimal set of ICU lab tests to order. Our approach, EXplainable
Off-policy learning with Side Information for ICU blood Test Orders (ExOSITO)
creates an interpretable assistive tool for clinicians to order lab tests by
considering both the observed and predicted future status of each patient. We
pose this problem as a causal bandit trained using offline data and a reward
function derived from clinically-approved rules; we introduce a novel learning
framework that integrates clinical knowledge with observational data to bridge
the gap between the optimal and logging policies. The learned policy function
provides interpretable clinical information and reduces costs without omitting
any vital lab orders, outperforming both a physician's policy and prior
approaches to this practical problem.

</details>


### [108] [The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes](https://arxiv.org/abs/2504.17300)
*Wencong You,Daniel Lowd*

Main category: cs.LG

TL;DR: 论文探讨了文本分类器中的后门攻击，提出了一种更隐蔽的攻击方法AttrBkd，能够绕过人工检测并保持高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击使用的触发词通常不自然，容易被人工检测过滤，导致攻击失败。作者认为成功的攻击应使文本在有/无触发词时对人类无差别，并填补了攻击隐蔽性缺乏全面人工评估的空白。

Method: 提出AttrBkd方法，通过三种策略设计隐蔽的触发属性（如从现有基线攻击中提取细粒度属性），并进行了全面的人工评估。

Result: AttrBkd在攻击成功率上更高，且更隐蔽（人类检测率更低），证明攻击可通过自然外观绕过检测。人工标注还揭示了自动评估指标与人类判断的不一致。

Conclusion: 隐蔽的后门攻击能够有效绕过人工检测，凸显了人工评估的重要性，同时指出自动指标需进一步优化以匹配人类感知。

Abstract: Backdoor attacks on text classifiers can cause them to predict a predefined
label when a particular "trigger" is present. Prior attacks often rely on
triggers that are ungrammatical or otherwise unusual, leading to conspicuous
attacks. As a result, human annotators, who play a critical role in curating
training data in practice, can easily detect and filter out these unnatural
texts during manual inspection, reducing the risk of such attacks. We argue
that a key criterion for a successful attack is for text with and without
triggers to be indistinguishable to humans. However, prior work neither
directly nor comprehensively evaluated attack subtlety and invisibility with
human involvement. We bridge the gap by conducting thorough human evaluations
to assess attack subtlety. We also propose \emph{AttrBkd}, consisting of three
recipes for crafting subtle yet effective trigger attributes, such as
extracting fine-grained attributes from existing baseline backdoor attacks. Our
human evaluations find that AttrBkd with these baseline-derived attributes is
often more effective (higher attack success rate) and more subtle (fewer
instances detected by humans) than the original baseline backdoor attacks,
demonstrating that backdoor attacks can bypass detection by being inconspicuous
and appearing natural even upon close inspection, while still remaining
effective. Our human annotation also provides information not captured by
automated metrics used in prior work, and demonstrates the misalignment of
these metrics with human judgment.

</details>


### [109] [Machine learning-based condition monitoring of powertrains in modern electric drives](https://arxiv.org/abs/2504.17305)
*Dinan Li,Panagiotis Kakosimos,Luca Peretti*

Main category: cs.LG

TL;DR: 论文探讨了如何利用工业驱动器中的数据通过机器学习模型优化电力模块的热模型，测试了从线性模型到深度神经网络的不同方法，并评估了它们在工业嵌入式系统中的性能。


<details>
  <summary>Details</summary>
Motivation: 数字化转型的进步为工业领域带来了革命，利用数据分析和机器学习可以优化资产性能，尤其是通过电力驱动器中的数据提升工业系统的智能化。

Method: 利用现代电力驱动器中已有的数据，设计了测试台，训练并验证了热数字孪生模型，采用从传统线性模型到深度神经网络的不同方法寻找最佳解决方案。

Result: 通过多种评估指标对比了不同机器学习方法的性能，并探讨了它们在工业嵌入式系统中的适用性。

Conclusion: 研究表明，数据驱动的热模型可以有效提升电力模块的温度预测能力，深度学习模型在性能上表现优越，适合工业应用。

Abstract: The recent technological advances in digitalization have revolutionized the
industrial sector. Leveraging data analytics has now enabled the collection of
deep insights into the performance and, as a result, the optimization of
assets. Industrial drives, for example, already accumulate all the necessary
information to control electric machines. These signals include but are not
limited to currents, frequency, and temperature. Integrating machine learning
(ML) models responsible for predicting the evolution of those directly
collected or implicitly derived parameters enhances the smartness of industrial
systems even further. In this article, data already residing in most modern
electric drives has been used to develop a data-driven thermal model of a power
module. A test bench has been designed and used specifically for training and
validating the thermal digital twin undergoing various static and dynamic
operating profiles. Different approaches, from traditional linear models to
deep neural networks, have been implemented to emanate the best ML model for
estimating the case temperature of a power module. Several evaluation metrics
were then used to assess the investigated methods' performance and
implementation in industrial embedded systems.

</details>


### [110] [Class-Conditional Distribution Balancing for Group Robust Classification](https://arxiv.org/abs/2504.17314)
*Miaoyun Zhao,Qiang Zhang,Chenrong Li*

Main category: cs.LG

TL;DR: 本文提出了一种无需依赖偏差标注或预测的新方法，通过重新加权样本平衡类条件分布，有效消除虚假相关性，从而提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的偏差标注或大规模预训练模型，资源需求高且不适用于罕见领域。本文旨在解决这一问题。

Method: 通过样本重新加权策略平衡类条件分布，减少虚假因素与标签信息之间的互信息，自动突出少数群体和类。

Result: 实验表明，该方法性能优于或媲美依赖偏差监督的方法。

Conclusion: 提出了一种简单有效的鲁棒学习方法，无需偏差标注即可实现类条件分布平衡，为资源有限场景提供了解决方案。

Abstract: Spurious correlations that lead models to correct predictions for the wrong
reasons pose a critical challenge for robust real-world generalization.
Existing research attributes this issue to group imbalance and addresses it by
maximizing group-balanced or worst-group accuracy, which heavily relies on
expensive bias annotations. A compromise approach involves predicting bias
information using extensively pretrained foundation models, which requires
large-scale data and becomes impractical for resource-limited rare domains. To
address these challenges, we offer a novel perspective by reframing the
spurious correlations as imbalances or mismatches in class-conditional
distributions, and propose a simple yet effective robust learning method that
eliminates the need for both bias annotations and predictions. With the goal of
reducing the mutual information between spurious factors and label information,
our method leverages a sample reweighting strategy to achieve class-conditional
distribution balancing, which automatically highlights minority groups and
classes, effectively dismantling spurious correlations and producing a debiased
data distribution for classification. Extensive experiments and analysis
demonstrate that our approach consistently delivers state-of-the-art
performance, rivaling methods that rely on bias supervision.

</details>


### [111] [Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization](https://arxiv.org/abs/2504.17355)
*Xiaohan Huang,Dongjie Wang,Zhiyuan Ning,Ziyue Qiao,Qingqing Long,Haowei Zhu,Yi Du,Min Wu,Yuanchun Zhou,Meng Xiao*

Main category: cs.LG

TL;DR: 提出了TCTO，一种基于多智能体强化学习的框架，通过图驱动路径优化自动化特征工程，解决现有方法忽略动态依赖关系的问题。其核心创新是建模特征为节点、变换为边的交互图，动态剪枝和回溯以优化探索效率，并支持历史子图复用。实验证明其在多种数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有特征变换方法常将变换操作视为孤立步骤，忽略步骤间的动态依赖关系，导致手动成本高且效率低。TCTO旨在通过协作式强化学习动态建模与优化变换路径，以提升自动化特征工程的效果。

Method: 提出基于多智能体强化学习的TCTO框架，构建动态特征-变换交互图（节点为特征，边为变换），通过图剪枝、回溯减少冗余操作，增强探索稳定性，并复用历史高效用子图。

Result: 在多个数据集上的实验表明，TCTO能显著提升下游任务性能，通过动态剪枝和路径优化减少冗余操作，同时保持高可追溯性与适应性。

Conclusion: TCTO通过图驱动的多智能体强化学习有效解决了特征变换中的动态依赖问题，其动态交互图设计与剪枝策略为自动化特征工程提供了新方向。

Abstract: Feature transformation methods aim to find an optimal mathematical
feature-feature crossing process that generates high-value features and
improves the performance of downstream machine learning tasks. Existing
frameworks, though designed to mitigate manual costs, often treat feature
transformations as isolated operations, ignoring dynamic dependencies between
transformation steps. To address the limitations, we propose TCTO, a
collaborative multi-agent reinforcement learning framework that automates
feature engineering through graph-driven path optimization. The framework's
core innovation lies in an evolving interaction graph that models features as
nodes and transformations as edges. Through graph pruning and backtracking, it
dynamically eliminates low-impact edges, reduces redundant operations, and
enhances exploration stability. This graph also provides full traceability to
empower TCTO to reuse high-utility subgraphs from historical transformations.
To demonstrate the efficacy and adaptability of our approach, we conduct
comprehensive experiments and case studies, which show superior performance
across a range of datasets.

</details>


### [112] [Doubly Adaptive Social Learning](https://arxiv.org/abs/2504.17370)
*Marco Carpentiero,Virginia Bordignon,Vincenzo Matta,Ali H. Sayed*

Main category: cs.LG

TL;DR: 论文提出了一种双重自适应社交学习策略（A²SL），通过在社交学习中加入两个自适应阶段来应对动态变化，确保所有代理最终能正确识别真实假设。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，传统社交学习方法可能因模型或真实假设的变化而失效，导致错误决策。因此，需要一种自适应策略来跟踪这些变化。

Method: A²SL策略包含两个阶段：1) 使用随机梯度下降更新决策模型的漂移；2) 自适应更新信念以跟踪变化的真实假设。这两个阶段通过两个自适应参数调控错误概率。

Result: 理论分析和实验均表明，只要自适应参数足够小，所有代理能够一致地识别真实假设，且错误选择概率收敛到与自适应参数同阶的值。

Conclusion: A²SL策略在动态环境下表现出色，能够有效适应模型和真实假设的变化，为在线社交学习提供了可靠解决方案。

Abstract: In social learning, a network of agents assigns probability scores (beliefs)
to some hypotheses of interest, which rule the generation of local streaming
data observed by each agent. Belief formation takes place by means of an
iterative two-step procedure where: i) the agents update locally their beliefs
by using some likelihood model; and ii) the updated beliefs are combined with
the beliefs of the neighboring agents, using a pooling rule. This procedure can
fail to perform well in the presence of dynamic drifts, leading the agents to
incorrect decision making. Here, we focus on the fully online setting where
both the true hypothesis and the likelihood models can change over time. We
propose the doubly adaptive social learning ($\text{A}^2\text{SL}$) strategy,
which infuses social learning with the necessary adaptation capabilities. This
goal is achieved by exploiting two adaptation stages: i) a stochastic gradient
descent update to learn and track the drifts in the decision model; ii) and an
adaptive belief update to track the true hypothesis changing over time. These
stages are controlled by two adaptation parameters that govern the evolution of
the error probability for each agent. We show that all agents learn
consistently for sufficiently small adaptation parameters, in the sense that
they ultimately place all their belief mass on the true hypothesis. In
particular, the probability of choosing the wrong hypothesis converges to
values on the order of the adaptation parameters. The theoretical analysis is
illustrated both on synthetic data and by applying the $\text{A}^2\text{SL}$
strategy to a social learning problem in the online setting using real data.

</details>


### [113] [Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware](https://arxiv.org/abs/2504.17403)
*Hans Rosenberger,Rodrigo Fischer,Johanna S. Fröhlich,Ali Bereyhi,Ralf R. Müller*

Main category: cs.LG

TL;DR: 提出了一种通过剪枝、权重共享和线性计算编码（LCC）结合的方法，减少神经网络在FPGA等可重构硬件上的推理计算量，优化了加法操作而非传统的内存压缩。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模的增大，资源高效实现变得尤为重要，传统压缩技术关注减少内存使用，而本文专注于减少推理时的加法操作以适配硬件需求。

Method: 结合剪枝（通过正则化训练）、权重共享和线性计算编码（LCC），在硬件友好的方式下优化计算量。

Result: 方法在多层感知机和ResNet-34等大规模深度神经网络上均表现出竞争力。

Conclusion: 提出的压缩方案在减少计算量的同时保持了性能，尤其适用于硬件实现，为高效神经网络推理提供了新思路。

Abstract: As state of the art neural networks (NNs) continue to grow in size, their
resource-efficient implementation becomes ever more important. In this paper,
we introduce a compression scheme that reduces the number of computations
required for NN inference on reconfigurable hardware such as FPGAs. This is
achieved by combining pruning via regularized training, weight sharing and
linear computation coding (LCC). Contrary to common NN compression techniques,
where the objective is to reduce the memory used for storing the weights of the
NNs, our approach is optimized to reduce the number of additions required for
inference in a hardware-friendly manner. The proposed scheme achieves
competitive performance for simple multilayer perceptrons, as well as for large
scale deep NNs such as ResNet-34.

</details>


### [114] [Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks](https://arxiv.org/abs/2504.17421)
*Yang Liu,Bingjie Yan,Tianyuan Zou,Jianqing Zhang,Zixuan Gu,Jianbing Ding,Xidong Wang,Jingyi Li,Xiaozhou Ye,Ye Ouyang,Qiang Yang,Ya-Qin Zhang*

Main category: cs.LG

TL;DR: 探讨大模型与小模型协作的潜力，以高效适应私有领域并释放AI新潜能。


<details>
  <summary>Details</summary>
Motivation: 大模型需要大量数据和计算资源，而小模型虽能力有限但更高效。研究如何通过两者协作解决这一问题。

Method: 探讨多种模型协作策略，分析潜在挑战与机会。

Result: 提出行业驱动的多目标基准研究，聚焦真实私有数据集和应用。

Conclusion: 倡导大模型与小模型协同工作，推动AI在私有领域的应用与发展。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
they require vast amounts of data and computational resources. In contrast,
smaller models (SMs), while less powerful, can be more efficient and tailored
to specific domains. In this position paper, we argue that taking a
collaborative approach, where large and small models work synergistically, can
accelerate the adaptation of LLMs to private domains and unlock new potential
in AI. We explore various strategies for model collaboration and identify
potential challenges and opportunities. Building upon this, we advocate for
industry-driven research that prioritizes multi-objective benchmarks on
real-world private datasets and applications.

</details>


### [115] [CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning](https://arxiv.org/abs/2504.17448)
*Jun Zhang,Jue Wang,Huan Li,Zhongle Xie,Ke Chen,Lidan Shou*

Main category: cs.LG

TL;DR: CHASe通过考虑客户数据分布的异质性，提出了一种新的联邦主动学习方法，有效提升模型精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦主动学习方法未考虑客户数据分布的异质性及其对全局和局部模型参数的影响，导致模型精度下降。

Method: CHASe通过跟踪认知变化、校准决策边界和改进数据选择效率来优化数据选择过程。

Result: 实验表明，CHASe在多种数据集和模型复杂度下均优于其他基线方法。

Conclusion: CHASe在联邦主动学习中有效解决了数据分布异质性问题，显著提升了模型性能。

Abstract: Active learning (AL) reduces human annotation costs for machine learning
systems by strategically selecting the most informative unlabeled data for
annotation, but performing it individually may still be insufficient due to
restricted data diversity and annotation budget. Federated Active Learning
(FAL) addresses this by facilitating collaborative data selection and model
training, while preserving the confidentiality of raw data samples. Yet,
existing FAL methods fail to account for the heterogeneity of data distribution
across clients and the associated fluctuations in global and local model
parameters, adversely affecting model accuracy. To overcome these challenges,
we propose CHASe (Client Heterogeneity-Aware Data Selection), specifically
designed for FAL. CHASe focuses on identifying those unlabeled samples with
high epistemic variations (EVs), which notably oscillate around the decision
boundaries during training. To achieve both effectiveness and efficiency,
\model{} encompasses techniques for 1) tracking EVs by analyzing inference
inconsistencies across training epochs, 2) calibrating decision boundaries of
inaccurate models with a new alignment loss, and 3) enhancing data selection
efficiency via a data freeze and awaken mechanism with subset sampling.
Experiments show that CHASe surpasses various established baselines in terms of
effectiveness and efficiency, validated across diverse datasets, model
complexities, and heterogeneous federation settings.

</details>


### [116] [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
*Jun Zhang,Jue Wang,Huan Li,Lidan Shou,Ke Chen,Gang Chen,Qin Xie,Guiming Xie,Xuejian Gong*

Main category: cs.LG

TL;DR: HMI是一种基于分层知识管理的多租户推理系统，通过分层管理PLM知识（通用、领域特定和任务特定），显著降低GPU内存占用，支持单GPU高效服务多达10,000个hPLM，精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型（PLM）的高计算需求在多租户环境中效率低下，需专用硬件支持，HMI旨在通过分层知识管理实现资源高效利用。

Method: 1. 将PLM知识分为通用、领域特定和任务特定三层，构建分层PLM（hPLM）以减少内存占用；2. 通过频率构建领域知识树和参数交换管理任务知识；3. 系统优化包括分层知识预取和批量矩阵乘法并行实现。

Result: HMI在单个GPU上高效支持10,000个hBERT和hGPT模型，仅轻微影响准确性。

Conclusion: HMI通过分层知识管理和系统优化，显著提升多租户环境下PLM服务的资源效率和吞吐量。

Abstract: The significant computational demands of pretrained language models (PLMs),
which often require dedicated hardware, present a substantial challenge in
serving them efficiently, especially in multi-tenant environments. To address
this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant
Inference system, designed to manage tenants with distinct PLMs
resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM
knowledge into general, domain-specific, and task-specific. Leveraging insights
on knowledge acquisition across different model layers, we construct
hierarchical PLMs (hPLMs) by extracting and storing knowledge at different
levels, significantly reducing GPU memory usage per tenant. Secondly, we
establish hierarchical knowledge management for hPLMs generated by various
tenants in HMI. We manage domain-specific knowledge with acceptable storage
increases by constructing and updating domain-specific knowledge trees based on
frequency. We manage task-specific knowledge within limited GPU memory through
parameter swapping. Finally, we propose system optimizations to enhance
resource utilization and inference throughput. These include fine-grained
pipelining via hierarchical knowledge prefetching to overlap CPU and I/O
operations with GPU computations, and optimizing parallel implementations with
batched matrix multiplications. Our experimental results demonstrate that the
proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a
single GPU, with only a negligible compromise in accuracy.

</details>


### [117] [Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience](https://arxiv.org/abs/2504.17461)
*Vipin Singh,Tianheng Ling,Teodor Chiaburu,Felix Biessmann*

Main category: cs.LG

TL;DR: 该论文提出了一种评估神经网络架构用于城市合流制排水系统（CSS）时间序列预测的协议，重点关注预测性能、模型复杂性和抗干扰能力。研究发现全局模型预测性能更高，但局部模型在分散场景中更具韧性，适合物联网部署。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端降雨频率增加，给城市合流制排水系统带来巨大压力，传统物理模型成本高且难以适应动态变化，因此研究机器学习方法作为高效替代方案。

Method: 提出了一种评估神经网络的协议，比较全局模型和局部模型的性能，并引入误差模型测试抗干扰能力。

Result: 全局模型预测性能更好，但局部模型在分散部署时更具韧性；长预测时域模型对数据扰动更鲁棒。

Conclusion: 研究为城市废水管理提供了可解释且可靠的机器学习解决方案，适合可持续发展和物联网部署。

Abstract: Climate change increases the frequency of extreme rainfall, placing a
significant strain on urban infrastructures, especially Combined Sewer Systems
(CSS). Overflows from overburdened CSS release untreated wastewater into
surface waters, posing environmental and public health risks. Although
traditional physics-based models are effective, they are costly to maintain and
difficult to adapt to evolving system dynamics. Machine Learning (ML)
approaches offer cost-efficient alternatives with greater adaptability. To
systematically assess the potential of ML for modeling urban infrastructure
systems, we propose a protocol for evaluating Neural Network architectures for
CSS time series forecasting with respect to predictive performance, model
complexity, and robustness to perturbations. In addition, we assess model
performance on peak events and critical fluctuations, as these are the key
regimes for urban wastewater management. To investigate the feasibility of
lightweight models suitable for IoT deployment, we compare global models, which
have access to all information, with local models, which rely solely on nearby
sensor readings. Additionally, to explore the security risks posed by network
outages or adversarial attacks on urban infrastructure, we introduce error
models that assess the resilience of models. Our results demonstrate that while
global models achieve higher predictive performance, local models provide
sufficient resilience in decentralized scenarios, ensuring robust modeling of
urban infrastructure. Furthermore, models with longer native forecast horizons
exhibit greater robustness to data perturbations. These findings contribute to
the development of interpretable and reliable ML solutions for sustainable
urban wastewater management. The implementation is available in our GitHub
repository.

</details>


### [118] [GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework](https://arxiv.org/abs/2504.17471)
*Yacine Belal,Mohamed Maouche,Sonia Ben Mokhtar,Anthony Simonet-Boulogne*

Main category: cs.LG

TL;DR: Gossip Learning (GL) 依赖动态通信图实现快速收敛，但对拜占庭攻击的鲁棒性不足。GRANITE 框架通过历史感知拜占庭抵抗对等采样协议和自适应概率阈值，在稀疏动态图中实现鲁棒学习。


<details>
  <summary>Details</summary>
Motivation: 研究动态图中 GL 对拜占庭攻击的脆弱性，尤其是攻击者通过操纵对等采样协议扩大模型投毒的情况。

Method: 提出 GRANITE 框架，结合 HaPS 协议（历史感知拜占庭抵抗对等采样）和 APT（自适应概率阈值），通过跟踪历史标识和动态调整聚合阈值来抵御攻击。

Result: 实验表明，GRANITE 在 30% 拜占庭节点下仍能保持收敛，提升学习速度，并在比当前理论稀疏 9 倍的图中有效。

Conclusion: GRANITE 通过历史和自适应机制，显著提升了稀疏动态图中 GL 的鲁棒性和效率。

Abstract: Gossip Learning (GL) is a decentralized learning paradigm where users
iteratively exchange and aggregate models with a small set of neighboring
peers. Recent GL approaches rely on dynamic communication graphs built and
maintained using Random Peer Sampling (RPS) protocols. Thanks to graph
dynamics, GL can achieve fast convergence even over extremely sparse
topologies. However, the robustness of GL over dy- namic graphs to Byzantine
(model poisoning) attacks remains unaddressed especially when Byzantine nodes
attack the RPS protocol to scale up model poisoning. We address this issue by
introducing GRANITE, a framework for robust learning over sparse, dynamic
graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two
key components (i) a History-aware Byzantine-resilient Peer Sampling protocol
(HaPS), which tracks previously encountered identifiers to reduce adversarial
influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which
leverages an estimate of Byzantine presence to set aggregation thresholds with
formal guarantees. Empirical results confirm that GRANITE maintains convergence
with up to 30% Byzantine nodes, improves learning speed via adaptive filtering
of poisoned models and obtains these results in up to 9 times sparser graphs
than dictated by current theory.

</details>


### [119] [Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning](https://arxiv.org/abs/2504.17490)
*Mingqi Yuan,Qi Wang,Guozheng Ma,Bo Li,Xin Jin,Yunbo Wang,Xiaokang Yang,Wenjun Zeng,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出Plasticine框架，用于评估深度强化学习中的塑性优化，包含13种缓解方法、10种评估指标及渐进非平稳学习场景。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习系统因塑性损失导致适应能力下降的问题，并提供统一基准和评估协议。

Method: 开发开源的Plasticine框架，集成多种缓解方法、评估指标及学习场景。

Result: Plasticine成为首个系统性量化塑性损失、评估缓解策略并分析塑性动态的工具。

Conclusion: Plasticine为研究人员提供了一个标准化平台，推动了塑性优化领域的进一步研究。

Abstract: Developing lifelong learning agents is crucial for artificial general
intelligence. However, deep reinforcement learning (RL) systems often suffer
from plasticity loss, where neural networks gradually lose their ability to
adapt during training. Despite its significance, this field lacks unified
benchmarks and evaluation protocols. We introduce Plasticine, the first
open-source framework for benchmarking plasticity optimization in deep RL.
Plasticine provides single-file implementations of over 13 mitigation methods,
10 evaluation metrics, and learning scenarios with increasing non-stationarity
levels from standard to open-ended environments. This framework enables
researchers to systematically quantify plasticity loss, evaluate mitigation
strategies, and analyze plasticity dynamics across different contexts. Our
documentation, examples, and source code are available at
https://github.com/RLE-Foundation/Plasticine.

</details>


### [120] [Prototype-enhanced prediction in graph neural networks for climate applications](https://arxiv.org/abs/2504.17492)
*Nawid Keshtmand,Elena Fillola,Jeffrey Nicholas Clark,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.LG

TL;DR: 论文提出使用原型（近似模拟器输出）作为输入来提升高维模拟器输出质量的方法，应用于温室气体排放监测中的大气扩散模拟，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管数据驱动的模拟器能减少计算成本，但其输出质量仍需提升。通过引入原型作为额外输入，旨在优化模拟器的预测准确性。

Method: 比较基线模型与使用原型作为附加输入的模型，原型可通过随机选择或数据驱动方法（如k-means）获取。

Result: 原型模型表现更优，即使原型数量少或随机选择，且k-means选原型能使某些指标性能提升近10%。

Conclusion: 原型方法能有效提升模拟器性能，数据驱动的原型选择策略进一步优化结果。

Abstract: Data-driven emulators are increasingly being used to learn and emulate
physics-based simulations, reducing computational expense and run time. Here,
we present a structured way to improve the quality of these high-dimensional
emulated outputs, through the use of prototypes: an approximation of the
emulator's output passed as an input, which informs the model and leads to
better predictions. We demonstrate our approach to emulate atmospheric
dispersion, key for greenhouse gas emissions monitoring, by comparing a
baseline model to models trained using prototypes as an additional input. The
prototype models achieve better performance, even with few prototypes and even
if they are chosen at random, but we show that choosing the prototypes through
data-driven methods (k-means) can lead to almost 10\% increased performance in
some metrics.

</details>


### [121] [Goal-Oriented Time-Series Forecasting: Foundation Framework Design](https://arxiv.org/abs/2504.17493)
*Luca-Andrei Fechete,Mohamed Sana,Fadhel Ayed,Nicola Piovesan,Wenjie Li,Antonio De Domenico,Tareq Si Salem*

Main category: cs.LG

TL;DR: 论文提出了一种新的时间序列预测训练方法，能够根据实际应用需求动态调整预测范围，提升预测准确性及应用性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测仅关注最小化预测误差，忽视了实际应用中预测范围的特定需求。

Method: 将整个信号范围的预测分解为小段，动态加权组合以生成准确预测。

Result: 在标准数据集及无线通信新数据集中测试，不仅提升了预测准确性，还改善了应用性能。

Conclusion: 为创建更紧密连接预测与决策的预测系统提供了基础。

Abstract: Traditional time-series forecasting often focuses only on minimizing
prediction errors, ignoring the specific requirements of real-world
applications that employ them. This paper presents a new training methodology,
which allows a forecasting model to dynamically adjust its focus based on the
importance of forecast ranges specified by the end application. Unlike previous
methods that fix these ranges beforehand, our training approach breaks down
predictions over the entire signal range into smaller segments, which are then
dynamically weighted and combined to produce accurate forecasts. We tested our
method on standard datasets, including a new dataset from wireless
communication, and found that not only it improves prediction accuracy but also
improves the performance of end application employing the forecasting model.
This research provides a basis for creating forecasting systems that better
connect prediction and decision-making in various practical applications.

</details>


### [122] [Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening](https://arxiv.org/abs/2504.17497)
*Radia Berreziga,Mohammed Brahimi,Khairedine Kraim,Hamid Azzoune*

Main category: cs.LG

TL;DR: 该论文提出了一种混合架构，结合图卷积网络（GCN）和大语言模型（LLM）嵌入，以在药物发现中通过局部结构学习和全局化学知识的集成实现更优的虚拟筛选性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法如SVM和XGBoost依赖预定义的分子表示，可能导致信息丢失和偏差。而深度学习（如GCN）能够直接处理分子图，但缺乏全局化学知识。作者提出将GCN与LLM嵌入结合，以弥补各自的不足。

Method: 提出了一种混合架构，将GCN与预训练的LLM嵌入结合。LLM嵌入可预先计算并存储，避免重复计算。关键创新是在每个GCN层后拼接LLM嵌入，而非仅在最终层，以更深入地集成全局上下文。

Result: 混合模型F1-score达到88.8％，优于独立GCN（87.9％）、XGBoost（85.5％）和SVM（85.4％）基线。

Conclusion: 通过结合GCN的局部结构学习和LLM的全局化学知识，混合架构显著提高了虚拟筛选的性能，为药物发现提供了更高效和准确的工具。

Abstract: Virtual screening plays a critical role in modern drug discovery by enabling
the identification of promising candidate molecules for experimental
validation. Traditional machine learning methods such as support vector
machines (SVM) and XGBoost rely on predefined molecular representations, often
leading to information loss and potential bias. In contrast, deep learning
approaches-particularly Graph Convolutional Networks (GCNs)-offer a more
expressive and unbiased alternative by operating directly on molecular graphs.
Meanwhile, Large Language Models (LLMs) have recently demonstrated
state-of-the-art performance in drug design, thanks to their capacity to
capture complex chemical patterns from large-scale data via attention
mechanisms.
  In this paper, we propose a hybrid architecture that integrates GCNs with
LLM-derived embeddings to combine localized structural learning with global
chemical knowledge. The LLM embeddings can be precomputed and stored in a
molecular feature library, removing the need to rerun the LLM during training
or inference and thus maintaining computational efficiency. We found that
concatenating the LLM embeddings after each GCN layer-rather than only at the
final layer-significantly improves performance, enabling deeper integration of
global context throughout the network. The resulting model achieves superior
results, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),
XGBoost (85.5%), and SVM (85.4%) baselines.

</details>


### [123] [Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data](https://arxiv.org/abs/2504.17503)
*Davide Prosperino,Haochun Ma,Christoph Räth*

Main category: cs.LG

TL;DR: 研究了数据非线性程度对储备池计算机设计的影响，发现预测性能最优时储备池的非线性与数据的非线性匹配，并提出了一种估计未知时间序列最小非线性的方法。


<details>
  <summary>Details</summary>
Motivation: 探索储备池计算机的非线性设计如何与输入数据的非线性程度相匹配，以提高预测性能。

Method: 通过将最小储备池简化为单一可调非线性参数，研究预测性能如何随储备池非线性变化；提出估计最小非线性的方法，并将其应用于传统储备池架构。

Result: 预测性能在储备池非线性与数据非线性匹配时最佳；对于含多重非线性的数据，匹配最小非线性可正确重建信号相关维度。

Conclusion: 为储备池计算机设计提供了依据，使其能够根据目标系统的内在复杂性进行优化，特别是在资源受限的情况下表现优异。

Abstract: We study how the degree of nonlinearity in the input data affects the optimal
design of reservoir computers, focusing on how closely the model's nonlinearity
should align with that of the data. By reducing minimal RCs to a single tunable
nonlinearity parameter, we explore how the predictive performance varies with
the degree of nonlinearity in the reservoir. To provide controlled testbeds, we
generalize to the fractional Halvorsen system, a novel chaotic system with
fractional exponents. Our experiments reveal that the prediction performance is
maximized when the reservoir's nonlinearity matches the nonlinearity present in
the data. In cases where multiple nonlinearities are present in the data, we
find that the correlation dimension of the predicted signal is reconstructed
correctly when the smallest nonlinearity is matched. We use this observation to
propose a method for estimating the minimal nonlinearity in unknown time series
by sweeping the reservoir exponent and identifying the transition to a
successful reconstruction. Applying this method to both synthetic and
real-world datasets, including financial time series, we demonstrate its
practical viability. Finally, we transfer these insights to classical RC by
augmenting traditional architectures with fractional, generalized reservoir
states. This yields performance gains, particularly in resource-constrained
scenarios such as physical reservoirs, where increasing reservoir size is
impractical or economically unviable. Our work provides a principled route
toward tailoring RCs to the intrinsic complexity of the systems they aim to
model.

</details>


### [124] [Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity](https://arxiv.org/abs/2504.17520)
*Zhuojun Tian,Zhaoyang Zhang,Yiwei Li,Mehdi Bennis*

Main category: cs.LG

TL;DR: 论文提出了一种基于分布式强彩票假设（DSLTH）的高效通信个性化学习算法，通过固定全局实值参数并更新个性化二值掩码来解决去中心化学习中的数据与节点异构性问题，并结合分组稀疏正则化与掩码聚合算法优化硬件实现。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习中存在数据和节点异构性的挑战，需要一种既能高效通信又满足个性化需求的学习方法。

Method: 固定全局实值参数，通过个性化二值掩码修剪本地模型，加入分组稀疏正则化，并设计掩码聚合算法以实现结构化稀疏和本地数据适应的模型更新。

Result: 数值模拟验证了DSLTH的有效性，并展示了所提算法在异构节点条件下的优越性能。

Conclusion: 提出的方法成功平衡了代理间的关联性与个性化需求，为异构环境下的去中心化学习提供了高效解决方案。

Abstract: To jointly tackle the challenges of data and node heterogeneity in
decentralized learning, we propose a distributed strong lottery ticket
hypothesis (DSLTH), based on which a communication-efficient personalized
learning algorithm is developed. In the proposed method, each local model is
represented as the Hadamard product of global real-valued parameters and a
personalized binary mask for pruning. The local model is learned by updating
and fusing the personalized binary masks while the real-valued parameters are
fixed among different agents. To further reduce the complexity of hardware
implementation, we incorporate a group sparse regularization term in the loss
function, enabling the learned local model to achieve structured sparsity.
Then, a binary mask aggregation algorithm is designed by introducing an
intermediate aggregation tensor and adding a personalized fine-tuning step in
each iteration, which constrains model updates towards the local data
distribution. The proposed method effectively leverages the relativity among
agents while meeting personalized requirements in heterogeneous node
conditions. We also provide a theoretical proof for the DSLTH, establishing it
as the foundation of the proposed method. Numerical simulations confirm the
validity of the DSLTH and demonstrate the effectiveness of the proposed
algorithm.

</details>


### [125] [Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks](https://arxiv.org/abs/2504.17526)
*Yuelin Liu,Haiyuan Li,Xenofon Vasilakos,Rasheed Hussain,Dimitra Simeonidou*

Main category: cs.LG

TL;DR: 论文提出了CTO-TP框架，通过异步多智能体深度强化学习优化边缘计算任务卸载和资源分配，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 6G和万物互联的发展导致计算资源需求激增，传统单边缘服务器卸载策略存在资源利用不均和性能瓶颈问题，需要更高效的解决方案。

Method: 采用异步多智能体深度强化学习，结合Transformer驱动的预测，实现边缘间协作和异步训练，优化任务卸载与资源分配。

Result: CTO-TP相比基准方案，系统延迟降低80%，能耗减少87%。

Conclusion: CTO-TP框架通过协作和异步训练，有效解决了边缘计算中的延迟和能耗问题，为未来网络提供了高效任务卸载方案。

Abstract: Future networks (including 6G) are poised to accelerate the realisation of
Internet of Everything. However, it will result in a high demand for computing
resources to support new services. Mobile Edge Computing (MEC) is a promising
solution, enabling to offload computation-intensive tasks to nearby edge
servers from the end-user devices, thereby reducing latency and energy
consumption. However, relying solely on a single MEC server for task offloading
can lead to uneven resource utilisation and suboptimal performance in complex
scenarios. Additionally, traditional task offloading strategies specialise in
centralised policy decisions, which unavoidably entail extreme transmission
latency and reach computational bottleneck. To fill the gaps, we propose a
latency and energy efficient Cooperative Task Offloading framework with
Transformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent
deep reinforcement learning to address these challenges. This approach fosters
edge-edge cooperation and decreases the synchronous waiting time by performing
asynchronous training, optimising task offloading, and resource allocation
across distributed networks. The performance evaluation demonstrates that the
proposed CTO-TP algorithm reduces up to 80% overall system latency and 87%
energy consumption compared to the baseline schemes.

</details>


### [126] [TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction](https://arxiv.org/abs/2504.17528)
*Weijie Liu,Ziwei Zhan,Carlee Joe-Wong,Edith Ngai,Jingpu Duan,Deke Guo,Xu Chen,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 传统联邦学习方法因统一的模型修正系数导致过校正现象，影响模型性能与收敛。本文提出的TACO算法通过细粒度、客户端特定的梯度修正和聚合，解决了非独立同分布数据问题，并减少了计算开销。实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在处理非独立同分布数据时，因采用统一的模型修正系数导致过校正现象，影响模型收敛与性能，亟需一种更精细的解决方案。

Method: 提出TACO算法，通过客户端特定的梯度修正和轻量级聚合方法，减少过校正并降低计算开销，无需额外信息。

Result: 理论分析与实验表明，TACO在模型准确性和训练效率上优于现有方法，且收敛更稳定。

Conclusion: TACO有效解决了非独立同分布数据导致的过校正问题，为联邦学习提供了高效、稳定的优化方案。

Abstract: Non-independent and identically distributed (Non-IID) data across edge
clients have long posed significant challenges to federated learning (FL)
training in edge computing environments. Prior works have proposed various
methods to mitigate this statistical heterogeneity. While these works can
achieve good theoretical performance, in this work we provide the first
investigation into a hidden over-correction phenomenon brought by the uniform
model correction coefficients across clients adopted by existing methods. Such
over-correction could degrade model performance and even cause failures in
model convergence. To address this, we propose TACO, a novel algorithm that
addresses the non-IID nature of clients' data by implementing fine-grained,
client-specific gradient correction and model aggregation, steering local
models towards a more accurate global optimum. Moreover, we verify that leading
FL algorithms generally have better model accuracy in terms of communication
rounds rather than wall-clock time, resulting from their extra computation
overhead imposed on clients. To enhance the training efficiency, TACO deploys a
lightweight model correction and tailored aggregation approach that requires
minimum computation overhead and no extra information beyond the synchronized
model parameters. To validate TACO's effectiveness, we present the first FL
convergence analysis that reveals the root cause of over-correction. Extensive
experiments across various datasets confirm TACO's superior and stable
performance in practice.

</details>


### [127] [Learning Isometric Embeddings of Road Networks using Multidimensional Scaling](https://arxiv.org/abs/2504.17534)
*Juan Carlos Climent Pardo*

Main category: cs.LG

TL;DR: 论文提出使用图表示和多维缩放（MDS）技术来提升自动驾驶任务中基于学习的运动规划器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的自动驾驶应用泛化能力不足，无法覆盖多样化的道路场景。

Method: 利用图表示道路网络，并采用多维缩放（MDS）技术构建特征空间。

Result: 分析了先进的图表示和MDS方法在自动驾驶中的应用，提出嵌入图节点以简化学习和降维。

Conclusion: 该研究为设计泛化的自动驾驶运动规划器提供了一种新的方法。

Abstract: The lack of generalization in learning-based autonomous driving applications
is shown by the narrow range of road scenarios that vehicles can currently
cover. A generalizable approach should capture many distinct road structures
and topologies, as well as consider traffic participants, and dynamic changes
in the environment, so that vehicles can navigate and perform motion planning
tasks even in the most difficult situations. Designing suitable feature spaces
for neural network-based motion planers that encapsulate all kinds of road
scenarios is still an open research challenge. This paper tackles this
learning-based generalization challenge and shows how graph representations of
road networks can be leveraged by using multidimensional scaling (MDS)
techniques in order to obtain such feature spaces. State-of-the-art graph
representations and MDS approaches are analyzed for the autonomous driving use
case. Finally, the option of embedding graph nodes is discussed in order to
perform easier learning procedures and obtain dimensionality reduction.

</details>


### [128] [Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis](https://arxiv.org/abs/2504.17568)
*Ivan Rossi,Flavio Sartori,Cesare Rollo,Giovanni Birolo,Piero Fariselli,Tiziana Sanavia*

Main category: cs.LG

TL;DR: 该研究评估了放松线性比例风险假设的机器学习和深度学习方法，在真实和合成数据集上比较了它们与惩罚Cox模型的性能，并提出了更合适的评估指标（如Antolini's C-index和Brier's score）来全面评估生存分析方法。


<details>
  <summary>Details</summary>
Motivation: 传统的Cox模型依赖于线性和比例风险假设，限制了其在复杂数据中的应用。本研究旨在探索更灵活的机器学习和深度学习方法，并评估它们在非线性或非比例风险情况下的表现。

Method: 研究测试了八种模型（包括六种非线性模型，其中四种是非比例风险的），在三个合成和三个真实数据集上进行基准测试，并使用Antolini's C-index和Brier's score作为评估指标。

Result: 尽管Cox回归通常表现良好，但研究揭示了在特定条件下（如样本量、非线性和非比例风险），机器学习和深度学习方法可能更优。此外，使用Antolini's C-index和Brier's score能更全面地评估模型性能。

Conclusion: 生存分析应根据数据特性（如样本量、非线性或非比例风险）选择合适的方法。研究提供了代码和文档以便复现测试。

Abstract: Survival analysis often relies on Cox models, assuming both linearity and
proportional hazards (PH). This study evaluates machine and deep learning
methods that relax these constraints, comparing their performance with
penalized Cox models on a benchmark of three synthetic and three real datasets.
In total, eight different models were tested, including six non-linear models
of which four were also non-PH. Although Cox regression often yielded
satisfactory performance, we showed the conditions under which machine and deep
learning models can perform better. Indeed, the performance of these methods
has often been underestimated due to the improper use of Harrell's concordance
index (C-index) instead of more appropriate scores such as Antolini's
concordance index, which generalizes C-index in cases where the PH assumption
does not hold. In addition, since occasionally high C-index models happen to be
badly calibrated, combining Antolini's C-index with Brier's score is useful to
assess the overall performance of a survival method. Results on our benchmark
data showed that survival prediction should be approached by testing different
methods to select the most appropriate one according to sample size,
non-linearity and non-PH conditions. To allow an easy reproducibility of these
tests on our benchmark data, code and documentation are freely available at
https://github.com/compbiomed-unito/survhive.

</details>


### [129] [TileLang: A Composable Tiled Programming Model for AI Systems](https://arxiv.org/abs/2504.17577)
*Lei Wang,Yu Cheng,Yining Shi,Zhengju Tang,Zhiwen Mo,Wenhao Xie,Lingxiao Ma,Yuqing Xia,Jilong Xue,Fan Yang,Zhi Yang*

Main category: cs.LG

TL;DR: TileLang是一个通用的分块编程模型，旨在简化AI内核编程，通过解耦调度空间与数据流，提升性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代AI内核编程复杂且硬件优化繁琐，现有编译工具在易用性和表达能力上存在不足，TileLang试图解决这些问题。

Method: TileLang将调度空间（线程绑定、布局、张量化、流水线）与数据流解耦，并通过注解和原语封装优化，让开发者专注数据流本身。

Result: 在多种设备上的实验表明，TileLang在关键内核中实现了最先进的性能，验证了其统一块线程范式的高效性。

Conclusion: TileLang通过透明调度能力和灵活性，满足了现代AI系统开发的需求，显著提升了内核编程效率。

Abstract: Modern AI workloads rely heavily on optimized computing kernels for both
training and inference. These AI kernels follow well-defined data-flow
patterns, such as moving tiles between DRAM and SRAM and performing a sequence
of computations on those tiles. However, writing high-performance kernels
remains complex despite the clarity of these patterns. Achieving peak
performance requires careful, hardware-centric optimizations to fully leverage
modern accelerators. While domain-specific compilers attempt to reduce the
burden of writing high-performance kernels, they often struggle with usability
and expressiveness gaps. In this paper, we present TileLang, a generalized
tiled programming model for more efficient AI Kernel programming. TileLang
decouples scheduling space (thread binding, layout, tensorize and pipeline)
from dataflow, and encapsulated them as a set of customization annotations and
primitives. This approach allows users to focus on the kernel's data-flow
itself, while leaving most other optimizations to compilers. We conduct
comprehensive experiments on commonly-used devices, across numerous
experiments, our evaluation shows that TileLang can achieve state-of-the-art
performance in key kernels, demonstrating that its unified block-and-thread
paradigm and transparent scheduling capabilities deliver both the power and
flexibility demanded by modern AI system development.

</details>


### [130] [Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization](https://arxiv.org/abs/2504.17578)
*Hongshu Guo,Wenjie Qiu,Zeyuan Ma,Xinglin Zhang,Jun Zhang,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: 本文提出了LCC，一种基于学习的合作协同进化框架，通过神经网络动态选择分解策略，提高了大规模全局优化问题的解决效率和资源利用，并展示了良好的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有合作协同进化（CC）方法需要专家知识选择或设计变量分解策略，限制了其广泛应用。本文受元黑盒优化启发，旨在通过自动学习动态调度分解策略，降低对专家知识的依赖。

Method: LCC框架利用神经网络参数化分解策略选择器，通过精心设计的优化状态特征集动态选择最优策略。网络采用近端策略优化方法，在代表性问题上以强化学习方式训练，以最大化优化性能。

Result: 实验表明，LCC在优化效果和资源消耗上优于现有基线方法，并展现出对未见过问题的良好迁移能力。

Conclusion: LCC通过动态学习分解策略，不仅提升了优化性能，还减少了对专家知识的依赖，具有广泛的实际应用潜力。

Abstract: Recent research in Cooperative Coevolution~(CC) have achieved promising
progress in solving large-scale global optimization problems. However, existing
CC paradigms have a primary limitation in that they require deep expertise for
selecting or designing effective variable decomposition strategies. Inspired by
advancements in Meta-Black-Box Optimization, this paper introduces LCC, a
pioneering learning-based cooperative coevolution framework that dynamically
schedules decomposition strategies during optimization processes. The
decomposition strategy selector is parameterized through a neural network,
which processes a meticulously crafted set of optimization status features to
determine the optimal strategy for each optimization step. The network is
trained via the Proximal Policy Optimization method in a reinforcement learning
manner across a collection of representative problems, aiming to maximize the
expected optimization performance. Extensive experimental results demonstrate
that LCC not only offers certain advantages over state-of-the-art baselines in
terms of optimization effectiveness and resource consumption, but it also
exhibits promising transferability towards unseen problems.

</details>


### [131] [Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation](https://arxiv.org/abs/2504.17601)
*Erik Bergh*

Main category: cs.LG

TL;DR: 该论文提出了一种新型降维方法，结合线性方法的可解释性和非线性变换的表达能力，通过高斯加权的线性变换实现复杂非线性映射，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法（如t-SNE和PCA）在表达能力和可解释性之间存在权衡，本文旨在填补这一空白，提供既强大又透明的降维解决方案。

Method: 提出一种算法，通过高斯加权的线性变换构建高维与低维空间之间的非线性映射，每个变换可独立分析，保留线性方法的可解释性。

Result: 该方法既能实现强大的降维效果，又能提供对变换空间的透明洞察，包括识别 suppressed 维度以及空间扩张/收缩的机制。

Conclusion: 通过用户友好的软件包推广，该算法有望在学术界和工业界得到广泛应用，兼具实用性和解释性优势。

Abstract: Dimensionality reduction techniques are fundamental for analyzing and
visualizing high-dimensional data. With established methods like t-SNE and PCA
presenting a trade-off between representational power and interpretability.
This paper introduces a novel approach that bridges this gap by combining the
interpretability of linear methods with the expressiveness of non-linear
transformations. The proposed algorithm constructs a non-linear mapping between
high-dimensional and low-dimensional spaces through a combination of linear
transformations, each weighted by Gaussian functions. This architecture enables
complex non-linear transformations while preserving the interpretability
advantages of linear methods, as each transformation can be analyzed
independently. The resulting model provides both powerful dimensionality
reduction and transparent insights into the transformed space. Techniques for
interpreting the learned transformations are presented, including methods for
identifying suppressed dimensions and how space is expanded and contracted.
These tools enable practitioners to understand how the algorithm preserves and
modifies geometric relationships during dimensionality reduction. To ensure the
practical utility of this algorithm, the creation of user-friendly software
packages is emphasized, facilitating its adoption in both academia and
industry.

</details>


### [132] [TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation](https://arxiv.org/abs/2504.17613)
*Bowen Deng,Chang Xu,Hao Li,Yuhao Huang,Min Hou,Jiang Bian*

Main category: cs.LG

TL;DR: 提出TarDiff，一种目标导向的扩散框架，用于生成优化的合成EHR数据，提升下游模型性能，解决了数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅复制真实数据的统计分布，无法保证模型性能提升，尤其是稀有但重要的情况。需要生成能优化特定临床模型性能的合成数据。

Method: 采用目标导向的扩散框架（TarDiff），通过在反向扩散过程中嵌入任务特定的影响梯度，量化合成样本对下游模型性能的提升贡献。

Result: 在六个公开EHR数据集上，TarDiff在AUPRC和AUROC指标上分别提升了20.4%和18.4%，优于现有方法。

Conclusion: TarDiff不仅保持时间保真度，还显著提升下游模型性能，为医疗数据分析中的数据稀缺和类别不平衡提供了有效解决方案。

Abstract: Synthetic Electronic Health Record (EHR) time-series generation is crucial
for advancing clinical machine learning models, as it helps address data
scarcity by providing more training data. However, most existing approaches
focus primarily on replicating statistical distributions and temporal
dependencies of real-world data. We argue that fidelity to observed data alone
does not guarantee better model performance, as common patterns may dominate,
limiting the representation of rare but important conditions. This highlights
the need for generate synthetic samples to improve performance of specific
clinical models to fulfill their target outcomes. To address this, we propose
TarDiff, a novel target-oriented diffusion framework that integrates
task-specific influence guidance into the synthetic data generation process.
Unlike conventional approaches that mimic training data distributions, TarDiff
optimizes synthetic samples by quantifying their expected contribution to
improving downstream model performance through influence functions.
Specifically, we measure the reduction in task-specific loss induced by
synthetic samples and embed this influence gradient into the reverse diffusion
process, thereby steering the generation towards utility-optimized data.
Evaluated on six publicly available EHR datasets, TarDiff achieves
state-of-the-art performance, outperforming existing methods by up to 20.4% in
AUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only
preserves temporal fidelity but also enhances downstream model performance,
offering a robust solution to data scarcity and class imbalance in healthcare
analytics.

</details>


### [133] [Decentralized Time Series Classification with ROCKET Features](https://arxiv.org/abs/2504.17617)
*Bruno Casella,Matthias Jakobs,Marco Aldinucci,Sebastian Buschjäger*

Main category: cs.LG

TL;DR: DROCKS是一个完全去中心化的联邦学习框架，用于时间序列分类（TSC），解决了传统客户-服务器架构中的单点故障和数据隐私问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和数据法规，联邦学习成为处理分布式时间序列数据的理想方法，但传统客户-服务器架构存在单点故障和隐私泄露风险。

Method: DROCKS采用去中心化架构，利用ROCKET特征，通过节点间的结构化路径训练全局模型，每个节点优化模型并选择最有效的本地核传递至后继节点。

Result: 在UCR存档上的实验表明，DROCKS在性能上优于现有的客户-服务器联邦学习方法，且对节点故障和恶意攻击更具弹性。

Conclusion: DROCKS为解决分布式时间序列分类中的隐私和鲁棒性问题提供了一种有效的去中心化解决方案。

Abstract: Time series classification (TSC) is a critical task with applications in
various domains, including healthcare, finance, and industrial monitoring. Due
to privacy concerns and data regulations, Federated Learning has emerged as a
promising approach for learning from distributed time series data without
centralizing raw information. However, most FL solutions rely on a
client-server architecture, which introduces robustness and confidentiality
risks related to the distinguished role of the server, which is a single point
of failure and can observe knowledge extracted from clients. To address these
challenges, we propose DROCKS, a fully decentralized FL framework for TSC that
leverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,
the global model is trained by sequentially traversing a structured path across
federation nodes, where each node refines the model and selects the most
effective local kernels before passing them to the successor. Extensive
experiments on the UCR archive demonstrate that DROCKS outperforms
state-of-the-art client-server FL approaches while being more resilient to node
failures and malicious attacks. Our code is available at
https://anonymous.4open.science/r/DROCKS-7FF3/README.md.

</details>


### [134] [The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks](https://arxiv.org/abs/2504.17618)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 该论文研究了神经网络Hessian矩阵特征值谱密度（HESD）的类型及其对泛化能力的影响，提出了统一的分析方法，并探讨了训练过程中HESD的变化及其影响。


<details>
  <summary>Details</summary>
Motivation: 研究Hessian矩阵特征值谱密度（HESD）类型与神经网络泛化能力的关系，以验证之前提出的泛化准则的适用性，并探究影响HESD类型的因素。

Method: 通过多种优化器、数据集及预处理方法的实验，分析HESD类型（主要为正或主要为负）及其成因，并提出判定HESD类型的条件和方法。

Result: 实验表明，HESD主要为正值（MP-HESD）是训练和微调中的常见现象，而负值主导（MN-HESD）则由外部梯度操作引起。同时发现了准奇异（QS）HESD现象及其对分析方法的影响。

Conclusion: 论文提出了统一的HESD分析方法，并指出在不同情况下HESD类型的变化会影响泛化能力的评估，为Hessian矩阵分析提供了更全面的视角。

Abstract: Hessians of neural network (NN) contain essential information about the
curvature of NN loss landscapes which can be used to estimate NN generalization
capabilities. We have previously proposed generalization criteria that rely on
the observation that Hessian eigenvalue spectral density (HESD) behaves
similarly for a wide class of NNs. This paper further studies their
applicability by investigating factors that can result in different types of
HESD. We conduct a wide range of experiments showing that HESD mainly has
positive eigenvalues (MP-HESD) for NN training and fine-tuning with various
optimizers on different datasets with different preprocessing and augmentation
procedures. We also show that mainly negative HESD (MN-HESD) is a consequence
of external gradient manipulation, indicating that the previously proposed
Hessian analysis methodology cannot be applied in such cases. We also propose
criteria and corresponding conditions to determine HESD type and estimate NN
generalization potential. These HESD types and previously proposed
generalization criteria are combined into a unified HESD analysis methodology.
Finally, we discuss how HESD changes during training, and show the occurrence
of quasi-singular (QS) HESD and its influence on the proposed methodology and
on the conventional assumptions about the relation between Hessian eigenvalues
and NN loss landscape curvature.

</details>


### [135] [PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph](https://arxiv.org/abs/2504.17641)
*Shengtao Zhang,Haokai Zhang,Shiqi Lou,Zicheng Wang,Zinan Zeng,Yilin Wang,Minnan Luo*

Main category: cs.LG

TL;DR: 论文提出了一种名为PTCL的方法，用于解决动态节点分类中仅能获取最终标签的问题，通过伪标签和时间课程学习策略提升性能，并提出了统一框架FLiD。


<details>
  <summary>Details</summary>
Motivation: 动态节点分类在实际场景中难以获取所有时间戳的标签（标注成本高、标签不确定性），而最终时间戳的标签更容易获得，因此需要一种方法利用最终标签解决动态分类问题。

Method: PTCL通过时间解耦架构（分离主干网络和解码器）生成伪标签，并结合时间课程学习策略（为接近最终时间戳的伪标签分配更高权重）优化模型。

Result: 实验表明PTCL在多个真实场景中优于其他方法，同时提出了新数据集CoOAG和统一框架FLiD。

Conclusion: PTCL有效解决了标签有限的动态节点分类问题，并通过框架FLiD为研究提供了标准化工具。

Abstract: Dynamic node classification is critical for modeling evolving systems like
financial transactions and academic collaborations. In such systems,
dynamically capturing node information changes is critical for dynamic node
classification, which usually requires all labels at every timestamp. However,
it is difficult to collect all dynamic labels in real-world scenarios due to
high annotation costs and label uncertainty (e.g., ambiguous or delayed labels
in fraud detection). In contrast, final timestamp labels are easier to obtain
as they rely on complete temporal patterns and are usually maintained as a
unique label for each user in many open platforms, without tracking the history
data. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum
Learning), a pioneering method addressing label-limited dynamic node
classification where only final labels are available. PTCL introduces: (1) a
temporal decoupling architecture separating the backbone (learning time-aware
representations) and decoder (strictly aligned with final labels), which
generate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that
prioritizes pseudo-labels closer to the final timestamp by assigning them
higher weights using an exponentially decaying function. We contribute a new
academic dataset (CoOAG), capturing long-range research interest in dynamic
graph. Experiments across real-world scenarios demonstrate PTCL's consistent
superiority over other methods adapted to this task. Beyond methodology, we
propose a unified framework FLiD (Framework for Label-Limited Dynamic Node
Classification), consisting of a complete preparation workflow, training
pipeline, and evaluation standards, and supporting various models and datasets.
The code can be found at https://github.com/3205914485/FLiD.

</details>


### [136] [Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction](https://arxiv.org/abs/2504.17655)
*Farhad Pourkamali-Anaraki*

Main category: cs.LG

TL;DR: 该论文通过实证分析了在复杂真实环境中应用共形预测方法的效果，重点研究了预训练模型与小样本标注数据的结合、校准技术的影响，并探讨了模型压缩的潜力。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺且高度变化的真实环境中，标准评估方法难以适用。本研究旨在验证共形预测方法在这种复杂场景中的有效性，并探索其在不确定性估计和模型压缩中的应用潜力。

Method: 采用预训练模型（MobileNet、DenseNet、ResNet）进行微调，结合共形预测生成预测集。通过对比有无温度缩放的校准流程，评估覆盖率和预测集大小的权衡关系。

Result: 即使标注数据较少，共形预测仍能提供有价值的复杂任务不确定性估计。温度缩放未必减小预测集，需谨慎使用。模型压缩技术在实际部署中表现出较大潜力。

Conclusion: 未来研究应关注噪声/模糊标签对共形预测的影响，并探索高效的模型精简策略。

Abstract: This paper presents a comprehensive empirical analysis of conformal
prediction methods on a challenging aerial image dataset featuring diverse
events in unconstrained environments. Conformal prediction is a powerful
post-hoc technique that takes the output of any classifier and transforms it
into a set of likely labels, providing a statistical guarantee on the coverage
of the true label. Unlike evaluations on standard benchmarks, our study
addresses the complexities of data-scarce and highly variable real-world
settings. We investigate the effectiveness of leveraging pretrained models
(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to
generate informative prediction sets. To further evaluate the impact of
calibration, we consider two parallel pipelines (with and without temperature
scaling) and assess performance using two key metrics: empirical coverage and
average prediction set size. This setup allows us to systematically examine how
calibration choices influence the trade-off between reliability and efficiency.
Our findings demonstrate that even with relatively small labeled samples and
simple nonconformity scores, conformal prediction can yield valuable
uncertainty estimates for complex tasks. Moreover, our analysis reveals that
while temperature scaling is often employed for calibration, it does not
consistently lead to smaller prediction sets, underscoring the importance of
careful consideration in its application. Furthermore, our results highlight
the significant potential of model compression techniques within the conformal
prediction pipeline for deployment in resource-constrained environments. Based
on our observations, we advocate for future research to delve into the impact
of noisy or ambiguous labels on conformal prediction performance and to explore
effective model reduction strategies.

</details>


### [137] [Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models](https://arxiv.org/abs/2504.17660)
*Julius Vetter,Manuel Gloeckler,Daniel Gedon,Jakob H. Macke*

Main category: cs.LG

TL;DR: 本文提出了一种名为NPE-PF的新方法，利用预训练的表格基础模型（如TabPFN）进行基于模拟的贝叶斯推理（SBI），显著提高了模拟效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决SBI中模拟成本高的问题，利用预训练模型减少所需模拟次数，同时保持或提升推理准确性。

Method: 提出NPE-PF方法，使用TabPFN作为预训练的自回归条件密度估计器，进行后验估计，无需选择、训练或调参。

Result: NPE-PF在多个任务中表现优异，模拟效率显著提升，有时模拟次数减少数量级，且对模型错误设定更鲁棒。

Conclusion: NPE-PF为SBI提供了一条新路径，通过免训练、通用推理模型为广泛的随机逆问题提供高效、易用且灵活的解决方案。

Abstract: Simulation-based inference (SBI) offers a flexible and general approach to
performing Bayesian inference: In SBI, a neural network is trained on synthetic
data simulated from a model and used to rapidly infer posterior distributions
for observed data. A key goal for SBI is to achieve accurate inference with as
few simulations as possible, especially for expensive simulators. In this work,
we address this challenge by repurposing recent probabilistic foundation models
for tabular data: We show how tabular foundation models -- specifically TabPFN
-- can be used as pre-trained autoregressive conditional density estimators for
SBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks
(NPE-PF) and show that it is competitive with current SBI approaches in terms
of accuracy for both benchmark tasks and two complex scientific inverse
problems. Crucially, it often substantially outperforms them in terms of
simulation efficiency, sometimes requiring orders of magnitude fewer
simulations. NPE-PF eliminates the need for inference network selection,
training, and hyperparameter tuning. We also show that it exhibits superior
robustness to model misspecification and can be scaled to simulation budgets
that exceed the context size limit of TabPFN. NPE-PF provides a new direction
for SBI, where training-free, general-purpose inference models offer efficient,
easy-to-use, and flexible solutions for a wide range of stochastic inverse
problems.

</details>


### [138] [On Multivariate Financial Time Series Classification](https://arxiv.org/abs/2504.17664)
*Grégory Bournassenko*

Main category: cs.LG

TL;DR: 对比了机器学习和深度学习在金融时间序列分析中的应用，探讨了小数据与大数据的挑战和优势，并比较了传统方法（如SVM）与现代架构（如ConvTimeNet）的效果。结果强调了大数据在金融时间序列预测中的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索机器学习和深度学习在金融市场的多变量时间序列分析中的表现，尤其是大数据和小数据方法的差异及其应用价值。

Method: 通过对比传统方法（如支持向量机SVM）和现代深度学习架构（如ConvTimeNet），分析了其在金融时间序列数据上的性能。

Result: 研究发现，大数据方法在金融时间序列分析和预测中具有显著优势，强调了对大数据深入理解和应用的重要性。

Conclusion: 结论指出现代深度学习方法（如ConvTimeNet）在大规模金融时间序列数据分析中表现优异，大数据技术的应用是未来研究的关键方向。

Abstract: This article investigates the use of Machine Learning and Deep Learning
models in multivariate time series analysis within financial markets. It
compares small and big data approaches, focusing on their distinct challenges
and the benefits of scaling. Traditional methods such as SVMs are contrasted
with modern architectures like ConvTimeNet. The results show the importance of
using and understanding Big Data in depth in the analysis and prediction of
financial time series.

</details>


### [139] [Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence](https://arxiv.org/abs/2504.17703)
*Edward Collins,Michel Wang*

Main category: cs.LG

TL;DR: 联邦学习（FL）是一种分布式机器学习范式，允许多方在保护数据隐私的前提下协作训练共享模型。本文综述了FL的核心架构、技术挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决数据隐私、安全性和合规性需求，适用于医疗、金融和物联网等领域。

Method: 介绍FL的生命周期、技术挑战（如非独立同分布数据处理、隐私保护机制）和新兴趋势（如个性化FL）。

Result: 总结了FL的应用场景、评估指标和开放研究问题。

Conclusion: FL在隐私保护和分布式学习方面具有巨大潜力，未来发展需解决可扩展性、效率和信任问题。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field
of distributed machine learning, enabling multiple clients such as mobile
devices, edge nodes, or organizations to collaboratively train a shared global
model without the need to centralize sensitive data. This decentralized
approach addresses growing concerns around data privacy, security, and
regulatory compliance, making it particularly attractive in domains such as
healthcare, finance, and smart IoT systems. This survey provides a concise yet
comprehensive overview of Federated Learning, beginning with its core
architecture and communication protocol. We discuss the standard FL lifecycle,
including local training, model aggregation, and global updates. A particular
emphasis is placed on key technical challenges such as handling non-IID
(non-independent and identically distributed) data, mitigating system and
hardware heterogeneity, reducing communication overhead, and ensuring privacy
through mechanisms like differential privacy and secure aggregation.
Furthermore, we examine emerging trends in FL research, including personalized
FL, cross-device versus cross-silo settings, and integration with other
paradigms such as reinforcement learning and quantum computing. We also
highlight real-world applications and summarize benchmark datasets and
evaluation metrics commonly used in FL research. Finally, we outline open
research problems and future directions to guide the development of scalable,
efficient, and trustworthy FL systems.

</details>


### [140] [Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation](https://arxiv.org/abs/2504.17709)
*Stefan Jonas,Angela Meyer*

Main category: cs.LG

TL;DR: 论文提出了一种基于CycleGAN的深度学习方法，通过域映射技术，将缺乏训练数据的风力涡轮机SCADA数据映射到具有代表性数据的涡轮机域上，从而改善训练数据稀缺条件下的异常检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统的风电机组智能监控依赖大量训练数据，但实际中许多新装机组数据稀缺，导致基于数据驱动的正常行为模型（NBM）不可靠。本文旨在解决训练数据不足时的故障诊断问题。

Method: 采用CycleGAN进行域映射，将数据稀缺的风电机组SCADA数据映射到数据充足的机组域上，从而复用已有的NB模型。验证了7台差异显著的风电机组间的数据映射效果。

Result: 实验表明，该方法在1个月或2周训练数据下，F1分数分别提升10.3%和16.8%，且在所有数据稀缺场景（1-8周）下均优于传统微调方法。

Conclusion: 通过域映射技术，可显著提升数据稀缺条件下的故障诊断可靠性，为风电异常检测提供新方向。

Abstract: Intelligent condition monitoring of wind turbines is essential for reducing
downtimes. Machine learning models trained on wind turbine operation data are
commonly used to detect anomalies and, eventually, operation faults. However,
data-driven normal behavior models (NBMs) require a substantial amount of
training data, as NBMs trained with scarce data may result in unreliable fault
diagnosis. To overcome this limitation, we present a novel generative deep
learning approach to make SCADA samples from one wind turbine lacking training
data resemble SCADA data from wind turbines with representative training data.
Through CycleGAN-based domain mapping, our method enables the application of an
NBM trained on an existing wind turbine to one with severely limited data. We
demonstrate our approach on field data mapping SCADA samples across 7
substantially different WTs. Our findings show significantly improved fault
diagnosis in wind turbines with scarce data. Our method achieves the most
similar anomaly scores to an NBM trained with abundant data, outperforming NBMs
trained on scarce training data with improvements of +10.3% in F1-score when 1
month of training data is available and +16.8% when 2 weeks are available. The
domain mapping approach outperforms conventional fine-tuning at all considered
degrees of data scarcity, ranging from 1 to 8 weeks of training data. The
proposed technique enables earlier and more reliable fault diagnosis in newly
installed wind farms, demonstrating a novel and promising research direction to
improve anomaly detection when faced with training data scarcity.

</details>


### [141] [Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations](https://arxiv.org/abs/2504.17717)
*Óscar Escudero-Arnanz,Antonio G. Marques,Inmaculada Mora-Jiménez,Joaquín Álvarez-Rodríguez,Cristina Soguero-Ruiz*

Main category: cs.LG

TL;DR: 该研究提出了一个可解释的机器学习框架，用于预测多药耐药性（MDR），通过患者相似性度量和图分析方法，提升了预测准确性和临床解释性。


<details>
  <summary>Details</summary>
Motivation: 多药耐药性（MDR）是全球健康的重要问题，导致住院时间、医疗费用和死亡率增加。研究旨在通过可解释的机器学习方法，实现准确的MDR预测并为临床决策提供支持。

Method: 将患者建模为多元时间序列（MTS），使用动态时间规整（DTW）和时间聚类核等度量患者相似性，结合逻辑回归、随机森林和支持向量机进行分类。通过谱聚类和t-SNE可视化高风险集群。

Result: 该框架在ICU电子健康记录上的AUC达到81%，优于基线模型，并识别出抗生素使用、侵入性操作等关键风险因素和临床相关集群。

Conclusion: 患者相似性结合图分析方法不仅提高了MDR预测的准确性，还提供了可解释的临床见解，支持早期检测和患者分层。

Abstract: Background and Objectives: Multidrug Resistance (MDR) is a critical global
health issue, causing increased hospital stays, healthcare costs, and
mortality. This study proposes an interpretable Machine Learning (ML) framework
for MDR prediction, aiming for both accurate inference and enhanced
explainability.
  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing
clinical progression and patient-to-patient interactions. Similarity among
patients is quantified using MTS-based methods: descriptive statistics, Dynamic
Time Warping, and Time Cluster Kernel. These similarity measures serve as
inputs for MDR classification via Logistic Regression, Random Forest, and
Support Vector Machines, with dimensionality reduction and kernel
transformations improving model performance. For explainability, patient
similarity networks are constructed from these metrics. Spectral clustering and
t-SNE are applied to identify MDR-related subgroups and visualize high-risk
clusters, enabling insight into clinically relevant patterns.
  Results: The framework was validated on ICU Electronic Health Records from
the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms
baseline ML and deep learning models by leveraging graph-based patient
similarity. The approach identifies key risk factors -- prolonged antibiotic
use, invasive procedures, co-infections, and extended ICU stays -- and reveals
clinically meaningful clusters. Code and results are available at
\https://github.com/oscarescuderoarnanz/DM4MTS.
  Conclusions: Patient similarity representations combined with graph-based
analysis provide accurate MDR prediction and interpretable insights. This
method supports early detection, risk factor identification, and patient
stratification, highlighting the potential of explainable ML in critical care.

</details>


### [142] [Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees](https://arxiv.org/abs/2504.17721)
*Cheng Shen,Yuewei Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于统计校准的方法，用于提升钢铁表面缺陷检测的可靠性。通过定义损失函数和风险级别，确保测试集上的预期错误率受控，同时验证了方法的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统钢铁表面缺陷检测依赖人工或深度学习方法（如Mask R-CNN），但存在效率低、成本高和模型不可靠（如过拟合和标注不确定性）的问题。

Method: 使用独立同分布（i.i.d）校准数据评估模型性能，定义损失函数量化错误率，并通过统计阈值构建预测集（如缺陷区域），确保测试集错误率受控。

Result: 方法能严格限制测试集错误率，且预测集大小与风险级别呈负相关，验证了模型不确定性的统计严谨性和方法适应性。

Conclusion: 提出的统计校准方法显著提升了缺陷检测的可靠性和效率，适用于工业场景中的实际应用。

Abstract: In industrial settings, surface defects on steel can significantly compromise
its service life and elevate potential safety risks. Traditional defect
detection methods predominantly rely on manual inspection, which suffers from
low efficiency and high costs. Although automated defect detection approaches
based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,
their reliability remains challenged due to data annotation uncertainties
during deep model training and overfitting issues. These limitations may lead
to detection deviations when processing the given new test samples, rendering
automated detection processes unreliable. To address this challenge, we first
evaluate the detection model's practical performance through calibration data
that satisfies the independent and identically distributed (i.i.d) condition
with test data. Specifically, we define a loss function for each calibration
sample to quantify detection error rates, such as the complement of recall rate
and false discovery rate. Subsequently, we derive a statistically rigorous
threshold based on a user-defined risk level to identify high-probability
defective pixels in test images, thereby constructing prediction sets (e.g.,
defect regions). This methodology ensures that the expected error rate (mean
error rate) on the test set remains strictly bounced by the predefined risk
level. Additionally, we observe a negative correlation between the average
prediction set size and the risk level on the test set, establishing a
statistically rigorous metric for assessing detection model uncertainty.
Furthermore, our study demonstrates robust and efficient control over the
expected test set error rate across varying calibration-to-test partitioning
ratios, validating the method's adaptability and operational effectiveness.

</details>


### [143] [Towards Robust LLMs: an Adversarial Robustness Measurement Framework](https://arxiv.org/abs/2504.17723)
*Natan Levy,Adiel Ashrov,Guy Katz*

Main category: cs.LG

TL;DR: 论文提出了RoMA框架，用于量化大语言模型（LLM）对抗扰动输入的鲁棒性，无需模型参数。RoMA在计算效率和准确性上表现优异，并揭示了模型鲁棒性在不同任务和扰动类型间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在人工智能领域有广泛应用，但其对抗扰动的脆弱性影响其在高风险场景中的可靠性。现有研究多集中于视觉神经网络，对LLM鲁棒性的探讨不足。

Method: 作者将RoMA框架应用于LLM鲁棒性评估，通过比对其与形式化验证方法的评估结果，验证RoMA的准确性与效率。

Result: 实证显示，模型鲁棒性在不同模型、任务类别及扰动类型间存在显著差异。RoMA能高效且准确地量化这一特性。

Conclusion: 研究为LLM提供了任务特定的鲁棒性评估方法，支持开发者根据应用需求选择模型，推动更可靠语言模型的开发。

Abstract: The rise of Large Language Models (LLMs) has revolutionized artificial
intelligence, yet these models remain vulnerable to adversarial perturbations,
undermining their reliability in high-stakes applications. While adversarial
robustness in vision-based neural networks has been extensively studied, LLM
robustness remains under-explored. We adapt the Robustness Measurement and
Assessment (RoMA) framework to quantify LLM resilience against adversarial
inputs without requiring access to model parameters. By comparing RoMA's
estimates to those of formal verification methods, we demonstrate its accuracy
with minimal error margins while maintaining computational efficiency. Our
empirical evaluation reveals that robustness varies significantly not only
between different models but also across categories within the same task and
between various types of perturbations. This non-uniformity underscores the
need for task-specific robustness evaluations, enabling practitioners to
compare and select models based on application-specific robustness
requirements. Our work provides a systematic methodology to assess LLM
robustness, advancing the development of more reliable language models for
real-world deployment.

</details>


### [144] [Interpretable Early Detection of Parkinson's Disease through Speech Analysis](https://arxiv.org/abs/2504.17739)
*Lorenzo Simone,Mauro Giuseppe Camporeale,Vito Marco Rubino,Vincenzo Gervasi,Giovanni Dimauro*

Main category: cs.LG

TL;DR: 使用深度学习从语音记录中早期检测帕金森病，并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的早期症状之一是语音障碍，通过机器学习可以及时检测，提供诊断机会。

Method: 提出深度学习方法，从语音记录中识别疾病，并关联预测性语音模式与发音特征。

Result: 在意大利帕金森语音和语言数据库中评估，分类性能优于现有方法，并增强了预测的可解释性。

Conclusion: 该方法不仅展示了竞争力，还能识别影响预测的关键语音特征，为理解神经肌肉损伤提供了依据。

Abstract: Parkinson's disease is a progressive neurodegenerative disorder affecting
motor and non-motor functions, with speech impairments among its earliest
symptoms. Speech impairments offer a valuable diagnostic opportunity, with
machine learning advances providing promising tools for timely detection. In
this research, we propose a deep learning approach for early Parkinson's
disease detection from speech recordings, which also highlights the vocal
segments driving predictions to enhance interpretability. This approach seeks
to associate predictive speech patterns with articulatory features, providing a
basis for interpreting underlying neuromuscular impairments. We evaluated our
approach using the Italian Parkinson's Voice and Speech Database, containing
831 audio recordings from 65 participants, including both healthy individuals
and patients. Our approach showed competitive classification performance
compared to state-of-the-art methods, while providing enhanced interpretability
by identifying key speech features influencing predictions.

</details>


### [145] [Embedding Empirical Distributions for Computing Optimal Transport Maps](https://arxiv.org/abs/2504.17740)
*Mingchen Jiang,Peng Xu,Xichen Ye,Xiaohui Chen,Yun Yang,Yifan Chen*

Main category: cs.LG

TL;DR: TL;DR


<details>
  <summary>Details</summary>
Motivation: 动机

Method: 方法

Result: 结果

Conclusion: 结论

Abstract: Distributional data have become increasingly prominent in modern signal
processing, highlighting the necessity of computing optimal transport (OT) maps
across multiple probability distributions. Nevertheless, recent studies on
neural OT methods predominantly focused on the efficient computation of a
single map between two distributions. To address this challenge, we introduce a
novel approach to learning transport maps for new empirical distributions.
Specifically, we employ the transformer architecture to produce embeddings from
distributional data of varying length; these embeddings are then fed into a
hypernetwork to generate neural OT maps. Various numerical experiments were
conducted to validate the embeddings and the generated OT maps. The model
implementation and the code are provided on
https://github.com/jiangmingchen/HOTET.

</details>


### [146] [MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction](https://arxiv.org/abs/2504.17749)
*Steven E. Wilson,Sina Khanmohammadi*

Main category: cs.LG

TL;DR: 本文提出了一种名为MSGCN的新方法，用于多层网络中跨层链接权重的预测，解决了现有GNN在此任务上的不足，并验证了其高效性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在多种学习任务中表现优异，但链接权重预测这一复杂任务（尤其是多层网络中的跨层链接）尚未得到足够重视。本文旨在填补这一研究空白。

Method: 提出了Multiplex Spatial Graph Convolution Network（MSGCN），通过空间嵌入多层信息来预测跨层链接权重，扩展了空间图卷积在多层网络中的应用。

Result: 实验表明，MSGCN在多种多层网络结构中均展现出鲁棒、准确且可泛化的链接权重预测性能。

Conclusion: MSGCN为多层网络中的链接权重预测提供了有效解决方案，并在复杂场景中表现出优越性能。

Abstract: Graph Neural Networks (GNNs) have been widely used for various learning
tasks, ranging from node classification to link prediction. They have
demonstrated excellent performance in multiple domains involving
graph-structured data. However, an important category of learning tasks, namely
link weight prediction, has received less emphasis due to its increased
complexity compared to binary link classification. Link weight prediction
becomes even more challenging when considering multilayer networks, where nodes
can be interconnected across multiple layers. To address these challenges, we
propose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),
which spatially embeds information across multiple layers to predict interlayer
link weights. The MSGCN model generalizes spatial graph convolution to
multiplex networks and captures the geometric structure of nodes across
multiple layers. Extensive experiments using data with known interlayer link
information show that the MSGCN model has robust, accurate, and generalizable
link weight prediction performance across a wide variety of multiplex network
structures.

</details>


### [147] [Disaggregated Deep Learning via In-Physics Computing at Radio Frequency](https://arxiv.org/abs/2504.17752)
*Zhihui Gao,Sri Krishna Vadlamani,Kfir Sulimany,Dirk Englund,Tingjun Chen*

Main category: cs.LG

TL;DR: WISE架构通过无线广播和射频计算实现能效优化的深度学习推理，显著提升边缘设备的计算效率。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，传统数字计算架构难以满足实时深度学习推理的高能耗需求，需新型计算架构解决此问题。

Method: 采用无线广播传输模型权重，并结合射频端的复数矩阵向量乘法计算，实现能效优化。

Result: 实验显示WISE实现95.7%图像分类准确率，能耗仅6.0 fJ/MAC，计算效率达165.8 TOPS/W。

Conclusion: WISE架构显著提升了边缘设备深度学习推理的能效，比传统数字计算效率高出两个数量级。

Abstract: Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,
rely on deep learning to enable a wide range of intelligent applications,
including object recognition, environment perception, and autonomous
navigation. However, deploying deep learning models directly on the often
resource-constrained edge devices demands significant memory footprints and
computational power for real-time inference using traditional digital computing
architectures. In this paper, we present WISE, a novel computing architecture
for wireless edge networks designed to overcome energy constraints in deep
learning inference. WISE achieves this goal through two key innovations:
disaggregated model access via wireless broadcasting and in-physics computation
of general complex-valued matrix-vector multiplications directly at radio
frequency. Using a software-defined radio platform with wirelessly broadcast
model weights over the air, we demonstrate that WISE achieves 95.7% image
classification accuracy with ultra-low operation power of 6.0 fJ/MAC per
client, corresponding to a computation efficiency of 165.8 TOPS/W. This
approach enables energy-efficient deep learning inference on wirelessly
connected edge devices, achieving more than two orders of magnitude improvement
in efficiency compared to traditional digital computing.

</details>


### [148] [Replay to Remember: Retaining Domain Knowledge in Streaming Language Models](https://arxiv.org/abs/2504.17780)
*Sneh Pillai*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级方法，结合了LoRA和最小回放机制，在计算和数据流限制下实现大型语言模型的实时领域适应，有效缓解了灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在持续学习中常面临灾难性遗忘的挑战，现有方法在严格的计算和数据流限制下的实时领域适应研究较少。

Method: 结合LoRA（低秩适应）和最小回放机制，在医学问答、遗传学和法律三个领域的数据流中进行实验。

Result: 实验表明，即使是最小的回放也能显著稳定模型并部分恢复领域知识，尽管灾难性遗忘依然存在。

Conclusion: 该研究为在资源受限的实际场景中部署适应性强的语言模型提供了实用见解。

Abstract: Continual learning in large language models (LLMs) typically encounters the
critical challenge of catastrophic forgetting, where previously acquired
knowledge deteriorates upon exposure to new data. While techniques like replay
buffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have
been proposed, few studies investigate real-time domain adaptation under strict
computational and data-stream constraints. In this paper, we demonstrate a
lightweight method combining LoRA and a minimal replay mechanism in a realistic
streaming setting across three diverse knowledge domains: medical question
answering, genetics, and law. Using perplexity, semantic similarity, and
GPT-based human-like evaluation metrics, we quantify the model's adaptation,
forgetting, and recovery over time. Our experiments reveal that while
catastrophic forgetting naturally occurs, even minimal replay significantly
stabilizes and partially restores domain-specific knowledge. This study
contributes practical insights for deploying adaptable LLMs in
resource-constrained, real-world scenarios.

</details>


### [149] [A Novel Graph Transformer Framework for Gene Regulatory Network Inference](https://arxiv.org/abs/2504.16961)
*Binon Teji,Swarup Roy*

Main category: cs.LG

TL;DR: 本文提出了一种通过整合多网络信息和深度学习技术（如自编码器、BERT和图变换器）来改进基因调控网络（GRN）推断的方法GT-GRN，显著提高了推断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统GRN推断方法易受噪声影响且难以反映真实生物调控关系，因此需要结合先验知识、位置信息和多网络整合以提高推断效果。

Method: 利用自编码器从原始数据中捕获基因表达模式，通过随机游走和BERT编码先验知识生成基因全局嵌入，再结合位置编码和图变换器模型GT-GRN进行推断。

Result: GT-GRN在实验中表现优于现有方法，展示了更高的准确性和鲁棒性。

Conclusion: 整合多网络信息和深度学习技术能显著提升GRN推断效果，GT-GRN为未来研究提供了新方向。

Abstract: The inference of gene regulatory networks (GRNs) is a foundational stride
towards deciphering the fundamentals of complex biological systems. Inferring a
possible regulatory link between two genes can be formulated as a link
prediction problem. Inference of GRNs via gene coexpression profiling data may
not always reflect true biological interactions, as its susceptibility to noise
and misrepresenting true biological regulatory relationships. Most GRN
inference methods face several challenges in the network reconstruction phase.
Therefore, it is important to encode gene expression values, leverege the prior
knowledge gained from the available inferred network structures and positional
informations of the input network nodes towards inferring a better and more
confident GRN network reconstruction. In this paper, we explore the integration
of multiple inferred networks to enhance the inference of Gene Regulatory
Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene
expression patterns directly from raw data, preserving intricate biological
signals. Then, we embed the prior knowledge from GRN structures transforming
them into a text-like representation using random walks, which are then encoded
with a masked language model, BERT, to generate global embeddings for each gene
across all networks. Additionally, we embed the positional encodings of the
input gene networks to better identify the position of each unique gene within
the graph. These embeddings are integrated into graph transformer-based model,
termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the
topological structure of the ground truth network while incorporating the
enriched encoded information. Experimental results demonstrate that GT-GRN
significantly outperforms existing GRN inference methods, achieving superior
accuracy and highlighting the robustness of our approach.

</details>


### [150] [Backslash: Rate Constrained Optimized Training of Large Language Models](https://arxiv.org/abs/2504.16968)
*Jun Wu,Jiangtao Wen,Yuxing Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为Backslash的训练时压缩方法，通过率失真优化实现参数冗余减少，在多种任务中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练后的参数压缩研究已很多，但训练阶段的压缩仍未被充分探索。旨在解决这一问题。

Method: 采用率失真优化（RDO）方法，提出Rate-Constrained Training（Backslash），实现模型精度与复杂度的灵活权衡。

Result: 实验显示Backslash可减少60%-90%内存占用且无精度损失，同时在修剪鲁棒性、泛化能力和边缘设备推理加速方面表现优异。

Conclusion: Backslash是一种高效、灵活的训练时压缩方法，适用于多种场景，显著优于训练后压缩技术。

Abstract: The rapid advancement of large-language models (LLMs) has driven extensive
research into parameter compression after training has been completed, yet
compression during the training phase remains largely unexplored. In this work,
we introduce Rate-Constrained Training (Backslash), a novel training-time
compression approach based on rate-distortion optimization (RDO). Backslash
enables a flexible trade-off between model accuracy and complexity,
significantly reducing parameter redundancy while preserving performance.
Experiments in various architectures and tasks demonstrate that Backslash can
reduce memory usage by 60\% - 90\% without accuracy loss and provides
significant compression gain compared to compression after training. Moreover,
Backslash proves to be highly versatile: it enhances generalization with small
Lagrange multipliers, improves model robustness to pruning (maintaining
accuracy even at 80\% pruning rates), and enables network simplification for
accelerated inference on edge devices.

</details>


### [151] [STFM: A Spatio-Temporal Information Fusion Model Based on Phase Space Reconstruction for Sea Surface Temperature Prediction](https://arxiv.org/abs/2504.16970)
*Yin Wang,Chunlin Gong,Xiang Wu,Hanleran Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于数据驱动的海表温度预测框架，通过相空间重构和时空融合映射技术，在少量训练数据下实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 海表温度（SST）的准确预测对生产规划至关重要，但现有方法（物理数值模拟或传统机器学习）因计算复杂、数据依赖性强或可解释性差而受限，需要一种高效且数据需求低的替代方案。

Method: 采用相空间重构技术构建初始延迟吸引子对，并通过设计的时空融合映射（STFM）揭示其内在关联，避免传统模型的局限性。

Result: 在对比实验中，该方法以少量训练数据实现了较高的预测精度，显著优于传统物理模拟或纯机器学习方法。

Conclusion: 提出的框架通过数据驱动技术有效捕捉SST非线性动态，兼具高效性和低数据需求，为海洋环境预测提供了新思路。

Abstract: The sea surface temperature (SST), a key environmental parameter, is crucial
to optimizing production planning, making its accurate prediction a vital
research topic. However, the inherent nonlinearity of the marine dynamic system
presents significant challenges. Current forecasting methods mainly include
physics-based numerical simulations and data-driven machine learning
approaches. The former, while describing SST evolution through differential
equations, suffers from high computational complexity and limited
applicability, whereas the latter, despite its computational benefits, requires
large datasets and faces interpretability challenges. This study presents a
prediction framework based solely on data-driven techniques. Using phase space
reconstruction, we construct initial-delay attractor pairs with a mathematical
homeomorphism and design a Spatio-Temporal Fusion Mapping (STFM) to uncover
their intrinsic connections. Unlike conventional models, our method captures
SST dynamics efficiently through phase space reconstruction and achieves high
prediction accuracy with minimal training data in comparative tests

</details>


### [152] [Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications](https://arxiv.org/abs/2504.16972)
*Hossein Ahmadi,Sajjad Emdadi Mahdimahalleh,Arman Farahat,Banafsheh Saffari*

Main category: cs.LG

TL;DR: 该综述总结了自编码器和视觉Transformer在无监督信号分析中的最新进展，探讨了它们在特征提取、异常检测和分类中的应用，并指出了可解释性、扩展性和领域泛化方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 快速增长的未标记时间序列数据（如无线通信、雷达、生物医学工程和物联网）推动了对无监督学习方法的深入研究，特别是自编码器和视觉Transformer的应用潜力。

Method: 研究综述了自编码器和视觉Transformer的架构、应用及新兴趋势，重点关注它们在特征提取、异常检测和信号分类中的表现。

Result: 研究发现混合架构和自监督学习在信号分析中表现优异，但仍面临可解释性、扩展性和领域泛化等挑战。

Conclusion: 本文为开发更鲁棒、自适应的信号智能模型提供了方法创新与实际应用的桥梁，并指出了未来研究方向。

Abstract: The rapid growth of unlabeled time-series data in domains such as wireless
communications, radar, biomedical engineering, and the Internet of Things (IoT)
has driven advancements in unsupervised learning. This review synthesizes
recent progress in applying autoencoders and vision transformers for
unsupervised signal analysis, focusing on their architectures, applications,
and emerging trends. We explore how these models enable feature extraction,
anomaly detection, and classification across diverse signal types, including
electrocardiograms, radar waveforms, and IoT sensor data. The review highlights
the strengths of hybrid architectures and self-supervised learning, while
identifying challenges in interpretability, scalability, and domain
generalization. By bridging methodological innovations and practical
applications, this work offers a roadmap for developing robust, adaptive models
for signal intelligence.

</details>


### [153] [Safety Pretraining: Toward the Next Generation of Safe AI](https://arxiv.org/abs/2504.16980)
*Pratyush Maini,Sachin Goyal,Dylan Sam,Alex Robey,Yash Savani,Yiding Jiang,Andy Zou,Zacharcy C. Lipton,J. Zico Kolter*

Main category: cs.LG

TL;DR: 该论文提出了一种数据中心的预训练框架，从根本上增强LLMs的安全性，通过过滤有害数据、生成安全数据集及标注训练，显著降低有害内容生成率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在关键场景中的应用增多，其生成有害或毒性内容的风险成为主要挑战，传统的后处理对齐方法效果有限，因此需要一种从预训练阶段就嵌入安全性的方法。

Method: 1. 使用GPT-4标注的1万例数据训练安全分类器，过滤600B tokens；2. 生成100B tokens的合成安全数据集；3. 创建RefuseWeb和Moral Education数据集；4. 在预训练中注入Harmfulness-Tag标注；5. 进行安全性评估。

Result: 安全预训练模型将攻击成功率从38.8%降至8.4%，且在标准LLM安全基准上无性能损失。

Conclusion: 通过数据中心的预训练框架，能在预训练阶段有效内化安全性，显著降低LLMs生成有害内容的风险。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, the risk of generating harmful or toxic content remains a central
challenge. Post-hoc alignment methods are brittle: once unsafe patterns are
learned during pretraining, they are hard to remove. We present a data-centric
pretraining framework that builds safety into the model from the start. Our
contributions include: (i) a safety classifier trained on 10,000 GPT-4 labeled
examples, used to filter 600B tokens; (ii) the largest synthetic safety dataset
to date (100B tokens) generated via recontextualization of harmful web data;
(iii) RefuseWeb and Moral Education datasets that convert harmful prompts into
refusal dialogues and web-style educational material; (iv) Harmfulness-Tag
annotations injected during pretraining to flag unsafe content and steer away
inference from harmful generations; and (v) safety evaluations measuring base
model behavior before instruction tuning. Our safety-pretrained models reduce
attack success rates from 38.8% to 8.4% with no performance degradation on
standard LLM safety benchmarks.

</details>


### [154] [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
*Amin Karbasi,Omar Montasser,John Sous,Grigoris Velegkas*

Main category: cs.LG

TL;DR: 摘要介绍了检测大型语言模型（LLM）产生幻觉的自检测框架的理论可行性。通过将幻觉检测与语言识别等同，证明仅使用正确样本训练的检测器在大多数情况下无法实现；而结合专家标注的正负样本训练后，则可能对所有可数语言集合实现检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）易产生“幻觉”（错误输出）。本文旨在从理论上探讨自动检测此类幻觉的可行性，为实际部署提供指导。

Method: 借鉴Gold-Angluin语言识别框架，将幻觉检测问题转化为语言识别任务。理论分析了两类训练数据（仅正确样本 vs. 正负样本）对检测能力的影响。

Result: 1. 仅用正确样本训练的检测器对大多数语言集合无法实现检测；2. 加入专家标注的负样本后，检测对所有可数语言集合成为可能。

Conclusion: 专家标注反馈（如RLHF中的负样本）是训练有效幻觉检测器的关键。成果支持反馈机制在LLM可靠性优化中的必要性。

Abstract: Is automated hallucination detection possible? In this work, we introduce a
theoretical framework to analyze the feasibility of automatically detecting
hallucinations produced by large language models (LLMs). Inspired by the
classical Gold-Angluin framework for language identification and its recent
adaptation to language generation by Kleinberg and Mullainathan, we investigate
whether an algorithm, trained on examples drawn from an unknown target language
$K$ (selected from a countable collection) and given access to an LLM, can
reliably determine whether the LLM's outputs are correct or constitute
hallucinations.
  First, we establish an equivalence between hallucination detection and the
classical task of language identification. We prove that any hallucination
detection method can be converted into a language identification method, and
conversely, algorithms solving language identification can be adapted for
hallucination detection. Given the inherent difficulty of language
identification, this implies that hallucination detection is fundamentally
impossible for most language collections if the detector is trained using only
correct examples from the target language.
  Second, we show that the use of expert-labeled feedback, i.e., training the
detector with both positive examples (correct statements) and negative examples
(explicitly labeled incorrect statements), dramatically changes this
conclusion. Under this enriched training regime, automated hallucination
detection becomes possible for all countable language collections.
  These results highlight the essential role of expert-labeled examples in
training hallucination detectors and provide theoretical support for
feedback-based methods, such as reinforcement learning with human feedback
(RLHF), which have proven critical for reliable LLM deployment.

</details>


### [155] [Democracy of AI Numerical Weather Models: An Example of Global Forecasting with FourCastNetv2 Made by a University Research Lab Using GPU](https://arxiv.org/abs/2504.17028)
*Iman Khadir,Shane Stevenson,Henry Li,Kyle Krick,Abram Burrows,David Hall,Stan Posey,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 论文展示了如何利用GPU和免费AI模型（如FourCastNetv2）在高校研究小组中普及AI驱动的全球天气预报模型，并探讨了资源限制下的可行性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决资源有限的高校研究小组在AI天气预报中面临的挑战，推动AI在数值天气预报（NWP）中的普及。

Method: 利用FourCastNetv2的API进行预测，并使用NVIDIA硬件训练原始FourCastNet模型，同时研究数据管理、训练效率和模型验证。

Result: 展示了资源有限的研究组使用NVIDIA A100的能力和局限性，并提供了相关GitHub材料作为研究和教育的初步指南。

Conclusion: 论文为高校研究组和课程提供了AI天气预报的研究和教育框架，有助于在数字经济中普及AI驱动的NWP。

Abstract: This paper demonstrates the feasibility of democratizing AI-driven global
weather forecasting models among university research groups by leveraging
Graphics Processing Units (GPUs) and freely available AI models, such as
NVIDIA's FourCastNetv2. FourCastNetv2 is an NVIDIA's advanced neural network
for weather prediction and is trained on a 73-channel subset of the European
Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset
at single levels and different pressure levels. Although the training
specifications for FourCastNetv2 are not released to the public, the training
documentation of the model's first generation, FourCastNet, is available to all
users. The training had 64 A100 GPUs and took 16 hours to complete. Although
NVIDIA's models offer significant reductions in both time and cost compared to
traditional Numerical Weather Prediction (NWP), reproducing published
forecasting results presents ongoing challenges for resource-constrained
university research groups with limited GPU availability. We demonstrate both
(i) leveraging FourCastNetv2 to create predictions through the designated
application programming interface (API) and (ii) utilizing NVIDIA hardware to
train the original FourCastNet model. Further, this paper demonstrates the
capabilities and limitations of NVIDIA A100's for resource-limited research
groups in universities. We also explore data management, training efficiency,
and model validation, highlighting the advantages and challenges of using
limited high-performance computing resources. Consequently, this paper and its
corresponding GitHub materials may serve as an initial guide for other
university research groups and courses related to machine learning, climate
science, and data science to develop research and education programs on AI
weather forecasting, and hence help democratize the AI NWP in the digital
economy.

</details>


### [156] [Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation](https://arxiv.org/abs/2504.17058)
*Rahul Vishwakarma*

Main category: cs.LG

TL;DR: 论文提出了一种融合共形预测与GAN的新框架cGAN，旨在为合成数据提供统计保证和不确定性量化，解决了现有生成模型缺乏严格统计验证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型虽能生成逼真样本，但缺乏对底层数据分布的统计保障，限制了在高风险领域的应用。因此，需开发能提供统计保证的合成数据生成方法。

Method: 通过结合多种共形预测方法（如ICP、Mondrian、交叉共形预测及Venn-Abers预测器），将不确定性量化集成到GAN中，提出cGAN框架。

Result: cGAN在保持传统GAN生成能力的同时，增强了校准性，并为合成数据提供了有限样本有效性保证和渐近效率性质。

Conclusion: cGAN通过共形预测为合成数据赋予统计保障，拓展了其在医疗、金融等高要求领域的可靠应用。

Abstract: The generation of high-quality synthetic data presents significant challenges
in machine learning research, particularly regarding statistical fidelity and
uncertainty quantification. Existing generative models produce compelling
synthetic samples but lack rigorous statistical guarantees about their relation
to the underlying data distribution, limiting their applicability in critical
domains requiring robust error bounds. We address this fundamental limitation
by presenting a novel framework that incorporates conformal prediction
methodologies into Generative Adversarial Networks (GANs). By integrating
multiple conformal prediction paradigms including Inductive Conformal
Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction,
and Venn-Abers Predictors, we establish distribution-free uncertainty
quantification in generated samples. This approach, termed Conformalized GAN
(cGAN), demonstrates enhanced calibration properties while maintaining the
generative power of traditional GANs, producing synthetic data with provable
statistical guarantees. We provide rigorous mathematical proofs establishing
finite-sample validity guarantees and asymptotic efficiency properties,
enabling the reliable application of synthetic data in high-stakes domains
including healthcare, finance, and autonomous systems.

</details>


### [157] [Antenna Near-Field Reconstruction from Far-Field Data Using Convolutional Neural Networks](https://arxiv.org/abs/2504.17065)
*Sahar Bagherkhani,Jackson Christopher Earls,Franco De Flaviis,Pierre Baldi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于卷积神经网络（CNN）的远场-近场（FF-NF）变换方法，用于从天线远场数据重建近场分布，避免了显式解析变换的需求。


<details>
  <summary>Details</summary>
Motivation: 电磁场重建在许多应用中至关重要，如天线诊断、电磁干扰分析和系统建模。传统方法依赖解析变换，限制了其灵活性和适用性。

Method: 使用卷积神经网络（CNN）训练配对的远场和近场数据，以均方误差（MSE）作为评估指标。

Result: 最佳模型的训练误差为0.0199，测试误差为0.3898。可视化比较显示模型能有效捕捉复杂的电磁场行为。

Conclusion: 深度学习在电磁场重建中展现出潜力，提供了一种无需显式解析变换的高效解决方案。

Abstract: Electromagnetic field reconstruction is crucial in many applications,
including antenna diagnostics, electromagnetic interference analysis, and
system modeling. This paper presents a deep learning-based approach for
Far-Field to Near-Field (FF-NF) transformation using Convolutional Neural
Networks (CNNs). The goal is to reconstruct near-field distributions from the
far-field data of an antenna without relying on explicit analytical
transformations. The CNNs are trained on paired far-field and near-field data
and evaluated using mean squared error (MSE). The best model achieves a
training error of 0.0199 and a test error of 0.3898. Moreover, visual
comparisons between the predicted and true near-field distributions demonstrate
the model's effectiveness in capturing complex electromagnetic field behavior,
highlighting the potential of deep learning in electromagnetic field
reconstruction.

</details>


### [158] [Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching](https://arxiv.org/abs/2504.17066)
*Kewen Peng,Yicheng Yang,Hao Zhuo,Tim Menzies*

Main category: cs.LG

TL;DR: 本文提出FairMatch方法，通过倾向评分匹配评估和减少偏见，确保公平性评估和缓解的效果，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统公平性学习方法在训练和测试数据随机采样时可能仍存在偏见，影响公平性指标的可靠性，因此需要改进公平性评估方法。

Method: 提出FairMatch方法，利用倾向评分匹配在测试集中识别对照组和处理组，调整子组决策阈值，并对无法匹配的样本进行概率校准。

Result: 实验表明，FairMatch能准确定位模型无偏见的测试数据子集，并显著减少剩余数据的偏见。

Conclusion: 倾向评分匹配为公平性评估和缓解提供了有效方法，且无需牺牲预测性能。

Abstract: Fairness-aware learning aims to mitigate discrimination against specific
protected social groups (e.g., those categorized by gender, ethnicity, age)
while minimizing predictive performance loss. Despite efforts to improve
fairness in machine learning, prior studies have shown that many models remain
unfair when measured against various fairness metrics. In this paper, we
examine whether the way training and testing data are sampled affects the
reliability of reported fairness metrics. Since training and test sets are
often randomly sampled from the same population, bias present in the training
data may still exist in the test data, potentially skewing fairness
assessments. To address this, we propose FairMatch, a post-processing method
that applies propensity score matching to evaluate and mitigate bias. FairMatch
identifies control and treatment pairs with similar propensity scores in the
test set and adjusts decision thresholds for different subgroups accordingly.
For samples that cannot be matched, we perform probabilistic calibration using
fairness-aware loss functions. Experimental results demonstrate that our
approach can (a) precisely locate subsets of the test data where the model is
unbiased, and (b) significantly reduce bias on the remaining data. Overall,
propensity score matching offers a principled way to improve both fairness
evaluation and mitigation, without sacrificing predictive performance.

</details>


### [159] [In-Context Learning can distort the relationship between sequence likelihoods and biological fitness](https://arxiv.org/abs/2504.17068)
*Pranav Kantroo,Günter P. Wagner,Benjamin B. Machta*

Main category: cs.LG

TL;DR: 论文探讨了语言模型在预测生物序列可行性时，由于上下文学习导致的序列似然分数与实验适应度测量之间的失真现象，尤其在高重复基序序列中表现明显。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了揭示语言模型在预测生物序列适应度时可能出现的失真现象，特别是重复基序导致的异常高似然分数。

Method: 研究方法包括使用基于掩码语言建模目标的蛋白质语言模型（尤其是Transformer架构模型），分析重复基序对似然分数的影响及其机制。

Result: 结果显示，Transformer模型易受重复基序影响，通过类似查找操作的高似然分数覆盖了模型先验；此现象也适用于不完全重复序列和其他生物特征（如RNA发夹结构中的反向互补基序）。

Conclusion: 结论指出，上下文学习可能扭曲语言模型对生物序列适应度的预测，需警惕重复基序等特征对模型输出的潜在影响。

Abstract: Language models have emerged as powerful predictors of the viability of
biological sequences. During training these models learn the rules of the
grammar obeyed by sequences of amino acids or nucleotides. Once trained, these
models can take a sequence as input and produce a likelihood score as an
output; a higher likelihood implies adherence to the learned grammar and
correlates with experimental fitness measurements. Here we show that in-context
learning can distort the relationship between fitness and likelihood scores of
sequences. This phenomenon most prominently manifests as anomalously high
likelihood scores for sequences that contain repeated motifs. We use protein
language models with different architectures trained on the masked language
modeling objective for our experiments, and find transformer-based models to be
particularly vulnerable to this effect. This behavior is mediated by a look-up
operation where the model seeks the identity of the masked position by using
the other copy of the repeated motif as a reference. This retrieval behavior
can override the model's learned priors. This phenomenon persists for
imperfectly repeated sequences, and extends to other kinds of biologically
relevant features such as reversed complement motifs in RNA sequences that fold
into hairpin structures.

</details>


### [160] [Sparse Phased Array Optimization Using Deep Learning](https://arxiv.org/abs/2504.17073)
*David Lu,Lior Maman,Jackson Earls,Amir Boag,Pierre Baldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的稀疏相控阵优化方法，通过减少栅瓣，优化天线布局，显著提升了天线阵列的性能。


<details>
  <summary>Details</summary>
Motivation: 天线阵列在多个领域有广泛应用，但其设计面临非凸挑战和高自由度问题，因此需要高效的优化方法。

Method: 采用深度学习近似非凸成本函数，结合梯度下降和定制化惩罚机制优化天线坐标。

Result: 在十种初始成本最低的阵列配置上，成本降低幅度达411%至643%，平均提升552%。

Conclusion: 该方法显著降低旁瓣水平，为超精准波束成形和下一代无线及雷达系统奠定了基础。

Abstract: Antenna arrays are widely used in wireless communication, radar systems,
radio astronomy, and military defense to enhance signal strength, directivity,
and interference suppression. We introduce a deep learning-based optimization
approach that enhances the design of sparse phased arrays by reducing grating
lobes. This approach begins by generating sparse array configurations to
address the non-convex challenges and extensive degrees of freedom inherent in
array design. We use neural networks to approximate the non-convex cost
function that estimates the energy ratio between the main and side lobes. This
differentiable approximation facilitates cost function minimization through
gradient descent, optimizing the antenna elements' coordinates and leading to
an improved layout. Additionally, we incorporate a tailored penalty mechanism
that includes various physical and design constraints into the optimization
process, enhancing its robustness and practical applicability. We demonstrate
the effectiveness of our method by applying it to the ten array configurations
with the lowest initial costs, achieving further cost reductions ranging from
411% to 643%, with an impressive average improvement of 552%. By significantly
reducing side lobe levels in antenna arrays, this breakthrough paves the way
for ultra-precise beamforming, enhanced interference mitigation, and
next-generation wireless and radar systems with unprecedented efficiency and
clarity.

</details>


### [161] [Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy](https://arxiv.org/abs/2504.17074)
*William R. Keely,Otto Lamminpää,Steffen Mauceri,Sean M. R. Crowell,Christopher W. O'Dell,Gregory R. McGarragh*

Main category: cs.LG

TL;DR: 论文提出了一种基于扩散的方法，用于高效且准确地估算温室气体浓度，解决了现有最优估计算法的计算成本高和后验分布非高斯问题。


<details>
  <summary>Details</summary>
Motivation: 当前使用最优估计（OE）方法估算温室气体浓度的算法计算成本高，且对后验分布的非高斯性处理不足，导致不确定性估计不准确。未来卫星任务将产生更多数据，因此需要更快速、准确的算法。

Method: 论文提出了一种基于扩散的方法，能够灵活处理高斯或非高斯后验分布，并在NASA的OCO-2光谱仪上实现显著的计算速度提升。

Result: 该方法在计算效率上优于当前最优估计算法，且能更准确地量化不确定性。

Conclusion: 该方法的成功应用将推动实时全球碳源汇监测，对制定气候政策具有重要意义。

Abstract: Satellite-based estimates of greenhouse gas (GHG) properties from
observations of reflected solar spectra are integral for understanding and
monitoring complex terrestrial systems and their impact on the carbon cycle due
to their near global coverage. Known as retrieval, making GHG concentration
estimations from these observations is a non-linear Bayesian inverse problem,
which is operationally solved using a computationally expensive algorithm
called Optimal Estimation (OE), providing a Gaussian approximation to a
non-Gaussian posterior. This leads to issues in solver algorithm convergence,
and to unrealistically confident uncertainty estimates for the retrieved
quantities. Upcoming satellite missions will provide orders of magnitude more
data than the current constellation of GHG observers. Development of fast and
accurate retrieval algorithms with robust uncertainty quantification is
critical. Doing so stands to provide substantial climate impact of moving
towards the goal of near continuous real-time global monitoring of carbon
sources and sinks which is essential for policy making. To achieve this goal,
we propose a diffusion-based approach to flexibly retrieve a Gaussian or
non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer,
while providing a substantial computational speed-up over the current
operational state-of-the-art.

</details>


### [162] [A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices](https://arxiv.org/abs/2504.17079)
*Esam Mahdi,C. Martin-Barreiro,X. Cabezas*

Main category: cs.LG

TL;DR: 本文提出了一种结合注意力Transformer和GRU的深度学习混合模型，用于提升加密货币价格预测的准确性，并在性能评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决加密货币价格预测的挑战，结合Transformer和GRU的优势，提出了一个混合模型。

Method: 采用Transformer和GRU的混合模型，基于历史价格、交易量和Fear and Greed指数进行预测。

Result: 与RBFN、GRNN、BiLSTM和BiGRU相比，混合模型在MSE、RMSE、MAE和MAPE指标上表现最优。

Conclusion: 该混合模型在金融预测任务中表现出高效性，为加密货币市场的实时决策提供了有价值的工具。

Abstract: In this article, we introduce a novel deep learning hybrid model that
integrates attention Transformer and Gated Recurrent Unit (GRU) architectures
to improve the accuracy of cryptocurrency price predictions. By combining the
Transformer's strength in capturing long-range patterns with the GRU's ability
to model short-term and sequential trends, the hybrid model provides a
well-rounded approach to time series forecasting. We apply the model to predict
the daily closing prices of Bitcoin and Ethereum based on historical data that
include past prices, trading volumes, and the Fear and Greed index. We evaluate
the performance of our proposed model by comparing it with four other machine
learning models: two are non-sequential feedforward models: Radial Basis
Function Network (RBFN) and General Regression Neural Network (GRNN), and two
are bidirectional sequential memory-based models: Bidirectional Long-Short-Term
Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance
of the model is assessed using several metrics, including Mean Squared Error
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean
Absolute Percentage Error (MAPE), along with statistical validation through the
nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test.
The results demonstrate that our hybrid model consistently achieves superior
accuracy, highlighting its effectiveness for financial prediction tasks. These
findings provide valuable insights for improving real-time decision making in
cryptocurrency markets and support the growing use of hybrid deep learning
models in financial analytics.

</details>


### [163] [GeoRDF2Vec Learning Location-Aware Entity Representations in Knowledge Graphs](https://arxiv.org/abs/2504.17099)
*Martin Boeckling,Heiko Paulheim,Sarah Detzler*

Main category: cs.LG

TL;DR: 该论文提出了一种改进的RDF2Vec方法，通过引入几何信息生成具有位置感知的实体嵌入，并在多个基准数据集上验证了其优于非位置感知的RDF2Vec和GeoTransE。


<details>
  <summary>Details</summary>
Motivation: 知识图谱中存在大量空间实体（如城市、建筑等），但现有实体表示学习方法通常忽略了这些实体的几何信息。作者希望通过结合几何信息改进实体嵌入的学习。

Method: 方法扩展了RDF2Vec，通过从地理节点“淹没”图谱来考虑可到达的节点，并在图游走过程中引入空间偏置权重。

Result: 实验表明，该方法在多个基准数据集上表现优于非位置感知的RDF2Vec和GeoTransE。

Conclusion: 结合几何信息能有效提升实体嵌入的学习效果，为空间实体提供更好的表示。

Abstract: Many knowledge graphs contain a substantial number of spatial entities, such
as cities, buildings, and natural landmarks. For many of these entities, exact
geometries are stored within the knowledge graphs. However, most existing
approaches for learning entity representations do not take these geometries
into account. In this paper, we introduce a variant of RDF2Vec that
incorporates geometric information to learn location-aware embeddings of
entities. Our approach expands different nodes by flooding the graph from
geographic nodes, ensuring that each reachable node is considered. Based on the
resulting flooded graph, we apply a modified version of RDF2Vec that biases
graph walks using spatial weights. Through evaluations on multiple benchmark
datasets, we demonstrate that our approach outperforms both non-location-aware
RDF2Vec and GeoTransE.

</details>


### [164] [Discovering the Precursors of Traffic Breakdowns Using Spatiotemporal Graph Attribution Networks](https://arxiv.org/abs/2504.17109)
*Zhaobin Mo,Xiangyi Liao,Dominik A. Karbowski,Yanbing Wang*

Main category: cs.LG

TL;DR: 论文提出了一种结合时空图神经网络和Shapley值的新方法，用于识别和解释交通中断的前兆，并在Interstate-24数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解和预测交通中断的前兆对提高道路安全和交通流管理至关重要，但现有黑盒神经网络预测缺乏可解释性。

Method: 通过将Shapley解释方法扩展到时空场景，结合时空图神经网络（ST-GNNs），提出了一种新的可解释性方法。

Result: 在Interstate-24数据上的实验表明，道路拓扑和急刹车是导致交通中断的主要因素。

Conclusion: 该方法填补了黑盒神经网络预测与可解释原因之间的空白，为交通管理提供了实用工具。

Abstract: Understanding and predicting the precursors of traffic breakdowns is critical
for improving road safety and traffic flow management. This paper presents a
novel approach combining spatiotemporal graph neural networks (ST-GNNs) with
Shapley values to identify and interpret traffic breakdown precursors. By
extending Shapley explanation methods to a spatiotemporal setting, our proposed
method bridges the gap between black-box neural network predictions and
interpretable causes. We demonstrate the method on the Interstate-24 data, and
identify that road topology and abrupt braking are major factors that lead to
traffic breakdowns.

</details>


### [165] [Scalable Permutation-Aware Modeling for Temporal Set Prediction](https://arxiv.org/abs/2504.17140)
*Ashish Ranjan,Ayush Agarwal,Shalin Barot,Sushant Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种新颖且可扩展的框架，通过置换等变和置换不变变换高效建模集合动态，显著减少训练和推理时间，同时保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖复杂架构和大量计算开销，限制了其可扩展性。

Method: 利用置换等变和置换不变变换的高效建模集合动态。

Result: 在多个公共基准测试中，性能与或优于最先进模型，同时显著降低计算时间。

Conclusion: 该方法在高效且可扩展的时序集合预测中表现出色。

Abstract: Temporal set prediction involves forecasting the elements that will appear in
the next set, given a sequence of prior sets, each containing a variable number
of elements. Existing methods often rely on intricate architectures with
substantial computational overhead, which hampers their scalability. In this
work, we introduce a novel and scalable framework that leverages
permutation-equivariant and permutation-invariant transformations to
efficiently model set dynamics. Our approach significantly reduces both
training and inference time while maintaining competitive performance.
Extensive experiments on multiple public benchmarks show that our method
achieves results on par with or superior to state-of-the-art models across
several evaluation metrics. These results underscore the effectiveness of our
model in enabling efficient and scalable temporal set prediction.

</details>


### [166] [OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection](https://arxiv.org/abs/2504.17160)
*Alberto Fernández-Hernández,Jose I. Mestre,Manuel F. Dolz,Jose Duato,Enrique S. Quintana-Ortí*

Main category: cs.LG

TL;DR: OUI是一种新工具，用于监控DNN训练动态并指导正则化超参数（如Weight Decay）的选择，无需验证数据即可判断过拟合或欠拟合状态。实验证明OUI能更快收敛且提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统超参数调整依赖验证数据且耗时，OUI旨在提供更高效、无需验证的过拟合/欠拟合监测方法。

Method: 提出OUI指标，通过实验在DenseNet、EfficientNet和ResNet上验证其有效性，对比传统指标（如损失、准确率）。

Result: OUI能快速收敛，指导WD超参数选择，显著提升泛化能力和验证分数。

Conclusion: OUI为超参数调优提供高效工具，尤其适用于早期训练阶段的WD优化。

Abstract: We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for
monitoring the training dynamics of Deep Neural Networks (DNNs) and identifying
optimal regularization hyperparameters. Specifically, we validate that OUI can
effectively guide the selection of the Weight Decay (WD) hyperparameter by
indicating whether a model is overfitting or underfitting during training
without requiring validation data. Through experiments on DenseNet-BC-100 with
CIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,
we show that maintaining OUI within a prescribed interval correlates strongly
with improved generalization and validation scores. Notably, OUI converges
significantly faster than traditional metrics such as loss or accuracy,
enabling practitioners to identify optimal WD (hyperparameter) values within
the early stages of training. By leveraging OUI as a reliable indicator, we can
determine early in training whether the chosen WD value leads the model to
underfit the training data, overfit, or strike a well-balanced trade-off that
maximizes validation scores. This enables more precise WD tuning for optimal
performance on the tested datasets and DNNs. All code for reproducing these
experiments is available at https://github.com/AlbertoFdezHdez/OUI.

</details>


### [167] [A Double-Norm Aggregated Tensor Latent Factorization Model for Temporal-Aware Traffic Speed Imputation](https://arxiv.org/abs/2504.17196)
*Jiawen Hou,Hao Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于TATSI的交通速度数据填补方法，通过结合$L_2$-norm和$SL_1$-norm损失函数提高算法鲁棒性和精度。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中，传感器缺失或故障导致交通速度数据不完整，现有基于张量分解的方法因依赖$L_2$-norm而鲁棒性不足。

Method: 提出TATSI方法，结合$L_2$-norm和$SL_1$-norm的损失函数，采用SLF-NMU算法进行非负潜在因子分析。

Result: 在三个真实交通速度数据集上的实验表明，TATSI能更准确捕捉时间模式，填补缺失数据的精度优于现有方法。

Conclusion: TATSI通过优化损失函数和算法设计，显著提升了交通速度数据填补的准确性和鲁棒性。

Abstract: In intelligent transportation systems (ITS), traffic management departments
rely on sensors, cameras, and GPS devices to collect real-time traffic data.
Traffic speed data is often incomplete due to sensor failures, data
transmission delays, or occlusions, resulting in missing speed data in certain
road segments. Currently, tensor decomposition based methods are extensively
utilized, they mostly rely on the $L_2$-norm to construct their learning
objectives, which leads to reduced robustness in the algorithms. To address
this, we propose Temporal-Aware Traffic Speed Imputation (TATSI), which
combines the $L_2$-norm and smooth $L_1$ (${SL}_1$)-norm in its loss function,
thereby achieving both high accuracy and robust performance in imputing missing
time-varying traffic speed data. TATSI adopts a single latent factor-dependent,
nonnegative, and multiplicative update (SLF-NMU) approach, which serves as an
efficient solver for performing nonnegative latent factor analysis (LFA) on a
tensor. Empirical studies on three real-world time-varying traffic speed
datasets demonstrate that, compared with state-of-the-art traffic speed
predictors, TATSI more precisely captures temporal patterns, thereby yielding
the most accurate imputations for missing traffic speed data.

</details>


### [168] [Synthetic Power Flow Data Generation Using Physics-Informed Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2504.17210)
*Junfei Wang,Darshana Upadhyay,Marzia Zaman,Pirathayini Srikantha*

Main category: cs.LG

TL;DR: 该论文提出了一种基于去噪扩散概率模型的物理信息生成框架，用于合成可行的电力潮流数据，解决实际数据受限的问题。


<details>
  <summary>Details</summary>
Motivation: 智能电网中许多数据驱动模块依赖高质量的电力潮流数据，但由于隐私和操作限制，真实数据往往有限。

Method: 结合辅助训练和物理信息损失函数的去噪扩散概率模型，确保生成数据既具有统计保真度，又符合电力系统可行性。

Result: 在IEEE 14总线和30总线基准系统上的评估表明，该方法能捕捉关键分布特性并泛化到分布外场景，且在可行性、多样性和统计特征准确性上优于三种基线模型。

Conclusion: 该研究展示了生成模型在数据驱动电力系统应用中的潜力。

Abstract: Many data-driven modules in smart grid rely on access to high-quality power
flow data; however, real-world data are often limited due to privacy and
operational constraints. This paper presents a physics-informed generative
framework based on Denoising Diffusion Probabilistic Models (DDPMs) for
synthesizing feasible power flow data. By incorporating auxiliary training and
physics-informed loss functions, the proposed method ensures that the generated
data exhibit both statistical fidelity and adherence to power system
feasibility. We evaluate the approach on the IEEE 14-bus and 30-bus benchmark
systems, demonstrating its ability to capture key distributional properties and
generalize to out-of-distribution scenarios. Comparative results show that the
proposed model outperforms three baseline models in terms of feasibility,
diversity, and accuracy of statistical features. This work highlights the
potential of integrating generative modelling into data-driven power system
applications.

</details>


### [169] [Enhancing Variational Autoencoders with Smooth Robust Latent Encoding](https://arxiv.org/abs/2504.17219)
*Hyomin Lee,Minseon Kim,Sangwon Jang,Jongheon Jeong,Sung Ju Hwang*

Main category: cs.LG

TL;DR: SRL-VAE通过对抗训练提升VAE的生成质量和鲁棒性，证明对抗训练可以同时增强保真度和鲁棒性，挑战了传统观念。


<details>
  <summary>Details</summary>
Motivation: 传统对抗训练在生成模型中因担心保真度下降而被忽视，本文挑战这一观念，旨在通过对抗训练同时提升生成质量和鲁棒性。

Method: 提出SRL-VAE框架，通过对抗扰动平滑潜在空间，结合原始表示正则化，以最小计算开销提升预训练VAE的鲁棒性和保真度。

Result: 实验表明，SRL-VAE在图像重建、文本引导编辑等任务中提升生成质量，并有效抵御Nightshade等攻击。

Conclusion: SRL-VAE证明了对抗训练可以同时增强生成模型的保真度和鲁棒性，为生成模型的鲁棒性研究提供了新范式。

Abstract: Variational Autoencoders (VAEs) have played a key role in scaling up
diffusion-based generative models, as in Stable Diffusion, yet questions
regarding their robustness remain largely underexplored. Although adversarial
training has been an established technique for enhancing robustness in
predictive models, it has been overlooked for generative models due to concerns
about potential fidelity degradation by the nature of trade-offs between
performance and robustness. In this work, we challenge this presumption,
introducing Smooth Robust Latent VAE (SRL-VAE), a novel adversarial training
framework that boosts both generation quality and robustness. In contrast to
conventional adversarial training, which focuses on robustness only, our
approach smooths the latent space via adversarial perturbations, promoting more
generalizable representations while regularizing with originality
representation to sustain original fidelity. Applied as a post-training step on
pre-trained VAEs, SRL-VAE improves image robustness and fidelity with minimal
computational overhead. Experiments show that SRL-VAE improves both generation
quality, in image reconstruction and text-guided image editing, and robustness,
against Nightshade attacks and image editing attacks. These results establish a
new paradigm, showing that adversarial training, once thought to be detrimental
to generative models, can instead enhance both fidelity and robustness.

</details>


### [170] [Multi-Modal Traffic Analysis: Integrating Time-Series Forecasting, Accident Prediction, and Image Classification](https://arxiv.org/abs/2504.17232)
*Nivedita M,Yasmeen Shajitha S*

Main category: cs.LG

TL;DR: 提出一个结合时间序列预测、分类和计算机视觉的集成机器学习框架，用于高级交通分析，效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 为了提升智能交通系统的实时监控、事故预防和资源优化能力。

Method: 使用ARIMA(2,0,1)模型预测交通流量、XGBoost分类器评估事故严重性、CNN处理交通图像分类。

Result: 交通预测MAE为2.1，事故分类准确率100%，图像分类准确率92%，框架表现优于基线模型。

Conclusion: 模块化设计支持智能城市系统部署，推动智能交通系统的发展。

Abstract: This study proposes an integrated machine learning framework for advanced
traffic analysis, combining time-series forecasting, classification, and
computer vision techniques. The system utilizes an ARIMA(2,0,1) model for
traffic prediction (MAE: 2.1), an XGBoost classifier for accident severity
classification (100% accuracy on balanced data), and a Convolutional Neural
Network (CNN) for traffic image classification (92% accuracy). Tested on
diverse datasets, the framework outperforms baseline models and identifies key
factors influencing accident severity, including weather and road
infrastructure. Its modular design supports deployment in smart city systems
for real-time monitoring, accident prevention, and resource optimization,
contributing to the evolution of intelligent transportation systems.

</details>


### [171] [NeuralGrok: Accelerate Grokking by Neural Gradient Transformation](https://arxiv.org/abs/2504.17243)
*Xinyu Zhou,Simin Fan,Martin Jaggi,Jie Fu*

Main category: cs.LG

TL;DR: 提出NeuralGrok方法，通过动态梯度变换加速Transformer在算术任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Grokking现象，即模型在长时间过拟合后突然泛化的现象，探索如何加速这一过程。

Method: 基于双层优化算法，训练一个辅助模块动态调整梯度分量对泛化的贡献。

Result: 实验表明NeuralGrok显著加速泛化，并减少模型复杂度，优于传统正则化方法。

Conclusion: 通过提出AGE指标，揭示了NeuralGrok降低模型复杂度以促进泛化的机制，为理解泛化能力提供了新视角。

Abstract: Grokking is proposed and widely studied as an intricate phenomenon in which
generalization is achieved after a long-lasting period of overfitting. In this
work, we propose NeuralGrok, a novel gradient-based approach that learns an
optimal gradient transformation to accelerate the generalization of
transformers in arithmetic tasks. Specifically, NeuralGrok trains an auxiliary
module (e.g., an MLP block) in conjunction with the base model. This module
dynamically modulates the influence of individual gradient components based on
their contribution to generalization, guided by a bilevel optimization
algorithm. Our extensive experiments demonstrate that NeuralGrok significantly
accelerates generalization, particularly in challenging arithmetic tasks. We
also show that NeuralGrok promotes a more stable training paradigm, constantly
reducing the model's complexity, while traditional regularization methods, such
as weight decay, can introduce substantial instability and impede
generalization. We further investigate the intrinsic model complexity
leveraging a novel Absolute Gradient Entropy (AGE) metric, which explains that
NeuralGrok effectively facilitates generalization by reducing the model
complexity. We offer valuable insights on the grokking phenomenon of
Transformer models, which encourages a deeper understanding of the fundamental
principles governing generalization ability.

</details>


### [172] [Targeted AMP generation through controlled diffusion with efficient embeddings](https://arxiv.org/abs/2504.17247)
*Diogo Soares,Leon Hetzel,Paulina Szymczak,Fabian Theis,Stephan Günnemann,Ewa Szczurek*

Main category: cs.LG

TL;DR: OmegAMP是一种基于扩散生成模型的框架，用于高效生成具有特定性质的抗菌肽（AMP），显著降低假阳性率并提升多样性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在AMP发现中存在实验命中率低、可控性不足和建模效率低的问题，需要一种更高效、可控的解决方案。

Method: 采用扩散生成模型结合低维嵌入、精确可控机制和新型分类器，定向生成具有特定理化性质和活性的AMP。

Result: OmegAMP在AMP发现各阶段达到最先进性能，显著提升了对抗菌耐药性的计算研究潜力。

Conclusion: 该框架为AMP发现提供了高效、可控的生成方法，为解决抗菌耐药性问题提供了新工具。

Abstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical
challenges such as low experimental hit rates as well as the need for nuanced
controllability and efficient modeling of peptide properties. To address these
challenges, we introduce OmegAMP, a framework that leverages a diffusion-based
generative model with efficient low-dimensional embeddings, precise
controllability mechanisms, and novel classifiers with drastically reduced
false positive rates for candidate filtering. OmegAMP enables the targeted
generation of AMPs with specific physicochemical properties, activity profiles,
and species-specific effectiveness. Moreover, it maximizes sample diversity
while ensuring faithfulness to the underlying data distribution during
generation. We demonstrate that OmegAMP achieves state-of-the-art performance
across all stages of the AMP discovery pipeline, significantly advancing the
potential of computational frameworks in combating antimicrobial resistance.

</details>


### [173] [Group Downsampling with Equivariant Anti-aliasing](https://arxiv.org/abs/2504.17258)
*Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 论文提出了一种针对群等变架构（如G-CNNs）的均匀下采样层的通用方法，通过选择合适的子群和反混叠技术，改进图像分类任务的准确性和等变性，并减少模型大小。


<details>
  <summary>Details</summary>
Motivation: 在群等变架构（如G-CNNs）中，下采样层对学习高级特征和减少计算量至关重要。然而，如何在有限群上实现通用的下采样并避免混叠仍需研究。

Method: 针对给定的有限群和下采样率，(a)提出算法选择合适子群；(b)研究带限概念并提出反混叠方法。该方法扩展了经典采样理论，适用于循环群（周期性信号）时恢复理想低通滤波的标准下采样。

Result: 实验表明，提出的下采样方法在图像分类任务中提高了准确性，更好地保持了等变性，同时减少了G等变网络的模型大小。

Conclusion: 该方法为群等变架构提供了一种通用的下采样解决方案，显著提升了性能与效率，具有理论和实践意义。

Abstract: Downsampling layers are crucial building blocks in CNN architectures, which
help to increase the receptive field for learning high-level features and
reduce the amount of memory/computation in the model. In this work, we study
the generalization of the uniform downsampling layer for group equivariant
architectures, e.g., G-CNNs. That is, we aim to downsample signals (feature
maps) on general finite groups with anti-aliasing. This involves the following:
(a) Given a finite group and a downsampling rate, we present an algorithm to
form a suitable choice of subgroup. (b) Given a group and a subgroup, we study
the notion of bandlimited-ness and propose how to perform anti-aliasing.
Notably, our method generalizes the notion of downsampling based on classical
sampling theory. When the signal is on a cyclic group, i.e., periodic, our
method recovers the standard downsampling of an ideal low-pass filter followed
by a subsampling operation. Finally, we conducted experiments on image
classification tasks demonstrating that the proposed downsampling operation
improves accuracy, better preserves equivariance, and reduces model size when
incorporated into G-equivariant networks

</details>


### [174] [Symbolic Representation for Any-to-Any Generative Tasks](https://arxiv.org/abs/2504.17261)
*Jiaqi Chen,Xiaoye Zhu,Yue Wang,Tianyang Liu,Xinhui Chen,Ying Chen,Chak Tou Leong,Yifei Ke,Joseph Liu,Yiwen Yuan,Julian McAuley,Li-jia Li*

Main category: cs.LG

TL;DR: 提出一种符号化生成任务描述语言及推理引擎，能以结构化符号流表示任意多模态任务，无需大规模训练，支持高效、可编辑的任务执行。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型依赖大规模训练和隐式神经表示，计算成本高且灵活性有限，本文旨在通过显式符号表示解决这些问题。

Method: 基于三个核心符号原语（函数、参数、拓扑逻辑）构建框架，利用预训练语言模型将自然语言指令直接映射到符号化工作流，无需任务特定调优。

Result: 在12种多模态生成任务中表现优异，内容质量媲美或超越现有统一模型，同时具备更高效率、可编辑性和可中断性。

Conclusion: 符号化任务表示为生成式AI提供了低成本、可扩展的基础能力提升路径。

Abstract: We propose a symbolic generative task description language and a
corresponding inference engine capable of representing arbitrary multimodal
tasks as structured symbolic flows. Unlike conventional generative models that
rely on large-scale training and implicit neural representations to learn
cross-modal mappings, often at high computational cost and with limited
flexibility, our framework introduces an explicit symbolic representation
comprising three core primitives: functions, parameters, and topological logic.
Leveraging a pre-trained language model, our inference engine maps natural
language instructions directly to symbolic workflows in a training-free manner.
Our framework successfully performs over 12 diverse multimodal generative
tasks, demonstrating strong performance and flexibility without the need for
task-specific tuning. Experiments show that our method not only matches or
outperforms existing state-of-the-art unified models in content quality, but
also offers greater efficiency, editability, and interruptibility. We believe
that symbolic task representations provide a cost-effective and extensible
foundation for advancing the capabilities of generative AI.

</details>


### [175] [Signal Recovery from Random Dot-Product Graphs Under Local Differential Privacy](https://arxiv.org/abs/2504.17274)
*Siddharth Vishwanath,Jonathan Hehir*

Main category: cs.LG

TL;DR: 该论文研究了在ε-边局部差分隐私条件下从图中恢复潜在信息的问题，发现标准隐私机制会导致潜在位置的几何失真，并提出了一种调整后的统计推断方法，能实现潜在位置的一致性恢复且接近最优。


<details>
  <summary>Details</summary>
Motivation: 研究在隐私保护条件下从图中恢复潜在信息的可行性，扩展了隐私社区检测文献的应用范围。

Method: 利用广义随机点积图模型，调整统计推断程序以处理由隐私机制引入的几何失真。

Result: 证明了该方法能一致性恢复潜在位置且接近极小极大最优，并能恢复几何与拓扑信息。

Conclusion: 该方法在更丰富的模型和推断任务中扩展了隐私保护数据恢复的能力。

Abstract: We consider the problem of recovering latent information from graphs under
$\varepsilon$-edge local differential privacy where the presence of
relationships/edges between two users/vertices remains confidential, even from
the data curator. For the class of generalized random dot-product graphs, we
show that a standard local differential privacy mechanism induces a specific
geometric distortion in the latent positions. Leveraging this insight, we show
that consistent recovery of the latent positions is achievable by appropriately
adjusting the statistical inference procedure for the privatized graph.
Furthermore, we prove that our procedure is nearly minimax-optimal under local
edge differential privacy constraints. Lastly, we show that this framework
allows for consistent recovery of geometric and topological information
underlying the latent positions, as encoded in their persistence diagrams. Our
results extend previous work from the private community detection literature to
a substantially richer class of models and inferential tasks.

</details>


### [176] [HeRB: Heterophily-Resolved Structure Balancer for Graph Neural Networks](https://arxiv.org/abs/2504.17276)
*Ke-Jia Chen,Wenhui Mu,Zheng Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为HeRB的方法，通过减少异质性边并增加同质性边，以及传递同质性知识，解决了GNN中的结构不平衡问题。实验证明HeRB在多种数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: GNN在处理图数据时面临结构不平衡问题，现有方法未考虑异质性（即相连节点具有不同标签或特征），导致效果不佳。论文旨在通过解决异质性问题来改进这一挑战。

Method: HeRB方法包含两个创新组件：1）异质性减弱增强模块，减少类间边并增加类内边；2）同质性知识传递机制，将同质性信息从头部节点传递到尾部节点。

Result: 实验结果表明，HeRB在两个同质性和六个异质性基准数据集上表现优异，消融研究进一步验证了两个组件的有效性。

Conclusion: HeRB通过解决异质性并传递同质性知识，有效改善了GNN的结构不平衡问题，为图数据表示提供了新思路。

Abstract: Recent research has witnessed the remarkable progress of Graph Neural
Networks (GNNs) in the realm of graph data representation. However, GNNs still
encounter the challenge of structural imbalance. Prior solutions to this
problem did not take graph heterophily into account, namely that connected
nodes process distinct labels or features, thus resulting in a deficiency in
effectiveness. Upon verifying the impact of heterophily on solving the
structural imbalance problem, we propose to rectify the heterophily first and
then transfer homophilic knowledge. To the end, we devise a method named HeRB
(Heterophily-Resolved Structure Balancer) for GNNs. HeRB consists of two
innovative components: 1) A heterophily-lessening augmentation module which
serves to reduce inter-class edges and increase intra-class edges; 2) A
homophilic knowledge transfer mechanism to convey homophilic information from
head nodes to tail nodes. Experimental results demonstrate that HeRB achieves
superior performance on two homophilic and six heterophilic benchmark datasets,
and the ablation studies further validate the efficacy of two proposed
components.

</details>


### [177] [ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders](https://arxiv.org/abs/2504.17277)
*Zongliang Ji,Andre Carlos Kajdacsy-Balla Amaral,Anna Goldenberg,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 提出了名为ExOSITO的新方法，结合离策略学习和特权信息，优化ICU实验室检测订单，降低临床负担和成本，同时确保必要的检测。


<details>
  <summary>Details</summary>
Motivation: ICU中实验室检测的过度订购增加了医疗负担和环境成本，需要一种方法在保证必要信息的前提下减少不必要的检测。

Method: 结合离策略学习和特权信息，利用离线数据和临床规则设计的奖励函数，训练一个因果赌博机模型，生成可解释的检测订单策略。

Result: ExOSITO方法在不遗漏重要检测的情况下降低了成本，优于医生策略和现有方法。

Conclusion: ExOSITO为ICU实验室检测订单提供了一种优化的、可解释的解决方案，平衡了临床需求和资源节约。

Abstract: Ordering a minimal subset of lab tests for patients in the intensive care
unit (ICU) can be challenging. Care teams must balance between ensuring the
availability of the right information and reducing the clinical burden and
costs associated with each lab test order. Most in-patient settings experience
frequent over-ordering of lab tests, but are now aiming to reduce this burden
on both hospital resources and the environment. This paper develops a novel
method that combines off-policy learning with privileged information to
identify the optimal set of ICU lab tests to order. Our approach, EXplainable
Off-policy learning with Side Information for ICU blood Test Orders (ExOSITO)
creates an interpretable assistive tool for clinicians to order lab tests by
considering both the observed and predicted future status of each patient. We
pose this problem as a causal bandit trained using offline data and a reward
function derived from clinically-approved rules; we introduce a novel learning
framework that integrates clinical knowledge with observational data to bridge
the gap between the optimal and logging policies. The learned policy function
provides interpretable clinical information and reduces costs without omitting
any vital lab orders, outperforming both a physician's policy and prior
approaches to this practical problem.

</details>


### [178] [The Ultimate Cookbook for Invisible Poison: Crafting Subtle Clean-Label Text Backdoors with Style Attributes](https://arxiv.org/abs/2504.17300)
*Wencong You,Daniel Lowd*

Main category: cs.LG

TL;DR: 本文提出了一种更隐蔽且有效的文本分类器后门攻击方法AttrBkd，通过人类评估验证其不易被检测的特性和较高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击的触发词通常不自然或不符合语法，容易被人类标注者发现和过滤，因此需要更隐蔽的攻击方法。

Method: 提出了AttrBkd方法，通过从现有基线后门攻击中提取细粒度属性，构建不易被察觉的触发属性。

Result: 人类评估表明，AttrBkd比基线方法更具隐蔽性（更少被检测到）且攻击成功率更高。

Conclusion: 后门攻击可以通过隐蔽性绕过人类检测，同时保持高效，且现有自动化指标与人类判断存在不一致。

Abstract: Backdoor attacks on text classifiers can cause them to predict a predefined
label when a particular "trigger" is present. Prior attacks often rely on
triggers that are ungrammatical or otherwise unusual, leading to conspicuous
attacks. As a result, human annotators, who play a critical role in curating
training data in practice, can easily detect and filter out these unnatural
texts during manual inspection, reducing the risk of such attacks. We argue
that a key criterion for a successful attack is for text with and without
triggers to be indistinguishable to humans. However, prior work neither
directly nor comprehensively evaluated attack subtlety and invisibility with
human involvement. We bridge the gap by conducting thorough human evaluations
to assess attack subtlety. We also propose \emph{AttrBkd}, consisting of three
recipes for crafting subtle yet effective trigger attributes, such as
extracting fine-grained attributes from existing baseline backdoor attacks. Our
human evaluations find that AttrBkd with these baseline-derived attributes is
often more effective (higher attack success rate) and more subtle (fewer
instances detected by humans) than the original baseline backdoor attacks,
demonstrating that backdoor attacks can bypass detection by being inconspicuous
and appearing natural even upon close inspection, while still remaining
effective. Our human annotation also provides information not captured by
automated metrics used in prior work, and demonstrates the misalignment of
these metrics with human judgment.

</details>


### [179] [Machine learning-based condition monitoring of powertrains in modern electric drives](https://arxiv.org/abs/2504.17305)
*Dinan Li,Panagiotis Kakosimos,Luca Peretti*

Main category: cs.LG

TL;DR: 本文利用现代电力驱动中已有的数据，开发了一种数据驱动的热模型，并通过静态和动态操作剖面训练和验证了热数字孪生，比较了从传统线性模型到深度神经网络的不同方法，以选择最佳的温度估计模型。


<details>
  <summary>Details</summary>
Motivation: 数字化技术进步推动了工业领域的变革，数据分析和机器学习为工业系统的智能化提供了新可能。本文旨在利用电力驱动中的数据，优化工业资产性能。

Method: 设计了测试台，用于训练和验证热数字孪生，比较了传统线性模型和深度神经网络等方法。

Result: 通过多种评估指标，评估了不同方法在工业嵌入式系统中的性能和实现效果。

Conclusion: 研究表明，数据驱动的热模型可以有效地估计功率模块的壳温，为工业系统智能化提供了实用方法。

Abstract: The recent technological advances in digitalization have revolutionized the
industrial sector. Leveraging data analytics has now enabled the collection of
deep insights into the performance and, as a result, the optimization of
assets. Industrial drives, for example, already accumulate all the necessary
information to control electric machines. These signals include but are not
limited to currents, frequency, and temperature. Integrating machine learning
(ML) models responsible for predicting the evolution of those directly
collected or implicitly derived parameters enhances the smartness of industrial
systems even further. In this article, data already residing in most modern
electric drives has been used to develop a data-driven thermal model of a power
module. A test bench has been designed and used specifically for training and
validating the thermal digital twin undergoing various static and dynamic
operating profiles. Different approaches, from traditional linear models to
deep neural networks, have been implemented to emanate the best ML model for
estimating the case temperature of a power module. Several evaluation metrics
were then used to assess the investigated methods' performance and
implementation in industrial embedded systems.

</details>


### [180] [Class-Conditional Distribution Balancing for Group Robust Classification](https://arxiv.org/abs/2504.17314)
*Miaoyun Zhao,Qiang Zhang,Chenrong Li*

Main category: cs.LG

TL;DR: 该论文提出一种新方法，通过重新加权样本实现类条件分布平衡，消除对偏差标注的依赖，有效解决虚假相关性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决因虚假相关性导致的模型误判问题，传统方法依赖昂贵的偏差标注或大规模预训练，而本方法旨在无需这些资源即可实现鲁棒学习。

Method: 方法包括将虚假相关性重新定义为类条件分布不平衡，采用样本重新加权策略以减少虚假因素与标签信息间的互信息。

Result: 实验结果表明，该方法在无需偏差监督的情况下，性能与依赖偏差标注的方法相当，甚至更优。

Conclusion: 结论表明，该方法简单有效，可自动平衡分布并解决虚假相关性，适用于资源受限的领域。

Abstract: Spurious correlations that lead models to correct predictions for the wrong
reasons pose a critical challenge for robust real-world generalization.
Existing research attributes this issue to group imbalance and addresses it by
maximizing group-balanced or worst-group accuracy, which heavily relies on
expensive bias annotations. A compromise approach involves predicting bias
information using extensively pretrained foundation models, which requires
large-scale data and becomes impractical for resource-limited rare domains. To
address these challenges, we offer a novel perspective by reframing the
spurious correlations as imbalances or mismatches in class-conditional
distributions, and propose a simple yet effective robust learning method that
eliminates the need for both bias annotations and predictions. With the goal of
reducing the mutual information between spurious factors and label information,
our method leverages a sample reweighting strategy to achieve class-conditional
distribution balancing, which automatically highlights minority groups and
classes, effectively dismantling spurious correlations and producing a debiased
data distribution for classification. Extensive experiments and analysis
demonstrate that our approach consistently delivers state-of-the-art
performance, rivaling methods that rely on bias supervision.

</details>


### [181] [Collaborative Multi-Agent Reinforcement Learning for Automated Feature Transformation with Graph-Driven Path Optimization](https://arxiv.org/abs/2504.17355)
*Xiaohan Huang,Dongjie Wang,Zhiyuan Ning,Ziyue Qiao,Qingqing Long,Haowei Zhu,Yi Du,Min Wu,Yuanchun Zhou,Meng Xiao*

Main category: cs.LG

TL;DR: 该论文提出了一种名为TCTO的多智能体强化学习框架，通过图驱动路径优化自动化特征工程，解决了现有方法忽视变换步骤间动态依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的特征变换方法多将变换视为孤立操作，忽略了步骤间的动态依赖关系，导致效果不佳。为此，研究者提出TCTO框架，通过图建模动态优化特征变换。

Method: TCTO采用多智能体强化学习框架，构建了一个演化的交互图（节点为特征，边为变换），通过图剪枝和回溯动态优化路径，并支持历史子图复用。

Result: 实验证明，TCTO在多种数据集上表现优异，显著提升了特征工程的效率和下游任务性能。

Conclusion: TCTO通过动态图建模和强化学习协同优化特征变换，为自动化特征工程提供了可扩展且高效的解决方案。

Abstract: Feature transformation methods aim to find an optimal mathematical
feature-feature crossing process that generates high-value features and
improves the performance of downstream machine learning tasks. Existing
frameworks, though designed to mitigate manual costs, often treat feature
transformations as isolated operations, ignoring dynamic dependencies between
transformation steps. To address the limitations, we propose TCTO, a
collaborative multi-agent reinforcement learning framework that automates
feature engineering through graph-driven path optimization. The framework's
core innovation lies in an evolving interaction graph that models features as
nodes and transformations as edges. Through graph pruning and backtracking, it
dynamically eliminates low-impact edges, reduces redundant operations, and
enhances exploration stability. This graph also provides full traceability to
empower TCTO to reuse high-utility subgraphs from historical transformations.
To demonstrate the efficacy and adaptability of our approach, we conduct
comprehensive experiments and case studies, which show superior performance
across a range of datasets.

</details>


### [182] [Doubly Adaptive Social Learning](https://arxiv.org/abs/2504.17370)
*Marco Carpentiero,Virginia Bordignon,Vincenzo Matta,Ali H. Sayed*

Main category: cs.LG

TL;DR: 该论文提出了一种双重自适应社交学习策略（A²SL），用于解决动态漂移下社交学习中的决策问题。


<details>
  <summary>Details</summary>
Motivation: 在动态变化的真实假设和似然模型下，传统社交学习方法表现不佳，因此需要开发能够适应这些动态变化的策略。

Method: 论文采用了双阶段自适应机制：一是使用随机梯度下降更新学习决策模型的漂移；二是通过自适应信念更新跟踪变化的真实假设。这两个阶段由两个适应参数控制。

Result: 理论分析和实验（合成数据及真实数据）表明，当适应参数足够小时，所有代理均能一致学习并最终集中信念于真实假设，错误概率收敛至与适应参数同阶。

Conclusion: A²SL策略成功解决了动态环境下的社交学习问题，并通过理论和实证验证了其有效性。

Abstract: In social learning, a network of agents assigns probability scores (beliefs)
to some hypotheses of interest, which rule the generation of local streaming
data observed by each agent. Belief formation takes place by means of an
iterative two-step procedure where: i) the agents update locally their beliefs
by using some likelihood model; and ii) the updated beliefs are combined with
the beliefs of the neighboring agents, using a pooling rule. This procedure can
fail to perform well in the presence of dynamic drifts, leading the agents to
incorrect decision making. Here, we focus on the fully online setting where
both the true hypothesis and the likelihood models can change over time. We
propose the doubly adaptive social learning ($\text{A}^2\text{SL}$) strategy,
which infuses social learning with the necessary adaptation capabilities. This
goal is achieved by exploiting two adaptation stages: i) a stochastic gradient
descent update to learn and track the drifts in the decision model; ii) and an
adaptive belief update to track the true hypothesis changing over time. These
stages are controlled by two adaptation parameters that govern the evolution of
the error probability for each agent. We show that all agents learn
consistently for sufficiently small adaptation parameters, in the sense that
they ultimately place all their belief mass on the true hypothesis. In
particular, the probability of choosing the wrong hypothesis converges to
values on the order of the adaptation parameters. The theoretical analysis is
illustrated both on synthetic data and by applying the $\text{A}^2\text{SL}$
strategy to a social learning problem in the online setting using real data.

</details>


### [183] [Coding for Computation: Efficient Compression of Neural Networks for Reconfigurable Hardware](https://arxiv.org/abs/2504.17403)
*Hans Rosenberger,Rodrigo Fischer,Johanna S. Fröhlich,Ali Bereyhi,Ralf R. Müller*

Main category: cs.LG

TL;DR: 该论文提出了一种针对可重构硬件（如FPGA）的神经网络压缩方案，通过剪枝、权重共享和线性计算编码（LCC）减少推理时的加法运算量，而非传统的内存优化方法。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络规模增大，资源高效实现变得至关重要。传统压缩技术主要减少权重存储内存，但本文旨在硬件友好的方式下减少推理时的计算量。

Method: 结合剪枝（通过正则化训练）、权重共享和线性计算编码（LCC）优化计算效率，尤其针对加法运算。

Result: 方案在简单多层感知器和大型深度神经网络（如ResNet-34）上均表现优异。

Conclusion: 该方法为硬件端神经网络推理提供了高效的计算优化路径，尤其在加法运算减少方面具有竞争力。

Abstract: As state of the art neural networks (NNs) continue to grow in size, their
resource-efficient implementation becomes ever more important. In this paper,
we introduce a compression scheme that reduces the number of computations
required for NN inference on reconfigurable hardware such as FPGAs. This is
achieved by combining pruning via regularized training, weight sharing and
linear computation coding (LCC). Contrary to common NN compression techniques,
where the objective is to reduce the memory used for storing the weights of the
NNs, our approach is optimized to reduce the number of additions required for
inference in a hardware-friendly manner. The proposed scheme achieves
competitive performance for simple multilayer perceptrons, as well as for large
scale deep NNs such as ResNet-34.

</details>


### [184] [Towards Harnessing the Collaborative Power of Large and Small Models for Domain Tasks](https://arxiv.org/abs/2504.17421)
*Yang Liu,Bingjie Yan,Tianyuan Zou,Jianqing Zhang,Zixuan Gu,Jianbing Ding,Xidong Wang,Jingyi Li,Xiaozhou Ye,Ye Ouyang,Qiang Yang,Ya-Qin Zhang*

Main category: cs.LG

TL;DR: 论文主张大语言模型（LLMs）与小模型（SMs）协同合作，以提升LLMs在私有领域的适应性和效率，同时探讨了合作策略及行业驱动研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs表现出强大能力，但其依赖大量数据和计算资源，限制了在私有领域的应用。而SMs虽然能力较弱，但更高效且可针对性优化。因此，研究如何通过协作结合两者的优势，以解决LLMs在实际应用中的局限性。

Method: 论文提出了LLMs与SMs协同合作的策略，并通过分析合作模式，探讨潜在挑战与机遇。同时，呼吁行业推动研究，基于真实私有数据集和应用开发多目标基准。

Result: 研究展示了协同合作模式的潜力，能够加速LLMs在私有领域的适应性，并提高整体效率。

Conclusion: 论文认为，LLMs与SMs的协同合作是未来AI发展的重要方向，并建议通过行业驱动的多目标基准研究进一步验证其实际价值。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
they require vast amounts of data and computational resources. In contrast,
smaller models (SMs), while less powerful, can be more efficient and tailored
to specific domains. In this position paper, we argue that taking a
collaborative approach, where large and small models work synergistically, can
accelerate the adaptation of LLMs to private domains and unlock new potential
in AI. We explore various strategies for model collaboration and identify
potential challenges and opportunities. Building upon this, we advocate for
industry-driven research that prioritizes multi-objective benchmarks on
real-world private datasets and applications.

</details>


### [185] [CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated Active Learning](https://arxiv.org/abs/2504.17448)
*Jun Zhang,Jue Wang,Huan Li,Zhongle Xie,Ke Chen,Lidan Shou*

Main category: cs.LG

TL;DR: Federated Active Learning (FAL) 结合主动学习和联邦学习，解决了数据分布不均和模型参数波动的问题，但现有方法未考虑数据异构性。论文提出 CHASe 方法，通过跟踪认知变化、优化决策边界和高效数据选择，显著提升了模型性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的主动学习方法在数据多样性和标注预算有限的场景下效果不足，而现有联邦主动学习方法未能有效处理数据分布异构性和模型参数波动的问题。因此，需要一种新方法来解决这些挑战，提升模型性能。

Method: 提出 CHASe 方法，通过三个关键步骤：1) 跟踪认知变化（EVs），2) 使用新的对齐损失校准不准确模型的决策边界，3) 通过数据冻结和唤醒机制提高数据选择效率。

Result: 实验结果表明，CHASe 在多种数据集、模型复杂度和异构联邦设置下，在效果和效率上均优于现有基准方法。

Conclusion: CHASe 通过关注数据异构性和认知变化，显著提升了联邦主动学习的性能，同时保持了高效的数据选择机制。

Abstract: Active learning (AL) reduces human annotation costs for machine learning
systems by strategically selecting the most informative unlabeled data for
annotation, but performing it individually may still be insufficient due to
restricted data diversity and annotation budget. Federated Active Learning
(FAL) addresses this by facilitating collaborative data selection and model
training, while preserving the confidentiality of raw data samples. Yet,
existing FAL methods fail to account for the heterogeneity of data distribution
across clients and the associated fluctuations in global and local model
parameters, adversely affecting model accuracy. To overcome these challenges,
we propose CHASe (Client Heterogeneity-Aware Data Selection), specifically
designed for FAL. CHASe focuses on identifying those unlabeled samples with
high epistemic variations (EVs), which notably oscillate around the decision
boundaries during training. To achieve both effectiveness and efficiency,
\model{} encompasses techniques for 1) tracking EVs by analyzing inference
inconsistencies across training epochs, 2) calibrating decision boundaries of
inaccurate models with a new alignment loss, and 3) enhancing data selection
efficiency via a data freeze and awaken mechanism with subset sampling.
Experiments show that CHASe surpasses various established baselines in terms of
effectiveness and efficiency, validated across diverse datasets, model
complexities, and heterogeneous federation settings.

</details>


### [186] [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
*Jun Zhang,Jue Wang,Huan Li,Lidan Shou,Ke Chen,Gang Chen,Qin Xie,Guiming Xie,Xuejian Gong*

Main category: cs.LG

TL;DR: HMI系统通过分层知识管理优化多租户预训练语言模型推理，减少GPU内存占用并提升效率，实验表明单GPU可支持上万hPLMs。


<details>
  <summary>Details</summary>
Motivation: 针对预训练语言模型在多租户环境中的高计算资源需求问题，提出高效资源管理方案。

Method: 分层管理知识（通用/领域/任务相关）、构建hPLMs、优化系统（知识预取、批矩阵计算）。

Result: 单GPU支持10,000个hPLMs，精度损失可忽略。

Conclusion: HMI系统显著提升多租户环境下PLM的推理效率与资源利用率。

Abstract: The significant computational demands of pretrained language models (PLMs),
which often require dedicated hardware, present a substantial challenge in
serving them efficiently, especially in multi-tenant environments. To address
this, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant
Inference system, designed to manage tenants with distinct PLMs
resource-efficiently. Our approach is three-fold: Firstly, we categorize PLM
knowledge into general, domain-specific, and task-specific. Leveraging insights
on knowledge acquisition across different model layers, we construct
hierarchical PLMs (hPLMs) by extracting and storing knowledge at different
levels, significantly reducing GPU memory usage per tenant. Secondly, we
establish hierarchical knowledge management for hPLMs generated by various
tenants in HMI. We manage domain-specific knowledge with acceptable storage
increases by constructing and updating domain-specific knowledge trees based on
frequency. We manage task-specific knowledge within limited GPU memory through
parameter swapping. Finally, we propose system optimizations to enhance
resource utilization and inference throughput. These include fine-grained
pipelining via hierarchical knowledge prefetching to overlap CPU and I/O
operations with GPU computations, and optimizing parallel implementations with
batched matrix multiplications. Our experimental results demonstrate that the
proposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a
single GPU, with only a negligible compromise in accuracy.

</details>


### [187] [Evaluating Time Series Models for Urban Wastewater Management: Predictive Performance, Model Complexity and Resilience](https://arxiv.org/abs/2504.17461)
*Vipin Singh,Tianheng Ling,Teodor Chiaburu,Felix Biessmann*

Main category: cs.LG

TL;DR: 论文提出了一种评估神经网络架构用于城市合流制排水系统时间序列预测的协议，比较了全局和局部模型的性能及稳健性，并探讨了模型在极端事件中的表现。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致极端降雨频率增加，给城市合流制排水系统带来压力，传统物理模型成本高且难以适应动态变化，机器学习提供了一种更具适应性的低成本替代方案。

Method: 提出了一种评估神经网络架构的协议，通过比较全局模型和局部模型的预测性能、模型复杂性和对扰动的稳健性，评估了模型在峰值事件和关键波动中的表现。

Result: 全局模型预测性能更高，但局部模型在分散化场景中提供了足够的稳健性；具有更长预测周期的模型对数据扰动表现更稳健。

Conclusion: 研究为可持续城市废水管理提供了可解释且可靠的机器学习解决方案，相关实现已开源。

Abstract: Climate change increases the frequency of extreme rainfall, placing a
significant strain on urban infrastructures, especially Combined Sewer Systems
(CSS). Overflows from overburdened CSS release untreated wastewater into
surface waters, posing environmental and public health risks. Although
traditional physics-based models are effective, they are costly to maintain and
difficult to adapt to evolving system dynamics. Machine Learning (ML)
approaches offer cost-efficient alternatives with greater adaptability. To
systematically assess the potential of ML for modeling urban infrastructure
systems, we propose a protocol for evaluating Neural Network architectures for
CSS time series forecasting with respect to predictive performance, model
complexity, and robustness to perturbations. In addition, we assess model
performance on peak events and critical fluctuations, as these are the key
regimes for urban wastewater management. To investigate the feasibility of
lightweight models suitable for IoT deployment, we compare global models, which
have access to all information, with local models, which rely solely on nearby
sensor readings. Additionally, to explore the security risks posed by network
outages or adversarial attacks on urban infrastructure, we introduce error
models that assess the resilience of models. Our results demonstrate that while
global models achieve higher predictive performance, local models provide
sufficient resilience in decentralized scenarios, ensuring robust modeling of
urban infrastructure. Furthermore, models with longer native forecast horizons
exhibit greater robustness to data perturbations. These findings contribute to
the development of interpretable and reliable ML solutions for sustainable
urban wastewater management. The implementation is available in our GitHub
repository.

</details>


### [188] [GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework](https://arxiv.org/abs/2504.17471)
*Yacine Belal,Mohamed Maouche,Sonia Ben Mokhtar,Anthony Simonet-Boulogne*

Main category: cs.LG

TL;DR: 论文提出了GRANITE框架，用于在稀疏动态图中抵御拜占庭节点的攻击，通过历史感知的拜占庭弹性Peer Sampling协议（HaPS）和自适应概率阈值（APT）实现鲁棒学习，实验表明其在30%拜占庭节点下仍能保持收敛。


<details>
  <summary>Details</summary>
Motivation: 现有Gossip Learning（GL）方法在动态图中收敛快，但缺乏对拜占庭攻击的鲁棒性，尤其是当拜占庭节点通过攻击RPS协议放大模型投毒时。

Method: GRANITE结合了HaPS协议（通过跟踪历史标识减少对抗影响）和APT（基于拜占庭节点比例动态设定聚合阈值），实现鲁棒学习。

Result: 实验证明，GRANITE能在30%拜占庭节点下保持收敛，学习速度更快，且支持比现有理论稀疏9倍的图结构。

Conclusion: GRANITE为稀疏动态图中的拜占庭攻击提供了有效解决方案，兼具鲁棒性和效率。

Abstract: Gossip Learning (GL) is a decentralized learning paradigm where users
iteratively exchange and aggregate models with a small set of neighboring
peers. Recent GL approaches rely on dynamic communication graphs built and
maintained using Random Peer Sampling (RPS) protocols. Thanks to graph
dynamics, GL can achieve fast convergence even over extremely sparse
topologies. However, the robustness of GL over dy- namic graphs to Byzantine
(model poisoning) attacks remains unaddressed especially when Byzantine nodes
attack the RPS protocol to scale up model poisoning. We address this issue by
introducing GRANITE, a framework for robust learning over sparse, dynamic
graphs in the presence of a fraction of Byzantine nodes. GRANITE relies on two
key components (i) a History-aware Byzantine-resilient Peer Sampling protocol
(HaPS), which tracks previously encountered identifiers to reduce adversarial
influence over time, and (ii) an Adaptive Probabilistic Threshold (APT), which
leverages an estimate of Byzantine presence to set aggregation thresholds with
formal guarantees. Empirical results confirm that GRANITE maintains convergence
with up to 30% Byzantine nodes, improves learning speed via adaptive filtering
of poisoned models and obtains these results in up to 9 times sparser graphs
than dictated by current theory.

</details>


### [189] [Plasticine: Accelerating Research in Plasticity-Motivated Deep Reinforcement Learning](https://arxiv.org/abs/2504.17490)
*Mingqi Yuan,Qi Wang,Guozheng Ma,Bo Li,Xin Jin,Yunbo Wang,Xiaokang Yang,Wenjun Zeng,Dacheng Tao*

Main category: cs.LG

TL;DR: Plasticine是首个开源的深度强化学习(RL)框架，用于基准测试可塑性优化，提供13种缓解方法、10种评估指标及不同非平稳性场景。


<details>
  <summary>Details</summary>
Motivation: 深度RL系统存在可塑性损失问题(神经网络逐渐丧失适应能力)，但缺乏统一的基准和评估协议。

Method: 开发Plasticine框架，集成多种缓解方法、评估指标及渐进非平稳环境场景。

Result: 提供系统化工具量化可塑性损失、评估缓解策略，并分析不同上下文中的可塑性动态。

Conclusion: Plasticine填补了可塑性优化领域的空白，支持标准化研究。

Abstract: Developing lifelong learning agents is crucial for artificial general
intelligence. However, deep reinforcement learning (RL) systems often suffer
from plasticity loss, where neural networks gradually lose their ability to
adapt during training. Despite its significance, this field lacks unified
benchmarks and evaluation protocols. We introduce Plasticine, the first
open-source framework for benchmarking plasticity optimization in deep RL.
Plasticine provides single-file implementations of over 13 mitigation methods,
10 evaluation metrics, and learning scenarios with increasing non-stationarity
levels from standard to open-ended environments. This framework enables
researchers to systematically quantify plasticity loss, evaluate mitigation
strategies, and analyze plasticity dynamics across different contexts. Our
documentation, examples, and source code are available at
https://github.com/RLE-Foundation/Plasticine.

</details>


### [190] [Prototype-enhanced prediction in graph neural networks for climate applications](https://arxiv.org/abs/2504.17492)
*Nawid Keshtmand,Elena Fillola,Jeffrey Nicholas Clark,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.LG

TL;DR: 论文提出了一种使用原型提升数据驱动仿真器性能的方法，通过将原型的近似输出作为输入来提高预测质量，并在大气扩散仿真中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动仿真器虽能减少计算成本，但其高维输出质量有待提升。本文旨在通过引入原型（即仿真器输出的近似值）作为额外输入，以提高仿真精度。

Method: 研究比较了基线模型与使用原型作为附加输入的模型，其中原型通过随机选择或数据驱动方法（如k均值）生成。

Result: 原型模型表现更优，即使原型数量少或随机选择也能提升性能，而通过k均值等方法选择原型可使某些指标提升近10%。

Conclusion: 原型作为额外输入能显著提升仿真器性能，数据驱动的原型选择方法进一步优化了结果。

Abstract: Data-driven emulators are increasingly being used to learn and emulate
physics-based simulations, reducing computational expense and run time. Here,
we present a structured way to improve the quality of these high-dimensional
emulated outputs, through the use of prototypes: an approximation of the
emulator's output passed as an input, which informs the model and leads to
better predictions. We demonstrate our approach to emulate atmospheric
dispersion, key for greenhouse gas emissions monitoring, by comparing a
baseline model to models trained using prototypes as an additional input. The
prototype models achieve better performance, even with few prototypes and even
if they are chosen at random, but we show that choosing the prototypes through
data-driven methods (k-means) can lead to almost 10\% increased performance in
some metrics.

</details>


### [191] [Goal-Oriented Time-Series Forecasting: Foundation Framework Design](https://arxiv.org/abs/2504.17493)
*Luca-Andrei Fechete,Mohamed Sana,Fadhel Ayed,Nicola Piovesan,Wenjie Li,Antonio De Domenico,Tareq Si Salem*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列预测训练方法，能够根据终端应用的需求动态调整预测范围的重要性，而非固定范围。该方法通过分段预测和动态加权提高了预测精度，并在多个数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法仅关注最小化预测误差，而忽略了实际应用中预测范围的重要性需求。本文旨在解决这一问题，通过动态调整预测权重，使模型更符合实际应用的需求。

Method: 提出了一种新的训练方法，将整个信号范围划分为小段预测，并根据终端应用的需求动态加权组合这些预测段，以提高预测准确性。

Result: 在包括无线通信数据集在内的多个标准数据集上测试表明，该方法不仅提升了预测精度，还改善了终端应用的性能。

Conclusion: 本研究为构建更紧密连接预测与实际决策的系统奠定了基础，适用于多种实际应用场景。

Abstract: Traditional time-series forecasting often focuses only on minimizing
prediction errors, ignoring the specific requirements of real-world
applications that employ them. This paper presents a new training methodology,
which allows a forecasting model to dynamically adjust its focus based on the
importance of forecast ranges specified by the end application. Unlike previous
methods that fix these ranges beforehand, our training approach breaks down
predictions over the entire signal range into smaller segments, which are then
dynamically weighted and combined to produce accurate forecasts. We tested our
method on standard datasets, including a new dataset from wireless
communication, and found that not only it improves prediction accuracy but also
improves the performance of end application employing the forecasting model.
This research provides a basis for creating forecasting systems that better
connect prediction and decision-making in various practical applications.

</details>


### [192] [Combining GCN Structural Learning with LLM Chemical Knowledge for or Enhanced Virtual Screening](https://arxiv.org/abs/2504.17497)
*Radia Berreziga,Mohammed Brahimi,Khairedine Kraim,Hamid Azzoune*

Main category: cs.LG

TL;DR: 论文提出一种结合GCN与LLM嵌入的混合架构，用于虚拟筛选，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖预定义分子表示可能导致信息丢失或偏差，而深度学习方法如GCN和LLM能更全面捕捉化学信息。

Method: 提出混合架构，将GCN的局部结构学习与LLM的全局化学知识结合，并在每层GCN后连接LLM嵌入以提升性能。

Result: 模型F1-score达88.8%，优于GCN（87.9%）、XGBoost（85.5%）和SVM（85.4%）。

Conclusion: 混合架构通过融合局部与全局信息，显著提升虚拟筛选效果，且计算高效。

Abstract: Virtual screening plays a critical role in modern drug discovery by enabling
the identification of promising candidate molecules for experimental
validation. Traditional machine learning methods such as support vector
machines (SVM) and XGBoost rely on predefined molecular representations, often
leading to information loss and potential bias. In contrast, deep learning
approaches-particularly Graph Convolutional Networks (GCNs)-offer a more
expressive and unbiased alternative by operating directly on molecular graphs.
Meanwhile, Large Language Models (LLMs) have recently demonstrated
state-of-the-art performance in drug design, thanks to their capacity to
capture complex chemical patterns from large-scale data via attention
mechanisms.
  In this paper, we propose a hybrid architecture that integrates GCNs with
LLM-derived embeddings to combine localized structural learning with global
chemical knowledge. The LLM embeddings can be precomputed and stored in a
molecular feature library, removing the need to rerun the LLM during training
or inference and thus maintaining computational efficiency. We found that
concatenating the LLM embeddings after each GCN layer-rather than only at the
final layer-significantly improves performance, enabling deeper integration of
global context throughout the network. The resulting model achieves superior
results, with an F1-score of (88.8%), outperforming standalone GCN (87.9%),
XGBoost (85.5%), and SVM (85.4%) baselines.

</details>


### [193] [Tailored minimal reservoir computing: on the bidirectional connection between nonlinearities in the reservoir and in data](https://arxiv.org/abs/2504.17503)
*Davide Prosperino,Haochun Ma,Christoph Räth*

Main category: cs.LG

TL;DR: 研究了输入数据的非线性程度如何影响储层计算机的最优设计，发现储层的非线性与数据非线性匹配时预测性能最佳，并提出了一种估计未知时间序列最小非线性的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨储层计算机的非线性设计如何与输入数据的非线性特性匹配，以优化预测性能。

Method: 通过简化储层计算机为单一可调非线性参数，并使用分数阶Halvorsen系统作为测试平台，分析预测性能与非线性匹配的关系。

Result: 预测性能在储层非线性与数据非线性匹配时达到最佳；在数据存在多种非线性时，匹配最小非线性可正确重构信号的关联维数。

Conclusion: 提出了一种实用的非线性匹配方法，并验证了其在合成和实际数据集上的有效性，为储层计算机的设计提供了理论依据。

Abstract: We study how the degree of nonlinearity in the input data affects the optimal
design of reservoir computers, focusing on how closely the model's nonlinearity
should align with that of the data. By reducing minimal RCs to a single tunable
nonlinearity parameter, we explore how the predictive performance varies with
the degree of nonlinearity in the reservoir. To provide controlled testbeds, we
generalize to the fractional Halvorsen system, a novel chaotic system with
fractional exponents. Our experiments reveal that the prediction performance is
maximized when the reservoir's nonlinearity matches the nonlinearity present in
the data. In cases where multiple nonlinearities are present in the data, we
find that the correlation dimension of the predicted signal is reconstructed
correctly when the smallest nonlinearity is matched. We use this observation to
propose a method for estimating the minimal nonlinearity in unknown time series
by sweeping the reservoir exponent and identifying the transition to a
successful reconstruction. Applying this method to both synthetic and
real-world datasets, including financial time series, we demonstrate its
practical viability. Finally, we transfer these insights to classical RC by
augmenting traditional architectures with fractional, generalized reservoir
states. This yields performance gains, particularly in resource-constrained
scenarios such as physical reservoirs, where increasing reservoir size is
impractical or economically unviable. Our work provides a principled route
toward tailoring RCs to the intrinsic complexity of the systems they aim to
model.

</details>


### [194] [Communication-Efficient Personalized Distributed Learning with Data and Node Heterogeneity](https://arxiv.org/abs/2504.17520)
*Zhuojun Tian,Zhaoyang Zhang,Yiwei Li,Mehdi Bennis*

Main category: cs.LG

TL;DR: 论文提出了一种分布式强彩票假设（DSLTH）来应对去中心化学习中的数据与节点异质性挑战，并开发了一种通信高效的个性化学习算法。通过固定全局参数并更新个性化二进制掩码实现本地模型学习，结合群稀疏正则化降低硬件复杂度，并通过个性化调整步骤优化模型。理论证明与数值实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习中数据和节点异质性带来的挑战，同时满足个性化需求并降低通信与硬件实现开销。

Method: 1. 提出DSLTH假设；2. 本地模型表示为全局参数与个性化二进制掩码的Hadamard积；3. 固定全局参数，更新并融合掩码；4. 引入群稀疏正则化实现结构化稀疏；5. 设计二进制掩码聚合算法，加入个性化微调步骤。

Result: 理论证明了DSLTH的有效性，数值实验显示算法在异质性条件下能高效实现个性化学习。

Conclusion: 所提方法通过掩码个性化与结构化稀疏优化，在保证通信效率的同时适应了节点异质性需求。

Abstract: To jointly tackle the challenges of data and node heterogeneity in
decentralized learning, we propose a distributed strong lottery ticket
hypothesis (DSLTH), based on which a communication-efficient personalized
learning algorithm is developed. In the proposed method, each local model is
represented as the Hadamard product of global real-valued parameters and a
personalized binary mask for pruning. The local model is learned by updating
and fusing the personalized binary masks while the real-valued parameters are
fixed among different agents. To further reduce the complexity of hardware
implementation, we incorporate a group sparse regularization term in the loss
function, enabling the learned local model to achieve structured sparsity.
Then, a binary mask aggregation algorithm is designed by introducing an
intermediate aggregation tensor and adding a personalized fine-tuning step in
each iteration, which constrains model updates towards the local data
distribution. The proposed method effectively leverages the relativity among
agents while meeting personalized requirements in heterogeneous node
conditions. We also provide a theoretical proof for the DSLTH, establishing it
as the foundation of the proposed method. Numerical simulations confirm the
validity of the DSLTH and demonstrate the effectiveness of the proposed
algorithm.

</details>


### [195] [Cooperative Task Offloading through Asynchronous Deep Reinforcement Learning in Mobile Edge Computing for Future Networks](https://arxiv.org/abs/2504.17526)
*Yuelin Liu,Haiyuan Li,Xenofon Vasilakos,Rasheed Hussain,Dimitra Simeonidou*

Main category: cs.LG

TL;DR: 论文提出了一个基于Transformer预测的协作任务卸载框架CTO-TP，利用异步多智能体深度强化学习优化边缘计算中的任务卸载和资源分配，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 未来网络（如6G）将带来高计算资源需求，传统MEC单服务器任务卸载可能导致资源利用不均和性能不佳，集中式决策又存在延迟和计算瓶颈。

Method: 提出CTO-TP框架，结合Transformer预测和异步多智能体深度强化学习，实现边缘协作和异步训练，优化分布式网络中的任务卸载与资源分配。

Result: 实验表明，CTO-TP相比基线方案降低了80%的系统延迟和87%的能耗。

Conclusion: CTO-TP有效解决了边缘计算中的延迟和能耗问题，为未来网络的高效任务卸载提供了可行方案。

Abstract: Future networks (including 6G) are poised to accelerate the realisation of
Internet of Everything. However, it will result in a high demand for computing
resources to support new services. Mobile Edge Computing (MEC) is a promising
solution, enabling to offload computation-intensive tasks to nearby edge
servers from the end-user devices, thereby reducing latency and energy
consumption. However, relying solely on a single MEC server for task offloading
can lead to uneven resource utilisation and suboptimal performance in complex
scenarios. Additionally, traditional task offloading strategies specialise in
centralised policy decisions, which unavoidably entail extreme transmission
latency and reach computational bottleneck. To fill the gaps, we propose a
latency and energy efficient Cooperative Task Offloading framework with
Transformer-driven Prediction (CTO-TP), leveraging asynchronous multi-agent
deep reinforcement learning to address these challenges. This approach fosters
edge-edge cooperation and decreases the synchronous waiting time by performing
asynchronous training, optimising task offloading, and resource allocation
across distributed networks. The performance evaluation demonstrates that the
proposed CTO-TP algorithm reduces up to 80% overall system latency and 87%
energy consumption compared to the baseline schemes.

</details>


### [196] [TACO: Tackling Over-correction in Federated Learning with Tailored Adaptive Correction](https://arxiv.org/abs/2504.17528)
*Weijie Liu,Ziwei Zhan,Carlee Joe-Wong,Edith Ngai,Jingpu Duan,Deke Guo,Xu Chen,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 论文提出了TACO算法，通过细粒度的客户端特定梯度校正和模型聚合来解决联邦学习中非独立同分布数据问题，同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法因统一模型校正系数存在过校正现象，可能降低模型性能甚至导致收敛失败，需设计一种兼顾准确性和效率的解决方案。

Method: 提出TACO算法，采用细粒度客户端特定梯度校正和轻量级聚合，避免额外计算开销。

Result: 实验验证TACO在多种数据集上表现优越且稳定，首次揭示了过校正的根源。

Conclusion: TACO有效解决了非独立同分布数据问题，提升了联邦学习在边缘计算环境中的性能与效率。

Abstract: Non-independent and identically distributed (Non-IID) data across edge
clients have long posed significant challenges to federated learning (FL)
training in edge computing environments. Prior works have proposed various
methods to mitigate this statistical heterogeneity. While these works can
achieve good theoretical performance, in this work we provide the first
investigation into a hidden over-correction phenomenon brought by the uniform
model correction coefficients across clients adopted by existing methods. Such
over-correction could degrade model performance and even cause failures in
model convergence. To address this, we propose TACO, a novel algorithm that
addresses the non-IID nature of clients' data by implementing fine-grained,
client-specific gradient correction and model aggregation, steering local
models towards a more accurate global optimum. Moreover, we verify that leading
FL algorithms generally have better model accuracy in terms of communication
rounds rather than wall-clock time, resulting from their extra computation
overhead imposed on clients. To enhance the training efficiency, TACO deploys a
lightweight model correction and tailored aggregation approach that requires
minimum computation overhead and no extra information beyond the synchronized
model parameters. To validate TACO's effectiveness, we present the first FL
convergence analysis that reveals the root cause of over-correction. Extensive
experiments across various datasets confirm TACO's superior and stable
performance in practice.

</details>


### [197] [Learning Isometric Embeddings of Road Networks using Multidimensional Scaling](https://arxiv.org/abs/2504.17534)
*Juan Carlos Climent Pardo*

Main category: cs.LG

TL;DR: 该论文探讨了如何利用图表示和多维缩放（MDS）技术解决基于学习的自动驾驶应用中缺乏泛化能力的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的自动驾驶应用缺乏泛化能力，仅限于有限的场景。论文旨在设计能够捕捉多样化道路结构和动态环境的特征空间。

Method: 采用图表示道路网络，并结合多维缩放（MDS）技术来生成特征空间。还分析了最先进的图表示和MDS方法。

Result: 研究表明，通过图节点嵌入可以实现更简单的学习过程和降维。

Conclusion: 论文提出了一种利用图表示和MDS技术提升自动驾驶泛化能力的方法，为未来研究提供了方向。

Abstract: The lack of generalization in learning-based autonomous driving applications
is shown by the narrow range of road scenarios that vehicles can currently
cover. A generalizable approach should capture many distinct road structures
and topologies, as well as consider traffic participants, and dynamic changes
in the environment, so that vehicles can navigate and perform motion planning
tasks even in the most difficult situations. Designing suitable feature spaces
for neural network-based motion planers that encapsulate all kinds of road
scenarios is still an open research challenge. This paper tackles this
learning-based generalization challenge and shows how graph representations of
road networks can be leveraged by using multidimensional scaling (MDS)
techniques in order to obtain such feature spaces. State-of-the-art graph
representations and MDS approaches are analyzed for the autonomous driving use
case. Finally, the option of embedding graph nodes is discussed in order to
perform easier learning procedures and obtain dimensionality reduction.

</details>


### [198] [Beyond Cox Models: Assessing the Performance of Machine-Learning Methods in Non-Proportional Hazards and Non-Linear Survival Analysis](https://arxiv.org/abs/2504.17568)
*Ivan Rossi,Flavio Sartori,Cesare Rollo,Giovanni Birolo,Piero Fariselli,Tiziana Sanavia*

Main category: cs.LG

TL;DR: 论文评估了机器学习和深度学习方法在生存分析中的表现，发现某些情况下这些方法优于传统Cox模型，并提出了更合适的性能评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统Cox模型依赖线性性和比例风险假设，研究旨在探索放宽这些约束的替代方法。

Method: 测试了八种模型（包括六种非线性和四种非比例风险模型），使用合成和真实数据集，比较Harrell's和Antolini's C-index及Brier's score。

Result: 结果表明在特定条件下（如非线性和非比例风险），机器学习和深度学习方法表现更优。

Conclusion: 建议根据数据特性选择合适方法，并提供了开源代码以促进复现。

Abstract: Survival analysis often relies on Cox models, assuming both linearity and
proportional hazards (PH). This study evaluates machine and deep learning
methods that relax these constraints, comparing their performance with
penalized Cox models on a benchmark of three synthetic and three real datasets.
In total, eight different models were tested, including six non-linear models
of which four were also non-PH. Although Cox regression often yielded
satisfactory performance, we showed the conditions under which machine and deep
learning models can perform better. Indeed, the performance of these methods
has often been underestimated due to the improper use of Harrell's concordance
index (C-index) instead of more appropriate scores such as Antolini's
concordance index, which generalizes C-index in cases where the PH assumption
does not hold. In addition, since occasionally high C-index models happen to be
badly calibrated, combining Antolini's C-index with Brier's score is useful to
assess the overall performance of a survival method. Results on our benchmark
data showed that survival prediction should be approached by testing different
methods to select the most appropriate one according to sample size,
non-linearity and non-PH conditions. To allow an easy reproducibility of these
tests on our benchmark data, code and documentation are freely available at
https://github.com/compbiomed-unito/survhive.

</details>


### [199] [TileLang: A Composable Tiled Programming Model for AI Systems](https://arxiv.org/abs/2504.17577)
*Lei Wang,Yu Cheng,Yining Shi,Zhengju Tang,Zhiwen Mo,Wenhao Xie,Lingxiao Ma,Yuqing Xia,Jilong Xue,Fan Yang,Zhi Yang*

Main category: cs.LG

TL;DR: TileLang是一种通用的分块编程模型，旨在简化高性能AI内核编程，通过分离调度空间与数据流，让开发者更专注于数据流本身，同时由编译器处理大多数优化。实验证明其能实现顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载依赖优化的计算内核，但编写高性能内核复杂且需硬件优化，现有编译器在可用性和表达性上有不足。

Method: 提出TileLang，通过将调度空间（线程绑定、布局、张量化与流水线）与数据流解耦，封装为自定义注解和原语。

Result: 在多种设备和实验中，TileLang在关键内核中实现了最先进的性能。

Conclusion: TileLang的统一分块-线程范式及透明调度能力，满足了现代AI系统开发对性能与灵活性的需求。

Abstract: Modern AI workloads rely heavily on optimized computing kernels for both
training and inference. These AI kernels follow well-defined data-flow
patterns, such as moving tiles between DRAM and SRAM and performing a sequence
of computations on those tiles. However, writing high-performance kernels
remains complex despite the clarity of these patterns. Achieving peak
performance requires careful, hardware-centric optimizations to fully leverage
modern accelerators. While domain-specific compilers attempt to reduce the
burden of writing high-performance kernels, they often struggle with usability
and expressiveness gaps. In this paper, we present TileLang, a generalized
tiled programming model for more efficient AI Kernel programming. TileLang
decouples scheduling space (thread binding, layout, tensorize and pipeline)
from dataflow, and encapsulated them as a set of customization annotations and
primitives. This approach allows users to focus on the kernel's data-flow
itself, while leaving most other optimizations to compilers. We conduct
comprehensive experiments on commonly-used devices, across numerous
experiments, our evaluation shows that TileLang can achieve state-of-the-art
performance in key kernels, demonstrating that its unified block-and-thread
paradigm and transparent scheduling capabilities deliver both the power and
flexibility demanded by modern AI system development.

</details>


### [200] [Advancing CMA-ES with Learning-Based Cooperative Coevolution for Scalable Optimization](https://arxiv.org/abs/2504.17578)
*Hongshu Guo,Wenjie Qiu,Zeyuan Ma,Xinglin Zhang,Jun Zhang,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: 本文提出了一种基于学习的协同进化框架LCC，通过神经网络动态选择分解策略，提升了大规模全局优化问题的解决效果，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的协同进化方法在选择或设计变量分解策略时需要深厚的专业知识，这限制了其应用。本文受元黑盒优化的启发，旨在通过动态调度分解策略来克服这一局限。

Method: LCC框架采用神经网络参数化分解策略选择器，利用精心设计的优化状态特征动态选择最优策略，并通过强化学习方法（PPO）在代表性问题上训练网络以最大化优化性能。

Result: 实验表明，LCC在优化效果和资源消耗上优于现有方法，并展现出对未见问题的良好迁移性。

Conclusion: LCC通过学习动态调度分解策略，解决了传统协同进化方法对专家知识的依赖问题，为大规模优化提供了更高效的解决方案。

Abstract: Recent research in Cooperative Coevolution~(CC) have achieved promising
progress in solving large-scale global optimization problems. However, existing
CC paradigms have a primary limitation in that they require deep expertise for
selecting or designing effective variable decomposition strategies. Inspired by
advancements in Meta-Black-Box Optimization, this paper introduces LCC, a
pioneering learning-based cooperative coevolution framework that dynamically
schedules decomposition strategies during optimization processes. The
decomposition strategy selector is parameterized through a neural network,
which processes a meticulously crafted set of optimization status features to
determine the optimal strategy for each optimization step. The network is
trained via the Proximal Policy Optimization method in a reinforcement learning
manner across a collection of representative problems, aiming to maximize the
expected optimization performance. Extensive experimental results demonstrate
that LCC not only offers certain advantages over state-of-the-art baselines in
terms of optimization effectiveness and resource consumption, but it also
exhibits promising transferability towards unseen problems.

</details>


### [201] [Interpretable non-linear dimensionality reduction using gaussian weighted linear transformation](https://arxiv.org/abs/2504.17601)
*Erik Bergh*

Main category: cs.LG

TL;DR: 该论文提出了一种结合线性和非线性降维方法优势的新算法，既能保留线性方法的可解释性，又具备非线性变换的表达能力。通过高斯函数加权线性变换，构建高维与低维空间的非线性映射，同时提供工具帮助理解变换过程。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法（如t-SNE和PCA）在表达能力和可解释性之间存在权衡，本文旨在填补这一空白，提供既能有效降维又易于解释的方法。

Method: 算法通过高斯函数加权的线性变换组合构建非线性映射，每步变换可独立分析，同时开发了用户友好的软件包以促进实用。

Result: 所提模型实现了强大的降维能力，并保留了线性方法的可解释性，提供了对变换空间的透明洞察。

Conclusion: 该算法通过结合线性和非线性方法的优势，为高维数据分析和可视化提供了高效且可解释的解决方案，适合学术和工业应用。

Abstract: Dimensionality reduction techniques are fundamental for analyzing and
visualizing high-dimensional data. With established methods like t-SNE and PCA
presenting a trade-off between representational power and interpretability.
This paper introduces a novel approach that bridges this gap by combining the
interpretability of linear methods with the expressiveness of non-linear
transformations. The proposed algorithm constructs a non-linear mapping between
high-dimensional and low-dimensional spaces through a combination of linear
transformations, each weighted by Gaussian functions. This architecture enables
complex non-linear transformations while preserving the interpretability
advantages of linear methods, as each transformation can be analyzed
independently. The resulting model provides both powerful dimensionality
reduction and transparent insights into the transformed space. Techniques for
interpreting the learned transformations are presented, including methods for
identifying suppressed dimensions and how space is expanded and contracted.
These tools enable practitioners to understand how the algorithm preserves and
modifies geometric relationships during dimensionality reduction. To ensure the
practical utility of this algorithm, the creation of user-friendly software
packages is emphasized, facilitating its adoption in both academia and
industry.

</details>


### [202] [TarDiff: Target-Oriented Diffusion Guidance for Synthetic Electronic Health Record Time Series Generation](https://arxiv.org/abs/2504.17613)
*Bowen Deng,Chang Xu,Hao Li,Yuhao Huang,Min Hou,Jiang Bian*

Main category: cs.LG

TL;DR: TarDiff是一种新型的目标导向扩散框架，通过任务特定影响指导生成合成EHR时间序列数据，显著提升下游模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注数据分布的复现，但忽略了模型性能的提升需求，尤其是稀有但重要条件的表示不足问题。

Method: 提出TarDiff框架，利用影响函数量化合成数据对下游任务损失的减少，并将其融入反向扩散过程，生成优化数据。

Result: 在六个公共EHR数据集上，TarDiff在AUPRC和AUROC上分别比现有方法提升20.4%和18.4%。

Conclusion: TarDiff不仅保持时间保真度，还显著提升模型性能，为解决医疗数据分析中的数据稀缺和类别不平衡提供了有效方案。

Abstract: Synthetic Electronic Health Record (EHR) time-series generation is crucial
for advancing clinical machine learning models, as it helps address data
scarcity by providing more training data. However, most existing approaches
focus primarily on replicating statistical distributions and temporal
dependencies of real-world data. We argue that fidelity to observed data alone
does not guarantee better model performance, as common patterns may dominate,
limiting the representation of rare but important conditions. This highlights
the need for generate synthetic samples to improve performance of specific
clinical models to fulfill their target outcomes. To address this, we propose
TarDiff, a novel target-oriented diffusion framework that integrates
task-specific influence guidance into the synthetic data generation process.
Unlike conventional approaches that mimic training data distributions, TarDiff
optimizes synthetic samples by quantifying their expected contribution to
improving downstream model performance through influence functions.
Specifically, we measure the reduction in task-specific loss induced by
synthetic samples and embed this influence gradient into the reverse diffusion
process, thereby steering the generation towards utility-optimized data.
Evaluated on six publicly available EHR datasets, TarDiff achieves
state-of-the-art performance, outperforming existing methods by up to 20.4% in
AUPRC and 18.4% in AUROC. Our results demonstrate that TarDiff not only
preserves temporal fidelity but also enhances downstream model performance,
offering a robust solution to data scarcity and class imbalance in healthcare
analytics.

</details>


### [203] [Decentralized Time Series Classification with ROCKET Features](https://arxiv.org/abs/2504.17617)
*Bruno Casella,Matthias Jakobs,Marco Aldinucci,Sebastian Buschjäger*

Main category: cs.LG

TL;DR: 提出了DROCKS，一个完全去中心化的联邦学习框架，用于时间序列分类，避免了传统客户-服务器架构的脆弱性和隐私风险，并利用ROCKET特征提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和数据监管问题，联邦学习在时间序列分类任务中具有潜力，但传统客户-服务器架构存在单点故障和隐私泄露风险，因此需要一种更健壮的去中心化方法。

Method: DROCKS采用完全去中心化的架构，通过ROCKET特征提取和节点间的顺序路径传递，逐步优化全局模型并选择最佳局部核函数。

Result: 在UCR数据集上的实验显示，DROCKS性能优于现有客户-服务器联邦学习方法，同时对节点故障和恶意攻击更具弹性。

Conclusion: DROCKS提供了一种更安全、高效的时间序列分类联邦学习解决方案，解决了传统方法的局限性。

Abstract: Time series classification (TSC) is a critical task with applications in
various domains, including healthcare, finance, and industrial monitoring. Due
to privacy concerns and data regulations, Federated Learning has emerged as a
promising approach for learning from distributed time series data without
centralizing raw information. However, most FL solutions rely on a
client-server architecture, which introduces robustness and confidentiality
risks related to the distinguished role of the server, which is a single point
of failure and can observe knowledge extracted from clients. To address these
challenges, we propose DROCKS, a fully decentralized FL framework for TSC that
leverages ROCKET (RandOm Convolutional KErnel Transform) features. In DROCKS,
the global model is trained by sequentially traversing a structured path across
federation nodes, where each node refines the model and selects the most
effective local kernels before passing them to the successor. Extensive
experiments on the UCR archive demonstrate that DROCKS outperforms
state-of-the-art client-server FL approaches while being more resilient to node
failures and malicious attacks. Our code is available at
https://anonymous.4open.science/r/DROCKS-7FF3/README.md.

</details>


### [204] [The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks](https://arxiv.org/abs/2504.17618)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 研究者进一步研究了Hessian特征值谱密度（HESD）在神经网络（NN）中的表现，发现训练和微调主要产生正特征值HESD，而梯度操纵则导致负特征值HESD。他们提出了判断HESD类型和估计NN泛化能力的标准，并通过实验验证其方法论的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨Hessian特征值谱密度（HESD）对神经网络泛化能力的影响，并分析不同操作（如梯度操纵）如何改变HESD类型。

Method: 通过多种优化器、数据集及预处理方法的实验，分析HESD的行为，并提出了判断HESD类型和泛化能力的标准。

Result: 发现训练和微调主要产生正特征值HESD，而外部梯度操纵导致负特征值HESD。研究还揭示了HESD在训练中的变化，包括准奇异HESD的影响。

Conclusion: 提出了一种统一的HESD分析方法论，扩展了对Hessian特征值与NN损失地貌关系的理解，适用于判断泛化潜力。

Abstract: Hessians of neural network (NN) contain essential information about the
curvature of NN loss landscapes which can be used to estimate NN generalization
capabilities. We have previously proposed generalization criteria that rely on
the observation that Hessian eigenvalue spectral density (HESD) behaves
similarly for a wide class of NNs. This paper further studies their
applicability by investigating factors that can result in different types of
HESD. We conduct a wide range of experiments showing that HESD mainly has
positive eigenvalues (MP-HESD) for NN training and fine-tuning with various
optimizers on different datasets with different preprocessing and augmentation
procedures. We also show that mainly negative HESD (MN-HESD) is a consequence
of external gradient manipulation, indicating that the previously proposed
Hessian analysis methodology cannot be applied in such cases. We also propose
criteria and corresponding conditions to determine HESD type and estimate NN
generalization potential. These HESD types and previously proposed
generalization criteria are combined into a unified HESD analysis methodology.
Finally, we discuss how HESD changes during training, and show the occurrence
of quasi-singular (QS) HESD and its influence on the proposed methodology and
on the conventional assumptions about the relation between Hessian eigenvalues
and NN loss landscape curvature.

</details>


### [205] [PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph](https://arxiv.org/abs/2504.17641)
*Shengtao Zhang,Haokai Zhang,Shiqi Lou,Zicheng Wang,Zinan Zeng,Yilin Wang,Minnan Luo*

Main category: cs.LG

TL;DR: 论文提出了PTCL方法，解决仅使用最终标签的动态节点分类问题，结合伪标签生成和时间课程学习策略，并在新数据集CoOAG上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 动态节点分类中难以获取所有时间戳的标签，而最终标签更易获得，但现有方法无法直接利用最终标签进行分类。

Method: PTCL方法包括时间解耦架构（分离主干网络和解码器）和时间课程学习策略（为接近最终时间戳的伪标签分配更高权重）。

Result: 实验证明PTCL在多个真实场景中优于其他方法，并提出了统一框架FLiD。

Conclusion: PTCL解决了仅用最终标签的动态节点分类问题，提供了方法论和工具支持。

Abstract: Dynamic node classification is critical for modeling evolving systems like
financial transactions and academic collaborations. In such systems,
dynamically capturing node information changes is critical for dynamic node
classification, which usually requires all labels at every timestamp. However,
it is difficult to collect all dynamic labels in real-world scenarios due to
high annotation costs and label uncertainty (e.g., ambiguous or delayed labels
in fraud detection). In contrast, final timestamp labels are easier to obtain
as they rely on complete temporal patterns and are usually maintained as a
unique label for each user in many open platforms, without tracking the history
data. To bridge this gap, we propose PTCL(Pseudo-label Temporal Curriculum
Learning), a pioneering method addressing label-limited dynamic node
classification where only final labels are available. PTCL introduces: (1) a
temporal decoupling architecture separating the backbone (learning time-aware
representations) and decoder (strictly aligned with final labels), which
generate pseudo-labels, and (2) a Temporal Curriculum Learning strategy that
prioritizes pseudo-labels closer to the final timestamp by assigning them
higher weights using an exponentially decaying function. We contribute a new
academic dataset (CoOAG), capturing long-range research interest in dynamic
graph. Experiments across real-world scenarios demonstrate PTCL's consistent
superiority over other methods adapted to this task. Beyond methodology, we
propose a unified framework FLiD (Framework for Label-Limited Dynamic Node
Classification), consisting of a complete preparation workflow, training
pipeline, and evaluation standards, and supporting various models and datasets.
The code can be found at https://github.com/3205914485/FLiD.

</details>


### [206] [Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction](https://arxiv.org/abs/2504.17655)
*Farhad Pourkamali-Anaraki*

Main category: cs.LG

TL;DR: 本文通过对航空图像数据进行实证分析，研究了不同预训练模型（如MobileNet、DenseNet、ResNet）在有限标记数据下的表现，以及校准方法（如温度缩放）对预测结果的影响。研究发现，即使在小样本场景下，整合预测方法仍能提供有意义的标签集，而温度缩放并不总能缩小预测集大小。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，数据稀缺且环境多样，传统的标准基准测试难以适用。本文旨在通过整合预测技术，为复杂任务提供统计保证的标签集，以应对真实环境中的挑战。

Method: 使用MobileNet、DenseNet和ResNet等预训练模型在有限标记数据上进行微调，并比较带与不带温度缩放的两条校准流程，通过实证覆盖率和平均预测集大小评估性能。

Result: 研究表明，小样本下整合预测方法仍能提供有效的标签集；温度缩放对缩小预测集的效果不稳定；模型压缩技术在资源受限场景中具有潜力。

Conclusion: 整合预测在复杂任务中能提供有用的不确定性估计，但需谨慎选择校准方法。未来研究应关注噪声标签的影响及模型简化策略。

Abstract: This paper presents a comprehensive empirical analysis of conformal
prediction methods on a challenging aerial image dataset featuring diverse
events in unconstrained environments. Conformal prediction is a powerful
post-hoc technique that takes the output of any classifier and transforms it
into a set of likely labels, providing a statistical guarantee on the coverage
of the true label. Unlike evaluations on standard benchmarks, our study
addresses the complexities of data-scarce and highly variable real-world
settings. We investigate the effectiveness of leveraging pretrained models
(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to
generate informative prediction sets. To further evaluate the impact of
calibration, we consider two parallel pipelines (with and without temperature
scaling) and assess performance using two key metrics: empirical coverage and
average prediction set size. This setup allows us to systematically examine how
calibration choices influence the trade-off between reliability and efficiency.
Our findings demonstrate that even with relatively small labeled samples and
simple nonconformity scores, conformal prediction can yield valuable
uncertainty estimates for complex tasks. Moreover, our analysis reveals that
while temperature scaling is often employed for calibration, it does not
consistently lead to smaller prediction sets, underscoring the importance of
careful consideration in its application. Furthermore, our results highlight
the significant potential of model compression techniques within the conformal
prediction pipeline for deployment in resource-constrained environments. Based
on our observations, we advocate for future research to delve into the impact
of noisy or ambiguous labels on conformal prediction performance and to explore
effective model reduction strategies.

</details>


### [207] [Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models](https://arxiv.org/abs/2504.17660)
*Julius Vetter,Manuel Gloeckler,Daniel Gedon,Jakob H. Macke*

Main category: cs.LG

TL;DR: 论文提出了NPE-PF方法，通过利用预训练的表格数据基础模型（如TabPFN），在仿真推断（SBI）中实现高效的后验估计，显著减少了仿真次数，并在准确性和鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 仿真推断（SBI）通常需要大量仿真数据，尤其是对昂贵的仿真器来说，成本较高。本文旨在利用预训练的表格数据基础模型，减少仿真需求，提高效率和准确性。

Method: 提出NPE-PF方法，通过预训练的TabPFN模型作为自回归条件密度估计器，直接用于SBI的后验估计，无需额外训练或超参数调优。

Result: NPE-PF在基准测试和复杂科学逆问题上表现优异，仿真效率显著提高，有时甚至需要数量级更少的仿真次数，同时展现出更强的鲁棒性和扩展性。

Conclusion: NPE-PF为SBI提供了一种无需训练、通用的解决方案，显著提升了仿真效率和易用性，适用于广泛的随机逆问题。

Abstract: Simulation-based inference (SBI) offers a flexible and general approach to
performing Bayesian inference: In SBI, a neural network is trained on synthetic
data simulated from a model and used to rapidly infer posterior distributions
for observed data. A key goal for SBI is to achieve accurate inference with as
few simulations as possible, especially for expensive simulators. In this work,
we address this challenge by repurposing recent probabilistic foundation models
for tabular data: We show how tabular foundation models -- specifically TabPFN
-- can be used as pre-trained autoregressive conditional density estimators for
SBI. We propose Neural Posterior Estimation with Prior-data Fitted Networks
(NPE-PF) and show that it is competitive with current SBI approaches in terms
of accuracy for both benchmark tasks and two complex scientific inverse
problems. Crucially, it often substantially outperforms them in terms of
simulation efficiency, sometimes requiring orders of magnitude fewer
simulations. NPE-PF eliminates the need for inference network selection,
training, and hyperparameter tuning. We also show that it exhibits superior
robustness to model misspecification and can be scaled to simulation budgets
that exceed the context size limit of TabPFN. NPE-PF provides a new direction
for SBI, where training-free, general-purpose inference models offer efficient,
easy-to-use, and flexible solutions for a wide range of stochastic inverse
problems.

</details>


### [208] [On Multivariate Financial Time Series Classification](https://arxiv.org/abs/2504.17664)
*Grégory Bournassenko*

Main category: cs.LG

TL;DR: 论文研究了机器学习和深度学习模型在金融市场的多变量时间序列分析中的应用，比较了小数据和大数据方法的挑战与优势，并对比了传统方法（如SVM）与现代架构（如ConvTimeNet）。结果强调了在金融时间序列分析与预测中深入理解和使用大数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索不同规模数据（小数据与大数据）在金融市场时间序列分析中的表现差异，以及现代深度学习模型相较于传统方法的优势。

Method: 研究方法包括对比小数据与大数据方法，以及传统模型（如SVM）和现代架构（如ConvTimeNet）在多变量时间序列分析中的表现。

Result: 研究结果表明，大数据在金融时间序列分析与预测中具有显著优势，且现代深度学习模型（如ConvTimeNet）表现优于传统方法。

Conclusion: 结论指出，深入理解和使用大数据对金融时间序列分析至关重要，现代深度学习架构在这一领域具有重要潜力。

Abstract: This article investigates the use of Machine Learning and Deep Learning
models in multivariate time series analysis within financial markets. It
compares small and big data approaches, focusing on their distinct challenges
and the benefits of scaling. Traditional methods such as SVMs are contrasted
with modern architectures like ConvTimeNet. The results show the importance of
using and understanding Big Data in depth in the analysis and prediction of
financial time series.

</details>


### [209] [Federated Learning: A Survey on Privacy-Preserving Collaborative Intelligence](https://arxiv.org/abs/2504.17703)
*Edward Collins,Michel Wang*

Main category: cs.LG

TL;DR: 本文综述了联邦学习的核心架构、技术挑战及实际应用，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过分布式训练解决数据隐私和合规问题，适用于医疗、金融和物联网等领域。本文旨在提供关于联邦学习的全面概述和分析。

Method: 文章介绍了联邦学习的生命周期，包括本地训练、模型聚合和全局更新，并讨论了非独立同分布数据、系统异构性、通信开销等技术挑战。

Result: 综述强调了隐私保护机制、新兴研究趋势（如个性化联邦学习和量子计算整合）以及实际应用的案例和评估指标。

Conclusion: 联邦学习的未来研究方向包括提高系统的可扩展性、高效性和可信度，解决开放性问题以推动其广泛应用。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in the field
of distributed machine learning, enabling multiple clients such as mobile
devices, edge nodes, or organizations to collaboratively train a shared global
model without the need to centralize sensitive data. This decentralized
approach addresses growing concerns around data privacy, security, and
regulatory compliance, making it particularly attractive in domains such as
healthcare, finance, and smart IoT systems. This survey provides a concise yet
comprehensive overview of Federated Learning, beginning with its core
architecture and communication protocol. We discuss the standard FL lifecycle,
including local training, model aggregation, and global updates. A particular
emphasis is placed on key technical challenges such as handling non-IID
(non-independent and identically distributed) data, mitigating system and
hardware heterogeneity, reducing communication overhead, and ensuring privacy
through mechanisms like differential privacy and secure aggregation.
Furthermore, we examine emerging trends in FL research, including personalized
FL, cross-device versus cross-silo settings, and integration with other
paradigms such as reinforcement learning and quantum computing. We also
highlight real-world applications and summarize benchmark datasets and
evaluation metrics commonly used in FL research. Finally, we outline open
research problems and future directions to guide the development of scalable,
efficient, and trustworthy FL systems.

</details>


### [210] [Fault Diagnosis in New Wind Turbines using Knowledge from Existing Turbines by Generative Domain Adaptation](https://arxiv.org/abs/2504.17709)
*Stefan Jonas,Angela Meyer*

Main category: cs.LG

TL;DR: 论文提出了一种基于CycleGAN的生成式深度学习方法，用于解决风电数据不足时正常行为模型训练不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 智能风电监控需要大量数据训练可靠模型，但新安装的风机往往缺乏足够数据，影响故障诊断效果。为了解决这一问题，作者提出了跨风机数据域映射的方法。

Method: 利用CycleGAN将缺乏训练数据的风机SCADA数据映射到已有充足数据的风机数据域，使得训练好的NBM可直接应用于数据匮乏的风机。

Result: 实验表明，该方法在训练数据稀缺时显著提升了故障诊断效果，F1分数比传统方法提高10.3%（1个月数据）和16.8%（2周数据）。

Conclusion: 该方法为数据稀缺场景下的异常检测提供了新思路，能实现更早、更可靠的故障诊断。

Abstract: Intelligent condition monitoring of wind turbines is essential for reducing
downtimes. Machine learning models trained on wind turbine operation data are
commonly used to detect anomalies and, eventually, operation faults. However,
data-driven normal behavior models (NBMs) require a substantial amount of
training data, as NBMs trained with scarce data may result in unreliable fault
diagnosis. To overcome this limitation, we present a novel generative deep
learning approach to make SCADA samples from one wind turbine lacking training
data resemble SCADA data from wind turbines with representative training data.
Through CycleGAN-based domain mapping, our method enables the application of an
NBM trained on an existing wind turbine to one with severely limited data. We
demonstrate our approach on field data mapping SCADA samples across 7
substantially different WTs. Our findings show significantly improved fault
diagnosis in wind turbines with scarce data. Our method achieves the most
similar anomaly scores to an NBM trained with abundant data, outperforming NBMs
trained on scarce training data with improvements of +10.3% in F1-score when 1
month of training data is available and +16.8% when 2 weeks are available. The
domain mapping approach outperforms conventional fine-tuning at all considered
degrees of data scarcity, ranging from 1 to 8 weeks of training data. The
proposed technique enables earlier and more reliable fault diagnosis in newly
installed wind farms, demonstrating a novel and promising research direction to
improve anomaly detection when faced with training data scarcity.

</details>


### [211] [Early Detection of Multidrug Resistance Using Multivariate Time Series Analysis and Interpretable Patient-Similarity Representations](https://arxiv.org/abs/2504.17717)
*Óscar Escudero-Arnanz,Antonio G. Marques,Inmaculada Mora-Jiménez,Joaquín Álvarez-Rodríguez,Cristina Soguero-Ruiz*

Main category: cs.LG

TL;DR: 提出了一种可解释的机器学习框架，通过患者多维时间序列相似性分析预测多药耐药性，效果优于基线模型，并提供了临床可解释的见解。


<details>
  <summary>Details</summary>
Motivation: 多药耐药性是全球卫生危机，增加医疗成本和死亡率，现有模型缺乏可解释性。本研究旨在开发一种既准确又可解释的预测框架。

Method: 患者数据建模为多维时间序列（MTS），采用DTW、Time Cluster Kernel等方法量化相似性，结合逻辑回归、随机森林、SVM进行分类，并通过相似性网络和聚类分析增强可解释性。

Result: 在ICU电子健康记录上验证，AUC达81%，优于基线模型，识别出关键风险因素（如长期抗生素使用、侵入性操作），并生成临床相关患者聚类。

Conclusion: 结合患者相似性和图分析的方法实现了高精度且可解释的预测，支持早期检测和患者分层，展示了可解释ML在重症监护中的潜力。

Abstract: Background and Objectives: Multidrug Resistance (MDR) is a critical global
health issue, causing increased hospital stays, healthcare costs, and
mortality. This study proposes an interpretable Machine Learning (ML) framework
for MDR prediction, aiming for both accurate inference and enhanced
explainability.
  Methods: Patients are modeled as Multivariate Time Series (MTS), capturing
clinical progression and patient-to-patient interactions. Similarity among
patients is quantified using MTS-based methods: descriptive statistics, Dynamic
Time Warping, and Time Cluster Kernel. These similarity measures serve as
inputs for MDR classification via Logistic Regression, Random Forest, and
Support Vector Machines, with dimensionality reduction and kernel
transformations improving model performance. For explainability, patient
similarity networks are constructed from these metrics. Spectral clustering and
t-SNE are applied to identify MDR-related subgroups and visualize high-risk
clusters, enabling insight into clinically relevant patterns.
  Results: The framework was validated on ICU Electronic Health Records from
the University Hospital of Fuenlabrada, achieving an AUC of 81%. It outperforms
baseline ML and deep learning models by leveraging graph-based patient
similarity. The approach identifies key risk factors -- prolonged antibiotic
use, invasive procedures, co-infections, and extended ICU stays -- and reveals
clinically meaningful clusters. Code and results are available at
\https://github.com/oscarescuderoarnanz/DM4MTS.
  Conclusions: Patient similarity representations combined with graph-based
analysis provide accurate MDR prediction and interpretable insights. This
method supports early detection, risk factor identification, and patient
stratification, highlighting the potential of explainable ML in critical care.

</details>


### [212] [Conformal Segmentation in Industrial Surface Defect Detection with Statistical Guarantees](https://arxiv.org/abs/2504.17721)
*Cheng Shen,Yuewei Liu*

Main category: cs.LG

TL;DR: 论文提出了一种通过校准数据和统计阈值改进钢铁表面缺陷检测的方法，以解决传统方法效率低和自动检测不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 传统钢铁表面缺陷检测方法效率低且成本高，基于CNN的自动检测方法因数据标注不确定性和过拟合问题可靠性不足。

Method: 使用满足i.i.d条件的校准数据评估模型性能，定义损失函数量化错误率，并基于用户定义风险水平计算统计阈值构建预测集。

Result: 该方法确保了测试集的期望错误率严格受限于预定义风险水平，并验证了模型不确定性的统计严谨性。

Conclusion: 提出的方法在多种校准-测试比例下均能有效控制错误率，具备适应性和操作有效性。

Abstract: In industrial settings, surface defects on steel can significantly compromise
its service life and elevate potential safety risks. Traditional defect
detection methods predominantly rely on manual inspection, which suffers from
low efficiency and high costs. Although automated defect detection approaches
based on Convolutional Neural Networks(e.g., Mask R-CNN) have advanced rapidly,
their reliability remains challenged due to data annotation uncertainties
during deep model training and overfitting issues. These limitations may lead
to detection deviations when processing the given new test samples, rendering
automated detection processes unreliable. To address this challenge, we first
evaluate the detection model's practical performance through calibration data
that satisfies the independent and identically distributed (i.i.d) condition
with test data. Specifically, we define a loss function for each calibration
sample to quantify detection error rates, such as the complement of recall rate
and false discovery rate. Subsequently, we derive a statistically rigorous
threshold based on a user-defined risk level to identify high-probability
defective pixels in test images, thereby constructing prediction sets (e.g.,
defect regions). This methodology ensures that the expected error rate (mean
error rate) on the test set remains strictly bounced by the predefined risk
level. Additionally, we observe a negative correlation between the average
prediction set size and the risk level on the test set, establishing a
statistically rigorous metric for assessing detection model uncertainty.
Furthermore, our study demonstrates robust and efficient control over the
expected test set error rate across varying calibration-to-test partitioning
ratios, validating the method's adaptability and operational effectiveness.

</details>


### [213] [Towards Robust LLMs: an Adversarial Robustness Measurement Framework](https://arxiv.org/abs/2504.17723)
*Natan Levy,Adiel Ashrov,Guy Katz*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型(LLMs)在对抗性扰动下的鲁棒性，提出了RoMA框架来量化评估模型的稳健性，结果显示不同模型和任务的鲁棒性存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在人工智能领域取得了巨大成功，但其对抗性扰动的脆弱性影响了在高风险应用中的可靠性。现有研究主要关注视觉神经网络的对抗鲁棒性，而语言模型的鲁棒性研究较少。

Method: 作者将Robustness Measurement and Assessment (RoMA)框架应用于评估LLM的鲁棒性，无需访问模型参数，并通过与形式验证方法比较验证其准确性。

Result: 实验结果显示，不同模型、同一任务内的不同类别以及各类扰动之间，鲁棒性存在显著差异。RoMA框架在保持计算效率的同时，误差极小。

Conclusion: 研究表明任务特定的鲁棒性评估是必要的，RoMA框架为开发更可靠的语言模型提供了系统方法。

Abstract: The rise of Large Language Models (LLMs) has revolutionized artificial
intelligence, yet these models remain vulnerable to adversarial perturbations,
undermining their reliability in high-stakes applications. While adversarial
robustness in vision-based neural networks has been extensively studied, LLM
robustness remains under-explored. We adapt the Robustness Measurement and
Assessment (RoMA) framework to quantify LLM resilience against adversarial
inputs without requiring access to model parameters. By comparing RoMA's
estimates to those of formal verification methods, we demonstrate its accuracy
with minimal error margins while maintaining computational efficiency. Our
empirical evaluation reveals that robustness varies significantly not only
between different models but also across categories within the same task and
between various types of perturbations. This non-uniformity underscores the
need for task-specific robustness evaluations, enabling practitioners to
compare and select models based on application-specific robustness
requirements. Our work provides a systematic methodology to assess LLM
robustness, advancing the development of more reliable language models for
real-world deployment.

</details>


### [214] [Interpretable Early Detection of Parkinson's Disease through Speech Analysis](https://arxiv.org/abs/2504.17739)
*Lorenzo Simone,Mauro Giuseppe Camporeale,Vito Marco Rubino,Vincenzo Gervasi,Giovanni Dimauro*

Main category: cs.LG

TL;DR: 利用深度学习从语音记录中早期检测帕金森病，并通过解释性分析揭示关键语音特征。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期症状中语音障碍较为突出，为诊断提供了机会。机器学习技术的发展为及时检测提供了可能。

Method: 提出了一种深度学习方法，不仅能够从语音记录中检测疾病，还能通过解释性技术突出影响预测的语音片段。

Result: 方法在包含65名参与者的831条录音数据集上表现优异，分类性能与前沿方法相当，同时增强了可解释性。

Conclusion: 该研究为帕金森病的早期诊断提供了有效工具，并通过解释性分析有助于理解潜在的神经肌肉损伤。

Abstract: Parkinson's disease is a progressive neurodegenerative disorder affecting
motor and non-motor functions, with speech impairments among its earliest
symptoms. Speech impairments offer a valuable diagnostic opportunity, with
machine learning advances providing promising tools for timely detection. In
this research, we propose a deep learning approach for early Parkinson's
disease detection from speech recordings, which also highlights the vocal
segments driving predictions to enhance interpretability. This approach seeks
to associate predictive speech patterns with articulatory features, providing a
basis for interpreting underlying neuromuscular impairments. We evaluated our
approach using the Italian Parkinson's Voice and Speech Database, containing
831 audio recordings from 65 participants, including both healthy individuals
and patients. Our approach showed competitive classification performance
compared to state-of-the-art methods, while providing enhanced interpretability
by identifying key speech features influencing predictions.

</details>


### [215] [Embedding Empirical Distributions for Computing Optimal Transport Maps](https://arxiv.org/abs/2504.17740)
*Mingchen Jiang,Peng Xu,Xichen Ye,Xiaohui Chen,Yun Yang,Yifan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构和超网络的新方法，用于学习多概率分布间的神经最优输运映射。


<details>
  <summary>Details</summary>
Motivation: 现代信号处理中分布数据日益重要，但现有神经OT方法多局限于两分布间的单一映射，无法满足多分布映射需求。

Method: 利用Transformer生成可变长度分布数据的嵌入表示，并通过超网络生成神经最优输运映射。

Result: 通过数值实验验证了嵌入表示及生成的OT映射的有效性，模型代码已开源。

Conclusion: 该方法成功解决了多分布间的神经OT映射问题，实验验证了其可行性和有效性。

Abstract: Distributional data have become increasingly prominent in modern signal
processing, highlighting the necessity of computing optimal transport (OT) maps
across multiple probability distributions. Nevertheless, recent studies on
neural OT methods predominantly focused on the efficient computation of a
single map between two distributions. To address this challenge, we introduce a
novel approach to learning transport maps for new empirical distributions.
Specifically, we employ the transformer architecture to produce embeddings from
distributional data of varying length; these embeddings are then fed into a
hypernetwork to generate neural OT maps. Various numerical experiments were
conducted to validate the embeddings and the generated OT maps. The model
implementation and the code are provided on
https://github.com/jiangmingchen/HOTET.

</details>


### [216] [MSGCN: Multiplex Spatial Graph Convolution Network for Interlayer Link Weight Prediction](https://arxiv.org/abs/2504.17749)
*Steven E. Wilson,Sina Khanmohammadi*

Main category: cs.LG

TL;DR: 本文提出了一种名为MSGCN的新方法，用于多层网络中的链路权重预测。该方法通过多图层空间嵌入信息，克服了现有技术在复杂性上的局限，并在实验中展示了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络在链路权重预测任务上关注不足，尤其是在多层网络中的复杂场景下，传统方法难以应对。本文旨在填补这一研究空白。

Method: 作者提出了Multiplex Spatial Graph Convolution Network (MSGCN)，通过将空间图卷积推广到多层网络，并捕捉节点在多图层中的几何结构，来预测层间链路权重。

Result: 实验表明，MSGCN在多种多层网络结构上均表现出稳健、准确且可泛化的链路权重预测性能。

Conclusion: MSGCN为多层网络中的链路权重预测提供了一种有效解决方案，具有广泛的应用潜力。

Abstract: Graph Neural Networks (GNNs) have been widely used for various learning
tasks, ranging from node classification to link prediction. They have
demonstrated excellent performance in multiple domains involving
graph-structured data. However, an important category of learning tasks, namely
link weight prediction, has received less emphasis due to its increased
complexity compared to binary link classification. Link weight prediction
becomes even more challenging when considering multilayer networks, where nodes
can be interconnected across multiple layers. To address these challenges, we
propose a new method named Multiplex Spatial Graph Convolution Network (MSGCN),
which spatially embeds information across multiple layers to predict interlayer
link weights. The MSGCN model generalizes spatial graph convolution to
multiplex networks and captures the geometric structure of nodes across
multiple layers. Extensive experiments using data with known interlayer link
information show that the MSGCN model has robust, accurate, and generalizable
link weight prediction performance across a wide variety of multiplex network
structures.

</details>


### [217] [Disaggregated Deep Learning via In-Physics Computing at Radio Frequency](https://arxiv.org/abs/2504.17752)
*Zhihui Gao,Sri Krishna Vadlamani,Kfir Sulimany,Dirk Englund,Tingjun Chen*

Main category: cs.LG

TL;DR: WISE是一种新型无线边缘网络计算架构，通过无线广播和射频物理计算实现高效深度学习推理。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备因资源限制导致传统数字计算架构下深度学习模型部署的高内存和高计算需求问题。

Method: 采用无线广播模型权重和在射频直接进行复数矩阵向量乘法计算的创新方法。

Result: 实验展示了95.7%的图像分类精度和超低能耗（6.0 fJ/MAC），计算效率达165.8 TOPS/W。

Conclusion: WISE在无线连接的边缘设备上实现了比传统数字计算高出两个数量级的能效提升。

Abstract: Modern edge devices, such as cameras, drones, and Internet-of-Things nodes,
rely on deep learning to enable a wide range of intelligent applications,
including object recognition, environment perception, and autonomous
navigation. However, deploying deep learning models directly on the often
resource-constrained edge devices demands significant memory footprints and
computational power for real-time inference using traditional digital computing
architectures. In this paper, we present WISE, a novel computing architecture
for wireless edge networks designed to overcome energy constraints in deep
learning inference. WISE achieves this goal through two key innovations:
disaggregated model access via wireless broadcasting and in-physics computation
of general complex-valued matrix-vector multiplications directly at radio
frequency. Using a software-defined radio platform with wirelessly broadcast
model weights over the air, we demonstrate that WISE achieves 95.7% image
classification accuracy with ultra-low operation power of 6.0 fJ/MAC per
client, corresponding to a computation efficiency of 165.8 TOPS/W. This
approach enables energy-efficient deep learning inference on wirelessly
connected edge devices, achieving more than two orders of magnitude improvement
in efficiency compared to traditional digital computing.

</details>


### [218] [Replay to Remember: Retaining Domain Knowledge in Streaming Language Models](https://arxiv.org/abs/2504.17780)
*Sneh Pillai*

Main category: cs.LG

TL;DR: 论文提出了一种结合LoRA和最小回放机制的轻量级方法，用于大语言模型在严格计算和数据流约束下的实时域适应，实验表明即使最小回放也能显著稳定并部分恢复领域知识。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型持续学习中的灾难性遗忘问题，尤其在计算资源和数据流受限的现实场景下。

Method: 结合LoRA和最小回放机制，在医学问答、遗传学和法律三个领域进行实时流式实验。

Result: 实验表明，即使最小回放也能显著稳定模型并部分恢复领域知识，减轻灾难性遗忘的影响。

Conclusion: 该方法为资源受限的实际场景中部署适应性强的语言模型提供了实用见解。

Abstract: Continual learning in large language models (LLMs) typically encounters the
critical challenge of catastrophic forgetting, where previously acquired
knowledge deteriorates upon exposure to new data. While techniques like replay
buffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have
been proposed, few studies investigate real-time domain adaptation under strict
computational and data-stream constraints. In this paper, we demonstrate a
lightweight method combining LoRA and a minimal replay mechanism in a realistic
streaming setting across three diverse knowledge domains: medical question
answering, genetics, and law. Using perplexity, semantic similarity, and
GPT-based human-like evaluation metrics, we quantify the model's adaptation,
forgetting, and recovery over time. Our experiments reveal that while
catastrophic forgetting naturally occurs, even minimal replay significantly
stabilizes and partially restores domain-specific knowledge. This study
contributes practical insights for deploying adaptable LLMs in
resource-constrained, real-world scenarios.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [219] [A Framework for the Assurance of AI-Enabled Systems](https://arxiv.org/abs/2504.16937)
*Ariel S. Kapusta,David Jin,Peter M. Teague,Robert A. Houston,Jonathan B. Elliott,Grace Y. Park,Shelby S. Holdren*

Main category: cs.AI

TL;DR: 本文提出了一种基于声明的风险管理和AI系统保障框架，以解决快速部署与严格评估之间的冲突，确保AI能力在国防应用中的可信赖性。


<details>
  <summary>Details</summary>
Motivation: 美国国防部希望通过快速开发和部署AI能力保持战略优势，但AI算法的强大特性带来技术、安全和伦理挑战，可能阻碍其应用。本文旨在解决这些挑战，确保AI系统的可信赖性。

Method: 提出一个基于声明的框架，用于AI系统的风险管理和保障，支持不同采购路径的项目，确保AI系统在生命周期内实现任务目标且不引入不可接受的风险。

Result: 贡献包括AI保障的框架过程、相关定义以促进AI保障话题的讨论，以及AI保障的重要考虑因素。

Conclusion: 该框架为国防部提供了在快速部署有效AI能力的同时不忽视关键风险或削弱利益相关者信任的机制。

Abstract: The United States Department of Defense (DOD) looks to accelerate the
development and deployment of AI capabilities across a wide spectrum of defense
applications to maintain strategic advantages. However, many common features of
AI algorithms that make them powerful, such as capacity for learning,
large-scale data ingestion, and problem-solving, raise new technical, security,
and ethical challenges. These challenges may hinder adoption due to uncertainty
in development, testing, assurance, processes, and requirements.
Trustworthiness through assurance is essential to achieve the expected value
from AI.
  This paper proposes a claims-based framework for risk management and
assurance of AI systems that addresses the competing needs for faster
deployment, successful adoption, and rigorous evaluation. This framework
supports programs across all acquisition pathways provide grounds for
sufficient confidence that an AI-enabled system (AIES) meets its intended
mission goals without introducing unacceptable risks throughout its lifecycle.
The paper's contributions are a framework process for AI assurance, a set of
relevant definitions to enable constructive conversations on the topic of AI
assurance, and a discussion of important considerations in AI assurance. The
framework aims to provide the DOD a robust yet efficient mechanism for swiftly
fielding effective AI capabilities without overlooking critical risks or
undermining stakeholder trust.

</details>


### [220] [Rational Inference in Formal Concept Analysis](https://arxiv.org/abs/2504.16938)
*Lucas Carr,Nicholas Leisegang,Thomas Meyer,Sergei Obiedkov*

Main category: cs.AI

TL;DR: 本文通过KLM框架在FCA中构建了可废止推理，保持了非单调推理的原则，并提供了比命题情况更上下文相关的结论。


<details>
  <summary>Details</summary>
Motivation: 传统的FCA中的蕴含关系无法处理错误数据或异常数据，而非单调推理在FCA中的应用尚未深入研究。本文旨在解决这一不足。

Method: 作者扩展了KLM框架，构建了FCA中的可废止推理语义，基于可能世界的偏好排序，并保持了非单调推理的特性。

Result: 该方法在FCA中实现了与KLM框架一致的非单调推理，同时提供了更上下文化的推理能力，比命题情况更具相关性。

Conclusion: 本文成功将KLM框架引入FCA，不仅保持了非单调推理的核心原则，还提供了更强的上下文适应性和推理相关性。

Abstract: Defeasible conditionals are a form of non-monotonic inference which enable
the expression of statements like "if $\phi$ then normally $\psi$". The KLM
framework defines a semantics for the propositional case of defeasible
conditionals by construction of a preference ordering over possible worlds. The
pattern of reasoning induced by these semantics is characterised by consequence
relations satisfying certain desirable properties of non-monotonic reasoning.
In FCA, implications are used to describe dependencies between attributes.
However, these implications are unsuitable to reason with erroneous data or
data prone to exceptions. Until recently, the topic of non-monotonic inference
in FCA has remained largely uninvestigated. In this paper, we provide a
construction of the KLM framework for defeasible reasoning in FCA and show that
this construction remains faithful to the principle of non-monotonic inference
described in the original framework. We present an additional argument that,
while remaining consistent with the original ideas around non-monotonic
reasoning, the defeasible reasoning we propose in FCA offers a more contextual
view on inference, providing the ability for more relevant conclusions to be
drawn when compared to the propositional case.

</details>


### [221] [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
*Emre Can Acikgoz,Cheng Qian,Hongru Wang,Vardhan Dongre,Xiusi Chen,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 这篇综述论文探讨了当前基于大语言模型的对话代理的能力和局限，并提出下一代对话代理的需求和目标。


<details>
  <summary>Details</summary>
Motivation: 旨在分析当前对话代理的不足，并为未来更接近人类智能水平的可扩展系统指明方向。

Method: 通过将对话代理的能力组织为推理、监控和控制三个维度，并基于此构建新的分类法来系统分析。

Result: 识别了当前研究的空白，并提出了未来的关键研究方向，如真实评估、长期多轮推理能力等。

Conclusion: 文章为对话代理领域提供了结构化基础，强调了现有局限，并展望了未来研究，推动了通用人工智能的发展。

Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational
AI from traditional dialogue systems into sophisticated agents capable of
autonomous actions, contextual awareness, and multi-turn interactions with
users. Yet, fundamental questions about their capabilities, limitations, and
paths forward remain open. This survey paper presents a desideratum for
next-generation Conversational Agents - what has been achieved, what challenges
persist, and what must be done for more scalable systems that approach
human-level intelligence. To that end, we systematically analyze LLM-driven
Conversational Agents by organizing their capabilities into three primary
dimensions: (i) Reasoning - logical, systematic thinking inspired by human
intelligence for decision making, (ii) Monitor - encompassing self-awareness
and user interaction monitoring, and (iii) Control - focusing on tool
utilization and policy following. Building upon this, we introduce a novel
taxonomy by classifying recent work on Conversational Agents around our
proposed desideratum. We identify critical research gaps and outline key
directions, including realistic evaluations, long-term multi-turn reasoning
skills, self-evolution capabilities, collaborative and multi-agent task
completion, personalization, and proactivity. This work aims to provide a
structured foundation, highlight existing limitations, and offer insights into
potential future research directions for Conversational Agents, ultimately
advancing progress toward Artificial General Intelligence (AGI). We maintain a
curated repository of papers at:
https://github.com/emrecanacikgoz/awesome-conversational-agents.

</details>


### [222] [A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs](https://arxiv.org/abs/2504.17006)
*Jalal Arabneydi,Saiful Islam,Srijita Das,Sai Krishna Gottipati,William Duguay,Cloderic Mars,Matthew E. Taylor,Matthew Guzdial,Antoine Fagette,Younes Zerouali*

Main category: cs.AI

TL;DR: 提出了一种新颖的多层分层人机协作深度强化学习算法，结合了自学习、模仿学习和迁移学习。在无人机场景中验证该方法能加速训练并提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习的发展，人机协作方法为决策问题提供了新的机遇，但如何系统整合人类输入仍具挑战。

Method: 采用了三层学习（自学习、模仿学习、转移学习）和三种人类输入（奖励、动作、演示），并基于开源工具Cogment实现。

Result: 实验证明人机协作能加速训练、提高性能，且建议的作用是引导梯度方法并降低方差。建议量也需适中以避免过拟合或欠拟合。

Conclusion: 人机协作在多场景下均表现出色，尤其是在复杂任务中具有显著优势。

Abstract: With the growing popularity of deep reinforcement learning (DRL),
human-in-the-loop (HITL) approach has the potential to revolutionize the way we
approach decision-making problems and create new opportunities for human-AI
collaboration. In this article, we introduce a novel multi-layered hierarchical
HITL DRL algorithm that comprises three types of learning: self learning,
imitation learning and transfer learning. In addition, we consider three forms
of human inputs: reward, action and demonstration. Furthermore, we discuss main
challenges, trade-offs and advantages of HITL in solving complex problems and
how human information can be integrated in the AI solution systematically. To
verify our technical results, we present a real-world unmanned aerial vehicles
(UAV) problem wherein a number of enemy drones attack a restricted area. The
objective is to design a scalable HITL DRL algorithm for ally drones to
neutralize the enemy drones before they reach the area. To this end, we first
implement our solution using an award-winning open-source HITL software called
Cogment. We then demonstrate several interesting results such as (a) HITL leads
to faster training and higher performance, (b) advice acts as a guiding
direction for gradient methods and lowers variance, and (c) the amount of
advice should neither be too large nor too small to avoid over-training and
under-training. Finally, we illustrate the role of human-AI cooperation in
solving two real-world complex scenarios, i.e., overloaded and decoy attacks.

</details>


### [223] [Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification](https://arxiv.org/abs/2504.17017)
*Balaji Rao,William Eiers,Carlo Lipizzi*

Main category: cs.AI

TL;DR: 该论文提出了一个框架，用于生成形式化语言的完整证明，结合内置策略和现成的自动定理证明器。框架包括自然语言陈述生成、LLM生成形式化证明及启发式构建最终证明的模块。通过两阶段微调训练LLM，并在miniF2F-test基准和Isabelle证明助手中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的兴起，形式化验证软件代码属性成为重要任务。尽管代码专用模型在生成Lean4和Isabelle代码方面取得成功，但广义定理证明仍未完全解决，成为衡量LLM推理能力的基准。

Method: 框架包含三个组件：生成代码的自然语言陈述、LLM生成形式化证明、启发式构建最终证明的模块。采用两阶段微调训练LLM：SFT-based训练确保语法正确的Isabelle代码生成，RL-based训练鼓励生成能被定理证明器验证的证明。

Result: 在miniF2F-test基准和Isabelle证明助手中验证了框架的有效性，并设计了验证AWS S3桶访问策略代码正确性的用例。此外，基于FVELER数据集整理了一个数据集供未来训练使用。

Conclusion: 该框架为形式化验证和LLM的机械可解释性探索提供了新途径，展示了在定理证明和代码验证中的潜力。

Abstract: Formally verifying properties of software code has been a highly desirable
task, especially with the emergence of LLM-generated code. In the same vein,
they provide an interesting avenue for the exploration of formal verification
and mechanistic interpretability. Since the introduction of code-specific
models, despite their successes in generating code in Lean4 and Isabelle, the
task of generalized theorem proving still remains far from being fully solved
and will be a benchmark for reasoning capability in LLMs. In this work, we
introduce a framework that generates whole proofs in a formal language to be
used within systems that utilize the power of built-in tactics and
off-the-shelf automated theorem provers. Our framework includes 3 components:
generating natural language statements of the code to be verified, an LLM that
generates formal proofs for the given statement, and a module employing
heuristics for building the final proof. To train the LLM, we employ a 2-stage
fine-tuning process, where we first use SFT-based training to enable the model
to generate syntactically correct Isabelle code and then RL-based training that
encourages the model to generate proofs verified by a theorem prover. We
validate our framework using the miniF2F-test benchmark and the Isabelle proof
assistant and design a use case to verify the correctness of the AWS S3 bucket
access policy code. We also curate a dataset based on the
FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.

</details>


### [224] [Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments](https://arxiv.org/abs/2504.17087)
*Yuran Li,Jama Hussein Mohamud,Chongren Sun,Di Wu,Benoit Boulet*

Main category: cs.AI

TL;DR: 论文提出了一个三阶段的元评判选择管道，通过多智能体协作和综合评分标准，改善了大型语言模型（LLM）在复杂任务中的评判表现，相比单智能体基准有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在复杂任务中应用增多，评判其表现的效率与准确性成为挑战。现有研究多关注LLM与人类偏好的对齐，但忽视了人类评判的偏差与错误，且多响应场景下的LLM评判选择研究不足。

Method: 提出三阶段管道：1) 与GPT-4和专家制定评分标准；2) 用三个高级LLM智能体评分；3) 设阈值过滤低分评判。引入多智能体协作与综合标准。

Result: 在JudgeBench数据集上，相比原始评判和单智能体基线，分别提升15.55%和8.37%。

Conclusion: LLM作为元评判潜力显著，为未来构建LLM评判的强化学习偏好数据集奠定了基础。

Abstract: Large language models (LLMs) are being widely applied across various fields,
but as tasks become more complex, evaluating their responses is increasingly
challenging. Compared to human evaluators, the use of LLMs to support
performance evaluation offers a more efficient alternative. However, most
studies focus mainly on aligning LLMs' judgments with human preferences,
overlooking the existence of biases and mistakes in human judgment.
Furthermore, how to select suitable LLM judgments given multiple potential LLM
responses remains underexplored. To address these two aforementioned issues, we
propose a three-stage meta-judge selection pipeline: 1) developing a
comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM
agents to score judgments, and 3) applying a threshold to filter out
low-scoring judgments. Compared to methods using a single LLM as both judge and
meta-judge, our pipeline introduces multi-agent collaboration and a more
comprehensive rubric. Experimental results on the JudgeBench dataset show about
15.55\% improvement compared to raw judgments and about 8.37\% improvement over
the single-agent baseline. Our work demonstrates the potential of LLMs as
meta-judges and lays the foundation for future research on constructing
preference datasets for LLM-as-a-judge reinforcement learning.

</details>


### [225] [AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models](https://arxiv.org/abs/2504.17179)
*Mohammad Zarei,Melanie A Jutras,Eliana Evans,Mike Tan,Omid Aaramoon*

Main category: cs.AI

TL;DR: 本文提出了一种利用生成式与可解释AI技术来理解和缓解自动驾驶汽车（AV）罕见故障模式（RFMs）的新方法，提升AV的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车依赖AI进行物体检测，但即便经过大量数据训练，仍难以识别罕见故障模式（RFMs），被称为‘长尾挑战’。本文旨在通过技术手段解决这一问题。

Method: 使用分割掩码生成环境掩码，结合文本提示输入定制扩散模型，通过对抗性噪声优化引导Stable Diffusion修复模型，生成能暴露AI漏洞的多样化环境图像，并生成自然语言描述。

Result: 生成的图像能有效暴露AI系统的脆弱性，自然语言描述为开发者和决策者提供了改进AV系统安全性的指导。

Conclusion: 该方法为理解和缓解AV系统的罕见故障模式提供了有效工具，有助于提升自动驾驶的安全性和可靠性。

Abstract: Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately
detect objects and interpret their surroundings. However, even when trained
using millions of miles of real-world data, AVs are often unable to detect rare
failure modes (RFMs). The problem of RFMs is commonly referred to as the
"long-tail challenge", due to the distribution of data including many instances
that are very rarely seen. In this paper, we present a novel approach that
utilizes advanced generative and explainable AI techniques to aid in
understanding RFMs. Our methods can be used to enhance the robustness and
reliability of AVs when combined with both downstream model training and
testing. We extract segmentation masks for objects of interest (e.g., cars) and
invert them to create environmental masks. These masks, combined with carefully
crafted text prompts, are fed into a custom diffusion model. We leverage the
Stable Diffusion inpainting model guided by adversarial noise optimization to
generate images containing diverse environments designed to evade object
detection models and expose vulnerabilities in AI systems. Finally, we produce
natural language descriptions of the generated RFMs that can guide developers
and policymakers to improve the safety and reliability of AV systems.

</details>


### [226] [Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning](https://arxiv.org/abs/2504.17282)
*Lynn Cherif,Flemming Kondrup,David Venuto,Ankit Anand,Doina Precup,Khimya Khetarpal*

Main category: cs.AI

TL;DR: 论文提出了CoGA方法，通过意图驱动的动作空间约束和预训练视觉语言模型生成代码，显著提升了在低数据环境下的样本效率，并在MiniWob++基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在网络GUI中，基于统一动作空间的自主导航代理通常需要大量领域专家示范才能表现良好，且在稀疏奖励和大动作空间环境中样本效率低下。论文旨在通过意图驱动的动作空间约束解决这一问题。

Method: 论文提出了CoGA方法，利用预训练视觉语言模型生成代码，通过完全自动化的程序生成和验证流程确定可行的动作，并将其整合到强化学习代理中。

Result: 在MiniWob++基准测试中，CoGA展示了比纯强化学习代理高几个数量级的样本效率，其生成的程序能在一类任务中泛化，且在少量专家示范下表现优于或等同于行为克隆。

Conclusion: 通过意图驱动的动作空间约束，CoGA显著提高了在低数据环境下的样本效率，为网络GUI中的自主导航提供了高效解决方案。

Abstract: Agents that can autonomously navigate the web through a graphical user
interface (GUI) using a unified action space (e.g., mouse and keyboard actions)
can require very large amounts of domain-specific expert demonstrations to
achieve good performance. Low sample efficiency is often exacerbated in
sparse-reward and large-action-space environments, such as a web GUI, where
only a few actions are relevant in any given situation. In this work, we
consider the low-data regime, with limited or no access to expert behavior. To
enable sample-efficient learning, we explore the effect of constraining the
action space through $\textit{intent-based affordances}$ -- i.e., considering
in any situation only the subset of actions that achieve a desired outcome. We
propose $\textbf{Code as Generative Affordances}$ $(\textbf{$\texttt{CoGA}$})$,
a method that leverages pre-trained vision-language models (VLMs) to generate
code that determines affordable actions through implicit intent-completion
functions and using a fully-automated program generation and verification
pipeline. These programs are then used in-the-loop of a reinforcement learning
agent to return a set of affordances given a pixel observation. By greatly
reducing the number of actions that an agent must consider, we demonstrate on a
wide range of tasks in the MiniWob++ benchmark that: $\textbf{1)}$
$\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,
$\textbf{2)}$ $\texttt{CoGA}$'s programs can generalize within a family of
tasks, and $\textbf{3)}$ $\texttt{CoGA}$ performs better or on par compared
with behavior cloning when a small number of expert demonstrations is
available.

</details>


### [227] [AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining](https://arxiv.org/abs/2504.17295)
*Shahrzad Khayatbashi,Viktor Sjölind,Anders Granåker,Amin Jalali*

Main category: cs.AI

TL;DR: 本文通过保险行业的案例研究，展示了大型语言模型（LLM）在自动化知识密集型任务中的应用，并利用对象中心流程挖掘（OCPM）评估其对流程可扩展性的影响。研究表明，LLM显著提升运营能力，但也带来新的动态需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 自动化知识密集型任务推动了数字化转型，但需评估传统与AI增强流程变体共存的影响，特别是LLM在真实场景中的实用性。

Method: 采用对象中心流程挖掘（OCPM）方法，通过保险行业部署LLM自动化理赔部分识别的案例研究进行分析。

Result: LLM显著提升了运营能力，但引入了新的流程动态，需进一步优化。OCPM在真实场景中展现出优势与局限性。

Conclusion: 研究表明LLM和OCPM在推动流程自动化中具有潜力，但需持续优化以适应新动态，并在更多场景中验证其适用性。

Abstract: Recent advancements in Artificial Intelligence (AI), particularly Large
Language Models (LLMs), have enhanced organizations' ability to reengineer
business processes by automating knowledge-intensive tasks. This automation
drives digital transformation, often through gradual transitions that improve
process efficiency and effectiveness. To fully assess the impact of such
automation, a data-driven analysis approach is needed - one that examines how
traditional and AI-enhanced process variants coexist during this transition.
Object-Centric Process Mining (OCPM) has emerged as a valuable method that
enables such analysis, yet real-world case studies are still needed to
demonstrate its applicability. This paper presents a case study from the
insurance sector, where an LLM was deployed in production to automate the
identification of claim parts, a task previously performed manually and
identified as a bottleneck for scalability. To evaluate this transformation, we
apply OCPM to assess the impact of AI-driven automation on process scalability.
Our findings indicate that while LLMs significantly enhance operational
capacity, they also introduce new process dynamics that require further
refinement. This study also demonstrates the practical application of OCPM in a
real-world setting, highlighting its advantages and limitations.

</details>


### [228] [Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning](https://arxiv.org/abs/2504.17356)
*Weiliang Zhang,Xiaohan Huang,Yi Du,Ziyue Qiao,Qingqing Long,Zhen Meng,Yuanchun Zhou,Meng Xiao*

Main category: cs.AI

TL;DR: 本文提出了一种基于分层强化学习（HRLFS）的特征选择方法，通过LLM提取特征数学和语义信息并聚类，构建分层代理机制优化复杂数据集处理。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在处理复杂数据集时效率不足，因单特征单代理的范式导致计算复杂度高。本文旨在解决这一问题。

Method: 使用LLM提取特征数学和语义信息，聚类后为每个簇构建分层代理，减少代理数量。

Result: 实验显示HRLFS在提升下游ML性能的同时，通过减少代理数量显著加速运行时间。

Conclusion: HRLFS通过分层代理机制有效优化特征选择，兼具高效性和可扩展性。

Abstract: Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.

</details>


### [229] [Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation](https://arxiv.org/abs/2504.17402)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisarkka,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在领域特定本体生成中的应用，评估了DeepSeek和o1-preview两种模型在多领域中的表现，结果显示它们在所有领域均表现一致，表明LLM方法具有领域无关的可扩展性潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在本体工程中显示出潜力，但领域特定本体生成的适用性尚不明确，因此需探索其在自动本体生成中的表现。

Method: 通过基于能力问题（CQs）和用户故事生成本体，评估DeepSeek和o1-preview在六个不同领域的表现，共使用95个CQs测试模型推理能力。

Result: 实验结果显示，两种LLMs在所有领域中表现一致，证明其在领域无关的本体生成任务中具有泛化能力。

Conclusion: LLM方法在可扩展且领域无关的本体构建中展现出潜力，为增强自动推理和知识表示技术的研究奠定了基础。

Abstract: Large Language Models (LLMs) have shown significant potential for ontology
engineering. However, it is still unclear to what extent they are applicable to
the task of domain-specific ontology generation. In this study, we explore the
application of LLMs for automated ontology generation and evaluate their
performance across different domains. Specifically, we investigate the
generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both
equipped with reasoning capabilities, by generating ontologies from a set of
competency questions (CQs) and related user stories. Our experimental setup
comprises six distinct domains carried out in existing ontology engineering
projects and a total of 95 curated CQs designed to test the models' reasoning
for ontology engineering. Our findings show that with both LLMs, the
performance of the experiments is remarkably consistent across all domains,
indicating that these methods are capable of generalizing ontology generation
tasks irrespective of the domain. These results highlight the potential of
LLM-based approaches in achieving scalable and domain-agnostic ontology
construction and lay the groundwork for further research into enhancing
automated reasoning and knowledge representation techniques.

</details>


### [230] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/abs/2504.17404)
*Feifei Zhao,Yuwei Wang,Enmeng Lu,Dongcheng Zhao,Bing Han,Haibo Tong,Yao Liang,Dongqi Liang,Kang Sun,Lei Wang,Yitao Liang,Chao Liu,Yaodong Yang,Yi Zeng*

Main category: cs.AI

TL;DR: 论文探讨了从AI到超级人工智能（ASI）演进中的超级对齐问题，提出了一种结合外部监督和内在主动对齐的框架，以实现人类与AI的可持续共生社会。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越强大和自主，可能超越人类智能水平（ASI），但这也带来了失控、违背人类价值观甚至灾难性后果的风险。因此，如何确保超级AI与人类意图和价值观保持一致（超级对齐）成为亟待解决的问题。

Method: 论文重新定义了超级对齐为“人类与AI共同迈向可持续共生社会的对齐”，提出了一个结合外部监督和内在主动对齐的框架。外部监督强调以人类为中心的决策，辅以可解释的自动化评估与修正；内在主动对齐则基于自我意识、反思和共情，主动推断人类意图并考虑人类福祉。

Result: 通过结合外部驱动和内在驱动的对齐方法，论文展示了如何实现人类与AI的协同对齐，从而为安全、有益的通用人工智能（AGI）和超级人工智能（ASI）铺平道路。

Conclusion: 论文提出的超级对齐框架为人类与AI的可持续共生社会提供了可行路径，强调外部监督与内在主动对齐的整合是实现安全、有益ASI的关键。

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems much
smarter than humans, remain aligned with human (compatible) intentions and
values. Existing scalable oversight and weak-to-strong generalization methods
may prove substantially infeasible and inadequate when facing ASI. We must
explore safer and more pluralistic frameworks and approaches for
superalignment. In this paper, we redefine superalignment as the human-AI
co-alignment towards a sustainable symbiotic society, and highlight a framework
that integrates external oversight and intrinsic proactive alignment. External
oversight superalignment should be grounded in human-centered ultimate
decision, supplemented by interpretable automated evaluation and correction, to
achieve continuous alignment with humanity's evolving values. Intrinsic
proactive superalignment is rooted in a profound understanding of the self,
others, and society, integrating self-awareness, self-reflection, and empathy
to spontaneously infer human intentions, distinguishing good from evil and
proactively considering human well-being, ultimately attaining human-AI
co-alignment through iterative interaction. The integration of
externally-driven oversight with intrinsically-driven proactive alignment
empowers sustainable symbiotic societies through human-AI co-alignment, paving
the way for achieving safe and beneficial AGI and ASI for good, for human, and
for a symbiotic ecology.

</details>


### [231] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/abs/2504.17531)
*Justus Flerlage,Ilja Behnke,Odej Kao*

Main category: cs.AI

TL;DR: 论文探讨了利用大型语言模型（LLM）生成代码以实现用户意图的可行性，展示了AI与人类协作解决复杂工作流程的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的提升，特别是LLMs的发展，传统的用户与设备交互方式需要重新评估，以实现更高效的意图解析和工作流生成。

Method: 通过向LLM（如GPT-4o-mini）提供用户意图和简化的API来生成代码，并分析其执行效果。

Result: 实验证明该方法总体可行，LLM在生成符合用户意图的代码工作流方面表现优异。

Conclusion: AI生成的代码工作流为混合协作模式提供了新方向，展现了LLM在实际应用中的潜力。

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code, which is tantamount to the generation of workflows
comprising a multitude of interdependent steps. This development represents a
significant progression in the realm of hybrid workflows, where human and
artificial intelligence collaborate to address user intentions, with the former
responsible for defining these intentions and the latter for implementing the
solutions to address them. In this paper, we investigate the feasibility of
generating and executing workflows through code generation that results from
prompting an LLM with a concrete user intention, such as \emph{Please send my
car title to my insurance company}, and a simplified application programming
interface for a GUI-less operating system. We provide in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate a general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>


### [232] [Auditing the Ethical Logic of Generative AI Models](https://arxiv.org/abs/2504.17544)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah*

Main category: cs.AI

TL;DR: 论文提出了一个五维审计模型来评估大型语言模型的伦理推理能力，发现模型在伦理决策上趋于一致，但在解释严谨性和道德优先级上存在差异，且思维链提示和优化模型显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型越来越多地应用于高风险领域，亟需可靠方法评估其伦理推理能力。

Method: 采用五维审计模型（分析质量、伦理考虑广度、解释深度、一致性和决断力），结合多轮提示和新型伦理困境测试，评估7种主要LLM。

Result: 模型在伦理决策上表现一致，但解释严谨性和道德优先级差异明显；思维链提示和优化模型显著提升审计指标得分。

Conclusion: 研究提出了一种可扩展的AI伦理评估方法，并强调了AI在复杂决策中辅助人类伦理推理的潜力。

Abstract: As generative AI models become increasingly integrated into high-stakes
domains, the need for robust methods to evaluate their ethical reasoning
becomes increasingly important. This paper introduces a five-dimensional audit
model -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth
of Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic
of leading large language models (LLMs). Drawing on traditions from applied
ethics and higher-order thinking, we present a multi-battery prompt approach,
including novel ethical dilemmas, to probe the models' reasoning across diverse
contexts. We benchmark seven major LLMs finding that while models generally
converge on ethical decisions, they vary in explanatory rigor and moral
prioritization. Chain-of-Thought prompting and reasoning-optimized models
significantly enhance performance on our audit metrics. This study introduces a
scalable methodology for ethical benchmarking of AI systems and highlights the
potential for AI to complement human moral reasoning in complex decision-making
contexts.

</details>


### [233] [A Framework for the Assurance of AI-Enabled Systems](https://arxiv.org/abs/2504.16937)
*Ariel S. Kapusta,David Jin,Peter M. Teague,Robert A. Houston,Jonathan B. Elliott,Grace Y. Park,Shelby S. Holdren*

Main category: cs.AI

TL;DR: 美国国防部（DOD）希望通过加速AI能力的开发与部署以保持战略优势，但AI算法的强大特性也带来了技术、安全和伦理挑战。本文提出了一种基于声明的风险管理与保障框架，以平衡快速部署、成功采用和严格评估的需求。


<details>
  <summary>Details</summary>
Motivation: 为应对AI算法在国防应用中的技术、安全和伦理挑战，解决开发、测试和保障中的不确定性，确保AI系统的可信度和有效性。

Method: 提出基于声明的风险管理与保障框架，支持所有获取途径的项目，确保AI系统在其生命周期中满足任务目标且不引入不可接受的风险。

Result: 提供了一个AI保障的框架流程、相关定义以及重要考虑事项的讨论，旨在为DOD提供高效且稳健的机制。

Conclusion: 该框架旨在帮助DOD快速部署有效的AI能力，同时不忽视关键风险或削弱利益相关者的信任。

Abstract: The United States Department of Defense (DOD) looks to accelerate the
development and deployment of AI capabilities across a wide spectrum of defense
applications to maintain strategic advantages. However, many common features of
AI algorithms that make them powerful, such as capacity for learning,
large-scale data ingestion, and problem-solving, raise new technical, security,
and ethical challenges. These challenges may hinder adoption due to uncertainty
in development, testing, assurance, processes, and requirements.
Trustworthiness through assurance is essential to achieve the expected value
from AI.
  This paper proposes a claims-based framework for risk management and
assurance of AI systems that addresses the competing needs for faster
deployment, successful adoption, and rigorous evaluation. This framework
supports programs across all acquisition pathways provide grounds for
sufficient confidence that an AI-enabled system (AIES) meets its intended
mission goals without introducing unacceptable risks throughout its lifecycle.
The paper's contributions are a framework process for AI assurance, a set of
relevant definitions to enable constructive conversations on the topic of AI
assurance, and a discussion of important considerations in AI assurance. The
framework aims to provide the DOD a robust yet efficient mechanism for swiftly
fielding effective AI capabilities without overlooking critical risks or
undermining stakeholder trust.

</details>


### [234] [Rational Inference in Formal Concept Analysis](https://arxiv.org/abs/2504.16938)
*Lucas Carr,Nicholas Leisegang,Thomas Meyer,Sergei Obiedkov*

Main category: cs.AI

TL;DR: 该论文将KLM框架引入FCA，提出一种适用于非单调推理的可废止条件推理方法，并在上下文中提供更相关的结论。


<details>
  <summary>Details</summary>
Motivation: 目前FCA中的隐含性规则无法处理错误数据或例外情况，非单调推理在FCA领域尚未充分研究。

Method: 通过构造偏好序对可能世界建模，将KLM框架扩展到FCA中，并保持其非单调推理特性。

Result: 所提出的方法在FCA中保持了非单调推理特性，并提供了比命题情境更上下文化的推理能力。

Conclusion: 该方法不仅与KLM框架一致，还提升了推理的上下文相关性。

Abstract: Defeasible conditionals are a form of non-monotonic inference which enable
the expression of statements like "if $\phi$ then normally $\psi$". The KLM
framework defines a semantics for the propositional case of defeasible
conditionals by construction of a preference ordering over possible worlds. The
pattern of reasoning induced by these semantics is characterised by consequence
relations satisfying certain desirable properties of non-monotonic reasoning.
In FCA, implications are used to describe dependencies between attributes.
However, these implications are unsuitable to reason with erroneous data or
data prone to exceptions. Until recently, the topic of non-monotonic inference
in FCA has remained largely uninvestigated. In this paper, we provide a
construction of the KLM framework for defeasible reasoning in FCA and show that
this construction remains faithful to the principle of non-monotonic inference
described in the original framework. We present an additional argument that,
while remaining consistent with the original ideas around non-monotonic
reasoning, the defeasible reasoning we propose in FCA offers a more contextual
view on inference, providing the ability for more relevant conclusions to be
drawn when compared to the propositional case.

</details>


### [235] [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
*Emre Can Acikgoz,Cheng Qian,Hongru Wang,Vardhan Dongre,Xiusi Chen,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）驱动的对话代理的现状、挑战与未来发展目标，提出了三个核心维度（推理、监控、控制）的分类法，并指出了研究空白与关键方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs驱动的对话代理的潜力、局限性与未来发展方向，以实现更接近人类智能的对话系统。

Method: 通过系统性分析，围绕推理、监控和控制三个维度对对话代理进行分类，并基于此提出新的分类法。

Result: 明确了当前技术的研究空白，如长期多轮推理、自我进化能力和协作任务完成等，并提出了未来研究方向。

Conclusion: 本文为对话代理的研究提供了结构化框架，指出未来需聚焦于更现实的评估、个性化与主动性等功能，推动人工通用智能的发展。

Abstract: Recent advances in Large Language Models (LLMs) have propelled conversational
AI from traditional dialogue systems into sophisticated agents capable of
autonomous actions, contextual awareness, and multi-turn interactions with
users. Yet, fundamental questions about their capabilities, limitations, and
paths forward remain open. This survey paper presents a desideratum for
next-generation Conversational Agents - what has been achieved, what challenges
persist, and what must be done for more scalable systems that approach
human-level intelligence. To that end, we systematically analyze LLM-driven
Conversational Agents by organizing their capabilities into three primary
dimensions: (i) Reasoning - logical, systematic thinking inspired by human
intelligence for decision making, (ii) Monitor - encompassing self-awareness
and user interaction monitoring, and (iii) Control - focusing on tool
utilization and policy following. Building upon this, we introduce a novel
taxonomy by classifying recent work on Conversational Agents around our
proposed desideratum. We identify critical research gaps and outline key
directions, including realistic evaluations, long-term multi-turn reasoning
skills, self-evolution capabilities, collaborative and multi-agent task
completion, personalization, and proactivity. This work aims to provide a
structured foundation, highlight existing limitations, and offer insights into
potential future research directions for Conversational Agents, ultimately
advancing progress toward Artificial General Intelligence (AGI). We maintain a
curated repository of papers at:
https://github.com/emrecanacikgoz/awesome-conversational-agents.

</details>


### [236] [A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs](https://arxiv.org/abs/2504.17006)
*Jalal Arabneydi,Saiful Islam,Srijita Das,Sai Krishna Gottipati,William Duguay,Cloderic Mars,Matthew E. Taylor,Matthew Guzdial,Antoine Fagette,Younes Zerouali*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新型的多层分级HITL DRL算法，结合了三种学习方式（自主学习、模仿学习和迁移学习）和三种人类输入形式（奖励、动作和示范），并通过无人机对抗实例验证了其效率和性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着深度强化学习（DRL）的普及，人机协同（HITL）方法有望革新决策问题的解决方式，创造人AI协作的新机会。论文旨在探索如何系统地整合人类信息到AI解决方案中，并解决复杂问题中的挑战与权衡。

Method: 论文提出了一种多层分级的HITL DRL算法，综合了三种学习方式：自主学习、模仿学习和迁移学习。同时考虑了三种人类输入形式：奖励、动作和示范。通过Cogment开源软件实现，并在无人机对抗场景中验证。

Result: 实验显示HITL显著加速训练并提升性能：人类建议为梯度方法提供方向性指导并降低方差，同时建议量的适度控制避免了过拟合或欠拟合。在超负荷和诱饵攻击两种复杂场景中，人机协作展现出高效解决问题的能力。

Conclusion: 论文证明了HITL DRL算法在复杂任务中的有效性，强调人类输入的合理整合能够优化AI解决方案。未来研究方向包括进一步探索人机协作的边界与效率提升。

Abstract: With the growing popularity of deep reinforcement learning (DRL),
human-in-the-loop (HITL) approach has the potential to revolutionize the way we
approach decision-making problems and create new opportunities for human-AI
collaboration. In this article, we introduce a novel multi-layered hierarchical
HITL DRL algorithm that comprises three types of learning: self learning,
imitation learning and transfer learning. In addition, we consider three forms
of human inputs: reward, action and demonstration. Furthermore, we discuss main
challenges, trade-offs and advantages of HITL in solving complex problems and
how human information can be integrated in the AI solution systematically. To
verify our technical results, we present a real-world unmanned aerial vehicles
(UAV) problem wherein a number of enemy drones attack a restricted area. The
objective is to design a scalable HITL DRL algorithm for ally drones to
neutralize the enemy drones before they reach the area. To this end, we first
implement our solution using an award-winning open-source HITL software called
Cogment. We then demonstrate several interesting results such as (a) HITL leads
to faster training and higher performance, (b) advice acts as a guiding
direction for gradient methods and lowers variance, and (c) the amount of
advice should neither be too large nor too small to avoid over-training and
under-training. Finally, we illustrate the role of human-AI cooperation in
solving two real-world complex scenarios, i.e., overloaded and decoy attacks.

</details>


### [237] [Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification](https://arxiv.org/abs/2504.17017)
*Balaji Rao,William Eiers,Carlo Lipizzi*

Main category: cs.AI

TL;DR: 该论文提出了一个框架，用于生成形式化语言的完整证明，以验证软件代码（如LLM生成的代码）的正确性。框架包含三个模块：自然语言语句生成、形式化证明生成和启发式模块。通过两阶段微调训练LLM，并在miniF2F测试基准和Isabelle证明助手中验证效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成代码的普及，形式化验证代码属性的需求日益增长，但通用定理证明仍是一个未解决的挑战。本文旨在通过结合形式化验证和LLM的能力，提升代码验证的效率和准确性。

Method: 框架包括自然语言语句生成、基于LLM的形式化证明生成和启发式模块。采用两阶段微调（SFT和RL）训练LLM，确保生成的代码语法正确且可被定理证明器验证。

Result: 在miniF2F测试基准和Isabelle证明助手中验证了框架的有效性，并设计了AWS S3桶访问策略代码的验证用例。同时创建了基于FVEL_ER数据集的训练数据集。

Conclusion: 该框架为LLM生成代码的形式化验证提供了可行方案，结合了自然语言与形式化证明的优势，未来可通过进一步优化提升通用定理证明能力。

Abstract: Formally verifying properties of software code has been a highly desirable
task, especially with the emergence of LLM-generated code. In the same vein,
they provide an interesting avenue for the exploration of formal verification
and mechanistic interpretability. Since the introduction of code-specific
models, despite their successes in generating code in Lean4 and Isabelle, the
task of generalized theorem proving still remains far from being fully solved
and will be a benchmark for reasoning capability in LLMs. In this work, we
introduce a framework that generates whole proofs in a formal language to be
used within systems that utilize the power of built-in tactics and
off-the-shelf automated theorem provers. Our framework includes 3 components:
generating natural language statements of the code to be verified, an LLM that
generates formal proofs for the given statement, and a module employing
heuristics for building the final proof. To train the LLM, we employ a 2-stage
fine-tuning process, where we first use SFT-based training to enable the model
to generate syntactically correct Isabelle code and then RL-based training that
encourages the model to generate proofs verified by a theorem prover. We
validate our framework using the miniF2F-test benchmark and the Isabelle proof
assistant and design a use case to verify the correctness of the AWS S3 bucket
access policy code. We also curate a dataset based on the
FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.

</details>


### [238] [Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments](https://arxiv.org/abs/2504.17087)
*Yuran Li,Jama Hussein Mohamud,Chongren Sun,Di Wu,Benoit Boulet*

Main category: cs.AI

TL;DR: 论文提出了一种三阶段元评估选择流程，通过多智能体协作和综合评分标准，提升了大型语言模型（LLMs）作为评估者的性能，实验结果显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度增加，LLMs评估响应变得困难。现有研究多关注对齐人类偏好，忽视了人类判断中的偏见和错误，且未深入探索如何从多个潜在LLM响应中选择合适判断。

Method: 采用三阶段流程：1) 与GPT-4和人类专家共同制定综合评分标准；2) 使用三个高级LLM智能体评分；3) 通过阈值过滤低分判断。

Result: 在JudgeBench数据集上，相比原始判断和单智能体基线，分别提升15.55%和8.37%。

Conclusion: 研究证明了LLMs作为元评估者的潜力，并为未来构建偏好数据集以支持LLM-as-a-judge强化学习奠定了基础。

Abstract: Large language models (LLMs) are being widely applied across various fields,
but as tasks become more complex, evaluating their responses is increasingly
challenging. Compared to human evaluators, the use of LLMs to support
performance evaluation offers a more efficient alternative. However, most
studies focus mainly on aligning LLMs' judgments with human preferences,
overlooking the existence of biases and mistakes in human judgment.
Furthermore, how to select suitable LLM judgments given multiple potential LLM
responses remains underexplored. To address these two aforementioned issues, we
propose a three-stage meta-judge selection pipeline: 1) developing a
comprehensive rubric with GPT-4 and human experts, 2) using three advanced LLM
agents to score judgments, and 3) applying a threshold to filter out
low-scoring judgments. Compared to methods using a single LLM as both judge and
meta-judge, our pipeline introduces multi-agent collaboration and a more
comprehensive rubric. Experimental results on the JudgeBench dataset show about
15.55\% improvement compared to raw judgments and about 8.37\% improvement over
the single-agent baseline. Our work demonstrates the potential of LLMs as
meta-judges and lays the foundation for future research on constructing
preference datasets for LLM-as-a-judge reinforcement learning.

</details>


### [239] [AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models](https://arxiv.org/abs/2504.17179)
*Mohammad Zarei,Melanie A Jutras,Eliana Evans,Mike Tan,Omid Aaramoon*

Main category: cs.AI

TL;DR: 论文提出了一种利用生成式AI和可解释AI技术的新方法，以解决自动驾驶车辆中罕见故障模式（RFMs）的检测问题，旨在提升系统的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）依赖AI检测物体和解读周围环境，但在实际数据训练后仍难以检测到罕见的故障模式（RFMs），即'长尾问题'。为解决这一问题，研究提出了一种新方法。

Method: 提出了一种结合生成式AI和可解释AI的技术。通过从对象分割掩模生成环境掩模，结合文本提示，使用定制的扩散模型（基于Stable Diffusion修复模型）生成多样化的对抗性图像，以暴露AI系统的漏洞。

Result: 该方法能生成包含RFMs的图像和自然语言描述，帮助开发者和政策制定者提高自动驾驶系统的安全性和可靠性。

Conclusion: 论文提出的方法有效解决了长尾问题，通过生成对抗性环境和自然语言解释，为自动驾驶系统的优化提供了实用工具。

Abstract: Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately
detect objects and interpret their surroundings. However, even when trained
using millions of miles of real-world data, AVs are often unable to detect rare
failure modes (RFMs). The problem of RFMs is commonly referred to as the
"long-tail challenge", due to the distribution of data including many instances
that are very rarely seen. In this paper, we present a novel approach that
utilizes advanced generative and explainable AI techniques to aid in
understanding RFMs. Our methods can be used to enhance the robustness and
reliability of AVs when combined with both downstream model training and
testing. We extract segmentation masks for objects of interest (e.g., cars) and
invert them to create environmental masks. These masks, combined with carefully
crafted text prompts, are fed into a custom diffusion model. We leverage the
Stable Diffusion inpainting model guided by adversarial noise optimization to
generate images containing diverse environments designed to evade object
detection models and expose vulnerabilities in AI systems. Finally, we produce
natural language descriptions of the generated RFMs that can guide developers
and policymakers to improve the safety and reliability of AV systems.

</details>


### [240] [Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning](https://arxiv.org/abs/2504.17282)
*Lynn Cherif,Flemming Kondrup,David Venuto,Ankit Anand,Doina Precup,Khimya Khetarpal*

Main category: cs.AI

TL;DR: 该论文提出了CoGA方法，利用预训练的视觉语言模型生成代码，通过意图完成函数约束动作空间，从而在网页GUI导航任务中显著提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 在网页GUI导航任务中，传统方法依赖大量专家演示且样本效率低，尤其是在稀疏奖励和大动作空间环境下。论文旨在通过约束动作空间来解决这一问题。

Method: 提出了CoGA方法，利用预训练的视觉语言模型生成代码，结合自动程序生成和验证流程，通过意图完成函数确定可执行动作。

Result: 实验表明，CoGA在MiniWob++基准测试中样本效率显著优于强化学习代理，且能泛化到同类任务，性能与少量专家演示的行为克隆相当或更好。

Conclusion: CoGA通过约束动作空间和利用预训练模型，显著提升了网页GUI导航任务的样本效率和性能。

Abstract: Agents that can autonomously navigate the web through a graphical user
interface (GUI) using a unified action space (e.g., mouse and keyboard actions)
can require very large amounts of domain-specific expert demonstrations to
achieve good performance. Low sample efficiency is often exacerbated in
sparse-reward and large-action-space environments, such as a web GUI, where
only a few actions are relevant in any given situation. In this work, we
consider the low-data regime, with limited or no access to expert behavior. To
enable sample-efficient learning, we explore the effect of constraining the
action space through $\textit{intent-based affordances}$ -- i.e., considering
in any situation only the subset of actions that achieve a desired outcome. We
propose $\textbf{Code as Generative Affordances}$ $(\textbf{$\texttt{CoGA}$})$,
a method that leverages pre-trained vision-language models (VLMs) to generate
code that determines affordable actions through implicit intent-completion
functions and using a fully-automated program generation and verification
pipeline. These programs are then used in-the-loop of a reinforcement learning
agent to return a set of affordances given a pixel observation. By greatly
reducing the number of actions that an agent must consider, we demonstrate on a
wide range of tasks in the MiniWob++ benchmark that: $\textbf{1)}$
$\texttt{CoGA}$ is orders of magnitude more sample efficient than its RL agent,
$\textbf{2)}$ $\texttt{CoGA}$'s programs can generalize within a family of
tasks, and $\textbf{3)}$ $\texttt{CoGA}$ performs better or on par compared
with behavior cloning when a small number of expert demonstrations is
available.

</details>


### [241] [AI-Enhanced Business Process Automation: A Case Study in the Insurance Domain Using Object-Centric Process Mining](https://arxiv.org/abs/2504.17295)
*Shahrzad Khayatbashi,Viktor Sjölind,Anders Granåker,Amin Jalali*

Main category: cs.AI

TL;DR: 论文探讨了如何通过大型语言模型（LLM）和对象中心流程挖掘（OCPM）优化保险行业的理赔流程，发现虽然LLM提升了效率，但也带来了新的流程动态。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示AI驱动的流程自动化在现实业务中的影响，尤其在知识密集型任务中，LLM如何与传统流程共存并优化效率。

Method: 采用对象中心流程挖掘（OCPM）方法，结合保险行业的实际案例，评估LLM自动化对流程可扩展性的影响。

Result: LLM显著提升了操作能力，但也引入了需要进一步优化的新流程动态。OCPM在现实场景中展现了其价值和局限性。

Conclusion: 研究表明AI增强的流程自动化具有潜力，但需持续优化以适应新动态。OCPM是评估此类转型的有效工具。

Abstract: Recent advancements in Artificial Intelligence (AI), particularly Large
Language Models (LLMs), have enhanced organizations' ability to reengineer
business processes by automating knowledge-intensive tasks. This automation
drives digital transformation, often through gradual transitions that improve
process efficiency and effectiveness. To fully assess the impact of such
automation, a data-driven analysis approach is needed - one that examines how
traditional and AI-enhanced process variants coexist during this transition.
Object-Centric Process Mining (OCPM) has emerged as a valuable method that
enables such analysis, yet real-world case studies are still needed to
demonstrate its applicability. This paper presents a case study from the
insurance sector, where an LLM was deployed in production to automate the
identification of claim parts, a task previously performed manually and
identified as a bottleneck for scalability. To evaluate this transformation, we
apply OCPM to assess the impact of AI-driven automation on process scalability.
Our findings indicate that while LLMs significantly enhance operational
capacity, they also introduce new process dynamics that require further
refinement. This study also demonstrates the practical application of OCPM in a
real-world setting, highlighting its advantages and limitations.

</details>


### [242] [Comprehend, Divide, and Conquer: Feature Subspace Exploration via Multi-Agent Hierarchical Reinforcement Learning](https://arxiv.org/abs/2504.17356)
*Weiliang Zhang,Xiaohan Huang,Yi Du,Ziyue Qiao,Qingqing Long,Zhen Meng,Yuanchun Zhou,Meng Xiao*

Main category: cs.AI

TL;DR: 论文提出了一种名为HRLFS的新方法，通过结合大型语言模型和分层强化学习，优化特征选择过程，提升下游机器学习任务性能并减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的特征选择方法在处理复杂数据集时效率低下，主要因为每个特征对应一个代理的范式效率不高。

Method: HRLFS利用大型语言模型提取特征的数学和语义信息，进行聚类后构建分层代理，以减少代理数量并优化特征子空间探索。

Result: 实验证明HRLFS在效率、扩展性和鲁棒性上表现优越，显著提升了机器学习任务性能并加速了运行时间。

Conclusion: HRLFS通过分层代理策略有效解决了传统方法在处理复杂数据集时的效率问题，为特征选择提供了新思路。

Abstract: Feature selection aims to preprocess the target dataset, find an optimal and
most streamlined feature subset, and enhance the downstream machine learning
task. Among filter, wrapper, and embedded-based approaches, the reinforcement
learning (RL)-based subspace exploration strategy provides a novel objective
optimization-directed perspective and promising performance. Nevertheless, even
with improved performance, current reinforcement learning approaches face
challenges similar to conventional methods when dealing with complex datasets.
These challenges stem from the inefficient paradigm of using one agent per
feature and the inherent complexities present in the datasets. This observation
motivates us to investigate and address the above issue and propose a novel
approach, namely HRLFS. Our methodology initially employs a Large Language
Model (LLM)-based hybrid state extractor to capture each feature's mathematical
and semantic characteristics. Based on this information, features are
clustered, facilitating the construction of hierarchical agents for each
cluster and sub-cluster. Extensive experiments demonstrate the efficiency,
scalability, and robustness of our approach. Compared to contemporary or the
one-feature-one-agent RL-based approaches, HRLFS improves the downstream ML
performance with iterative feature subspace exploration while accelerating
total run time by reducing the number of agents involved.

</details>


### [243] [Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation](https://arxiv.org/abs/2504.17402)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisarkka,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 摘要探讨了大型语言模型（LLMs）在领域无关的自动化本体生成中的表现，通过多领域实验验证了其通用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在领域特定本体生成任务中的适用性和通用性。

Method: 方法包括使用两种先进LLMs（DeepSeek和o1-preview）生成本体，并通过95个CQs和相关用户故事测试其在六个领域中的表现。

Result: 结果显示两种LLMs在所有领域中表现一致，表明其能够通用化地完成本体生成任务。

Conclusion: 结论指出LLM方法具备可扩展且领域无关的本体构建潜力，并为进一步研究自动化推理和知识表示技术奠定基础。

Abstract: Large Language Models (LLMs) have shown significant potential for ontology
engineering. However, it is still unclear to what extent they are applicable to
the task of domain-specific ontology generation. In this study, we explore the
application of LLMs for automated ontology generation and evaluate their
performance across different domains. Specifically, we investigate the
generalizability of two state-of-the-art LLMs, DeepSeek and o1-preview, both
equipped with reasoning capabilities, by generating ontologies from a set of
competency questions (CQs) and related user stories. Our experimental setup
comprises six distinct domains carried out in existing ontology engineering
projects and a total of 95 curated CQs designed to test the models' reasoning
for ontology engineering. Our findings show that with both LLMs, the
performance of the experiments is remarkably consistent across all domains,
indicating that these methods are capable of generalizing ontology generation
tasks irrespective of the domain. These results highlight the potential of
LLM-based approaches in achieving scalable and domain-agnostic ontology
construction and lay the groundwork for further research into enhancing
automated reasoning and knowledge representation techniques.

</details>


### [244] [Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI Co-Alignment to Sustainable Symbiotic Society](https://arxiv.org/abs/2504.17404)
*Feifei Zhao,Yuwei Wang,Enmeng Lu,Dongcheng Zhao,Bing Han,Haibo Tong,Yao Liang,Dongqi Liang,Kang Sun,Lei Wang,Yitao Liang,Chao Liu,Yaodong Yang,Yi Zeng*

Main category: cs.AI

TL;DR: 本文重新定义了超级对齐（superalignment）为人类与AI协同对齐，以实现可持续共生社会。提出了一种整合外部监督与内在主动对齐的框架，以确保超级智能AI的安全和利益。


<details>
  <summary>Details</summary>
Motivation: 随着AI向超级智能（ASI）发展，其可能超出人类控制并引发灾难性后果。现有方法难以应对这一挑战，需探索更安全的超级对齐框架。

Method: 提出了外部监督和内在主动对齐相结合的双轨框架：外部监督以人类为中心决策为主，辅以自动化评估；内在主动对齐则通过自我认知和共情能力实现与人类的动态协同对齐。

Result: 通过双轨框架实现人类与AI的协同对齐，为可持续共生社会奠定基础，确保超级智能AI的安全和人类福祉。

Conclusion: 整合外部监督和内在主动对齐是实现超级智能安全且有益的关键，推动人-AI共生生态的可持续发展。

Abstract: Artificial Intelligence (AI) systems are becoming increasingly powerful and
autonomous, and may progress to surpass human intelligence levels, namely
Artificial Superintelligence (ASI). During the progression from AI to ASI, it
may exceed human control, violate human values, and even lead to irreversible
catastrophic consequences in extreme cases. This gives rise to a pressing issue
that needs to be addressed: superalignment, ensuring that AI systems much
smarter than humans, remain aligned with human (compatible) intentions and
values. Existing scalable oversight and weak-to-strong generalization methods
may prove substantially infeasible and inadequate when facing ASI. We must
explore safer and more pluralistic frameworks and approaches for
superalignment. In this paper, we redefine superalignment as the human-AI
co-alignment towards a sustainable symbiotic society, and highlight a framework
that integrates external oversight and intrinsic proactive alignment. External
oversight superalignment should be grounded in human-centered ultimate
decision, supplemented by interpretable automated evaluation and correction, to
achieve continuous alignment with humanity's evolving values. Intrinsic
proactive superalignment is rooted in a profound understanding of the self,
others, and society, integrating self-awareness, self-reflection, and empathy
to spontaneously infer human intentions, distinguishing good from evil and
proactively considering human well-being, ultimately attaining human-AI
co-alignment through iterative interaction. The integration of
externally-driven oversight with intrinsically-driven proactive alignment
empowers sustainable symbiotic societies through human-AI co-alignment, paving
the way for achieving safe and beneficial AGI and ASI for good, for human, and
for a symbiotic ecology.

</details>


### [245] [Towards Machine-Generated Code for the Resolution of User Intentions](https://arxiv.org/abs/2504.17531)
*Justus Flerlage,Ilja Behnke,Odej Kao*

Main category: cs.AI

TL;DR: 摘要讨论了AI（特别是大型语言模型LLMs）如何改变用户与设备的交互方式，提出通过模型生成的代码实现用户意图解析，验证了GPT-4o-mini在生成代码导向工作流方面的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前用户需要通过高级应用实现目标，而AI的发展提供了通过模型生成代码直接解决用户意图的新可能，这标志着人机协作的进步。

Method: 研究通过向LLM（如GPT-4o-mini）提供用户意图和简化API，生成并执行代码工作流，分析不同意图、生成代码及执行效果。

Result: 结果表明该方法是可行的，且GPT-4o-mini在根据用户意图生成代码工作流方面表现出色。

Conclusion: AI驱动的代码生成能有效解决用户意图，为未来人机协作工作流提供了新方向。

Abstract: The growing capabilities of Artificial Intelligence (AI), particularly Large
Language Models (LLMs), prompt a reassessment of the interaction mechanisms
between users and their devices. Currently, users are required to use a set of
high-level applications to achieve their desired results. However, the advent
of AI may signal a shift in this regard, as its capabilities have generated
novel prospects for user-provided intent resolution through the deployment of
model-generated code, which is tantamount to the generation of workflows
comprising a multitude of interdependent steps. This development represents a
significant progression in the realm of hybrid workflows, where human and
artificial intelligence collaborate to address user intentions, with the former
responsible for defining these intentions and the latter for implementing the
solutions to address them. In this paper, we investigate the feasibility of
generating and executing workflows through code generation that results from
prompting an LLM with a concrete user intention, such as \emph{Please send my
car title to my insurance company}, and a simplified application programming
interface for a GUI-less operating system. We provide in-depth analysis and
comparison of various user intentions, the resulting code, and its execution.
The findings demonstrate a general feasibility of our approach and that the
employed LLM, GPT-4o-mini, exhibits remarkable proficiency in the generation of
code-oriented workflows in accordance with provided user intentions.

</details>


### [246] [Auditing the Ethical Logic of Generative AI Models](https://arxiv.org/abs/2504.17544)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah*

Main category: cs.AI

TL;DR: 本文提出了一种五维审计模型，用于评估大型语言模型（LLMs）的伦理逻辑，发现模型在伦理决策上趋同但解释严谨性和道德优先级存在差异，思维链提示和优化模型显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型在高风险领域的应用增多，亟需评估其伦理推理能力的稳健方法。

Method: 采用五维审计模型（分析质量、伦理考虑广度、解释深度、一致性和决断性），通过多组提示（包括新颖的伦理困境）评估LLMs的推理能力。

Result: 七种主流LLMs在伦理决策上趋同，但解释严谨性和道德优先级表现不一，思维链提示和优化模型显著提高了审计指标表现。

Conclusion: 研究为AI系统的伦理评估提供了可扩展方法，并展示了AI在复杂决策中辅助人类道德推理的潜力。

Abstract: As generative AI models become increasingly integrated into high-stakes
domains, the need for robust methods to evaluate their ethical reasoning
becomes increasingly important. This paper introduces a five-dimensional audit
model -- assessing Analytic Quality, Breadth of Ethical Considerations, Depth
of Explanation, Consistency, and Decisiveness -- to evaluate the ethical logic
of leading large language models (LLMs). Drawing on traditions from applied
ethics and higher-order thinking, we present a multi-battery prompt approach,
including novel ethical dilemmas, to probe the models' reasoning across diverse
contexts. We benchmark seven major LLMs finding that while models generally
converge on ethical decisions, they vary in explanatory rigor and moral
prioritization. Chain-of-Thought prompting and reasoning-optimized models
significantly enhance performance on our audit metrics. This study introduces a
scalable methodology for ethical benchmarking of AI systems and highlights the
potential for AI to complement human moral reasoning in complex decision-making
contexts.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [247] [Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy](https://arxiv.org/abs/2504.17124)
*Ming Du,Mark Wolfman,Chengjun Sun,Shelly D. Kelly,Mathew J. Cherukara*

Main category: physics.app-ph

TL;DR: 提出了一种结合领域知识的贝叶斯优化方法，显著减少了XANES光谱数据采集所需的点数（仅需常规采样的15-20%），同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统XANES光谱数据采集耗时且缺乏对光谱特征的针对性采样，限制了动态实验的时间分辨率。

Method: 采用知识注入的贝叶斯优化方法，结合吸收边和前边缘峰等光谱特征信息进行自适应采样。

Result: 新方法在电池材料和催化剂上验证，吸收边峰值误差小于0.03 eV，整体均方根误差低于0.005，显著提升了数据采集效率和动态实验的时间分辨率。

Conclusion: 该方法提高了XANES实验的自动化程度，解决了采样不足或过度的问题，特别适用于高时间分辨或有限测量时间的动态实验。

Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful
technique for characterizing the chemical state and symmetry of individual
elements within materials, but requires collecting data at many energy points
which can be time-consuming. While adaptive sampling methods exist for
efficiently collecting spectroscopic data, they often lack domain-specific
knowledge about XANES spectra structure. Here we demonstrate a
knowledge-injected Bayesian optimization approach for adaptive XANES data
collection that incorporates understanding of spectral features like absorption
edges and pre-edge peaks. We show this method accurately reconstructs the
absorption edge of XANES spectra using only 15-20% of the measurement points
typically needed for conventional sampling, while maintaining the ability to
determine the x-ray energy of the sharp peak after absorption edge with errors
less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and
overall root-mean-square errors less than 0.005 compared to compared to
traditionally sampled spectra. Our experiments on battery materials and
catalysts demonstrate the method's effectiveness for both static and dynamic
XANES measurements, improving data collection efficiency and enabling better
time resolution for tracking chemical changes. This approach advances the
degree of automation in XANES experiments reducing the common errors of under-
or over-sampling points in near the absorption edge and enabling dynamic
experiments that require high temporal resolution or limited measurement time.

</details>


### [248] [Demonstration of an AI-driven workflow for dynamic x-ray spectroscopy](https://arxiv.org/abs/2504.17124)
*Ming Du,Mark Wolfman,Chengjun Sun,Shelly D. Kelly,Mathew J. Cherukara*

Main category: physics.app-ph

TL;DR: 提出的知识注入贝叶斯优化技术显著提高了XANES数据采集效率，仅需传统方法15-20%的测量点即可精准重构吸收边，误差控制在极低水平。


<details>
  <summary>Details</summary>
Motivation: 传统XANES数据采集耗时且缺乏对光谱结构的领域知识利用，需要一种兼顾效率与精度的自适应方法。

Method: 结合光谱特征（如吸收边、前边峰）的知识注入贝叶斯优化，实现自适应采样。

Result: 在电池材料和催化剂实验中，该方法以少量测量点（15-20%）实现吸收边重构，关键特征误差＜0.03 eV，整体误差＜0.005。

Conclusion: 该方法提升了XANES实验自动化水平，解决了传统过/欠采样问题，尤其适用于高时间分辨的动态实验。

Abstract: X-ray absorption near edge structure (XANES) spectroscopy is a powerful
technique for characterizing the chemical state and symmetry of individual
elements within materials, but requires collecting data at many energy points
which can be time-consuming. While adaptive sampling methods exist for
efficiently collecting spectroscopic data, they often lack domain-specific
knowledge about XANES spectra structure. Here we demonstrate a
knowledge-injected Bayesian optimization approach for adaptive XANES data
collection that incorporates understanding of spectral features like absorption
edges and pre-edge peaks. We show this method accurately reconstructs the
absorption edge of XANES spectra using only 15-20% of the measurement points
typically needed for conventional sampling, while maintaining the ability to
determine the x-ray energy of the sharp peak after absorption edge with errors
less than 0.03 eV, the absorption edge with errors less than 0.1 eV; and
overall root-mean-square errors less than 0.005 compared to compared to
traditionally sampled spectra. Our experiments on battery materials and
catalysts demonstrate the method's effectiveness for both static and dynamic
XANES measurements, improving data collection efficiency and enabling better
time resolution for tracking chemical changes. This approach advances the
degree of automation in XANES experiments reducing the common errors of under-
or over-sampling points in near the absorption edge and enabling dynamic
experiments that require high temporal resolution or limited measurement time.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [249] [Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks](https://arxiv.org/abs/2504.17346)
*Tran Thuy Nga Truong,Jooyong Kim*

Main category: cs.NE

TL;DR: 这篇论文提出了一种名为双个体遗传算法（Dual-Individual GA）的增强遗传算法，用于优化神经网络的二分类任务（如猫与非猫分类）。该方法仅使用两个个体（Leader和Follower）进行交叉，分别专注于开发和探索，并通过自适应层维度机制和帕累托优势排序提升性能。实验结果显示，该方法在训练和测试准确率上优于传统梯度方法。


<details>
  <summary>Details</summary>
Motivation: 传统梯度方法在神经网络优化中存在手动调参和局部最优问题。本文旨在通过双个体遗传算法解决这些问题，提高分类任务的效率和性能。

Method: 提出双个体遗传算法（Dual-Individual GA），使用Leader和Follower两个参数集进行交叉，分别负责开发和探索。引入了自适应层维度机制，并生成10种层架构配置（每种参数集5种），通过帕累托优势和成本排序优化。

Result: 实验表明，该方法在三层神经网络上实现了99.04%的训练准确率和80%的测试准确率（成本=0.034），优于梯度方法（98%训练准确率，80%测试准确率，成本=0.092）。

Conclusion: 双个体遗传算法在神经网络优化中表现出高效性和有效性，尤其在自动调参和避免局部最优方面具有优势。

Abstract: This paper introduces an enhanced Genetic Algorithm technique called
Dual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural
networks for binary image classification tasks, such as cat vs. non-cat
classification. The proposed method employs only two individuals for crossover,
represented by two parameter sets: Leader and Follower. The Leader focuses on
exploitation, representing the primary optimal solution at even-indexed
positions (0, 2, 4, ...), while the Follower promotes exploration by preserving
diversity and avoiding premature convergence, operating at odd-indexed
positions (1, 3, 5, ...). Leader and Follower are modeled as two phases or
roles. The key contributions of this work are threefold: (1) a self-adaptive
layer dimension mechanism that eliminates the need for manual tuning of layer
architectures; (2) generates two parameter sets, leader and follower parameter
sets, with 10 layer architecture configurations (5 for each set), ranked by
Pareto dominance and cost. post-optimization; and (3) demonstrated superior
performance compared to traditional gradient-based methods. Experimental
results show that the Dual-Individual GA achieves 99.04% training accuracy and
80% testing accuracy (cost = 0.034) on a three-layer network with architecture
[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%
training accuracy and 80% testing accuracy (cost = 0.092) on a four-layer
network with architecture [12288, 20, 7, 5, 1]. These findings highlight the
efficiency and effectiveness of the proposed method in optimizing neural
networks.

</details>


### [250] [Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN](https://arxiv.org/abs/2504.17751)
*Enqi Zhang*

Main category: cs.NE

TL;DR: 本文探讨了将脉冲神经网络（SNNs）视为二元激活的循环神经网络（RNNs）的视角，并分析了现有SNN架构在序列建模中的挑战。作者提出了固定不应期SNN架构以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 探讨SNNs在序列建模中的潜力，并解决传统SNN架构在长序列建模、生物启发组件的理论不足以及并行训练限制等方面的挑战。

Method: 通过对SNN中重置机制和不应期的系统分析，重新评估这些生物机制的必要性，并提出固定不应期的SNN架构。

Result: 提出了新的理论解释和见解，并设计了固定不应期SNN架构，以优化序列建模性能。

Conclusion: 固定不应期SNN架构为序列建模任务提供了一种有效解决方案，同时为SNN的理论研究提供了新视角。

Abstract: In the field of image recognition, spiking neural networks (SNNs) have
achieved performance comparable to conventional artificial neural networks
(ANNs). In such applications, SNNs essentially function as traditional neural
networks with quantized activation values. This article focuses on an another
alternative perspective,viewing SNNs as binary-activated recurrent neural
networks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN
architectures face several fundamental challenges in sequence modeling: (1)
Traditional models lack effective memory mechanisms for long-range sequence
modeling; (2) The biological-inspired components in SNNs (such as reset
mechanisms and refractory period applications) remain theoretically
under-explored for sequence tasks; (3) The RNN-like computational paradigm in
SNNs prevents parallel training across different timesteps.To address these
challenges, this study conducts a systematic analysis of the fundamental
mechanisms underlying reset operations and refractory periods in
binary-activated RNN-based SNN sequence models. We re-examine whether such
biological mechanisms are strictly necessary for generating sparse spiking
patterns, provide new theoretical explanations and insights, and ultimately
propose the fixed-refractory-period SNN architecture for sequence modeling.

</details>


### [251] [Dual-Individual Genetic Algorithm: A Dual-Individual Approach for Efficient Training of Multi-Layer Neural Networks](https://arxiv.org/abs/2504.17346)
*Tran Thuy Nga Truong,Jooyong Kim*

Main category: cs.NE

TL;DR: 论文提出了一种名为Dual-Individual GA的改进遗传算法，用于优化神经网络在二元图像分类（如猫与非猫分类）任务中的性能。该方法通过Leader和Follower两个参数集实现探索与开发的平衡，并展示了在训练和测试准确率上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 传统梯度方法在神经网络优化中存在收敛速度慢和手动调参问题，该研究旨在通过一种高效的遗传算法替代方案来自动优化网络架构和参数。

Method: 采用Dual-Individual GA，通过Leader（开发）和Follower（探索）两个角色的参数集交替优化，并结合自适应的网络层维度机制避免手动调参。

Result: 实验结果显示，该方法在三层网络上实现了99.04%的训练准确率和80%的测试准确率（成本0.034），优于传统梯度方法的98%训练准确率和80%测试准确率（成本0.092）。

Conclusion: Dual-Individual GA在神经网络优化中展现出高效性和有效性，尤其是在自动调整网络架构和参数方面具有显著优势。

Abstract: This paper introduces an enhanced Genetic Algorithm technique called
Dual-Individual Genetic Algorithm (Dual-Individual GA), which optimizes neural
networks for binary image classification tasks, such as cat vs. non-cat
classification. The proposed method employs only two individuals for crossover,
represented by two parameter sets: Leader and Follower. The Leader focuses on
exploitation, representing the primary optimal solution at even-indexed
positions (0, 2, 4, ...), while the Follower promotes exploration by preserving
diversity and avoiding premature convergence, operating at odd-indexed
positions (1, 3, 5, ...). Leader and Follower are modeled as two phases or
roles. The key contributions of this work are threefold: (1) a self-adaptive
layer dimension mechanism that eliminates the need for manual tuning of layer
architectures; (2) generates two parameter sets, leader and follower parameter
sets, with 10 layer architecture configurations (5 for each set), ranked by
Pareto dominance and cost. post-optimization; and (3) demonstrated superior
performance compared to traditional gradient-based methods. Experimental
results show that the Dual-Individual GA achieves 99.04% training accuracy and
80% testing accuracy (cost = 0.034) on a three-layer network with architecture
[12288, 17, 4, 1], outperforming a gradient-based approach that achieves 98%
training accuracy and 80% testing accuracy (cost = 0.092) on a four-layer
network with architecture [12288, 20, 7, 5, 1]. These findings highlight the
efficiency and effectiveness of the proposed method in optimizing neural
networks.

</details>


### [252] [Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN](https://arxiv.org/abs/2504.17751)
*Enqi Zhang*

Main category: cs.NE

TL;DR: SNNs被视为二进制激活的RNN用于序列建模，但目前存在长程记忆、生物启發機制理论不足及难以并行训练的问题。本研究分析了重置和不应期机制的必要性，并提出固定不应期结构的SNN架构。


<details>
  <summary>Details</summary>
Motivation: 探索SNNs作为序列建模工具的潜力，解决其现有架构在长程记忆、生物启发机制理论支持不足及并行训练限制上的问题。

Method: 系统分析SNN中重置和不应期机制，理论验证其必要性，最终提出固定不应期SNN架构。

Result: 研究发现某些生物启发机制并非必需，固定不应期架构能有效提升序列建模性能。

Conclusion: 固定不应期SNN架构为序列建模提供了新的解决方案，同时简化了生物启发机制的理论复杂性。

Abstract: In the field of image recognition, spiking neural networks (SNNs) have
achieved performance comparable to conventional artificial neural networks
(ANNs). In such applications, SNNs essentially function as traditional neural
networks with quantized activation values. This article focuses on an another
alternative perspective,viewing SNNs as binary-activated recurrent neural
networks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN
architectures face several fundamental challenges in sequence modeling: (1)
Traditional models lack effective memory mechanisms for long-range sequence
modeling; (2) The biological-inspired components in SNNs (such as reset
mechanisms and refractory period applications) remain theoretically
under-explored for sequence tasks; (3) The RNN-like computational paradigm in
SNNs prevents parallel training across different timesteps.To address these
challenges, this study conducts a systematic analysis of the fundamental
mechanisms underlying reset operations and refractory periods in
binary-activated RNN-based SNN sequence models. We re-examine whether such
biological mechanisms are strictly necessary for generating sparse spiking
patterns, provide new theoretical explanations and insights, and ultimately
propose the fixed-refractory-period SNN architecture for sequence modeling.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [253] [Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems](https://arxiv.org/abs/2504.17102)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Main category: math.OC

TL;DR: 该论文提出了一种通过神经网络学习可验证的收缩度量的方法，用于离散时间非线性动态系统，解决了现有方法在非平滑动态系统下的局限性，并通过高效的验证工具和合成方法展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的收缩度量方法在处理复杂非线性系统时，尤其是在非平滑动态（如ReLU激活的神经网络控制器）下，缺乏可扩展和有效的工具。论文旨在填补这一空白，提供一种新的理论框架和计算工具。

Method: 论文提出了一种新的充分条件，用于验证一般离散时间非线性系统的神经网络收缩度量，仅需假设动态的连续性。该方法利用先进的神经网络验证工具α,β-CROWN进行高效验证，并结合数据采样技术合成收缩度量。

Result: 通过多个非线性系统的实验，验证了该方法的有效性，成功合成了神经网络收缩度量并进行了形式化验证。

Conclusion: 论文提出的方法为非线性动态系统中的收缩度量提供了一种可扩展且高效的学习与验证框架，为实际应用中的稳定性分析提供了新工具。

Abstract: Contraction metrics are crucial in control theory because they provide a
powerful framework for analyzing stability, robustness, and convergence of
various dynamical systems. However, identifying these metrics for complex
nonlinear systems remains an open challenge due to the lack of scalable and
effective tools. This paper explores the approach of learning verifiable
contraction metrics parametrized as neural networks (NNs) for discrete-time
nonlinear dynamical systems. While prior works on formal verification of
contraction metrics for general nonlinear systems have focused on convex
optimization methods (e.g. linear matrix inequalities, etc) under the
assumption of continuously differentiable dynamics, the growing prevalence of
NN-based controllers, often utilizing ReLU activations, introduces challenges
due to the non-smooth nature of the resulting closed-loop dynamics. To bridge
this gap, we establish a new sufficient condition for establishing formal
neural contraction metrics for general discrete-time nonlinear systems assuming
only the continuity of the dynamics. We show that from a computational
perspective, our sufficient condition can be efficiently verified using the
state-of-the-art neural network verifier $\alpha,\!\beta$-CROWN, which scales
up non-convex neural network verification via novel integration of symbolic
linear bound propagation and branch-and-bound. Built upon our analysis tool, we
further develop a learning method for synthesizing neural contraction metrics
from sampled data. Finally, our approach is validated through the successful
synthesis and verification of NN contraction metrics for various nonlinear
examples.

</details>


### [254] [Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems](https://arxiv.org/abs/2504.17102)
*Haoyu Li,Xiangru Zhong,Bin Hu,Huan Zhang*

Main category: math.OC

TL;DR: 论文提出了一种通过学习神经网络参数化的可验证收缩度量方法，用于离散时间非线性动力系统，解决了现有方法在处理非平滑动态时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂非线性系统中识别收缩度量时面临可扩展性和有效性不足的问题，尤其是在处理基于ReLU激活的神经网络控制器时，其非平滑动态带来了挑战。

Method: 作者提出了一个新的充分条件，仅需动力系统的连续性假设，并利用先进的神经网络验证工具$α,β$-CROWN进行高效验证，进一步开发了从采样数据中学习收缩度量的方法。

Result: 通过多个非线性系统的实验，验证了该方法能成功合成和验证神经收缩度量。

Conclusion: 该方法为处理非平滑动态的神经网络控制器提供了有效的收缩度量分析工具，具有理论和实际应用价值。

Abstract: Contraction metrics are crucial in control theory because they provide a
powerful framework for analyzing stability, robustness, and convergence of
various dynamical systems. However, identifying these metrics for complex
nonlinear systems remains an open challenge due to the lack of scalable and
effective tools. This paper explores the approach of learning verifiable
contraction metrics parametrized as neural networks (NNs) for discrete-time
nonlinear dynamical systems. While prior works on formal verification of
contraction metrics for general nonlinear systems have focused on convex
optimization methods (e.g. linear matrix inequalities, etc) under the
assumption of continuously differentiable dynamics, the growing prevalence of
NN-based controllers, often utilizing ReLU activations, introduces challenges
due to the non-smooth nature of the resulting closed-loop dynamics. To bridge
this gap, we establish a new sufficient condition for establishing formal
neural contraction metrics for general discrete-time nonlinear systems assuming
only the continuity of the dynamics. We show that from a computational
perspective, our sufficient condition can be efficiently verified using the
state-of-the-art neural network verifier $\alpha,\!\beta$-CROWN, which scales
up non-convex neural network verification via novel integration of symbolic
linear bound propagation and branch-and-bound. Built upon our analysis tool, we
further develop a learning method for synthesizing neural contraction metrics
from sampled data. Finally, our approach is validated through the successful
synthesis and verification of NN contraction metrics for various nonlinear
examples.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [255] [Quantum Autoencoder for Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2504.17548)
*Kilian Tscharke,Maximilian Wendlinger,Afrae Ahouzi,Pallavi Bhardwaj,Kaweh Amoi-Taleghani,Michael Schrödl-Baumann,Pascal Debus*

Main category: quant-ph

TL;DR: 提出了一种基于量子自编码器（QAE）的多元时间序列异常检测框架，适用于企业级环境，验证了其性能与传统神经网络自编码器相当，但参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有量子自编码器在异常检测中仅适用于单变量数据，无法满足企业级高维多变量时间序列的需求，因此需要开发新框架。

Method: 设计并实验验证了一种专为多变量时间序列异常检测的QAE架构，通过理论开发和实验验证其可行性。

Result: 实验表明，该QAE模型在性能上与神经网络自编码器竞争，且训练参数更少，适用于企业级系统的实际数据。

Conclusion: 提出的QAE框架是半监督异常检测的有效替代方案，适用于企业级高维多变量时间序列场景。

Abstract: Anomaly Detection (AD) defines the task of identifying observations or events
that deviate from typical - or normal - patterns, a critical capability in IT
security for recognizing incidents such as system misconfigurations, malware
infections, or cyberattacks. In enterprise environments like SAP HANA Cloud
systems, this task often involves monitoring high-dimensional, multivariate
time series (MTS) derived from telemetry and log data. With the advent of
quantum machine learning offering efficient calculations in high-dimensional
latent spaces, many avenues open for dealing with such complex data. One
approach is the Quantum Autoencoder (QAE), an emerging and promising method
with potential for application in both data compression and AD. However, prior
applications of QAEs to time series AD have been restricted to univariate data,
limiting their relevance for real-world enterprise systems. In this work, we
introduce a novel QAE-based framework designed specifically for MTS AD towards
enterprise scale. We theoretically develop and experimentally validate the
architecture, demonstrating that our QAE achieves performance competitive with
neural-network-based autoencoders while requiring fewer trainable parameters.
We evaluate our model on datasets that closely reflect SAP system telemetry and
show that the proposed QAE is a viable and efficient alternative for
semisupervised AD in real-world enterprise settings.

</details>


### [256] [On the Generalization of Adversarially Trained Quantum Classifiers](https://arxiv.org/abs/2504.17690)
*Petros Georgiou,Aaron Mark Thomas,Sharu Theresa Jose,Osvaldo Simeone*

Main category: quant-ph

TL;DR: 本文研究了量子分类器在对抗攻击下的泛化误差边界，发现对抗训练在经典高维输入下样本复杂度增加可忽略，但对量子态攻击则依赖于嵌入的希尔伯特空间维度。


<details>
  <summary>Details</summary>
Motivation: 量子分类器容易受到对抗攻击的影响，因此需要研究如何通过对抗训练提高其鲁棒性，并量化这种训练的泛化性能。

Method: 采用对抗训练方法，使用对抗损失函数训练量子分类器，并理论分析了对抗训练在不同攻击场景下的泛化误差边界。

Result: 理论分析表明，对于经典输入的高维情况，对抗训练的额外样本复杂度可忽略；而对量子态攻击，泛化误差依赖于嵌入的希尔伯特空间维度。数值实验验证了理论结果。

Conclusion: 对抗训练能有效提升量子分类器的鲁棒性，但泛化性能受输入类型和嵌入维度影响，为实际应用提供了理论指导。

Abstract: Quantum classifiers are vulnerable to adversarial attacks that manipulate
their input classical or quantum data. A promising countermeasure is
adversarial training, where quantum classifiers are trained by using an
attack-aware, adversarial loss function. This work establishes novel bounds on
the generalization error of adversarially trained quantum classifiers when
tested in the presence of perturbation-constrained adversaries. The bounds
quantify the excess generalization error incurred to ensure robustness to
adversarial attacks as scaling with the training sample size $m$ as
$1/\sqrt{m}$, while yielding insights into the impact of the quantum embedding.
For quantum binary classifiers employing \textit{rotation embedding}, we find
that, in the presence of adversarial attacks on classical inputs $\mathbf{x}$,
the increase in sample complexity due to adversarial training over conventional
training vanishes in the limit of high dimensional inputs $\mathbf{x}$. In
contrast, when the adversary can directly attack the quantum state
$\rho(\mathbf{x})$ encoding the input $\mathbf{x}$, the excess generalization
error depends on the choice of embedding only through its Hilbert space
dimension. The results are also extended to multi-class classifiers. We
validate our theoretical findings with numerical experiments.

</details>


### [257] [Quantum Autoencoder for Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2504.17548)
*Kilian Tscharke,Maximilian Wendlinger,Afrae Ahouzi,Pallavi Bhardwaj,Kaweh Amoi-Taleghani,Michael Schrödl-Baumann,Pascal Debus*

Main category: quant-ph

TL;DR: 提出了一种基于量子自编码器（QAE）的新型框架，专门用于多变量时间序列（MTS）异常检测（AD），并在企业级SAP系统中验证了其性能与神经网络自编码器相当，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 企业环境中如SAP HANA Cloud系统的高维多变量时间序列异常检测需求迫切，但现有量子自编码器方法仅限于单变量数据，缺乏实用性。

Method: 开发并实验验证了一种量子自编码器框架，适用于多变量时间序列异常检测。

Result: 该框架在SAP系统数据集上表现优异，性能与神经网络自编码器相当，但参数更少。

Conclusion: 量子自编码器是多变量时间序列异常检测的高效可行替代方案，尤其适用于企业级应用。

Abstract: Anomaly Detection (AD) defines the task of identifying observations or events
that deviate from typical - or normal - patterns, a critical capability in IT
security for recognizing incidents such as system misconfigurations, malware
infections, or cyberattacks. In enterprise environments like SAP HANA Cloud
systems, this task often involves monitoring high-dimensional, multivariate
time series (MTS) derived from telemetry and log data. With the advent of
quantum machine learning offering efficient calculations in high-dimensional
latent spaces, many avenues open for dealing with such complex data. One
approach is the Quantum Autoencoder (QAE), an emerging and promising method
with potential for application in both data compression and AD. However, prior
applications of QAEs to time series AD have been restricted to univariate data,
limiting their relevance for real-world enterprise systems. In this work, we
introduce a novel QAE-based framework designed specifically for MTS AD towards
enterprise scale. We theoretically develop and experimentally validate the
architecture, demonstrating that our QAE achieves performance competitive with
neural-network-based autoencoders while requiring fewer trainable parameters.
We evaluate our model on datasets that closely reflect SAP system telemetry and
show that the proposed QAE is a viable and efficient alternative for
semisupervised AD in real-world enterprise settings.

</details>


### [258] [On the Generalization of Adversarially Trained Quantum Classifiers](https://arxiv.org/abs/2504.17690)
*Petros Georgiou,Aaron Mark Thomas,Sharu Theresa Jose,Osvaldo Simeone*

Main category: quant-ph

TL;DR: 该论文研究了对抗训练对量子分类器泛化误差的影响，并量化了其与训练样本量的关系，特别是在高维输入和量子态攻击下的表现。


<details>
  <summary>Details</summary>
Motivation: 量子分类器易受对抗攻击，需要通过对抗训练提高其鲁棒性，而理解这种训练对泛化误差的影响是关键。

Method: 作者通过理论分析量化对抗训练对泛化误差的界限，并引入旋转嵌入等方法，结合数值实验验证理论结果。

Result: 发现对抗训练的泛化误差与样本量成反比，高维输入下对抗训练的额外样本复杂性可忽略，而量子态攻击下的误差与嵌入的希尔伯特空间维数相关。

Conclusion: 对抗训练能有效提升量子分类器的鲁棒性，尤其在特定条件下（如高维输入），其额外成本较低，为实际应用提供了理论支持。

Abstract: Quantum classifiers are vulnerable to adversarial attacks that manipulate
their input classical or quantum data. A promising countermeasure is
adversarial training, where quantum classifiers are trained by using an
attack-aware, adversarial loss function. This work establishes novel bounds on
the generalization error of adversarially trained quantum classifiers when
tested in the presence of perturbation-constrained adversaries. The bounds
quantify the excess generalization error incurred to ensure robustness to
adversarial attacks as scaling with the training sample size $m$ as
$1/\sqrt{m}$, while yielding insights into the impact of the quantum embedding.
For quantum binary classifiers employing \textit{rotation embedding}, we find
that, in the presence of adversarial attacks on classical inputs $\mathbf{x}$,
the increase in sample complexity due to adversarial training over conventional
training vanishes in the limit of high dimensional inputs $\mathbf{x}$. In
contrast, when the adversary can directly attack the quantum state
$\rho(\mathbf{x})$ encoding the input $\mathbf{x}$, the excess generalization
error depends on the choice of embedding only through its Hilbert space
dimension. The results are also extended to multi-class classifiers. We
validate our theoretical findings with numerical experiments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [259] [Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference](https://arxiv.org/abs/2504.17129)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TL;DR: 提出N-PACE算法解决非线性一般和动态博弈中机器人间目标函数未知的问题，利用迭代线性二次逼近，通过显式建模对方学习动态实现快速无偏学习，并实现意图沟通。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设一方为完全信息专家，易导致估计偏差与协调失败，需解决非线性动态博弈中目标函数未知的挑战。

Method: N-PACE算法，通过迭代线性二次逼近非线性博弈，显式建模对方学习动态以推断其目标函数。

Result: 实现无偏快速学习，推断对方未知目标函数，同时支持多智能体系统中的意图沟通。

Conclusion: N-PACE为非线性一般和动态博弈提供高效解决方案，提升任务完成与安全性。

Abstract: Human-robot interactions can be modeled as incomplete-information general-sum
dynamic games since the objective functions of both agents are not explicitly
known to each other. However, solving for equilibrium policies for such games
presents a major challenge, especially if the games involve nonlinear
underlying dynamics. To simplify the problem, existing work often assumes that
one agent is an expert with complete information about its peer, which can lead
to biased estimates and failures in coordination. To address this challenge, we
propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for
general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)
approximation of the nonlinear general-sum game, each agent explicitly models
the learning dynamics of its peer agent while inferring their objective
functions, leading to unbiased fast learning in inferring the unknown objective
function of the peer agent, which is critical for task completion and safety
assurance. Additionally, we demonstrate how N-PACE enables \textbf{intent
communication} in such multi-agent systems by explicitly modeling the peer's
learning dynamics.

</details>


### [260] [PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games](https://arxiv.org/abs/2504.17128)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TL;DR: 论文提出了PACE框架，用于解决不完全信息下的线性二次微分博弈问题，通过建模对手的学习动态来实时推断其成本函数参数，并展示了其稳定性和收敛优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决多智能体控制中由于信息不完全（如不知道对手的成本函数）导致的博弈复杂性增加问题。

Method: 方法为Peer-Aware Cost Estimation (PACE)框架，通过建模对手的学习动态来推断其成本函数参数，并动态调整控制策略。

Result: 理论保证了参数估计的收敛性和系统状态的稳定性，数值研究显示PACE在稳定性和收敛速度上优于假设对手完全信息的方法。

Conclusion: PACE框架为解决不完全信息下的微分博弈提供了一种有效方法，尤其在动态环境下表现优异。

Abstract: In this paper, we address the problem of a two-player linear quadratic
differential game with incomplete information, a scenario commonly encountered
in multi-agent control, human-robot interaction (HRI), and approximation
methods for solving general-sum differential games. While solutions to such
linear differential games are typically obtained through coupled Riccati
equations, the complexity increases when agents have incomplete information,
particularly when neither is aware of the other's cost function. To tackle this
challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework
for learning the cost parameters of the other agent. In PACE, each agent treats
its peer as a learning agent rather than a stationary optimal agent, models
their learning dynamics, and leverages this dynamic to infer the cost function
parameters of the other agent. This approach enables agents to infer each
other's objective function in real time based solely on their previous state
observations and dynamically adapt their control policies. Furthermore, we
provide a theoretical guarantee for the convergence of parameter estimation and
the stability of system states in PACE. Additionally, in our numerical studies,
we demonstrate how modeling the learning dynamics of the other agent benefits
PACE, compared to approaches that approximate the other agent as having
complete information, particularly in terms of stability and convergence speed.

</details>


### [261] [Peer-Aware Cost Estimation in Nonlinear General-Sum Dynamic Games for Mutual Learning and Intent Inference](https://arxiv.org/abs/2504.17129)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TL;DR: 该论文提出了N-PACE算法，用于解决非线性不完全信息动态博弈中的人机交互问题，通过迭代线性近似和显式建模对方学习动态，实现高效无偏的协作学习与意图传递。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设一方为完全信息专家，可能导致估计偏差和协作失败。为解决这一问题，作者提出N-PACE算法，旨在实现更公平、高效的多智能体协作。

Method: 采用N-PACE算法，通过迭代线性二次近似非线性博弈，显式建模对方学习动态以推断其目标函数，实现快速无偏学习。

Result: N-PACE能够高效推断对方未知目标函数，并支持多智能体系统中的意图传递，提升任务完成与安全保障。

Conclusion: N-PACE通过建模对方学习动态解决了非线性动态博弈中的协作问题，为复杂人机交互提供了新思路。

Abstract: Human-robot interactions can be modeled as incomplete-information general-sum
dynamic games since the objective functions of both agents are not explicitly
known to each other. However, solving for equilibrium policies for such games
presents a major challenge, especially if the games involve nonlinear
underlying dynamics. To simplify the problem, existing work often assumes that
one agent is an expert with complete information about its peer, which can lead
to biased estimates and failures in coordination. To address this challenge, we
propose a nonlinear peer-aware cost estimation (N-PACE) algorithm for
general-sum dynamic games. In N-PACE, using iterative linear quadratic (LQ)
approximation of the nonlinear general-sum game, each agent explicitly models
the learning dynamics of its peer agent while inferring their objective
functions, leading to unbiased fast learning in inferring the unknown objective
function of the peer agent, which is critical for task completion and safety
assurance. Additionally, we demonstrate how N-PACE enables \textbf{intent
communication} in such multi-agent systems by explicitly modeling the peer's
learning dynamics.

</details>


### [262] [PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games](https://arxiv.org/abs/2504.17128)
*Seyed Yousef Soltanian,Wenlong Zhang*

Main category: eess.SY

TL;DR: 本文提出了一种基于模型的Peer-Aware Cost Estimation（PACE）框架，用于解决信息不完全的双人线性二次微分博弈问题。PACE通过建模对方的学习动态来推断其成本函数参数，实时调整控制策略，并提供了参数估计收敛性和系统稳定性的理论保证。数值研究表明，PACE在稳定性和收敛速度上优于近似认为对方具备完全信息的方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体控制、人机交互和通用微分博弈的近似求解中，信息不完全的双人线性二次微分博弈是一个常见但复杂的问题。传统的耦合Riccati方程解法在信息不完全时难以适用，尤其是当双方均不了解对方的成本函数时。因此，需要一种新方法来实时推断对方的目标函数并调整策略。

Method: 提出了PACE框架。每个智能体将对方视为学习体而非静态最优体，建模其学习动态，并利用这些动态推断对方的成本函数参数。该方法仅需历史状态观测数据来实现实时推断和策略调整。

Result: PACE在理论上保证了参数估计的收敛性和系统状态的稳定性。数值研究表明，与近似认为对方具备完全信息的方法相比，PACE在稳定性和收敛速度上表现更优。

Conclusion: PACE通过建模对方的学习动态，有效地解决了信息不完全的线性二次微分博弈问题，具有较高的实用性和理论保证，为多智能体博弈中的实时策略调整提供了新思路。

Abstract: In this paper, we address the problem of a two-player linear quadratic
differential game with incomplete information, a scenario commonly encountered
in multi-agent control, human-robot interaction (HRI), and approximation
methods for solving general-sum differential games. While solutions to such
linear differential games are typically obtained through coupled Riccati
equations, the complexity increases when agents have incomplete information,
particularly when neither is aware of the other's cost function. To tackle this
challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework
for learning the cost parameters of the other agent. In PACE, each agent treats
its peer as a learning agent rather than a stationary optimal agent, models
their learning dynamics, and leverages this dynamic to infer the cost function
parameters of the other agent. This approach enables agents to infer each
other's objective function in real time based solely on their previous state
observations and dynamically adapt their control policies. Furthermore, we
provide a theoretical guarantee for the convergence of parameter estimation and
the stability of system states in PACE. Additionally, in our numerical studies,
we demonstrate how modeling the learning dynamics of the other agent benefits
PACE, compared to approaches that approximate the other agent as having
complete information, particularly in terms of stability and convergence speed.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [263] [Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste](https://arxiv.org/abs/2504.17539)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.CR

TL;DR: 论文提出了一种名为PoUI的新型共识机制，结合AI任务与区块链，兼顾安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 为解决现有区块链共识机制（如PoW和PoS）在资源消耗和中心化风险上的不足，提出更高效且实用的替代方案。

Method: 设计PoUI机制，让参与者通过完成AI任务（如语言处理或图像分析）获得代币并用于网络验证，同时通过智能合约协调节点协作。

Result: PoUI在保持网络安全性的同时，提供了实际应用的效益，减少了资源浪费。

Conclusion: PoUI是一种有潜力的混合共识机制，平衡了区块链的安全性、效率和实用性，尤其适合AI与区块链结合的场景。

Abstract: Blockchain technology enables secure, transparent data management in
decentralized systems, supporting applications from cryptocurrencies like
Bitcoin to tokenizing real-world assets like property. Its scalability and
sustainability hinge on consensus mechanisms balancing security and efficiency.
Proof of Work (PoW), used by Bitcoin, ensures security through energy-intensive
computations but demands significant resources. Proof of Stake (PoS), as in
Ethereum post-Merge, selects validators based on staked cryptocurrency,
offering energy efficiency but risking centralization from wealth
concentration. With AI models straining computational resources, we propose
Proof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,
workers perform AI tasks like language processing or image analysis to earn
coins, which are staked to secure the network, blending security with practical
utility. Decentralized nodes--job posters, market coordinators, workers, and
validators --collaborate via smart contracts to manage tasks and rewards.

</details>


### [264] [Proof of Useful Intelligence (PoUI): Blockchain Consensus Beyond Energy Waste](https://arxiv.org/abs/2504.17539)
*Zan-Kai Chong,Hiroyuki Ohsaki,Bryan Ng*

Main category: cs.CR

TL;DR: PoUI是一种混合共识机制，结合AI任务执行与加密货币质押，兼顾网络安全与实际应用。


<details>
  <summary>Details</summary>
Motivation: 区块链的共识机制在安全性和效率间需要平衡，PoW能耗高，PoS可能导致中心化，因此提出PoUI以解决这些问题。

Method: 通过去中心化节点协作，PoUI让工人执行AI任务获取代币并质押以保障网络，智能合约管理任务与奖励。

Result: PoUI在保障网络安全的同时，为AI任务提供实用价值，兼顾资源利用与去中心化需求。

Conclusion: PoUI为区块链共识机制提供了创新解决方案，平衡了安全、效率与实际应用。

Abstract: Blockchain technology enables secure, transparent data management in
decentralized systems, supporting applications from cryptocurrencies like
Bitcoin to tokenizing real-world assets like property. Its scalability and
sustainability hinge on consensus mechanisms balancing security and efficiency.
Proof of Work (PoW), used by Bitcoin, ensures security through energy-intensive
computations but demands significant resources. Proof of Stake (PoS), as in
Ethereum post-Merge, selects validators based on staked cryptocurrency,
offering energy efficiency but risking centralization from wealth
concentration. With AI models straining computational resources, we propose
Proof of Useful Intelligence (PoUI), a hybrid consensus mechanism. In PoUI,
workers perform AI tasks like language processing or image analysis to earn
coins, which are staked to secure the network, blending security with practical
utility. Decentralized nodes--job posters, market coordinators, workers, and
validators --collaborate via smart contracts to manage tasks and rewards.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [265] [Robo-Troj: Attacking LLM-based Task Planners](https://arxiv.org/abs/2504.17070)
*Mohaiminul Al Nahian,Zainab Altaweel,David Reitano,Sabbir Ahmed,Saumitra Lohokare,Shiqi Zhang,Adnan Siraj Rakin*

Main category: cs.RO

TL;DR: 该论文提出了Robo-Troj，首个针对基于大语言模型（LLM）任务规划器的多触发器后门攻击方法，通过优化触发器选择以增强攻击效果，揭示了LLM任务规划器的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLM任务规划系统的安全漏洞，尤其是多触发器后门攻击的可行性，以促进机器人系统安全的发展。

Method: 开发了Robo-Troj，一种多触发器后门攻击方法，通过训练适应不同机器人应用场景，并使用优化方法选择最有效的触发词。

Result: 论文展示了Robo-Troj攻击的有效性，成功通过特定触发词（如“herical”）激活恶意行为（如厨房机器人切手）。

Conclusion: 研究揭示了LLM任务规划器的安全弱点，强调需加强安全性设计以防范类似攻击。

Abstract: Robots need task planning methods to achieve goals that require more than
individual actions. Recently, large language models (LLMs) have demonstrated
impressive performance in task planning. LLMs can generate a step-by-step
solution using a description of actions and the goal. Despite the successes in
LLM-based task planning, there is limited research studying the security
aspects of those systems. In this paper, we develop Robo-Troj, the first
multi-trigger backdoor attack for LLM-based task planners, which is the main
contribution of this work. As a multi-trigger attack, Robo-Troj is trained to
accommodate the diversity of robot application domains. For instance, one can
use unique trigger words, e.g., "herical", to activate a specific malicious
behavior, e.g., cutting hand on a kitchen robot. In addition, we develop an
optimization method for selecting the trigger words that are most effective.
Through demonstrating the vulnerability of LLM-based planners, we aim to
promote the development of secured robot systems.

</details>


### [266] [Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation](https://arxiv.org/abs/2504.17424)
*Tomoki Mizuno,Kazuya Yabashi,Tsuyoshi Tasaki*

Main category: cs.RO

TL;DR: 论文提出了一种同时估计姿态和下一视角（NV）的神经网络方法，用于提升零售场景中简单形状产品的姿态估计成功率，比传统数学模型方法高出7.4%，并验证了其84.2%的产品展示成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于神经网络的RGBD相机姿态估计方法在缺乏纹理和形状特征时精度下降，而数学模型方法难以有效估计简单形状物体的下一视角（NV）。因此，研究聚焦于姿态估计与NV估计的关联性，希望通过同时优化两者提升性能。

Method: 开发了一种新的神经网络，能够同时进行姿态估计和下一视角（NV）估计，利用姿态估计的准确性提升NV估计的精度。

Result: 实验表明，该方法将姿态估计成功率提升至77.3%（比数学模型高7.4%），并实现了84.2%的产品展示成功率。

Conclusion: 联合优化姿态估计与NV估计的神经网络方法在简单形状物体任务中显著优于传统模型，验证了其在实际机器人应用中的有效性。

Abstract: We have developed a new method to estimate a Next Viewpoint (NV) which is
effective for pose estimation of simple-shaped products for product display
robots in retail stores. Pose estimation methods using Neural Networks (NN)
based on an RGBD camera are highly accurate, but their accuracy significantly
decreases when the camera acquires few texture and shape features at a current
view point. However, it is difficult for previous mathematical model-based
methods to estimate effective NV which is because the simple shaped objects
have few shape features. Therefore, we focus on the relationship between the
pose estimation and NV estimation. When the pose estimation is more accurate,
the NV estimation is more accurate. Therefore, we develop a new pose estimation
NN that estimates NV simultaneously. Experimental results showed that our NV
estimation realized a pose estimation success rate 77.3\%, which was 7.4pt
higher than the mathematical model-based NV calculation did. Moreover, we
verified that the robot using our method displayed 84.2\% of products.

</details>


### [267] [Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control](https://arxiv.org/abs/2504.17771)
*Haochen Wang,Zhiwei Shi,Chengxi Zhu,Yafei Qiao,Cheng Zhang,Fan Yang,Pengjie Ren,Lan Lu,Dong Xuan*

Main category: cs.RO

TL;DR: "HAMLET"提出了一种混合控制框架，结合模型与学习策略（模仿+强化学习），用于敏捷机器人（如羽毛球机器人）的高效安全控制，成功率达90%以上。


<details>
  <summary>Details</summary>
Motivation: 现有学习型策略（如模仿或强化学习）虽能处理复杂任务，但缺乏与模型方法的结合，导致训练复杂度高且安全性不足。此研究旨在融合二者，提升敏捷机器人控制的稳定性和成功率。

Method: 1. 模型底盘运动提供基座；2. 基于物理信息的“模仿+强化”训练框架（IL+RL），利用特权信息指导策略学习；3. 在模仿阶段训练评价模型以减少策略切换时的性能下降。

Result: 自研羽毛球机器人实验显示：对抗发球机成功率94.5%，对抗人类选手成功率90.7%。框架可泛化至其他敏捷任务（如抓取、乒乓球）。

Conclusion: HAMLET通过混合方法显著提升敏捷机器人控制的性能与安全性，验证了模型与学习策略协同的有效性，具备广泛的应用潜力。

Abstract: Learning-based methods, such as imitation learning (IL) and reinforcement
learning (RL), can produce excel control policies over challenging agile robot
tasks, such as sports robot. However, no existing work has harmonized
learning-based policy with model-based methods to reduce training complexity
and ensure the safety and stability for agile badminton robot control. In this
paper, we introduce \ourmethod, a novel hybrid control system for agile
badminton robots. Specifically, we propose a model-based strategy for chassis
locomotion which provides a base for arm policy. We introduce a
physics-informed ``IL+RL'' training framework for learning-based arm policy. In
this train framework, a model-based strategy with privileged information is
used to guide arm policy training during both IL and RL phases. In addition, we
train the critic model during IL phase to alleviate the performance drop issue
when transitioning from IL to RL. We present results on our self-engineered
badminton robot, achieving 94.5% success rate against the serving machine and
90.7% success rate against human players. Our system can be easily generalized
to other agile mobile manipulation tasks such as agile catching and table
tennis. Our project website: https://dreamstarring.github.io/HAMLET/.

</details>


### [268] [Robo-Troj: Attacking LLM-based Task Planners](https://arxiv.org/abs/2504.17070)
*Mohaiminul Al Nahian,Zainab Altaweel,David Reitano,Sabbir Ahmed,Saumitra Lohokare,Shiqi Zhang,Adnan Siraj Rakin*

Main category: cs.RO

TL;DR: 本文介绍了Robo-Troj，首个针对基于LLM的任务规划系统的多触发后门攻击，并开发了优化触发器选择的方法，以揭示其安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在任务规划中表现突出，但其安全性研究较少。本文旨在揭示LLM规划系统的漏洞，推动安全机器人系统的发展。

Method: 开发了多触发后门攻击Robo-Troj，通过独特触发器激活恶意行为，并优化触发器选择以提高攻击效果。

Result: 展示了LLM规划系统的脆弱性，Robo-Troj能有效利用触发器操控机器人行为。

Conclusion: 本文强调了LLM规划系统的安全风险，呼吁加强相关防御措施的研究。

Abstract: Robots need task planning methods to achieve goals that require more than
individual actions. Recently, large language models (LLMs) have demonstrated
impressive performance in task planning. LLMs can generate a step-by-step
solution using a description of actions and the goal. Despite the successes in
LLM-based task planning, there is limited research studying the security
aspects of those systems. In this paper, we develop Robo-Troj, the first
multi-trigger backdoor attack for LLM-based task planners, which is the main
contribution of this work. As a multi-trigger attack, Robo-Troj is trained to
accommodate the diversity of robot application domains. For instance, one can
use unique trigger words, e.g., "herical", to activate a specific malicious
behavior, e.g., cutting hand on a kitchen robot. In addition, we develop an
optimization method for selecting the trigger words that are most effective.
Through demonstrating the vulnerability of LLM-based planners, we aim to
promote the development of secured robot systems.

</details>


### [269] [Object Pose Estimation by Camera Arm Control Based on the Next Viewpoint Estimation](https://arxiv.org/abs/2504.17424)
*Tomoki Mizuno,Kazuya Yabashi,Tsuyoshi Tasaki*

Main category: cs.RO

TL;DR: 论文提出了一种新的Next Viewpoint (NV)估计方法，用于零售展示机器人对简单形状产品的姿态估计，通过结合姿态估计与NV估计的神经网络，显著提高了估计精度与成功率。


<details>
  <summary>Details</summary>
Motivation: 基于RGBD相机的神经网络姿态估计方法在纹理和形状特征少时精度下降，而传统数学模型方法对简单形状物体的NV估计效果差，因此研究如何通过结合姿态与NV估计提升性能。

Method: 开发了一种新的姿态估计神经网络，同时估计姿态和Next Viewpoint (NV)，利用两者之间的关联提升估计精度。

Result: 实验表明，该方法将姿态估计成功率提升至77.3%，比数学模型方法高7.4%；机器人使用该方法成功展示了84.2%的产品。

Conclusion: 结合姿态与NV估计的神经网络方法有效解决了简单形状物体姿态估计的难点，显著提升了机器人展示的成功率。

Abstract: We have developed a new method to estimate a Next Viewpoint (NV) which is
effective for pose estimation of simple-shaped products for product display
robots in retail stores. Pose estimation methods using Neural Networks (NN)
based on an RGBD camera are highly accurate, but their accuracy significantly
decreases when the camera acquires few texture and shape features at a current
view point. However, it is difficult for previous mathematical model-based
methods to estimate effective NV which is because the simple shaped objects
have few shape features. Therefore, we focus on the relationship between the
pose estimation and NV estimation. When the pose estimation is more accurate,
the NV estimation is more accurate. Therefore, we develop a new pose estimation
NN that estimates NV simultaneously. Experimental results showed that our NV
estimation realized a pose estimation success rate 77.3\%, which was 7.4pt
higher than the mathematical model-based NV calculation did. Moreover, we
verified that the robot using our method displayed 84.2\% of products.

</details>


### [270] [Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control](https://arxiv.org/abs/2504.17771)
*Haochen Wang,Zhiwei Shi,Chengxi Zhu,Yafei Qiao,Cheng Zhang,Fan Yang,Pengjie Ren,Lan Lu,Dong Xuan*

Main category: cs.RO

TL;DR: 论文提出了一种混合控制系统HAMLET，结合模型和学习策略，用于敏捷羽毛球机器人控制，训练简化且保证安全，成功率高达90%以上。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的控制策略（如模仿学习和强化学习）在敏捷机器人任务中表现优秀，但缺乏与模型方法的结合，难以兼顾训练简化与安全稳定性。

Method: 使用模型策略控制底盘运动，基于物理信息的‘IL+RL’框架训练手臂策略，并在IL阶段训练评论家模型以减少过渡性能下降。

Result: 在自研羽毛球机器人上实现94.5%对发球机成功率和90.7%对人类玩家成功率，可推广至其他敏捷任务。

Conclusion: HAMLET系统有效结合模型与学习策略，显著提升机器人任务成功率与泛化能力。

Abstract: Learning-based methods, such as imitation learning (IL) and reinforcement
learning (RL), can produce excel control policies over challenging agile robot
tasks, such as sports robot. However, no existing work has harmonized
learning-based policy with model-based methods to reduce training complexity
and ensure the safety and stability for agile badminton robot control. In this
paper, we introduce \ourmethod, a novel hybrid control system for agile
badminton robots. Specifically, we propose a model-based strategy for chassis
locomotion which provides a base for arm policy. We introduce a
physics-informed ``IL+RL'' training framework for learning-based arm policy. In
this train framework, a model-based strategy with privileged information is
used to guide arm policy training during both IL and RL phases. In addition, we
train the critic model during IL phase to alleviate the performance drop issue
when transitioning from IL to RL. We present results on our self-engineered
badminton robot, achieving 94.5% success rate against the serving machine and
90.7% success rate against human players. Our system can be easily generalized
to other agile mobile manipulation tasks such as agile catching and table
tennis. Our project website: https://dreamstarring.github.io/HAMLET/.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [271] [Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models](https://arxiv.org/abs/2504.17077)
*Dongjin Seo,Soobin Um,Sangbin Lee,Jong Chul Ye,Haejun Chung*

Main category: physics.optics

TL;DR: AdjointDiffusion是一种物理引导的框架，将伴随敏感度梯度集成到扩散模型采样过程中，用于高效、可制造的自由形式光子器件设计。


<details>
  <summary>Details</summary>
Motivation: 传统逆向设计方法（人为直觉、全局优化或伴随梯度法）需要复杂的二值化和滤波步骤，而深度学习方法需要大量模拟（10^5至10^6）。本文旨在克服这些限制。

Method: 训练扩散网络于合成、制造感知的二进制掩码数据集，并在推断过程中注入伴随梯度，引导生成高FoM的解决方案。

Result: 在弯曲波导和CMOS图像传感器颜色路由器设计中，AdjointDiffusion效率与可制造性优于非线性优化器（如MMA和SLSQP），模拟次数远少于纯深度学习方法（约2x10^2 vs 10^5-10^6）。

Conclusion: AdjointDiffusion提供了一种简化、高效且制造感知的下一代光子器件设计流程，消除了复杂的二值化调度并减少了模拟开销。

Abstract: Designing free-form photonic devices is fundamentally challenging due to the
vast number of possible geometries and the complex requirements of fabrication
constraints. Traditional inverse-design approaches--whether driven by human
intuition, global optimization, or adjoint-based gradient methods--often
involve intricate binarization and filtering steps, while recent deep learning
strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To
overcome these limitations, we present AdjointDiffusion, a physics-guided
framework that integrates adjoint sensitivity gradients into the sampling
process of diffusion models. AdjointDiffusion begins by training a diffusion
network on a synthetic, fabrication-aware dataset of binary masks. During
inference, we compute the adjoint gradient of a candidate structure and inject
this physics-based guidance at each denoising step, steering the generative
process toward high figure-of-merit (FoM) solutions without additional
post-processing. We demonstrate our method on two canonical photonic design
problems--a bent waveguide and a CMOS image sensor color router--and show that
our method consistently outperforms state-of-the-art nonlinear optimizers (such
as MMA and SLSQP) in both efficiency and manufacturability, while using orders
of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning
approaches (approximately 10^5 to 10^6). By eliminating complex binarization
schedules and minimizing simulation overhead, AdjointDiffusion offers a
streamlined, simulation-efficient, and fabrication-aware pipeline for
next-generation photonic device design. Our open-source implementation is
available at https://github.com/dongjin-seo2020/AdjointDiffusion.

</details>


### [272] [Physics-guided and fabrication-aware inverse design of photonic devices using diffusion models](https://arxiv.org/abs/2504.17077)
*Dongjin Seo,Soobin Um,Sangbin Lee,Jong Chul Ye,Haejun Chung*

Main category: physics.optics

TL;DR: AdjointDiffusion 是一种将伴随敏感性梯度整合到扩散模型中的物理引导框架，显著减少了设计自由形态光子器件所需的模拟次数。


<details>
  <summary>Details</summary>
Motivation: 解决自由形态光子器件设计中的几何复杂性、制造约束以及传统方法（如全局优化、伴随梯度法）和深度学习策略的高计算成本问题。

Method: 训练扩散网络于合成数据集，在推理阶段注入基于物理的伴随梯度指导，无需后处理即可优化器件的性能指标。

Result: 在弯曲波导和 CMOS 图像传感器颜色路由器等设计问题上，性能优于非线性优化器（如 MMA 和 SLSQP），且模拟次数减少至约 200 次。

Conclusion: AdjointDiffusion 提供了一个高效、制造友好的光子器件设计流程，开源实现已发布。

Abstract: Designing free-form photonic devices is fundamentally challenging due to the
vast number of possible geometries and the complex requirements of fabrication
constraints. Traditional inverse-design approaches--whether driven by human
intuition, global optimization, or adjoint-based gradient methods--often
involve intricate binarization and filtering steps, while recent deep learning
strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To
overcome these limitations, we present AdjointDiffusion, a physics-guided
framework that integrates adjoint sensitivity gradients into the sampling
process of diffusion models. AdjointDiffusion begins by training a diffusion
network on a synthetic, fabrication-aware dataset of binary masks. During
inference, we compute the adjoint gradient of a candidate structure and inject
this physics-based guidance at each denoising step, steering the generative
process toward high figure-of-merit (FoM) solutions without additional
post-processing. We demonstrate our method on two canonical photonic design
problems--a bent waveguide and a CMOS image sensor color router--and show that
our method consistently outperforms state-of-the-art nonlinear optimizers (such
as MMA and SLSQP) in both efficiency and manufacturability, while using orders
of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning
approaches (approximately 10^5 to 10^6). By eliminating complex binarization
schedules and minimizing simulation overhead, AdjointDiffusion offers a
streamlined, simulation-efficient, and fabrication-aware pipeline for
next-generation photonic device design. Our open-source implementation is
available at https://github.com/dongjin-seo2020/AdjointDiffusion.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [273] [High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services](https://arxiv.org/abs/2504.17203)
*Shivasankari Kannan,Yeounoh Chung,Amita Gondi,Tristan Swadell,Fatma Ozcan*

Main category: cs.DB

TL;DR: 该论文提出了一种利用大型语言模型（LLM）和预处理后处理步骤生成高保真测试数据的方法，解决了传统方法在复杂SQL架构中生成语义一致的测试数据的不足。


<details>
  <summary>Details</summary>
Motivation: 工业环境中缺乏高保真的测试数据，传统方法无法处理复杂的数据结构和语义关系，限制了SQL代码生成服务的测试覆盖率和效果。

Method: 利用大型语言模型（如Gemini）并结合策略性的预处理和后处理步骤，生成符合复杂SQL架构约束且语义一致的测试数据。

Result: 该方法成功生成了能够支持复杂SQL查询（如连接、聚合和嵌套子查询）的高保真测试数据，提升了SQL代码生成服务的测试效果。

Conclusion: LLM为基础的测试数据生成方法在工业环境中具有实际应用价值，尤其在缺乏生产数据的情况下，能够有效支持SQL代码生成服务的测试需求。

Abstract: The demand for high-fidelity test data is paramount in industrial settings
where access to production data is largely restricted. Traditional data
generation methods often fall short, struggling with low-fidelity and the
ability to model complex data structures and semantic relationships that are
critical for testing complex SQL code generation services like Natural Language
to SQL (NL2SQL). In this paper, we address the critical need for generating
syntactically correct and semantically ``meaningful'' mock data for complex
schema that includes columns with nested structures that we frequently
encounter in Google SQL code generation workloads. We highlight the limitations
of existing approaches used in production, particularly their inability to
handle large and complex schema, as well as the lack of semantically coherent
test data that lead to limited test coverage. We demonstrate that by leveraging
Large Language Models (LLMs) and incorporating strategic pre- and
post-processing steps, we can generate realistic high-fidelity test data that
adheres to complex structural constraints and maintains semantic integrity to
the test targets (SQL queries/functions). This approach supports comprehensive
testing of complex SQL queries involving joins, aggregations, and even deeply
nested subqueries, ensuring robust evaluation of SQL code generation services,
like NL2SQL and SQL Code Assistant services. Our results demonstrate the
practical utility of an out-of-the-box LLM (\textit{gemini}) based test data
generation for industrial SQL code generation services where generating
realistic test data is essential due to the frequent unavailability of
production datasets.

</details>


### [274] [High-Fidelity And Complex Test Data Generation For Real-World SQL Code Generation Services](https://arxiv.org/abs/2504.17203)
*Shivasankari Kannan,Yeounoh Chung,Amita Gondi,Tristan Swadell,Fatma Ozcan*

Main category: cs.DB

TL;DR: 论文提出了一种利用大型语言模型（LLMs）生成高保真测试数据的方法，解决了传统方法在复杂SQL代码生成服务（如NL2SQL）中数据生成低保真且语义不连贯的问题。


<details>
  <summary>Details</summary>
Motivation: 工业环境中对高保真测试数据的需求很高，但传统方法难以生成符合复杂结构和语义关系的测试数据，限制了测试覆盖率。

Method: 通过结合LLMs（如Gemini）及前后处理步骤，生成符合复杂结构约束且语义连贯的模拟数据。

Result: 该方法能够支持涉及连接、聚合和嵌套子查询的复杂SQL查询测试，显著提升了SQL代码生成服务的测试效果。

Conclusion: 基于LLMs的测试数据生成方法为工业级SQL代码生成服务提供了高效且实用的解决方案。

Abstract: The demand for high-fidelity test data is paramount in industrial settings
where access to production data is largely restricted. Traditional data
generation methods often fall short, struggling with low-fidelity and the
ability to model complex data structures and semantic relationships that are
critical for testing complex SQL code generation services like Natural Language
to SQL (NL2SQL). In this paper, we address the critical need for generating
syntactically correct and semantically ``meaningful'' mock data for complex
schema that includes columns with nested structures that we frequently
encounter in Google SQL code generation workloads. We highlight the limitations
of existing approaches used in production, particularly their inability to
handle large and complex schema, as well as the lack of semantically coherent
test data that lead to limited test coverage. We demonstrate that by leveraging
Large Language Models (LLMs) and incorporating strategic pre- and
post-processing steps, we can generate realistic high-fidelity test data that
adheres to complex structural constraints and maintains semantic integrity to
the test targets (SQL queries/functions). This approach supports comprehensive
testing of complex SQL queries involving joins, aggregations, and even deeply
nested subqueries, ensuring robust evaluation of SQL code generation services,
like NL2SQL and SQL Code Assistant services. Our results demonstrate the
practical utility of an out-of-the-box LLM (\textit{gemini}) based test data
generation for industrial SQL code generation services where generating
realistic test data is essential due to the frequent unavailability of
production datasets.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [275] [Towards a HIPAA Compliant Agentic AI System in Healthcare](https://arxiv.org/abs/2504.17669)
*Subash Neupane,Shaswata Mitra,Sudip Mittal,Shahram Rahimi*

Main category: cs.MA

TL;DR: 本文提出了一个符合HIPAA标准的Agentic AI框架，用于医疗工作流程中的敏感数据处理，通过动态策略执行确保合规性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在临床工作流程中的应用增加，如何在处理受保护健康信息（PHI）时确保符合HIPAA等法规成为关键挑战。

Method: 框架整合了基于属性的访问控制（ABAC）、结合正则表达式和BERT模型的混合PHI清理管道，以及不可变的审计跟踪。

Result: 该框架能有效管理PHI，减少信息泄露风险，并通过审计跟踪支持合规性验证。

Conclusion: 该工作为Agentic AI在医疗领域的合规应用提供了可行方案，但仍需进一步验证和完善。

Abstract: Agentic AI systems powered by Large Language Models (LLMs) as their
foundational reasoning engine, are transforming clinical workflows such as
medical report generation and clinical summarization by autonomously analyzing
sensitive healthcare data and executing decisions with minimal human oversight.
However, their adoption demands strict compliance with regulatory frameworks
such as Health Insurance Portability and Accountability Act (HIPAA),
particularly when handling Protected Health Information (PHI). This
work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that
enforces regulatory compliance through dynamic, context-aware policy
enforcement. Our framework integrates three core mechanisms: (1)
Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid
PHI sanitization pipeline combining regex patterns and BERT-based model to
minimize leakage, and (3) immutable audit trails for compliance verification.

</details>


### [276] [Towards a HIPAA Compliant Agentic AI System in Healthcare](https://arxiv.org/abs/2504.17669)
*Subash Neupane,Shaswata Mitra,Sudip Mittal,Shahram Rahimi*

Main category: cs.MA

TL;DR: 论文介绍了一种符合HIPAA标准的代理型AI框架，通过动态、情境感知的策略执行确保医疗数据处理合规。


<details>
  <summary>Details</summary>
Motivation: 为解决代理型AI系统在处理敏感医疗数据时的合规问题，特别是HIPAA对健康信息保护的要求。

Method: 框架整合了三种机制：ABAC实现细粒度访问控制、结合正则表达式和BERT模型的混合PHI脱敏流程、以及不可变审计跟踪。

Result: 框架能够有效治理PHI数据，减少信息泄露，并支持合规验证。

Conclusion: 该框架为医疗AI系统提供了可行的HIPAA合规解决方案，展现了代理型AI在临床工作流中的潜力。

Abstract: Agentic AI systems powered by Large Language Models (LLMs) as their
foundational reasoning engine, are transforming clinical workflows such as
medical report generation and clinical summarization by autonomously analyzing
sensitive healthcare data and executing decisions with minimal human oversight.
However, their adoption demands strict compliance with regulatory frameworks
such as Health Insurance Portability and Accountability Act (HIPAA),
particularly when handling Protected Health Information (PHI). This
work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that
enforces regulatory compliance through dynamic, context-aware policy
enforcement. Our framework integrates three core mechanisms: (1)
Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid
PHI sanitization pipeline combining regex patterns and BERT-based model to
minimize leakage, and (3) immutable audit trails for compliance verification.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [277] [Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space](https://arxiv.org/abs/2504.17236)
*Xiqiang Qu,Jun Chen,Lei Yu,Xiangyu Xu*

Main category: cs.IT

TL;DR: 该论文建立了在有限共同随机性下，基于平方误差失真和平方Wasserstein-2感知度量的失真率-感知权衡的单字母表征，并证明了对高斯源可显式评估。同时澄清了多种通用表示概念。


<details>
  <summary>Details</summary>
Motivation: 研究失真率-感知权衡问题，尤其是在有限共同随机性条件下，为信息理论和数据压缩领域提供理论基础。

Method: 采用单字母表征方法，结合平方误差失真和平方Wasserstein-2感知度量，针对高斯源进行显式评估。

Result: 成功建立了失真率-感知权衡的单字母表征，并证明其对高斯源的可评估性。

Conclusion: 该研究为失真率-感知权衡问题提供了理论框架，尤其在有限共同随机性和高斯源条件下具有重要意义，同时澄清了通用表示概念。

Abstract: We establish a single-letter characterization of the fundamental
distortion-rate-perception tradeoff with limited common randomness under the
squared error distortion measure and the squared Wasserstein-2 perception
measure. Moreover, it is shown that this single-letter characterization can be
explicitly evaluated for the Gaussian source. Various notions of universal
representation are also clarified.

</details>


### [278] [Rate-Distortion-Perception Theory for the Quadratic Wasserstein Space](https://arxiv.org/abs/2504.17236)
*Xiqiang Qu,Jun Chen,Lei Yu,Xiangyu Xu*

Main category: cs.IT

TL;DR: 该论文提出了在平方误差失真和Wasserstein-2感知度量下的失真-率-感知基本折衷的单字母表征，并证明了对高斯源可以显式评估，同时澄清了通用表示的多种概念。


<details>
  <summary>Details</summary>
Motivation: 研究在失真率与感知度量（平方误差和Wasserstein-2）之间的基本折衷关系，特别是在有限共同随机性条件下，为源编码理论提供新的见解。

Method: 通过单字母表征来建立失真-率-感知的折衷关系，并针对高斯源进行显式评估，同时分析通用表示的不同概念。

Result: 成功建立了单字母表征的失真-率-感知折衷关系，并证明其在高斯源下的可评估性，同时对通用表示进行了分类和澄清。

Conclusion: 研究为源编码理论提供了新的理论框架和技术工具，特别是在失真和感知度量之间的折衷关系上，同时对通用表示的分类也为后续研究提供了基础。

Abstract: We establish a single-letter characterization of the fundamental
distortion-rate-perception tradeoff with limited common randomness under the
squared error distortion measure and the squared Wasserstein-2 perception
measure. Moreover, it is shown that this single-letter characterization can be
explicitly evaluated for the Gaussian source. Various notions of universal
representation are also clarified.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [279] [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)
*Zhenhailong Wang,Senthil Purushwalkam,Caiming Xiong,Silvio Savarese,Heng Ji,Ran Xu*

Main category: cs.CV

TL;DR: DyMU是一个无需训练的高效框架，通过动态合并视觉令牌（DToMe）和虚拟令牌解合并（VTU）减少视觉语言模型的计算负担，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型中固定长度输出导致的计算效率低下问题，同时避免额外微调的需求。

Method: 使用Dynamic Token Merging（DToMe）根据图像复杂度动态合并相似令牌，以及Virtual Token Unmerging（VTU）模拟完整序列的注意力动态。

Result: DyMU能减少视觉令牌数量32%-85%，性能与完整模型相当，适用于多种VLM架构。

Conclusion: DyMU是一种无需训练的动态优化方法，显著提升计算效率且不影响性能。

Abstract: We present DyMU, an efficient, training-free framework that dynamically
reduces the computational burden of vision-language models (VLMs) while
maintaining high task performance. Our approach comprises two key components.
First, Dynamic Token Merging (DToMe) reduces the number of visual token
embeddings by merging similar tokens based on image complexity, addressing the
inherent inefficiency of fixed-length outputs in vision transformers. Second,
Virtual Token Unmerging (VTU) simulates the expected token sequence for large
language models (LLMs) by efficiently reconstructing the attention dynamics of
a full sequence, thus preserving the downstream performance without additional
fine-tuning. Unlike previous approaches, our method dynamically adapts token
compression to the content of the image and operates completely training-free,
making it readily applicable to most state-of-the-art VLM architectures.
Extensive experiments on image and video understanding tasks demonstrate that
DyMU can reduce the average visual token count by 32%-85% while achieving
comparable performance to full-length models across diverse VLM architectures,
including the recently popularized AnyRes-based visual encoders. Furthermore,
through qualitative analyses, we demonstrate that DToMe effectively adapts
token reduction based on image complexity and, unlike existing systems,
provides users more control over computational costs. Project page:
https://mikewangwzhl.github.io/dymu/.

</details>


### [280] [Distilling semantically aware orders for autoregressive image generation](https://arxiv.org/abs/2504.17069)
*Rishav Pramanik,Antoine Poupon,Juan A. Rodriguez,Masih Aminbeidokhti,David Vazquez,Christopher Pal,Zhaozheng Yin,Marco Pedersoli*

Main category: cs.CV

TL;DR: 研究了基于自回归的图像生成方法，指出传统的光栅扫描顺序不理想，并提出一种任意顺序生成+微调的方法，提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成采用固定的光栅扫描顺序（左上到右下），忽略了图像内容的因果关系，导致生成效果不佳。本文旨在通过动态生成顺序改进这一问题。

Method: 1. 训练模型以任意顺序生成图像块；2. 推断生成顺序并微调模型，优化生成质量。

Result: 在两个数据集上验证，新方法生成的图像质量优于传统光栅扫描顺序，且无需额外标注或增加训练成本。

Conclusion: 动态生成顺序能更好地建模图像内容的因果关系，显著提升自回归图像生成的质量。

Abstract: Autoregressive patch-based image generation has recently shown competitive
results in terms of image quality and scalability. It can also be easily
integrated and scaled within Vision-Language models. Nevertheless,
autoregressive models require a defined order for patch generation. While a
natural order based on the dictation of the words makes sense for text
generation, there is no inherent generation order that exists for image
generation. Traditionally, a raster-scan order (from top-left to bottom-right)
guides autoregressive image generation models. In this paper, we argue that
this order is suboptimal, as it fails to respect the causality of the image
content: for instance, when conditioned on a visual description of a sunset, an
autoregressive model may generate clouds before the sun, even though the color
of clouds should depend on the color of the sun and not the inverse. In this
work, we show that first by training a model to generate patches in
any-given-order, we can infer both the content and the location (order) of each
patch during generation. Secondly, we use these extracted orders to finetune
the any-given-order model to produce better-quality images. Through our
experiments, we show on two datasets that this new generation method produces
better images than the traditional raster-scan approach, with similar training
costs and no extra annotations.

</details>


### [281] [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
*Ling You,Wenxuan Huang,Xinni Xie,Xiangyi Wei,Bangyan Li,Shaohui Lin,Yang Li,Changbo Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为TimeSoccer的端到端多模态大语言模型，用于足球比赛的单锚点密集视频字幕生成，解决了现有方法依赖时间先验或两步范式的问题，并在长视频理解和全局上下文建模上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的足球视频字幕生成方法要么依赖时间先验无法端到端处理，要么采用复杂的两步范式且缺乏全局上下文建模，导致性能不佳。因此，需要一种能直接处理长视频并生成高质量字幕的方法。

Method: 提出TimeSoccer模型，通过MoFA-Select模块自适应选择代表性帧，结合端到端训练联合预测时间戳和生成字幕，实现长视频全局建模。

Result: TimeSoccer在单锚点密集视频字幕任务上达到最先进性能，生成的字幕时间对齐准确且语义相关性强。

Conclusion: TimeSoccer为足球长视频字幕生成提供了一种高效的端到端解决方案，显著提升了性能和应用潜力。

Abstract: Soccer is a globally popular sporting event, typically characterized by long
matches and distinctive highlight moments. Recent advances in Multimodal Large
Language Models (MLLMs) offer promising capabilities in temporal grounding and
video understanding, soccer commentary generation often requires precise
temporal localization and semantically rich descriptions over long-form video.
However, existing soccer MLLMs often rely on the temporal a priori for caption
generation, so they cannot process the soccer video end-to-end. While some
traditional approaches follow a two-step paradigm that is complex and fails to
capture the global context to achieve suboptimal performance. To solve the
above issues, we present TimeSoccer, the first end-to-end soccer MLLM for
Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
TimeSoccer jointly predicts timestamps and generates captions in a single pass,
enabling global context modeling across 45-minute matches. To support long
video understanding of soccer matches, we introduce MoFA-Select, a
training-free, motion-aware frame compression module that adaptively selects
representative frames via a coarse-to-fine strategy, and incorporates
complementary training paradigms to strengthen the model's ability to handle
long temporal sequences. Extensive experiments demonstrate that our TimeSoccer
achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end
form, generating high-quality commentary with accurate temporal alignment and
strong semantic relevance.

</details>


### [282] [A Comprehensive Review on RNA Subcellular Localization Prediction](https://arxiv.org/abs/2504.17162)
*Cece Zhang,Xuehuan Zhu,Nick Peterson,Jieqiong Wang,Shibiao Wan*

Main category: cs.CV

TL;DR: 这篇论文综述了人工智能（AI）和机器学习（ML）在RNA亚细胞定位预测中的最新进展，讨论了序列、图像及混合方法的应用，并指出了数据稀缺和缺乏基准测试等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法（如原位杂交）成本高且耗时，AI/ML方法可大规模预测RNA定位，加速研究并指导疾病治疗。

Method: 论文回顾了序列、图像及混合方法，整合多元数据预测RNA亚细胞定位。

Result: AI/ML方法展现出高效预测潜力，但仍面临数据不足和标准化问题。

Conclusion: 该综述为RNA定位研究提供了创新方向，并呼吁解决数据和基准挑战。

Abstract: The subcellular localization of RNAs, including long non-coding RNAs
(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,
plays a critical role in determining their biological functions. For instance,
lncRNAs are predominantly associated with chromatin and act as regulators of
gene transcription and chromatin structure, while mRNAs are distributed across
the nucleus and cytoplasm, facilitating the transport of genetic information
for protein synthesis. Understanding RNA localization sheds light on processes
like gene expression regulation with spatial and temporal precision. However,
traditional wet lab methods for determining RNA localization, such as in situ
hybridization, are often time-consuming, resource-demanding, and costly. To
overcome these challenges, computational methods leveraging artificial
intelligence (AI) and machine learning (ML) have emerged as powerful
alternatives, enabling large-scale prediction of RNA subcellular localization.
This paper provides a comprehensive review of the latest advancements in
AI-based approaches for RNA subcellular localization prediction, covering
various RNA types and focusing on sequence-based, image-based, and hybrid
methodologies that combine both data types. We highlight the potential of these
methods to accelerate RNA research, uncover molecular pathways, and guide
targeted disease treatments. Furthermore, we critically discuss the challenges
in AI/ML approaches for RNA subcellular localization, such as data scarcity and
lack of benchmarks, and opportunities to address them. This review aims to
serve as a valuable resource for researchers seeking to develop innovative
solutions in the field of RNA subcellular localization and beyond.

</details>


### [283] [We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback](https://arxiv.org/abs/2504.17180)
*Minkyu Choi,S P Sharan,Harsh Goel,Sahil Shah,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 当前文本生成视频（T2V）模型在处理复杂或多对象提示时存在语义和时间一致性问题。作者提出了一种无需训练的神经符号反馈视频增强方法，显著提升了生成视频的逻辑和时间对齐性能。


<details>
  <summary>Details</summary>
Motivation: T2V模型在处理复杂文本提示时生成效果不佳且计算成本高，因此需要一种无需训练的方法来改进视频生成的逻辑和时间一致性。

Method: 提出了一个基于神经符号反馈的零训练视频增强框架，通过分析视频表达识别语义不一致的部分，并针对性调整原始视频。

Result: 实验证明，该方法在开源和专有T2V模型上均显著提升了约40%的逻辑和时间对齐性能。

Conclusion: 神经符号反馈是一种有效提升T2V生成质量的途径，适用于复杂提示场景且无需额外训练成本。

Abstract: Current text-to-video (T2V) generation models are increasingly popular due to
their ability to produce coherent videos from textual prompts. However, these
models often struggle to generate semantically and temporally consistent videos
when dealing with longer, more complex prompts involving multiple objects or
sequential events. Additionally, the high computational costs associated with
training or fine-tuning make direct improvements impractical. To overcome these
limitations, we introduce \(\projectname\), a novel zero-training video
refinement pipeline that leverages neuro-symbolic feedback to automatically
enhance video generation, achieving superior alignment with the prompts. Our
approach first derives the neuro-symbolic feedback by analyzing a formal video
representation and pinpoints semantically inconsistent events, objects, and
their corresponding frames. This feedback then guides targeted edits to the
original video. Extensive empirical evaluations on both open-source and
proprietary T2V models demonstrate that \(\projectname\) significantly enhances
temporal and logical alignment across diverse prompts by almost $40\%$.

</details>


### [284] [MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing](https://arxiv.org/abs/2504.17213)
*Shiwen Cao,Zhaoxing Zhang,Junming Jiao,Juyi Qiao,Guowen Song,Rong Shen*

Main category: cs.CV

TL;DR: MCAF是一种基于代理的无训练框架，通过多模态由粗到精的注意力聚焦实现高效视频理解，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频理解尤其是长视频理解因信息冗余而困难，需要全局注意力分配以准确理解。

Method: MCAF采用多模态由粗到精的注意力聚焦和扩张时间扩展机制，结合自反馈机制迭代调整注意力。

Result: 在EgoSchema上表现提升5%，在Next-QA和IntentQA上分别提升0.2%和0.3%，在长视频数据集Video-MME上也优于其他方法。

Conclusion: MCAF通过创新注意力机制显著提升视频理解性能，尤其在长视频任务中表现突出。

Abstract: Even in the era of rapid advances in large models, video understanding,
particularly long videos, remains highly challenging. Compared with textual or
image-based information, videos commonly contain more information with
redundancy, requiring large models to strategically allocate attention at a
global level for accurate comprehension. To address this, we propose MCAF, an
agent-based, training-free framework perform video understanding through
Multimodal Coarse-to-fine Attention Focusing. The key innovation lies in its
ability to sense and prioritize segments of the video that are highly relevant
to the understanding task. First, MCAF hierarchically concentrates on highly
relevant frames through multimodal information, enhancing the correlation
between the acquired contextual information and the query. Second, it employs a
dilated temporal expansion mechanism to mitigate the risk of missing crucial
details when extracting information from these concentrated frames. In
addition, our framework incorporates a self-reflection mechanism utilizing the
confidence level of the model's responses as feedback. By iteratively applying
these two creative focusing strategies, it adaptively adjusts attention to
capture highly query-connected context and thus improves response accuracy.
MCAF outperforms comparable state-of-the-art methods on average. On the
EgoSchema dataset, it achieves a remarkable 5% performance gain over the
leading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms
the current state-of-the-art standard by 0.2% and 0.3% respectively. On the
Video-MME dataset, which features videos averaging nearly an hour in length,
MCAF also outperforms other agent-based methods.

</details>


### [285] [Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+](https://arxiv.org/abs/2504.17306)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Sameh Mbarek*

Main category: cs.CV

TL;DR: 该研究通过针对每种糖尿病视网膜病变病变类型实现二元分割方法，并结合后处理步骤提高分割准确性，克服了数据集限制和标注复杂性。


<details>
  <summary>Details</summary>
Motivation: 提高糖尿病视网膜病变病变（如微动脉瘤、出血、渗出物等）的分割精度，解决数据集限制和标注复杂性的挑战。

Method: 采用针对每种病变类型的二元分割方法，结合后处理步骤（如裁剪和LAB图像的CLAHE处理），并使用DeepLabv3+模型及数据增强技术。

Result: 在IDRID数据集上验证，分割准确率达到99%，证明了方法的有效性。

Conclusion: 研究展示了创新策略在医学图像分析中的高效性，特别是在糖尿病视网膜病变病变的精确分割方面。

Abstract: To improve the segmentation of diabetic retinopathy lesions (microaneurysms,
hemorrhages, exudates, and soft exudates), we implemented a binary segmentation
method specific to each type of lesion. As post-segmentation, we combined the
individual model outputs into a single image to better analyze the lesion
types. This approach facilitated parameter optimization and improved accuracy,
effectively overcoming challenges related to dataset limitations and annotation
complexity. Specific preprocessing steps included cropping and applying
contrast-limited adaptive histogram equalization to the L channel of the LAB
image. Additionally, we employed targeted data augmentation techniques to
further refine the model's efficacy. Our methodology utilized the DeepLabv3+
model, achieving a segmentation accuracy of 99%. These findings highlight the
efficacy of innovative strategies in advancing medical image analysis,
particularly in the precise segmentation of diabetic retinopathy lesions. The
IDRID dataset was utilized to validate and demonstrate the robustness of our
approach.

</details>


### [286] [DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model](https://arxiv.org/abs/2504.17315)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Pengfei Li,Shuang Wu,Chong Li,Junhao Zhu,Hao Yang*

Main category: cs.CV

TL;DR: 华为翻译服务中心提出了一种结合多任务学习和感知链式思维训练的端到端文档图像机器翻译框架，利用先进的开源视觉语言模型，提升复杂布局文档的翻译能力。


<details>
  <summary>Details</summary>
Motivation: 解决复杂布局文档的端到端机器翻译问题，结合OCR和非OCR任务于统一框架。

Method: 采用多任务学习和感知链式思维训练框架，结合最小贝叶斯解码和后处理策略。

Result: 展示了有效的文档图像机器翻译方法，系统详述了训练到推理的全流程。

Conclusion: 提出的统一框架在复杂布局文档翻译任务中表现优异，为相关领域提供了新思路。

Abstract: This paper presents the technical solution proposed by Huawei Translation
Service Center (HW-TSC) for the "End-to-End Document Image Machine Translation
for Complex Layouts" competition at the 19th International Conference on
Document Analysis and Recognition (DIMT25@ICDAR2025). Leveraging
state-of-the-art open-source large vision-language model (LVLM), we introduce a
training framework that combines multi-task learning with perceptual
chain-of-thought to develop a comprehensive end-to-end document translation
system. During the inference phase, we apply minimum Bayesian decoding and
post-processing strategies to further enhance the system's translation
capabilities. Our solution uniquely addresses both OCR-based and OCR-free
document image translation tasks within a unified framework. This paper
systematically details the training methods, inference strategies, LVLM base
models, training data, experimental setups, and results, demonstrating an
effective approach to document image machine translation.

</details>


### [287] [StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies](https://arxiv.org/abs/2504.17401)
*Xu Wang,Jialang Xu,Shuai Zhang,Baoru Huang,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 提出StereoMamba架构用于RAMIS中的立体视差估计，结合FE-Mamba和MFF模块，在SCARED基准测试中表现出高精度（EPE 2.64 px）和速度（21.28 FPS），并展示强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 针对RAMIS中立体视差估计的准确性、鲁棒性和推理速度之间的平衡问题，设计专用架构以满足临床应用需求。

Method: 采用FE-Mamba模块增强长程空间依赖，通过MFF模块融合多尺度特征，实现高效立体匹配。

Result: 在SCARED基准上EPE为2.64 px，深度MAE为2.55 mm，推理速度21.28 FPS；在零样本测试中SSIM（0.8970）和PSNR（16.0761）最优。

Conclusion: StereoMamba在精度-效率权衡上表现优异，并具备跨数据集泛化能力，适合RAMIS应用。

Abstract: Stereo disparity estimation is crucial for obtaining depth information in
robot-assisted minimally invasive surgery (RAMIS). While current deep learning
methods have made significant advancements, challenges remain in achieving an
optimal balance between accuracy, robustness, and inference speed. To address
these challenges, we propose the StereoMamba architecture, which is
specifically designed for stereo disparity estimation in RAMIS. Our approach is
based on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances
long-range spatial dependencies both within and across stereo images. To
effectively integrate multi-scale features from FE-Mamba, we then introduce a
novel Multidimensional Feature Fusion (MFF) module. Experiments against the
state-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba
achieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the
second-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining
an inference speed of 21.28 FPS for a pair of high-resolution images
(1280*1024), striking the optimum balance between accuracy, robustness, and
efficiency. Furthermore, by comparing synthesized right images, generated from
warping left images using the generated disparity maps, with the actual right
image, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),
exhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS
datasets.

</details>


### [288] [FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding](https://arxiv.org/abs/2504.17447)
*De-An Huang,Subhashree Radhakrishnan,Zhiding Yu,Jan Kautz*

Main category: cs.CV

TL;DR: 本文提出了一种名为FRAG的帧选择增强生成方法，通过独立评分选择输入中的相关帧，仅基于这些帧生成最终输出，无需长上下文处理，适用于长视频和多页文档。


<details>
  <summary>Details</summary>
Motivation: 由于长上下文多模态模型在训练和推理中的计算成本高，模型大小和性能受限，作者探索了一个无需长上下文处理的替代方向。

Method: 提出了Frame Selection Augmented Generation (FRAG)，核心是通过独立评分选择输入中最相关的帧（Top-K选择），然后仅基于这些帧生成输出，无需调整现有LMM。

Result: 实验表明，FRAG显著提升性能：视频任务中InternVL2-76B在MLVU和Video-MME上分别提升5.8%和3.7%；文档任务中MP-DocVQA上比专用长文档LMM提升超20%。

Conclusion: FRAG的简单框架无需微调即可提升现有LMM在长视频和文档任务中的性能，并达到最先进水平。

Abstract: There has been impressive progress in Large Multimodal Models (LMMs). Recent
works extend these models to long inputs, including multi-page documents and
long videos. However, the model size and performance of these long context
models are still limited due to the computational cost in both training and
inference. In this work, we explore an orthogonal direction and process long
inputs without long context LMMs. We propose Frame Selection Augmented
Generation (FRAG), where the model first selects relevant frames within the
input, and then only generates the final outputs based on the selected frames.
The core of the selection process is done by scoring each frame independently,
which does not require long context processing. The frames with the highest
scores are then selected by a simple Top-K selection. We show that this
frustratingly simple framework is applicable to both long videos and multi-page
documents using existing LMMs without any fine-tuning. We consider two models,
LLaVA-OneVision and InternVL2, in our experiments and show that FRAG
consistently improves the performance and achieves state-of-the-art
performances for both long video and long document understanding. For videos,
FRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on
Video-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA
compared with recent LMMs specialized in long document understanding. Code is
available at: https://github.com/NVlabs/FRAG

</details>


### [289] [Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data](https://arxiv.org/abs/2504.17474)
*Weiran Pan,Wei Wei,Feida Zhu,Yong Deng*

Main category: cs.CV

TL;DR: 提出了一种新颖的样本选择方法，通过跟踪模型预测置信度的趋势而非仅依赖损失值，以更准确地区分正确标注但难学习的样本与错误标注样本，从而提升噪声标签下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将小损失样本视为正确标注，但一些正确标注但难学习的样本在训练早期可能表现出高损失，导致基于损失阈值选择样本时面临精度与召回率的权衡问题。

Method: 通过跟踪标注标签与其他类别间置信度差距的趋势（使用Mann-Kendall检验评估），识别潜在的正确标注样本，并将该方法作为即插即用组件集成到现有技术中。

Result: 在多个标准基准和真实数据集上的实验表明，该方法显著提升了现有噪声标签学习技术的性能。

Conclusion: 通过分析置信度趋势而非单一损失值，能够更有效地解决噪声标签下的样本选择难题，为相关领域提供了实用且高效的解决方案。

Abstract: We propose a novel sample selection method for image classification in the
presence of noisy labels. Existing methods typically consider small-loss
samples as correctly labeled. However, some correctly labeled samples are
inherently difficult for the model to learn and can exhibit high loss similar
to mislabeled samples in the early stages of training. Consequently, setting a
threshold on per-sample loss to select correct labels results in a trade-off
between precision and recall in sample selection: a lower threshold may miss
many correctly labeled hard-to-learn samples (low recall), while a higher
threshold may include many mislabeled samples (low precision). To address this
issue, our goal is to accurately distinguish correctly labeled yet
hard-to-learn samples from mislabeled ones, thus alleviating the trade-off
dilemma. We achieve this by considering the trends in model prediction
confidence rather than relying solely on loss values. Empirical observations
show that only for correctly labeled samples, the model's prediction confidence
for the annotated labels typically increases faster than for any other classes.
Based on this insight, we propose tracking the confidence gaps between the
annotated labels and other classes during training and evaluating their trends
using the Mann-Kendall Test. A sample is considered potentially correctly
labeled if all its confidence gaps tend to increase. Our method functions as a
plug-and-play component that can be seamlessly integrated into existing sample
selection techniques. Experiments on several standard benchmarks and real-world
datasets demonstrate that our method enhances the performance of existing
methods for learning with noisy labels.

</details>


### [290] [An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm](https://arxiv.org/abs/2504.17540)
*Ahmadreza Shateri,Negar Nourani,Morteza Dorrigiv,Hamid Nasiri*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的框架，用于从皮肤病变图像中自动检测猴痘，结合迁移学习、降维和机器学习技术，并利用优化算法调优，达到高准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于猴痘在全球非传统流行区域的传播引发公共卫生担忧，早期准确诊断对疾病管理至关重要。

Method: 使用Xception架构进行深度特征提取，PCA降维，NGBoost分类，并通过AVOA算法优化超参数。

Result: 模型性能优异，准确率97.53%，F1分数97.72%，AUC 97.47%，并通过Grad-CAM和LIME增强了可解释性。

Conclusion: 该框架为资源有限环境中的早期诊断提供了高效工具。

Abstract: The recent global spread of monkeypox, particularly in regions where it has
not historically been prevalent, has raised significant public health concerns.
Early and accurate diagnosis is critical for effective disease management and
control. In response, this study proposes a novel deep learning-based framework
for the automated detection of monkeypox from skin lesion images, leveraging
the power of transfer learning, dimensionality reduction, and advanced machine
learning techniques. We utilize the newly developed Monkeypox Skin Lesion
Dataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to
train and evaluate our models. The proposed framework employs the Xception
architecture for deep feature extraction, followed by Principal Component
Analysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting
(NGBoost) algorithm for classification. To optimize the model's performance and
generalization, we introduce the African Vultures Optimization Algorithm (AVOA)
for hyperparameter tuning, ensuring efficient exploration of the parameter
space. Our results demonstrate that the proposed AVOA-NGBoost model achieves
state-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%
and an AUC of 97.47%. Additionally, we enhance model interpretability using
Grad-CAM and LIME techniques, providing insights into the decision-making
process and highlighting key features influencing classification. This
framework offers a highly precise and efficient diagnostic tool, potentially
aiding healthcare providers in early detection and diagnosis, particularly in
resource-constrained environments.

</details>


### [291] [A Genealogy of Multi-Sensor Foundation Models in Remote Sensing](https://arxiv.org/abs/2504.17177)
*Kevin Lane,Morteza Karimzadeh*

Main category: cs.CV

TL;DR: 该综述探讨了遥感领域基础模型的现状，分析了其在计算机视觉成功应用中的优缺点，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索基础模型在遥感表征学习中的应用现状、挑战及未来发展方向。

Method: 通过分析现有方法与计算机视觉领域的关联，评估其表征学习质量及多传感器利用程度。

Result: 指出了现有方法的优缺点，并提出了如何利用未标注、季节性和多传感器数据的改进方向。

Conclusion: 结论强调了遥感基础模型的潜力，并呼吁进一步研究如何优化这些模型以更好地利用多传感器数据。

Abstract: Foundation models have garnered increasing attention for representation
learning in remote sensing, primarily adopting approaches that have
demonstrated success in computer vision with minimal domain-specific
modification. However, the development and application of foundation models in
this field are still burgeoning, as there are a variety of competing approaches
that each come with significant benefits and drawbacks. This paper examines
these approaches along with their roots in the computer vision field in order
to characterize potential advantages and pitfalls while outlining future
directions to further improve remote sensing-specific foundation models. We
discuss the quality of the learned representations and methods to alleviate the
need for massive compute resources. We place emphasis on the multi-sensor
aspect of Earth observations, and the extent to which existing approaches
leverage multiple sensors in training foundation models in relation to
multi-modal foundation models. Finally, we identify opportunities for further
harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote
sensing observations.

</details>


### [292] [Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior](https://arxiv.org/abs/2504.17551)
*Lin Che,Yizi Chen,Tanhua Jin,Martin Raubal,Konrad Schindler,Peter Kiefer*

Main category: cs.CV

TL;DR: 论文提出了一种无监督对比聚类模型，结合地理先验知识，用于街景图像的土地利用分类，解决了传统方法在复杂城市环境中缺乏精度和泛化能力的问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有遥感技术在复杂城市环境中缺乏地面细节信息，且监督分类方法受限于高质量标注数据的稀缺性和泛化困难，研究旨在通过无监督方法提升土地利用分类的精度和灵活性。

Method: 引入了一种无监督对比聚类模型，结合地理先验（Tobler法则），并通过简单的视觉分配集群，实现灵活且定制化的土地利用制图。

Result: 实验表明，该方法能从两个城市的街景图像数据集中生成土地利用地图，且具有通用性和可扩展性。

Conclusion: 该方法依赖于地理数据的空间一致性，可适用于各种街景图像可用的场景，支持大规模、无监督的土地利用制图和更新。

Abstract: Urban land use classification and mapping are critical for urban planning,
resource management, and environmental monitoring. Existing remote sensing
techniques often lack precision in complex urban environments due to the
absence of ground-level details. Unlike aerial perspectives, street view images
provide a ground-level view that captures more human and social activities
relevant to land use in complex urban scenes. Existing street view-based
methods primarily rely on supervised classification, which is challenged by the
scarcity of high-quality labeled data and the difficulty of generalizing across
diverse urban landscapes. This study introduces an unsupervised contrastive
clustering model for street view images with a built-in geographical prior, to
enhance clustering performance. When combined with a simple visual assignment
of the clusters, our approach offers a flexible and customizable solution to
land use mapping, tailored to the specific needs of urban planners. We
experimentally show that our method can generate land use maps from geotagged
street view image datasets of two cities. As our methodology relies on the
universal spatial coherence of geospatial data ("Tobler's law"), it can be
adapted to various settings where street view images are available, to enable
scalable, unsupervised land use mapping and updating. The code will be
available at https://github.com/lin102/CCGP.

</details>


### [293] [STCL:Curriculum learning Strategies for deep learning image steganography models](https://arxiv.org/abs/2504.17609)
*Fengchun Liu,Tong Zhang,Chunying Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为STCL的训练策略，通过逐步从易到难选择图像训练深度学习隐写模型，提高了隐写图像质量和模型收敛速度，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对基于深度学习的图像隐写模型在隐写图像质量差和网络收敛慢的问题，提出了STCL训练策略，以改善模型性能。

Method: 采用基于教师模型的难度评估策略和基于拐点的训练调度策略，逐步从易到难选择训练图像，优化训练过程。

Result: 在ALASKA2、VOC2012和ImageNet数据集上的实验表明，该方法显著提升了PSNR、SSIM和解码精度，同时降低了隐写分析分数。

Conclusion: STCL策略有效提升了隐写模型的性能，生成的图像质量高且隐蔽性好，实验证明了其广泛适用性。

Abstract: Aiming at the problems of poor quality of steganographic images and slow
network convergence of image steganography models based on deep learning, this
paper proposes a Steganography Curriculum Learning training strategy (STCL) for
deep learning image steganography models. So that only easy images are selected
for training when the model has poor fitting ability at the initial stage, and
gradually expand to more difficult images, the strategy includes a difficulty
evaluation strategy based on the teacher model and an knee point-based training
scheduling strategy. Firstly, multiple teacher models are trained, and the
consistency of the quality of steganographic images under multiple teacher
models is used as the difficulty score to construct the training subsets from
easy to difficult. Secondly, a training control strategy based on knee points
is proposed to reduce the possibility of overfitting on small training sets and
accelerate the training process. Experimental results on three large public
datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image
steganography scheme is able to improve the model performance under multiple
algorithmic frameworks, which not only has a high PSNR, SSIM score, and
decoding accuracy, but also the steganographic images generated by the model
under the training of the STCL strategy have a low steganography analysis
scores. You can find our code at
\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.

</details>


### [294] [Enhancing CNNs robustness to occlusions with bioinspired filters for border completion](https://arxiv.org/abs/2504.17619)
*Catarina P. Coutinho,Aneeqa Merhab,Janko Petkovic,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.CV

TL;DR: 利用视觉皮层机制的数学模型改进CNN滤波器，在遮挡MNIST图像测试中提升了性能。


<details>
  <summary>Details</summary>
Motivation: 基于视觉皮层边界补全机制的数学模型，改进CNN的滤波器设计。

Method: 将视觉皮层机制的数学模型融入LeNet 5，设计定制化滤波器。

Result: 在遮挡MNIST图像测试中，准确性等性能指标一致提升。

Conclusion: 视觉皮层机制的数学模型可有效优化CNN滤波器设计，提高性能。

Abstract: We exploit the mathematical modeling of the visual cortex mechanism for
border completion to define custom filters for CNNs. We see a consistent
improvement in performance, particularly in accuracy, when our modified LeNet 5
is tested with occluded MNIST images.

</details>


### [295] [Hierarchical and Multimodal Data for Daily Activity Understanding](https://arxiv.org/abs/2504.17696)
*Ghazal Kaviani,Yavuz Yarici,Seulgi Kim,Mohit Prabhushankar,Ghassan AlRegib,Mashhour Solh,Ameya Patil*

Main category: cs.CV

TL;DR: DARai是一个多模态、分层标注的数据集，包含50名参与者在10种环境中的200小时多传感器数据，用于研究人类活动。


<details>
  <summary>Details</summary>
Motivation: 理解真实世界中复杂的人类活动，并为机器学习模型提供多模态数据支持。

Method: 数据集包含三个层次标注（高层次活动、低层次动作、细粒度步骤），并进行单模态和多模态传感器融合实验。

Result: 实验展示了DARai在识别、时间定位和未来动作预测中的价值，并揭示了单个传感器的局限性。

Conclusion: DARai为人类中心应用提供了重要的数据集和挑战性任务，支持多模态和领域变化研究。

Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced
"Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to
understand human activities in real-world settings. DARai consists of
continuous scripted and unscripted recordings of 50 participants in 10
different environments, totaling over 200 hours of data from 20 sensors
including multiple camera views, depth and radar sensors, wearable inertial
measurement units (IMUs), electromyography (EMG), insole pressure sensors,
biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three
levels of hierarchy: (i) high-level activities (L1) that are independent tasks,
(ii) lower-level actions (L2) that are patterns shared between activities, and
(iii) fine-grained procedures (L3) that detail the exact execution steps for
actions. The dataset annotations and recordings are designed so that 22.7% of
L2 actions are shared between L1 activities and 14.2% of L3 procedures are
shared between L2 actions. The overlap and unscripted nature of DARai allows
counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai
in uncovering important challenges in human-centered applications.
Specifically, we conduct unimodal and multimodal sensor fusion experiments for
recognition, temporal localization, and future action anticipation across all
hierarchical annotation levels. To highlight the limitations of individual
sensors, we also conduct domain-variant experiments that are enabled by DARai's
multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai
website:
https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/

</details>


### [296] [EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor](https://arxiv.org/abs/2504.17735)
*Akhil Padmanabha,Saravanan Govindarajan,Hwanmun Kim,Sergio Ortiz,Rahul Rajan,Doruk Senkal,Sneha Kadetotad*

Main category: cs.CV

TL;DR: 该论文提出了一种名为 EgoCHARM 的资源高效机器学习算法，用于通过头戴式惯性测量单元（IMU）识别高层次和低层次活动，取得了较好的性能表现。


<details>
  <summary>Details</summary>
Motivation: 智能眼镜上的人类活动识别（HAR）有广泛的应用场景，如健康/健身追踪和情境感知 AI 助手输入。然而，现有的自我中心活动识别方法要么性能较低，要么计算资源消耗大。因此，作者希望开发一种资源（内存、计算、功耗、样本）高效的方法来解决这些问题。

Method: 作者提出了一种层次化算法 EgoCHARM，采用半监督学习策略，主要依赖高层次活动标签进行训练，以学习通用的低层次运动嵌入，进而有效用于低层次活动识别。

Result: 该方法在 9 个高层次和 3 个低层次活动上的 F1 分数分别达到 0.826 和 0.855，模型参数量仅为 63k（高层次）和 22k（低层次），使得低层次编码器可以直接部署在现有的 IMU 芯片上。

Conclusion: 论文通过敏感性分析展示了使用自我中心 IMU 进行活动识别的机会与局限性，证明了 EgoCHARM 在实际应用中的高效性和可行性。

Abstract: Human activity recognition (HAR) on smartglasses has various use cases,
including health/fitness tracking and input for context-aware AI assistants.
However, current approaches for egocentric activity recognition suffer from low
performance or are resource-intensive. In this work, we introduce a resource
(memory, compute, power, sample) efficient machine learning algorithm,
EgoCHARM, for recognizing both high level and low level activities using a
single egocentric (head-mounted) Inertial Measurement Unit (IMU). Our
hierarchical algorithm employs a semi-supervised learning strategy, requiring
primarily high level activity labels for training, to learn generalizable low
level motion embeddings that can be effectively utilized for low level activity
recognition. We evaluate our method on 9 high level and 3 low level activities
achieving 0.826 and 0.855 F1 scores on high level and low level activity
recognition respectively, with just 63k high level and 22k low level model
parameters, allowing the low level encoder to be deployed directly on current
IMU chips with compute. Lastly, we present results and insights from a
sensitivity analysis and highlight the opportunities and limitations of
activity recognition using egocentric IMUs.

</details>


### [297] [DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)
*Zhenhailong Wang,Senthil Purushwalkam,Caiming Xiong,Silvio Savarese,Heng Ji,Ran Xu*

Main category: cs.CV

TL;DR: DyMU是一种无需训练的高效框架，通过动态调整视觉标记的数量来降低视觉-语言模型的计算负担，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉变换器中固定长度输出的效率问题，同时避免额外的微调需求。

Method: 结合动态标记合并（DToMe）和虚拟标记解合并（VTU），根据图像复杂度动态调整标记数量。

Result: 在多个视觉-语言模型架构中，DyMU能将视觉标记数量减少32%-85%，同时性能接近完整模型。

Conclusion: DyMU提供了一种无需训练的动态方法，显著降低计算成本，同时保持高性能。

Abstract: We present DyMU, an efficient, training-free framework that dynamically
reduces the computational burden of vision-language models (VLMs) while
maintaining high task performance. Our approach comprises two key components.
First, Dynamic Token Merging (DToMe) reduces the number of visual token
embeddings by merging similar tokens based on image complexity, addressing the
inherent inefficiency of fixed-length outputs in vision transformers. Second,
Virtual Token Unmerging (VTU) simulates the expected token sequence for large
language models (LLMs) by efficiently reconstructing the attention dynamics of
a full sequence, thus preserving the downstream performance without additional
fine-tuning. Unlike previous approaches, our method dynamically adapts token
compression to the content of the image and operates completely training-free,
making it readily applicable to most state-of-the-art VLM architectures.
Extensive experiments on image and video understanding tasks demonstrate that
DyMU can reduce the average visual token count by 32%-85% while achieving
comparable performance to full-length models across diverse VLM architectures,
including the recently popularized AnyRes-based visual encoders. Furthermore,
through qualitative analyses, we demonstrate that DToMe effectively adapts
token reduction based on image complexity and, unlike existing systems,
provides users more control over computational costs. Project page:
https://mikewangwzhl.github.io/dymu/.

</details>


### [298] [Distilling semantically aware orders for autoregressive image generation](https://arxiv.org/abs/2504.17069)
*Rishav Pramanik,Antoine Poupon,Juan A. Rodriguez,Masih Aminbeidokhti,David Vazquez,Christopher Pal,Zhaozheng Yin,Marco Pedersoli*

Main category: cs.CV

TL;DR: 使用任意顺序生成图像块的方法优于传统的光栅扫描顺序，生成的图像质量更高，且无需额外标注或训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归图像生成采用光栅扫描顺序，但此顺序可能不符合图像内容的因果关系，导致生成效果不佳。

Method: 先训练模型实现任意顺序生成图像块，推断内容和位置顺序，再用这些顺序微调模型。

Result: 新方法在多个数据集上表现优于传统光栅扫描顺序，生成的图像质量更高。

Conclusion: 任意顺序生成能更好地捕捉图像内容的因果关系，效果优于传统方法。

Abstract: Autoregressive patch-based image generation has recently shown competitive
results in terms of image quality and scalability. It can also be easily
integrated and scaled within Vision-Language models. Nevertheless,
autoregressive models require a defined order for patch generation. While a
natural order based on the dictation of the words makes sense for text
generation, there is no inherent generation order that exists for image
generation. Traditionally, a raster-scan order (from top-left to bottom-right)
guides autoregressive image generation models. In this paper, we argue that
this order is suboptimal, as it fails to respect the causality of the image
content: for instance, when conditioned on a visual description of a sunset, an
autoregressive model may generate clouds before the sun, even though the color
of clouds should depend on the color of the sun and not the inverse. In this
work, we show that first by training a model to generate patches in
any-given-order, we can infer both the content and the location (order) of each
patch during generation. Secondly, we use these extracted orders to finetune
the any-given-order model to produce better-quality images. Through our
experiments, we show on two datasets that this new generation method produces
better images than the traditional raster-scan approach, with similar training
costs and no extra annotations.

</details>


### [299] [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
*Ling You,Wenxuan Huang,Xinni Xie,Xiangyi Wei,Bangyan Li,Shaohui Lin,Yang Li,Changbo Wang*

Main category: cs.CV

TL;DR: TimeSoccer是一个端到端的多模态大语言模型，专门用于足球比赛的单锚点密集视频字幕生成（SDVC），能够联合预测时间戳并生成字幕，解决了现有方法依赖先验时间信息或两步范式导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 足球比赛的解说道通常需要精确的时间定位和语义丰富的描述，但现有的足球MLLMs依赖先验时间信息，无法端到端处理长视频。传统方法复杂且无法捕捉全局上下文，表现不佳。

Method: 提出了TimeSoccer，通过单次推理联合预测时间戳和生成字幕，支持45分钟比赛的全局上下文建模。引入MoFA-Select模块，无需训练，通过粗到细策略自适应选择代表性帧，并辅以互补训练范式增强长时序处理能力。

Result: 实验证明TimeSoccer在SDVC任务上实现了端到端的State-of-The-Art性能，生成高质量且时间对齐准确的解说。

Conclusion: TimeSoccer通过端到端设计和MoFA-Select模块，显著提升了足球比赛长视频的理解和解说生成能力。

Abstract: Soccer is a globally popular sporting event, typically characterized by long
matches and distinctive highlight moments. Recent advances in Multimodal Large
Language Models (MLLMs) offer promising capabilities in temporal grounding and
video understanding, soccer commentary generation often requires precise
temporal localization and semantically rich descriptions over long-form video.
However, existing soccer MLLMs often rely on the temporal a priori for caption
generation, so they cannot process the soccer video end-to-end. While some
traditional approaches follow a two-step paradigm that is complex and fails to
capture the global context to achieve suboptimal performance. To solve the
above issues, we present TimeSoccer, the first end-to-end soccer MLLM for
Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
TimeSoccer jointly predicts timestamps and generates captions in a single pass,
enabling global context modeling across 45-minute matches. To support long
video understanding of soccer matches, we introduce MoFA-Select, a
training-free, motion-aware frame compression module that adaptively selects
representative frames via a coarse-to-fine strategy, and incorporates
complementary training paradigms to strengthen the model's ability to handle
long temporal sequences. Extensive experiments demonstrate that our TimeSoccer
achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end
form, generating high-quality commentary with accurate temporal alignment and
strong semantic relevance.

</details>


### [300] [A Comprehensive Review on RNA Subcellular Localization Prediction](https://arxiv.org/abs/2504.17162)
*Cece Zhang,Xuehuan Zhu,Nick Peterson,Jieqiong Wang,Shibiao Wan*

Main category: cs.CV

TL;DR: 本文综述了基于AI的RNA亚细胞定位预测方法的最新进展，包括不同RNA类型的序列、图像及混合方法，探讨了这些方法加速RNA研究的潜力及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验方法耗时耗力且成本高，因此需要开发高效的计算方法来预测RNA亚细胞定位。

Method: 通过AI和机器学习技术，结合序列、图像或混合数据，实现RNA亚细胞定位的大规模预测。

Result: 这些计算工具有助于揭示RNA的空间分布规律，推动RNA功能研究和疾病治疗。

Conclusion: AI/ML方法在RNA亚细胞定位领域显示出巨大潜力，但仍需解决数据稀缺和标准化等挑战。

Abstract: The subcellular localization of RNAs, including long non-coding RNAs
(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,
plays a critical role in determining their biological functions. For instance,
lncRNAs are predominantly associated with chromatin and act as regulators of
gene transcription and chromatin structure, while mRNAs are distributed across
the nucleus and cytoplasm, facilitating the transport of genetic information
for protein synthesis. Understanding RNA localization sheds light on processes
like gene expression regulation with spatial and temporal precision. However,
traditional wet lab methods for determining RNA localization, such as in situ
hybridization, are often time-consuming, resource-demanding, and costly. To
overcome these challenges, computational methods leveraging artificial
intelligence (AI) and machine learning (ML) have emerged as powerful
alternatives, enabling large-scale prediction of RNA subcellular localization.
This paper provides a comprehensive review of the latest advancements in
AI-based approaches for RNA subcellular localization prediction, covering
various RNA types and focusing on sequence-based, image-based, and hybrid
methodologies that combine both data types. We highlight the potential of these
methods to accelerate RNA research, uncover molecular pathways, and guide
targeted disease treatments. Furthermore, we critically discuss the challenges
in AI/ML approaches for RNA subcellular localization, such as data scarcity and
lack of benchmarks, and opportunities to address them. This review aims to
serve as a valuable resource for researchers seeking to develop innovative
solutions in the field of RNA subcellular localization and beyond.

</details>


### [301] [We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback](https://arxiv.org/abs/2504.17180)
*Minkyu Choi,S P Sharan,Harsh Goel,Sahil Shah,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的零样本视频优化流水线，通过神经符号反馈自动提升视频生成质量，解决了现有文本到视频模型在复杂提示下语义和时间一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型在长而复杂的提示下难以保持语义和时间一致性，且直接改进模型的训练成本高昂。

Method: 提出了一种神经符号反馈驱动的视频优化方法，先分析视频的符号表示以识别不一致之处，再针对性修改视频。

Result: 在不同文本到视频模型上，该方法将时间和逻辑一致性提升了近40%。

Conclusion: 该方法能显著提升文本到视频生成的语义和时间一致性，且无需额外训练，是一种高效的解决方案。

Abstract: Current text-to-video (T2V) generation models are increasingly popular due to
their ability to produce coherent videos from textual prompts. However, these
models often struggle to generate semantically and temporally consistent videos
when dealing with longer, more complex prompts involving multiple objects or
sequential events. Additionally, the high computational costs associated with
training or fine-tuning make direct improvements impractical. To overcome these
limitations, we introduce \(\projectname\), a novel zero-training video
refinement pipeline that leverages neuro-symbolic feedback to automatically
enhance video generation, achieving superior alignment with the prompts. Our
approach first derives the neuro-symbolic feedback by analyzing a formal video
representation and pinpoints semantically inconsistent events, objects, and
their corresponding frames. This feedback then guides targeted edits to the
original video. Extensive empirical evaluations on both open-source and
proprietary T2V models demonstrate that \(\projectname\) significantly enhances
temporal and logical alignment across diverse prompts by almost $40\%$.

</details>


### [302] [MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing](https://arxiv.org/abs/2504.17213)
*Shiwen Cao,Zhaoxing Zhang,Junming Jiao,Juyi Qiao,Guowen Song,Rong Shen*

Main category: cs.CV

TL;DR: 论文提出了MCAF框架，一种基于代理的无训练方法，用于长视频理解，通过多模态粗到细注意力聚焦和扩张时间扩展机制提高准确性。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因信息冗余和全局注意力分配困难而具挑战性，需要创新的方法来提升理解准确性。

Method: MCAF框架通过多模态粗到细注意力聚焦策略和扩张时间扩展机制，结合自反馈机制，动态调整注意力以捕捉关键信息。

Result: 在EgoSchema数据集上表现提升5%，在Next-QA和IntentQA数据集上分别提升0.2%和0.3%，在Video-MME数据集上也优于其他代理方法。

Conclusion: MCAF通过创新的注意力聚焦和扩展机制，显著提升了长视频理解的性能，优于现有最先进方法。

Abstract: Even in the era of rapid advances in large models, video understanding,
particularly long videos, remains highly challenging. Compared with textual or
image-based information, videos commonly contain more information with
redundancy, requiring large models to strategically allocate attention at a
global level for accurate comprehension. To address this, we propose MCAF, an
agent-based, training-free framework perform video understanding through
Multimodal Coarse-to-fine Attention Focusing. The key innovation lies in its
ability to sense and prioritize segments of the video that are highly relevant
to the understanding task. First, MCAF hierarchically concentrates on highly
relevant frames through multimodal information, enhancing the correlation
between the acquired contextual information and the query. Second, it employs a
dilated temporal expansion mechanism to mitigate the risk of missing crucial
details when extracting information from these concentrated frames. In
addition, our framework incorporates a self-reflection mechanism utilizing the
confidence level of the model's responses as feedback. By iteratively applying
these two creative focusing strategies, it adaptively adjusts attention to
capture highly query-connected context and thus improves response accuracy.
MCAF outperforms comparable state-of-the-art methods on average. On the
EgoSchema dataset, it achieves a remarkable 5% performance gain over the
leading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms
the current state-of-the-art standard by 0.2% and 0.3% respectively. On the
Video-MME dataset, which features videos averaging nearly an hour in length,
MCAF also outperforms other agent-based methods.

</details>


### [303] [Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+](https://arxiv.org/abs/2504.17306)
*Meher Boulaabi,Takwa Ben Aïcha Gader,Afef Kacem Echi,Sameh Mbarek*

Main category: cs.CV

TL;DR: 该论文提出了一种针对糖尿病视网膜病变病灶的二元分割方法，通过结合多个模型的输出以提高分析精度，克服了数据集和标注复杂性的限制，最终实现了99%的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 改进糖尿病视网膜病变病灶（如微动脉瘤、出血、渗出物等）的精确分割，以解决数据集质量不足和标注复杂性高的挑战。

Method: 采用针对每种病灶的二元分割方法，结合DeepLabv3+模型，并通过数据增强和特定的预处理步骤（如裁剪和LAB图像的L通道对比度调整）优化模型。

Result: 在IDRID数据集上验证，分割准确率达到99%。

Conclusion: 该方法的创新策略有效提升了医学图像分析的精度，尤其在糖尿病视网膜病变病灶的分割中表现突出。

Abstract: To improve the segmentation of diabetic retinopathy lesions (microaneurysms,
hemorrhages, exudates, and soft exudates), we implemented a binary segmentation
method specific to each type of lesion. As post-segmentation, we combined the
individual model outputs into a single image to better analyze the lesion
types. This approach facilitated parameter optimization and improved accuracy,
effectively overcoming challenges related to dataset limitations and annotation
complexity. Specific preprocessing steps included cropping and applying
contrast-limited adaptive histogram equalization to the L channel of the LAB
image. Additionally, we employed targeted data augmentation techniques to
further refine the model's efficacy. Our methodology utilized the DeepLabv3+
model, achieving a segmentation accuracy of 99%. These findings highlight the
efficacy of innovative strategies in advancing medical image analysis,
particularly in the precise segmentation of diabetic retinopathy lesions. The
IDRID dataset was utilized to validate and demonstrate the robustness of our
approach.

</details>


### [304] [DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model](https://arxiv.org/abs/2504.17315)
*Zhanglin Wu,Tengfei Song,Ning Xie,Weidong Zhang,Pengfei Li,Shuang Wu,Chong Li,Junhao Zhu,Hao Yang*

Main category: cs.CV

TL;DR: 华为翻译服务中心提出了一种端到端文档图像机器翻译的解决方案，结合多任务学习和感知链式思维，采用最小贝叶斯解码和后处理策略提升翻译效果。


<details>
  <summary>Details</summary>
Motivation: 解决复杂布局文档图像翻译问题，统一处理OCR和无OCR任务，提出高效翻译方案。

Method: 结合多任务学习和感知链式思维的训练框架，最小贝叶斯解码和后处理策略进行推理。

Result: 实现了高效的端到端文档图像翻译，验证了方法的有效性。

Conclusion: 统一框架有效解决了复杂布局文档的翻译问题，展示了技术方案的先进性。

Abstract: This paper presents the technical solution proposed by Huawei Translation
Service Center (HW-TSC) for the "End-to-End Document Image Machine Translation
for Complex Layouts" competition at the 19th International Conference on
Document Analysis and Recognition (DIMT25@ICDAR2025). Leveraging
state-of-the-art open-source large vision-language model (LVLM), we introduce a
training framework that combines multi-task learning with perceptual
chain-of-thought to develop a comprehensive end-to-end document translation
system. During the inference phase, we apply minimum Bayesian decoding and
post-processing strategies to further enhance the system's translation
capabilities. Our solution uniquely addresses both OCR-based and OCR-free
document image translation tasks within a unified framework. This paper
systematically details the training methods, inference strategies, LVLM base
models, training data, experimental setups, and results, demonstrating an
effective approach to document image machine translation.

</details>


### [305] [StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies](https://arxiv.org/abs/2504.17401)
*Xu Wang,Jialang Xu,Shuai Zhang,Baoru Huang,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 论文提出StereoMamba架构，针对机器人辅助微创手术中的立体视差估计问题，结合FE-Mamba和MFF模块，在精度、鲁棒性和推理速度上取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在机器人辅助微创手术（RAMIS）的立体视差估计中，仍难以兼顾精度、鲁棒性和推理速度，需提出更优解决方案。

Method: 设计StereoMamba架构，包含FE-Mamba模块（增强长距离空间依赖）和MFF模块（多尺度特征融合），优化立体视差估计。

Result: 在SCARED基准测试中，EPE为2.64 px，深度MAE为2.55 mm，Bad2和Bad3分别为41.49%和26.99%，推理速度达21.28 FPS。在RIS2017和StereoMIS数据集上零样本泛化表现优异。

Conclusion: StereoMamba在RAMIS中实现了精度、鲁棒性与效率的最佳平衡，并展现出强大的泛化能力。

Abstract: Stereo disparity estimation is crucial for obtaining depth information in
robot-assisted minimally invasive surgery (RAMIS). While current deep learning
methods have made significant advancements, challenges remain in achieving an
optimal balance between accuracy, robustness, and inference speed. To address
these challenges, we propose the StereoMamba architecture, which is
specifically designed for stereo disparity estimation in RAMIS. Our approach is
based on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances
long-range spatial dependencies both within and across stereo images. To
effectively integrate multi-scale features from FE-Mamba, we then introduce a
novel Multidimensional Feature Fusion (MFF) module. Experiments against the
state-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba
achieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the
second-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining
an inference speed of 21.28 FPS for a pair of high-resolution images
(1280*1024), striking the optimum balance between accuracy, robustness, and
efficiency. Furthermore, by comparing synthesized right images, generated from
warping left images using the generated disparity maps, with the actual right
image, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),
exhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS
datasets.

</details>


### [306] [FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding](https://arxiv.org/abs/2504.17447)
*De-An Huang,Subhashree Radhakrishnan,Zhiding Yu,Jan Kautz*

Main category: cs.CV

TL;DR: 提出了Frame Selection Augmented Generation (FRAG)方法，通过独立评分选择输入中的相关帧，仅基于这些帧生成输出，避免了长上下文处理的计算成本。该方法适用于长视频和多页文档，无需微调即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）在处理长输入（如多页文档和长视频）时，模型规模和性能受限于训练和推理的计算成本。FRAG探索了一种无需长上下文处理的替代方向。

Method: FRAG通过独立评分每个帧并选择得分最高的Top-K帧，仅基于这些帧生成最终输出，避免了长上下文处理的需求。

Result: 在长视频和多页文档理解任务中，FRAG显著提升了性能。例如，在视频任务中提升了InternVL2-76B模型5.8%和3.7%；在文档任务中比专门的长文档理解模型提升了20%以上。

Conclusion: FRAG通过简单但高效的帧选择方法，显著提升了多模态模型在长输入任务中的性能，且无需额外微调。

Abstract: There has been impressive progress in Large Multimodal Models (LMMs). Recent
works extend these models to long inputs, including multi-page documents and
long videos. However, the model size and performance of these long context
models are still limited due to the computational cost in both training and
inference. In this work, we explore an orthogonal direction and process long
inputs without long context LMMs. We propose Frame Selection Augmented
Generation (FRAG), where the model first selects relevant frames within the
input, and then only generates the final outputs based on the selected frames.
The core of the selection process is done by scoring each frame independently,
which does not require long context processing. The frames with the highest
scores are then selected by a simple Top-K selection. We show that this
frustratingly simple framework is applicable to both long videos and multi-page
documents using existing LMMs without any fine-tuning. We consider two models,
LLaVA-OneVision and InternVL2, in our experiments and show that FRAG
consistently improves the performance and achieves state-of-the-art
performances for both long video and long document understanding. For videos,
FRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on
Video-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA
compared with recent LMMs specialized in long document understanding. Code is
available at: https://github.com/NVlabs/FRAG

</details>


### [307] [Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data](https://arxiv.org/abs/2504.17474)
*Weiran Pan,Wei Wei,Feida Zhu,Yong Deng*

Main category: cs.CV

TL;DR: 提出了一种新的样本选择方法，用于在标签噪声存在的情况下进行图像分类，通过分析模型预测置信度趋势而非仅依赖损失值，解决了传统方法在权衡精确度和召回率时的困境。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将小损失样本视为正确标签，但某些难学习的正确标签样本在训练初期可能表现出与错误标签样本类似的高损失，导致样本选择时在精确度和召回率之间难以权衡。

Method: 通过跟踪训练期间标注标签与其他类别之间的置信度差距，并使用Mann-Kendall检验评估其趋势，判断样本是否为正确标签。此方法可作为即插即用组件与现有样本选择技术结合。

Result: 在多个标准基准和真实数据集上的实验表明，该方法显著提升了现有噪声标签学习方法的性能。

Conclusion: 通过关注置信度趋势而非损失值，该方法更准确地区分正确标签的难学习样本与错误标签样本，有效缓解了样本选择中的权衡问题。

Abstract: We propose a novel sample selection method for image classification in the
presence of noisy labels. Existing methods typically consider small-loss
samples as correctly labeled. However, some correctly labeled samples are
inherently difficult for the model to learn and can exhibit high loss similar
to mislabeled samples in the early stages of training. Consequently, setting a
threshold on per-sample loss to select correct labels results in a trade-off
between precision and recall in sample selection: a lower threshold may miss
many correctly labeled hard-to-learn samples (low recall), while a higher
threshold may include many mislabeled samples (low precision). To address this
issue, our goal is to accurately distinguish correctly labeled yet
hard-to-learn samples from mislabeled ones, thus alleviating the trade-off
dilemma. We achieve this by considering the trends in model prediction
confidence rather than relying solely on loss values. Empirical observations
show that only for correctly labeled samples, the model's prediction confidence
for the annotated labels typically increases faster than for any other classes.
Based on this insight, we propose tracking the confidence gaps between the
annotated labels and other classes during training and evaluating their trends
using the Mann-Kendall Test. A sample is considered potentially correctly
labeled if all its confidence gaps tend to increase. Our method functions as a
plug-and-play component that can be seamlessly integrated into existing sample
selection techniques. Experiments on several standard benchmarks and real-world
datasets demonstrate that our method enhances the performance of existing
methods for learning with noisy labels.

</details>


### [308] [An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm](https://arxiv.org/abs/2504.17540)
*Ahmadreza Shateri,Negar Nourani,Morteza Dorrigiv,Hamid Nasiri*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的自动检测猴痘皮肤病变图像的框架，结合Xception架构、PCA降维和NGBoost分类算法，并使用AVOA优化超参数，实现了97.53%的准确率。


<details>
  <summary>Details</summary>
Motivation: 猴痘在非历史流行区的全球传播引发公共卫生担忧，早期精确诊断对疾病管理至关重要。

Method: 使用Xception提取特征，PCA降维，NGBoost分类，AVOA优化超参数，并通过MSLD数据集训练和评估模型。

Result: 模型达到97.53%准确率、97.72% F1-score和97.47% AUC，并通过Grad-CAM和LIME增强可解释性。

Conclusion: 该框架提供了一种高精度、高效的诊断工具，有助于资源有限环境中的早期检测。

Abstract: The recent global spread of monkeypox, particularly in regions where it has
not historically been prevalent, has raised significant public health concerns.
Early and accurate diagnosis is critical for effective disease management and
control. In response, this study proposes a novel deep learning-based framework
for the automated detection of monkeypox from skin lesion images, leveraging
the power of transfer learning, dimensionality reduction, and advanced machine
learning techniques. We utilize the newly developed Monkeypox Skin Lesion
Dataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to
train and evaluate our models. The proposed framework employs the Xception
architecture for deep feature extraction, followed by Principal Component
Analysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting
(NGBoost) algorithm for classification. To optimize the model's performance and
generalization, we introduce the African Vultures Optimization Algorithm (AVOA)
for hyperparameter tuning, ensuring efficient exploration of the parameter
space. Our results demonstrate that the proposed AVOA-NGBoost model achieves
state-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%
and an AUC of 97.47%. Additionally, we enhance model interpretability using
Grad-CAM and LIME techniques, providing insights into the decision-making
process and highlighting key features influencing classification. This
framework offers a highly precise and efficient diagnostic tool, potentially
aiding healthcare providers in early detection and diagnosis, particularly in
resource-constrained environments.

</details>


### [309] [A Genealogy of Multi-Sensor Foundation Models in Remote Sensing](https://arxiv.org/abs/2504.17177)
*Kevin Lane,Morteza Karimzadeh*

Main category: cs.CV

TL;DR: 该论文综述了遥感领域基础模型的现状，探讨了其与计算机视觉方法的异同，分析了多种方法的优缺点，并强调了多传感器数据的利用及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 遥感领域的基础模型发展迅速，但大多直接借鉴计算机视觉方法，缺乏针对性的优化。本文旨在探讨这些方法的优缺点，并提出改进方向以更好地适应遥感数据的特性（如多传感器、季节性）。

Method: 通过分析现有基础模型方法及其在计算机视觉中的根源，结合遥感数据的多传感器特性，评估了不同方法的适用性与局限性。

Result: 总结了当前方法的优势（如通用性）与不足（如计算资源需求高），提出了利用未标记数据、多传感器融合等未来研究方向。

Conclusion: 遥感领域的基础模型需进一步结合领域特性优化，尤其是多传感器和季节性数据的利用，是未来提升模型性能的关键。

Abstract: Foundation models have garnered increasing attention for representation
learning in remote sensing, primarily adopting approaches that have
demonstrated success in computer vision with minimal domain-specific
modification. However, the development and application of foundation models in
this field are still burgeoning, as there are a variety of competing approaches
that each come with significant benefits and drawbacks. This paper examines
these approaches along with their roots in the computer vision field in order
to characterize potential advantages and pitfalls while outlining future
directions to further improve remote sensing-specific foundation models. We
discuss the quality of the learned representations and methods to alleviate the
need for massive compute resources. We place emphasis on the multi-sensor
aspect of Earth observations, and the extent to which existing approaches
leverage multiple sensors in training foundation models in relation to
multi-modal foundation models. Finally, we identify opportunities for further
harnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote
sensing observations.

</details>


### [310] [Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior](https://arxiv.org/abs/2504.17551)
*Lin Che,Yizi Chen,Tanhua Jin,Martin Raubal,Konrad Schindler,Peter Kiefer*

Main category: cs.CV

TL;DR: 论文提出了一种无监督对比聚类模型，结合地理先验，用于街景图像的土地利用分类，提高了聚类效果，适用于多种城市环境。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感技术在复杂城市环境中精度不足，而基于街景图像的监督分类方法受限于高质量标注数据的稀缺和泛化能力差。本文旨在解决这些问题。

Method: 采用无监督对比聚类模型，结合地理先验信息，通过简单的视觉分配实现土地利用分类。

Result: 实验表明，该方法可以从两个城市的街景图像数据集中生成土地利用地图，适用于多种场景。

Conclusion: 该方法基于地理数据的空间一致性，可实现可扩展的无监督土地利用制图和更新，为城市规划提供了灵活的自定义解决方案。

Abstract: Urban land use classification and mapping are critical for urban planning,
resource management, and environmental monitoring. Existing remote sensing
techniques often lack precision in complex urban environments due to the
absence of ground-level details. Unlike aerial perspectives, street view images
provide a ground-level view that captures more human and social activities
relevant to land use in complex urban scenes. Existing street view-based
methods primarily rely on supervised classification, which is challenged by the
scarcity of high-quality labeled data and the difficulty of generalizing across
diverse urban landscapes. This study introduces an unsupervised contrastive
clustering model for street view images with a built-in geographical prior, to
enhance clustering performance. When combined with a simple visual assignment
of the clusters, our approach offers a flexible and customizable solution to
land use mapping, tailored to the specific needs of urban planners. We
experimentally show that our method can generate land use maps from geotagged
street view image datasets of two cities. As our methodology relies on the
universal spatial coherence of geospatial data ("Tobler's law"), it can be
adapted to various settings where street view images are available, to enable
scalable, unsupervised land use mapping and updating. The code will be
available at https://github.com/lin102/CCGP.

</details>


### [311] [STCL:Curriculum learning Strategies for deep learning image steganography models](https://arxiv.org/abs/2504.17609)
*Fengchun Liu,Tong Zhang,Chunying Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的图像隐写术训练策略STCL，通过从易到难的渐进学习，提升隐写图像质量和模型收敛速度，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对基于深度学习的图像隐写术模型存在的隐写图像质量差和网络收敛慢问题，本文旨在通过渐进学习策略优化训练过程。

Method: 提出了STCL训练策略，包括基于教师模型的难度评估和基于拐点的训练调度策略，逐步选择从易到难的图像进行训练。

Result: 在ALASKA2、VOC2012和ImageNet等数据集上实验表明，STCL策略能显著提升模型性能，隐写图像质量高且抗检测性强。

Conclusion: STCL策略有效解决了深度学习图像隐写术的训练挑战，显著提升了隐写图像质量和模型性能。

Abstract: Aiming at the problems of poor quality of steganographic images and slow
network convergence of image steganography models based on deep learning, this
paper proposes a Steganography Curriculum Learning training strategy (STCL) for
deep learning image steganography models. So that only easy images are selected
for training when the model has poor fitting ability at the initial stage, and
gradually expand to more difficult images, the strategy includes a difficulty
evaluation strategy based on the teacher model and an knee point-based training
scheduling strategy. Firstly, multiple teacher models are trained, and the
consistency of the quality of steganographic images under multiple teacher
models is used as the difficulty score to construct the training subsets from
easy to difficult. Secondly, a training control strategy based on knee points
is proposed to reduce the possibility of overfitting on small training sets and
accelerate the training process. Experimental results on three large public
datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image
steganography scheme is able to improve the model performance under multiple
algorithmic frameworks, which not only has a high PSNR, SSIM score, and
decoding accuracy, but also the steganographic images generated by the model
under the training of the STCL strategy have a low steganography analysis
scores. You can find our code at
\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.

</details>


### [312] [Enhancing CNNs robustness to occlusions with bioinspired filters for border completion](https://arxiv.org/abs/2504.17619)
*Catarina P. Coutinho,Aneeqa Merhab,Janko Petkovic,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.CV

TL;DR: 论文通过模拟视觉皮层机制改进CNN滤波器，提高了LeNet 5在遮挡MNIST图像上的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用视觉皮层的数学建模来增强CNN的性能，特别是在处理遮挡图像时。

Method: 基于视觉皮层的边界完成机制设计自定义滤波器，并将其整合到改进的LeNet 5架构中。

Result: 改进后的模型在处理遮挡MNIST图像时表现出更高的准确性。

Conclusion: 模拟生物视觉机制可以有效地提升CNN在处理部分遮挡图像时的性能。

Abstract: We exploit the mathematical modeling of the visual cortex mechanism for
border completion to define custom filters for CNNs. We see a consistent
improvement in performance, particularly in accuracy, when our modified LeNet 5
is tested with occluded MNIST images.

</details>


### [313] [Hierarchical and Multimodal Data for Daily Activity Understanding](https://arxiv.org/abs/2504.17696)
*Ghazal Kaviani,Yavuz Yarici,Seulgi Kim,Mohit Prabhushankar,Ghassan AlRegib,Mashhour Solh,Ameya Patil*

Main category: cs.CV

TL;DR: DARai是一个多模态、层次化标注的数据集，用于理解真实环境中的人类活动，包含超过200小时的传感器数据，并设计了层次化标注以捕捉活动复杂性。实验展示了其在多模态融合和领域变异任务中的价值。


<details>
  <summary>Details</summary>
Motivation: 构建DARai旨在通过多传感器数据和层次化标注，解决理解复杂人类活动的挑战，尤其是在真实环境中的应用。

Method: 数据集包含50名参与者在10种环境中的连续录制数据，标注分为三个层次：高层次活动、低层次动作和细粒度步骤。实验包括单模态和多模态传感器融合任务。

Result: 实验展示了DARai在识别、时间定位和未来动作预测任务中的有效性，并揭示了单一传感器的局限性。

Conclusion: DARai为人类中心应用提供了丰富的数据资源，支持多模态和领域变异研究，数据集和代码已公开。

Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced
"Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to
understand human activities in real-world settings. DARai consists of
continuous scripted and unscripted recordings of 50 participants in 10
different environments, totaling over 200 hours of data from 20 sensors
including multiple camera views, depth and radar sensors, wearable inertial
measurement units (IMUs), electromyography (EMG), insole pressure sensors,
biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three
levels of hierarchy: (i) high-level activities (L1) that are independent tasks,
(ii) lower-level actions (L2) that are patterns shared between activities, and
(iii) fine-grained procedures (L3) that detail the exact execution steps for
actions. The dataset annotations and recordings are designed so that 22.7% of
L2 actions are shared between L1 activities and 14.2% of L3 procedures are
shared between L2 actions. The overlap and unscripted nature of DARai allows
counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai
in uncovering important challenges in human-centered applications.
Specifically, we conduct unimodal and multimodal sensor fusion experiments for
recognition, temporal localization, and future action anticipation across all
hierarchical annotation levels. To highlight the limitations of individual
sensors, we also conduct domain-variant experiments that are enabled by DARai's
multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai
website:
https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/

</details>


### [314] [EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor](https://arxiv.org/abs/2504.17735)
*Akhil Padmanabha,Saravanan Govindarajan,Hwanmun Kim,Sergio Ortiz,Rahul Rajan,Doruk Senkal,Sneha Kadetotad*

Main category: cs.CV

TL;DR: 提出了一种资源高效的机器学习算法EgoCHARM，通过单一头戴式IMU识别高低层活动，采用半监督学习策略，在高低层活动识别上表现优异且参数少。


<details>
  <summary>Details</summary>
Motivation: 当前以自我为中心的活动识别方法性能低或资源消耗大，因此需要一种更高效的算法。

Method: 使用层次化半监督学习策略，基于高层面活动标签训练通用低层面运动嵌入。

Result: 在9种高层面和3种低层面活动识别上，F1分数分别为0.826和0.855，仅需少量参数。

Conclusion: EgoCHARM展示了使用头戴式IMU进行活动识别的潜力，同时分析并指出了其局限性。

Abstract: Human activity recognition (HAR) on smartglasses has various use cases,
including health/fitness tracking and input for context-aware AI assistants.
However, current approaches for egocentric activity recognition suffer from low
performance or are resource-intensive. In this work, we introduce a resource
(memory, compute, power, sample) efficient machine learning algorithm,
EgoCHARM, for recognizing both high level and low level activities using a
single egocentric (head-mounted) Inertial Measurement Unit (IMU). Our
hierarchical algorithm employs a semi-supervised learning strategy, requiring
primarily high level activity labels for training, to learn generalizable low
level motion embeddings that can be effectively utilized for low level activity
recognition. We evaluate our method on 9 high level and 3 low level activities
achieving 0.826 and 0.855 F1 scores on high level and low level activity
recognition respectively, with just 63k high level and 22k low level model
parameters, allowing the low level encoder to be deployed directly on current
IMU chips with compute. Lastly, we present results and insights from a
sensitivity analysis and highlight the opportunities and limitations of
activity recognition using egocentric IMUs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [315] [Intrinsic Barriers to Explaining Deep Foundation Models](https://arxiv.org/abs/2504.16948)
*Zhen Tan,Huan Liu*

Main category: cs.CY

TL;DR: 该论文探讨了深度基础模型（DFMs）的可解释性挑战是否源于其内在复杂性，并分析了当前解释方法的局限性及其对技术验证与治理的影响。


<details>
  <summary>Details</summary>
Motivation: 随着深度基础模型（DFMs）的能力不断增强，其复杂性也使得理解其内部工作机制变得愈发困难，这直接关系到模型的信任、安全与问责。论文旨在探索这种挑战是暂时的还是本质性的。

Method: 研究通过分析DFMs的基本特性，并结合当前可解释性方法在处理这些模型时的局限性，探讨了解释的可行性。

Result: 研究揭示了DFMs的内在复杂性可能是解释难度的一个本质性障碍，而现有的解释方法在面对这一挑战时存在明显的不足。

Conclusion: 论文强调，必须重新思考如何验证和治理这些强大技术，可能需要开发全新的解释性方法或从根本上调整对模型可解释性的期望。

Abstract: Deep Foundation Models (DFMs) offer unprecedented capabilities but their
increasing complexity presents profound challenges to understanding their
internal workings-a critical need for ensuring trust, safety, and
accountability. As we grapple with explaining these systems, a fundamental
question emerges: Are the difficulties we face merely temporary hurdles,
awaiting more sophisticated analytical techniques, or do they stem from
\emph{intrinsic barriers} deeply rooted in the nature of these large-scale
models themselves? This paper delves into this critical question by examining
the fundamental characteristics of DFMs and scrutinizing the limitations
encountered by current explainability methods when confronted with this
inherent challenge. We probe the feasibility of achieving satisfactory
explanations and consider the implications for how we must approach the
verification and governance of these powerful technologies.

</details>


### [316] [Approaches to Responsible Governance of GenAI in Organizations](https://arxiv.org/abs/2504.17044)
*Dhari Gandhi,Himanshu Joshi,Lucas Hartman,Shabnam Hassani*

Main category: cs.CY

TL;DR: 论文探讨了如何在快速发展的生成式AI（GenAI）中实施负责任的治理框架，提出了兼顾创新与监督的行动建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，其伦理、责任和社会影响带来的挑战日益凸显，需要建立有效的治理机制。

Method: 通过文献综述、现有治理框架分析和行业圆桌讨论，提炼出核心治理原则并制定实用指南（ResAI）。

Result: 研究强调适应性风险评估工具、持续监控和跨领域协作的重要性，并提供了可操作的治理建议。

Conclusion: 本文为组织提供了一个结构化的基础（ResAI指南），以将GenAI与伦理、法律和最佳实践对齐。

Abstract: The rapid evolution of Generative AI (GenAI) has introduced unprecedented
opportunities while presenting complex challenges around ethics,
accountability, and societal impact. This paper draws on a literature review,
established governance frameworks, and industry roundtable discussions to
identify core principles for integrating responsible GenAI governance into
diverse organizational structures. Our objective is to provide actionable
recommendations for a balanced, risk-based governance approach that enables
both innovation and oversight. Findings emphasize the need for adaptable risk
assessment tools, continuous monitoring practices, and cross-sector
collaboration to establish trustworthy GenAI. These insights provide a
structured foundation and Responsible GenAI Guide (ResAI) for organizations to
align GenAI initiatives with ethical, legal, and operational best practices.

</details>


### [317] [Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement](https://arxiv.org/abs/2504.17393)
*Vesna Nowack,Dalal Alrajeh,Carolina Gutierrez Muñoz,Katie Thomas,William Hobson,Catherine Hamilton-Giachritsis,Patrick Benjamin,Tim Grant,Juliane A. Kloess,Jessica Woodhams*

Main category: cs.CY

TL;DR: 该论文探讨了AI在法律执法领域的应用，重点关注用户需求与系统设计，强调人类在系统中的关键角色和系统无法完全自动化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补AI辅助法律执法系统设计中用户需求不明确的空白，并理解人类在此类系统中的责任与期望。

Method: 通过定性研究，分析法律执法机构的决策过程，识别现有实践的局限性，并探索用户需求。

Result: 研究发现系统需高效处理数据、满足可扩展性、准确性等需求，同时强调人类参与数据验证和系统适应的重要性。

Conclusion: 由于法律执法领域的动态复杂性，系统无法完全自动化，人机协作是关键。

Abstract: Artificial Intelligence (AI) has become an important part of our everyday
lives, yet user requirements for designing AI-assisted systems in law
enforcement remain unclear. To address this gap, we conducted qualitative
research on decision-making within a law enforcement agency. Our study aimed to
identify limitations of existing practices, explore user requirements and
understand the responsibilities that humans expect to undertake in these
systems.
  Participants in our study highlighted the need for a system capable of
processing and analysing large volumes of data efficiently to help in crime
detection and prevention. Additionally, the system should satisfy requirements
for scalability, accuracy, justification, trustworthiness and adaptability to
be adopted in this domain. Participants also emphasised the importance of
having end users review the input data that might be challenging for AI to
interpret, and validate the generated output to ensure the system's accuracy.
To keep up with the evolving nature of the law enforcement domain, end users
need to help the system adapt to the changes in criminal behaviour and
government guidance, and technical experts need to regularly oversee and
monitor the system. Furthermore, user-friendly human interaction with the
system is essential for its adoption and some of the participants confirmed
they would be happy to be in the loop and provide necessary feedback that the
system can learn from. Finally, we argue that it is very unlikely that the
system will ever achieve full automation due to the dynamic and complex nature
of the law enforcement domain.

</details>


### [318] [Flexibility of German gas-fired generation: evidence from clustering empirical operation](https://arxiv.org/abs/2504.16943)
*Chiara Fusar Bassini,Alice Lixuan Xu,Jorge Sánchez Canales,Lion Hirth,Lynn H. Kaack*

Main category: cs.CY

TL;DR: 论文通过深度学习聚类德国60%以上的大型天然气发电机组，根据其实际灵活性分为两类调峰机组和两类非调峰机组。非调峰机组灵活性较低，占样本中83%必须运行发电量，需监管改革以提升市场响应。


<details>
  <summary>Details</summary>
Motivation: 能源模型中发电机组灵活性假设通常基于技术特征，但实际运行中服务义务和市场激励可能限制其灵活性。论文旨在通过实证分析揭示实际灵活性与技术假设的差异。

Method: 采用深度学习处理2019-2023年小时级发电数据，将时间序列转化为易聚类表征，对德国100MWp以上天然气机组进行聚类。

Result: 识别出两类调峰机组和两类非调峰机组，非调峰机组（占50%样本）灵活性较低，贡献了83%必须运行发电量。

Conclusion: 需通过监管改革解决非调峰机组市场响应不足的问题，以释放其灵活性潜力。

Abstract: A key input to energy models are assumptions about the flexibility of power
generation units, i.e., how quickly and often they can start up. These
assumptions are usually calibrated on the technical characteristics of the
units, such as installed capacity or technology type. However, even if power
generation units technically can dispatch flexibly, service obligations and
market incentives may constrain their operation. Here, we cluster over 60% of
German national gas generation (generation units of 100 MWp or above) based on
their empirical flexibility. We process the hourly dispatch of sample units
between 2019 and 2023 using a novel deep learning approach, that transforms
time series into easy-to-cluster representations. We identify two clusters of
peaker units and two clusters of non-peaker units, whose different empirical
flexibility is quantified by cluster-level ramp rates. Non-peaker units, around
half of the sample, are empirically less flexible than peakers, and make up for
more than 83% of sample must-run generation. Regulatory changes addressing the
low market responsiveness of non-peakers are needed to unlock their
flexibility.

</details>


### [319] [Engineering the Law-Machine Learning Translation Problem: Developing Legally Aligned Models](https://arxiv.org/abs/2504.16969)
*Mathias Hanson,Gregory Lewkowicz,Sam Verboven*

Main category: cs.CY

TL;DR: 该论文提出了一个五阶段跨学科框架，以在机器学习模型开发中整合法律与技术分析，解决法律遵从性与预测性能之间的复杂权衡问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术的发展需要在遵守法律的同时保持高预测性能，但传统方法未能充分解决法律与技术的交叉复杂性。

Method: 引入一个五阶段跨学科框架，结合法律与机器学习技术分析，指导模型开发中的法律操作化和评估指标选择。

Result: 框架能够帮助设计法律合规且高性能的机器学习模型，并通过反洗钱案例研究验证其有效性。

Conclusion: 该框架填补了法律概念分析与机器学习确定性需求之间的空白，为法律与技术的协同提供了实践路径。

Abstract: Organizations developing machine learning-based (ML) technologies face the
complex challenge of achieving high predictive performance while respecting the
law. This intersection between ML and the law creates new complexities. As ML
model behavior is inferred from training data, legal obligations cannot be
operationalized in source code directly. Rather, legal obligations require
"indirect" operationalization. However, choosing context-appropriate
operationalizations presents two compounding challenges: (1) laws often permit
multiple valid operationalizations for a given legal obligation-each with
varying degrees of legal adequacy; and, (2) each operationalization creates
unpredictable trade-offs among the different legal obligations and with
predictive performance. Evaluating these trade-offs requires metrics (or
heuristics), which are in turn difficult to validate against legal obligations.
Current methodologies fail to fully address these interwoven challenges as they
either focus on legal compliance for traditional software or on ML model
development without adequately considering legal complexities. In response, we
introduce a five-stage interdisciplinary framework that integrates legal and
ML-technical analysis during ML model development. This framework facilitates
designing ML models in a legally aligned way and identifying high-performing
models that are legally justifiable. Legal reasoning guides choices for
operationalizations and evaluation metrics, while ML experts ensure technical
feasibility, performance optimization and an accurate interpretation of metric
values. This framework bridges the gap between more conceptual analysis of law
and ML models' need for deterministic specifications. We illustrate its
application using a case study in the context of anti-money laundering.

</details>


### [320] [Intrinsic Barriers to Explaining Deep Foundation Models](https://arxiv.org/abs/2504.16948)
*Zhen Tan,Huan Liu*

Main category: cs.CY

TL;DR: 探讨深度基础模型（DFMs）解释性的挑战是暂时性还是内在性的，分析当前方法局限性及其对技术验证与治理的影响。


<details>
  <summary>Details</summary>
Motivation: 随着DFMs能力的提升，理解其内部机制以确保信任、安全和问责变得至关重要。论文意在区分解释难度是暂时的技术限制还是模型本质的固有障碍。

Method: 通过分析DFMs的基本特性和当前可解释性方法在应对这些特性时的局限性，探究获得满意解释的可行性。

Result: 指出现有方法面临DFMs内在复杂性时的不足，强调解释难度的根源可能深植于模型自身的大规模特性中。

Conclusion: 呼吁重新思考DFMs的验证与治理策略，正视其内在解释障碍，推动更适应大规模模型特性的新方法发展。

Abstract: Deep Foundation Models (DFMs) offer unprecedented capabilities but their
increasing complexity presents profound challenges to understanding their
internal workings-a critical need for ensuring trust, safety, and
accountability. As we grapple with explaining these systems, a fundamental
question emerges: Are the difficulties we face merely temporary hurdles,
awaiting more sophisticated analytical techniques, or do they stem from
\emph{intrinsic barriers} deeply rooted in the nature of these large-scale
models themselves? This paper delves into this critical question by examining
the fundamental characteristics of DFMs and scrutinizing the limitations
encountered by current explainability methods when confronted with this
inherent challenge. We probe the feasibility of achieving satisfactory
explanations and consider the implications for how we must approach the
verification and governance of these powerful technologies.

</details>


### [321] [Approaches to Responsible Governance of GenAI in Organizations](https://arxiv.org/abs/2504.17044)
*Dhari Gandhi,Himanshu Joshi,Lucas Hartman,Shabnam Hassani*

Main category: cs.CY

TL;DR: 论文提出了一个平衡创新与监管的风险治理框架（ResAI），用于指导生成式AI的伦理和合规实践。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展带来了伦理和责任挑战，需要一种兼顾创新和监管的治理方法。

Method: 通过文献综述、现有治理框架和行业圆桌讨论，提炼核心原则并设计ResAI指南。

Result: 提出了适应性风险评估工具、持续监控机制和跨领域协作，为组织提供实践基础。

Conclusion: ResAI为组织提供结构化指南，确保生成式AI符合伦理、法律和操作最佳实践。

Abstract: The rapid evolution of Generative AI (GenAI) has introduced unprecedented
opportunities while presenting complex challenges around ethics,
accountability, and societal impact. This paper draws on a literature review,
established governance frameworks, and industry roundtable discussions to
identify core principles for integrating responsible GenAI governance into
diverse organizational structures. Our objective is to provide actionable
recommendations for a balanced, risk-based governance approach that enables
both innovation and oversight. Findings emphasize the need for adaptable risk
assessment tools, continuous monitoring practices, and cross-sector
collaboration to establish trustworthy GenAI. These insights provide a
structured foundation and Responsible GenAI Guide (ResAI) for organizations to
align GenAI initiatives with ethical, legal, and operational best practices.

</details>


### [322] [Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement](https://arxiv.org/abs/2504.17393)
*Vesna Nowack,Dalal Alrajeh,Carolina Gutierrez Muñoz,Katie Thomas,William Hobson,Catherine Hamilton-Giachritsis,Patrick Benjamin,Tim Grant,Juliane A. Kloess,Jessica Woodhams*

Main category: cs.CY

TL;DR: 论文研究了AI在执法领域的使用需求，发现用户对系统的高效性、可扩展性、准确性等有较高要求，并强调人工监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索执法领域中AI辅助系统的用户需求，填补现有实践中的不足。

Method: 通过定性研究，分析了执法机构的决策过程，收集用户对AI系统的需求。

Result: 用户需要高效处理大数据的系统，同时强调人工验证和系统适应性，认为完全自动化难以实现。

Conclusion: AI在执法领域需结合人工监督与用户反馈，完全自动化可能性低。

Abstract: Artificial Intelligence (AI) has become an important part of our everyday
lives, yet user requirements for designing AI-assisted systems in law
enforcement remain unclear. To address this gap, we conducted qualitative
research on decision-making within a law enforcement agency. Our study aimed to
identify limitations of existing practices, explore user requirements and
understand the responsibilities that humans expect to undertake in these
systems.
  Participants in our study highlighted the need for a system capable of
processing and analysing large volumes of data efficiently to help in crime
detection and prevention. Additionally, the system should satisfy requirements
for scalability, accuracy, justification, trustworthiness and adaptability to
be adopted in this domain. Participants also emphasised the importance of
having end users review the input data that might be challenging for AI to
interpret, and validate the generated output to ensure the system's accuracy.
To keep up with the evolving nature of the law enforcement domain, end users
need to help the system adapt to the changes in criminal behaviour and
government guidance, and technical experts need to regularly oversee and
monitor the system. Furthermore, user-friendly human interaction with the
system is essential for its adoption and some of the participants confirmed
they would be happy to be in the loop and provide necessary feedback that the
system can learn from. Finally, we argue that it is very unlikely that the
system will ever achieve full automation due to the dynamic and complex nature
of the law enforcement domain.

</details>


### [323] [Flexibility of German gas-fired generation: evidence from clustering empirical operation](https://arxiv.org/abs/2504.16943)
*Chiara Fusar Bassini,Alice Lixuan Xu,Jorge Sánchez Canales,Lion Hirth,Lynn H. Kaack*

Main category: cs.CY

TL;DR: 论文摘要指出，即便发电机组技术允许灵活运行，实际运行中可能受制于服务义务和市场激励。作者使用深度学习聚类德国天然气发电机组，量化其灵活性差异，并提出监管改革建议以提升非峰值机组的市场响应能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示发电机组实际灵活性与技术潜力间的差距，强调仅依赖技术特征校准能源模型可能低估运行约束。通过实证分析，填补市场激励对灵活性影响的认知空白。

Method: 方法包括：1) 基于德国100MW以上天然气机组的每小时调度数据(2019-2023)；2) 使用深度学习将时间序列转化为可聚类表征；3) 通过聚类识别峰值与非峰值机组，并计算集群级爬坡率。

Result: 结果显示：1) 样本中约50%为非峰值机组，其灵活性显著低于峰值机组；2) 非峰值机组贡献了83%以上的必须运行发电量；3) 两类机组的爬坡率差异量化了灵活性差距。

Conclusion: 结论认为现有市场机制限制了非峰值机组的灵活性潜力，需通过监管改革（如调整服务义务或激励政策）释放其灵活运行能力，以优化电力系统调度。

Abstract: A key input to energy models are assumptions about the flexibility of power
generation units, i.e., how quickly and often they can start up. These
assumptions are usually calibrated on the technical characteristics of the
units, such as installed capacity or technology type. However, even if power
generation units technically can dispatch flexibly, service obligations and
market incentives may constrain their operation. Here, we cluster over 60% of
German national gas generation (generation units of 100 MWp or above) based on
their empirical flexibility. We process the hourly dispatch of sample units
between 2019 and 2023 using a novel deep learning approach, that transforms
time series into easy-to-cluster representations. We identify two clusters of
peaker units and two clusters of non-peaker units, whose different empirical
flexibility is quantified by cluster-level ramp rates. Non-peaker units, around
half of the sample, are empirically less flexible than peakers, and make up for
more than 83% of sample must-run generation. Regulatory changes addressing the
low market responsiveness of non-peakers are needed to unlock their
flexibility.

</details>


### [324] [Engineering the Law-Machine Learning Translation Problem: Developing Legally Aligned Models](https://arxiv.org/abs/2504.16969)
*Mathias Hanson,Gregory Lewkowicz,Sam Verboven*

Main category: cs.CY

TL;DR: 论文提出了一个五阶段跨学科框架，旨在帮助组织在开发机器学习技术时实现高预测性能并遵守法律。该框架结合法律与机器学习技术分析，解决法律合规性与模型性能间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术的快速发展带来了法律合规性的新挑战。传统方法未能充分解决法律与机器学习模型开发之间的复杂关系，需要一种新框架来平衡法律要求与模型性能。

Method: 论文提出了一个五阶段跨学科框架，整合法律与机器学习技术分析，指导模型开发中的法律合规性操作化及性能优化。通过案例研究（反洗钱领域）验证了方法的实用性。

Result: 该框架成功帮助设计出法律合规且性能优越的机器学习模型，并通过案例研究展示了其在具体应用中的有效性。

Conclusion: 该框架为机器学习模型开发提供了一种系统的方法，能够在法律合规性与技术性能之间取得平衡，适用于实际场景中的复杂需求。

Abstract: Organizations developing machine learning-based (ML) technologies face the
complex challenge of achieving high predictive performance while respecting the
law. This intersection between ML and the law creates new complexities. As ML
model behavior is inferred from training data, legal obligations cannot be
operationalized in source code directly. Rather, legal obligations require
"indirect" operationalization. However, choosing context-appropriate
operationalizations presents two compounding challenges: (1) laws often permit
multiple valid operationalizations for a given legal obligation-each with
varying degrees of legal adequacy; and, (2) each operationalization creates
unpredictable trade-offs among the different legal obligations and with
predictive performance. Evaluating these trade-offs requires metrics (or
heuristics), which are in turn difficult to validate against legal obligations.
Current methodologies fail to fully address these interwoven challenges as they
either focus on legal compliance for traditional software or on ML model
development without adequately considering legal complexities. In response, we
introduce a five-stage interdisciplinary framework that integrates legal and
ML-technical analysis during ML model development. This framework facilitates
designing ML models in a legally aligned way and identifying high-performing
models that are legally justifiable. Legal reasoning guides choices for
operationalizations and evaluation metrics, while ML experts ensure technical
feasibility, performance optimization and an accurate interpretation of metric
values. This framework bridges the gap between more conceptual analysis of law
and ML models' need for deterministic specifications. We illustrate its
application using a case study in the context of anti-money laundering.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [325] [Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems](https://arxiv.org/abs/2504.17354)
*Tarik Sahin,Jacopo Bonari,Sebastian Brandstaeter,Alexander Popp*

Main category: cs.CE

TL;DR: 提出了一种基于数据驱动的代理模型框架，用于快速预测粗糙表面接触的有效接触面积，解决了传统数值方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 粗糙表面接触的有效接触面积在磨损、密封和热或电传导等多物理现象中至关重要，但现有数值方法计算成本高，限制了其在多查询场景中的应用。

Method: 训练多种机器学习算法（如Kernel Ridge Regressor和Gaussian Process Regressor）在预计算数据集上，输入为载荷和粗糙度参数，输出为有效接触面积，并进行超参数优化以比较准确性和效率。

Result: Kernel Ridge Regressor在准确性和效率之间取得了最佳平衡，适合通用代理建模；Gaussian Process Regressor适用于需要不确定性量化的场景。模型在新仿真场景中验证了泛化能力。

Conclusion: 代理建模方法在多查询任务中实用且高效，尽管数据库生成是主要成本。

Abstract: The effective contact area in rough surface contact plays a critical role in
multi-physics phenomena such as wear, sealing, and thermal or electrical
conduction. Although accurate numerical methods, like the Boundary Element
Method (BEM), are available to compute this quantity, their high computational
cost limits their applicability in multi-query contexts, such as uncertainty
quantification, parameter identification, and multi-scale algorithms, where
many repeated evaluations are required. This study proposes a surrogate
modeling framework for predicting the effective contact area using
fast-to-evaluate data-driven techniques. Various machine learning algorithms
are trained on a precomputed dataset, where the inputs are the imposed load and
statistical roughness parameters, and the output is the corresponding effective
contact area. All models undergo hyperparameter optimization to enable fair
comparisons in terms of predictive accuracy and computational efficiency,
evaluated using established quantitative metrics. Among the models, the Kernel
Ridge Regressor demonstrates the best trade-off between accuracy and
efficiency, achieving high predictive accuracy, low prediction time, and
minimal training overhead-making it a strong candidate for general-purpose
surrogate modeling. The Gaussian Process Regressor provides an attractive
alternative when uncertainty quantification is required, although it incurs
additional computational cost due to variance estimation. The generalization
capability of the Kernel Ridge model is validated on an unseen simulation
scenario, confirming its ability to transfer to new configurations. Database
generation constitutes the dominant cost in the surrogate modeling process.
Nevertheless, the approach proves practical and efficient for multi-query
tasks, even when accounting for this initial expense.

</details>


### [326] [polyGen: A Learning Framework for Atomic-level Polymer Structure Generation](https://arxiv.org/abs/2504.17656)
*Ayush Jain,Rampi Ramprasad*

Main category: cs.CE

TL;DR: 本文介绍了polyGen，一种专为生成合成聚合物3D结构的潜在扩散模型，填补了该领域的空白。通过利用分子编码和有限的数据集训练，polyGen能够生成线性和支化结构，但对高原子数重复单元的处理仍有不足。


<details>
  <summary>Details</summary>
Motivation: 合成聚合物的3D结构生成在多个技术领域具有重要意义，但目前缺乏专门的生成方法。polyGen旨在解决这一挑战，成为首个专注于合成聚合物结构的生成模型。

Method: polyGen采用潜在扩散模型，通过分子编码捕获聚合物连接的架构。由于数据集有限（3855 DFT优化结构），研究还结合了分子结构数据以提高训练效果，并设计了结构匹配标准进行基准测试。

Result: polyGen能够有效地生成了多样化的线性和支化结构，但对高原子数重复单元的表现有所下降。这是首个在原子水平上生成聚合物结构的模型，展现了处理结构灵活性的潜力。

Conclusion: polyGen为聚合物科学中的原子级结构生成提供了新的范例，尽管存在数据集的限制，但初步结果表明其在聚合物结构预测上的可行性，为未来研究奠定了基础。

Abstract: Synthetic polymeric materials underpin fundamental technologies in the
energy, electronics, consumer goods, and medical sectors, yet their development
still suffers from prolonged design timelines. Although polymer informatics
tools have supported speedup, polymer simulation protocols continue to face
significant challenges: on-demand generation of realistic 3D atomic structures
that respect the conformational diversity of polymer structures. Generative
algorithms for 3D structures of inorganic crystals, bio-polymers, and small
molecules exist, but have not addressed synthetic polymers. In this work, we
introduce polyGen, the first latent diffusion model designed specifically to
generate realistic polymer structures from minimal inputs such as the repeat
unit chemistry alone, leveraging a molecular encoding that captures polymer
connectivity throughout the architecture. Due to a scarce dataset of only 3855
DFT-optimized polymer structures, we augment our training with DFT-optimized
molecular structures, showing improvement in joint learning between similar
chemical structures. We also establish structure matching criteria to benchmark
our approach on this novel problem. polyGen effectively generates diverse
conformations of both linear chains and complex branched structures, though its
performance decreases when handling repeat units with a high atom count. Given
these initial results, polyGen represents a paradigm shift in atomic-level
structure generation for polymer science-the first proof-of-concept for
predicting realistic atomic-level polymer conformations while accounting for
their intrinsic structural flexibility.

</details>


### [327] [Data-Driven Surrogate Modeling Techniques to Predict the Effective Contact Area of Rough Surface Contact Problems](https://arxiv.org/abs/2504.17354)
*Tarik Sahin,Jacopo Bonari,Sebastian Brandstaeter,Alexander Popp*

Main category: cs.CE

TL;DR: 论文提出了一种使用机器学习快速预测粗糙表面有效接触面积的替代建模框架，其中Kernel Ridge Regressor在准确性和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管BEM等数值方法能准确计算粗糙表面的有效接触面积（对磨损、密封等多物理现象至关重要），但其高计算成本限制了在多查询场景中的应用。

Method: 通过训练多种机器学习算法（输入为载荷和粗糙参数，输出为有效接触面积），并进行超参数优化，比较预测准确性和计算效率。

Result: Kernel Ridge Regressor在准确性、预测时间和训练开销上表现最优；Gaussian Process Regressor适用于需不确定性量化的场景。模型在新场景中验证了泛化能力。

Conclusion: 替代建模方法在多查询任务中实用高效，尽管数据库生成是主要成本，但整体优于传统高成本数值方法。

Abstract: The effective contact area in rough surface contact plays a critical role in
multi-physics phenomena such as wear, sealing, and thermal or electrical
conduction. Although accurate numerical methods, like the Boundary Element
Method (BEM), are available to compute this quantity, their high computational
cost limits their applicability in multi-query contexts, such as uncertainty
quantification, parameter identification, and multi-scale algorithms, where
many repeated evaluations are required. This study proposes a surrogate
modeling framework for predicting the effective contact area using
fast-to-evaluate data-driven techniques. Various machine learning algorithms
are trained on a precomputed dataset, where the inputs are the imposed load and
statistical roughness parameters, and the output is the corresponding effective
contact area. All models undergo hyperparameter optimization to enable fair
comparisons in terms of predictive accuracy and computational efficiency,
evaluated using established quantitative metrics. Among the models, the Kernel
Ridge Regressor demonstrates the best trade-off between accuracy and
efficiency, achieving high predictive accuracy, low prediction time, and
minimal training overhead-making it a strong candidate for general-purpose
surrogate modeling. The Gaussian Process Regressor provides an attractive
alternative when uncertainty quantification is required, although it incurs
additional computational cost due to variance estimation. The generalization
capability of the Kernel Ridge model is validated on an unseen simulation
scenario, confirming its ability to transfer to new configurations. Database
generation constitutes the dominant cost in the surrogate modeling process.
Nevertheless, the approach proves practical and efficient for multi-query
tasks, even when accounting for this initial expense.

</details>


### [328] [polyGen: A Learning Framework for Atomic-level Polymer Structure Generation](https://arxiv.org/abs/2504.17656)
*Ayush Jain,Rampi Ramprasad*

Main category: cs.CE

TL;DR: polyGen是一种新型的生成模型，通过最小输入（如重复单元化学式）生成真实的聚合物3D结构，填补了合成聚合物结构生成领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有工具在生成聚合物3D结构时面临挑战，尤其是缺乏能反映构象多样性的方法。polyGen旨在解决这一问题，为聚合物科学提供原子级的结构生成能力。

Method: 采用潜扩散模型（latent diffusion model），利用分子编码捕捉聚合物连接性，并通过数据增强（结合DFT优化的分子结构）提升训练效果。

Result: polyGen能生成线性和支化聚合物的多样构象，但对高原子数重复单元的性能下降，展现了在原子级结构生成上的潜力。

Conclusion: polyGen是聚合物科学中原子级结构生成的范式转变，首次实现了兼顾结构灵活性的真实聚合物构象预测。

Abstract: Synthetic polymeric materials underpin fundamental technologies in the
energy, electronics, consumer goods, and medical sectors, yet their development
still suffers from prolonged design timelines. Although polymer informatics
tools have supported speedup, polymer simulation protocols continue to face
significant challenges: on-demand generation of realistic 3D atomic structures
that respect the conformational diversity of polymer structures. Generative
algorithms for 3D structures of inorganic crystals, bio-polymers, and small
molecules exist, but have not addressed synthetic polymers. In this work, we
introduce polyGen, the first latent diffusion model designed specifically to
generate realistic polymer structures from minimal inputs such as the repeat
unit chemistry alone, leveraging a molecular encoding that captures polymer
connectivity throughout the architecture. Due to a scarce dataset of only 3855
DFT-optimized polymer structures, we augment our training with DFT-optimized
molecular structures, showing improvement in joint learning between similar
chemical structures. We also establish structure matching criteria to benchmark
our approach on this novel problem. polyGen effectively generates diverse
conformations of both linear chains and complex branched structures, though its
performance decreases when handling repeat units with a high atom count. Given
these initial results, polyGen represents a paradigm shift in atomic-level
structure generation for polymer science-the first proof-of-concept for
predicting realistic atomic-level polymer conformations while accounting for
their intrinsic structural flexibility.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [329] [On the workflow, opportunities and challenges of developing foundation model in geophysics](https://arxiv.org/abs/2504.17384)
*Hanlin Sheng,Xinming Wu,Hang Gao,Haibin Di,Sergey Fomel,Jintao Li,Xu Si*

Main category: physics.geo-ph

TL;DR: 该论文提出了一个完整框架，系统地探讨了基础模型与地球物理数据结合的全流程，包括数据处理、模型选择、预训练策略和部署，并针对地球物理数据的特点提出了针对性解决方案，推动该领域创新。


<details>
  <summary>Details</summary>
Motivation: 地球物理领域基础模型应用逐步扩展，但缺乏全流程的综述研究。本文旨在填补这一空白，提供系统性的技术和方法指导。

Method: 从数据收集、预处理到模型架构选择、预训练策略和部署，逐阶段分析关键技术，尤其针对地球物理数据的多样性、复杂性和物理一致性约束提出解决方案。

Result: 通过结合迁移学习和物理约束，减少对标注数据的依赖，提升计算效率，并增强模型的物理一致性和可解释性。

Conclusion: 论文不仅填补了地球物理领域基础模型全流程综述的空白，还为实际应用提供了指导，推动了该领域的技术创新。

Abstract: Foundation models, as a mainstream technology in artificial intelligence,
have demonstrated immense potential across various domains in recent years,
particularly in handling complex tasks and multimodal data. In the field of
geophysics, although the application of foundation models is gradually
expanding, there is currently a lack of comprehensive reviews discussing the
full workflow of integrating foundation models with geophysical data. To
address this gap, this paper presents a complete framework that systematically
explores the entire process of developing foundation models in conjunction with
geophysical data. From data collection and preprocessing to model architecture
selection, pre-training strategies, and model deployment, we provide a detailed
analysis of the key techniques and methodologies at each stage. In particular,
considering the diversity, complexity, and physical consistency constraints of
geophysical data, we discuss targeted solutions to address these challenges.
Furthermore, we discuss how to leverage the transfer learning capabilities of
foundation models to reduce reliance on labeled data, enhance computational
efficiency, and incorporate physical constraints into model training, thereby
improving physical consistency and interpretability. Through a comprehensive
summary and analysis of the current technological landscape, this paper not
only fills the gap in the geophysics domain regarding a full-process review of
foundation models but also offers valuable practical guidance for their
application in geophysical data analysis, driving innovation and advancement in
the field.

</details>


### [330] [Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space](https://arxiv.org/abs/2504.17321)
*Michael J. Smith,Luke Fleming,James E. Geach,Ryan J. Roberts,Freddie Kalaitzis,James Banister*

Main category: physics.geo-ph

TL;DR: Dargana是EarthPT时间序列基础模型的微调版本，仅用3%的预训练数据和5%的计算资源实现高精度树冠分类（10米分辨率），在Cornwall测试中表现优异（ROC-AUC 0.98，PR-AUC 0.83），并能识别细粒度结构（如树篱）和动态变化（如新林地）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示预训练大型观测模型（如EarthPT）如何通过少量数据和计算资源微调，实现精细、动态的土地覆盖监测，为自然资本管理和保护提供可扩展工具。

Method: 通过微调EarthPT模型，使用<3%的预训练数据和5%的计算资源，训练Dargana模型用于树冠分类（分辨率为10米），区分针叶树和阔叶树，并测试其在Cornwall地区的性能。

Result: 模型在未见过卫星图像上达到像素级ROC-AUC 0.98和PR-AUC 0.83，能识别训练样本限制下的细粒度结构（如树篱）并跟踪树冠覆盖的时序变化（如新林地建立）。

Conclusion: Dargana证明了预训练模型经高效微调后，可支持高分辨率、动态土地覆盖监测，为自然管理和保护提供实用工具。

Abstract: We present Dargana, a fine-tuned variant of the EarthPT time-series
foundation model that achieves specialisation using <3% of its pre-training
data volume and 5% of its pre-training compute. Dargana is fine-tuned to
generate regularly updated classification of tree canopy cover at 10m
resolution, distinguishing conifer and broadleaved tree types. Using Cornwall,
UK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a
PR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine
structures like hedgerows and coppice below the training sample limit, and can
track temporal changes to canopy cover such as new woodland establishment. Our
results demonstrate how pre-trained Large Observation Models like EarthPT can
be specialised for granular, dynamic land cover monitoring from space,
providing a valuable, scalable tool for natural capital management and
conservation.

</details>


### [331] [HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time](https://arxiv.org/abs/2504.17420)
*Louisa Pawusch,Stefania Scheurer,Wolfgang Nowak,Reed Maxwell*

Main category: physics.geo-ph

TL;DR: 这篇论文提出了一种名为HydroStartML的机器学习模拟器，用于快速预测流域的初始水位深度（DTWT），从而加速水文模型的启动过程。


<details>
  <summary>Details</summary>
Motivation: 传统的水文模型启动过程需要大量计算资源，通过迭代计算达到稳态。这种方法的计算成本高，尤其是在初始DTWT配置远离稳态时。为了解决这一问题，作者开发了HydroStartML。

Method: HydroStartML基于美国本土的稳态DTWT数据进行训练，利用导水率和地表坡度等数据预测流域的初始DTWT配置。

Result: 实验表明，使用HydroStartML的预测结果作为初始配置，比传统方法（如空间恒定的DTWT）更快达到稳态。即使面对训练中未出现的地形，模拟器也能准确预测接近稳态的配置，尤其在DTWT较深的区域显著减少了计算量。

Conclusion: HydroStartML为结合机器学习与传统模拟的混合方法提供了可能性，有助于提高水文学的预测精度和效率，从而更好地支持水资源管理和环境相互作用的研究。

Abstract: Finding the initial depth-to-water table (DTWT) configuration of a catchment
is a critical challenge when simulating the hydrological cycle with integrated
models, significantly impacting simulation outcomes. Traditionally, this
involves iterative spin-up computations, where the model runs under constant
atmospheric settings until steady-state is achieved. These so-called model
spin-ups are computationally expensive, often requiring many years of simulated
time, particularly when the initial DTWT configuration is far from steady
state.
  To accelerate the model spin-up process we developed HydroStartML, a machine
learning emulator trained on steady-state DTWT configurations across the
contiguous United States. HydroStartML predicts, based on available data like
conductivity and surface slopes, a DTWT configuration of the respective
watershed, which can be used as an initial DTWT.
  Our results show that initializing spin-up computations with HydroStartML
predictions leads to faster convergence than with other initial configurations
like spatially constant DTWTs. The emulator accurately predicts configurations
close to steady state, even for terrain configurations not seen in training,
and allows especially significant reductions in computational spin-up effort in
regions with deep DTWTs. This work opens the door for hybrid approaches that
blend machine learning and traditional simulation, enhancing predictive
accuracy and efficiency in hydrology for improving water resource management
and understanding complex environmental interactions.

</details>


### [332] [On the workflow, opportunities and challenges of developing foundation model in geophysics](https://arxiv.org/abs/2504.17384)
*Hanlin Sheng,Xinming Wu,Hang Gao,Haibin Di,Sergey Fomel,Jintao Li,Xu Si*

Main category: physics.geo-ph

TL;DR: 本文提出了一个将基础模型与地球物理数据结合的完整框架，填补了该领域流程综述的空白，并提供了实用的技术指导和挑战解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决地球物理领域基础模型应用流程综述缺失的问题，并探索如何结合基础模型与多样化、复杂的地球物理数据。

Method: 系统化分析了从数据收集、预处理到模型架构选择、预训练策略及部署的全流程，并讨论了针对地球物理数据特性的解决方案。

Result: 提供了结合基础模型与地球物理数据的完整框架，提升了模型的物理一致性和可解释性，降低了标注数据的依赖。

Conclusion: 本文不仅填补了地球物理领域基础模型全流程综述的空白，还推动了该领域的技术创新和实际应用。

Abstract: Foundation models, as a mainstream technology in artificial intelligence,
have demonstrated immense potential across various domains in recent years,
particularly in handling complex tasks and multimodal data. In the field of
geophysics, although the application of foundation models is gradually
expanding, there is currently a lack of comprehensive reviews discussing the
full workflow of integrating foundation models with geophysical data. To
address this gap, this paper presents a complete framework that systematically
explores the entire process of developing foundation models in conjunction with
geophysical data. From data collection and preprocessing to model architecture
selection, pre-training strategies, and model deployment, we provide a detailed
analysis of the key techniques and methodologies at each stage. In particular,
considering the diversity, complexity, and physical consistency constraints of
geophysical data, we discuss targeted solutions to address these challenges.
Furthermore, we discuss how to leverage the transfer learning capabilities of
foundation models to reduce reliance on labeled data, enhance computational
efficiency, and incorporate physical constraints into model training, thereby
improving physical consistency and interpretability. Through a comprehensive
summary and analysis of the current technological landscape, this paper not
only fills the gap in the geophysics domain regarding a full-process review of
foundation models but also offers valuable practical guidance for their
application in geophysical data analysis, driving innovation and advancement in
the field.

</details>


### [333] [Dargana: fine-tuning EarthPT for dynamic tree canopy mapping from space](https://arxiv.org/abs/2504.17321)
*Michael J. Smith,Luke Fleming,James E. Geach,Ryan J. Roberts,Freddie Kalaitzis,James Banister*

Main category: physics.geo-ph

TL;DR: Dargana是一个基于EarthPT时间序列基础模型微调的变体，用不到3%的预训练数据和5%的计算资源实现了对树冠覆盖的精细分类（10米分辨率），区分针叶树和阔叶树。在Cornwall的测试中，模型表现优异（ROC-AUC 0.98，PR-AUC 0.83），并能识别细微结构（如树篱）和动态变化（如新林地）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示预训练的大型观测模型（如EarthPT）如何通过少量数据和计算资源的微调，专门用于高粒度、动态的土地覆盖监测，为自然资本管理和保护提供可扩展的工具。

Method: Dargana通过对EarthPT模型进行微调，使用少于3%的预训练数据和5%的计算资源，专注于树冠覆盖分类（10米分辨率），区分针叶树和阔叶树。测试采用Cornwell, UK的卫星图像。

Result: 模型在测试数据上表现优异（像素级ROC-AUC 0.98，PR-AUC 0.83），能识别低于训练样本限的细微结构（如树篱）并跟踪树冠覆盖的时序变化（如新林地）。

Conclusion: 研究表明，通过少量数据和计算资源的微调，预训练的大型观测模型可高效用于高粒度、动态的土地覆盖监测，为自然资源管理和保护提供了可行的解决方案。

Abstract: We present Dargana, a fine-tuned variant of the EarthPT time-series
foundation model that achieves specialisation using <3% of its pre-training
data volume and 5% of its pre-training compute. Dargana is fine-tuned to
generate regularly updated classification of tree canopy cover at 10m
resolution, distinguishing conifer and broadleaved tree types. Using Cornwall,
UK, as a test case, the model achieves a pixel-level ROC-AUC of 0.98 and a
PR-AUC of 0.83 on unseen satellite imagery. Dargana can identify fine
structures like hedgerows and coppice below the training sample limit, and can
track temporal changes to canopy cover such as new woodland establishment. Our
results demonstrate how pre-trained Large Observation Models like EarthPT can
be specialised for granular, dynamic land cover monitoring from space,
providing a valuable, scalable tool for natural capital management and
conservation.

</details>


### [334] [HydroStartML: A combined machine learning and physics-based approach to reduce hydrological model spin-up time](https://arxiv.org/abs/2504.17420)
*Louisa Pawusch,Stefania Scheurer,Wolfgang Nowak,Reed Maxwell*

Main category: physics.geo-ph

TL;DR: 引入HydroStartML机器学习模拟器，通过预测初始水位深度配置加速水文模型启动，减少计算成本并提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统的水文模型启动过程需要大量计算的迭代旋转计算，尤其是初始水位深度配置远离稳态时，耗时且成本高昂。

Method: 开发HydroStartML机器学习模拟器，基于导水率和地表坡度等数据预测稳态水位深度配置，作为模型的初始输入。

Result: HydroStartML的预测显著缩短了模型启动时间，特别是在水位深度较深的区域，且对新地形的预测也表现良好。

Conclusion: HydroStartML展示了机器学习与传统水文模拟结合在提高预测精度和效率方面的潜力，为水资源管理和环境研究提供了新工具。

Abstract: Finding the initial depth-to-water table (DTWT) configuration of a catchment
is a critical challenge when simulating the hydrological cycle with integrated
models, significantly impacting simulation outcomes. Traditionally, this
involves iterative spin-up computations, where the model runs under constant
atmospheric settings until steady-state is achieved. These so-called model
spin-ups are computationally expensive, often requiring many years of simulated
time, particularly when the initial DTWT configuration is far from steady
state.
  To accelerate the model spin-up process we developed HydroStartML, a machine
learning emulator trained on steady-state DTWT configurations across the
contiguous United States. HydroStartML predicts, based on available data like
conductivity and surface slopes, a DTWT configuration of the respective
watershed, which can be used as an initial DTWT.
  Our results show that initializing spin-up computations with HydroStartML
predictions leads to faster convergence than with other initial configurations
like spatially constant DTWTs. The emulator accurately predicts configurations
close to steady state, even for terrain configurations not seen in training,
and allows especially significant reductions in computational spin-up effort in
regions with deep DTWTs. This work opens the door for hybrid approaches that
blend machine learning and traditional simulation, enhancing predictive
accuracy and efficiency in hydrology for improving water resource management
and understanding complex environmental interactions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [335] [Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation](https://arxiv.org/abs/2504.17114)
*Valentin Langer,Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TL;DR: 论文提出了一种基于多器官分割的动态PET图像分析方法，通过整合主动脉、门静脉、肺动脉和输尿管的图像输入函数，改进了动力学模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅使用主动脉的输入函数，忽略了解剖变异和复杂血管贡献，限制了动力学分析的准确性。

Method: 结合肝脏、肺、肾脏和膀胱的高分辨率CT分割，整合器官特异性血液供应来源，优化动力学建模。

Result: 在9名患者的动态PET数据中测试，肝脏和肺的平均均方误差分别降低13.39%和10.42%。

Conclusion: 多输入函数方法显著提升了动力学分析的准确性，有望推动示踪动力学模型在临床的常规应用。

Abstract: Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron
emission tomography (PET) requires anatomically constrained modelling of
image-derived input functions (IDIFs). Traditionally, IDIFs are obtained from
the aorta, neglecting anatomical variations and complex vascular contributions.
This study proposes a multi-organ segmentation-based approach that integrates
IDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using
high-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we
incorporate organ-specific blood supply sources to improve kinetic modelling.
Our method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,
resulting in a mean squared error (MSE) reduction of $13.39\%$ for the liver
and $10.42\%$ for the lungs. These initial results highlight the potential of
multiple IDIFs in improving anatomical modelling and fully leveraging dynamic
PET imaging. This approach could facilitate the integration of tracer kinetic
modelling into clinical routine.

</details>


### [336] [Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET](https://arxiv.org/abs/2504.17122)
*Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TL;DR: 该论文提出了一种基于隐式神经表示（INRs）的生理神经表示方法，用于个性化动力学参数估计，解决了传统方法计算量大和深度神经网络（DNNs）需要大量训练数据的问题，并在[$^{18}$F]FDG动态PET/CT数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态PET成像的常规方法计算量大且空间分辨率有限，而DNNs虽然提供了替代方案，但需要大量数据和计算资源。因此，作者提出了一种更高效、低数据需求的高分辨率参数成像方法。

Method: 利用隐式神经表示（INRs）学习连续函数，结合3D CT基础模型的解剖先验，进行个性化动力学参数估计。

Result: 在[$^{18}$F]FDG动态PET/CT数据集上的实验表明，该方法具有更高的空间分辨率、更低的均方误差和更好的解剖一致性，尤其在肿瘤和高血管区域表现突出。

Conclusion: INRs在个性化、数据高效的示踪动力学建模中展现出潜力，未来可应用于肿瘤表征、分割和预后评估等领域。

Abstract: Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables
non-invasive quantification of glucose metabolism through kinetic analysis,
often modelled by the two-tissue compartment model (TCKM). However, voxel-wise
kinetic parameter estimation using conventional methods is computationally
intensive and limited by spatial resolution. Deep neural networks (DNNs) offer
an alternative but require large training datasets and significant
computational resources. To address these limitations, we propose a
physiological neural representation based on implicit neural representations
(INRs) for personalized kinetic parameter estimation. INRs, which learn
continuous functions, allow for efficient, high-resolution parametric imaging
with reduced data requirements. Our method also integrates anatomical priors
from a 3D CT foundation model to enhance robustness and precision in kinetic
modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset
and compare it to state-of-the-art DNNs. Results demonstrate superior spatial
resolution, lower mean-squared error, and improved anatomical consistency,
particularly in tumour and highly vascularized regions. Our findings highlight
the potential of INRs for personalized, data-efficient tracer kinetic
modelling, enabling applications in tumour characterization, segmentation, and
prognostic assessment.

</details>


### [337] [3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations](https://arxiv.org/abs/2504.17255)
*Shaoyu Pei,Renxiong Wu,Hao Zheng,Lang Qin,Shuaichen Lin,Yuxing Gan,Wenjing Huang,Zhixuan Wang,Mohan Qin,Yong Liu,Guangming Ni*

Main category: eess.IV

TL;DR: 研究者提出基于3D变换器的多目标分割框架，实现了汗腺的实时、非侵入性3D分割，首次量化了汗腺形态随温度变化的细微差异。


<details>
  <summary>Details</summary>
Motivation: 现有汗腺形态观察方法多为二维、体外且破坏性，亟需实时、非侵入、可量化的技术。

Method: 结合滑动窗口、联合空间-通道注意力机制及深浅层异质性，提出3D变换器分割框架，利用OCT皮肤体积数据实现精准分割。

Result: 首次实现汗腺3D形态随温度变化的可视化和量化，为正常汗腺形态提供基准。

Conclusion: 该方法为汗腺结构的个体差异及病理变化研究提供了工具，推动了皮肤病学研究及临床应用。

Abstract: Skin, the primary regulator of heat exchange, relies on sweat glands for
thermoregulation. Alterations in sweat gland morphology play a crucial role in
various pathological conditions and clinical diagnoses. Current methods for
observing sweat gland morphology are limited by their two-dimensional, in
vitro, and destructive nature, underscoring the urgent need for real-time,
non-invasive, quantifiable technologies. We proposed a novel three-dimensional
(3D) transformer-based multi-object segmentation framework, integrating a
sliding window approach, joint spatial-channel attention mechanism, and
architectural heterogeneity between shallow and deep layers. Our proposed
network enables precise 3D sweat gland segmentation from skin volume data
captured by optical coherence tomography (OCT). For the first time, subtle
variations of sweat gland 3D morphology in response to temperature changes,
have been visualized and quantified. Our approach establishes a benchmark for
normal sweat gland morphology and provides a real-time, non-invasive tool for
quantifying 3D structural parameters. This enables the study of individual
variability and pathological changes in sweat gland structure, advancing
dermatological research and clinical applications, including thermoregulation
and bromhidrosis treatment.

</details>


### [338] [Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation](https://arxiv.org/abs/2504.17114)
*Valentin Langer,Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TL;DR: 本研究提出了一种基于多器官分割的方法，整合来自主动脉、门静脉、肺动脉和尿道的图像衍生输入函数（IDIFs），以提高动态PET中[$^{18}$F]FDG分布的动力学分析准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅从主动脉获取IDIFs，忽略了解剖变化和复杂血管贡献，无法充分利用动态PET成像的潜力。

Method: 通过结合肝脏、肺、肾脏和膀胱的高分辨率CT分割，整合器官特异性血液供应源，改进动力学建模。

Result: 在9例患者的动态[$^{18}$F]FDG PET数据中，肝脏和肺的均方误差（MSE）分别降低了13.39%和10.42%。

Conclusion: 多IDIFs方法有望改善解剖建模并推动示踪动力学模型在临床中的常规应用。

Abstract: Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron
emission tomography (PET) requires anatomically constrained modelling of
image-derived input functions (IDIFs). Traditionally, IDIFs are obtained from
the aorta, neglecting anatomical variations and complex vascular contributions.
This study proposes a multi-organ segmentation-based approach that integrates
IDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using
high-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we
incorporate organ-specific blood supply sources to improve kinetic modelling.
Our method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,
resulting in a mean squared error (MSE) reduction of $13.39\%$ for the liver
and $10.42\%$ for the lungs. These initial results highlight the potential of
multiple IDIFs in improving anatomical modelling and fully leveraging dynamic
PET imaging. This approach could facilitate the integration of tracer kinetic
modelling into clinical routine.

</details>


### [339] [Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET](https://arxiv.org/abs/2504.17122)
*Kartikay Tehlan,Thomas Wendler*

Main category: eess.IV

TL;DR: 该论文提出了一种基于隐式神经表示（INRs）的生理神经表示方法，用于个性化动态PET图像中葡萄糖代谢的动力学参数估计，解决了传统方法计算量大和空间分辨率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的动力学参数估计方法计算量大且受限于空间分辨率，而深度神经网络（DNNs）需要大量训练数据和计算资源，因此需要一种更高效且数据需求较少的方法。

Method: 提出了一种基于INRs的生理神经表示方法，结合3D CT基础模型的解剖先验，以提升动力学建模的鲁棒性和精确性。

Result: 在[$^{18}$F]FDG动态PET/CT数据集上的实验结果表明，该方法具有更高的空间分辨率、更低的均方误差以及更好的解剖一致性，尤其是在肿瘤和高度血管化区域。

Conclusion: INRs在个性化、数据高效的示踪动力学建模中展现出潜力，适用于肿瘤表征、分割和预后评估等应用。

Abstract: Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables
non-invasive quantification of glucose metabolism through kinetic analysis,
often modelled by the two-tissue compartment model (TCKM). However, voxel-wise
kinetic parameter estimation using conventional methods is computationally
intensive and limited by spatial resolution. Deep neural networks (DNNs) offer
an alternative but require large training datasets and significant
computational resources. To address these limitations, we propose a
physiological neural representation based on implicit neural representations
(INRs) for personalized kinetic parameter estimation. INRs, which learn
continuous functions, allow for efficient, high-resolution parametric imaging
with reduced data requirements. Our method also integrates anatomical priors
from a 3D CT foundation model to enhance robustness and precision in kinetic
modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset
and compare it to state-of-the-art DNNs. Results demonstrate superior spatial
resolution, lower mean-squared error, and improved anatomical consistency,
particularly in tumour and highly vascularized regions. Our findings highlight
the potential of INRs for personalized, data-efficient tracer kinetic
modelling, enabling applications in tumour characterization, segmentation, and
prognostic assessment.

</details>


### [340] [3D Deep-learning-based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations](https://arxiv.org/abs/2504.17255)
*Shaoyu Pei,Renxiong Wu,Hao Zheng,Lang Qin,Shuaichen Lin,Yuxing Gan,Wenjing Huang,Zhixuan Wang,Mohan Qin,Yong Liu,Guangming Ni*

Main category: eess.IV

TL;DR: 论文提出了一种基于3D transformer的多目标分割框架，用于实时、无创地量化皮肤汗腺的3D形态，首次实现了温度变化下汗腺细微形态的可视化与量化。


<details>
  <summary>Details</summary>
Motivation: 当前汗腺形态观察方法局限于二维、体外且有损，迫切需要实时、无创、可量化的技术。

Method: 采用滑动窗口方法、联合空间通道注意力机制及深浅层架构异质性，构建了基于OCT的3D汗腺分割网络。

Result: 首次实现了温度变化下汗腺3D形态的细微变化可视化与量化，为正常汗腺形态建立了基准。

Conclusion: 该技术为汗腺结构的个体差异与病理变化研究提供了工具，推动了皮肤病学研究和临床应用，如体温调节和腋臭治疗。

Abstract: Skin, the primary regulator of heat exchange, relies on sweat glands for
thermoregulation. Alterations in sweat gland morphology play a crucial role in
various pathological conditions and clinical diagnoses. Current methods for
observing sweat gland morphology are limited by their two-dimensional, in
vitro, and destructive nature, underscoring the urgent need for real-time,
non-invasive, quantifiable technologies. We proposed a novel three-dimensional
(3D) transformer-based multi-object segmentation framework, integrating a
sliding window approach, joint spatial-channel attention mechanism, and
architectural heterogeneity between shallow and deep layers. Our proposed
network enables precise 3D sweat gland segmentation from skin volume data
captured by optical coherence tomography (OCT). For the first time, subtle
variations of sweat gland 3D morphology in response to temperature changes,
have been visualized and quantified. Our approach establishes a benchmark for
normal sweat gland morphology and provides a real-time, non-invasive tool for
quantifying 3D structural parameters. This enables the study of individual
variability and pathological changes in sweat gland structure, advancing
dermatological research and clinical applications, including thermoregulation
and bromhidrosis treatment.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [341] [Plasma State Monitoring and Disruption Characterization using Multimodal VAEs](https://arxiv.org/abs/2504.17710)
*Yoeri Poels,Alessandro Pau,Christian Donner,Giulio Romanelli,Olivier Sauter,Cristina Venturini,Vlado Menkovski,the TCV team,the WPTE team*

Main category: physics.plasm-ph

TL;DR: 论文提出了一种可解释的等离子体状态表征方法，结合变分自编码器（VAE）框架，通过低维潜在表示分析托卡马克等离子体数据，旨在预测和区分不同类型的破裂事件。


<details>
  <summary>Details</summary>
Motivation: 托卡马克等离子体破裂会带来严重的设备和热负荷问题，但现有数据驱动模型的解释性不足。本文旨在开发一种可解释的方法，通过分析等离子体状态数据来预测和表征破裂。

Method: 扩展了VAE框架，引入（1）等离子体轨迹的连续投影；（2）多模态结构分隔运行状态；（3）针对破裂状态的分隔。基于1600次TCV放电数据验证方法。

Result: 方法能有效区分不同破裂类型，并通过统计特性连续指示破裂风险，还能识别与破裂相关的等离子体参数。

Conclusion: 该方法可解释性强，能成功区分不同运行状态及其与破裂的关系，为未来设备运行提供参考。

Abstract: When a plasma disrupts in a tokamak, significant heat and electromagnetic
loads are deposited onto the surrounding device components. These forces scale
with plasma current and magnetic field strength, making disruptions one of the
key challenges for future devices. Unfortunately, disruptions are not fully
understood, with many different underlying causes that are difficult to
anticipate. Data-driven models have shown success in predicting them, but they
only provide limited interpretability. On the other hand, large-scale
statistical analyses have been a great asset to understanding disruptive
patterns. In this paper, we leverage data-driven methods to find an
interpretable representation of the plasma state for disruption
characterization. Specifically, we use a latent variable model to represent
diagnostic measurements as a low-dimensional, latent representation. We build
upon the Variational Autoencoder (VAE) framework, and extend it for (1)
continuous projections of plasma trajectories; (2) a multimodal structure to
separate operating regimes; and (3) separation with respect to disruptive
regimes. Subsequently, we can identify continuous indicators for the disruption
rate and the disruptivity based on statistical properties of measurement data.
The proposed method is demonstrated using a dataset of approximately 1600 TCV
discharges, selecting for flat-top disruptions or regular terminations. We
evaluate the method with respect to (1) the identified disruption risk and its
correlation with other plasma properties; (2) the ability to distinguish
different types of disruptions; and (3) downstream analyses. For the latter, we
conduct a demonstrative study on identifying parameters connected to
disruptions using counterfactual-like analysis. Overall, the method can
adequately identify distinct operating regimes characterized by varying
proximity to disruptions in an interpretable manner.

</details>


### [342] [Plasma State Monitoring and Disruption Characterization using Multimodal VAEs](https://arxiv.org/abs/2504.17710)
*Yoeri Poels,Alessandro Pau,Christian Donner,Giulio Romanelli,Olivier Sauter,Cristina Venturini,Vlado Menkovski,the TCV team,the WPTE team*

Main category: physics.plasm-ph

TL;DR: 利用变分自编码器（VAE）框架，该论文提出了一种可解释的低维表示方法，用于表征等离子体状态和预测托卡马克中的破裂事件。通过分析约1600次TCV放电数据，方法能区分不同操作状态并识别破裂风险。


<details>
  <summary>Details</summary>
Motivation: 托卡马克中等离子体破裂会带来严重的热和电磁负载，但破裂的成因复杂且难以预测。现有数据驱动模型预测效果虽好，但解释性不足，不利于深入理解破裂机制。

Method: 基于VAE框架，作者开发了一种潜在变量模型，用于将诊断数据映射到低维空间，并扩展了其功能以支持连续投影、多模态分离和破裂区域区分。

Result: 该方法能有效识别不同操作状态与破裂风险的关联，并通过反事实分析识别与破裂相关的参数。

Conclusion: 所提方法以可解释的方式区分了不同操作状态及其与破裂的接近程度，为理解破裂机制提供了新工具。

Abstract: When a plasma disrupts in a tokamak, significant heat and electromagnetic
loads are deposited onto the surrounding device components. These forces scale
with plasma current and magnetic field strength, making disruptions one of the
key challenges for future devices. Unfortunately, disruptions are not fully
understood, with many different underlying causes that are difficult to
anticipate. Data-driven models have shown success in predicting them, but they
only provide limited interpretability. On the other hand, large-scale
statistical analyses have been a great asset to understanding disruptive
patterns. In this paper, we leverage data-driven methods to find an
interpretable representation of the plasma state for disruption
characterization. Specifically, we use a latent variable model to represent
diagnostic measurements as a low-dimensional, latent representation. We build
upon the Variational Autoencoder (VAE) framework, and extend it for (1)
continuous projections of plasma trajectories; (2) a multimodal structure to
separate operating regimes; and (3) separation with respect to disruptive
regimes. Subsequently, we can identify continuous indicators for the disruption
rate and the disruptivity based on statistical properties of measurement data.
The proposed method is demonstrated using a dataset of approximately 1600 TCV
discharges, selecting for flat-top disruptions or regular terminations. We
evaluate the method with respect to (1) the identified disruption risk and its
correlation with other plasma properties; (2) the ability to distinguish
different types of disruptions; and (3) downstream analyses. For the latter, we
conduct a demonstrative study on identifying parameters connected to
disruptions using counterfactual-like analysis. Overall, the method can
adequately identify distinct operating regimes characterized by varying
proximity to disruptions in an interpretable manner.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [343] [A Machine Learning Approach for Denoising and Upsampling HRTFs](https://arxiv.org/abs/2504.17586)
*Xuyi Hu,Jian Li,Lorenzo Picinali,Aidan O. T. Hogg*

Main category: cs.SD

TL;DR: 该论文提出了一种结合HRTF Denoisy U-Net和AE-GAN的新方法，用于从稀疏、嘈杂的HRTF测量中进行上采样，显著减少了测量点的数量并保持了低误差。


<details>
  <summary>Details</summary>
Motivation: 个性化HRTF能提高声音定位准确性，但传统测量方法耗时且需无噪环境。现有机器学习方法虽减少了测量点，但仍需受控环境。该研究旨在解决这一限制。

Method: 采用HRTF Denoisy U-Net去噪和AE-GAN上采样技术，仅需三个测量点即可生成高质量的HRTF数据。

Result: 实验结果显示，该方法在log-spectral distortion (LSD)误差为5.41 dB，余弦相似度损失为0.0070，验证了其有效性。

Conclusion: 该方法在减少测量点和环境限制的同时，保持了HRTF数据的质量，为虚拟沉浸式音频提供了实用解决方案。

Abstract: The demand for realistic virtual immersive audio continues to grow, with
Head-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how
sound reaches our ears, reflecting unique anatomical features and enhancing
spatial perception. It has been shown that personalized HRTFs improve
localization accuracy, but their measurement remains time-consuming and
requires a noise-free environment. Although machine learning has been shown to
reduce the required measurement points and, thus, the measurement time, a
controlled environment is still necessary. This paper proposes a method to
address this constraint by presenting a novel technique that can upsample
sparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy
U-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)
for upsampling from three measurement points. The proposed method achieves a
log-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of
0.0070, demonstrating the method's effectiveness in HRTF upsampling.

</details>


### [344] [Unleashing the Power of Natural Audio Featuring Multiple Sound Sources](https://arxiv.org/abs/2504.17782)
*Xize Cheng,Slytherin Wang,Zehan Wang,Rongjie Huang,Tao Jin,Zhou Zhao*

Main category: cs.SD

TL;DR: ClearSep是一种创新的通用声音分离框架，通过数据引擎分解真实世界中的混合音频，结合重混评估指标和定制训练策略，显著提升了自然音频场景下的分离性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖人工混合音频训练，难以泛化到真实环境中的自然混合音频，ClearSep旨在解决这一局限。

Method: 利用数据引擎分解自然混合音频，引入重混评估指标迭代优化分离性能，并设计针对独立音轨的训练策略。

Result: ClearSep在多个声音分离任务中取得最先进性能，展示了其在自然音频场景中的应用潜力。

Conclusion: ClearSep通过真实数据驱动的方法，有效推动了自然音频场景中的声音分离技术发展。

Abstract: Universal sound separation aims to extract clean audio tracks corresponding
to distinct events from mixed audio, which is critical for artificial auditory
perception. However, current methods heavily rely on artificially mixed audio
for training, which limits their ability to generalize to naturally mixed audio
collected in real-world environments. To overcome this limitation, we propose
ClearSep, an innovative framework that employs a data engine to decompose
complex naturally mixed audio into multiple independent tracks, thereby
allowing effective sound separation in real-world scenarios. We introduce two
remix-based evaluation metrics to quantitatively assess separation quality and
use these metrics as thresholds to iteratively apply the data engine alongside
model training, progressively optimizing separation performance. In addition,
we propose a series of training strategies tailored to these separated
independent tracks to make the best use of them. Extensive experiments
demonstrate that ClearSep achieves state-of-the-art performance across multiple
sound separation tasks, highlighting its potential for advancing sound
separation in natural audio scenarios. For more examples and detailed results,
please visit our demo page at https://clearsep.github.io.

</details>


### [345] [A Machine Learning Approach for Denoising and Upsampling HRTFs](https://arxiv.org/abs/2504.17586)
*Xuyi Hu,Jian Li,Lorenzo Picinali,Aidan O. T. Hogg*

Main category: cs.SD

TL;DR: 提出了一种新方法，通过结合HRTF Denoisy U-Net和AE-GAN，在噪声环境下从稀疏测量点进行HRTF上采样，效果显著。


<details>
  <summary>Details</summary>
Motivation: 个性化HRTF测量耗时且需噪声环境，现有机器学习方法仍需控制环境，需解决这一问题。

Method: 结合HRTF Denoisy U-Net去噪和AE-GAN上采样，从三个测量点生成高质量HRTF。

Result: LSD误差5.41 dB，余弦相似损失0.0070，验证了方法的有效性。

Conclusion: 该方法在噪声环境下实现了HRTF的高效上采样，为虚拟音频体验提供了实用方案。

Abstract: The demand for realistic virtual immersive audio continues to grow, with
Head-Related Transfer Functions (HRTFs) playing a key role. HRTFs capture how
sound reaches our ears, reflecting unique anatomical features and enhancing
spatial perception. It has been shown that personalized HRTFs improve
localization accuracy, but their measurement remains time-consuming and
requires a noise-free environment. Although machine learning has been shown to
reduce the required measurement points and, thus, the measurement time, a
controlled environment is still necessary. This paper proposes a method to
address this constraint by presenting a novel technique that can upsample
sparse, noisy HRTF measurements. The proposed approach combines an HRTF Denoisy
U-Net for denoising and an Autoencoding Generative Adversarial Network (AE-GAN)
for upsampling from three measurement points. The proposed method achieves a
log-spectral distortion (LSD) error of 5.41 dB and a cosine similarity loss of
0.0070, demonstrating the method's effectiveness in HRTF upsampling.

</details>


### [346] [Unleashing the Power of Natural Audio Featuring Multiple Sound Sources](https://arxiv.org/abs/2504.17782)
*Xize Cheng,Slytherin Wang,Zehan Wang,Rongjie Huang,Tao Jin,Zhou Zhao*

Main category: cs.SD

TL;DR: ClearSep是一个新型框架，通过数据引擎分解自然混合音频为独立音轨，并采用两种重混评价指标优化分离性能，在多种声音分离任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工混合音频训练，难以泛化到真实环境的自然混合音频，故提出ClearSep以解决此限制。

Method: 使用数据引擎分解自然混合音频，引入重混评价指标迭代优化，并设计针对分离音轨的训练策略。

Result: 在多个声音分离任务中达到最先进性能，展示了在自然音频场景中的潜力。

Conclusion: ClearSep通过创新框架和评价指标，有效提升了自然混合音频的分离性能，具有实际应用前景。

Abstract: Universal sound separation aims to extract clean audio tracks corresponding
to distinct events from mixed audio, which is critical for artificial auditory
perception. However, current methods heavily rely on artificially mixed audio
for training, which limits their ability to generalize to naturally mixed audio
collected in real-world environments. To overcome this limitation, we propose
ClearSep, an innovative framework that employs a data engine to decompose
complex naturally mixed audio into multiple independent tracks, thereby
allowing effective sound separation in real-world scenarios. We introduce two
remix-based evaluation metrics to quantitatively assess separation quality and
use these metrics as thresholds to iteratively apply the data engine alongside
model training, progressively optimizing separation performance. In addition,
we propose a series of training strategies tailored to these separated
independent tracks to make the best use of them. Extensive experiments
demonstrate that ClearSep achieves state-of-the-art performance across multiple
sound separation tasks, highlighting its potential for advancing sound
separation in natural audio scenarios. For more examples and detailed results,
please visit our demo page at https://clearsep.github.io.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [347] [You Are What You Bought: Generating Customer Personas for E-commerce Applications](https://arxiv.org/abs/2504.17304)
*Yimin Shi,Yang Fei,Shiqi Zhang,Haixun Wang,Xiaokui Xiao*

Main category: cs.IR

TL;DR: 论文提出了GPLR方法，利用预训练LLM生成客户多角色标识，结合随机游走技术提高效率，并在推荐和客户分群任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有用户嵌入方法难以理解和融入外部知识，限制了客户分群、搜索导航和产品推荐的效果。因此，论文提出客户角色标识的概念，以提供可读性强的显式用户表示。

Method: 采用GPLR方法，结合预训练LLM推断客户角色，并通过随机游走技术扩展至全体用户；提出RevAff技术以优化时间复杂度和误差保证。

Result: 在三个真实电商数据集中，角色标识的引入使图卷积推荐模型的NDCG@K和F1-Score@K提升高达12%。

Conclusion: 客户角色标识显著提升了显式用户表示的可读性和应用性能，验证了其在电商场景中的实用价值。

Abstract: In e-commerce, user representations are essential for various applications.
Existing methods often use deep learning techniques to convert customer
behaviors into implicit embeddings. However, these embeddings are difficult to
understand and integrate with external knowledge, limiting the effectiveness of
applications such as customer segmentation, search navigation, and product
recommendations. To address this, our paper introduces the concept of the
customer persona. Condensed from a customer's numerous purchasing histories, a
customer persona provides a multi-faceted and human-readable characterization
of specific purchase behaviors and preferences, such as Busy Parents or Bargain
Hunters.
  This work then focuses on representing each customer by multiple personas
from a predefined set, achieving readable and informative explicit user
representations. To this end, we propose an effective and efficient solution
GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer
personas for customers. To reduce overhead, GPLR applies LLM-based labeling to
only a fraction of users and utilizes a random walk technique to predict
personas for the remaining customers. We further propose RevAff, which provides
an absolute error $\epsilon$ guarantee while improving the time complexity of
the exact solution by a factor of at least
$O(\frac{\epsilon\cdot|E|N}{|E|+N\log N})$, where $N$ represents the number of
customers and products, and $E$ represents the interactions between them. We
evaluate the performance of our persona-based representation in terms of
accuracy and robustness for recommendation and customer segmentation tasks
using three real-world e-commerce datasets. Most notably, we find that
integrating customer persona representations improves the state-of-the-art
graph convolution-based recommendation model by up to 12% in terms of NDCG@K
and F1-Score@K.

</details>


### [348] [IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval](https://arxiv.org/abs/2504.17529)
*Youngjune Lee,Haeyu Jeong,Changgeon Lim,Jeong Choi,Hongjun Lim,Hangon Kim,Jiyoon Kwon,Saehun Kim*

Main category: cs.IR

TL;DR: 该论文提出了一种名为IRA的框架，用于动态适应在线社区平台中用户兴趣和文档的变化，通过兴趣单元和语义检索机制实现高效、可扩展的个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 在线社区平台需要动态个性化检索和推荐系统，以实时适应不断变化的用户兴趣和新文档，但在大规模工业环境中优化模型仍具挑战性。

Method: IRA框架包含兴趣单元（捕捉用户兴趣并通过累积更新调整）和基于语义关系的检索过程（避免依赖点击信号以减少时间偏差）。

Result: 通过在真实数据集上的广泛实验，包括在NAVER CAFE平台的实际部署，验证了IRA的有效性。

Conclusion: IRA能够持续适应用户偏好变化，提供细粒度的个性化服务，且不受过去训练分布的限制。

Abstract: Online community platforms require dynamic personalized retrieval and
recommendation that can continuously adapt to evolving user interests and new
documents. However, optimizing models to handle such changes in real-time
remains a major challenge in large-scale industrial settings. To address this,
we propose the Interest-aware Representation and Alignment (IRA) framework, an
efficient and scalable approach that dynamically adapts to new interactions
through a cumulative structure. IRA leverages two key mechanisms: (1) Interest
Units that capture diverse user interests as contextual texts, while
reinforcing or fading over time through cumulative updates, and (2) a retrieval
process that measures the relevance between Interest Units and documents based
solely on semantic relationships, eliminating dependence on click signals to
mitigate temporal biases. By integrating cumulative Interest Unit updates with
the retrieval process, IRA continuously adapts to evolving user preferences,
ensuring robust and fine-grained personalization without being constrained by
past training distributions. We validate the effectiveness of IRA through
extensive experiments on real-world datasets, including its deployment in the
Home Section of NAVER's CAFE, South Korea's leading community platform.

</details>


### [349] [You Are What You Bought: Generating Customer Personas for E-commerce Applications](https://arxiv.org/abs/2504.17304)
*Yimin Shi,Yang Fei,Shiqi Zhang,Haixun Wang,Xiaokui Xiao*

Main category: cs.IR

TL;DR: 提出了可读性强的客户人设（persona）表示方法GPLR，结合预训练大模型和随机游走技术，显著提升了推荐和客户分群任务的性能，NDCG@K和F1-Score@K最高提升12%。


<details>
  <summary>Details</summary>
Motivation: 现有电商用户嵌入方法缺乏可解释性且难以融合外部知识，限制了客户分群、搜索导航和推荐等应用效果。

Method: 提出GPLR方法，利用预训练大模型为部分用户推断人设标签，并通过随机游走技术扩展至全体用户；同时提出RevAff算法，在保证误差界限的同时优化计算效率。

Result: 在三个真实电商数据集上验证，人设表示使图卷积推荐模型的NDCG@K和F1-Score@K最高提升12%。

Conclusion: 显式、可解释的客户人设表示能有效提升电商下游任务性能，且GPLR方法兼顾效果与效率。

Abstract: In e-commerce, user representations are essential for various applications.
Existing methods often use deep learning techniques to convert customer
behaviors into implicit embeddings. However, these embeddings are difficult to
understand and integrate with external knowledge, limiting the effectiveness of
applications such as customer segmentation, search navigation, and product
recommendations. To address this, our paper introduces the concept of the
customer persona. Condensed from a customer's numerous purchasing histories, a
customer persona provides a multi-faceted and human-readable characterization
of specific purchase behaviors and preferences, such as Busy Parents or Bargain
Hunters.
  This work then focuses on representing each customer by multiple personas
from a predefined set, achieving readable and informative explicit user
representations. To this end, we propose an effective and efficient solution
GPLR. To ensure effectiveness, GPLR leverages pre-trained LLMs to infer
personas for customers. To reduce overhead, GPLR applies LLM-based labeling to
only a fraction of users and utilizes a random walk technique to predict
personas for the remaining customers. We further propose RevAff, which provides
an absolute error $\epsilon$ guarantee while improving the time complexity of
the exact solution by a factor of at least
$O(\frac{\epsilon\cdot|E|N}{|E|+N\log N})$, where $N$ represents the number of
customers and products, and $E$ represents the interactions between them. We
evaluate the performance of our persona-based representation in terms of
accuracy and robustness for recommendation and customer segmentation tasks
using three real-world e-commerce datasets. Most notably, we find that
integrating customer persona representations improves the state-of-the-art
graph convolution-based recommendation model by up to 12% in terms of NDCG@K
and F1-Score@K.

</details>


### [350] [IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval](https://arxiv.org/abs/2504.17529)
*Youngjune Lee,Haeyu Jeong,Changgeon Lim,Jeong Choi,Hongjun Lim,Hangon Kim,Jiyoon Kwon,Saehun Kim*

Main category: cs.IR

TL;DR: IRA框架通过动态兴趣单元和语义检索实现个性化推荐，适应实时用户兴趣变化，并已在NAVER的CAFE平台验证效果。


<details>
  <summary>Details</summary>
Motivation: 在线社区平台需动态个性化推荐，但实时适应变化在大规模工业场景中仍是挑战。

Method: 提出IRA框架，结合兴趣单元（动态更新用户兴趣）和基于语义的检索（减少时间偏差）。

Result: 实验验证IRA在真实数据集中有效，并成功部署于韩国领先社区平台NAVER CAFE。

Conclusion: IRA无需依赖历史训练分布，持续适应用户偏好，实现鲁棒个性化推荐。

Abstract: Online community platforms require dynamic personalized retrieval and
recommendation that can continuously adapt to evolving user interests and new
documents. However, optimizing models to handle such changes in real-time
remains a major challenge in large-scale industrial settings. To address this,
we propose the Interest-aware Representation and Alignment (IRA) framework, an
efficient and scalable approach that dynamically adapts to new interactions
through a cumulative structure. IRA leverages two key mechanisms: (1) Interest
Units that capture diverse user interests as contextual texts, while
reinforcing or fading over time through cumulative updates, and (2) a retrieval
process that measures the relevance between Interest Units and documents based
solely on semantic relationships, eliminating dependence on click signals to
mitigate temporal biases. By integrating cumulative Interest Unit updates with
the retrieval process, IRA continuously adapts to evolving user preferences,
ensuring robust and fine-grained personalization without being constrained by
past training distributions. We validate the effectiveness of IRA through
extensive experiments on real-world datasets, including its deployment in the
Home Section of NAVER's CAFE, South Korea's leading community platform.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [351] [S2Vec: Self-Supervised Geospatial Embeddings](https://arxiv.org/abs/2504.16942)
*Shushman Choudhury,Elad Aharoni,Chandrakumari Suvarna,Iveel Tsogsuren,Abdul Rahman Kreidieh,Chun-Ta Lu,Neha Arora*

Main category: cs.SI

TL;DR: S2Vec是一种新型的自监督框架，用于学习地理空间嵌入，通过S2几何库分区和掩码自编码技术生成通用地理表示，并在社会经济预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建可扩展的通用地理环境表示对地理空间人工智能应用至关重要。

Method: 使用S2几何库将大区域划分为离散S2单元，将特征向量栅格化为图像，并应用掩码自编码技术编码特征向量。

Result: S2Vec在三个大规模社会经济预测任务中表现优异，且与图像嵌入结合可进一步提升性能。

Conclusion: S2Vec能学习有效的地理通用表示，并能与其他数据模态互补。

Abstract: Scalable general-purpose representations of the built environment are crucial
for geospatial artificial intelligence applications. This paper introduces
S2Vec, a novel self-supervised framework for learning such geospatial
embeddings. S2Vec uses the S2 Geometry library to partition large areas into
discrete S2 cells, rasterizes built environment feature vectors within cells as
images, and applies masked autoencoding on these rasterized images to encode
the feature vectors. This approach yields task-agnostic embeddings that capture
local feature characteristics and broader spatial relationships. We evaluate
S2Vec on three large-scale socioeconomic prediction tasks, showing its
competitive performance against state-of-the-art image-based embeddings. We
also explore the benefits of combining S2Vec embeddings with image-based
embeddings downstream, showing that such multimodal fusion can often improve
performance. Our results highlight how S2Vec can learn effective
general-purpose geospatial representations and how it can complement other data
modalities in geospatial artificial intelligence.

</details>


### [352] [MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation](https://arxiv.org/abs/2504.16946)
*Xiaotong Ye,Nicolas Bougie,Toshihiko Yamasaki,Narimasa Watanabe*

Main category: cs.SI

TL;DR: 论文提出了一种可扩展的模拟框架，用于生成城市中4000多个代理的复杂移动行为，解决了现有方法在交通选择简化和计算资源需求上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟现代城市行为时过于简化交通选择，且计算资源需求高，无法支持大规模人口模拟。

Method: 构建虚拟城市模型，进行广泛调查以模拟群体行为选择与移动偏好，开发可扩展的模拟框架。

Result: 框架支持超过4000个代理的模拟，通过微观和宏观分析验证了生成行为的真实性，并探索了从移动模式预测人群密度等实验。

Conclusion: 该框架不仅提高了城市行为模拟的复杂性和规模，还为交通偏好等趋势分析提供了新视角。

Abstract: Generative agents offer promising capabilities for simulating realistic urban
behaviors. However, existing methods oversimplify transportation choices in
modern cities, and require prohibitive computational resources for large-scale
population simulation. To address these limitations, we first present a virtual
city that features multiple functional buildings and transportation modes.
Then, we conduct extensive surveys to model behavioral choices and mobility
preferences among population groups. Building on these insights, we introduce a
simulation framework that captures the complexity of urban mobility while
remaining scalable, enabling the simulation of over 4,000 agents. To assess the
realism of the generated behaviors, we perform a series of micro and
macro-level analyses. Beyond mere performance comparison, we explore insightful
experiments, such as predicting crowd density from movement patterns and
identifying trends in vehicle preferences across agent demographics.

</details>


### [353] [SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments](https://arxiv.org/abs/2504.16947)
*Dachun Sun,You Lyu,Jinning Li,Yizhuo Chen,Tianshi Wang,Tomoyoshi Kimura,Tarek Abdelzaher*

Main category: cs.SI

TL;DR: SCRAG是一个结合检索增强生成（RAG）与社会计算的预测框架，用于预测社交媒体帖子的社区响应，相比大型语言模型（LLM）有10%以上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统LLM依赖静态训练数据且易产生幻觉，难以在动态社交媒体环境中准确预测社区响应。SCRAG通过结合RAG与社会计算来解决这一问题。

Method: SCRAG整合LLM与RAG技术，通过检索目标社区的历史响应和外部知识（如新闻文章）来注入时间敏感上下文，联合预测新帖子的社区响应。

Result: 在X平台（原Twitter）的六种场景测试中，SCRAG平均关键指标提升超过10%，并能有效捕捉多样的意识形态和细微差别。

Conclusion: SCRAG为需要精准社区响应洞察的应用提供了一种有效的社会计算工具。

Abstract: This paper introduces SCRAG, a prediction framework inspired by social
computing, designed to forecast community responses to real or hypothetical
social media posts. SCRAG can be used by public relations specialists (e.g., to
craft messaging in ways that avoid unintended misinterpretations) or public
figures and influencers (e.g., to anticipate social responses), among other
applications related to public sentiment prediction, crisis management, and
social what-if analysis. While large language models (LLMs) have achieved
remarkable success in generating coherent and contextually rich text, their
reliance on static training data and susceptibility to hallucinations limit
their effectiveness at response forecasting in dynamic social media
environments. SCRAG overcomes these challenges by integrating LLMs with a
Retrieval-Augmented Generation (RAG) technique rooted in social computing.
Specifically, our framework retrieves (i) historical responses from the target
community to capture their ideological, semantic, and emotional makeup, and
(ii) external knowledge from sources such as news articles to inject
time-sensitive context. This information is then jointly used to forecast the
responses of the target community to new posts or narratives. Extensive
experiments across six scenarios on the X platform (formerly Twitter), tested
with various embedding models and LLMs, demonstrate over 10% improvements on
average in key evaluation metrics. A concrete example further shows its
effectiveness in capturing diverse ideologies and nuances. Our work provides a
social computing tool for applications where accurate and concrete insights
into community responses are crucial.

</details>


### [354] [S2Vec: Self-Supervised Geospatial Embeddings](https://arxiv.org/abs/2504.16942)
*Shushman Choudhury,Elad Aharoni,Chandrakumari Suvarna,Iveel Tsogsuren,Abdul Rahman Kreidieh,Chun-Ta Lu,Neha Arora*

Main category: cs.SI

TL;DR: S2Vec是一种自监督框架，通过S2几何库分割区域并栅格化特征向量，学习通用地理空间表示，在多项任务中表现优异，且能与其他数据模态融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 构建可扩展的通用地理空间表示对地理空间人工智能应用至关重要。

Method: 使用S2几何库将大区域划分为离散S2单元，栅格化单元内特征向量为图像，并应用掩码自编码学习嵌入。

Result: 在三个大规模社会经济预测任务中表现优异，与图像嵌入结合时性能进一步提升。

Conclusion: S2Vec能学习有效的通用地理空间表示，并能与其他数据模态互补。

Abstract: Scalable general-purpose representations of the built environment are crucial
for geospatial artificial intelligence applications. This paper introduces
S2Vec, a novel self-supervised framework for learning such geospatial
embeddings. S2Vec uses the S2 Geometry library to partition large areas into
discrete S2 cells, rasterizes built environment feature vectors within cells as
images, and applies masked autoencoding on these rasterized images to encode
the feature vectors. This approach yields task-agnostic embeddings that capture
local feature characteristics and broader spatial relationships. We evaluate
S2Vec on three large-scale socioeconomic prediction tasks, showing its
competitive performance against state-of-the-art image-based embeddings. We
also explore the benefits of combining S2Vec embeddings with image-based
embeddings downstream, showing that such multimodal fusion can often improve
performance. Our results highlight how S2Vec can learn effective
general-purpose geospatial representations and how it can complement other data
modalities in geospatial artificial intelligence.

</details>


### [355] [MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation](https://arxiv.org/abs/2504.16946)
*Xiaotong Ye,Nicolas Bougie,Toshihiko Yamasaki,Narimasa Watanabe*

Main category: cs.SI

TL;DR: 该论文提出了一种可扩展的生成代理框架，用于模拟大规模城市人口的行为和交通选择，解决了现有方法在计算资源和行为简化方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有生成代理方法在现代城市交通选择上过于简化，且计算资源需求高，难以支持大规模人口模拟。

Method: 首先构建虚拟城市模型，包含多功能建筑和交通方式；通过广泛调查建模人群行为和移动偏好；最后提出可扩展的仿真框架。

Result: 能够模拟超过4,000个代理的行为，并通过微观和宏观分析验证其真实性。

Conclusion: 该框架不仅能高效模拟复杂城市移动行为，还能支持预测人群密度和分析交通工具偏好趋势等应用。

Abstract: Generative agents offer promising capabilities for simulating realistic urban
behaviors. However, existing methods oversimplify transportation choices in
modern cities, and require prohibitive computational resources for large-scale
population simulation. To address these limitations, we first present a virtual
city that features multiple functional buildings and transportation modes.
Then, we conduct extensive surveys to model behavioral choices and mobility
preferences among population groups. Building on these insights, we introduce a
simulation framework that captures the complexity of urban mobility while
remaining scalable, enabling the simulation of over 4,000 agents. To assess the
realism of the generated behaviors, we perform a series of micro and
macro-level analyses. Beyond mere performance comparison, we explore insightful
experiments, such as predicting crowd density from movement patterns and
identifying trends in vehicle preferences across agent demographics.

</details>


### [356] [SCRAG: Social Computing-Based Retrieval Augmented Generation for Community Response Forecasting in Social Media Environments](https://arxiv.org/abs/2504.16947)
*Dachun Sun,You Lyu,Jinning Li,Yizhuo Chen,Tianshi Wang,Tomoyoshi Kimura,Tarek Abdelzaher*

Main category: cs.SI

TL;DR: SCRAG 是一个结合了检索增强生成（RAG）技术的预测框架，用于预测社区对社交媒体帖子的反应，比传统LLM方法在动态环境中更有效。


<details>
  <summary>Details</summary>
Motivation: 传统LLMs依赖静态数据且易产生幻觉，无法有效预测动态社交媒体环境中的社区反应。SCRAG旨在填补这一空白，为公共关系专家和公众人物提供更准确的社区反应预测工具。

Method: SCRAG通过检索目标社区的历史反应（意识形态、语义和情感构成）以及外部知识（如新闻文章），结合LLMs和RAG技术，生成对社区新帖子或叙事的反应预测。

Result: 在X平台（原Twitter）的六种场景中，SCRAG在关键评估指标上平均提高了10%以上，并能有效捕捉多样化的意识形态和细微差别。

Conclusion: SCRAG为需要准确洞察社区反应的应用提供了一个有效的社会计算工具，尤其在公共关系、危机管理和社交假设分析中具有重要价值。

Abstract: This paper introduces SCRAG, a prediction framework inspired by social
computing, designed to forecast community responses to real or hypothetical
social media posts. SCRAG can be used by public relations specialists (e.g., to
craft messaging in ways that avoid unintended misinterpretations) or public
figures and influencers (e.g., to anticipate social responses), among other
applications related to public sentiment prediction, crisis management, and
social what-if analysis. While large language models (LLMs) have achieved
remarkable success in generating coherent and contextually rich text, their
reliance on static training data and susceptibility to hallucinations limit
their effectiveness at response forecasting in dynamic social media
environments. SCRAG overcomes these challenges by integrating LLMs with a
Retrieval-Augmented Generation (RAG) technique rooted in social computing.
Specifically, our framework retrieves (i) historical responses from the target
community to capture their ideological, semantic, and emotional makeup, and
(ii) external knowledge from sources such as news articles to inject
time-sensitive context. This information is then jointly used to forecast the
responses of the target community to new posts or narratives. Extensive
experiments across six scenarios on the X platform (formerly Twitter), tested
with various embedding models and LLMs, demonstrate over 10% improvements on
average in key evaluation metrics. A concrete example further shows its
effectiveness in capturing diverse ideologies and nuances. Our work provides a
social computing tool for applications where accurate and concrete insights
into community responses are crucial.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [357] [Physics-informed features in supervised machine learning](https://arxiv.org/abs/2504.17112)
*Margherita Lampani,Sabrina Guastavino,Michele Piana,Federico Benvenuto*

Main category: stat.ML

TL;DR: 提出一种基于物理知识的特征机器学习方法，通过非线性特征映射提升模型可解释性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统特征机器学习方法忽略物理意义，限制了模型的可解释性，尤其在科学应用中尤为突出。

Method: 结合物理定律和量纲分析，构建非线性特征映射，并利用特征排序识别未知物理机制。

Result: 该方法提升了回归任务的预测性能和分类技能评分，并可能发现新的物理方程。

Conclusion: 物理知识融入机器学习可增强模型解释性和性能，为可解释机器学习开辟新途径。

Abstract: Supervised machine learning involves approximating an unknown functional
relationship from a limited dataset of features and corresponding labels. The
classical approach to feature-based machine learning typically relies on
applying linear regression to standardized features, without considering their
physical meaning. This may limit model explainability, particularly in
scientific applications. This study proposes a physics-informed approach to
feature-based machine learning that constructs non-linear feature maps informed
by physical laws and dimensional analysis. These maps enhance model
interpretability and, when physical laws are unknown, allow for the
identification of relevant mechanisms through feature ranking. The method aims
to improve both predictive performance in regression tasks and classification
skill scores by integrating domain knowledge into the learning process, while
also enabling the potential discovery of new physical equations within the
context of explainable machine learning.

</details>


### [358] [Causal rule ensemble approach for multi-arm data](https://arxiv.org/abs/2504.17166)
*Ke Wan,Kensuke Tanioka,Toshio Shimokawa*

Main category: stat.ML

TL;DR: 该论文提出了一种可解释的机器学习框架，用于多臂试验中的异质性治疗效果（HTE）估计，通过规则集成方法平衡了预测准确性和可解释性，并在仿真和真实数据中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有HTE估计方法主要用于二元治疗，且依赖黑盒模型，难以适用于多臂试验且缺乏可解释性。因此需要一种既能处理多干预又能保持可解释性的方法。

Method: 采用基于规则的集成方法，包括规则生成、规则集成和HTE估计三个步骤，确保方法既准确又可解释。

Result: 在仿真和实际数据中，该方法相比现有多臂HTE估计方法偏差更低、准确性更高，且提供了清晰的协变量影响解释。

Conclusion: 该框架填补了多臂HTE估计中准确性与可解释性的空白，为精准医学提供了实用工具。

Abstract: Heterogeneous treatment effect (HTE) estimation is critical in medical
research. It provides insights into how treatment effects vary among
individuals, which can provide statistical evidence for precision medicine.
While most existing methods focus on binary treatment situations, real-world
applications often involve multiple interventions. However, current HTE
estimation methods are primarily designed for binary comparisons and often rely
on black-box models, which limit their applicability and interpretability in
multi-arm settings. To address these challenges, we propose an interpretable
machine learning framework for HTE estimation in multi-arm trials. Our method
employs a rule-based ensemble approach consisting of rule generation, rule
ensemble, and HTE estimation, ensuring both predictive accuracy and
interpretability. Through extensive simulation studies and real data
applications, the performance of our method was evaluated against
state-of-the-art multi-arm HTE estimation approaches. The results indicate that
our approach achieved lower bias and higher estimation accuracy compared with
those of existing methods. Furthermore, the interpretability of our framework
allows clearer insights into how covariates influence treatment effects,
facilitating clinical decision making. By bridging the gap between accuracy and
interpretability, our study contributes a valuable tool for multi-arm HTE
estimation, supporting precision medicine.

</details>


### [359] [Likelihood-Free Variational Autoencoders](https://arxiv.org/abs/2504.17622)
*Chen Xu,Qiang Wang,Lijun Sun*

Main category: stat.ML

TL;DR: 该论文提出了一种名为EnVAE的无似然生成框架，通过能量评分构建重建损失，避免了传统VAE中因预定义似然函数导致的模糊重建问题，并进一步提出Fast EnVAE（FEnVAE）以提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器（VAE）依赖预定义的似然函数（如各向同性高斯分布），易导致似然函数设定错误，尤其在处理高维数据（如图像）时会出现模糊重建和低保真问题。为此，作者提出了一种无需显式参数化密度函数的无似然生成框架。

Method: 提出EnVAE框架，采用确定性解码器和能量评分（一种适当的评分规则）构建重建损失；为提升效率，进一步提出FEnVAE，利用解码器的局部平滑性和潜在变量后验分布的锐度，实现单样本高效训练目标。

Result: 在标准基准测试中，EnVAE在重建和生成质量上优于基于似然的基线方法，且计算效率高，可无缝集成到现有VAE流程中。

Conclusion: EnVAE为生成模型中的灵活、非参数化分布学习提供了一种通用、可扩展且统计理论支持的新方法。

Abstract: Variational Autoencoders (VAEs) typically rely on a probabilistic decoder
with a predefined likelihood, most commonly an isotropic Gaussian, to model the
data conditional on latent variables. While convenient for optimization, this
choice often leads to likelihood misspecification, resulting in blurry
reconstructions and poor data fidelity, especially for high-dimensional data
such as images. In this work, we propose \textit{EnVAE}, a novel
likelihood-free generative framework that has a deterministic decoder and
employs the energy score -- a proper scoring rule -- to build the
reconstruction loss. This enables likelihood-free inference without requiring
explicit parametric density functions. To address the computational
inefficiency of the energy score, we introduce a fast variant, \textit{FEnVAE},
based on the local smoothness of the decoder and the sharpness of the posterior
distribution of latent variables. This yields an efficient single-sample
training objective that integrates seamlessly into existing VAE pipelines with
minimal overhead. Empirical results on standard benchmarks demonstrate that
\textit{EnVAE} achieves superior reconstruction and generation quality compared
to likelihood-based baselines. Our framework offers a general, scalable, and
statistically principled alternative for flexible and nonparametric
distribution learning in generative modeling.

</details>


### [360] [Evaluating Uncertainty in Deep Gaussian Processes](https://arxiv.org/abs/2504.17719)
*Matthijs van der Lende,Jeremias Lino Ferrao,Niclas Müller-Hof*

Main category: stat.ML

TL;DR: 本文评估了深度高斯过程（DGPs）和深度Sigma点过程（DSPPs）在回归和分类任务中的不确定性量化性能，发现DSPPs在分布内校准表现良好，但在分布偏移下的鲁棒性不如深度集成方法。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中可靠的不确定性估计至关重要，但深度高斯过程和深度Sigma点过程在分布偏移下的校准和鲁棒性尚未充分研究。

Method: 通过回归任务（CASP数据集）和分类任务（ESR数据集）评估预测性能（MAE、准确率）、校准（NLL、ECE）以及在合成特征级分布偏移下的鲁棒性。

Result: DSPPs在分布内校准表现优异，但在分布偏移下深度集成方法在性能和校准上均表现更鲁棒。基于GP的方法在某些指标上表现出敏感性。

Conclusion: 深度集成方法作为鲁棒基准表现突出，而深度GP方法虽在分布内校准良好，但其在分布偏移下的实际鲁棒性需谨慎评估。

Abstract: Reliable uncertainty estimates are crucial in modern machine learning. Deep
Gaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs
hierarchically, offering promising methods for uncertainty quantification
grounded in Bayesian principles. However, their empirical calibration and
robustness under distribution shift relative to baselines like Deep Ensembles
remain understudied. This work evaluates these models on regression (CASP
dataset) and classification (ESR dataset) tasks, assessing predictive
performance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)
and Expected Calibration Error (ECE), alongside robustness under various
synthetic feature-level distribution shifts. Results indicate DSPPs provide
strong in-distribution calibration leveraging their sigma point approximations.
However, compared to Deep Ensembles, which demonstrated superior robustness in
both per- formance and calibration under the tested shifts, the GP-based
methods showed vulnerabilities, exhibiting particular sensitivity in the
observed metrics. Our findings underscore ensembles as a robust baseline,
suggesting that while deep GP methods offer good in-distribution calibration,
their practical robustness under distribution shift requires careful
evaluation. To facilitate reproducibility, we make our code available at
https://github.com/matthjs/xai-gp.

</details>


### [361] [Physics-informed features in supervised machine learning](https://arxiv.org/abs/2504.17112)
*Margherita Lampani,Sabrina Guastavino,Michele Piana,Federico Benvenuto*

Main category: stat.ML

TL;DR: 该论文提出了一种基于物理学信息的特征映射方法，通过结合物理定律和维度分析提升模型可解释性，同时增强预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统特征机器学习的线性回归方法忽略了特征的物理意义，导致模型可解释性不足，尤其在科学应用中。

Method: 通过物理定律和维度分析构建非线性特征映射，结合领域知识提升模型解释性和性能。

Result: 该方法提高了回归任务的预测性能和分类技能分数，并可能通过特征排序发现新的物理方程。

Conclusion: 物理学信息的特征映射不仅增强了模型可解释性，还为可解释机器学习中的新物理发现提供了可能。

Abstract: Supervised machine learning involves approximating an unknown functional
relationship from a limited dataset of features and corresponding labels. The
classical approach to feature-based machine learning typically relies on
applying linear regression to standardized features, without considering their
physical meaning. This may limit model explainability, particularly in
scientific applications. This study proposes a physics-informed approach to
feature-based machine learning that constructs non-linear feature maps informed
by physical laws and dimensional analysis. These maps enhance model
interpretability and, when physical laws are unknown, allow for the
identification of relevant mechanisms through feature ranking. The method aims
to improve both predictive performance in regression tasks and classification
skill scores by integrating domain knowledge into the learning process, while
also enabling the potential discovery of new physical equations within the
context of explainable machine learning.

</details>


### [362] [Causal rule ensemble approach for multi-arm data](https://arxiv.org/abs/2504.17166)
*Ke Wan,Kensuke Tanioka,Toshio Shimokawa*

Main category: stat.ML

TL;DR: 论文提出了一种可解释的机器学习框架，用于多臂试验中的异质治疗效果（HTE）估计，通过规则集成方法在保持预测准确性的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有HTE估计方法多针对二元治疗且依赖黑盒模型，限制了多臂场景的适用性和可解释性，需要一种兼顾精度与解释性的方法。

Method: 采用基于规则的集成方法，包括规则生成、规则集成和HTE估计三步，确保预测准确性和模型可解释性。

Result: 与现有方法相比，该方法偏差更低、估计精度更高，且可清晰展示协变量对治疗效果的影响。

Conclusion: 该框架填补了多臂HTE估计中精度与解释性的空白，为精准医疗提供了实用工具。

Abstract: Heterogeneous treatment effect (HTE) estimation is critical in medical
research. It provides insights into how treatment effects vary among
individuals, which can provide statistical evidence for precision medicine.
While most existing methods focus on binary treatment situations, real-world
applications often involve multiple interventions. However, current HTE
estimation methods are primarily designed for binary comparisons and often rely
on black-box models, which limit their applicability and interpretability in
multi-arm settings. To address these challenges, we propose an interpretable
machine learning framework for HTE estimation in multi-arm trials. Our method
employs a rule-based ensemble approach consisting of rule generation, rule
ensemble, and HTE estimation, ensuring both predictive accuracy and
interpretability. Through extensive simulation studies and real data
applications, the performance of our method was evaluated against
state-of-the-art multi-arm HTE estimation approaches. The results indicate that
our approach achieved lower bias and higher estimation accuracy compared with
those of existing methods. Furthermore, the interpretability of our framework
allows clearer insights into how covariates influence treatment effects,
facilitating clinical decision making. By bridging the gap between accuracy and
interpretability, our study contributes a valuable tool for multi-arm HTE
estimation, supporting precision medicine.

</details>


### [363] [Likelihood-Free Variational Autoencoders](https://arxiv.org/abs/2504.17622)
*Chen Xu,Qiang Wang,Lijun Sun*

Main category: stat.ML

TL;DR: 提出了一种无似然的生成框架EnVAE，使用能量分数构建重建损失，避免了传统VAE中因似然预设导致的模糊问题。并通过快速变体FEnVAE解决了计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统VAE依赖预设的似然函数（如各向同性高斯），常导致数据保真度差（如图像模糊）。希望通过无似然方法提升生成质量。

Method: 1. 提出EnVAE，用能量分数代替似然函数构建重建损失；2. 引入FEnVAE，利用解码器局部平滑性和后验分布的尖锐性加速计算。

Result: 在标准测试集上，EnVAE在重建和生成质量上优于基于似然的基线方法。

Conclusion: EnVAE提供了一个通用、可扩展且统计严格的无似然生成框架，适用于非参数分布学习。

Abstract: Variational Autoencoders (VAEs) typically rely on a probabilistic decoder
with a predefined likelihood, most commonly an isotropic Gaussian, to model the
data conditional on latent variables. While convenient for optimization, this
choice often leads to likelihood misspecification, resulting in blurry
reconstructions and poor data fidelity, especially for high-dimensional data
such as images. In this work, we propose \textit{EnVAE}, a novel
likelihood-free generative framework that has a deterministic decoder and
employs the energy score -- a proper scoring rule -- to build the
reconstruction loss. This enables likelihood-free inference without requiring
explicit parametric density functions. To address the computational
inefficiency of the energy score, we introduce a fast variant, \textit{FEnVAE},
based on the local smoothness of the decoder and the sharpness of the posterior
distribution of latent variables. This yields an efficient single-sample
training objective that integrates seamlessly into existing VAE pipelines with
minimal overhead. Empirical results on standard benchmarks demonstrate that
\textit{EnVAE} achieves superior reconstruction and generation quality compared
to likelihood-based baselines. Our framework offers a general, scalable, and
statistically principled alternative for flexible and nonparametric
distribution learning in generative modeling.

</details>


### [364] [Evaluating Uncertainty in Deep Gaussian Processes](https://arxiv.org/abs/2504.17719)
*Matthijs van der Lende,Jeremias Lino Ferrao,Niclas Müller-Hof*

Main category: stat.ML

TL;DR: 该论文评估了深度高斯过程（DGP）和深度Sigma点过程（DSPP）在回归和分类任务中的不确定性量化表现，包括预测性能、校准以及分布偏移下的稳健性。结果表明，虽然DSPP在分布内校准表现良好，但深度集成方法在稳健性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估深度高斯过程和深度Sigma点过程在不确定性量化中的表现，尤其是在分布偏移情况下的稳健性，以对比深度集成等基线方法。

Method: 论文在回归（CASP数据集）和分类（ESR数据集）任务上测试了DGP和DSPP模型，评估了预测性能（如MAE和准确率）、校准（使用NLL和ECE指标）以及在合成特征级分布偏移下的稳健性。

Result: 结果显示，DSPP在分布内校准表现良好，但深度集成方法在所有测试的分布偏移中表现出更高的稳健性。GPs类方法在稳健性上表现较弱，特别是在某些指标上显示出敏感性。

Conclusion: 论文结论指出，尽管深度GPs方法在分布内校准上表现优秀，但其在分布偏移下的实际稳健性需要进一步评估。深度集成方法被证明是更具稳健性的基线。

Abstract: Reliable uncertainty estimates are crucial in modern machine learning. Deep
Gaussian Processes (DGPs) and Deep Sigma Point Processes (DSPPs) extend GPs
hierarchically, offering promising methods for uncertainty quantification
grounded in Bayesian principles. However, their empirical calibration and
robustness under distribution shift relative to baselines like Deep Ensembles
remain understudied. This work evaluates these models on regression (CASP
dataset) and classification (ESR dataset) tasks, assessing predictive
performance (MAE, Accu- racy), calibration using Negative Log-Likelihood (NLL)
and Expected Calibration Error (ECE), alongside robustness under various
synthetic feature-level distribution shifts. Results indicate DSPPs provide
strong in-distribution calibration leveraging their sigma point approximations.
However, compared to Deep Ensembles, which demonstrated superior robustness in
both per- formance and calibration under the tested shifts, the GP-based
methods showed vulnerabilities, exhibiting particular sensitivity in the
observed metrics. Our findings underscore ensembles as a robust baseline,
suggesting that while deep GP methods offer good in-distribution calibration,
their practical robustness under distribution shift requires careful
evaluation. To facilitate reproducibility, we make our code available at
https://github.com/matthjs/xai-gp.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [365] [Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline](https://arxiv.org/abs/2504.16979)
*Masoud Tafavvoghi,Lars Ailo Bongo,André Berli Delgado,Nikita Shvetsov,Anders Sildnes,Line Moi,Lill-Tove Rasmussen Busund,Kajsa Møllersen*

Main category: q-bio.QM

TL;DR: 该研究通过QuPath构建了一个端到端的肿瘤浸润淋巴细胞（TILs）评估流程，验证了自动化工具在乳腺癌H&E染色全切片图像中TILs评估的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种简单易用的自动化工具，用于精准评估乳腺癌H&E染色全切片图像中的TILs密度，以减少人工评估的主观性并提高效率。

Method: 研究首先训练了一个像素分类器分割肿瘤相关基质，随后使用预训练的StarDist深度学习模型检测细胞并训练二元分类器区分TILs。通过计算TIL密度并与病理学家评分对比评估流程效果。

Result: 研究在外部测试集上获得了0.71的Cohen's kappa值，表明自动化工具能够较准确地评估TILs密度，结果与病理学家评分高度一致。

Conclusion: 研究证实，利用现有软件可以实现乳腺癌H&E染色全切片图像中TILs的自动化评估，为临床研究提供了实用工具。

Abstract: In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)
assessment pipeline within QuPath, demonstrating the potential of easily
accessible tools to perform complex tasks in a fully automatic fashion. First,
we trained a pixel classifier to segment tumor, tumor-associated stroma, and
other tissue compartments in breast cancer H&E-stained whole-slide images (WSI)
to isolate tumor-associated stroma for subsequent analysis. Next, we applied a
pre-trained StarDist deep learning model in QuPath for cell detection and used
the extracted cell features to train a binary classifier distinguishing TILs
from other cells. To evaluate our TILs assessment pipeline, we calculated the
TIL density in each WSI and categorized them as low, medium, or high TIL
levels. Our pipeline was evaluated against pathologist-assigned TIL scores,
achieving a Cohen's kappa of 0.71 on the external test set, corroborating
previous research findings. These results confirm that existing software can
offer a practical solution for the assessment of TILs in H&E-stained WSIs of
breast cancer.

</details>


### [366] [Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline](https://arxiv.org/abs/2504.16979)
*Masoud Tafavvoghi,Lars Ailo Bongo,André Berli Delgado,Nikita Shvetsov,Anders Sildnes,Line Moi,Lill-Tove Rasmussen Busund,Kajsa Møllersen*

Main category: q-bio.QM

TL;DR: 该研究在QuPath中构建了一个端到端的肿瘤浸润淋巴细胞（TILs）评估流程，展示如何利用易访问工具自动化完成复杂任务，并在乳腺癌H&E染色全玻片图像中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在证明现有软件（如QuPath）可以自动化完成TILs评估这一复杂任务，为乳腺癌病理分析提供实用解决方案。

Method: 1. 训练像素分类器分割肿瘤及肿瘤相关间质；2. 使用预训练的StarDist模型检测细胞；3. 基于细胞特征训练二分类器区分TILs与其他细胞；4. 计算TIL密度并分类为低、中、高三级。

Result: 流程与病理学家评分对比的Cohen's kappa达0.71（外部测试集），验证了其可行性。

Conclusion: 现有软件可有效支持乳腺癌H&E切片中TILs的自动化评估，结果与人工评分一致性较高。

Abstract: In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)
assessment pipeline within QuPath, demonstrating the potential of easily
accessible tools to perform complex tasks in a fully automatic fashion. First,
we trained a pixel classifier to segment tumor, tumor-associated stroma, and
other tissue compartments in breast cancer H&E-stained whole-slide images (WSI)
to isolate tumor-associated stroma for subsequent analysis. Next, we applied a
pre-trained StarDist deep learning model in QuPath for cell detection and used
the extracted cell features to train a binary classifier distinguishing TILs
from other cells. To evaluate our TILs assessment pipeline, we calculated the
TIL density in each WSI and categorized them as low, medium, or high TIL
levels. Our pipeline was evaluated against pathologist-assigned TIL scores,
achieving a Cohen's kappa of 0.71 on the external test set, corroborating
previous research findings. These results confirm that existing software can
offer a practical solution for the assessment of TILs in H&E-stained WSIs of
breast cancer.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [367] [Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints](https://arxiv.org/abs/2504.17142)
*Siddharth Nair,Timothy F. Walsh,Greg Pickrell,Fabio Semperlotti*

Main category: physics.comp-ph

TL;DR: 本研究开发了一种基于强化学习的微电子组件设计方法，用于处理多物理约束下的高维解空间优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统全局优化方法在小规模设计参数下有效，但对于解空间复杂且多物理约束的微电子组件设计（如ASIC和HI中介层）面临挑战，因此需要新技术。

Method: 采用强化学习（RL）框架，针对ASIC芯片的键合互连几何形状和HI中介层组件布局进行优化，并在数值实验中验证。

Result: 提出的RL方法能够有效优化高维解空间中的组件布局，同时满足热弹性和设计约束。

Conclusion: 强化学习为复杂多物理约束下的微电子设计提供了高效优化方案，优于传统方法。

Abstract: This study focuses on the development of reinforcement learning based
techniques for the design of microelectronic components under multiphysics
constraints. While traditional design approaches based on global optimization
approaches are effective when dealing with a small number of design parameters,
as the complexity of the solution space and of the constraints increases
different techniques are needed. This is an important reason that makes the
design and optimization of microelectronic components (characterized by large
solution space and multiphysics constraints) very challenging for traditional
methods. By taking as prototypical elements an application-specific integrated
circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and
numerically test an optimization framework based on reinforcement learning
(RL). More specifically, we consider the optimization of the bonded
interconnect geometry for an ASIC chip as well as the placement of components
on a HI interposer while satisfying thermoelastic and design constraints. This
placement problem is particularly interesting because it features a
high-dimensional solution space.

</details>


### [368] [Reinforcement learning framework for the mechanical design of microelectronic components under multiphysics constraints](https://arxiv.org/abs/2504.17142)
*Siddharth Nair,Timothy F. Walsh,Greg Pickrell,Fabio Semperlotti*

Main category: physics.comp-ph

TL;DR: 开发基于强化学习的微电子组件设计方法，以解决传统方法在高维解空间和多物理约束下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在设计参数较少时有效，但在解空间复杂且多物理约束下难以应对，微电子组件设计尤其具有挑战性。

Method: 采用强化学习框架，以ASIC和HI中介层为例，优化键合互连几何形状和组件布局，同时满足热弹性和设计约束。

Result: 数值测试验证了强化学习在高维解空间和多物理约束优化中的有效性。

Conclusion: 强化学习为解决复杂微电子组件设计问题提供了新思路。

Abstract: This study focuses on the development of reinforcement learning based
techniques for the design of microelectronic components under multiphysics
constraints. While traditional design approaches based on global optimization
approaches are effective when dealing with a small number of design parameters,
as the complexity of the solution space and of the constraints increases
different techniques are needed. This is an important reason that makes the
design and optimization of microelectronic components (characterized by large
solution space and multiphysics constraints) very challenging for traditional
methods. By taking as prototypical elements an application-specific integrated
circuit (ASIC) and a heterogeneously integrated (HI) interposer, we develop and
numerically test an optimization framework based on reinforcement learning
(RL). More specifically, we consider the optimization of the bonded
interconnect geometry for an ASIC chip as well as the placement of components
on a HI interposer while satisfying thermoelastic and design constraints. This
placement problem is particularly interesting because it features a
high-dimensional solution space.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [369] [On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration](https://arxiv.org/abs/2504.17376)
*Maoyang Xiang,Ramesh Fernando,Bo Wang*

Main category: cs.AR

TL;DR: 该论文提出了一种高效框架，用于在Xilinx Kria KV260边缘平台上部署Qwen2.5-0.5B模型，通过激活感知权重量化（AWQ）和FPGA加速执行流水线，实现了55.08%的模型压缩率和5.1 tokens/秒的输出速率，优于基线性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上部署面临计算需求高、内存带宽限制和能耗等挑战，论文旨在解决这些问题。

Method: 采用激活感知权重量化（AWQ）与FPGA加速执行流水线，并提出混合执行策略，将计算密集型任务卸载到FPGA，CPU处理轻量任务。

Result: 实现了55.08%的模型压缩率和5.1 tokens/秒的输出速率，显著优于基线性能（2.8 tokens/秒）。

Conclusion: 该框架为边缘设备高效部署LLMs提供了可行方案，通过量化与硬件加速平衡了性能与资源消耗。

Abstract: Transformer-based Large Language Models (LLMs) have significantly advanced AI
capabilities but pose considerable challenges for deployment on edge devices
due to high computational demands, memory bandwidth constraints, and energy
consumption. This paper addresses these challenges by presenting an efficient
framework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge
platform, a heterogeneous system integrating an ARM Cortex-A53 CPU with
reconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization
(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances
both model compression rate and system throughput. Additionally, we propose a
hybrid execution strategy that intelligently offloads compute-intensive
operations to the FPGA while utilizing the CPU for lighter tasks, effectively
balancing the computational workload and maximizing overall performance. Our
framework achieves a model compression rate of 55.08% compared to the original
model and produces output at a rate of 5.1 tokens per second, outperforming the
baseline performance of 2.8 tokens per second.

</details>


### [370] [L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2504.17584)
*Qingyuan Liu,Liyan Chen,Yanning Yang,Haocheng Wang,Dong Du,Zhigang Mao,Naifeng Jing,Yubin Xia,Haibo Chen*

Main category: cs.AR

TL;DR: L3是一个硬件-软件协同设计的系统，通过集成DIMM-PIM和GPU设备，解决了LLM在处理长文本序列时GPU内存带宽和容量的瓶颈问题，实现了最高6.1倍的性能提升和更大的批处理规模。


<details>
  <summary>Details</summary>
Motivation: 由于GPU内存容量和带宽的限制，LLM在处理长文本序列时面临性能瓶颈，特别是解码阶段的多头注意力机制对存储和计算需求极高。

Method: L3系统通过硬件重新设计解决DIMM-PIM中的数据布局和计算单元不匹配问题，优化通信以隐藏数据传输开销，并使用自适应调度器最大化设备间并行性。

Result: 实验结果表明，L3比当前最先进的HBM-PIM解决方案快6.1倍，并显著提高了批处理规模。

Conclusion: L3通过硬件-软件协同设计和DIMM-PIM的集成，有效解决了LLM推理中的内存瓶颈问题，证明了其在性能和扩展性上的优势。

Abstract: Large Language Models (LLMs) increasingly require processing long text
sequences, but GPU memory limitations force difficult trade-offs between memory
capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its
capacity remains constrained. Offloading data to host-side DIMMs improves
capacity but introduces costly data swapping overhead. We identify that the
critical memory bottleneck lies in the decoding phase of multi-head attention
(MHA) exclusively, which demands substantial capacity for storing KV caches and
high bandwidth for attention computation. Our key insight reveals this
operation uniquely aligns with modern DIMM-based processing-in-memory (PIM)
architectures, which offers scalability of both capacity and bandwidth.
  Based on this observation and insight, we propose L3, a hardware-software
co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three
innovations: First, hardware redesigns resolve data layout mismatches and
computational element mismatches in DIMM-PIM, enhancing LLM inference
utilization. Second, communication optimization enables hiding the data
transfer overhead with the computation. Third, an adaptive scheduler
coordinates GPU-DIMM-PIM operations to maximize parallelism between devices.
Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup
over state-of-the-art HBM-PIM solutions while significantly improving batch
sizes.

</details>


### [371] [On-Device Qwen2.5: Efficient LLM Inference with Model Compression and Hardware Acceleration](https://arxiv.org/abs/2504.17376)
*Maoyang Xiang,Ramesh Fernando,Bo Wang*

Main category: cs.AR

TL;DR: 该论文提出了一种高效框架，将Qwen2.5-0.5B模型部署在Xilinx Kria KV260边缘平台上，通过激活感知权重量化（AWQ）和FPGA加速执行管线，实现了55.08%的模型压缩率和每秒5.1个令牌的输出速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在边缘设备上部署时面临计算需求高、内存带宽受限和能耗大的挑战，为此论文提出了一种优化框架。

Method: 结合激活感知权重量化（AWQ）与FPGA加速执行管线，采用混合执行策略，将计算密集型任务卸载到FPGA，轻量任务由CPU处理。

Result: 模型压缩率达到55.08%，输出速度提升至5.1 tokens/s，显著优于基准性能的2.8 tokens/s。

Conclusion: 该框架有效解决了LLMs在边缘设备上的部署难题，为实际应用提供了高性能与高效能的解决方案。

Abstract: Transformer-based Large Language Models (LLMs) have significantly advanced AI
capabilities but pose considerable challenges for deployment on edge devices
due to high computational demands, memory bandwidth constraints, and energy
consumption. This paper addresses these challenges by presenting an efficient
framework for deploying the Qwen2.5-0.5B model on the Xilinx Kria KV260 edge
platform, a heterogeneous system integrating an ARM Cortex-A53 CPU with
reconfigurable FPGA logic. Leveraging Activation-aware Weight Quantization
(AWQ) with FPGA-accelerated execution pipelines, the proposed approach enhances
both model compression rate and system throughput. Additionally, we propose a
hybrid execution strategy that intelligently offloads compute-intensive
operations to the FPGA while utilizing the CPU for lighter tasks, effectively
balancing the computational workload and maximizing overall performance. Our
framework achieves a model compression rate of 55.08% compared to the original
model and produces output at a rate of 5.1 tokens per second, outperforming the
baseline performance of 2.8 tokens per second.

</details>


### [372] [L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2504.17584)
*Qingyuan Liu,Liyan Chen,Yanning Yang,Haocheng Wang,Dong Du,Zhigang Mao,Naifeng Jing,Yubin Xia,Haibo Chen*

Main category: cs.AR

TL;DR: 论文提出了L3系统，结合DIMM-PIM和GPU设备，通过软硬件协同设计解决了LLM解码阶段的内存瓶颈，实现了6.1倍的加速。


<details>
  <summary>Details</summary>
Motivation: 由于GPU内存限制，处理长文本序列时需要权衡内存容量和带宽。作者发现多头注意力（MHA）的解码阶段是内存瓶颈的关键，并提出利用现代DIMM-PIM架构的优势来解决这一问题。

Method: 设计了L3系统，包括硬件改进以解决DIMM-PIM的数据布局和计算元素不匹配问题、优化通信以隐藏数据传输开销，以及使用自适应调度器协调GPU和DIMM-PIM的操作以实现最大并行性。

Result: 实验结果显示，L3比当前最先进的HBM-PIM解决方案提升了6.1倍的性能，同时显著增加了批量大小。

Conclusion: L3通过软硬件协同设计有效解决了LLM解码阶段的内存瓶颈，显著提升了处理长文本序列的性能。

Abstract: Large Language Models (LLMs) increasingly require processing long text
sequences, but GPU memory limitations force difficult trade-offs between memory
capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its
capacity remains constrained. Offloading data to host-side DIMMs improves
capacity but introduces costly data swapping overhead. We identify that the
critical memory bottleneck lies in the decoding phase of multi-head attention
(MHA) exclusively, which demands substantial capacity for storing KV caches and
high bandwidth for attention computation. Our key insight reveals this
operation uniquely aligns with modern DIMM-based processing-in-memory (PIM)
architectures, which offers scalability of both capacity and bandwidth.
  Based on this observation and insight, we propose L3, a hardware-software
co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three
innovations: First, hardware redesigns resolve data layout mismatches and
computational element mismatches in DIMM-PIM, enhancing LLM inference
utilization. Second, communication optimization enables hiding the data
transfer overhead with the computation. Third, an adaptive scheduler
coordinates GPU-DIMM-PIM operations to maximize parallelism between devices.
Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup
over state-of-the-art HBM-PIM solutions while significantly improving batch
sizes.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [373] [Analyzing Value Functions of States in Parametric Markov Chains](https://arxiv.org/abs/2504.17020)
*Kasper Engelen,Guillermo A. Pérez,Shrisha Rao*

Main category: cs.LO

TL;DR: 该论文提出了一种高效算法来简化参数化马尔可夫链（pMC）中的等价类，通过验证单调性并将其转化为状态可达概率的比较问题。实验表明，该方法能有效缩减模型规模并加速现有单调性和参数提升算法的验证。


<details>
  <summary>Details</summary>
Motivation: 尽管参数化马尔可夫链的验证已证明复杂，但通过研究单调性等更易验证的属性，可以简化过程。论文旨在通过等价类合并，提升实际应用中的预处理效率。

Method: 将单调性问题转化为状态间可达概率的比较问题，利用已有高效算法合并等价类，从而保留验证结果和单调性。实现了一种算法来合并“平凡”等价类。

Result: 实验显示，合并操作能缩减部分基准测试的模型规模，且在自定义测试中效果显著；还能加速单调性和参数提升算法的运行，适合作为快速预处理步骤。

Conclusion: 该算法通过简化等价类，有效提升了参数化马尔可夫链验证的效率，尤其在预处理阶段表现出实用价值。

Abstract: Parametric Markov chains (pMC) are used to model probabilistic systems with
unknown or partially known probabilities. Although (universal) pMC verification
for reachability properties is known to be coETR-complete, there have been
efforts to approach it using potentially easier-to-check properties such as
asking whether the pMC is monotonic in certain parameters. In this paper, we
first reduce monotonicity to asking whether the reachability probability from a
given state is never less than that of another given state. Recent results for
the latter property imply an efficient algorithm to collapse same-value
equivalence classes, which in turn preserves verification results and
monotonicity. We implement our algorithm to collapse "trivial" equivalence
classes in the pMC and show empirical evidence for the following: First, the
collapse gives reductions in size for some existing benchmarks and significant
reductions on some custom benchmarks; Second, the collapse speeds up existing
algorithms to check monotonicity and parameter lifting, and hence can be used
as a fast pre-processing step in practice.

</details>


### [374] [Analyzing Value Functions of States in Parametric Markov Chains](https://arxiv.org/abs/2504.17020)
*Kasper Engelen,Guillermo A. Pérez,Shrisha Rao*

Main category: cs.LO

TL;DR: 该论文提出了一种将参数马尔可夫链（pMC）单调性验证简化为状态间到达概率比较的方法，并通过高效算法合并等价类，从而提升现有验证算法的效率。


<details>
  <summary>Details</summary>
Motivation: 尽管参数马尔可夫链的通用验证问题是coETR完全的，但通过研究更易检查的单调性等性质来简化验证过程仍具有重要意义。

Method: 将单调性验证问题转化为状态间到达概率的比较，并利用高效算法合并等价类，从而保留验证结果和单调性。

Result: 实验证明，合并等价类能显著缩小部分基准模型的规模，并加速单调性和参数提升算法的运行效率。

Conclusion: 该算法可作为高效的预处理步骤，提升参数马尔可夫链验证的实际效率。

Abstract: Parametric Markov chains (pMC) are used to model probabilistic systems with
unknown or partially known probabilities. Although (universal) pMC verification
for reachability properties is known to be coETR-complete, there have been
efforts to approach it using potentially easier-to-check properties such as
asking whether the pMC is monotonic in certain parameters. In this paper, we
first reduce monotonicity to asking whether the reachability probability from a
given state is never less than that of another given state. Recent results for
the latter property imply an efficient algorithm to collapse same-value
equivalence classes, which in turn preserves verification results and
monotonicity. We implement our algorithm to collapse "trivial" equivalence
classes in the pMC and show empirical evidence for the following: First, the
collapse gives reductions in size for some existing benchmarks and significant
reductions on some custom benchmarks; Second, the collapse speeds up existing
algorithms to check monotonicity and parameter lifting, and hence can be used
as a fast pre-processing step in practice.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [375] [Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor](https://arxiv.org/abs/2504.16941)
*Zakaria Lamine,Abdelatif Hafid,Mohamed Rahouti*

Main category: q-bio.BM

TL;DR: 该研究提出了一种基于上同调的新数学模型，通过KEEL定理和边界类构建，用于蛋白质结构分析与预测，特别是鞭毛马达结构，为生物大分子建模提供了新的几何与代数基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过数学方法（如上同调）解决生物大分子（如蛋白质）的结构分析与预测问题，尤其针对鞭毛马达结构的复杂性。

Method: 利用KEEL定理证明上同调的必然性，通过边界类和曲线构建单纯复形，结合斜交换分级代数，应用结构定理连接不同的同调群。

Result: 模型成功应用于蛋白质结构（如鞭毛马达）的分析与预测，提供了对生物大分子几何和代数特性的新理解。

Conclusion: 该研究为结构生物学提供了新的数学建模工具，展现了其在生物大分子分析中的潜力。

Abstract: This study presents a novel mathematical model derived from cohomology,
leveraging the KEEL-proven theorem that establishes cohomology as tautological,
generated by boundary classes of curves with fixed dual graphs. Simplicial
complexes are constructed using skew-commutative graded algebra, and the
structure theorem is applied to connect distinct homologies, enabling precise
interpretations of the resulting geometric forms. The proposed model is
utilized for protein structure analysis and prediction, with a specific
application to the Flagellar Motor structure. This approach offers new insights
into the geometric and algebraic foundations of biological macromolecular
modeling, highlighting its potential for advancement in structural biology.

</details>


### [376] [Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates](https://arxiv.org/abs/2504.17624)
*Jigang Fan,Chunhao Zhu,Xiaobing Lan,Haiming Zhuang,Mingyu Li,Jian Zhang,Shaoyong Lu*

Main category: q-bio.BM

TL;DR: 研究通过新型计算与实验方法揭示了NTSR1的动态激活机制与偏向信号传导，发现了一个隐秘的变构位点，为开发靶向NTSR1的药物提供了原子层面的理论支持。


<details>
  <summary>Details</summary>
Motivation: NTSR1作为G蛋白偶联受体家族成员，不仅在多巴胺神经元活动中起调控作用，还可能通过偏向β-arrestin信号通路减少药物滥用，为治疗成瘾相关疾病提供了潜在途径。

Method: 结合了弹性带分子动力学模拟、马尔可夫状态模型、时间通信网络分析、定点突变及构象生物传感器等多种技术，探索NTSR1的激活与信号偏向机制。

Result: 揭示了NTSR1的动态逐步激活机制及关键信号网络，识别了受体细胞内区域的一个隐秘变构位点，阐明了极性网络、非保守离子锁与芳香簇的复杂相互作用。

Conclusion: 研究成果加深了对NTSR1原子层面激活机制的理解，为开发其变构调节剂及治疗成瘾等疾病提供了新策略。

Abstract: Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled
receptor superfamily, plays an important role in modulating dopaminergic
neuronal activity and eliciting opioid-independent analgesia. Recent studies
suggest that promoting \{beta}-arrestin-biased signaling in NTSR1 may diminish
drugs of abuse, such as psychostimulants, thereby offering a potential avenue
for treating human addiction-related disorders. In this study, we utilized a
novel computational and experimental approach that combined nudged elastic
band-based molecular dynamics simulations, Markov state models, temporal
communication network analysis, site-directed mutagenesis, and conformational
biosensors, to explore the intricate mechanisms underlying NTSR1 activation and
biased signaling. Our study reveals a dynamic stepwise transition mechanism and
activated transmission network associated with NTSR1 activation. It also yields
valuable insights into the complex interplay between the unique polar network,
non-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we
identified a cryptic allosteric site located in the intracellular region of the
receptor that exists in an intermediate state within the activation pathway.
Collectively, these findings contribute to a more profound understanding of
NTSR1 activation and biased signaling at the atomic level, thereby providing a
potential strategy for the development of NTSR1 allosteric modulators in the
realm of G protein-coupled receptor biology, biophysics, and medicine.

</details>


### [377] [Mathematical Modeling of Protein Structures: A Cohomology-Based Approach to the Flagellar Motor](https://arxiv.org/abs/2504.16941)
*Zakaria Lamine,Abdelatif Hafid,Mohamed Rahouti*

Main category: q-bio.BM

TL;DR: 该研究提出了一种基于上同调的新数学模型，利用KEEL定理构建并解析曲线边界类，应用于蛋白质结构分析，特别是鞭毛马达结构，为生物大分子建模提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过上同调的数学方法，为生物大分子（如蛋白质）的结构分析和预测提供更精确的几何与代数框架。

Method: 基于KEEL定理从上同调推导数学模型，利用斜交换分级代数和结构定理连接不同同调，构建单纯复形并解析几何形态。

Result: 模型成功应用于鞭毛马达结构的分析，展示了其在生物大分子建模中的潜力。

Conclusion: 该方法为结构生物学提供了新的数学工具，有望推动生物大分子几何与代数建模的进步。

Abstract: This study presents a novel mathematical model derived from cohomology,
leveraging the KEEL-proven theorem that establishes cohomology as tautological,
generated by boundary classes of curves with fixed dual graphs. Simplicial
complexes are constructed using skew-commutative graded algebra, and the
structure theorem is applied to connect distinct homologies, enabling precise
interpretations of the resulting geometric forms. The proposed model is
utilized for protein structure analysis and prediction, with a specific
application to the Flagellar Motor structure. This approach offers new insights
into the geometric and algebraic foundations of biological macromolecular
modeling, highlighting its potential for advancement in structural biology.

</details>


### [378] [Deciphering the unique dynamic activation pathway in a G protein-coupled receptor enables unveiling biased signaling and identifying cryptic allosteric sites in conformational intermediates](https://arxiv.org/abs/2504.17624)
*Jigang Fan,Chunhao Zhu,Xiaobing Lan,Haiming Zhuang,Mingyu Li,Jian Zhang,Shaoyong Lu*

Main category: q-bio.BM

TL;DR: 该研究结合多种计算和实验方法，揭示了NTSR1激活的逐步动态机制及其偏向性信号传导，发现了一个隐秘的变构位点，为GPCR生物学和药物开发提供了新策略。


<details>
  <summary>Details</summary>
Motivation: 研究NTSR1的激活和偏向性信号传导机制，以探索治疗成瘾相关疾病的潜在途径。

Method: 结合计算（弹性带分子动力学模拟、马尔可夫状态模型）和实验方法（位点定向诱变、构象生物传感器），分析NTSR1的激活机制。

Result: 揭示了NTSR1激活的动态逐步机制、信号传导网络及一个隐秘的变构位点。

Conclusion: 研究深化了对NTSR1激活机制的理解，为开发GPCR靶向药物提供了新思路。

Abstract: Neurotensin receptor 1 (NTSR1), a member of the Class A G protein-coupled
receptor superfamily, plays an important role in modulating dopaminergic
neuronal activity and eliciting opioid-independent analgesia. Recent studies
suggest that promoting \{beta}-arrestin-biased signaling in NTSR1 may diminish
drugs of abuse, such as psychostimulants, thereby offering a potential avenue
for treating human addiction-related disorders. In this study, we utilized a
novel computational and experimental approach that combined nudged elastic
band-based molecular dynamics simulations, Markov state models, temporal
communication network analysis, site-directed mutagenesis, and conformational
biosensors, to explore the intricate mechanisms underlying NTSR1 activation and
biased signaling. Our study reveals a dynamic stepwise transition mechanism and
activated transmission network associated with NTSR1 activation. It also yields
valuable insights into the complex interplay between the unique polar network,
non-conserved ion locks, and aromatic clusters in NTSR1 signaling. Moreover, we
identified a cryptic allosteric site located in the intracellular region of the
receptor that exists in an intermediate state within the activation pathway.
Collectively, these findings contribute to a more profound understanding of
NTSR1 activation and biased signaling at the atomic level, thereby providing a
potential strategy for the development of NTSR1 allosteric modulators in the
realm of G protein-coupled receptor biology, biophysics, and medicine.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [379] [Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks](https://arxiv.org/abs/2504.17029)
*Jeffrey Smith,Taisei Fujii,Jesse Craney,Charles Gretton*

Main category: astro-ph.IM

TL;DR: 利用机器学习方法从单一波前传感器图像中估计Fried参数，实现毫米级精度，适用于实时AO系统控制。


<details>
  <summary>Details</summary>
Motivation: 大气湍流影响地基望远镜观测质量，而AO系统依赖于Fried参数优化性能。传统方法难以实时准确估计该参数。

Method: 采用计算机视觉的机器学习方法，从Shack-Hartmann或金字塔波前传感器图像中直接估计r0，并通过COMPASS仿真工具评估。

Result: 开发出单一网络模型，在开环和闭环AO配置下均能实现毫米级精度估计，推理时间仅0.83毫秒。

Conclusion: 所提方法经济高效，适用于实时仪器控制，显著提升AO系统性能。

Abstract: Atmospheric turbulence degrades the quality of astronomical observations in
ground-based telescopes, leading to distorted and blurry images. Adaptive
Optics (AO) systems are designed to counteract these effects, using atmospheric
measurements captured by a wavefront sensor to make real-time corrections to
the incoming wavefront. The Fried parameter, r0, characterises the strength of
atmospheric turbulence and is an essential control parameter for optimising the
performance of AO systems and more recently sky profiling for Free Space
Optical (FSO) communication channels. In this paper, we develop a novel
data-driven approach, adapting machine learning methods from computer vision
for Fried parameter estimation from a single Shack-Hartmann or pyramid
wavefront sensor image. Using these data-driven methods, we present a detailed
simulation-based evaluation of our approach using the open-source COMPASS AO
simulation tool to evaluate both the Shack-Hartmann and pyramid wavefront
sensors. Our evaluation is over a range of guide star magnitudes, and realistic
noise, atmospheric and instrument conditions. Remarkably, we are able to
develop a single network-based estimator that is accurate in both open and
closed-loop AO configurations. Our method accurately estimates the Fried
parameter from a single WFS image directly from AO telemetry to a few
millimetres. Our approach is suitable for real time control, exhibiting 0.83ms
r0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby
demonstrating a compelling economic solution for use in real-time instrument
control.

</details>


### [380] [Fried Parameter Estimation from Single Wavefront Sensor Image with Artificial Neural Networks](https://arxiv.org/abs/2504.17029)
*Jeffrey Smith,Taisei Fujii,Jesse Craney,Charles Gretton*

Main category: astro-ph.IM

TL;DR: 摘要太长不看版：本文提出了一种基于机器学习的数据驱动方法，通过单张Shack-Hartmann或金字塔波前传感器图像估计Fried参数（r0），实现了在开放和闭环AO配置下的高精度实时控制。


<details>
  <summary>Details</summary>
Motivation: 地面望远镜的天文观测受大气湍流影响，导致图像模糊。自适应光学（AO）系统通过实时校正波前来解决问题，而Fried参数（r0）是优化AO系统性能的关键。本文旨在开发一种高效的数据驱动方法，直接从AO遥测中估计r0。

Method: 采用计算机视觉中的机器学习方法，通过单张波前传感器图像训练网络模型。使用COMPASS AO开源工具模拟不同星等、噪声和大气条件下的数据，评估Shack-Hartmann和金字塔波前传感器的性能。

Result: 开发了一种网络模型，在开放和闭环AO配置下均能准确估计r0（误差为毫米级），并在NVIDIA RTX 3090 GPU上实现0.83毫秒的实时推理速度。

Conclusion: 该方法为实时仪器控制提供了一种经济高效的解决方案，可直接集成到AO系统中。

Abstract: Atmospheric turbulence degrades the quality of astronomical observations in
ground-based telescopes, leading to distorted and blurry images. Adaptive
Optics (AO) systems are designed to counteract these effects, using atmospheric
measurements captured by a wavefront sensor to make real-time corrections to
the incoming wavefront. The Fried parameter, r0, characterises the strength of
atmospheric turbulence and is an essential control parameter for optimising the
performance of AO systems and more recently sky profiling for Free Space
Optical (FSO) communication channels. In this paper, we develop a novel
data-driven approach, adapting machine learning methods from computer vision
for Fried parameter estimation from a single Shack-Hartmann or pyramid
wavefront sensor image. Using these data-driven methods, we present a detailed
simulation-based evaluation of our approach using the open-source COMPASS AO
simulation tool to evaluate both the Shack-Hartmann and pyramid wavefront
sensors. Our evaluation is over a range of guide star magnitudes, and realistic
noise, atmospheric and instrument conditions. Remarkably, we are able to
develop a single network-based estimator that is accurate in both open and
closed-loop AO configurations. Our method accurately estimates the Fried
parameter from a single WFS image directly from AO telemetry to a few
millimetres. Our approach is suitable for real time control, exhibiting 0.83ms
r0 inference times on retail NVIDIA RTX 3090 GPU hardware, and thereby
demonstrating a compelling economic solution for use in real-time instrument
control.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [381] [Can deep neural networks learn biological vision?](https://arxiv.org/abs/2504.16940)
*Drew Linsley,Pinyuan Feng,Thomas Serre*

Main category: q-bio.NC

TL;DR: 现代深度神经网络（DNNs）在达到或超越人类识别精度后，其与灵长类动物视觉系统的对齐性反而下降。作者认为未来的生物视觉计算模型应脱离传统AI范式，采用更贴近人类视觉的数据和训练方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究为什么DNNs在性能提升后与生物视觉系统的对齐性反而下降，并提出未来模型应更注重生物视觉的特性。

Method: 提出未来模型应使用更接近人类视觉的数据（如数据饮食、训练流程和目标），而非依赖互联网数据基准。

Result: 当前DNNs依赖的视觉特征与灵长类动物不同，导致对齐性下降。

Conclusion: 结论是未来生物视觉模型需要脱离传统AI框架，采用更贴近生物视觉的训练方法。

Abstract: Deep neural networks (DNNs) once showed increasing alignment with primate
neural responses as they improved on computer vision benchmarks. This trend
raised the exciting possibility that better models of biological vision would
come as a byproduct of the deep learning revolution in artificial intelligence.
However, the trend has reversed over recent years as DNNs have scaled to human
or superhuman recognition accuracy, a divergence that may stem from modern DNNs
learning to rely on different visual features than primates to solve tasks.
Where will better computational models of biological vision come from? We
propose that vision science must break from artificial intelligence to develop
algorithms that are designed with biological visual systems in mind instead of
internet data benchmarks. We predict that the next generation of deep learning
models of biological vision will be trained with data diets, training routines,
and objectives that are closer to those that shape human vision than those that
are in use today.

</details>


### [382] [Can deep neural networks learn biological vision?](https://arxiv.org/abs/2504.16940)
*Drew Linsley,Pinyuan Feng,Thomas Serre*

Main category: q-bio.NC

TL;DR: 现代深度神经网络在视觉任务上表现超越人类，但逐渐偏离灵长类的视觉机制，未来生物视觉模型需从AI独立，专注于生物视觉的学习方式。


<details>
  <summary>Details</summary>
Motivation: 探讨DNNs与灵长类视觉机制的分歧，提出需要设计更贴近生物视觉的算法，而非仅依赖AI的数据驱动方式。

Method: 提出未来模型应通过更接近人类视觉的数据、训练流程和目标来优化。

Result: 当前DNNs依赖与灵长类不同的视觉特征，导致模型与生物视觉的差异扩大。

Conclusion: 呼吁视觉科学研究转向以生物视觉为核心的设计，推动下一代深度学习模型的生物合理性。

Abstract: Deep neural networks (DNNs) once showed increasing alignment with primate
neural responses as they improved on computer vision benchmarks. This trend
raised the exciting possibility that better models of biological vision would
come as a byproduct of the deep learning revolution in artificial intelligence.
However, the trend has reversed over recent years as DNNs have scaled to human
or superhuman recognition accuracy, a divergence that may stem from modern DNNs
learning to rely on different visual features than primates to solve tasks.
Where will better computational models of biological vision come from? We
propose that vision science must break from artificial intelligence to develop
algorithms that are designed with biological visual systems in mind instead of
internet data benchmarks. We predict that the next generation of deep learning
models of biological vision will be trained with data diets, training routines,
and objectives that are closer to those that shape human vision than those that
are in use today.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [383] [SCALAR: A Part-of-speech Tagger for Identifiers](https://arxiv.org/abs/2504.17038)
*Christian D. Newman,Brandon Scholten,Sophia Testa,Joshua A. C. Behler,Syreen Banabilah,Michael L. Collard,Michael J. Decker,Mohamed Wiem Mkaouer,Marcos Zampieri,Eman Abdullah AlOmar,Reem Alsuhaibani,Anthony Peruma,Jonathan I. Maletic*

Main category: cs.SE

TL;DR: SCALAR是一个专门用于将源代码标识符名称映射到对应词性标签序列的工具，通过训练模型提升注释准确性。


<details>
  <summary>Details</summary>
Motivation: 开发者使用的自然语言结构独特，现有词性标注工具在注释标识符时效果不佳，需专门优化。

Method: 基于scikit-learn的GradientBoostingClassifier训练模型，结合人工标注数据集，优化标识符的词性标注。

Result: SCALAR在标识符注释上优于旧版标注工具和现成词性标注工具，效果显著提升。

Conclusion: SCALAR为源代码标识符提供了更准确的词性标注方法，代码已开源。

Abstract: The paper presents the Source Code Analysis and Lexical Annotation Runtime
(SCALAR), a tool specialized for mapping (annotating) source code identifier
names to their corresponding part-of-speech tag sequence (grammar pattern).
SCALAR's internal model is trained using scikit-learn's
GradientBoostingClassifier in conjunction with a manually-curated oracle of
identifier names and their grammar patterns. This specializes the tagger to
recognize the unique structure of the natural language used by developers to
create all types of identifiers (e.g., function names, variable names etc.).
SCALAR's output is compared with a previous version of the tagger, as well as a
modern off-the-shelf part-of-speech tagger to show how it improves upon other
taggers' output for annotating identifiers. The code is available on Github

</details>


### [384] [Automatically Generating Rules of Malicious Software Packages via Large Language Model](https://arxiv.org/abs/2504.17198)
*XiangRui Zhang,HaoYu Chen,Yongzhong He,Wenjia Niu,Qiang Li*

Main category: cs.SE

TL;DR: 论文提出了一种名为RuleLLM的工具，利用大型语言模型（LLMs）自动化生成开源软件（OSS）生态系统的规则，以应对软件供应链攻击。RuleLLM通过分析恶意软件元数据和代码片段生成可直接部署的YARA和Semgrep规则，实验结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的安全工具依赖专家预定义的规则，难以应对新兴的软件供应链攻击，因此需要自动化的规则生成方法提升安全防护能力。

Method: RuleLLM通过提取恶意软件的元数据和代码片段作为输入，生成YARA和Semgrep规则。过程包括规则生成、规则优化和规则对齐三个子任务。

Result: 实验中，RuleLLM在1,633个恶意软件包上生成了763条规则（452 YARA、311 Semgrep），精确率达85.2%，召回率达91.8%，优于现有方法。

Conclusion: RuleLLM证明了利用LLMs自动化生成安全规则的可行性，并提出了一套规则分类法（11类38子类），为未来研究提供了参考。

Abstract: Today's security tools predominantly rely on predefined rules crafted by
experts, making them poorly adapted to the emergence of software supply chain
attacks. To tackle this limitation, we propose a novel tool, RuleLLM, which
leverages large language models (LLMs) to automate rule generation for OSS
ecosystems. RuleLLM extracts metadata and code snippets from malware as its
input, producing YARA and Semgrep rules that can be directly deployed in
software development. Specifically, the rule generation task involves three
subtasks: crafting rules, refining rules, and aligning rules. To validate
RuleLLM's effectiveness, we implemented a prototype system and conducted
experiments on the dataset of 1,633 malicious packages. The results are
promising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a
precision of 85.2\% and a recall of 91.8\%, outperforming state-of-the-art
(SOTA) tools and scored-based approaches. We further analyzed generated rules
and proposed a rule taxonomy: 11 categories and 38 subcategories.

</details>


### [385] [Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code](https://arxiv.org/abs/2504.17426)
*Michele Carissimi,Martina Saletta,Claudio Ferretti*

Main category: cs.SE

TL;DR: 该论文提出了一种结合大语言模型（LLM）和主题建模的新方法，用于自动识别Python代码库中的主题，实验表明其生成的代码表示具有更好的语义丰富性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解方法在语义提取和可解释性上存在不足，而LLM和主题建模的结合有望提升代码理解的效率和效果，从而支持软件维护、重用等任务。

Method: 通过LLM生成代码摘要，并对其应用主题建模技术，再将结果与仅基于函数名或现有文档字符串提取的主题进行对比。

Result: 实验证明，基于LLM生成的摘要提取的主题具有更高的内部一致性和语义丰富性，适用于代码搜索、自动文档生成等任务。

Conclusion: 该方法为代码语义理解提供了有效工具，未来可扩展至更多软件工程场景，如知识发现和代码库重组。

Abstract: Understanding source code is a topic of great interest in the software
engineering community, since it can help programmers in various tasks such as
software maintenance and reuse. Recent advances in large language models (LLMs)
have demonstrated remarkable program comprehension capabilities, while
transformer-based topic modeling techniques offer effective ways to extract
semantic information from text. This paper proposes and explores a novel
approach that combines these strengths to automatically identify meaningful
topics in a corpus of Python programs. Our method consists in applying topic
modeling on the descriptions obtained by asking an LLM to summarize the code.
To assess the internal consistency of the extracted topics, we compare them
against topics inferred from function names alone, and those derived from
existing docstrings. Experimental results suggest that leveraging LLM-generated
summaries provides interpretable and semantically rich representation of code
structure. The promising results suggest that our approach can be fruitfully
applied in various software engineering tasks such as automatic documentation
and tagging, code search, software reorganization and knowledge discovery in
large repositories.

</details>


### [386] [Detection, Classification and Prevalence of Self-Admitted Aging Debt](https://arxiv.org/abs/2504.17428)
*Murali Sridharan,Mika Mäntylä,Leevi Rantala*

Main category: cs.SE

TL;DR: 该论文提出了“老化债务”（Aging Debt, AD）的概念，通过分析源代码注释中的“自承认老化债务”（Self-Admitted Aging Debt, SAAD）来量化软件维护成本，并开发了一个分类法来区分活跃和休眠老化类型。研究发现，超过21%的开源软件库存在SAAD，且休眠老化是主要类型。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注动态运行时指标（如内存和性能），而忽视了源代码注释等进化指标。为了填补这一空白，论文提出AD概念，旨在量化软件维护增加的额外努力和成本。

Method: 采用混合方法，结合定性和定量分析，从源代码注释中提取SAAD模式并构建分类法，随后量化开源库中的AD类型。

Result: 分类法将软件老化分为活跃和休眠两类。分析9000多个开源库显示，21%存在SAAD，且休眠老化占主导。

Conclusion: 随着软件规模增长，老化问题日益突出。分类法为研究和实践提供了工具，帮助制定更主动的维护策略。

Abstract: Context: Previous research on software aging is limited with focus on dynamic
runtime indicators like memory and performance, often neglecting evolutionary
indicators like source code comments and narrowly examining legacy issues
within the TD context. Objective: We introduce the concept of Aging Debt (AD),
representing the increased maintenance efforts and costs needed to keep
software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed
in source code comments left by software developers. Method: We employ a
mixed-methods approach, combining qualitative and quantitative analyses to
detect and measure AD in software. This includes framing SAAD patterns from the
source code comments after analysing the source code context, then utilizing
the SAAD patterns to detect SAAD comments. In the process, we develop a
taxonomy for SAAD that reflects the temporal aging of software and its
associated debt. Then we utilize the taxonomy to quantify the different types
of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes
temporal software aging into Active and Dormant types. Our extensive analysis
of over 9,000+ Open Source Software (OSS) repositories reveals that more than
21% repositories exhibit signs of SAAD as observed from our gold standard SAAD
dataset. Notably, Dormant AD emerges as the predominant category, highlighting
a critical but often overlooked aspect of software maintenance. Conclusion: As
software volume grows annually, so do evolutionary aging and maintenance
challenges; our proposed taxonomy can aid researchers in detailed software
aging studies and help practitioners develop improved and proactive maintenance
strategies.

</details>


### [387] [SCALAR: A Part-of-speech Tagger for Identifiers](https://arxiv.org/abs/2504.17038)
*Christian D. Newman,Brandon Scholten,Sophia Testa,Joshua A. C. Behler,Syreen Banabilah,Michael L. Collard,Michael J. Decker,Mohamed Wiem Mkaouer,Marcos Zampieri,Eman Abdullah AlOmar,Reem Alsuhaibani,Anthony Peruma,Jonathan I. Maletic*

Main category: cs.SE

TL;DR: SCALAR是一种专门用于将源代码标识符名称映射到相应词性标记序列的工具，通过GradientBoostingClassifier训练，优于现有标注器。


<details>
  <summary>Details</summary>
Motivation: 开发SCALAR的目的是为了更准确地标注源代码中开发者使用的自然语言结构，特别是各类标识符（如函数名、变量名等）的语法模式。

Method: 使用scikit-learn的GradientBoostingClassifier训练内部模型，结合手动整理的标识符名称和语法模式数据集。

Result: SCALAR的输出优于先前版本及现代通用词性标注器，尤其是在标识符标注任务中表现更优。

Conclusion: SCALAR为源代码标识符的词性标注提供了更精准的工具，其代码已在Github上开源。

Abstract: The paper presents the Source Code Analysis and Lexical Annotation Runtime
(SCALAR), a tool specialized for mapping (annotating) source code identifier
names to their corresponding part-of-speech tag sequence (grammar pattern).
SCALAR's internal model is trained using scikit-learn's
GradientBoostingClassifier in conjunction with a manually-curated oracle of
identifier names and their grammar patterns. This specializes the tagger to
recognize the unique structure of the natural language used by developers to
create all types of identifiers (e.g., function names, variable names etc.).
SCALAR's output is compared with a previous version of the tagger, as well as a
modern off-the-shelf part-of-speech tagger to show how it improves upon other
taggers' output for annotating identifiers. The code is available on Github

</details>


### [388] [Automatically Generating Rules of Malicious Software Packages via Large Language Model](https://arxiv.org/abs/2504.17198)
*XiangRui Zhang,HaoYu Chen,Yongzhong He,Wenjia Niu,Qiang Li*

Main category: cs.SE

TL;DR: RuleLLM利用大语言模型自动生成开源软件生态系统的安全规则，提升了对软件供应链攻击的适应性。


<details>
  <summary>Details</summary>
Motivation: 当前安全工具依赖专家预定义规则，无法有效应对软件供应链攻击。

Method: 通过提取恶意软件的元数据和代码片段，生成YARA和Semgrep规则，包括规则设计、精炼和校准三个子任务。

Result: 在1633个恶意包数据集上测试，生成763条规则（452 YARA，311 Semgrep），准确率85.2%，召回率91.8%，优于现有工具。

Conclusion: RuleLLM通过自动化规则生成，显著提高了安全工具的适应性和效率。

Abstract: Today's security tools predominantly rely on predefined rules crafted by
experts, making them poorly adapted to the emergence of software supply chain
attacks. To tackle this limitation, we propose a novel tool, RuleLLM, which
leverages large language models (LLMs) to automate rule generation for OSS
ecosystems. RuleLLM extracts metadata and code snippets from malware as its
input, producing YARA and Semgrep rules that can be directly deployed in
software development. Specifically, the rule generation task involves three
subtasks: crafting rules, refining rules, and aligning rules. To validate
RuleLLM's effectiveness, we implemented a prototype system and conducted
experiments on the dataset of 1,633 malicious packages. The results are
promising that RuleLLM generated 763 rules (452 YARA and 311 Semgrep) with a
precision of 85.2\% and a recall of 91.8\%, outperforming state-of-the-art
(SOTA) tools and scored-based approaches. We further analyzed generated rules
and proposed a rule taxonomy: 11 categories and 38 subcategories.

</details>


### [389] [Towards Leveraging Large Language Model Summaries for Topic Modeling in Source Code](https://arxiv.org/abs/2504.17426)
*Michele Carissimi,Martina Saletta,Claudio Ferretti*

Main category: cs.SE

TL;DR: 该论文提出结合大语言模型（LLM）和主题建模技术，通过LLM生成代码摘要并应用主题建模，自动识别Python程序中的有意义主题。实验表明该方法能提供语义丰富的代码结构表示。


<details>
  <summary>Details</summary>
Motivation: 研究如何更好地理解源代码，以支持软件维护和重用等任务。利用LLM的程序理解能力和主题建模技术提取语义信息。

Method: 通过LLM生成代码摘要，再应用主题建模技术从摘要中提取主题。对比从函数名和现有文档字符串推断的主题，评估内部一致性。

Result: 实验结果表明，基于LLM生成的摘要提取的主题具有可解释性和语义丰富性。

Conclusion: 该方法可应用于自动文档生成、代码搜索、软件重构等任务，具有广阔的应用前景。

Abstract: Understanding source code is a topic of great interest in the software
engineering community, since it can help programmers in various tasks such as
software maintenance and reuse. Recent advances in large language models (LLMs)
have demonstrated remarkable program comprehension capabilities, while
transformer-based topic modeling techniques offer effective ways to extract
semantic information from text. This paper proposes and explores a novel
approach that combines these strengths to automatically identify meaningful
topics in a corpus of Python programs. Our method consists in applying topic
modeling on the descriptions obtained by asking an LLM to summarize the code.
To assess the internal consistency of the extracted topics, we compare them
against topics inferred from function names alone, and those derived from
existing docstrings. Experimental results suggest that leveraging LLM-generated
summaries provides interpretable and semantically rich representation of code
structure. The promising results suggest that our approach can be fruitfully
applied in various software engineering tasks such as automatic documentation
and tagging, code search, software reorganization and knowledge discovery in
large repositories.

</details>


### [390] [Detection, Classification and Prevalence of Self-Admitted Aging Debt](https://arxiv.org/abs/2504.17428)
*Murali Sridharan,Mika Mäntylä,Leevi Rantala*

Main category: cs.SE

TL;DR: 论文提出了‘衰老债务’(AD)概念，通过分析源代码注释中的‘自认衰老债务’(SAAD)来量化软件维护成本，并提出分类法区分活跃和休眠AD。分析显示21%的开源项目存在SAAD，休眠AD占主导。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注运行时指标（如内存、性能），而忽略了进化指标（如源代码注释）。通过引入AD和SAAD，论文试图填补这一空白。

Method: 采用混合方法（定性+定量分析），从源代码注释中提取SAAD模式并构建分类法，随后量化开源项目中的AD类型。

Result: 分类法将软件衰老分为活跃和休眠两类。分析9000+开源项目发现21%存在SAAD，休眠AD为主要类型。

Conclusion: 随着软件规模增长，衰老问题加剧；提出的分类法可帮助研究和实践者制定更主动的维护策略。

Abstract: Context: Previous research on software aging is limited with focus on dynamic
runtime indicators like memory and performance, often neglecting evolutionary
indicators like source code comments and narrowly examining legacy issues
within the TD context. Objective: We introduce the concept of Aging Debt (AD),
representing the increased maintenance efforts and costs needed to keep
software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed
in source code comments left by software developers. Method: We employ a
mixed-methods approach, combining qualitative and quantitative analyses to
detect and measure AD in software. This includes framing SAAD patterns from the
source code comments after analysing the source code context, then utilizing
the SAAD patterns to detect SAAD comments. In the process, we develop a
taxonomy for SAAD that reflects the temporal aging of software and its
associated debt. Then we utilize the taxonomy to quantify the different types
of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes
temporal software aging into Active and Dormant types. Our extensive analysis
of over 9,000+ Open Source Software (OSS) repositories reveals that more than
21% repositories exhibit signs of SAAD as observed from our gold standard SAAD
dataset. Notably, Dormant AD emerges as the predominant category, highlighting
a critical but often overlooked aspect of software maintenance. Conclusion: As
software volume grows annually, so do evolutionary aging and maintenance
challenges; our proposed taxonomy can aid researchers in detailed software
aging studies and help practitioners develop improved and proactive maintenance
strategies.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [391] [Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance](https://arxiv.org/abs/2504.17675)
*Caroline Panggabean,Devaraj Verma C,Bhagyashree Gogoi,Ranju Limbu,Rhythm Sarker*

Main category: cs.DC

TL;DR: 该论文提出了一种基于遗传算法（GA）的动态虚拟机（VM）放置与整合方法，旨在降低能耗并满足服务质量（QoS）约束，实验表明其在能效、迁移次数、SLA违规率和执行时间上优于传统启发式算法。


<details>
  <summary>Details</summary>
Motivation: 云计算环境需要动态高效的资源管理，以优化性能、降低能耗并满足SLA要求。传统启发式方法在动态负载下表现不佳，因此需要更智能的解决方案。

Method: 采用遗传算法（GA）动态调整虚拟机分配，结合实时负载变化进行优化。

Result: 实验显示，该方法显著降低了能耗、虚拟机迁移次数、SLA违规率和执行时间，并通过相关热图验证了其有效性。

Conclusion: 提出的GA方法在云资源优化中表现卓越，为动态负载下的资源管理提供了高效解决方案。

Abstract: Cloud computing environments demand dynamic and efficient resource management
to ensure optimal performance, reduced energy consumption, and adherence to
Service Level Agreements (SLAs). This paper presents a Genetic Algorithm
(GA)-based approach for Virtual Machine (VM) placement and consolidation,
aiming to minimize power usage while maintaining QoS constraints. The proposed
method dynamically adjusts VM allocation based on real-time workload
variations, outperforming traditional heuristics such as First Fit Decreasing
(FFD) and Best Fit Decreasing (BFD). Experimental results show notable
reductions in energy consumption, VM migrations, SLA violation rates, and
execution time. A correlation heatmap further illustrates strong relationships
among these key performance indicators, confirming the effectiveness of our
approach in optimizing cloud resource utilization.

</details>


### [392] [Optimized Cloud Resource Allocation Using Genetic Algorithms for Energy Efficiency and QoS Assurance](https://arxiv.org/abs/2504.17675)
*Caroline Panggabean,Devaraj Verma C,Bhagyashree Gogoi,Ranju Limbu,Rhythm Sarker*

Main category: cs.DC

TL;DR: 该论文提出了一种基于遗传算法的虚拟机放置和整合方法，旨在最小化能耗同时满足QoS约束，实验结果显示其优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 云计算环境需要动态高效的资源管理以优化性能、降低能耗并遵守SLA。

Method: 使用遗传算法动态调整虚拟机分配，适应实时工作负载变化。

Result: 实验显示显著降低了能耗、虚拟机迁移次数、SLA违规率和执行时间。

Conclusion: 该方法在优化云资源利用率方面表现出色，并通过相关性热图验证了其有效性。

Abstract: Cloud computing environments demand dynamic and efficient resource management
to ensure optimal performance, reduced energy consumption, and adherence to
Service Level Agreements (SLAs). This paper presents a Genetic Algorithm
(GA)-based approach for Virtual Machine (VM) placement and consolidation,
aiming to minimize power usage while maintaining QoS constraints. The proposed
method dynamically adjusts VM allocation based on real-time workload
variations, outperforming traditional heuristics such as First Fit Decreasing
(FFD) and Best Fit Decreasing (BFD). Experimental results show notable
reductions in energy consumption, VM migrations, SLA violation rates, and
execution time. A correlation heatmap further illustrates strong relationships
among these key performance indicators, confirming the effectiveness of our
approach in optimizing cloud resource utilization.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [393] [An introduction to R package `mvs`](https://arxiv.org/abs/2504.17546)
*Wouter van Loon*

Main category: stat.CO

TL;DR: 论文介绍了一个R包`mvs`，基于多视角堆叠（MVS）框架，专门用于处理多视角生物医学数据，通过分而治之的并行学习方法提升模型可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法忽视多视角数据的结构，限制了模型的解释性和性能，MVS框架旨在解决这一问题。

Method: MVS通过分别训练每个视角的学习算法，交叉验证评估预测能力，再用另一算法分配权重，实现分而治之的并行学习。

Result: MVS在高维数据中表现优异，可并行计算并自动选择重要视角，R包`mvs`实现了灵活的模型拟合。

Conclusion: `mvs`是处理多视角生物医学数据的有效工具，尤其适用于高维场景，且具备可扩展性和灵活性。

Abstract: In biomedical science, a set of objects or persons can often be described by
multiple distinct sets of features obtained from different data sources or
modalities (called "multi-view data"). Classical machine learning methods
ignore the multi-view structure of such data, limiting model interpretability
and performance. The R package `mvs` provides methods that were designed
specifically for dealing with multi-view data, based on the multi-view stacking
(MVS) framework. MVS is a form of supervised (machine) learning used to train
multi-view classification or prediction models. MVS works by training a
learning algorithm on each view separately, estimating the predictive power of
each view-specific model through cross-validation, and then using another
learning algorithm to assign weights to the view-specific models based on their
estimated predictions. MVS is a form of ensemble learning, dividing the large
multi-view learning problem into smaller sub-problems. Most of these
sub-problems can be solved in parallel, making it computationally attractive.
Additionally, the number of features of the sub-problems is greatly reduced
compared with the full multi-view learning problem. This makes MVS especially
useful when the total number of features is larger than the number of
observations (i.e., high-dimensional data). MVS can still be applied even if
the sub-problems are themselves high-dimensional by adding suitable penalty
terms to the learning algorithms. Furthermore, MVS can be used to automatically
select the views which are most important for prediction. The R package `mvs`
makes fitting MVS models, including such penalty terms, easily and openly
accessible. `mvs` allows for the fitting of stacked models with any number of
levels, with different penalty terms, different outcome distributions, and
provides several options for missing data handling.

</details>


### [394] [An introduction to R package `mvs`](https://arxiv.org/abs/2504.17546)
*Wouter van Loon*

Main category: stat.CO

TL;DR: 本文介绍了一种基于多视图堆叠（MVS）框架的R包`mvs`，用于处理多视图数据，通过分而治之的方法提升模型性能和可解释性，特别适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 生物医学科学中，多视图数据普遍存在，但传统机器学习方法忽略其多视图结构，限制了模型性能和可解释性。MVS框架旨在解决这一问题。

Method: MVS通过分别训练每个视图的学习算法，利用交叉验证评估各视图模型的预测能力，并加权整合这些模型。支持并行计算和惩罚项处理高维子问题。

Result: MVS框架在减少特征维度的同时提升了计算效率，且能自动选择重要视图，R包`mvs`提供了灵活的实现和缺失数据处理选项。

Conclusion: MVS是一种高效且灵活的多视图学习方法，特别适用于高维数据，R包`mvs`为其提供了易用的工具支持。

Abstract: In biomedical science, a set of objects or persons can often be described by
multiple distinct sets of features obtained from different data sources or
modalities (called "multi-view data"). Classical machine learning methods
ignore the multi-view structure of such data, limiting model interpretability
and performance. The R package `mvs` provides methods that were designed
specifically for dealing with multi-view data, based on the multi-view stacking
(MVS) framework. MVS is a form of supervised (machine) learning used to train
multi-view classification or prediction models. MVS works by training a
learning algorithm on each view separately, estimating the predictive power of
each view-specific model through cross-validation, and then using another
learning algorithm to assign weights to the view-specific models based on their
estimated predictions. MVS is a form of ensemble learning, dividing the large
multi-view learning problem into smaller sub-problems. Most of these
sub-problems can be solved in parallel, making it computationally attractive.
Additionally, the number of features of the sub-problems is greatly reduced
compared with the full multi-view learning problem. This makes MVS especially
useful when the total number of features is larger than the number of
observations (i.e., high-dimensional data). MVS can still be applied even if
the sub-problems are themselves high-dimensional by adding suitable penalty
terms to the learning algorithms. Furthermore, MVS can be used to automatically
select the views which are most important for prediction. The R package `mvs`
makes fitting MVS models, including such penalty terms, easily and openly
accessible. `mvs` allows for the fitting of stacked models with any number of
levels, with different penalty terms, different outcome distributions, and
provides several options for missing data handling.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [395] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)
*Felix Kares,Timo Speith,Hanwei Zhang,Markus Langer*

Main category: cs.HC

TL;DR: 研究比较了三种显著性地图（LIME、Grad-CAM 和 Guided Backpropagation）在主观、客观和数学评估方法中的表现，发现它们在评估结果上不一致，并探讨了数学指标与用户理解之间的关系。


<details>
  <summary>Details</summary>
Motivation: 显著性地图是解释神经网络分类的常用方法，但如何评估这些方法仍存在争议。研究旨在比较不同评估方法下显著性地图的表现，并探讨数学指标与用户理解的联系。

Method: 采用主体间研究设计（N=166），对三种显著性地图方法（LIME、Grad-CAM 和 Guided Backpropagation）进行主观信任与满意度测试、客观用户能力提升测试以及数学指标评分。

Result: 不同评估方法的结果不一致：主观信任无差异，Grad-CAM 最佳提升用户能力，Guided Backpropagation 数学指标最优；部分数学指标与用户理解相关，但关系反直觉。

Conclusion: 研究强调了在可解释 AI（XAI）评估中综合使用用户研究和数学指标的必要性，并揭示了当前评估方法的局限性。

Abstract: Saliency maps are a popular approach for explaining classifications of
(convolutional) neural networks. However, it remains an open question as to how
best to evaluate salience maps, with three families of evaluation methods
commonly being used: subjective user measures, objective user measures, and
mathematical metrics. We examine three of the most popular saliency map
approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between
subject study (N=166) across these families of evaluation methods. We test 1)
for subjective measures, if the maps differ with respect to user trust and
satisfaction; 2) for objective measures, if the maps increase users' abilities
and thus understanding of a model; 3) for mathematical metrics, which map
achieves the best ratings across metrics; and 4) whether the mathematical
metrics can be associated with objective user measures. To our knowledge, our
study is the first to compare several salience maps across all these evaluation
methods$-$with the finding that they do not agree in their assessment (i.e.,
there was no difference concerning trust and satisfaction, Grad-CAM improved
users' abilities best, and Guided Backpropagation had the most favorable
mathematical metrics). Additionally, we show that some mathematical metrics
were associated with user understanding, although this relationship was often
counterintuitive. We discuss these findings in light of general debates
concerning the complementary use of user studies and mathematical metrics in
the evaluation of explainable AI (XAI) approaches.

</details>


### [396] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)
*Ayushi Agrawal,Aditya Kondai,Kavita Vemuri*

Main category: cs.HC

TL;DR: AI面部评估工具对自我物化、自尊和情感反应有负面影响，尤其是女性更倾向于数字美化。研究发现这些工具可能无意中强化了社会偏见。


<details>
  <summary>Details</summary>
Motivation: 研究AI面部评估工具如何影响个体的自我物化、自尊和情感反应，特别关注性别差异。

Method: 两个样本使用不同版本的面部分析工具（批判性和中性版本），参与者完成相关量表和行为测量。

Result: 高自我物化和低自尊与更多外观美化行为相关。工具即使较为中性，仍引发负面情绪，且女性更倾向于数字美化。

Conclusion: AI工具可能无意中强化社会偏见，呼吁负责任的AI设计。未来研究将探讨训练数据中的意识形态如何影响用户态度和决策。

Abstract: AI-powered facial assessment tools are reshaping how individuals evaluate
appearance and internalize social judgments. This study examines the
psychological impact of such tools on self-objectification, self-esteem, and
emotional responses, with attention to gender differences. Two samples used
distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9
years), and another more neutral (N=51; M=19.9 years). Participants completed
validated self-objectification and self-esteem scales and custom items
measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and
perceived social emotion (PSE). Results revealed consistent links between high
self-objectification, low self-esteem, and increased appearance enhancement
behaviors across both versions. Despite softer framing, the newer tool still
evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit
feedback may reinforce appearance-related insecurities. Gender differences
emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital
enhancement and less likely to perceive emotional impact in others. These
findings reveal how AI tools may unintentionally reinforce and amplify existing
social biases and underscore the critical need for responsible AI design and
development. Future research will investigate how human ideologies embedded in
the training data of such tools shape their evaluative outputs, and how these,
in turn, influence user attitudes and decisions.

</details>


### [397] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)
*Robert Kaufman*

Main category: cs.HC

TL;DR: 该论文探讨了自动驾驶车辆（AVs）如何通过个性化、适应性强的通信策略满足不同骑手的需求，以提升信任和驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于当前AV设计对所有人及情境的单一处理方式，无法满足多样化需求。了解人类-AV系统中各因素的关系，有助于设计更有效的通信策略。

Method: 通过三项实证研究：1) 极端驾驶环境中优化通信策略；2) 分析错误通信系统的后果；3) 使用机器学习预测个体对AV的信任因素。

Result: 研究发现需任务敏感、情境适应且个性化的通信设计，强调透明度和适应性对提升骑手信任及性能的关键作用。

Conclusion: 结论指出，未来AV系统需针对个体需求和情境动态调整，为设计师和政策制定者提供实践指导，同时推动人机协作理论研究。

Abstract: Unresolved questions about how autonomous vehicles (AVs) should meet the
informational needs of riders hinder real-world adoption. Complicating our
ability to satisfy rider needs is that different people, goals, and driving
contexts have different criteria for what constitutes interaction success.
Unfortunately, most human-AV research and design today treats all people and
situations uniformly. It is crucial to understand how an AV should communicate
to meet rider needs, and how communications should change when the human-AV
complex system changes. I argue that understanding the relationships between
different aspects of the human-AV system can help us build improved and
adaptable AV communications. I support this argument using three empirical
studies. First, I identify optimal communication strategies that enhance
driving performance, confidence, and trust for learning in extreme driving
environments. Findings highlight the need for task-sensitive,
modality-appropriate communications tuned to learner cognitive limits and
goals. Next, I highlight the consequences of deploying faulty communication
systems and demonstrate the need for context-sensitive communications. Third, I
use machine learning (ML) to illuminate personal factors predicting trust in
AVs, emphasizing the importance of tailoring designs to individual traits and
concerns. Together, this dissertation supports the necessity of transparent,
adaptable, and personalized AV systems that cater to individual needs, goals,
and contextual demands. By considering the complex system within which human-AV
interactions occur, we can deliver valuable insights for designers,
researchers, and policymakers. This dissertation also provides a concrete
domain to study theories of human-machine joint action and situational
awareness, and can be used to guide future human-AI interaction research.
[shortened for arxiv]

</details>


### [398] [Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality](https://arxiv.org/abs/2504.17331)
*Süleyman Özdel,Kadir Burak Buldu,Enkelejda Kasneci,Efe Bozkir*

Main category: cs.HC

TL;DR: 该研究利用大型语言模型（LLM）提出了一种新型的虚拟现实（VR）导航技术，支持用户通过自然语言实现免手持的移动。实验对比了传统控制器瞬移、语音转向和LLM驱动方法，结果表明LLM方法在用户体验、沉浸感和防晕动症方面与传统方法相当，但能更好地提升用户注意力和参与度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决传统语音导航技术因依赖固定指令集而缺乏自然性和灵活性的问题，通过引入LLM实现更自然、免手持的VR移动方式，以提升可访问性和用户体验。

Method: 研究提出了基于LLM的自然语言导航技术，与控制器瞬移和语音转向进行对比。通过眼动追踪数据（如SHAP分析）和标准化问卷（如可用性、沉浸感、晕动症及认知负荷）评估用户注意力和参与度。

Result: LLM驱动的导航在可用性、沉浸感和晕动症评分上与传统方法相当，但能显著提升用户注意力和参与度。眼动数据分析显示不同技术下视觉注意和认知处理模式存在差异。

Conclusion: LLM驱动的免手持导航技术为VR空间提供了自然、舒适的可访问性解决方案，尤其在提升用户参与度方面具有潜力。

Abstract: Locomotion plays a crucial role in shaping the user experience within virtual
reality environments. In particular, hands-free locomotion offers a valuable
alternative by supporting accessibility and freeing users from reliance on
handheld controllers. To this end, traditional speech-based methods often
depend on rigid command sets, limiting the naturalness and flexibility of
interaction. In this study, we propose a novel locomotion technique powered by
large language models (LLMs), which allows users to navigate virtual
environments using natural language with contextual awareness. We evaluate
three locomotion methods: controller-based teleportation, voice-based steering,
and our language model-driven approach. Our evaluation measures include
eye-tracking data analysis, including explainable machine learning through SHAP
analysis as well as standardized questionnaires for usability, presence,
cybersickness, and cognitive load to examine user attention and engagement. Our
findings indicate that the LLM-driven locomotion possesses comparable
usability, presence, and cybersickness scores to established methods like
teleportation, demonstrating its novel potential as a comfortable, natural
language-based, hands-free alternative. In addition, it enhances user attention
within the virtual environment, suggesting greater engagement. Complementary to
these findings, SHAP analysis revealed that fixation, saccade, and
pupil-related features vary across techniques, indicating distinct patterns of
visual attention and cognitive processing. Overall, we state that our method
can facilitate hands-free locomotion in virtual spaces, especially in
supporting accessibility.

</details>


### [399] [Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform](https://arxiv.org/abs/2504.17173)
*Tianyu Zhang,Dongheng Zhang,Ruixu Geng,Xuecheng Xie,Shuai Yang,Yan Chen*

Main category: cs.HC

TL;DR: 提出了一个基于CSI的大规模WiFi定位学习框架，通过图结构建模异构CSI数据，结合预训练和置信度微调，在真实建筑中实现了2.17米的中位定位误差


<details>
  <summary>Details</summary>
Motivation: 解决现有CSI定位系统在可控小规模环境中的局限性，提升其在大规模实际部署中的泛化能力

Method: 使用图结构建模异构CSI数据，设计预训练任务利用未标记数据，并采用置信度感知微调策略

Result: 在五层楼25,600平方米的实验中，中位定位误差2.17米，楼层准确率99.49%，MAE降低18.7%

Conclusion: 该框架显著提升了大规模ISAC部署中CSI定位的性能和鲁棒性

Abstract: In recent years, Channel State Information (CSI), recognized for its
fine-grained spatial characteristics, has attracted increasing attention in
WiFi-based indoor localization. However, despite its potential, CSI-based
approaches have yet to achieve the same level of deployment scale and
commercialization as those based on Received Signal Strength Indicator (RSSI).
A key limitation lies in the fact that most existing CSI-based systems are
developed and evaluated in controlled, small-scale environments, limiting their
generalizability. To bridge this gap, we explore the deployment of a
large-scale CSI-based localization system involving over 400 Access Points
(APs) in a real-world building under the Integrated Sensing and Communication
(ISAC) paradigm. We highlight two critical yet often overlooked factors: the
underutilization of unlabeled data and the inherent heterogeneity of CSI
measurements. To address these challenges, we propose a novel CSI-based
learning framework for WiFi localization, tailored for large-scale ISAC
deployments on the server side. Specifically, we employ a novel graph-based
structure to model heterogeneous CSI data and reduce redundancy. We further
design a pretext pretraining task that incorporates spatial and temporal priors
to effectively leverage large-scale unlabeled CSI data. Complementarily, we
introduce a confidence-aware fine-tuning strategy to enhance the robustness of
localization results. In a leave-one-smartphone-out experiment spanning five
floors and 25, 600 m2, we achieve a median localization error of 2.17 meters
and a floor accuracy of 99.49%. This performance corresponds to an 18.7%
reduction in mean absolute error (MAE) compared to the best-performing
baseline.

</details>


### [400] [The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults](https://arxiv.org/abs/2504.17663)
*Michelle L. Ding,Harini Suresh*

Main category: cs.HC

TL;DR: 本文探讨了社会技术AI治理在预防成人AI生成非自愿亲密图像（AIG-NCII，俗称“深度伪造色情”）中的作用，并指出开源换脸模型和约200款“脱衣”软件构成的恶意技术生态系统（MTE）如何让普通用户在几分钟内创建AIG-NCII。通过分析NIST的AI 100-4报告，作者揭示了当前合成内容治理方法未能有效监管MTE及其缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示当前AI治理在防止AIG-NCII方面的不足，尤其是针对普通用户利用MTE工具快速生成有害内容的漏洞。

Method: 采用幸存者中心视角，结合技术生态系统分析（如开源换脸模型和脱衣软件）与政策分析（以NIST AI 100-4报告为例）。

Result: 当前治理方法无法有效监管MTE工具，且存在对漏洞成因的假设缺陷。

Conclusion: 需改进社会技术AI治理框架，填补对成人AIG-NCII的监管空白，并修正错误假设以应对恶意技术生态。

Abstract: In this paper, we adopt a survivor-centered approach to locate and dissect
the role of sociotechnical AI governance in preventing AI-Generated
Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as
"deep fake pornography." We identify a "malicious technical ecosystem" or
"MTE," comprising of open-source face-swapping models and nearly 200
"nudifying" software programs that allow non-technical users to create AIG-NCII
within minutes. Then, using the National Institute of Standards and Technology
(NIST) AI 100-4 report as a reflection of current synthetic content governance
methods, we show how the current landscape of practices fails to effectively
regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining
these gaps.

</details>


### [401] [INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models](https://arxiv.org/abs/2504.17677)
*Jarne Thys,Sebe Vanbrabant,Davy Vanacken,Gustavo Rovelo Ruiz*

Main category: cs.HC

TL;DR: 这篇论文介绍了 INSIGHT，一个结合多种 AI 工具辅助教学的概念验证，旨在通过动态构建 FAQ 和提供个性化支持改善高等教育中的教学过程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的兴起为教育领域带来了机遇与挑战，INSIGHT 的目标是解决如何有效整合 AI 技术以优化教学任务（如个性化教学），并应对师生互动弱化和隐私问题。

Method: INSIGHT 采用模块化设计，通过分析学生对 LLM 的提问，提取关键词动态构建 FAQ，同时为教师提供数据支持以实现个性化面授辅导。

Result: 该工具展示了如何利用 AI 工具支持教学流程，未来可通过数据驱动方式实现自适应学习，调整内容以满足学生进度和学习风格。

Conclusion: INSIGHT 为 AI 在教育中的应用提供了可行框架，未来可扩展为更具互动性和包容性的学习体验。

Abstract: The rise of AI, especially Large Language Models, presents challenges and
opportunities to integrate such technology into the classroom. AI has the
potential to revolutionize education by helping teaching staff with various
tasks, such as personalizing their teaching methods, but it also raises
concerns, for example, about the degradation of student-teacher interactions
and user privacy. This paper introduces INSIGHT, a proof of concept to combine
various AI tools to assist teaching staff and students in the process of
solving exercises. INSIGHT has a modular design that allows it to be integrated
into various higher education courses. We analyze students' questions to an LLM
by extracting keywords, which we use to dynamically build an FAQ from students'
questions and provide new insights for the teaching staff to use for more
personalized face-to-face support. Future work could build upon INSIGHT by
using the collected data to provide adaptive learning and adjust content based
on student progress and learning styles to offer a more interactive and
inclusive learning experience.

</details>


### [402] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)
*Felix Kares,Timo Speith,Hanwei Zhang,Markus Langer*

Main category: cs.HC

TL;DR: 该研究比较了三种流行的显著性图方法（LIME、Grad-CAM和Guided Backpropagation）在不同评估方法下的表现，发现评估方法间的结论不一致，且数学指标与用户理解的关系有时反直觉。


<details>
  <summary>Details</summary>
Motivation: 显著性图是解释神经网络分类的常用方法，但如何评估其效果尚无共识。研究旨在比较不同评估方法（主观用户测量、客观用户测量和数学指标）下的显著性图表现。

Method: 研究采用主体间设计（N=166），测试三种显著性图方法在信任与满意度（主观）、用户理解能力（客观）及数学指标上的表现，并分析数学指标与用户理解的关联。

Result: 结果表明，评估方法间结论不一致：Grad-CAM最能提升用户理解，Guided Backpropagation数学指标最佳，但主观信任无差异。数学指标与用户理解的关系部分反直觉。

Conclusion: 研究强调了用户研究与数学指标在可解释AI（XAI）评估中的互补性，并呼吁进一步探讨评估方法的一致性。

Abstract: Saliency maps are a popular approach for explaining classifications of
(convolutional) neural networks. However, it remains an open question as to how
best to evaluate salience maps, with three families of evaluation methods
commonly being used: subjective user measures, objective user measures, and
mathematical metrics. We examine three of the most popular saliency map
approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between
subject study (N=166) across these families of evaluation methods. We test 1)
for subjective measures, if the maps differ with respect to user trust and
satisfaction; 2) for objective measures, if the maps increase users' abilities
and thus understanding of a model; 3) for mathematical metrics, which map
achieves the best ratings across metrics; and 4) whether the mathematical
metrics can be associated with objective user measures. To our knowledge, our
study is the first to compare several salience maps across all these evaluation
methods$-$with the finding that they do not agree in their assessment (i.e.,
there was no difference concerning trust and satisfaction, Grad-CAM improved
users' abilities best, and Guided Backpropagation had the most favorable
mathematical metrics). Additionally, we show that some mathematical metrics
were associated with user understanding, although this relationship was often
counterintuitive. We discuss these findings in light of general debates
concerning the complementary use of user studies and mathematical metrics in
the evaluation of explainable AI (XAI) approaches.

</details>


### [403] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)
*Ayushi Agrawal,Aditya Kondai,Kavita Vemuri*

Main category: cs.HC

TL;DR: AI面部评估工具对自我物化、自尊和情绪有显著影响，尤其在性别差异上表现明显。两种工具版本均显示高自我物化与低自尊、更多外貌增强行为相关，负面情绪反应依旧存在。


<details>
  <summary>Details</summary>
Motivation: 研究AI面部评估工具对用户心理的影响，特别是自我物化和自尊，以及性别差异，以揭示潜在的社会偏见。

Method: 使用两种版本的面部分析工具（批评性和中性），通过问卷测量自我物化、自尊、情绪、外貌增强行为和社会情绪感知。

Result: 高自我物化与低自尊、更多外貌增强行为相关；中性工具仍引发负面情绪，女性更倾向数字增强且较少感知他人情绪。

Conclusion: AI工具可能无意中强化社会偏见，需负责任的AI设计。未来研究将探讨训练数据中的意识形态如何影响工具输出和用户态度。

Abstract: AI-powered facial assessment tools are reshaping how individuals evaluate
appearance and internalize social judgments. This study examines the
psychological impact of such tools on self-objectification, self-esteem, and
emotional responses, with attention to gender differences. Two samples used
distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9
years), and another more neutral (N=51; M=19.9 years). Participants completed
validated self-objectification and self-esteem scales and custom items
measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and
perceived social emotion (PSE). Results revealed consistent links between high
self-objectification, low self-esteem, and increased appearance enhancement
behaviors across both versions. Despite softer framing, the newer tool still
evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit
feedback may reinforce appearance-related insecurities. Gender differences
emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital
enhancement and less likely to perceive emotional impact in others. These
findings reveal how AI tools may unintentionally reinforce and amplify existing
social biases and underscore the critical need for responsible AI design and
development. Future research will investigate how human ideologies embedded in
the training data of such tools shape their evaluative outputs, and how these,
in turn, influence user attitudes and decisions.

</details>


### [404] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)
*Robert Kaufman*

Main category: cs.HC

TL;DR: 论文探讨了如何通过理解人车交互系统的复杂性来设计更透明、适应性强且个性化的自动驾驶车辆（AV）通信系统，以满足不同用户、目标和场景的需求。


<details>
  <summary>Details</summary>
Motivation: 当前人车交互研究大多将所有人及情境视为相同，但实际需求因个体和场景差异而不同，导致AV实际应用受阻。

Method: 通过三项实证研究：1）极端驾驶环境下优化通信策略；2）分析错误通信系统的后果；3）利用机器学习预测个人对AV的信任因素。

Result: 发现需任务敏感、情境适应且个性化的通信设计，强调透明度和适应性对提升用户信心和信任至关重要。

Conclusion: 需基于人车系统复杂性设计AV通信，为设计师和政策制定者提供见解，同时推动人机协同理论与未来研究。

Abstract: Unresolved questions about how autonomous vehicles (AVs) should meet the
informational needs of riders hinder real-world adoption. Complicating our
ability to satisfy rider needs is that different people, goals, and driving
contexts have different criteria for what constitutes interaction success.
Unfortunately, most human-AV research and design today treats all people and
situations uniformly. It is crucial to understand how an AV should communicate
to meet rider needs, and how communications should change when the human-AV
complex system changes. I argue that understanding the relationships between
different aspects of the human-AV system can help us build improved and
adaptable AV communications. I support this argument using three empirical
studies. First, I identify optimal communication strategies that enhance
driving performance, confidence, and trust for learning in extreme driving
environments. Findings highlight the need for task-sensitive,
modality-appropriate communications tuned to learner cognitive limits and
goals. Next, I highlight the consequences of deploying faulty communication
systems and demonstrate the need for context-sensitive communications. Third, I
use machine learning (ML) to illuminate personal factors predicting trust in
AVs, emphasizing the importance of tailoring designs to individual traits and
concerns. Together, this dissertation supports the necessity of transparent,
adaptable, and personalized AV systems that cater to individual needs, goals,
and contextual demands. By considering the complex system within which human-AV
interactions occur, we can deliver valuable insights for designers,
researchers, and policymakers. This dissertation also provides a concrete
domain to study theories of human-machine joint action and situational
awareness, and can be used to guide future human-AI interaction research.
[shortened for arxiv]

</details>


### [405] [Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality](https://arxiv.org/abs/2504.17331)
*Süleyman Özdel,Kadir Burak Buldu,Enkelejda Kasneci,Efe Bozkir*

Main category: cs.HC

TL;DR: 本文提出了一种基于大型语言模型（LLM）的新型虚拟现实导航方法，通过自然语言实现情境感知的双手自由移动。研究发现，该方法在可用性、存在感和晕动症方面与传统方法相当，同时提高了用户的注意力，表明其作为一种舒适、自然的语言驱动替代方案的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统语音导航方法依赖固定指令集，限制了交互的自然性和灵活性。因此，本文旨在探索一种更自然的语言驱动导航方法，以提升虚拟现实环境中的用户体验。

Method: 本文提出了一种基于大型语言模型的导航技术，并通过实验评估了三种方法：控制器传送、语音转向和LLM驱动导航。评估指标包括眼动数据分析（如SHAP分析）以及标准化问卷（如可用性、存在感、晕动症和认知负荷）。

Result: LLM驱动的方法在可用性、存在感和晕动症方面与传统方法表现相当，同时显著提升了用户的注意力和参与感。SHAP分析还揭示了不同技术下视觉注意力和认知处理的差异。

Conclusion: 基于LLM的导航方法为双手自由的虚拟现实移动提供了一种舒适的替代方案，尤其有助于提升无障碍性。

Abstract: Locomotion plays a crucial role in shaping the user experience within virtual
reality environments. In particular, hands-free locomotion offers a valuable
alternative by supporting accessibility and freeing users from reliance on
handheld controllers. To this end, traditional speech-based methods often
depend on rigid command sets, limiting the naturalness and flexibility of
interaction. In this study, we propose a novel locomotion technique powered by
large language models (LLMs), which allows users to navigate virtual
environments using natural language with contextual awareness. We evaluate
three locomotion methods: controller-based teleportation, voice-based steering,
and our language model-driven approach. Our evaluation measures include
eye-tracking data analysis, including explainable machine learning through SHAP
analysis as well as standardized questionnaires for usability, presence,
cybersickness, and cognitive load to examine user attention and engagement. Our
findings indicate that the LLM-driven locomotion possesses comparable
usability, presence, and cybersickness scores to established methods like
teleportation, demonstrating its novel potential as a comfortable, natural
language-based, hands-free alternative. In addition, it enhances user attention
within the virtual environment, suggesting greater engagement. Complementary to
these findings, SHAP analysis revealed that fixation, saccade, and
pupil-related features vary across techniques, indicating distinct patterns of
visual attention and cognitive processing. Overall, we state that our method
can facilitate hands-free locomotion in virtual spaces, especially in
supporting accessibility.

</details>


### [406] [Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform](https://arxiv.org/abs/2504.17173)
*Tianyu Zhang,Dongheng Zhang,Ruixu Geng,Xuecheng Xie,Shuai Yang,Yan Chen*

Main category: cs.HC

TL;DR: 该论文探讨了在大型实际建筑中部署基于CSI的WiFi定位系统，通过新颖的图结构和预训练任务解决未标记数据和CSI异质性问题，实现了2.17米的中位数定位误差。


<details>
  <summary>Details</summary>
Motivation: 尽管CSI在WiFi定位中具有潜力，但其部署规模和商业化程度仍不及RSSI，主要因为现有系统多在受控小规模环境中开发。本研究旨在通过大型实际环境部署填补这一差距。

Method: 提出一种基于图的框架建模异质CSI数据并减少冗余，设计了结合时空先验的预训练任务以利用未标记数据，并采用置信感知微调策略提升鲁棒性。

Result: 在覆盖五层楼、25,600平方米的实验中，中位数定位误差为2.17米，楼层准确率达99.49%，平均绝对误差比最优基线降低18.7%。

Conclusion: 论文证明了基于CSI的大规模ISAC部署的可行性，通过创新方法显著提升了定位性能，为实际应用提供了有效解决方案。

Abstract: In recent years, Channel State Information (CSI), recognized for its
fine-grained spatial characteristics, has attracted increasing attention in
WiFi-based indoor localization. However, despite its potential, CSI-based
approaches have yet to achieve the same level of deployment scale and
commercialization as those based on Received Signal Strength Indicator (RSSI).
A key limitation lies in the fact that most existing CSI-based systems are
developed and evaluated in controlled, small-scale environments, limiting their
generalizability. To bridge this gap, we explore the deployment of a
large-scale CSI-based localization system involving over 400 Access Points
(APs) in a real-world building under the Integrated Sensing and Communication
(ISAC) paradigm. We highlight two critical yet often overlooked factors: the
underutilization of unlabeled data and the inherent heterogeneity of CSI
measurements. To address these challenges, we propose a novel CSI-based
learning framework for WiFi localization, tailored for large-scale ISAC
deployments on the server side. Specifically, we employ a novel graph-based
structure to model heterogeneous CSI data and reduce redundancy. We further
design a pretext pretraining task that incorporates spatial and temporal priors
to effectively leverage large-scale unlabeled CSI data. Complementarily, we
introduce a confidence-aware fine-tuning strategy to enhance the robustness of
localization results. In a leave-one-smartphone-out experiment spanning five
floors and 25, 600 m2, we achieve a median localization error of 2.17 meters
and a floor accuracy of 99.49%. This performance corresponds to an 18.7%
reduction in mean absolute error (MAE) compared to the best-performing
baseline.

</details>


### [407] [The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults](https://arxiv.org/abs/2504.17663)
*Michelle L. Ding,Harini Suresh*

Main category: cs.HC

TL;DR: 摘要：本文以幸存者为中心，探讨了社会技术AI治理在预防AI生成的未经同意的成人私密图像（AIG-NCII）中的作用，分析了当前治理方法的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示当前AI治理方法在防止“深度伪造色情”方面的无效性，并指出存在的技术生态系统漏洞。

Method: 方法包括分析恶意技术生态系统（MTE），评估当前治理框架（如NIST AI 100-4报告）的不足。

Result: 结果表明现有治理方法未能有效规范MTE，且存在错误的假设。

Conclusion: 结论是当前AI治理框架需改进，以更好地应对AIG-NCII问题。

Abstract: In this paper, we adopt a survivor-centered approach to locate and dissect
the role of sociotechnical AI governance in preventing AI-Generated
Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as
"deep fake pornography." We identify a "malicious technical ecosystem" or
"MTE," comprising of open-source face-swapping models and nearly 200
"nudifying" software programs that allow non-technical users to create AIG-NCII
within minutes. Then, using the National Institute of Standards and Technology
(NIST) AI 100-4 report as a reflection of current synthetic content governance
methods, we show how the current landscape of practices fails to effectively
regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining
these gaps.

</details>


### [408] [INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models](https://arxiv.org/abs/2504.17677)
*Jarne Thys,Sebe Vanbrabant,Davy Vanacken,Gustavo Rovelo Ruiz*

Main category: cs.HC

TL;DR: 论文介绍了INSIGHT，一个结合AI工具辅助教学的概念验证，通过分析学生问题动态构建FAQ并为教师提供个性化支持。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在教育中的潜力与挑战，尤其是如何利用AI增强教学个性化同时解决师生互动和隐私问题。

Method: 提出模块化设计的INSIGHT系统，通过提取学生问题的关键词动态构建FAQ，辅助教师提供个性化支持。

Result: 初步验证INSIGHT在高等教育课程中的可行性与实用性。

Conclusion: 未来可通过收集数据实现自适应学习，进一步提升互动性与包容性。

Abstract: The rise of AI, especially Large Language Models, presents challenges and
opportunities to integrate such technology into the classroom. AI has the
potential to revolutionize education by helping teaching staff with various
tasks, such as personalizing their teaching methods, but it also raises
concerns, for example, about the degradation of student-teacher interactions
and user privacy. This paper introduces INSIGHT, a proof of concept to combine
various AI tools to assist teaching staff and students in the process of
solving exercises. INSIGHT has a modular design that allows it to be integrated
into various higher education courses. We analyze students' questions to an LLM
by extracting keywords, which we use to dynamically build an FAQ from students'
questions and provide new insights for the teaching staff to use for more
personalized face-to-face support. Future work could build upon INSIGHT by
using the collected data to provide adaptive learning and adjust content based
on student progress and learning styles to offer a more interactive and
inclusive learning experience.

</details>
