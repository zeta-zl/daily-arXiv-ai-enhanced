<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 143]
- [cs.LG](#cs.LG) [Total: 139]
- [cs.AI](#cs.AI) [Total: 17]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 8]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.CV](#cs.CV) [Total: 41]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [stat.ML](#stat.ML) [Total: 11]
- [cs.LO](#cs.LO) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.PL](#cs.PL) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.SP](#eess.SP) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
*Richard Khoury,Maxence Verhaverbeke,Julie A. Gramaccia*

Key words: 5W1H, 新闻提取, 法语新闻, 自动化, GPT-4

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要介绍了一个自动提取法语句新闻中5W1H信息的系统，其性能与GPT-4相当，并使用人工标注的250篇文章作为评估数据。

Motivation: 5W1H问题在新闻中极为重要，但当前缺乏从法语新闻中自动提取这些信息的工具，因此需要设计一个自动化系统来解决这一问题。

Method: 设计了一个自动化提取5W1H信息的算法，并创建了一个由人工标注的250篇魁北克新闻文章的数据集用于评估。

Result: 实验结果表明，该自动化提取系统的性能可以与大型语言模型GPT-4相媲美。

Conclusion: 该研究成功开发了一个高效的自动化5W1H提取工具，填补了法语新闻领域的空白。

Abstract: The 5W1H questions -- who, what, when, where, why and how -- are commonly
used in journalism to ensure that an article describes events clearly and
systematically. Answering them is a crucial prerequisites for tasks such as
summarization, clustering, and news aggregation. In this paper, we design the
first automated extraction pipeline to get 5W1H information from French news
articles. To evaluate the performance of our algo- rithm, we also create a
corpus of 250 Quebec news articles with 5W1H answers marked by four human
annotators. Our results demonstrate that our pipeline performs as well in this
task as the large language model GPT-4o.

</details>


### [2] [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
*Tingchen Fu,Jiawei Gu,Yafu Li,Xiaoye Qu,Yu Cheng*

Key words: 指令遵循、数学推理、语言模型、基准测试、训练冲突

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了MathIF基准，用于评估数学推理任务中语言模型的指令遵循能力，揭示了推理能力提升与指令遵循之间的冲突。

Motivation: 探索大规模语言模型在数学推理任务中遵循自然语言指令的能力，发现当前模型在推理能力提升时往往牺牲指令遵循性。

Method: 引入MathIF基准进行实证分析，测试模型在蒸馏长链思维或强化学习训练后的指令遵循表现。

Result: 推理能力强的模型在指令遵循上表现较差，尤其是生成长度增加时；简单干预可部分恢复指令遵循性，但会牺牲推理性能。

Conclusion: 当前LLM训练范式存在推理能力与指令遵循性的冲突，需要开发更具指令感知能力的推理模型。

Abstract: Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.

</details>


### [3] [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
*Mingyang Wang,Lukas Lange,Heike Adel,Yunpu Ma,Jannik Strötgen,Hinrich Schütze*

Key words: 推理语言模型, 语言混合, 多语言推理, 约束解码, 内部表征

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 系统研究了推理语言模型（RLMs）中的语言混合现象，分析了其模式、影响及内部原因，并探讨了语言选择对性能的影响以及模型内部表征与语言混合的关系。

Motivation: 研究RLMs在推理过程中出现的语言混合现象，探讨其对模型性能的影响及其内部原因。

Method: 通过15种语言、7种任务难度和18个主题领域的实验，分析语言混合的模式、影响及内部原因；采用约束解码法探究语言选择对性能的影响。

Result: 强制模型使用拉丁或汉语脚本进行推理显著提高了准确性；推理轨迹的脚本组成与模型内部表征高度一致。

Conclusion: 语言选择对RLMs性能有显著影响，优化多语言推理并提供更可控的推理语言可提升模型的可解释性和适应性。

Abstract: Reasoning language models (RLMs) excel at complex tasks by leveraging a
chain-of-thought process to generate structured intermediate steps. However,
language mixing, i.e., reasoning steps containing tokens from languages other
than the prompt, has been observed in their outputs and shown to affect
performance, though its impact remains debated. We present the first systematic
study of language mixing in RLMs, examining its patterns, impact, and internal
causes across 15 languages, 7 task difficulty levels, and 18 subject areas, and
show how all three factors influence language mixing. Moreover, we demonstrate
that the choice of reasoning language significantly affects performance:
forcing models to reason in Latin or Han scripts via constrained decoding
notably improves accuracy. Finally, we show that the script composition of
reasoning traces closely aligns with that of the model's internal
representations, indicating that language mixing reflects latent processing
preferences in RLMs. Our findings provide actionable insights for optimizing
multilingual reasoning and open new directions for controlling reasoning
languages to build more interpretable and adaptable RLMs.

</details>


### [4] [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)
*Leon Lin,Jun Zheng,Haidong Wang*

Key words: WebNovelBench, 长篇故事生成, LLM评估, 叙事质量

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: WebNovelBench是一个用于评估长篇故事生成能力的新基准，基于4000多部中文网络小说，通过多维度自动评估框架区分人类作品与LLM生成内容。

Motivation: 现有基准在规模、多样性或客观性上不足，难以评估LLM的长篇叙事能力。

Method: 利用大规模数据集，将评估任务设计为从梗概生成故事，并通过LLM自动评估八个叙事维度。

Result: 实验显示，WebNovelBench能有效区分人类杰作、流行网络小说和LLM生成内容，并对24个前沿LLM进行了排名。

Conclusion: 该基准为评估和改进LLM叙事能力提供了可扩展、可复现且数据驱动的方法。

Abstract: Robustly evaluating the long-form storytelling capabilities of Large Language
Models (LLMs) remains a significant challenge, as existing benchmarks often
lack the necessary scale, diversity, or objective measures. To address this, we
introduce WebNovelBench, a novel benchmark specifically designed for evaluating
long-form novel generation. WebNovelBench leverages a large-scale dataset of
over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story
generation task. We propose a multi-faceted framework encompassing eight
narrative quality dimensions, assessed automatically via an LLM-as-Judge
approach. Scores are aggregated using Principal Component Analysis and mapped
to a percentile rank against human-authored works. Our experiments demonstrate
that WebNovelBench effectively differentiates between human-written
masterpieces, popular web novels, and LLM-generated content. We provide a
comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling
abilities and offering insights for future development. This benchmark provides
a scalable, replicable, and data-driven methodology for assessing and advancing
LLM-driven narrative generation.

</details>


### [5] [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)
*Yihong Liu,Mingyang Wang,Amir Hossein Kargaran,Felicia Körner,Ercong Nie,Barbara Plank,François Yvon,Hinrich Schütze*

Key words: 大型语言模型、多语言事实记忆、跨语言一致性、预训练、跨语言迁移

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本研究追踪OLMo-7B模型在预训练过程中多语言事实记忆和跨语言一致性的演变，发现其改进主要受预训练语料中事实频率影响，同时揭示跨语言迁移在早期预训练阶段的促进作用。

Motivation: 探索预训练过程中多语言事实记忆和跨语言一致性的发展动态，填补当前研究多关注最终模型的空白。

Method: 以OLMo-7B为案例研究，分析其在预训练期间的准确性和一致性变化，结合事实频率和跨语言迁移效应进行剖析。

Result: 事实频率主导模型的多语言记忆能力，低频率非英语事实通过英语对应信息的跨语言迁移实现正确回忆。跨语言迁移主要存在于涉及命名实体的关系类型中。

Conclusion: 多语言事实获取通过频率驱动学习和跨语言迁移两条路径实现，前者更为普遍且语言无关，后者规模有限且集中于特定关系类型。

Abstract: Large Language Models (LLMs) are capable of recalling multilingual factual
knowledge present in their pretraining data. However, most studies evaluate
only the final model, leaving the development of factual recall and
crosslingual consistency throughout pretraining largely unexplored. In this
work, we trace how factual recall and crosslingual consistency evolve during
pretraining, focusing on OLMo-7B as a case study. We find that both accuracy
and consistency improve over time for most languages. We show that this
improvement is primarily driven by the fact frequency in the pretraining
corpus: more frequent facts are more likely to be recalled correctly,
regardless of language. Yet, some low-frequency facts in non-English languages
can still be correctly recalled. Our analysis reveals that these instances
largely benefit from crosslingual transfer of their English counterparts -- an
effect that emerges predominantly in the early stages of pretraining. We
pinpoint two distinct pathways through which multilingual factual knowledge
acquisition occurs: (1) frequency-driven learning, which is dominant and
language-agnostic, and (2) crosslingual transfer, which is limited in scale and
typically constrained to relation types involving named entities. We release
our code and data to facilitate further research at
https://github.com/cisnlp/multilingual-fact-tracing.

</details>


### [6] [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
*Yufan Zhuang,Liyuan Liu,Chandan Singh,Jingbo Shang,Jianfeng Gao*

Key words: 自动回归生成, Mixture of Inputs, 贝叶斯估计, 文本生成, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的自动回归生成方法Mixture of Inputs (MoI)，通过保留丢弃的令牌分布信息，提升生成文本的质量和推理能力。

Motivation: 传统的自动回归生成方法会丢弃下一令牌的分布信息，导致信息损失。MoI旨在保留这些信息以提升模型表现。

Method: MoI在生成令牌后，将离散令牌与之前丢弃的分布信息结合，采用贝叶斯估计方法生成连续后验期望作为新输入。

Result: MoI在数学推理、代码生成和博士级QA任务中均显著提升了多个模型的性能，且无需额外训练。

Conclusion: MoI通过保留分布信息，显著提升了自动回归生成的质量和推理能力。

Abstract: In standard autoregressive generation, an LLM predicts the next-token
distribution, samples a discrete token, and then discards the distribution,
passing only the sampled token as new input. To preserve this distribution's
rich information, we propose Mixture of Inputs (MoI), a training-free method
for autoregressive generation. After generating a token following the standard
paradigm, we construct a new input that blends the generated discrete token
with the previously discarded token distribution. Specifically, we employ a
Bayesian estimation method that treats the token distribution as the prior, the
sampled token as the observation, and replaces the conventional one-hot vector
with the continuous posterior expectation as the new model input. MoI allows
the model to maintain a richer internal representation throughout the
generation process, resulting in improved text quality and reasoning
capabilities. On mathematical reasoning, code generation, and PhD-level QA
tasks, MoI consistently improves performance across multiple models including
QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional
training and negligible computational overhead.

</details>


### [7] [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
*Wonje Jeung,Sangyeon Yoon,Albert No*

Key words: 机器遗忘、LLM、混合查询、SEPS、MP策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了SEPS评估框架，用于衡量模型在单提示中同时遗忘和保留信息的能力，并揭示了现有遗忘方法的两个关键失败模式。为解决这些问题，提出了Mixed Prompt（MP）遗忘策略，显著提升了遗忘效果。

Motivation: 现有遗忘指标未能捕捉现实场景中遗忘和保留查询共存的情况，导致评估不准确。

Method: 提出SEPS评估框架和Mixed Prompt（MP）遗忘策略，将遗忘和保留查询整合为统一的训练目标。

Result: MP策略显著提升了遗忘效果，在复杂设置中表现稳健，即使单提示包含多达八个混合查询。

Conclusion: SEPS和MP策略有效解决了混合查询场景中的遗忘问题，为未来研究提供了新方向。

Abstract: Machine unlearning aims to selectively remove targeted knowledge from Large
Language Models (LLMs), ensuring they forget specified content while retaining
essential information. Existing unlearning metrics assess whether a model
correctly answers retain queries and rejects forget queries, but they fail to
capture real-world scenarios where forget queries rarely appear in isolation.
In fact, forget and retain queries often coexist within the same prompt, making
mixed-query evaluation crucial.
  We introduce SEPS, an evaluation framework that explicitly measures a model's
ability to both forget and retain information within a single prompt. Through
extensive experiments across three benchmarks, we identify two key failure
modes in existing unlearning methods: (1) untargeted unlearning
indiscriminately erases both forget and retain content once a forget query
appears, and (2) targeted unlearning overfits to single-query scenarios,
leading to catastrophic failures when handling multiple queries. To address
these issues, we propose Mixed Prompt (MP) unlearning, a strategy that
integrates both forget and retain queries into a unified training objective.
Our approach significantly improves unlearning effectiveness, demonstrating
robustness even in complex settings with up to eight mixed forget and retain
queries in a single prompt.

</details>


### [8] [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)
*Wang Jiaqi,Wang bo,Guo fa,Cheng cheng,Yang li*

Key words: 大型语言模型、个性特征、分布式个性框架、人机交互、人工智能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）是否表现类似人类的个性特征，发现其个性是动态且依赖于输入的，提出了分布式个性框架。

Motivation: 探索LLM是否具有类似人类的个性特征，并验证传统个性评估工具的适用性。

Method: 采用行为基础的方法，通过三项实证研究：测试重测稳定性、跨变体一致性分析、角色扮演中的个性保留。

Result: LLM的个性特征表现出高变异性、输入敏感性和低一致性，缺乏长期稳定性。

Conclusion: LLM的个性是动态且外部依赖的，为构建LLM特定个性框架和人机交互提供了新视角。

Abstract: Large Language Models (LLMs) have demonstrated human-like capabilities in
language comprehension and generation, becoming active participants in social
and cognitive domains. This study investigates whether LLMs exhibit
personality-like traits and how these traits compare with human personality,
focusing on the applicability of conventional personality assessment tools. A
behavior-based approach was used across three empirical studies. Study 1
examined test-retest stability and found that LLMs show higher variability and
are more input-sensitive than humans, lacking long-term stability. Based on
this, we propose the Distributed Personality Framework, conceptualizing LLM
traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency
in personality measures and found LLMs' responses were highly sensitive to item
wording, showing low internal consistency compared to humans. Study 3 explored
personality retention during role-playing, showing LLM traits are shaped by
prompt and parameter settings. These findings suggest that LLMs express fluid,
externally dependent personality patterns, offering insights for constructing
LLM-specific personality frameworks and advancing human-AI interaction. This
work contributes to responsible AI development and extends the boundaries of
personality psychology in the age of intelligent systems.

</details>


### [9] [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
*Xi Wang,Jiaqian Hu,Safinah Ali*

Key words: MAATS, MQM, 多代理, 翻译优化, 语义准确性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MAATS是一个多代理自动翻译系统，利用MQM框架进行细粒度错误检测和翻译优化，通过多个专门化的AI代理和合成代理迭代改进翻译效果，表现出色。

Motivation: 现有的单代理翻译系统依赖自校正，局限性较大。MAATS旨在利用MQM框架和多个分工明确的代理，提升翻译的语义准确性和上下文适应性。

Method: MAATS采用多个专注于不同MQM类别（如准确性、流畅性、风格、术语）的AI代理，结合合成代理整合标注结果，迭代优化翻译。

Result: 在多种语言对和LLMs上，MAATS在自动指标和人工评估中显著优于零样本和单代理基线，尤其在语义准确性和跨语言适应上表现突出。

Conclusion: MAATS通过模块化代理和MQM框架的协同工作，缩小了黑盒LLMs与人工翻译流程的差距，提升了翻译的深度准确性。

Abstract: We present MAATS, a Multi Agent Automated Translation System that leverages
the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal
for error detection and refinement. MAATS employs multiple specialized AI
agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,
Style, Terminology), followed by a synthesis agent that integrates the
annotations to iteratively refine translations. This design contrasts with
conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs),
MAATS outperforms zero-shot and single-agent baselines with statistically
significant gains in both automatic metrics and human assessments. It excels
particularly in semantic accuracy, locale adaptation, and linguistically
distant language pairs. Qualitative analysis highlights its strengths in
multi-layered error diagnosis, omission detection across perspectives, and
context-aware refinement. By aligning modular agent roles with interpretable
MQM dimensions, MAATS narrows the gap between black-box LLMs and human
translation workflows, shifting focus from surface fluency to deeper semantic
and contextual fidelity.

</details>


### [10] [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)
*Drishya Karki,Michiel Kamphuis,Angelecia Frey*

Key words: EasyMath, 数学推理, 小型语言模型, 基准测试, 零样本

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: EasyMath 是一个用于小型语言模型实用数学推理的紧凑基准，涵盖 13 个类别，测试了 23 个模型，结果显示准确性与模型大小和训练相关。

Motivation: 研究小型语言模型在数学推理任务中的表现，为实际应用提供基准测试工具。

Method: 设计了涵盖 13 类数学问题的 EasyMath 基准，测试了 23 个模型（参数从 14M 到 4B），采用零样本设置进行精确、数值和符号检查。

Result: 模型准确性与规模和训练程度正相关，链式思考（chain-of-thought）带来小幅提升，大规模模型表现更一致。

Conclusion: 小型语言模型的数学推理能力与规模和训练紧密相关，链式思考方法效果有限。

Abstract: EasyMath is a compact benchmark for practical math reasoning in small
language models. It covers thirteen categories, from basic arithmetic and order
of operations to word problems, algebraic expressions, edge cases, and omits
specialist topics. We tested 23 models (14M to 4B parameters) using exact,
numerical, and symbolic checks on free-form answers in a zero-shot setting.
Accuracy rises with size and training, chain-of-thought adds modest gains, and
consistency improves at scale.

</details>


### [11] [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)
*Ryan Solgi,Kai Zhen,Rupak Vignesh Swaminathan,Nathan Susanj,Athanasios Mouchtaris,Siegfried Kunzmann,Zheng Zhang*

Key words: 大语言模型,低秩张量压缩,稀疏增强张量网络,模型微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了如何在资源受限设备上高效部署大语言模型，提出了一种名为Saten的低秩张量压缩方法，显著提升了模型的压缩效率和准确性。

Motivation: 大语言模型在资源受限设备上的部署需要高效的压缩技术，但其预训练后的高秩特性及缺乏预训练数据限制了现有方法的适用性。

Method: 提出了稀疏增强张量网络（Saten），在微调过程中对低秩张量化的大语言模型进行压缩。

Result: 实验表明，Saten在压缩效率和准确性上均优于现有方法，实现了最先进的性能。

Conclusion: Saten框架为后训练阶段的大语言模型压缩提供了一种高效的解决方案。

Abstract: The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose sparse augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.

</details>


### [12] [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
*Chin-Jou Li,Eunjung Yeo,Kwanghee Choi,Paula Andrea Pérez-Toro,Masao Someki,Rohan Kumar Das,Zhengjun Yue,Juan Rafael Orozco-Arroyave,Elmar Nöth,David R. Mortensen*

Key words: 自动语音识别, 构音障碍, 语音转换, 多语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过语音转换模型生成非英语语言中类似构音障碍的语音数据，用于改进多语言自动语音识别（ASR）在构音障碍语音中的表现。

Motivation: 解决非英语语言中构音障碍语音数据稀缺的问题。

Method: 利用英语构音障碍语音数据（UASpeech）微调语音转换模型，生成非英语健康语音（FLEURS）的构音障碍版本，并用于微调多语言ASR模型（MMS）。

Result: 在西班牙语、意大利语和泰米尔语的测试集上，提出的方法显著优于基线模型和传统数据增强技术。

Conclusion: 通过语音转换生成的数据有效模拟构音障碍特征，显著提升ASR性能。

Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging
due to data scarcity, particularly in non-English languages. To address this,
we fine-tune a voice conversion model on English dysarthric speech (UASpeech)
to encode both speaker characteristics and prosodic distortions, then apply it
to convert healthy non-English speech (FLEURS) into non-English dysarthric-like
speech. The generated data is then used to fine-tune a multilingual ASR model,
Massively Multilingual Speech (MMS), for improved dysarthric speech
recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE
(Tamil) demonstrates that VC with both speaker and prosody conversion
significantly outperforms the off-the-shelf MMS performance and conventional
augmentation techniques such as speed and tempo perturbation. Objective and
subjective analyses of the generated data further confirm that the generated
speech simulates dysarthric characteristics.

</details>


### [13] [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/abs/2505.14880)
*Chris Sypherd,Sergei Petrov,Sonny George,Vaishak Belle*

Key words: 大语言模型, 提示策略, 效率评估, token成本, Big-$O_{tok}$

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大语言模型的表现高度依赖提示策略，本文提出Big-$O_{tok}$框架和Token Cost评估效率，发现token使用增加带来的性能回报显著递减。

Motivation: 尽管大语言模型表现优异，但其实际效用受提示策略效率（性能和token使用平衡）的直接影响，需要更实用的评估指标。

Method: 提出Big-$O_{tok}$理论框架用于描述提示策略的token使用增长，并引入Token Cost（每性能的token成本）作为实证指标。

Result: 验证了Big-$O_{tok}$分析的有效性，发现token使用增加会导致性能回报急剧递减。

Conclusion: 强调效率评估的重要性，Big-$O_{tok}$和Token Cost为提示策略的实用性提供了新视角。

Abstract: In recent years, large language models have demonstrated remarkable
performance across diverse tasks. However, their task effectiveness is heavily
dependent on the prompting strategy used to elicit output, which can vary
widely in both performance and token usage. While task performance is often
used to determine prompting strategy success, we argue that
efficiency--balancing performance and token usage--can be a more practical
metric for real-world utility. To enable this, we propose Big-$O_{tok}$, a
theoretical framework for describing the token usage growth of prompting
strategies, and analyze Token Cost, an empirical measure of tokens per
performance. We apply these to several common prompting strategies and find
that increased token usage leads to drastically diminishing performance
returns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need
for efficiency-aware evaluations.

</details>


### [14] [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/abs/2505.14886)
*Danqing Wang,Zhuorui Ye,Xinran Zhao,Fei Fang,Lei Li*

Key words: 竞争性辩论, TreeDebater, Rehearsal Tree, Debate Flow Tree, 多代理系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: TreeDebater框架通过预演树和辩论流树结构优化了竞争性辩论中的时间分配和互动策略，显著优于现有系统。

Motivation: 竞争性辩论中时间限制和互动性评估的独特挑战需要新的框架来解决。

Method: 提出TreeDebater框架，使用Rehearsal Tree和Debate Flow Tree结构，结合时间分配和观众反馈。

Result: TreeDebater在多代理辩论系统中表现最优，且策略更接近人类专家。

Conclusion: TreeDebater在竞争性辩论中表现出色，尤其在时间管理和互动策略上。

Abstract: Winning competitive debates requires sophisticated reasoning and argument
skills. There are unique challenges in the competitive debate: (1) The time
constraints force debaters to make strategic choices about which points to
pursue rather than covering all possible arguments; (2) The persuasiveness of
the debate relies on the back-and-forth interaction between arguments, which a
single final game status cannot evaluate. To address these challenges, we
propose TreeDebater, a novel debate framework that excels in competitive
debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow
Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the
strength of the claim, while the Debate Flow Tree tracks the debate status to
identify the active actions. TreeDebater allocates its time budget among
candidate actions and uses the speech time controller and feedback from the
simulated audience to revise its statement. The human evaluation on both the
stage-level and the debate-level comparison shows that our TreeDebater
outperforms the state-of-the-art multi-agent debate system. Further
investigation shows that TreeDebater shows better strategies in limiting time
to important debate actions, aligning with the strategies of human debate
experts.

</details>


### [15] [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/abs/2505.14887)
*Nathan Roll,Calbert Graham,Yuka Tatsumi,Kim Tien Nguyen,Meghan Sumner,Dan Jurafsky*

Key words: 语音模型、上下文学习、适应性、词错误率、ASR

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了人类听众通过暴露适应不同说话者和语言变体的能力是否适用于现代语音模型。提出了一种可扩展的框架，通过少量示例（约50秒）将词错误率降低了19.7%。

Motivation: 研究语音模型是否能够通过少量示例适应不同说话者和语言变体，类似人类的适应能力。

Method: 使用Phi-4多模态模型，通过任务提示和音频-文本对的上下文学习（ICL）框架。

Result: 在12个示例（约50秒）下，词错误率平均降低了19.7%，尤其在低资源语言变体中表现显著。

Conclusion: ICL方案表现出类似人类的适应能力，提升了ASR的鲁棒性，但在某些变体中仍有差距。

Abstract: Human listeners readily adjust to unfamiliar speakers and language varieties
through exposure, but do these adaptation benefits extend to state-of-the-art
spoken language models? We introduce a scalable framework that allows for
in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts
and audio-text pairs, and find that as few as 12 example utterances (~50
seconds) at inference time reduce word error rates by a relative 19.7% (1.2
pp.) on average across diverse English corpora. These improvements are most
pronounced in low-resource varieties, when the context and target speaker
match, and when more examples are provided--though scaling our procedure yields
diminishing marginal returns to context length. Overall, we find that our novel
ICL adaptation scheme (1) reveals a similar performance profile to human
listeners, and (2) demonstrates consistent improvements to automatic speech
recognition (ASR) robustness across diverse speakers and language backgrounds.
While adaptation succeeds broadly, significant gaps remain for certain
varieties, revealing where current models still fall short of human
flexibility. We release our prompts and code on GitHub.

</details>


### [16] [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)
*Jacob X Li,Shreyas S Raman,Jessica Wan,Fahad Samman,Jazlyn Lin*

Key words: 大语言模型,状态跟踪,注意力机制,状态转换,符号计算

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了大语言模型（LLMs）在跟踪内部状态方面的能力，发现其状态转换动态建模存在局限性，尤其在状态空间增大或转换稀疏时准确性显著下降。

Motivation: 研究LLMs在确定性状态动态建模中的表现，以理解其内部状态跟踪能力的机制和限制。

Method: 在三个领域（Box Tracking、Abstract DFA Sequences和Complex Text Games）评估LLMs的状态转换准确性，并通过激活修补技术识别相关注意力头。

Result: 随着状态空间增大或转换稀疏，LLMs的准确性显著下降；特定注意力头负责状态信息传播，但状态-动作联合推理较弱。

Conclusion: LLMs的状态跟踪能力源于多个注意力头的分布式交互，而非显式符号计算。

Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring
internal state tracking, yet their ability to model state transition dynamics
remains poorly understood. We evaluate how well LLMs capture deterministic
state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and
Complex Text Games, each formalizable as a finite-state system. Across tasks,
we find that next-state prediction accuracy degrades with increasing
state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in
low-complexity settings but drops below 30% when the number of boxes or states
exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%
accuracy when the number of states is > 10 and transitions are < 30. Through
activation patching, we identify attention heads responsible for propagating
state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,
11, 12, and 14. While these heads successfully move relevant state features,
action information is not reliably routed to the final token, indicating weak
joint state-action reasoning. Our results suggest that state tracking in LLMs
emerges from distributed interactions of next-token heads rather than explicit
symbolic computation.

</details>


### [17] [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/abs/2505.14905)
*Xiaoyan Bai,Ike Peng,Aditya Singh,Chenhao Tan*

Key words: 概念冲突,大型语言模型,角色扮演,时间边界,模型行为

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LLMs遇到概念冲突时是否应指出问题或直接生成内容？本文引入概念冲突分析模型行为，发现模型在角色死亡时表现不佳，提出改进方法。

Motivation: 研究LLMs在用户提示或模型表示中概念冲突时的行为，以解决未定义或错误定义的问题。

Method: 通过角色扮演设置中的时间边界，提出三项行为指标（回避率、条件准确率和回答率）量化模型行为，并通过实验分析原因。

Result: 模型在角色死亡后未回避，准确率下降，主要由于死亡状态编码不可靠和角色扮演导致的时间表示偏移。

Conclusion: 概念冲突导致模型行为异常，未来需改进模型在此类情况下的行为一致性。

Abstract: Consider this prompt "Draw a unicorn with two horns". Should large language
models (LLMs) recognize that a unicorn has only one horn by definition and ask
users for clarifications, or proceed to generate something anyway? We introduce
concept incongruence to capture such phenomena where concept boundaries clash
with each other, either in user prompts or in model representations, often
leading to under-specified or mis-specified behaviors. In this work, we take
the first step towards defining and analyzing model behavior under concept
incongruence. Focusing on temporal boundaries in the Role-Play setting, we
propose three behavioral metrics--abstention rate, conditional accuracy, and
answer rate--to quantify model behavior under incongruence due to the role's
death. We show that models fail to abstain after death and suffer from an
accuracy drop compared to the Non-Role-Play setting. Through probing
experiments, we identify two main causes: (i) unreliable encoding of the
"death" state across different years, leading to unsatisfactory abstention
behavior, and (ii) role playing causes shifts in the model's temporal
representations, resulting in accuracy drops. We leverage these insights to
improve consistency in the model's abstention and answer behaviors. Our
findings suggest that concept incongruence leads to unexpected model behaviors
and point to future directions on improving model behavior under concept
incongruence.

</details>


### [18] [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/abs/2505.14906)
*Ye Yuan,Haolun Wu,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Key words: 6G网络,信息提取,语言模型,结构化实体,TeleSEE

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于语言模型的信息提取技术TeleSEE，用于从电信领域中提取结构化实体，通过高效的表示方法和分层并行解码提升准确性和处理速度。

Motivation: 6G网络中知识理解对推动网络智能和AI原生架构至关重要，信息提取能将碎片化电信知识转化为结构化格式，帮助AI模型更好地理解网络术语。

Method: TeleSEE采用令牌高效表示方法来预测实体类型和属性键，减少输出令牌数量并提高准确性；同时引入分层并行解码方法，优化标准编码器-解码器架构。

Result: 实验表明，TeleSEE比其他基线技术准确性更高，样本处理速度提升5到9倍。

Conclusion: TeleSEE是一种高效且准确的电信领域信息提取方法，适用于6G网络知识理解。

Abstract: Knowledge understanding is a foundational part of envisioned 6G networks to
advance network intelligence and AI-native network architectures. In this
paradigm, information extraction plays a pivotal role in transforming
fragmented telecom knowledge into well-structured formats, empowering diverse
AI models to better understand network terminologies. This work proposes a
novel language model-based information extraction technique, aiming to extract
structured entities from the telecom context. The proposed telecom structured
entity extraction (TeleSEE) technique applies a token-efficient representation
method to predict entity types and attribute keys, aiming to save the number of
output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a
hierarchical parallel decoding method, improving the standard encoder-decoder
architecture by integrating additional prompting and decoding strategies into
entity extraction tasks. In addition, to better evaluate the performance of the
proposed technique in the telecom domain, we further designed a dataset named
6GTech, including 2390 sentences and 23747 words from more than 100 6G-related
technical publications. Finally, the experiment shows that the proposed TeleSEE
method achieves higher accuracy than other baseline techniques, and also
presents 5 to 9 times higher sample processing speed.

</details>


### [19] [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/abs/2505.14917)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Sophia Ananiadou*

Key words: 大语言模型, 阴谋论检测, 情绪伪装, ConDID-v2, ConspEmoLLM-v2

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究提出了一种增强版数据集ConDID-v2和检测模型ConspEmoLLM-v2，用于识别LLM生成的情绪伪装阴谋论内容，效果优于现有方法。

Motivation: 为解决大语言模型（LLM）生成的阴谋论内容可能通过改变情绪特征伪装成正常文本的问题，现有检测方法不适用于此类内容。

Method: 开发了增强版数据集ConDID-v2，包含人工和LLM重写的阴谋论推文，并训练了改进模型ConspEmoLLM-v2。

Result: 实验显示，ConspEmoLLM-v2在原始和情绪伪装内容上的检测性能均优于先前模型和其他基线方法。

Conclusion: 该方法能有效应对LLM生成的情绪伪装阴谋论内容，提升了检测模型的鲁棒性。

Abstract: Despite the many benefits of large language models (LLMs), they can also
cause harm, e.g., through automatic generation of misinformation, including
conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories
by altering characteristic textual features, e.g., by transforming their
typically strong negative emotions into a more positive tone. Although several
studies have proposed automated conspiracy theory detection methods, they are
usually trained using human-authored text, whose features can vary from
LLM-generated text. Furthermore, several conspiracy detection models, including
the previously proposed ConspEmoLLM, rely heavily on the typical emotional
features of human-authored conspiracy content. As such, intentionally disguised
content may evade detection. To combat such issues, we firstly developed an
augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which
supplements human-authored conspiracy tweets with versions rewritten by an LLM
to reduce the negativity of their original sentiment. The quality of the
rewritten tweets was verified by combining human and LLM-based assessment. We
subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of
ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or
exceeds the performance of ConspEmoLLM on the original human-authored content
in ConDID, and considerably outperforms both ConspEmoLLM and several other
baselines when applied to sentiment-transformed tweets in ConDID-v2. The
project will be available at https://github.com/lzw108/ConspEmoLLM.

</details>


### [20] [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)
*Fadel M. Megahed,Ying-Ju Chen,L. Allision Jones-Farmer,Younghwa Lee,Jiawei Brooke Wang,Inez M. Zwetsloot*

Key words: 大型语言模型, 文本分类, 可靠性评估, 情感分析, 金融新闻

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究提出了一个评估大型语言模型（LLM）二元文本分类一致性的框架，通过心理测量原则确定样本量、开发无效响应指标，并在金融新闻情感分类案例中验证了14种LLM的可靠性。

Motivation: 解决缺乏可靠性评估方法的问题，为LLM分类任务提供系统化的选择和资源优化指导。

Method: 采用心理测量原则，设计样本量要求和可靠性指标，通过金融新闻情感分类的案例研究（包含14种LLM的多重测试）。

Result: 模型表现出高内部分类一致性（90-98%），小模型如gemma3:1B表现优于大模型，但对市场实际运动的预测表现随机。

Conclusion: 框架为LLM选择、样本规划和可靠性评估提供系统工具，帮助组织优化分类任务的资源分配。

Abstract: This study introduces a framework for evaluating consistency in large
language model (LLM) binary text classification, addressing the lack of
established reliability assessment methods. Adapting psychometric principles,
we determine sample size requirements, develop metrics for invalid responses,
and evaluate intra- and inter-rater reliability. Our case study examines
financial news sentiment classification across 14 LLMs (including
claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and
command-r-plus), with five replicates per model on 1,350 articles. Models
demonstrated high intra-rater consistency, achieving perfect agreement on
90-98% of examples, with minimal differences between expensive and economical
models from the same families. When validated against StockNewsAPI labels,
models achieved strong performance (accuracy 0.76-0.88), with smaller models
like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger
counterparts. All models performed at chance when predicting actual market
movements, indicating task constraints rather than model limitations. Our
framework provides systematic guidance for LLM selection, sample size planning,
and reliability assessment, enabling organizations to optimize resources for
classification tasks.

</details>


### [21] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
*Udita Patel,Rutu Mulkar,Jay Roberts,Cibi Chakravarthy Senthilkumar,Sujay Gandhi,Xiaofei Zheng,Naumaan Nayyar,Rafael Castrillo*

Key words: THELMA, RAG, QA, 评估框架, 无参考

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: THELMA是一个无参考的框架，用于全面评估RAG QA应用，包含六项相互关联的指标，帮助开发者优化流程。

Motivation: 为RAG QA应用提供一个无需标注数据或参考响应的精细化评估方法。

Method: 设计六项相互关联的指标，用于全面细粒度评估QA应用。

Result: THELMA能识别RAG组件中需要改进的部分。

Conclusion: THELMA为RAG QA应用提供了有效的评估和优化工具。

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>


### [22] [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)
*Sil Hamilton,Rebecca M. M. Hicke,Matthew Wilkens,David Mimno*

Key words: 长文本理解, 语言模型评估, TLDM基准, 小说分析

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出TLDM基准测试，用于评估语言模型在长文本中的理解能力，发现现有模型在大约64K tokens后表现不稳定。

Motivation: 尽管LLMs的上下文长度已大幅增加，但对其在复杂长文本中能力的评估仍不足，尤其是超出“大海捞针”方法的部分。小说因其复杂结构和长程语义依赖成为理想案例。

Method: 基于计算小说分析的现有工作，设计了TLDM基准测试，测试模型对情节摘要、故事世界配置和叙事时间的理解能力。

Result: 测试的7个前沿LLMs在超过64K tokens后均无法保持稳定理解。

Conclusion: 语言模型开发者需在复杂长文本场景中采用更全面的评估方法，而不仅仅是“中间丢失”基准。TLDM基准及相关代码和数据已公开以促进进一步研究。

Abstract: Although the context length of large language models (LLMs) has increased to
millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack
approaches has proven difficult. We argue that novels provide a case study of
subtle, complicated structure and long-range semantic dependencies often over
128k tokens in length. Inspired by work on computational novel analysis, we
release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's
ability to report plot summary, storyworld configuration, and elapsed narrative
time. We find that none of seven tested frontier LLMs retain stable
understanding beyond 64k tokens. Our results suggest language model developers
must look beyond "lost in the middle" benchmarks when evaluating model
performance in complex long-context scenarios. To aid in further development we
release the TLDM benchmark together with reference code and data.

</details>


### [23] [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/abs/2505.14963)
*Shan Chen,Pedro Moreira,Yuxin Xiao,Sam Schmidgall,Jeremy Warner,Hugo Aerts,Thomas Hartvigsen,Jack Gallifant,Danielle S. Bitterman*

Key words: 大型语言模型,临床决策,多跳推理,基准测试

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MedBrowseComp是一个新的基准测试，用于系统评估大型语言模型在临床实践中检索和合成多跳医学事实的能力，揭示了现有模型在临床推理中的性能不足。

Motivation: 临床实践需要整合多样化的知识库并确保严格准确性，但现有评估方法未能充分验证大型语言模型的实用性。

Method: MedBrowseComp包含1000多个人工策划的问题，模拟临床场景，测试模型从动态知识库中检索和合成信息的能力。

Result: 前沿代理系统的性能低至10%，表明现有模型与临床需求之间存在巨大差距。

Conclusion: MedBrowseComp为可靠的医学信息检索提供了明确的测试基准，并为未来模型升级设定了目标。

Abstract: Large language models (LLMs) are increasingly envisioned as decision-support
tools in clinical practice, yet safe clinical reasoning demands integrating
heterogeneous knowledge bases -- trials, primary studies, regulatory documents,
and cost data -- under strict accuracy constraints. Existing evaluations often
rely on synthetic prompts, reduce the task to single-hop factoid queries, or
conflate reasoning with open-ended generation, leaving their real-world utility
unclear. To close this gap, we present MedBrowseComp, the first benchmark that
systematically tests an agent's ability to reliably retrieve and synthesize
multi-hop medical facts from live, domain-specific knowledge bases.
MedBrowseComp contains more than 1,000 human-curated questions that mirror
clinical scenarios where practitioners must reconcile fragmented or conflicting
information to reach an up-to-date conclusion. Applying MedBrowseComp to
frontier agentic systems reveals performance shortfalls as low as ten percent,
exposing a critical gap between current LLM capabilities and the rigor demanded
in clinical settings. MedBrowseComp therefore offers a clear testbed for
reliable medical information seeking and sets concrete goals for future model
and toolchain upgrades. You can visit our project page at:
https://moreirap12.github.io/mbc-browse-app/

</details>


### [24] [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
*Prashanth Vijayaraghavan,Soroush Vosoughi,Lamogha Chizor,Raya Horesh,Rogerio Abreu de Paula,Ehsan Degan,Vandana Mukherjee*

Key words: 大型语言模型、种姓偏见、DECASTE框架、公平性评估、社会偏见

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出DECASTE框架，用于检测和评估大型语言模型（LLMs）中种姓偏见，揭示模型对印度边缘种姓群体的系统性偏见，并呼吁更全面的偏见评估方法。

Motivation: 大型语言模型在广泛应用的同时，仍反映出社会偏见，特别是针对印度边缘种姓群体的偏见，这一问题的研究尚不充分。

Method: 提出DECASTE框架，通过四个维度（社会文化、经济、教育、政治）和个性化提示策略，检测LLMs中的显性和隐性种姓偏见。

Result: 测试显示主流LLMs系统性强化种姓偏见，对边缘种姓（如达利特和首陀罗）的偏见评分显著高于主导种姓。

Conclusion: 研究揭示了LLMs中潜藏的种姓偏见，强调需开发更全面且包容的偏见评估方法，以降低实际部署的风险。

Abstract: Recent advancements in large language models (LLMs) have revolutionized
natural language processing (NLP) and expanded their applications across
diverse domains. However, despite their impressive capabilities, LLMs have been
shown to reflect and perpetuate harmful societal biases, including those based
on ethnicity, gender, and religion. A critical and underexplored issue is the
reinforcement of caste-based biases, particularly towards India's marginalized
caste groups such as Dalits and Shudras. In this paper, we address this gap by
proposing DECASTE, a novel, multi-dimensional framework designed to detect and
assess both implicit and explicit caste biases in LLMs. Our approach evaluates
caste fairness across four dimensions: socio-cultural, economic, educational,
and political, using a range of customized prompting strategies. By
benchmarking several state-of-the-art LLMs, we reveal that these models
systematically reinforce caste biases, with significant disparities observed in
the treatment of oppressed versus dominant caste groups. For example, bias
scores are notably elevated when comparing Dalits and Shudras with dominant
caste groups, reflecting societal prejudices that persist in model outputs.
These results expose the subtle yet pervasive caste biases in LLMs and
emphasize the need for more comprehensive and inclusive bias evaluation
methodologies that assess the potential risks of deploying such models in
real-world contexts.

</details>


### [25] [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/abs/2505.14972)
*Haoyi Qiu,Kung-Hsiang Huang,Ruichen Zheng,Jiao Sun,Nanyun Peng*

Key words: 大型视觉语言模型、文化安全、CROSS基准、多模态评估、模型改进

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CROSS是一个评估大型视觉语言模型文化安全推理能力的基准测试，揭示了现有模型在文化意识和规范遵从性方面的显著不足，并提出了两种改进方法。

Motivation: 现有模型在多模态安全测试中忽视文化规范，可能导致符号伤害，需要专门评估和提升模型的文化安全能力。

Method: 提出CROSS基准和CROSS-Eval框架，评估21种模型的文化安全维度，并通过监督微调和偏好调优改进模型性能。

Result: 最佳模型在文化意识和规范遵从性上表现不佳（61.79%和37.73%），但改进方法显著提升GPT-4o的能力（+60.14%和+55.2%）。

Conclusion: 增加推理能力有助于文化对齐，但问题未完全解决；通过微调和偏好调优可有效提升模型文化安全性。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in globally
distributed applications, such as tourism assistants, yet their ability to
produce culturally appropriate responses remains underexplored. Existing
multimodal safety benchmarks primarily focus on physical safety and overlook
violations rooted in cultural norms, which can result in symbolic harm. To
address this gap, we introduce CROSS, a benchmark designed to assess the
cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284
multilingual visually grounded queries from 16 countries, three everyday
domains, and 14 languages, where cultural norm violations emerge only when
images are interpreted in context. We propose CROSS-Eval, an intercultural
theory-based framework that measures four key dimensions: cultural awareness,
norm education, compliance, and helpfulness. Using this framework, we evaluate
21 leading LVLMs, including mixture-of-experts models and reasoning models.
Results reveal significant cultural safety gaps: the best-performing model
achieves only 61.79% in awareness and 37.73% in compliance. While some
open-source models reach GPT-4o-level performance, they still fall notably
short of proprietary models. Our results further show that increasing reasoning
capacity improves cultural alignment but does not fully resolve the issue. To
improve model performance, we develop two enhancement strategies: supervised
fine-tuning with culturally grounded, open-ended data and preference tuning
with contrastive response pairs that highlight safe versus unsafe behaviors.
These methods substantially improve GPT-4o's cultural awareness (+60.14%) and
compliance (+55.2%), while preserving general multimodal capabilities with
minimal performance reduction on general multimodal understanding benchmarks.

</details>


### [26] [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/abs/2505.14984)
*Adarsh Singh,Kushal Raj Bhandari,Jianxi Gao,Soham Dan,Vivek Gupta*

Key words: 表格问答（TQA），级联检索，稀疏检索，密集检索，自然语言处理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为CRAFT的级联检索方法，用于表格问答（TQA），结合稀疏和密集检索模型以提高性能，并通过生成表格描述进一步优化结果。

Motivation: 传统密集检索模型在大规模检索任务中计算成本高且缺乏适应性，因此需要一种更高效且灵活的方法来提高表格问答的效果。

Method: 采用级联检索方法，先用稀疏检索模型筛选候选表格，再应用密集模型和神经重排器，并结合表格描述生成（使用Gemini Flash 1.5）优化表格表示。

Result: CRAFT在检索性能上优于现有的稀疏、密集和混合检索方法，并在NQ-Tables数据集上展现了有效性。

Conclusion: CRAFT提供了一种高效且适应性强的表格问答解决方案，显著提升了检索性能。

Abstract: Table Question Answering (TQA) involves retrieving relevant tables from a
large corpus to answer natural language queries. Traditional dense retrieval
models, such as DTR and ColBERT, not only incur high computational costs for
large-scale retrieval tasks but also require retraining or fine-tuning on new
datasets, limiting their adaptability to evolving domains and knowledge. In
this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that
first uses a sparse retrieval model to filter a subset of candidate tables
before applying more computationally expensive dense models and neural
re-rankers. Our approach achieves better retrieval performance than
state-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further
enhance table representations by generating table descriptions and titles using
Gemini Flash 1.5. End-to-end TQA results using various Large Language Models
(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate
$\textbf{CRAFT}$ effectiveness.

</details>


### [27] [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990)
*Ishika Agarwal,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Key words: 代码转换,语言特定知识,推理能力,文化特定数据集

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文探讨了语言特定知识（LSK）现象，发现模型在某些语言中表现更好，并提出LSKExtractor方法来利用这一现象。实验显示准确性平均提升10%。

Motivation: 研究人类代码转换现象，探索模型在不同语言中可能的知识差异及推理能力提升的可能性。

Method: 提出LSKExtractor方法，利用文化特定数据集评估和利用语言模型的LSK。

Result: 实验表明，模型在某些语言中推理性能更优，平均准确性提升10%。

Conclusion: 研究发现语言模型在特定语言中表现更佳，推动了更具包容性的模型开发。

Abstract: Code-switching is a common phenomenon of alternating between different
languages in the same utterance, thought, or conversation. We posit that humans
code-switch because they feel more comfortable talking about certain topics and
domains in one language than another. With the rise of knowledge-intensive
language models, we ask ourselves the next, natural question: Could models hold
more knowledge on some topics in some language X? More importantly, could we
improve reasoning by changing the language that reasoning is performed in? We
coin the term Language Specific Knowledge (LSK) to represent this phenomenon.
As ethnic cultures tend to develop alongside different languages, we employ
culture-specific datasets (that contain knowledge about cultural and social
behavioral norms). We find that language models can perform better when using
chain-of-thought reasoning in some languages other than English, sometimes even
better in low-resource languages. Paired with previous works showing that
semantic similarity does not equate to representational similarity, we
hypothesize that culturally specific texts occur more abundantly in
corresponding languages, enabling specific knowledge to occur only in specific
"expert" languages. Motivated by our initial results, we design a simple
methodology called LSKExtractor to benchmark the language-specific knowledge
present in a language model and, then, exploit it during inference. We show our
results on various models and datasets, showing an average relative improvement
of 10% in accuracy. Our research contributes to the open-source development of
language models that are inclusive and more aligned with the cultural and
linguistic contexts in which they are deployed.

</details>


### [28] [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/abs/2505.14992)
*Zhihao Wen,Sheng Liang,Yaxiong Wu,Yongyue Zhang,Yong Liu*

Key words: 信息提取,LLMs,Dual-LoRA,增量模式缓存

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为DLISC的两阶段信息提取方法，针对资源受限设备上的LLMs，通过Dual-LoRA和增量模式缓存提升效率和效果。

Motivation: 解决在资源受限设备上部署LLMs进行信息提取时的幻觉、上下文长度限制和高延迟问题。

Method: 采用Dual-LoRA（Identification LoRA和Extraction LoRA）和增量模式缓存（Incremental Schema Caching）。

Result: 在多数据集实验中，DLISC在效率和效果上均表现出显著提升。

Conclusion: DLISC是一种针对资源受限设备的高效信息提取方法，解决了现有挑战。

Abstract: Information extraction (IE) plays a crucial role in natural language
processing (NLP) by converting unstructured text into structured knowledge.
Deploying computationally intensive large language models (LLMs) on
resource-constrained devices for information extraction is challenging,
particularly due to issues like hallucinations, limited context length, and
high latency-especially when handling diverse extraction schemas. To address
these challenges, we propose a two-stage information extraction approach
adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching
(DLISC), which enhances both schema identification and schema-aware extraction
in terms of effectiveness and efficiency. In particular, DLISC adopts an
Identification LoRA module for retrieving the most relevant schemas to a given
query, and an Extraction LoRA module for performing information extraction
based on the previously selected schemas. To accelerate extraction inference,
Incremental Schema Caching is incorporated to reduce redundant computation,
substantially improving efficiency. Extensive experiments across multiple
information extraction datasets demonstrate notable improvements in both
effectiveness and efficiency.

</details>


### [29] [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)
*Zixuan Ke,Austin Xu,Yifei Ming,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty*

Key words: 多代理系统,大型语言模型,自监督,动态适应性,元级设计

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SELF-MAS是一种自监督、仅在推理时运行的自动多代理系统（MAS）设计框架，无需验证集，动态调整代理配置，显著优于现有方法。

Motivation: 当前多代理系统依赖人工设计角色和协议，难以适应新任务；现有自动方法需要验证集且缺乏动态适应性。

Method: 提出SELF-MAS框架，通过元级设计迭代生成、评估和优化MAS配置，支持动态代理组合和问题分解。

Result: 在多个任务和不同规模的LLM上，SELF-MAS平均准确率提升7.44%，成本效益高。

Conclusion: 元级自监督设计为构建高效、自适应MAS提供了新方向。

Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large
Language Models (LLMs) hold significant potential for tackling complex tasks.
However, most current MAS depend on manually designed agent roles and
communication protocols. These manual designs often fail to align with the
underlying LLMs' strengths and struggle to adapt to novel tasks. Recent
automatic MAS approaches attempt to mitigate these limitations but typically
necessitate a validation-set for tuning and yield static MAS designs lacking
adaptability during inference. We introduce SELF-MAS, the first
self-supervised, inference-time only framework for automatic MAS design.
SELF-MAS employs meta-level design to iteratively generate, evaluate, and
refine MAS configurations tailored to each problem instance, without requiring
a validation set. Critically, it enables dynamic agent composition and problem
decomposition through meta-feedback on solvability and completeness.
Experiments across math, graduate-level QA, and software engineering
benchmarks, using both closed-source and open-source LLM back-bones of varying
sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS
baselines, achieving a 7.44% average accuracy improvement over the next
strongest baseline while maintaining cost-efficiency. These findings underscore
the promise of meta-level self-supervised design for creating effective and
adaptive MAS.

</details>


### [30] [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/abs/2505.15000)
*Chengwei Wei,Bin Wang,Jung-jae Kim,Nancy F. Chen*

Key words: 大型语言模型,语音模型,数学推理,Spoken-MQA,评测基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种新的评测基准Spoken-MQA，用于评估语音模型在数学推理任务中的表现，发现现有模型在直接算术问题和口语化数学表达理解方面存在困难。

Motivation: 研究大型语言模型和多媒体语言模型在口语输入数学推理任务中的能力，填补现有研究空白。

Method: 引入Spoken-MQA评测基准，涵盖多种数学问题类型，通过实验对比级联模型与端到端语音模型的性能。

Result: 语音模型在基础算术任务中表现良好，但在直接算术问题和口语数学表达理解上表现较差，数学知识推理能力显著下降。

Conclusion: 当前语音模型在数学推理任务中仍存在局限，尤其是对口语化表达的适应性不足，需要进一步改进。

Abstract: Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs)
have led to strong reasoning ability across a wide range of tasks. However,
their ability to perform mathematical reasoning from spoken input remains
underexplored. Prior studies on speech modality have mostly focused on factual
speech understanding or simple audio reasoning tasks, providing limited insight
into logical step-by-step reasoning, such as that required for mathematical
problem solving. To address this gap, we introduce Spoken Math Question
Answering (Spoken-MQA), a new benchmark designed to evaluate the mathematical
reasoning capabilities of speech-based models, including both cascade models
(ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of
math problems, including pure arithmetic, single-step and multi-step contextual
reasoning, and knowledge-oriented reasoning problems, all presented in
unambiguous natural spoken language. Through extensive experiments, we find
that: (1) while some speech LLMs perform competitively on contextual reasoning
tasks involving basic arithmetic, they still struggle with direct arithmetic
problems; (2) current LLMs exhibit a strong bias toward symbolic mathematical
expressions written in LaTex and have difficulty interpreting verbalized
mathematical expressions; and (3) mathematical knowledge reasoning abilities
are significantly degraded in current speech LLMs.

</details>


### [31] [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/abs/2505.15024)
*Furong Jia,David Sontag,Monica Agrawal*

Key words: 大型语言模型, 临床自然语言处理, 临床术语, 医学声明, 预训练数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了开源大型语言模型（LLMs）如何通过学习大型语料库来理解临床术语和应对无支持的医学声明，发现语料库中临床术语的频率与模型表现相关，但存在与现实使用不匹配的问题。

Motivation: 尽管LLMs未直接基于电子健康记录（EHR）数据训练，但它们在临床自然语言处理任务中表现出色。研究旨在探索LLMs如何通过语料库学习临床信息，尤其是临床术语解释和对无支持医学声明的响应。

Method: 通过两个关键视角分析：（1）临床术语理解（基于新数据集MedLingo），（2）对无支持医学声明的响应。研究调查了相关临床信息在预训练语料库中的频率及其与模型输出的关系。

Result: 发现临床术语在预训练语料库中的出现频率与模型表现相关，但临床笔记中常见的术语在语料库中出现较少。此外，模型可能复述语料库中无支持的医学声明。

Conclusion: 研究揭示了预训练语料库与现实使用之间的不匹配，强调了未来数据集构建中在线来源分类的重要性。

Abstract: Large language models (LLMs) have performed well across various clinical
natural language processing tasks, despite not being directly trained on
electronic health record (EHR) data. In this work, we examine how popular
open-source LLMs learn clinical information from large mined corpora through
two crucial but understudied lenses: (1) their interpretation of clinical
jargon, a foundational ability for understanding real-world clinical notes, and
(2) their responses to unsupported medical claims. For both use cases, we
investigate the frequency of relevant clinical information in their
corresponding pretraining corpora, the relationship between pretraining data
composition and model outputs, and the sources underlying this data. To isolate
clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo.
Unsurprisingly, we find that the frequency of clinical jargon mentions across
major pretraining corpora correlates with model performance. However, jargon
frequently appearing in clinical notes often rarely appears in pretraining
corpora, revealing a mismatch between available data and real-world usage.
Similarly, we find that a non-negligible portion of documents support disputed
claims that can then be parroted by models. Finally, we classified and analyzed
the types of online sources in which clinical jargon and unsupported medical
claims appear, with implications for future dataset composition.

</details>


### [32] [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)
*Wenqing Wu,Haixu Xi,Chengzhi Zhang*

Key words: peer review, confidence scores, deep learning, NLP, text-score consistency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文通过深度学习和NLP技术，分析了AI会议评审中文本与置信度得分的一致性，发现高置信度得分与论文拒稿相关，验证了评审的公平性。

Motivation: 现有研究缺乏对评审文本与置信度得分一致性的细粒度分析，可能遗漏关键细节，因此需要深入评估。

Method: 使用深度学习检测含糊句子和方面，分析报告长度、含糊词句频率、方面提及和情感，以评估文本与得分的一致性。

Result: 结果显示各个层面的文本与得分一致性高，回归分析表明高置信度得分与论文拒稿相关。

Conclusion: 研究验证了专家评估的可靠性和同行评审的公平性。

Abstract: Peer review is vital in academia for evaluating research quality. Top AI
conferences use reviewer confidence scores to ensure review reliability, but
existing studies lack fine-grained analysis of text-score consistency,
potentially missing key details. This work assesses consistency at word,
sentence, and aspect levels using deep learning and NLP conference review data.
We employ deep learning to detect hedge sentences and aspects, then analyze
report length, hedge word/sentence frequency, aspect mentions, and sentiment to
evaluate text-score alignment. Correlation, significance, and regression tests
examine confidence scores' impact on paper outcomes. Results show high
text-score consistency across all levels, with regression revealing higher
confidence scores correlate with paper rejection, validating expert assessments
and peer review fairness.

</details>


### [33] [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
*Haiyan Zhao,Xuansheng Wu,Fan Yang,Bo Shen,Ninghao Liu,Mengnan Du*

Key words: 线性概念向量、稀疏自编码器、噪声滤波、大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为SDCV的方法，通过稀疏自编码器从隐藏表征中去除噪声特征，从而提升线性概念向量的稳健性。

Motivation: 现有方法（如线性探测和均值差异）在从大语言模型（LLM）隐藏表征中提取线性概念向量时，受数据多样性引入的噪声影响，导致鲁棒性不足。

Method: 提出SDCV方法，利用稀疏自编码器过滤隐藏表征中的噪声特征，并在线性探测和均值差异中应用。

Result: 实验表明，SDCV提高了线性探测和均值差异方法的操控成功率。

Conclusion: 通过反事实实验和特征可视化验证了噪声假设，SDCV方法显著提升了线性概念向量的表现。

Abstract: Linear Concept Vectors have proven effective for steering large language
models (LLMs). While existing approaches like linear probing and
difference-in-means derive these vectors from LLM hidden representations,
diverse data introduces noises (i.e., irrelevant features) that challenge
steering robustness. To address this, we propose Sparse Autoencoder-Denoised
Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy
features from hidden representations. When applied to linear probing and
difference-in-means, our method improves their steering success rates. We
validate our noise hypothesis through counterfactual experiments and feature
visualizations.

</details>


### [34] [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/abs/2505.15045)
*Siyue Zhang,Yilun Zhao,Liyuan Geng,Arman Cohan,Anh Tuan Luu,Chen Zhao*

Key words: 大语言模型, 扩散语言模型, 文本嵌入, 双向注意力, 文档检索

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出使用扩散语言模型（Diffusion Language Model）来改进基于大语言模型（LLM）的文本嵌入方法，解决了LLM因单向注意力机制在双向文本嵌入任务中的不匹配问题。实验证明，该方法在多项任务中表现优异。

Motivation: LLM的嵌入模型在通用文本嵌入任务中表现优异，但其单向注意力机制与双向文本嵌入任务不匹配，限制了性能。扩散语言模型因其双向架构和在推理任务中的成功，成为改进方向。

Method: 研究首次系统地探讨了扩散语言嵌入模型，通过其固有的双向注意力机制优化文本嵌入任务。

Result: 该方法在长文本检索、推理密集型检索和指令跟随检索任务中分别优于LLM模型20%、8%和2%，并在传统文本嵌入基准测试中表现竞争性。

Conclusion: 双向注意力机制在编码长文本和复杂文本的全局上下文时至关重要。

Abstract: Large language model (LLM)-based embedding models, benefiting from large
scale pre-training and post-training, have begun to surpass BERT and T5-based
models on general-purpose text embedding tasks such as document retrieval.
However, a fundamental limitation of LLM embeddings lies in the unidirectional
attention used during autoregressive pre-training, which misaligns with the
bidirectional nature of text embedding tasks. To this end, We propose adopting
diffusion language models for text embeddings, motivated by their inherent
bidirectional architecture and recent success in matching or surpassing LLMs
especially on reasoning tasks. We present the first systematic study of the
diffusion language embedding model, which outperforms the LLM-based embedding
model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,
2% on instruction-following retrieval, and achieve competitive performance on
traditional text embedding benchmarks. Our analysis verifies that bidirectional
attention is crucial for encoding global context in long and complex text.

</details>


### [35] [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
*Yifan Wu,Lutao Yan,Leixian Shen,Yinan Mei,Jiannan Wang,Yuyu Luo*

Key words: 多模态大语言模型、图表理解、元数据生成、数据集构建、任务多样性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了ChartCards框架，通过生成结构化元数据支持多任务图表理解，并构建了MetaChart数据集，显著提高了模型性能。

Motivation: 多模态大语言模型在图表理解中的应用需要大量高质量数据集，但数据收集和训练成本高，因此提出了ChartCards框架以降低成本并提升效率。

Method: 提出了ChartCards框架，通过系统性合成图表信息（如数据表、可视化代码、视觉元素等）并生成统一元数据，支持多任务图表理解。还构建了MetaChart数据集，包含10,862张数据表和85K图表。

Result: 在六种模型上微调MetaChart数据集，平均性能提升5%，其中文本到图表检索和图表到表格任务的提升最为显著，分别达到17%和28%。

Conclusion: ChartCards框架和MetaChart数据集有效降低了数据成本并提升了多任务图表理解的性能。

Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new
opportunities for chart understanding. However, due to the fine-grained nature
of these tasks, applying MLLMs typically requires large, high-quality datasets
for task-specific fine-tuning, leading to high data collection and training
costs. To address this, we propose ChartCards, a unified chart-metadata
generation framework for multi-task chart understanding. ChartCards
systematically synthesizes various chart information, including data tables,
visualization code, visual elements, and multi-dimensional semantic captions.
By structuring this information into organized metadata, ChartCards enables a
single chart to support multiple downstream tasks, such as text-to-chart
retrieval, chart summarization, chart-to-table conversion, chart description,
and chart question answering. Using ChartCards, we further construct MetaChart,
a large-scale high-quality dataset containing 10,862 data tables, 85K charts,
and 170 K high-quality chart captions. We validate the dataset through
qualitative crowdsourcing evaluations and quantitative fine-tuning experiments
across various chart understanding tasks. Fine-tuning six different models on
MetaChart resulted in an average performance improvement of 5% across all
tasks. The most notable improvements are seen in text-to-chart retrieval and
chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements
of 17% and 28%, respectively.

</details>


### [36] [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)
*Gaurav Kumar,Debajyoti Mazumder,Ayush Garg,Jasabanta Patro*

Key words: 自动化事实核查, 语言模型, 证据推理, 微调策略

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于语言模型生成支持与反驳理由的自动化事实核查方法，显著优于现有基线。

Motivation: 解决现有自动化事实核查方法依赖语言模型嵌入知识或证据微调的不足，后者效果不佳。

Method: 利用语言模型的生成能力产生支持与反驳理由，并通过这些理由训练模型，同时对比不同提示与微调策略。

Result: 新方法在RAW-FC和LIAR-RAW数据集上分别提升了16.39%和44.26%的性能。

Conclusion: 该方法简单有效，为自动化事实核查提供了新思路。

Abstract: Automated fact-checking is a crucial task in this digital age. To verify a
claim, current approaches majorly follow one of two strategies i.e. (i) relying
on embedded knowledge of language models, and (ii) fine-tuning them with
evidence pieces. While the former can make systems to hallucinate, the later
have not been very successful till date. The primary reason behind this is that
fact verification is a complex process. Language models have to parse through
multiple pieces of evidence before making a prediction. Further, the evidence
pieces often contradict each other. This makes the reasoning process even more
complex. We proposed a simple yet effective approach where we relied on
entailment and the generative ability of language models to produce
''supporting'' and ''refuting'' justifications (for the truthfulness of a
claim). We trained language models based on these justifications and achieved
superior results. Apart from that, we did a systematic comparison of different
prompting and fine-tuning strategies, as it is currently lacking in the
literature. Some of our observations are: (i) training language models with raw
evidence sentences registered an improvement up to 8.20% in macro-F1, over the
best performing baseline for the RAW-FC dataset, (ii) similarly, training
language models with prompted claim-evidence understanding (TBE-2) registered
an improvement (with a margin up to 16.39%) over the baselines for the same
dataset, (iii) training language models with entailed justifications (TBE-3)
outperformed the baselines by a huge margin (up to 28.57% and 44.26% for
LIAR-RAW and RAW-FC, respectively). We have shared our code repository to
reproduce the results.

</details>


### [37] [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
*Feiyang Cai,Jiahui Bai,Tao Tang,Joshua Luo,Tianyu Zhu,Ling Liu,Feng Luo*

Key words: MolLangBench, 分子识别, 分子编辑, 分子生成, AI评估, 化学任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MolLangBench是一个用于评估分子-语言界面任务的基准，包括分子结构识别、编辑和生成。尽管当前最先进模型的表现在人类看来仍显不足，但该基准希望推动更有效且可靠的化学AI系统研究。

Motivation: 由于分子识别、编辑和生成对化学家和AI系统至关重要，但当前AI系统在此类任务上的表现不佳，因此需要开发一个高质量基准来推动研究。

Method: MolLangBench通过自动化工具和专家标注构建任务，评估语言与分子表示（如字符串、图像、图）的交互能力。

Result: 最先进的模型在识别和编辑任务中表现尚可（约79%），但在生成任务中仅为29%，显示AI系统的局限性。

Conclusion: MolLangBench揭示了当前AI系统的不足，并有望促进化学应用领域更有效的AI研究。

Abstract: Precise recognition, editing, and generation of molecules are essential
prerequisites for both chemists and AI systems tackling various chemical tasks.
We present MolLangBench, a comprehensive benchmark designed to evaluate
fundamental molecule-language interface tasks: language-prompted molecular
structure recognition, editing, and generation. To ensure high-quality,
unambiguous, and deterministic outputs, we construct the recognition tasks
using automated cheminformatics tools, and curate editing and generation tasks
through rigorous expert annotation and validation. MolLangBench supports the
evaluation of models that interface language with different molecular
representations, including linear strings, molecular images, and molecular
graphs. Evaluations of state-of-the-art models reveal significant limitations:
the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition
and editing tasks, which are intuitively simple for humans, and performs even
worse on the generation task, reaching only $29.0\%$ accuracy. These results
highlight the shortcomings of current AI systems in handling even preliminary
molecular recognition and manipulation tasks. We hope MolLangBench will
catalyze further research toward more effective and reliable AI systems for
chemical applications.

</details>


### [38] [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)
*Hongli Zhou,Hui Huang,Ziqing Zhao,Lvyuan Han,Huicheng Wang,Kehai Chen,Muyun Yang,Wei Bao,Jian Dong,Bing Xu,Conghui Zhu,Hailong Cao,Tiejun Zhao*

Key words: 大型语言模型, 基准测试, IRT, 测量质量, 人类偏好

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 对主流大型语言模型（LLM）基准测试的有效性进行批判性分析，并提出一种新框架PSN-IRT以改进基准测试的准确性和可靠性。

Motivation: 当前基准测试在评估大型语言模型时存在不一致性和区分度不足的问题，无法真实反映模型能力。

Method: 提出Pseudo-Siamese Network for Item Response Theory (PSN-IRT)框架，通过丰富项目参数增强IRT架构，进而分析基准测试的测量质量。

Result: PSN-IRT揭示了当前基准测试的显著缺陷，并能构建更小的基准测试，同时更符合人类偏好。

Conclusion: PSN-IRT为改进基准测试提供了一种更准确和可靠的方法。

Abstract: The evaluation of large language models (LLMs) via benchmarks is widespread,
yet inconsistencies between different leaderboards and poor separability among
top models raise concerns about their ability to accurately reflect authentic
model capabilities. This paper provides a critical analysis of benchmark
effectiveness, examining main-stream prominent LLM benchmarks using results
from diverse models. We first propose a new framework for accurate and reliable
estimations of item characteristics and model abilities. Specifically, we
propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced
Item Response Theory framework that incorporates a rich set of item parameters
within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive
analysis which reveals significant and varied shortcomings in the measurement
quality of current benchmarks. Furthermore, we demonstrate that leveraging
PSN-IRT is able to construct smaller benchmarks while maintaining stronger
alignment with human preference.

</details>


### [39] [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
*Jiashu He,Jinxuan Fan,Bowen Jiang,Ignacio Houine,Dan Roth,Alejandro Ribeiro*

Key words: Self-GIVE, LLM, 知识图谱, 强化学习, 生物医学QA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Self-GIVE是一种基于检索-强化学习的框架，通过自动关联思维增强LLM，解决了GIVE在效率、通用性和知识准确性方面的限制。

Motivation: 解决现有GIVE方法在大型语言模型中因知识图构建和修剪效率低下、通用性不足以及小规模模型部署困难的问题。

Method: 提出Self-GIVE框架，结合检索和强化学习，自动提取结构化信息以辅助推理，减少LLM调用和令牌开销。

Result: 在3B和7B模型上性能提升显著（最高28.5%→71.4%和78.6→90.5%），且7B模型表现媲美GPT3.5，同时令牌使用减少90%以上。

Conclusion: Self-GIVE有效提升了结构化检索与关联推理的可扩展性。

Abstract: When addressing complex questions that require new information, people often
associate the question with existing knowledge to derive a sensible answer. For
instance, when evaluating whether melatonin aids insomnia, one might associate
"hormones helping mental disorders" with "melatonin being a hormone and
insomnia a mental disorder" to complete the reasoning. Large Language Models
(LLMs) also require such associative thinking, particularly in resolving
scientific inquiries when retrieved knowledge is insufficient and does not
directly answer the question. Graph Inspired Veracity Extrapolation (GIVE)
addresses this by using a knowledge graph (KG) to extrapolate structured
knowledge. However, it involves the construction and pruning of many
hypothetical triplets, which limits efficiency and generalizability. We propose
Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic
associative thinking through reinforcement learning. Self-GIVE extracts
structured information and entity sets to assist the model in linking to the
queried concepts. We address GIVE's key limitations: (1) extensive LLM calls
and token overhead for knowledge extrapolation, (2) difficulty in deploying on
smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate
knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE
with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B
models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and
$\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging
biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or
outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%.
Self-GIVE enhances the scalable integration of structured retrieval and
reasoning with associative thinking.

</details>


### [40] [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/abs/2505.15063)
*Sarfraz Ahmad,Hasan Iqbal,Momina Ahsan,Numaan Naeem,Muhammad Ahsan Riaz Khan,Arham Riaz,Muhammad Arslan Manzoor,Yuxia Wang,Preslav Nakov*

Key words: 事实核查, 乌尔都语, 低资源语言, 大语言模型, 证据检索

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了首个专门针对乌尔都语的事实核查框架UrduFactCheck，填补了低资源语言事实核查的空白，并在多指标上优于基线方法。

Motivation: 由于大语言模型（LLMs）在乌尔都语等低资源语言中输出的事实可靠性问题，现有自动化事实核查工具主要集中于英语，缺乏针对乌尔都语的解决方案。

Method: 开发了一个模块化的事实核查框架UrduFactCheck，采用动态多策略证据检索管道，结合单语和基于翻译的方法，以解决乌尔都语高质量证据稀缺的问题。

Result: 实验表明，UrduFactCheck（尤其是其翻译增强变体）在多指标上优于基线和开源替代方案。同时，对12种SOTA LLMs在乌尔都语事实问答上的表现进行了评估。

Conclusion: UrduFactCheck为低资源语言的事实核查提供了有效工具，突出了专有和开源LLMs之间的差距。代码和数据集已开源。

Abstract: The rapid use of large language models (LLMs) has raised critical concerns
regarding the factual reliability of their outputs, especially in low-resource
languages such as Urdu. Existing automated fact-checking solutions
overwhelmingly focus on English, leaving a significant gap for the 200+ million
Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first
comprehensive, modular fact-checking framework specifically tailored for Urdu.
Our system features a dynamic, multi-strategy evidence retrieval pipeline that
combines monolingual and translation-based approaches to address the scarcity
of high-quality Urdu evidence. We curate and release two new hand-annotated
benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating
LLM factuality. Extensive experiments demonstrate that UrduFactCheck,
particularly its translation-augmented variants, consistently outperforms
baselines and open-source alternatives on multiple metrics. We further
benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in
Urdu, highlighting persistent gaps between proprietary and open-source models.
UrduFactCheck's code and datasets are open-sourced and publicly available at
https://github.com/mbzuai-nlp/UrduFactCheck.

</details>


### [41] [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
*Suhas BN,Yash Mahajan,Dominik Mattioli,Andrew M. Sherrill,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Key words: 小型语言模型、PTSD、共情对话、TIDE数据集、微调

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，小型语言模型（0.5B至5B参数）可通过微调在创伤知情、共情对话中表现更好，但效果受场景和用户影响，存在共情上限。

Motivation: 探讨小型语言模型是否能在PTSD患者的共情对话中发挥作用，并提供一个高质量的对话数据集。

Method: 使用TIDE数据集（10,000条对话）微调8个小型模型，并与前沿模型（Claude Sonnet 3.5）对比，通过人类评估和自动指标进行分析。

Result: 微调通常能提升共情效果，但效果因场景和用户而异；小型模型存在共情上限，不同用户群体对共情表达有不同偏好。

Conclusion: 研究为开发安全、高效且符合伦理的共情AI奠定了基础，但仍需上下文和用户感知的系统设计。

Abstract: Can small language models with 0.5B to 5B parameters meaningfully engage in
trauma-informed, empathetic dialogue for individuals with PTSD? We address this
question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning
500 diverse PTSD client personas and grounded in a three-factor empathy model:
emotion recognition, distress normalization, and supportive reflection. All
scenarios and reference responses were reviewed for realism and trauma
sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight
small language models before and after fine-tuning, comparing their outputs to
a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and
automatic metrics show that fine-tuning generally improves perceived empathy,
but gains are highly scenario- and user-dependent, with smaller models facing
an empathy ceiling. Demographic analysis shows older adults value distress
validation and graduate-educated users prefer nuanced replies, while gender
effects are minimal. We highlight the limitations of automatic metrics and the
need for context- and user-aware system design. Our findings, along with the
planned release of TIDE, provide a foundation for building safe,
resource-efficient, and ethically sound empathetic AI to supplement, not
replace, clinical mental health care.

</details>


### [42] [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/abs/2505.15069)
*Pratik Rakesh Singh,Kritarth Prasad,Mohammadi Zaki,Pankaj Wasnik*

Key words: 神经机器翻译,低资源语言,领域适应,bandit算法,模型选择

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了在低资源语言神经机器翻译（NMT）中，如何利用基于**bandit的算法**选择最佳模型，以解决领域适应任务中的数据不足和模型泛化问题，并在非洲语言中验证了方法的有效性。

Motivation: NMT系统在处理低资源语言领域适应任务时，由于训练数据有限和模型泛化能力不足，面临性能挑战。因此，在无法进行微调的情况下，选择最佳翻译模型至关重要。

Method: 采用包括**Upper Confidence Bound、Linear UCB、Neural Linear Bandit和Thompson Sampling**在内的bandit-based算法，以高效选择最适合特定领域的NMT模型。

Result: 在三种非洲语言和不同领域下的实验表明，该方法在目标数据存在或缺失的情况下均表现出鲁棒性和高效性。

Conclusion: 基于bandit的算法能够有效应对资源限制，为低资源语言的NMT模型选择提供了高置信度的解决方案。

Abstract: Neural Machine Translation (NMT) systems face significant challenges when
working with low-resource languages, particularly in domain adaptation tasks.
These difficulties arise due to limited training data and suboptimal model
generalization, As a result, selecting an optimal model for translation is
crucial for achieving strong performance on in-domain data, particularly in
scenarios where fine-tuning is not feasible or practical. In this paper, we
investigate strategies for selecting the most suitable NMT model for a given
domain using bandit-based algorithms, including Upper Confidence Bound, Linear
UCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively
addresses the resource constraints by facilitating optimal model selection with
high confidence. We evaluate the approach across three African languages and
domains, demonstrating its robustness and effectiveness in both scenarios where
target data is available and where it is absent.

</details>


### [43] [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/abs/2505.15071)
*Chen Huang,Junkai Luo,Xinzuo Wang,Wenqiang Lei,Jiancheng Lv*

Key words: LLMs, 网络流行语, 数据集, 定义生成, RESS

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了利用大型语言模型（LLMs）基于中文社交媒体UGC生成网络流行语定义的能力，并提出了新方法RESS。

Motivation: 通过中文社交媒体的大规模UGC研究网络流行语，探索LLMs生成准确定义的潜力。

Method: 引入首个中文网络流行语数据集CHEER，并提出新方法RESS以指导LLMs理解过程。

Result: RESS在定义生成任务中表现优于现有方法，但也揭示了LLMs的共性问题。

Conclusion: 研究为LLMs定义生成领域的未来进步奠定了基础。

Abstract: The massive user-generated content (UGC) available in Chinese social media is
giving rise to the possibility of studying internet buzzwords. In this paper,
we study if large language models (LLMs) can generate accurate definitions for
these buzzwords based on UGC as examples. Our work serves a threefold
contribution. First, we introduce CHEER, the first dataset of Chinese internet
buzzwords, each annotated with a definition and relevant UGC. Second, we
propose a novel method, called RESS, to effectively steer the comprehending
process of LLMs to produce more accurate buzzword definitions, mirroring the
skills of human language learning. Third, with CHEER, we benchmark the
strengths and weaknesses of various off-the-shelf definition generation methods
and our RESS. Our benchmark demonstrates the effectiveness of RESS while
revealing crucial shared challenges: over-reliance on prior exposure,
underdeveloped inferential abilities, and difficulty identifying high-quality
UGC to facilitate comprehension. We believe our work lays the groundwork for
future advancements in LLM-based definition generation. Our dataset and code
are available at https://github.com/SCUNLP/Buzzword.

</details>


### [44] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
*Yuhang Zhou,Jing Zhu,Shengyi Qian,Zhuokai Zhao,Xiyao Wang,Xiaoyu Liu,Ming Li,Paiheng Xu,Wei Ai,Furong Huang*

Key words: 大型语言模型, 强化学习, 人类反馈, 数据不平衡, 公平性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DISCO是一种改进GRPO的方法，通过域感知和难度感知的奖励缩放，解决了数据不平衡问题，提升了泛化性和公平性。

Motivation: GRPO在现实数据中因假设均衡分布和统一语义对齐而表现不佳，尤其在多域不平衡数据中，倾向于主导域，忽视小众域。

Method: 提出DISCO方法，包含域感知奖励缩放和难度感知奖励缩放，前者根据域频率重新加权优化，后者利用提示级自一致性优先学习高价值不确定提示。

Result: 在多个LLM和倾斜训练分布上的实验显示，DISCO将Qwen3模型的性能提升了5%，在多域对齐基准测试中创造了新记录。

Conclusion: DISCO通过创新的奖励策略有效解决了GRPO在数据不平衡和公平性上的不足。

Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.

</details>


### [45] [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
*Hao Wang,Pinzhi Huang,Jihan Yang,Saining Xie,Daisuke Kawahara*

Key words: 多模态大语言模型、跨语言一致性、文化知识、基准测试、视觉问答

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文介绍KnowRecall和VisRecall两个新基准，评估多模态大语言模型在多语言和文化知识一致性上的表现，发现现有模型仍需改进。

Motivation: 尽管多模态大语言模型在实际应用中表现优异，但在多语言和跨文化知识一致性方面仍存在挑战，论文旨在解决这一问题。

Method: 提出KnowRecall和VisRecall两个新基准，分别评估模型在15种语言的视觉问答和9种语言的视觉记忆一致性。

Result: 实验表明，当前的先进模型在多语言一致性上仍有不足，尤其是文化历史知识和视觉记忆方面。

Conclusion: 论文强调需要更鲁棒的方法，以开发真正多语言且文化感知的模型。

Abstract: The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.

</details>


### [46] [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/abs/2505.15087)
*Zhiyu Shen,Jiyuan Liu,Yunhe Pang,Yanghui Rao*

Key words: 多跳问答,自动合成框架,无监督学习,桥接问题,比较问题

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: HopWeaver是一种自动框架，用于从未结构化文本语料库中合成真实的多跳问题，无需人工干预，其生成的问题质量与人工标注数据集相当或更高，且成本更低。

Motivation: 解决手动标注多跳问题数据集的高成本问题以及现有合成方法生成的问题过于简单或需要大量人工指导的局限性。

Method: HopWeaver通过识别语料库中的互补文档，构建连贯的推理路径，自动合成两种类型（桥接和比较）的多跳问题。

Result: 实证评估显示，合成的问题质量与人工标注数据集相当或更高，且成本显著降低。

Conclusion: HopWeaver为缺乏标注资源的专业领域开发多跳问题数据集提供了有价值的解决方案。

Abstract: Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's
capability to integrate information from diverse sources. However, creating
extensive and high-quality MHQA datasets is challenging: (i) manual annotation
is expensive, and (ii) current synthesis methods often produce simplistic
questions or require extensive manual guidance. This paper introduces
HopWeaver, the first automatic framework synthesizing authentic multi-hop
questions from unstructured text corpora without human intervention. HopWeaver
synthesizes two types of multi-hop questions (bridge and comparison) using an
innovative approach that identifies complementary documents across corpora. Its
coherent pipeline constructs authentic reasoning paths that integrate
information across multiple documents, ensuring synthesized questions
necessitate authentic multi-hop reasoning. We further present a comprehensive
system for evaluating synthesized multi-hop questions. Empirical evaluations
demonstrate that the synthesized questions achieve comparable or superior
quality to human-annotated datasets at a lower cost. Our approach is valuable
for developing MHQA datasets in specialized domains with scarce annotated
resources. The code for HopWeaver is publicly available.

</details>


### [47] [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)
*Sona Elza Simon,Preethi Jyothi*

Key words: 跨语言迁移,稀疏微调,奇异值分解,低资源语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DeFT-X是一种新的可组合稀疏微调方法，通过去噪和奇异值分解改进跨语言迁移性能。

Motivation: 解决高资源语言到低资源语言的任务迁移问题，尤其是对极低资源语言的跨语言迁移。

Method: 使用奇异值分解对预训练模型权重矩阵去噪，再进行稀疏微调，生成更鲁棒的任务和语言特定稀疏掩码。

Result: 在极低资源语言的情感分类和自然语言推理任务中，DeFT-X表现优于或与SFT及其他基线方法相当。

Conclusion: DeFT-X通过去噪和稀疏微调显著提升了跨语言迁移的效果。

Abstract: Effective cross-lingual transfer remains a critical challenge in scaling the
benefits of large language models from high-resource to low-resource languages.
Towards this goal, prior studies have explored many approaches to combine task
knowledge from task-specific data in a (high-resource) source language and
language knowledge from unlabeled text in a (low-resource) target language. One
notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual
transfer that learns task-specific and language-specific sparse masks to select
a subset of the pretrained model's parameters that are further fine-tuned.
These sparse fine-tuned vectors (SFTs) are subsequently composed with the
pretrained model to facilitate zero-shot cross-lingual transfer to a task in a
target language, using only task-specific data from a source language. These
sparse masks for SFTs were identified using a simple magnitude-based pruning.
In our work, we introduce DeFT-X, a novel composable SFT approach that denoises
the weight matrices of a pretrained model before magnitude pruning using
singular value decomposition, thus yielding more robust SFTs. We evaluate
DeFT-X on a diverse set of extremely low-resource languages for sentiment
classification (NusaX) and natural language inference (AmericasNLI) and
demonstrate that it performs at par or outperforms SFT and other prominent
cross-lingual transfer baselines.

</details>


### [48] [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/abs/2505.15094)
*Jing Yu,Yuqi Tang,Kehua Feng,Mingyang Rao,Lei Liang,Zhiqiang Zhang,Mengshu Sun,Wen Zhang,Qiang Zhang,Keyan Ding,Huajun Chen*

Key words: 大型语言模型,科学评估,基准测试,上下文理解,多模态数据

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SciCUEval是专为评估大语言模型在科学领域上下文理解能力而构建的综合性基准数据集，涵盖多个科学子领域和数据类型，系统地评估核心能力，并为未来发展提供洞见。

Motivation: 当前基准测试主要关注通用领域，未能充分捕捉科学数据的复杂性，因此需要针对科学领域设计更全面的评估工具。

Method: 构建SciCUEval数据集，包含十个特定领域的子数据集，整合多种数据模态，通过多样化问题格式评估四项核心能力。

Result: 对大语言模型在SciCUEval上的广泛评估揭示了其在科学上下文理解中的优势和局限性。

Conclusion: SciCUEval为大语言模型在科学领域的未来发展提供了宝贵的评估框架和洞见。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in contextual
understanding and reasoning. However, evaluating their performance across
diverse scientific domains remains underexplored, as existing benchmarks
primarily focus on general domains and fail to capture the intricate complexity
of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive
benchmark dataset tailored to assess the scientific context understanding
capability of LLMs. It comprises ten domain-specific sub-datasets spanning
biology, chemistry, physics, biomedicine, and materials science, integrating
diverse data modalities including structured tables, knowledge graphs, and
unstructured texts. SciCUEval systematically evaluates four core competencies:
Relevant information identification, Information-absence detection,
Multi-source information integration, and Context-aware inference, through a
variety of question formats. We conduct extensive evaluations of
state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their
strengths and limitations in scientific context understanding, and offering
valuable insights for the future development of scientific-domain LLMs.

</details>


### [49] [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
*Ishmanbir Singh,Dipankar Srirag,Aditya Joshi*

Key words: 讽刺检测，PMP，可解释性，英语变体，大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种基于认知启发技术PMP的可解释讽刺检测方法，用于澳大利亚和印度英语，并在开源大型语言模型上验证了其有效性。

Motivation: 讽刺因表面与隐含情感的不一致而对情感分析构成挑战，尤其在涉及特定国家或地区时更加复杂。

Method: 利用PMP技术为澳大利亚和印度英语数据集BESSTIE手动添加讽刺解释，并与标准英语数据集FLUTE进行比较。

Result: 在GEMMA和LLAMA模型上，PMP方法在所有任务和数据集中均显著优于其他四种提示策略。

Conclusion: PMP能有效生成英语变体的讽刺解释，且外部知识检索可缓解上下文相关错误。

Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.

</details>


### [50] [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
*Aryaman Arora,Neil Rathi,Nikil Roashan Selvam,Róbert Csórdas,Dan Jurafsky,Christopher Potts*

Key words: 状态空间模型, 语言建模, 关联召回, Transformer, 因果干预

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究探讨了状态空间模型（SSMs）在语言建模中的性能问题，发现Transformer和Based SSM模型在关联召回（AR）任务中表现最佳，而其他SSM模型如H3和Hyena则失败。通过因果干预，揭示了成功模型通过归纳头存储键值关联，而失败模型仅依赖最后状态。研究发现不同架构即使准确率相近，机制差异显著。

Motivation: 研究动机在于揭示不同SSM架构在语言建模任务中的性能差异及其背后的机制，特别是对关联召回任务（AR）的能力。

Method: 研究方法包括在AR任务上的实验和因果干预分析，并引入了一个基于PCFG诱导的新任务——关联树召回（ATR）。

Result: 结果显示，仅Transformer和Based SSM模型能完全成功完成AR任务，而Mamba稍逊，其他SSM模型失败。机制上，成功模型通过归纳头存储键值关联，而失败模型仅依赖最后状态。

Conclusion: 结论指出，不同架构即使准确率相近，其机制可能有显著差异，强调了机制评估的重要性。

Abstract: State space models (SSMs) for language modelling promise an efficient and
performant alternative to quadratic-attention Transformers, yet show variable
performance on recalling basic information from the context. While performance
on synthetic tasks like Associative Recall (AR) can point to this deficiency,
behavioural metrics provide little information as to why--on a mechanistic
level--certain architectures fail and others succeed. To address this, we
conduct experiments on AR and find that only Transformers and Based SSM models
fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,
Hyena) fail. We then use causal interventions to explain why. We find that
Transformers and Based learn to store key-value associations in-context using
induction heads. By contrast, the SSMs compute these associations only at the
last state, with only Mamba succeeding because of its short convolution
component. To extend and deepen these findings, we introduce Associative
Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR
introduces language-like hierarchical structure into the AR setting. We find
that all architectures learn the same mechanism as they did for AR, and the
same three models succeed at the task. These results reveal that architectures
with similar accuracy may still have substantive differences, motivating the
adoption of mechanistic evaluations.

</details>


### [51] [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
*Ziliang Wang,Xuhui Zheng,Kang An,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Key words: 多跳推理, 强化学习, 步进式优化, 信息增益, 细粒度监督

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出StepSearch框架，通过步进式策略优化和细粒度奖励机制改进多跳推理任务中的搜索性能，显著超越基线方法。

Motivation: 现有基于强化学习的搜索方法在复杂多跳问答中表现不佳，主要因为全局稀疏奖励无法有效指导每一步搜索。

Method: 采用步进式近端策略优化方法，结合信息增益和冗余惩罚的中间奖励机制，构建细粒度搜索轨迹数据集。

Result: 在标准多跳问答基准测试中，3B和7B模型分别实现11.2%和4.2%的绝对提升。

Conclusion: 细粒度步进式监督能有效优化深度搜索LLMs，且在小规模训练数据下表现优异。

Abstract: Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our
implementation is publicly available at
https://github.com/zxh20001117/StepSearch.

</details>


### [52] [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)
*Ian Steenstra,Timothy W. Bickmore*

Key words: 大型语言模型, 心理治疗, 风险评估, 分类法, 人工智能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍了一种针对AI心理治疗师的新型风险分类法，旨在通过系统性评估减少潜在危害。

Motivation: 大型语言模型和智能虚拟代理作为心理治疗师的普及缺乏标准化评估方法，可能导致用户伤害和自杀等严重后果。

Method: 通过文献综述、专家访谈并结合临床标准（如DSM-5）和现有评估工具（如NEQ、UE-ATR）开发的分类法。

Result: 提出了一个结构化的风险分类法，适用于人类-AI会话和AI自动化评估场景，以检测风险因素。

Conclusion: 该分类法为AI驱动的心理健康支持提供了更安全和负责任的创新基础。

Abstract: The proliferation of Large Language Models (LLMs) and Intelligent Virtual
Agents acting as psychotherapists presents significant opportunities for
expanding mental healthcare access. However, their deployment has also been
linked to serious adverse outcomes, including user harm and suicide,
facilitated by a lack of standardized evaluation methodologies capable of
capturing the nuanced risks of therapeutic interaction. Current evaluation
techniques lack the sensitivity to detect subtle changes in patient cognition
and behavior during therapy sessions that may lead to subsequent
decompensation. We introduce a novel risk taxonomy specifically designed for
the systematic evaluation of conversational AI psychotherapists. Developed
through an iterative process including review of the psychotherapy risk
literature, qualitative interviews with clinical and legal experts, and
alignment with established clinical criteria (e.g., DSM-5) and existing
assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured
approach to identifying and assessing user/patient harms. We provide a
high-level overview of this taxonomy, detailing its grounding, and discuss
potential use cases. We discuss two use cases in detail: monitoring cognitive
model-based risk factors during a counseling conversation to detect unsafe
deviations, in both human-AI counseling sessions and in automated benchmarking
of AI psychotherapists with simulated patients. The proposed taxonomy offers a
foundational step towards establishing safer and more responsible innovation in
the domain of AI-driven mental health support.

</details>


### [53] [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/abs/2505.15110)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Key words: 表格推理, Row-of-Thought, 幻觉减少, 训练免费方法, 状态最先进

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种称为Row-of-Thought（RoT）的训练免费方法，通过逐行遍历表格并结合反思能力，显著提升了表格推理任务的性能，同时减少了幻觉内容。

Motivation: 现有的Long Chain-of-Thought（Long CoT）方法虽然在表格推理任务中表现优异，但训练成本高且存在表格内容幻觉问题，因此需要一种更高效可靠的方法。

Method: RoT通过迭代的逐行表格遍历，允许在每次遍历中进行推理扩展和基于反思的优化，利用大语言模型的反思能力，无需额外训练。

Result: 实验表明，RoT在非推理模型上的表现平均优于RLLMs达4.3%，并在WikiTableQuestions和TableBench上达到了最先进的结果。此外，RoT的推理令牌数少于Long CoT，效率更高。

Conclusion: RoT通过逐行遍历和反思机制，不仅降低了内容幻觉的风险，还在性能和效率上超越了现有的方法，为表格推理任务提供了一种有效的解决方案。

Abstract: The table reasoning task, crucial for efficient data acquisition, aims to
answer questions based on the given table. Recently, reasoning large language
models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance
reasoning capabilities, leading to brilliant performance on table reasoning.
However, Long CoT suffers from high cost for training and exhibits low
reliability due to table content hallucinations. Therefore, we propose
Row-of-Thought (RoT), which performs iteratively row-wise table traversal,
allowing for reasoning extension and reflection-based refinement at each
traversal. Scaling reasoning length by row-wise traversal and leveraging
reflection capabilities of LLMs, RoT is training-free. The sequential traversal
encourages greater attention to the table, thus reducing hallucinations.
Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an
average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions
and TableBench with comparable models, proving its effectiveness. Also, RoT
outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.

</details>


### [54] [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)
*Bowen Jin,Jinsung Yoon,Priyanka Kargupta,Sercan O. Arik,Jiawei Han*

Key words: 强化学习,大型语言模型,搜索代理,奖励公式,搜索引擎

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文研究了强化学习（RL）在训练基于大型语言模型（LLM）的搜索代理中的关键设计因素，包括奖励公式、LLM的选择和搜索引擎的作用，并提供了实践指导。

Motivation: RL被用于训练LLM以解决实际问题，但基于RL的搜索代理的最优设计尚不完全清楚，需进一步研究关键因素。

Method: 通过全面的实证研究，系统调查了奖励公式、LLM选择和搜索引擎在RL过程中的作用。

Result: 发现格式化奖励显著提升性能，LLM的规模和初始化对RL结果影响重大，搜索引擎选择对训练动态和代理鲁棒性至关重要。

Conclusion: 研究结果为实际应用中构建和部署LLM搜索代理提供了重要指导。

Abstract: Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.

</details>


### [55] [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)
*Jinghui Lu,Haiyang Yu,Siliang Xu,Shiwei Ran,Guozhi Tang,Siqi Wang,Bin Shan,Teng Fu,Hao Feng,Jingqun Tang,Han Wang,Can Huang*

Key words: CAR, CoT, perplexity, reasoning, efficiency

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: CAR框架通过基于困惑度动态切换简短回答与长推理，优化准确性与效率。

Motivation: 过度依赖链式推理（CoT）导致模型性能下降和输出冗长，CAR旨在解决这一问题。

Method: 提出CAR框架，首先生成简短答案并评估困惑度，仅当困惑度高时触发推理。

Result: 在多种VQA/KIE基准测试和文本推理数据集中，CAR表现优于简短回答和长推理方法。

Conclusion: CAR在准确性与效率之间实现了最佳平衡。

Abstract: Recent advancements in reasoning have significantly enhanced the capabilities
of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
across diverse tasks. However, excessive reliance on chain-of-thought (CoT)
reasoning can impair model performance and brings unnecessarily lengthened
outputs, reducing efficiency. Our work reveals that prolonged reasoning does
not universally improve accuracy and even degrade performance on simpler tasks.
To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel
framework that dynamically switches between short answers and long-form
reasoning based on the model perplexity. CAR first generates a short answer and
evaluates its perplexity, triggering reasoning only when the model exhibits low
confidence (i.e., high perplexity). Experiments across diverse multimodal
VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both
short-answer and long-form reasoning approaches, striking an optimal balance
between accuracy and efficiency.

</details>


### [56] [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
*Jeonghye Kim,Sojeong Rhee,Minbeom Kim,Dohyung Kim,Sangmook Lee,Youngchul Sung,Kyomin Jung*

Key words: LLM代理, ReAct, ReflAct, 状态对齐, 代理推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ReflAct通过持续反思代理状态与目标的一致性，解决了ReAct因信念不一致和目标对齐不足导致的推理错误，显著提升了代理的战略可靠性。

Motivation: ReAct在复杂环境中常因推理步骤不切实际或不连贯而导致代理状态与目标不一致，ReflAct旨在通过持续反思和状态对齐改进这一问题。

Method: 引入了ReflAct，将推理从仅规划下一步行动转变为持续反思代理状态与目标的相对关系，并显式地将决策基于状态与目标对齐。

Result: ReflAct比ReAct平均表现提升了27.7%，在ALFWorld中达到了93.3%的成功率，甚至超过了增强版的ReAct。

Conclusion: 强化核心推理主干是提升代理性能的关键，ReflAct通过持续反思和状态对齐显著提高了代理的战略可靠性。

Abstract: Recent advances in LLM agents have largely built on reasoning backbones like
ReAct, which interleave thought and action in complex environments. However,
ReAct often produces ungrounded or incoherent reasoning steps, leading to
misalignment between the agent's actual state and goal. Our analysis finds that
this stems from ReAct's inability to maintain consistent internal beliefs and
goal alignment, causing compounding errors and hallucinations. To address this,
we introduce ReflAct, a novel backbone that shifts reasoning from merely
planning next actions to continuously reflecting on the agent's state relative
to its goal. By explicitly grounding decisions in states and enforcing ongoing
goal alignment, ReflAct dramatically improves strategic reliability. This
design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%
on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even
outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),
showing that strengthening the core reasoning backbone is key to reliable agent
performance.

</details>


### [57] [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/abs/2505.15196)
*Weiqi Wang,Limeng Cui,Xin Liu,Sreyashi Nag,Wenju Xu,Chen Luo,Sheikh Muhammad Sarwar,Yang Li,Hansu Gu,Hui Liu,Changlong Yu,Jiaxin Bai,Yifan Gao,Haiyang Zhang,Qi He,Shuiwang Ji,Yangqiu Song*

Key words: 电子商务脚本规划, LLM, 语义相似性, 产品检索, EcomScriptBench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了电子商务脚本规划（EcomScript）任务，分为三个子任务，并开发了一个新框架，通过语义相似性关联产品和动作，构建了首个大规模数据集EcomScriptBench。实验显示现有（L）LMs在此任务上表现不佳，但引入购买意图可提升性能。

Motivation: 为满足电商顾客对LLM助手的需求，解决当前LLMs在脚本规划和产品检索上的不足，以及缺乏评估方法和数据的问题。

Method: 定义EcomScript任务，提出基于语义相似性的框架，关联产品和脚本步骤，并构建了包含60多万脚本的数据集EcomScriptBench。

Result: 实验表明现有（L）LMs在EcomScript任务上表现挑战性，但引入产品购买意图能提高性能。

Conclusion: 论文为电商脚本规划提供了新框架和数据集，验证了现有模型的局限性，并提出改进方向。

Abstract: Goal-oriented script planning, or the ability to devise coherent sequences of
actions toward specific goals, is commonly employed by humans to plan for
typical activities. In e-commerce, customers increasingly seek LLM-based
assistants to generate scripts and recommend products at each step, thereby
facilitating convenient and efficient shopping experiences. However, this
capability remains underexplored due to several challenges, including the
inability of LLMs to simultaneously conduct script planning and product
retrieval, difficulties in matching products caused by semantic discrepancies
between planned actions and search queries, and a lack of methods and benchmark
data for evaluation. In this paper, we step forward by formally defining the
task of E-commerce Script Planning (EcomScript) as three sequential subtasks.
We propose a novel framework that enables the scalable generation of
product-enriched scripts by associating products with each step based on the
semantic similarity between the actions and their purchase intentions. By
applying our framework to real-world e-commerce data, we construct the very
first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229
scripts sourced from 2.4 million products. Human annotations are then conducted
to provide gold labels for a sampled subset, forming an evaluation benchmark.
Extensive experiments reveal that current (L)LMs face significant challenges
with EcomScript tasks, even after fine-tuning, while injecting product purchase
intentions improves their performance.

</details>


### [58] [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
*Wonje Jeung,Sangyeon Yoon,Hyesoo Hong,Soeun Kim,Seungju Han,Youngjae Yu,Albert No*

Key words: 大型语言模型, 机器遗忘, 数据重叠, 基准评估, 选择性移除, 共享信息

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了DUSK基准，用于评估在数据重叠的现实场景下机器学习模型的遗忘能力，关注选择性移除特定内容的同时保留共享信息。

Motivation: 现有的遗忘评估通常假设遗忘和保留数据集完全分离，忽略了实际中二者可能重叠的情况。需开发能选择性移除特定内容的技术。

Method: 论文提出DUSK基准，构建包含相同事实但风格不同的文档集，部分内容共享，部分独特。评估方法是否能选择性移除独特内容。

Result: 评估了九种遗忘方法，发现大多数能移除表层文本，但难以在不损害共享内容的情况下清除深层知识。

Conclusion: DUSK作为公共基准，支持开发更精确可靠的遗忘技术，适用于现实场景。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about the unauthorized use of copyrighted or
sensitive data. Machine unlearning aims to remove such 'forget' data while
preserving utility and information from the 'retain' set. However, existing
evaluations typically assume that forget and retain sets are fully disjoint,
overlooking realistic scenarios where they share overlapping content. For
instance, a news article may need to be unlearned, even though the same event,
such as an earthquake in Japan, is also described factually on Wikipedia.
Effective unlearning should remove the specific phrasing of the news article
while preserving publicly supported facts. In this paper, we introduce DUSK, a
benchmark designed to evaluate unlearning methods under realistic data overlap.
DUSK constructs document sets that describe the same factual content in
different styles, with some shared information appearing across all sets and
other content remaining unique to each. When one set is designated for
unlearning, an ideal method should remove its unique content while preserving
shared facts. We define seven evaluation metrics to assess whether unlearning
methods can achieve this selective removal. Our evaluation of nine recent
unlearning methods reveals a key limitation: while most can remove
surface-level text, they often fail to erase deeper, context-specific knowledge
without damaging shared content. We release DUSK as a public benchmark to
support the development of more precise and reliable unlearning techniques for
real-world applications.

</details>


### [59] [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/abs/2505.15210)
*Jie Ma,Ning Qu,Zhitao Gao,Rui Xing,Jun Liu,Hongbin Pei,Jiang Xie,Linyun Song,Pinghui Wang,Jing Tao,Zhou Su*

Key words: 知识图谱, LLMs, 可信推理, Kahneman-Tversky优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: DP框架通过充分利用知识图谱的先验知识，提升LLMs的生成可信度和推理忠实性。

Motivation: 解决现有方法未充分利用知识图谱结构信息与约束知识的不足，提升LLMs的可信度。

Method: 采用渐进知识蒸馏策略和推理-反思策略，结合监督微调与Kahneman-Tversky优化。

Result: 在三个基准数据集上实现SOTA性能，ComplexWebQuestions上的Hit@1提升13%。

Conclusion: DP框架显著提升生成结果的可信度，具有灵活性与实用性。

Abstract: Knowledge graph-based retrieval-augmented generation seeks to mitigate
hallucinations in Large Language Models (LLMs) caused by insufficient or
outdated knowledge. However, existing methods often fail to fully exploit the
prior knowledge embedded in knowledge graphs (KGs), particularly their
structural information and explicit or implicit constraints. The former can
enhance the faithfulness of LLMs' reasoning, while the latter can improve the
reliability of response generation. Motivated by these, we propose a
trustworthy reasoning framework, termed Deliberation over Priors (DP), which
sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a
progressive knowledge distillation strategy that integrates structural priors
into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky
optimization, thereby improving the faithfulness of relation path generation.
Furthermore, our framework employs a reasoning-introspection strategy, which
guides LLMs to perform refined reasoning verification based on extracted
constraint priors, ensuring the reliability of response generation. Extensive
experiments on three benchmark datasets demonstrate that DP achieves new
state-of-the-art performance, especially a Hit@1 improvement of 13% on the
ComplexWebQuestions dataset, and generates highly trustworthy responses. We
also conduct various analyses to verify its flexibility and practicality. The
code is available at https://github.com/reml-group/Deliberation-on-Priors.

</details>


### [60] [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
*Sangyeon Yoon,Wonje Jeung,Albert No*

Key words: 大型推理模型, R-TOFU, 知识遗忘, 链式推理, 偏好优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了R-TOFU基准，专注于研究大型推理模型（LRMs）中的知识遗忘问题，并提供了一种新方法Reasoned IDK以平衡遗忘效果与模型性能。

Motivation: 研究针对大型推理模型（LRMs）在多步推理链中嵌入私有或受版权保护信息的问题，传统遗忘方法在此场景下表现不佳。

Method: 提出R-TOFU基准，结合链式推理注释，并开发了Reasoned IDK方法，通过偏好优化实现更有效的知识遗忘。

Result: 实验表明传统答案级遗忘方法在推理链中仍有残留知识，而Reasoned IDK在遗忘效果与模型效用间取得更好平衡。

Conclusion: R-TOFU基准和分析为LRMs中的知识遗忘研究提供了系统化基础，并揭示了在不同解码设置下评估的重要性。

Abstract: Large Reasoning Models (LRMs) embed private or copyrighted information not
only in their final answers but also throughout multi-step chain-of-thought
(CoT) traces, making reliable unlearning far more demanding than in standard
LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to
this setting. R-TOFU augments existing unlearning tasks with realistic CoT
annotations and provides step-wise metrics that expose residual knowledge
invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive
comparison of gradient-based and preference-optimization baselines and show
that conventional answer-only objectives leave substantial forget traces in
reasoning. We further propose Reasoned IDK, a preference-optimization variant
that preserves coherent yet inconclusive reasoning, achieving a stronger
balance between forgetting efficacy and model utility than earlier refusal
styles. Finally, we identify a failure mode: decoding variants such as
ZeroThink and LessThink can still reveal forgotten content despite seemingly
successful unlearning, emphasizing the need to evaluate models under diverse
decoding settings. Together, the benchmark, analysis, and new baseline
establish a systematic foundation for studying and improving unlearning in LRMs
while preserving their reasoning capabilities.

</details>


### [61] [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
*Qihan Wang,Shidong Pan,Tal Linzen,Emily Black*

Key words: 大型语言模型、文化多样性、多语言提示、生成多样性、幻觉

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种多语言提示方法，通过添加文化和语言提示来增强大型语言模型生成内容的多样性，实验证明该方法优于现有技术。

Motivation: 大型语言模型在生成内容中缺乏文化多样性和代表性，作者旨在通过多语言提示激活模型中嵌入的广泛文化知识。

Method: 提出多语言提示方法，生成包含不同文化线索的提示变体，综合生成结果以提高多样性。

Result: 多语言提示在多个模型上表现优于现有技术，其效果受语言资源水平和模型大小影响，与文化线索对齐的提示减少了幻觉。

Conclusion: 多语言提示是一种有效的增强模型生成内容多样性的方法，尤其适用于需要文化敏感性的任务。

Abstract: Large Language Models (LLMs) are known to lack cultural representation and
overall diversity in their generations, from expressing opinions to answering
factual questions. To mitigate this problem, we propose multilingual prompting:
a prompting method which generates several variations of a base prompt with
added cultural and linguistic cues from several cultures, generates responses,
and then combines the results. Building on evidence that LLMs have
language-specific knowledge, multilingual prompting seeks to increase diversity
by activating a broader range of cultural knowledge embedded in model training
data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA
70B, and LLaMA 8B), we show that multilingual prompting consistently
outperforms existing diversity-enhancing techniques such as high-temperature
sampling, step-by-step recall, and personas prompting. Further analyses show
that the benefits of multilingual prompting vary with language resource level
and model size, and that aligning the prompting language with the cultural cues
reduces hallucination about culturally-specific information.

</details>


### [62] [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)
*Zihao Jiang,Ben Liu,Miao Peng,Wenjie Xu,Yao Xiao,Zhenyan Shan,Min Peng*

Key words: 大语言模型,时间推理,可解释性,图结构,GETER

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一个名为GETER的框架，通过结合图结构和文本信息提升大语言模型在可解释时间推理中的表现，填补了现有研究忽视推理过程的空白。

Motivation: 现有研究过于注重提升大语言模型的时间推理性能，却忽略了可解释的推理过程。本文旨在填补这一空白，提出一个评估框架和解决方案。

Method: 通过时间知识图谱构建时间编码器捕捉查询的结构信息，设计结构-文本前缀适配器将图结构特征映射到文本嵌入空间，最后结合软图标记和指令调整生成解释文本。

Result: GETER框架在实验中表现出卓越的性能，展现了其有效性和强大的泛化能力。

Conclusion: GETER框架通过结合图结构和文本信息，显著提升了大语言模型的可解释时间推理能力，为相关研究提供了新的思路。

Abstract: While large language models (LLMs) show great potential in temporal
reasoning, most existing work focuses heavily on enhancing performance, often
neglecting the explainable reasoning processes underlying the results. To
address this gap, we introduce a comprehensive benchmark covering a wide range
of temporal granularities, designed to systematically evaluate LLMs'
capabilities in explainable temporal reasoning. Furthermore, our findings
reveal that LLMs struggle to deliver convincing explanations when relying
solely on textual information. To address challenge, we propose GETER, a novel
structure-aware generative framework that integrates Graph structures with text
for Explainable TEmporal Reasoning. Specifically, we first leverage temporal
knowledge graphs to develop a temporal encoder that captures structural
information for the query. Subsequently, we introduce a structure-text prefix
adapter to map graph structure features into the text embedding space. Finally,
LLMs generate explanation text by seamlessly integrating the soft graph token
with instruction-tuning prompt tokens. Experimental results indicate that GETER
achieves state-of-the-art performance while also demonstrating its
effectiveness as well as strong generalization capabilities. Our dataset and
code are available at https://github.com/carryTatum/GETER.

</details>


### [63] [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)
*Yerin Hwang,Dongryeol Lee,Kyungmin Min,Taegwan Kang,Yong-il Kim,Kyomin Jung*

Key words: 视觉语言模型, 对抗攻击, 文本-图像对齐, 评估偏差

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 大型视觉语言模型（LVLMs）在文本-图像对齐评估中存在视觉攻击漏洞，本研究首次探讨了视觉对抗攻击如何系统性地误导LVLMs评分，并提出了多领域基准FRAME验证其脆弱性。

Motivation: 研究LVLMs在评估文本-图像对齐时的视觉攻击脆弱性，填补了现有研究的空白。

Method: 定义视觉诱导偏差，构建多领域基准FRAME，并测试多种LVLMs在受攻击图像上的评分表现。

Result: 所有测试的LVLMs均表现出脆弱性，易受视觉攻击影响并高评分；多偏差组合会放大效果。

Conclusion: 当前LVLM评估系统存在明显漏洞，亟需更鲁棒的评估方法。

Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred
tools for judging text-image alignment, yet their robustness along the visual
modality remains underexplored. This work is the first study to address a key
research question: Can adversarial visual manipulations systematically fool
LVLM judges into assigning unfairly inflated scores? We define potential image
induced biases within the context of T2I evaluation and examine how these
biases affect the evaluations of LVLM judges. Moreover, we introduce a novel,
fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is
deliberately constructed to exhibit diverse score distributions. By introducing
the defined biases into the benchmark, we reveal that all tested LVLM judges
exhibit vulnerability across all domains, consistently inflating scores for
manipulated images. Further analysis reveals that combining multiple biases
amplifies their effects, and pairwise evaluations are similarly susceptible.
Moreover, we observe that visual biases persist under prompt-based mitigation
strategies, highlighting the vulnerability of current LVLM evaluation systems
and underscoring the urgent need for more robust LVLM judges.

</details>


### [64] [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/abs/2505.15255)
*Yuansheng Gao,Han Bao,Tong Zhang,Bin Li,Zonghui Wang,Wenzhi Chen*

Key words: 心理操纵,大型语言模型,多任务学习,知识蒸馏,数据扩展

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了一种名为MentalMAC的多任务反课程蒸馏方法，用于增强大型语言模型（LLM）在多轮对话中检测心理操纵的能力，并构建了一个真实数据集ReaMent。

Motivation: 心理操纵是一种隐蔽且广泛的心理虐待形式，对心理健康构成严重威胁，但由于其隐蔽性和复杂性，现有技术难以检测。

Method: 提出了MentalMAC方法，包括基于进化操作和言语行为理论的EvoSA无监督数据扩展方法、教师模型生成的多任务监督，以及从复杂任务到简单任务的渐进式知识蒸馏。

Result: 实验表明，该方法显著缩小了学生模型与教师模型之间的差距，并在关键评估指标上优于其他竞争模型。

Conclusion: MentalMAC有效提升了LLM检测心理操纵的能力，并构建了高质量的真实数据集。

Abstract: Mental manipulation is a subtle yet pervasive form of psychological abuse
that poses serious threats to mental health. Its covert nature and the
complexity of manipulation strategies make it challenging to detect, even for
state-of-the-art large language models (LLMs). This concealment also hinders
the manual collection of large-scale, high-quality annotations essential for
training effective models. Although recent efforts have sought to improve LLM's
performance on this task, progress remains limited due to the scarcity of
real-world annotated datasets. To address these challenges, we propose
MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'
ability to detect mental manipulation in multi-turn dialogue. Our approach
includes: (i) EvoSA, an unsupervised data expansion method based on
evolutionary operations and speech act theory; (ii) teacher-model-generated
multi-task supervision; and (iii) progressive knowledge distillation from
complex to simpler tasks. We then constructed the ReaMent dataset with 5,000
real-world dialogue samples, using a MentalMAC-distilled model to assist human
annotation. Vast experiments demonstrate that our method significantly narrows
the gap between student and teacher models and outperforms competitive LLMs
across key evaluation metrics. All code, datasets, and checkpoints will be
released upon paper acceptance. Warning: This paper contains content that may
be offensive to readers.

</details>


### [65] [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/abs/2505.15257)
*Weixiang Zhao,Jiahe Guo,Yang Deng,Tongtong Wu,Wenxuan Zhang,Yulin Hu,Xingyu Sui,Yanyan Zhao,Wanxiang Che,Bing Qin,Tat-Seng Chua,Ting Liu*

Key words: 多语言推理、大型语言模型、语言消融、跨语言泛化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 通过语言特异性表示的消融干预，提升多语言推理能力，证明了语言和推理在LLMs中的可分离性，且无需额外训练。

Motivation: 解决LLMs在多语言推理中表现不均衡的问题，尤其是低资源语言。灵感来源于人类推理和语言处理独立运作的认知神经科学理论。

Method: 在推理时消融语言特异性表示，分析其对多语言推理的影响，并在10个开源LLMs和11种语言上进行实验验证。

Result: 语言特异性消融显著提升多语言推理性能，同时语言和推理表示可在模型中解耦，且保持语言特征的完整性。

Conclusion: 揭示了LLMs多语言推理的内部机制，提出了一种轻量级且可解释的策略来提升跨语言泛化能力。

Abstract: Multilingual reasoning remains a significant challenge for large language
models (LLMs), with performance disproportionately favoring high-resource
languages. Drawing inspiration from cognitive neuroscience, which suggests that
human reasoning functions largely independently of language processing, we
hypothesize that LLMs similarly encode reasoning and language as separable
components that can be disentangled to enhance multilingual reasoning. To
evaluate this, we perform a causal intervention by ablating language-specific
representations at inference time. Experiments on 10 open-source LLMs spanning
11 typologically diverse languages show that this language-specific ablation
consistently boosts multilingual reasoning performance. Layer-wise analyses
further confirm that language and reasoning representations can be effectively
decoupled throughout the model, yielding improved multilingual reasoning
capabilities, while preserving top-layer language features remains essential
for maintaining linguistic fidelity. Compared to post-training such as
supervised fine-tuning or reinforcement learning, our training-free ablation
achieves comparable or superior results with minimal computational overhead.
These findings shed light on the internal mechanisms underlying multilingual
reasoning in LLMs and suggest a lightweight and interpretable strategy for
improving cross-lingual generalization.

</details>


### [66] [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/abs/2505.15261)
*Jiatao Li,Mao Ye,Cheng Peng,Xunjian Yin,Xiaojun Wan*

Key words: AI生成文本检测，零样本学习，多智能体框架，可解释性，语义引导

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为AGENT-X的零样本多智能体框架，旨在解决现有AI生成文本检测方法依赖大数据集和外部阈值调整的局限性，通过语义、风格和结构三个维度进行检测，并在实验中表现出优越的性能。

Motivation: 现有的AI生成文本检测方法依赖大量标注数据和外部阈值调整，限制了方法的可解释性、适应性和零样本效果，亟需一种更灵活、高效的方法。

Method: 提出AGENT-X框架，基于古典修辞学和系统功能语言学，将检测准则分为语义、风格和结构三个维度，由专门的智能体独立评估，并通过元智能体进行置信度感知的聚合。

Result: 在多个数据集上的实验表明，AGENT-X在准确率、可解释性和泛化能力上显著优于现有的监督学习和零样本方法。

Conclusion: AGENT-X通过多智能体框架和动态路由机制，实现了高效、可解释且无需阈值的AI生成文本检测，为相关领域提供了新思路。

Abstract: Existing AI-generated text detection methods heavily depend on large
annotated datasets and external threshold tuning, restricting interpretability,
adaptability, and zero-shot effectiveness. To address these limitations, we
propose AGENT-X, a zero-shot multi-agent framework informed by classical
rhetoric and systemic functional linguistics. Specifically, we organize
detection guidelines into semantic, stylistic, and structural dimensions, each
independently evaluated by specialized linguistic agents that provide explicit
reasoning and robust calibrated confidence via semantic steering. A meta agent
integrates these assessments through confidence-aware aggregation, enabling
threshold-free, interpretable classification. Additionally, an adaptive
Mixture-of-Agent router dynamically selects guidelines based on inferred
textual characteristics. Experiments on diverse datasets demonstrate that
AGENT-X substantially surpasses state-of-the-art supervised and zero-shot
approaches in accuracy, interpretability, and generalization.

</details>


### [67] [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277)
*Hyungjoo Chae,Sunghwan Kim,Junhee Cho,Seungone Kim,Seungjun Moon,Gyeom Hwangbo,Dongha Lim,Minjin Kim,Yeonjun Hwang,Minju Gwak,Dongwook Choi,Minseok Kang,Gwanhoon Im,ByeongUng Cho,Hyojun Kim,Jun Hee Han,Taeyoon Kwon,Minju Kim,Beong-woo Kwak,Dongjin Kang,Jinyoung Yeo*

Key words: 网页导航, 过程奖励模型, 多模态大语言模型, WebPRM Collection, WebRewardBench

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了首个过程奖励模型Web-Shepherd，用于评估网页导航轨迹的每一步，通过构建大规模数据集WebPRM Collection和基准测试WebRewardBench，显著提升了性能并降低了成本。

Motivation: 网页导航需要长序列决策，传统多模态大语言模型（MLLM）作为奖励模型存在速度和成本问题，亟需一种更高效的替代方案。

Method: 1. 构建WebPRM Collection数据集（40K步级偏好对和标注清单）；2. 引入WebRewardBench基准；3. 提出Web-Shepherd模型。

Result: Web-Shepherd在WebRewardBench上比GPT-4o准确率高30%；在WebArena-lite测试中，性能提升10.9%，成本降低10倍。

Conclusion: Web-Shepherd为网页导航任务提供了一种高效、低成本的过程奖励解决方案，推动了领域发展。

Abstract: Web navigation is a unique domain that can automate many repetitive real-life
tasks and is challenging as it requires long-horizon sequential decision making
beyond typical multimodal large language model (MLLM) tasks. Yet, specialized
reward models for web navigation that can be utilized during both training and
test-time have been absent until now. Despite the importance of speed and
cost-effectiveness, prior works have utilized MLLMs as reward models, which
poses significant constraints for real-world deployment. To address this, in
this work, we propose the first process reward model (PRM) called Web-Shepherd
which could assess web navigation trajectories in a step-level. To achieve
this, we first construct the WebPRM Collection, a large-scale dataset with 40K
step-level preference pairs and annotated checklists spanning diverse domains
and difficulty levels. Next, we also introduce the WebRewardBench, the first
meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe
that our Web-Shepherd achieves about 30 points better accuracy compared to
using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by
using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve
10.9 points better performance, in 10 less cost compared to using GPT-4o-mini
as the verifier. Our model, dataset, and code are publicly available at LINK.

</details>


### [68] [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Yuhang Guo*

Key words: In-Image Machine Translation (IIMT), complex scenarios, DebackX model, real-world backgrounds, text translation

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: In-Image Machine Translation (IIMT) focuses on translating text within images, but prior research was limited to simple scenarios. To address this, a new dataset with real-world backgrounds is created, and the DebackX model is proposed to improve performance in complex scenarios, showing better translation quality and visual results.

Motivation: Previous IIMT research was limited to simplified scenarios (e.g., one-line text on white backgrounds), which are impractical for real-world applications. The need for handling complex scenarios with real-world backgrounds motivated this study.

Method: The DebackX model separates background and text-image from the source, translates the text-image directly, and fuses it with the background to generate the target image.

Result: Experimental results demonstrate that DebackX improves both translation quality and visual effects in complex scenarios.

Conclusion: The proposed DebackX model effectively addresses the limitations of previous IIMT methods in complex scenarios, enhancing practicality and performance.

Abstract: In-Image Machine Translation (IIMT) aims to translate texts within images
from one language to another. Previous research on IIMT was primarily conducted
on simplified scenarios such as images of one-line text with black font in
white backgrounds, which is far from reality and impractical for applications
in the real world. To make IIMT research practically valuable, it is essential
to consider a complex scenario where the text backgrounds are derived from
real-world images. To facilitate research of complex scenario IIMT, we design
an IIMT dataset that includes subtitle text with real-world background. However
previous IIMT models perform inadequately in complex scenarios. To address the
issue, we propose the DebackX model, which separates the background and
text-image from the source image, performs translation on text-image directly,
and fuses the translated text-image with the background, to generate the target
image. Experimental results show that our model achieves improvements in both
translation quality and visual effect.

</details>


### [69] [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291)
*Joonho Yang,Seunghyun Yoon,Hwan Chang,Byeongjeong Kim,Hwanhee Lee*

Key words: 大型语言模型, 幻觉, 长文本生成, 位置分布, 摘要

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型在生成长文本时，幻觉现象在后段更集中的问题及其解决方法。

Motivation: 虽然大型语言模型生成的文本流畅，但幻觉问题是主要挑战，尤其是长文本后端幻觉的分布尚未深入研究。

Method: 以长文档摘要为例，分析了幻觉的位置分布，探索了注意力和解码动态的影响，并研究了缓解方法。

Result: 发现幻觉在后段更集中，并提出了针对性改善方法。

Conclusion: 幻觉在长文本后端更常见，需针对性优化以提升生成忠实度。

Abstract: Large Language Models (LLMs) have significantly advanced text generation
capabilities, including tasks like summarization, often producing coherent and
fluent outputs. However, faithfulness to source material remains a significant
challenge due to the generation of hallucinations. While extensive research
focuses on detecting and reducing these inaccuracies, less attention has been
paid to the positional distribution of hallucination within generated text,
particularly in long outputs. In this work, we investigate where hallucinations
occur in LLM-based long response generation, using long document summarization
as a key case study. Focusing on the challenging setting of long context-aware
long response generation, we find a consistent and concerning phenomenon:
hallucinations tend to concentrate disproportionately in the latter parts of
the generated long response. To understand this bias, we explore potential
contributing factors related to the dynamics of attention and decoding over
long sequences. Furthermore, we investigate methods to mitigate this positional
hallucination, aiming to improve faithfulness specifically in the concluding
segments of long outputs.

</details>


### [70] [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/abs/2505.15297)
*Xintong Wang,Yixiao Liu,Jingheng Pan,Liang Ding,Longyue Wang,Chris Biemann*

Key words: 中文去毒,情感对齐,ToxiRewriteCN,大语言模型,隐晦毒性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了首个针对中文的情感对齐去毒数据集ToxiRewriteCN，评估了17种LLM模型在去毒能力上的表现，发现模型在隐晦或上下文密集场景中表现不佳。

Motivation: 在线交流中，去毒化内容同时保留原意和情感是具有挑战性的任务，尤其在中文中通过表情、谐音等方式隐晦表达的毒性更难处理。

Method: 构建了包含1,556条标注三元组的中文去毒数据集ToxiRewriteCN，覆盖五种实际场景，评估了17种LLM模型的多维度表现。

Result: 商业和MoE模型整体表现最佳，但所有模型在隐晦或上下文密集的场景中难以平衡安全性与情感保真度。

Conclusion: ToxiRewriteCN为中文情感感知去毒研究提供了支持，未来需进一步提升模型在复杂场景中的表现。

Abstract: Detoxifying offensive language while preserving the speaker's original intent
is a challenging yet critical goal for improving the quality of online
interactions. Although large language models (LLMs) show promise in rewriting
toxic content, they often default to overly polite rewrites, distorting the
emotional tone and communicative intent. This problem is especially acute in
Chinese, where toxicity often arises implicitly through emojis, homophones, or
discourse context. We present ToxiRewriteCN, the first Chinese detoxification
dataset explicitly designed to preserve sentiment polarity. The dataset
comprises 1,556 carefully annotated triplets, each containing a toxic sentence,
a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five
real-world scenarios: standard expressions, emoji-induced and homophonic
toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs,
including commercial and open-source models with variant architectures, across
four dimensions: detoxification accuracy, fluency, content preservation, and
sentiment polarity. Results show that while commercial and MoE models perform
best overall, all models struggle to balance safety with emotional fidelity in
more subtle or context-heavy settings such as emoji, homophone, and
dialogue-based inputs. We release ToxiRewriteCN to support future research on
controllable, sentiment-aware detoxification for Chinese.

</details>


### [71] [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/abs/2505.15316)
*Xin Bai,Guanyi Chen,Tingting He,Chenlian Zhou,Yu Liu*

Key words: 情感支持对话, 多策略连续序列, 语料库分析, 大规模语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文重新定义了情感支持对话（ESC）任务，提出将支持性回应视为多策略连续序列，并通过实验证明大规模语言模型（LLM）在改进任务中优于监督模型和人类支持者。

Motivation: 现有的ESC任务定义过于简化支持回应结构，忽视了情感支持者常在同一轮对话中连续使用多策略的现象。

Method: 通过语料库分析重新定义ESC任务，并引入监督深度学习模型和LLM进行评估。

Result: 实验表明，在改进任务中，LLM表现优于监督模型和人类支持者，且展现更全面的支持能力。

Conclusion: 重新定义ESC任务能更真实反映实际支持行为，LLM在该任务中表现卓越。

Abstract: Emotional Support Conversations (ESC) are crucial for providing empathy,
validation, and actionable guidance to individuals in distress. However,
existing definitions of the ESC task oversimplify the structure of supportive
responses, typically modelling them as single strategy-utterance pairs. Through
a detailed corpus analysis of the ESConv dataset, we identify a common yet
previously overlooked phenomenon: emotional supporters often employ multiple
strategies consecutively within a single turn. We formally redefine the ESC
task to account for this, proposing a revised formulation that requires
generating the full sequence of strategy-utterance pairs given a dialogue
history. To facilitate this refined task, we introduce several modelling
approaches, including supervised deep learning models and large language
models. Our experiments show that, under this redefined task, state-of-the-art
LLMs outperform both supervised models and human supporters. Notably, contrary
to some earlier findings, we observe that LLMs frequently ask questions and
provide suggestions, demonstrating more holistic support capabilities.

</details>


### [72] [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/abs/2505.15323)
*Silvia Cappelletti,Tobia Poppi,Samuele Poppi,Zheng-Xin Yong,Diego Garcia-Olano,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Key words: Large Language Models, multiple-choice question answering, first-token probability, prefilling attack, AI safety

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为*prefilling attack*的方法，通过在模型输出前添加结构化前缀（如'*The correct option is:*'），有效改善了大型语言模型在多项选择题任务中的评估可靠性。

Motivation: 传统的*first-token probability* (FTP)方法在多项选择题任务中存在脆弱性，可能导致无关令牌的高概率或误解，影响评估的可靠性。

Method: 提出*prefilling attack*，通过添加自然语言前缀引导模型生成清晰的有效选项，无需修改模型参数。

Result: 实验表明，该方法显著提升了准确性、校准性和输出一致性，在效率和性能上与开放式生成方法相当。

Conclusion: prefilling是一种简单、鲁棒且低成本的方法，可提升FTP在多项选择题评估中的可靠性。

Abstract: Large Language Models (LLMs) are increasingly evaluated on multiple-choice
question answering (MCQA) tasks using *first-token probability* (FTP), which
selects the answer option whose initial token has the highest likelihood. While
efficient, FTP can be fragile: models may assign high probability to unrelated
tokens (*misalignment*) or use a valid token merely as part of a generic
preamble rather than as a clear answer choice (*misinterpretation*),
undermining the reliability of symbolic evaluation. We propose a simple
solution: the *prefilling attack*, a structured natural-language prefix (e.g.,
"*The correct option is:*") prepended to the model output. Originally explored
in AI safety, we repurpose prefilling to steer the model to respond with a
clean, valid option, without modifying its parameters. Empirically, the FTP
with prefilling strategy substantially improves accuracy, calibration, and
output consistency across a broad set of LLMs and MCQA benchmarks. It
outperforms standard FTP and often matches the performance of open-ended
generation approaches that require full decoding and external classifiers,
while being significantly more efficient. Our findings suggest that prefilling
is a simple, robust, and low-cost method to enhance the reliability of
FTP-based evaluation in multiple-choice settings.

</details>


### [73] [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)
*Yuhao Zhang,Xiangnan Ma,Kaiqi Kou,Peizhuo Liu,Weiqiao Shan,Benyou Wang,Tong Xiao,Yuxin Huang,Zhengtao Yu,Jingbo Zhu*

Key words: 语音翻译、单元语言、多任务学习、跨模态、跨语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于单元语言的文本无关语音翻译方法，解决了跨模态和跨语言的挑战，并通过多任务学习和任务提示建模提升了性能。

Motivation: 解决语音到语音翻译中跨模态特征提取和跨语言对齐的挑战。

Method: 通过单元语言作为文本类似表示，结合多任务学习和任务提示建模，指导语音建模过程。

Result: 在Voxpupil数据集的四种语言上，性能显著优于基线，接近有文本训练的模型。

Conclusion: 单元语言和多任务学习的结合是解决语音翻译挑战的有效方法。

Abstract: The success of building textless speech-to-speech translation (S2ST) models
has attracted much attention. However, S2ST still faces two main challenges: 1)
extracting linguistic features for various speech signals, called cross-modal
(CM), and 2) learning alignment of difference languages in long sequences,
called cross-lingual (CL). We propose the unit language to overcome the two
modeling challenges. The unit language can be considered a text-like
representation format, constructed using $n$-gram language modeling. We
implement multi-task learning to utilize the unit language in guiding the
speech modeling process. Our initial results reveal a conflict when applying
source and target unit languages simultaneously. We propose task prompt
modeling to mitigate this conflict. We conduct experiments on four languages of
the Voxpupil dataset. Our method demonstrates significant improvements over a
strong baseline and achieves performance comparable to models trained with
text.

</details>


### [74] [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
*Hao Fang,Jiawei Kong,Tianqu Zhuang,Yixiang Qiu,Kuofeng Gao,Bin Chen,Shu-Tao Xia,Yaowei Wang,Min Zhang*

Key words: 大型语言模型, 文本检测器, 对抗攻击, CoPA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种无需训练的方法CoPA，利用现成的大型语言模型生成更接近人类的文本，通过对比机器和人类词汇分布来规避文本检测器的检测。

Motivation: 现有规避文本检测器的方法需要大量数据和计算资源，且在高级检测算法下效果不佳。为了解决这些问题，提出了CoPA方法。

Method: CoPA通过精心设计的指令让LLM生成更人类化的文本，并通过对比机器和人类词汇分布来消除机器特征。

Result: 实验证明CoPA能有效欺骗多种文本检测器。

Conclusion: CoPA是一种高效且无需训练的对抗文本检测器的方法。

Abstract: The misuse of large language models (LLMs), such as academic plagiarism, has
driven the development of detectors to identify LLM-generated texts. To bypass
these detectors, paraphrase attacks have emerged to purposely rewrite these
texts to evade detection. Despite the success, existing methods require
substantial data and computational budgets to train a specialized paraphraser,
and their attack efficacy greatly reduces when faced with advanced detection
algorithms. To address this, we propose \textbf{Co}ntrastive
\textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that
effectively deceives text detectors using off-the-shelf LLMs. The first step is
to carefully craft instructions that encourage LLMs to produce more human-like
texts. Nonetheless, we observe that the inherent statistical biases of LLMs can
still result in some generated texts carrying certain machine-like attributes
that can be captured by detectors. To overcome this, CoPA constructs an
auxiliary machine-like word distribution as a contrast to the human-like
distribution generated by the LLM. By subtracting the machine-like patterns
from the human-like distribution during the decoding process, CoPA is able to
produce sentences that are less discernible by text detectors. Our theoretical
analysis suggests the superiority of the proposed attack. Extensive experiments
validate the effectiveness of CoPA in fooling text detectors across various
scenarios.

</details>


### [75] [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)
*Xiang Liu,Hong Chen,Xuming Hu,Xiaowen Chu*

Key words: 大型语言模型, KV缓存, 多轮对话, FlowKV, 信息保留

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FlowKV是一种新的KV缓存管理机制，通过多轮隔离避免重复压缩旧上下文，从而减轻信息丢失和遗忘问题，显著提升性能。

Motivation: 大型语言模型在多轮对话中，KV缓存的线性增长导致高计算成本，现有策略因重复压缩早期上下文而性能下降。

Method: 提出FlowKV的多轮隔离机制，仅对新生成的KV对进行压缩，保留历史压缩缓存，避免重复压缩。

Result: FlowKV在指令遵循准确性和用户偏好保留上表现优于基准策略，提升幅度从10.90%到75.40%。

Conclusion: FlowKV通过多轮隔离机制有效解决了KV缓存管理中的信息丢失和遗忘问题，显著提升了多轮对话性能。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-turn
conversational applications, where the management of the Key-Value (KV) Cache
presents a significant bottleneck. The linear growth of the KV Cache with
dialogue history imposes substantial computational costs, and existing eviction
strategies often degrade performance by repeatedly compressing early
conversational context, leading to information loss and context forgetting.
This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism}
for KV Cache management, which can be applied to any KV Cache compression
method without training. FlowKV's core innovation is a multi-turn isolation
mechanism that preserves the accumulated compressed KV cache from past turns.
Compression is then strategically applied only to the newly generated KV pairs
of the latest completed turn, effectively preventing the re-compression of
older context and thereby mitigating catastrophic forgetting. Our results
demonstrate that FlowKV consistently and significantly outperforms baseline
strategies in maintaining instruction-following accuracy and user preference
retention from 10.90\% to 75.40\%, particularly in later conversational turns.

</details>


### [76] [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)
*Enric Junqué de Fortuny*

Key words: 情感分类、心理学、数据集、跨领域

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出一个基于心理学的情感分类数据集，解决了现有数据集中类别不一致、样本量小或领域局限的问题。

Motivation: 由于现有情感分类数据集缺乏标准化且心理学基础不足，作者旨在填补这一空白。

Method: 作者利用Shaver的情感分类法，整合了多样化的文本来源，构建了一个统一框架。

Result: 提出了一个规模较大、跨领域一致的情感数据集。

Conclusion: 该数据集为跨领域情感识别研究提供了更一致的资源。

Abstract: Despite the wide-scale usage and development of emotion classification
datasets in NLP, the field lacks a standardized, large-scale resource that
follows a psychologically grounded taxonomy. Existing datasets either use
inconsistent emotion categories, suffer from limited sample size, or focus on
specific domains. The Super Emotion Dataset addresses this gap by harmonizing
diverse text sources into a unified framework based on Shaver's empirically
validated emotion taxonomy, enabling more consistent cross-domain emotion
recognition research.

</details>


### [77] [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/abs/2505.15353)
*Ryo Kishino,Yusuke Takase,Momose Oyama,Hiroaki Yamagiwa,Hidetoshi Shimodaira*

Key words: KL散度, 语言模型, 对数似然向量, 预训练轨迹, 权重空间

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究评估了一种基于对数似然向量测量语言模型间KL散度的方法，分析了不同条件下的结果，发现语言模型在预训练中的轨迹呈螺旋结构，而在对数似然空间中模型的轨迹比权重空间中更为受限。

Motivation: 为了更好地理解基于对数似然向量测量语言模型KL散度的行为，研究者系统地评估了这一度量在不同条件下的表现。

Method: 使用公开可用的语言模型，通过基于对数似然向量的方法测量KL散度，对比了预训练检查点、微调与基础模型以及不同层的差异。

Result: 研究发现语言模型在预训练中的轨迹呈现螺旋结构，而在对数似然空间中模型轨迹比权重空间中更为受限。

Conclusion: 基于对数似然向量的KL散度测量方法能够揭示语言模型训练过程中的动态特征，对数似然空间的轨迹比权重空间更具限制性。

Abstract: A recently proposed method enables efficient estimation of the KL divergence
between language models, including models with different architectures, by
assigning coordinates based on log-likelihood vectors. To better understand the
behavior of this metric, we systematically evaluate KL divergence across a wide
range of conditions using publicly available language models. Our analysis
covers comparisons between pretraining checkpoints, fine-tuned and base models,
and layers via the logit lens. We find that trajectories of language models, as
measured by KL divergence, exhibit a spiral structure during pretraining and
thread-like progressions across layers. Furthermore, we show that, in terms of
diffusion exponents, model trajectories in the log-likelihood space are more
constrained than those in weight space.

</details>


### [78] [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)
*Xabier de Zuazo,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Key words: 语音产生, MEG, 音素解码, 机器学习, 低频振荡

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该研究通过脑磁图（MEG）信号解码语音产生和感知任务中的音素，发现语音产生时的解码准确率显著高于被动听音。Elastic Net分类器表现最佳，低频波段对解码贡献最大。

Motivation: 探究语音产生的神经机制，推动认知神经科学理论发展及通信技术应用。

Method: 使用17名参与者的MEG数据，比较多种机器学习方法（如正则化线性模型和神经网络），分析15对音素的分类效果。

Result: 语音产生任务解码准确率76.6%，显著高于被动听音（51%）。Elastic Net表现最优，低频振荡（Delta和Theta波段）贡献最大。

Conclusion: 语音产生范式为研究神经机制提供了丰富信息，对改进脑机接口技术具有重要意义，但需进一步方法学优化。

Abstract: Understanding the neural mechanisms underlying speech production is essential
for both advancing cognitive neuroscience theory and developing practical
communication technologies. In this study, we investigated
magnetoencephalography signals to decode phones from brain activity during
speech production and perception (passive listening and voice playback) tasks.
Using a dataset comprising 17 participants, we performed pairwise phone
classification, extending our analysis to 15 phonetic pairs. Multiple machine
learning approaches, including regularized linear models and neural network
architectures, were compared to determine their effectiveness in decoding
phonetic information. Our results demonstrate significantly higher decoding
accuracy during speech production (76.6%) compared to passive listening and
playback modalities (~51%), emphasizing the richer neural information available
during overt speech. Among the models, the Elastic Net classifier consistently
outperformed more complex neural networks, highlighting the effectiveness of
traditional regularization techniques when applied to limited and
high-dimensional MEG datasets. Besides, analysis of specific brain frequency
bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)
and Theta (4-7 Hz), contributed the most substantially to decoding accuracy,
suggesting that these bands encode critical speech production-related neural
processes. Despite using advanced denoising methods, it remains unclear whether
decoding solely reflects neural activity or if residual muscular or movement
artifacts also contributed, indicating the need for further methodological
refinement. Overall, our findings underline the critical importance of
examining overt speech production paradigms, which, despite their complexity,
offer opportunities to improve brain-computer interfaces to help individuals
with severe speech impairments.

</details>


### [79] [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/abs/2505.15356)
*Weiming Zhang,Qingyao Li,Xinyi Dai,Jizheng Chen,Kounianhua Du,Weinan Zhang,Weiwen Liu,Yasheng Wang,Ruiming Tang,Yong Yu*

Key words: 自然语言推理, 代码调试, NL-DEBUGGING, 执行反馈

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出NL-DEBUGGING框架，利用自然语言作为中间表示改进代码调试，证明其优于传统方法并通过执行反馈直接优化。

Motivation: 传统代码级分析难以解决复杂的编程错误，而自然语言推理在代码任务中的应用尚未明确其最优形式及具体优势。

Method: NL-DEBUGGING框架以自然语言为中间表示，通过执行反馈指导调试，提供更广的修改空间。

Result: NL-DEBUGGING框架在代码调试任务中表现优于传统方法。

Conclusion: 自然语言推理有望推动自动化代码调试，解决复杂编程挑战。

Abstract: Debugging is a critical aspect of LLM's coding ability. Early debugging
efforts primarily focused on code-level analysis, which often falls short when
addressing complex programming errors that require a deeper understanding of
algorithmic logic. Recent advancements in large language models (LLMs) have
shifted attention toward leveraging natural language reasoning to enhance
code-related tasks. However, two fundamental questions remain unanswered: What
type of natural language format is most effective for debugging tasks? And what
specific benefits does natural language reasoning bring to the debugging
process? In this paper, we introduce NL-DEBUGGING, a novel framework that
employs natural language as an intermediate representation to improve code
debugging. By debugging at a natural language level, we demonstrate that
NL-DEBUGGING outperforms traditional debugging methods and enables a broader
modification space through direct refinement guided by execution feedback. Our
findings highlight the potential of natural language reasoning to advance
automated code debugging and address complex programming challenges.

</details>


### [80] [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/abs/2505.15372)
*Peng Wang,Ruihan Tao,Qiguang Chen,Mengkang Hu,Libo Qin*

Key words: 多语言代理, LLM, 基准测试, 跨语言对齐, GPT-4o

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了X-WebAgentBench，一个用于评估多语言代理在交互式网络环境中规划与交互性能的新基准，填补了现有研究以英语为主的不足。

Motivation: 尽管大型语言模型（LLM）代理在交互环境中取得了显著成功，但当前研究主要集中在英语场景，忽视了全球7000多种语言的需求。

Method: 作者提出了X-WebAgentBench，一个多语言代理基准，用于评估不同语言代理的规划和交互能力，同时测试了多种LLM和跨语言对齐方法的性能。

Result: 研究发现，即使是GPT-4o等先进模型结合跨语言技术，也未达到满意结果。

Conclusion: X-WebAgentBench有望成为现实应用中多语言代理场景的重要基准。

Abstract: Recently, large language model (LLM)-based agents have achieved significant
success in interactive environments, attracting significant academic and
industrial attention. Despite these advancements, current research
predominantly focuses on English scenarios. In reality, there are over 7,000
languages worldwide, all of which demand access to comparable agentic services.
Nevertheless, the development of language agents remains inadequate for meeting
the diverse requirements of multilingual agentic applications. To fill this
gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an
interactive web environment, which evaluates the planning and interaction
performance of language agents across multiple languages, thereby contributing
to the advancement of global agent intelligence. Additionally, we assess the
performance of various LLMs and cross-lingual alignment methods, examining
their effectiveness in enhancing agents. Our findings reveal that even advanced
models like GPT-4o, when combined with cross-lingual techniques, fail to
achieve satisfactory results. We hope that X-WebAgentBench can serve as a
valuable benchmark for multilingual agent scenario in real-world applications.

</details>


### [81] [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
*Yiming Huang,Junyan Zhang,Zihao Wang,Biquan Bie,Xuming Hu,Yi R.,Fung,Xinlei He*

Key words: 大语言模型,幻觉检测,不确定性,语义传播,语言生成

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出RePPL方法，通过两个维度重新校准不确定性测量，实现更好的幻觉检测和解释。

Motivation: 大语言模型存在幻觉问题，现有方法无法解释幻觉来源，因此需改进检测方法。

Method: 基于语义传播和语言生成的不确定性，RePPL方法为每个token分配可解释的不确定性分数，并以Perplexity-style Log-Average形式汇总。

Result: 在多个QA数据集上取得最佳综合检测性能（平均AUC 0.833），并能提供token级的解释分数。

Conclusion: RePPL不仅提升幻觉检测能力，还揭示了幻觉的混沌模式，具有应用潜力。

Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain
a vital obstacle to their trustworthy use. While previous works improved the
capability of hallucination detection by measuring uncertainty, they all lack
the ability to explain the provenance behind why hallucinations occur, i.e.,
which part of the inputs tends to trigger hallucinations. Recent works on the
prompt attack indicate that uncertainty exists in semantic propagation, where
attention mechanisms gradually fuse local token information into high-level
semantics across layers. Meanwhile, uncertainty also emerges in language
generation, due to its probability-based selection of high-level semantics for
sampled generations. Based on that, we propose RePPL to recalibrate uncertainty
measurement by these two aspects, which dispatches explainable uncertainty
scores to each token and aggregates in Perplexity-style Log-Average form as
total score. Experiments show that our method achieves the best comprehensive
detection performance across various QA datasets on advanced models (average
AUC of 0.833), and our method is capable of producing token-level uncertainty
scores as explanations for the hallucination. Leveraging these scores, we
preliminarily find the chaotic pattern of hallucination and showcase its
promising usage.

</details>


### [82] [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
*DongGeon Lee,Joonwon Jang,Jihae Jeong,Hwanjo Yu*

Key words: 视觉语言模型,模因,安全性评估,多轮交互

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究评估了视觉语言模型（VLMs）在面对真实模因（meme）图像时的安全性问题，发现其比合成图像更易产生有害输出。

Motivation: 随着视觉语言模型的快速部署，其安全性风险日益凸显。现有评估多依赖人工图像，无法反映真实用户分享的模因图像对模型安全性的影响。

Method: 研究提出MemeSafetyBench，包含50,430个实例，结合真实模因图像与有害/良性指令，基于综合安全分类法和LLM生成的指令，评估多个VLMs在单轮和多轮交互中的表现。

Result: VLMs对模因有害提示的脆弱性高于合成或排版图像；多轮交互部分缓解问题，但脆弱性仍存在。

Conclusion: 研究强调需开发更生态有效的评估方法和更强的安全机制。

Abstract: Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet
most evaluations rely on artificial images. This study asks: How safe are
current VLMs when confronted with meme images that ordinary users share? To
investigate this question, we introduce MemeSafetyBench, a 50,430-instance
benchmark pairing real meme images with both harmful and benign instructions.
Using a comprehensive safety taxonomy and LLM-based instruction generation, we
assess multiple VLMs across single and multi-turn interactions. We investigate
how real-world memes influence harmful outputs, the mitigating effects of
conversational context, and the relationship between model scale and safety
metrics. Our findings demonstrate that VLMs show greater vulnerability to
meme-based harmful prompts than to synthetic or typographic images. Memes
significantly increase harmful responses and decrease refusals compared to
text-only inputs. Though multi-turn interactions provide partial mitigation,
elevated vulnerability persists. These results highlight the need for
ecologically valid evaluations and stronger safety mechanisms.

</details>


### [83] [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/abs/2505.15392)
*Yiming Huang,Biquan Bie,Zuqiu Na,Weilin Ruan,Songxin Lei,Yutao Yue,Xinlei He*

Key words: 大语言模型、锚定效应、认知偏差、评估策略、SynAnchors

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现大语言模型普遍存在锚定效应，常规策略无法消除，推理可在一定程度上缓解，强调评估需关注认知偏差。

Motivation: 随着大语言模型的发展，认知偏差问题日益引发关注，本文旨在研究LLMs中的锚定效应及其解决策略。

Method: 通过引入新数据集SynAnchors并结合改进的评估指标，对主流LLMs进行基准测试。

Result: LLMs普遍存在锚定效应，浅层作用明显，常规策略无法消除，推理能部分缓解。

Conclusion: 呼吁LLM评估需从标准基准转向认知偏差相关的可信评估。

Abstract: The rise of Large Language Models (LLMs) like ChatGPT has advanced natural
language processing, yet concerns about cognitive biases are growing. In this
paper, we investigate the anchoring effect, a cognitive bias where the mind
relies heavily on the first information as anchors to make affected judgments.
We explore whether LLMs are affected by anchoring, the underlying mechanisms,
and potential mitigation strategies. To facilitate studies at scale on the
anchoring effect, we introduce a new dataset, SynAnchors. Combining refined
evaluation metrics, we benchmark current widely used LLMs. Our findings show
that LLMs' anchoring bias exists commonly with shallow-layer acting and is not
eliminated by conventional strategies, while reasoning can offer some
mitigation. This recontextualization via cognitive psychology urges that LLM
evaluations focus not on standard benchmarks or over-optimized robustness
tests, but on cognitive-bias-aware trustworthy evaluation.

</details>


### [84] [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404)
*Zhexin Zhang,Xian Qi Loye,Victor Shea-Jay Huang,Junxiao Yang,Qi Zhu,Shiyao Cui,Fei Mi,Lifeng Shang,Yingkang Wang,Hongning Wang,Minlie Huang*

Key words: 大型推理模型（LRMs），监督微调（SFT），安全性，数据蒸馏，数学推理

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文通过监督微调（SFT）研究如何提升大型推理模型（LRMs）的安全性，发现短或模板化的推理过程可以实现类似安全性能，且混合数学推理数据有助于平衡安全性和过度拒绝。

Motivation: 大型推理模型（LRMs）在数学和编程等推理密集型任务中表现出色，但其推理能力的提升并未直接转化为安全性能的改进，甚至可能降低安全性，因此研究如何增强LRMs的安全性成为一个重要问题。

Method: 通过监督微调（SFT）研究安全性提升方法，分析数据蒸馏中的三种关键失败模式，并探索短或模板化推理过程的效果。

Result: 发现显式解决数据蒸馏中的问题可以显著提升安全性，短或模板化推理过程能实现类似安全性能，且混合数学推理数据有助于平衡安全性和过度拒绝。

Conclusion: 研究表明，简单推理过程也能有效提升LRMs的安全性，为未来研究提供了更全面的视角。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success on
reasoning-intensive tasks such as mathematics and programming. However, their
enhanced reasoning capabilities do not necessarily translate to improved safety
performance-and in some cases, may even degrade it. This raises an important
research question: how can we enhance the safety of LRMs? In this paper, we
present a comprehensive empirical study on how to enhance the safety of LRMs
through Supervised Fine-Tuning (SFT). Our investigation begins with an
unexpected observation: directly distilling safe responses from DeepSeek-R1
fails to significantly enhance safety. We analyze this phenomenon and identify
three key failure patterns that contribute to it. We then demonstrate that
explicitly addressing these issues during the data distillation process can
lead to substantial safety improvements. Next, we explore whether a long and
complex reasoning process is necessary for achieving safety. Interestingly, we
find that simply using short or template-based reasoning process can attain
comparable safety performance-and are significantly easier for models to learn
than more intricate reasoning chains. These findings prompt a deeper reflection
on the role of reasoning in ensuring safety. Finally, we find that mixing math
reasoning data during safety fine-tuning is helpful to balance safety and
over-refusal. Overall, we hope our empirical study could provide a more
holistic picture on enhancing the safety of LRMs. The code and data used in our
experiments are released in https://github.com/thu-coai/LRM-Safety-Study.

</details>


### [85] [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/abs/2505.15422)
*Nudrat Habib,Tosin Adewumi,Marcus Liwicki,Elisa Barney*

Key words: 作者分析, 文献综述, 机器学习, 深度学习, 大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文是对作者分析领域的系统性文献综述，聚焦于作者归属和作者验证两个子任务，总结了2015至2024年的研究方法、技术、数据集及挑战。

Motivation: 作者分析在多个领域（如法医语言学、网络安全）具有重要应用，但缺乏对最新方法和技术挑战的全面综述。

Method: 综述包括传统机器学习方法、深度学习模型和大语言模型（LLMs），分析其演进、优势和局限性。

Result: 总结了关键方法、特征提取技术、数据集和新兴挑战，如低资源语言处理和多语言适应。

Conclusion: 研究为研究人员提供了当前趋势和挑战的概述，并指出未来研究方向，旨在推动更可靠、准确的作者分析系统发展。

Abstract: Authorship analysis plays an important role in diverse domains, including
forensic linguistics, academia, cybersecurity, and digital content
authentication. This paper presents a systematic literature review on two key
sub-tasks of authorship analysis; Author Attribution and Author Verification.
The review explores SOTA methodologies, ranging from traditional ML approaches
to DL models and LLMs, highlighting their evolution, strengths, and
limitations, based on studies conducted from 2015 to 2024. Key contributions
include a comprehensive analysis of methods, techniques, their corresponding
feature extraction techniques, datasets used, and emerging challenges in
authorship analysis. The study highlights critical research gaps, particularly
in low-resource language processing, multilingual adaptation, cross-domain
generalization, and AI-generated text detection. This review aims to help
researchers by giving an overview of the latest trends and challenges in
authorship analysis. It also points out possible areas for future study. The
goal is to support the development of better, more reliable, and accurate
authorship analysis system in diverse textual domain.

</details>


### [86] [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/abs/2505.15424)
*Yan-Shuo Liang,Wu-Jun Li*

Key words: 持续学习, LoRA, 门控模块, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为GainLoRA的新方法，通过引入门控模块整合新旧LoRA分支，有效缓解遗忘问题，提升语言模型的持续学习性能。实验表明其优于现有方法。

Motivation: 现有基于LoRA的持续学习方法在适应新任务时通常强制新旧LoRA分支平等贡献，可能导致遗忘问题。

Method: 提出GainLoRA方法，为每个新任务扩展LoRA分支，并引入门控模块整合新旧分支，减少新分支对旧任务的贡献。

Result: 实验证明GainLoRA在持续学习基准测试中优于现有方法。

Conclusion: GainLoRA通过门控机制有效缓解遗忘问题，提升模型整体性能。

Abstract: Continual learning (CL), which requires the model to learn multiple tasks
sequentially, is crucial for language models (LMs). Recently, low-rank
adaptation (LoRA), one of the most representative parameter-efficient
fine-tuning (PEFT) methods, has gained increasing attention in CL of LMs.
However, most existing CL methods based on LoRA typically expand a new LoRA
branch to learn each new task and force the new and old LoRA branches to
contribute equally to old tasks, potentially leading to forgetting. In this
work, we propose a new method, called gated integration of low-rank adaptation
(GainLoRA), for CL of LMs. GainLoRA expands a new LoRA branch for each new task
and introduces gating modules to integrate the new and old LoRA branches.
Furthermore, GainLoRA leverages the new gating module to minimize the
contribution from the new LoRA branch to old tasks, effectively mitigating
forgetting and improving the model's overall performance. Experimental results
on CL benchmarks demonstrate that GainLoRA outperforms existing
state-of-the-art methods.

</details>


### [87] [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/abs/2505.15426)
*Aleksandra Tomaszewska,Dariusz Czerski,Bartosz Żuk,Maciej Ogrodniczuk*

Key words: NeoN, 波兰语新词, 自动化检测, LLM, 语言学工具

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: NeoN 是一种用于检测和分析波兰语新词的工具，通过结合多种技术和自动化流程，显著减少了手工工作，同时保持高准确性。

Motivation: 传统的新词检测方法依赖大量手工审查，效率低下。NeoN 旨在通过自动化技术优化这一过程。

Method: 结合参考语料库、波兰语特定语言过滤器、基于LLM的精度提升过滤器及每日RSS监控，采用多层管道方法。

Result: 评估显示NeoN在减少手工工作的同时保持高准确性，为波兰语词汇创新追踪提供了高效解决方案。

Conclusion: NeoN 通过技术整合实现了新词检测和分析的高效自动化，为研究者提供了直观的可视化工具。

Abstract: NeoN, a tool for detecting and analyzing Polish neologisms. Unlike
traditional dictionary-based methods requiring extensive manual review, NeoN
combines reference corpora, Polish-specific linguistic filters, an LLM-driven
precision-boosting filter, and daily RSS monitoring in a multi-layered
pipeline. The system uses context-aware lemmatization, frequency analysis, and
orthographic normalization to extract candidate neologisms while consolidating
inflectional variants. Researchers can verify candidates through an intuitive
interface with visualizations and filtering controls. An integrated LLM module
automatically generates definitions and categorizes neologisms by domain and
sentiment. Evaluations show NeoN maintains high accuracy while significantly
reducing manual effort, providing an accessible solution for tracking lexical
innovation in Polish.

</details>


### [88] [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)
*Zhiwen Li,Die Chen,Mingyuan Fan,Cen Chen,Yaliang Li,Yanhao Wang,Wenmeng Zhou*

Key words: 扩散模型、不安全内容、社会偏见、语义方向向量、LoRA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种通过发现语义方向向量约束文本嵌入至安全区域的新方法，有效减少扩散模型生成的不安全内容和社会偏见。

Motivation: 扩散模型生成高保真图像的能力被广泛采用，但其可能产生不安全内容和社会偏见的问题限制了其实际应用。

Method: 在嵌入空间中发现语义方向向量以约束文本嵌入至安全区域，结合LoRA初始化以减少对其他语义的影响。

Result: 实验表明，该方法能有效减少不安全内容和社会偏见的生成，优于现有方法。

Conclusion: 新方法通过语义方向向量和LoRA技术，提升了扩散模型的稳健性和社会责任。

Abstract: The remarkable ability of diffusion models to generate high-fidelity images
has led to their widespread adoption. However, concerns have also arisen
regarding their potential to produce Not Safe for Work (NSFW) content and
exhibit social biases, hindering their practical use in real-world
applications. In response to this challenge, prior work has focused on
employing security filters to identify and exclude toxic text, or
alternatively, fine-tuning pre-trained diffusion models to erase sensitive
concepts. Unfortunately, existing methods struggle to achieve satisfactory
performance in the sense that they can have a significant impact on the normal
model output while still failing to prevent the generation of harmful content
in some cases. In this paper, we propose a novel self-discovery approach to
identifying a semantic direction vector in the embedding space to restrict text
embedding within a safe region. Our method circumvents the need for correcting
individual words within the input text and steers the entire text prompt
towards a safe region in the embedding space, thereby enhancing model
robustness against all possibly unsafe prompts. In addition, we employ Low-Rank
Adaptation (LoRA) for semantic direction vector initialization to reduce the
impact on the model performance for other semantics. Furthermore, our method
can also be integrated with existing methods to improve their social
responsibility. Extensive experiments on benchmark datasets demonstrate that
our method can effectively reduce NSFW content and mitigate social bias
generated by diffusion models compared to several state-of-the-art baselines.

</details>


### [89] [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/abs/2505.15428)
*Momose Oyama,Ryo Kishino,Hiroaki Yamagiwa,Hidetoshi Shimodaira*

Key words: 语言模型, KL散度, 重采样, 计算成本

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种重采样方法，通过选择重要性文本减少计算成本，保持KL散度估计精度。

Motivation: 解决构建模型映射时因依赖大量文本计算对数似然而导致的高计算成本问题。

Method: 提出一种重采样方法，根据每个文本在模型间对数似然的方差权重选择重要文本。

Result: 实验表明，该方法在文本数量减半的情况下仍达到均匀采样的性能，并能高效整合新模型。

Conclusion: 该方法实现了语言模型映射的可扩展和高效构建。

Abstract: We address the computational cost of constructing a model map, which embeds
diverse language models into a common space for comparison via KL divergence.
The map relies on log-likelihoods over a large text set, making the cost
proportional to the number of texts. To reduce this cost, we propose a
resampling method that selects important texts with weights proportional to the
variance of log-likelihoods across models for each text. Our method
significantly reduces the number of required texts while preserving the
accuracy of KL divergence estimates. Experiments show that it achieves
comparable performance to uniform sampling with about half as many texts, and
also facilitates efficient incorporation of new models into an existing map.
These results enable scalable and efficient construction of language model
maps.

</details>


### [90] [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)
*Ao Liu,Botong Zhou,Can Xu,Chayse Zhou,ChenChen Zhang,Chengcheng Xu,Chenhao Wang,Decheng Wu,Dengpeng Wu,Dian Jiao,Dong Du,Dong Wang,Feng Zhang,Fengzong Lian,Guanghui Xu,Guanwei Zhang,Hai Wang,Haipeng Luo,Han Hu,Huilin Xu,Jiajia Wu,Jianchen Zhu,Jianfeng Yan,Jiaqi Zhu,Jihong Zhang,Jinbao Xue,Jun Xia,Junqiang Zheng,Kai Liu,Kai Zhang,Kai Zheng,Kejiao Li,Keyao Wang,Lan Jiang,Lixin Liu,Lulu Wu,Mengyuan Huang,Peijie Yu,Peiqi Wang,Qian Wang,Qianbiao Xiang,Qibin Liu,Qingfeng Sun,Richard Guo,Ruobing Xie,Saiyong Yang,Shaohua Chen,Shihui Hu,Shuai Li,Shuaipeng Li,Shuang Chen,Suncong Zheng,Tao Yang,Tian Zhang,Tinghao Yu,Weidong Han,Weijie Liu,Weijin Zhou,Weikang Wang,Wesleye Chen,Xiao Feng,Xiaoqin Ren,Xingwu Sun,Xiong Kuang,Xuemeng Huang,Xun Cao,Yanfeng Chen,Yang Du,Yang Zhen,Yangyu Tao,Yaping Deng,Yi Shen,Yigeng Hong,Yiqi Chen,Yiqing Huang,Yuchi Deng,Yue Mao,Yulong Wang,Yuyuan Zeng,Zenan Xu,Zhanhui Kang,Zhe Zhao,ZhenXiang Yan,Zheng Fang,Zhichao Hu,Zhongzhi Chen,Zhuoyu Li,Zongwei Li,Alex Yan,Ande Liang,Baitong Liu,Beiping Pan,Bin Xing,Binghong Wu,Bingxin Qu,Bolin Ni,Boyu Wu,Chen Li,Cheng Jiang,Cheng Zhang,Chengjun Liu,Chengxu Yang,Chiyu Wang,Chong Zha,Daisy Yi,Di Wang,Fanyang Lu,Fei Chen,Feifei Liu,Feng Zheng,Guanghua Yu,Guiyang Li,Guohua Wang,Haisheng Lin,Han Liu,Han Wang,Hao Fei,Hao Lu,Haoqing Jiang,Haoran Sun,Haotian Zhu,Huangjin Dai,Huankui Chen,Huawen Feng,Huihui Cai,Huxin Peng,Jackson Lv,Jiacheng Shi,Jiahao Bu,Jianbo Li,Jianglu Hu,Jiangtao Guan,Jianing Xu,Jianwei Cai,Jiarong Zhang,Jiawei Song,Jie Jiang,Jie Liu,Jieneng Yang,Jihong Zhang,Jin lv,Jing Zhao,Jinjian Li,Jinxing Liu,Jun Zhao,Juntao Guo,Kai Wang,Kan Wu,Lei Fu,Lei He,Lei Wang,Li Liu,Liang Dong,Liya Zhan,Long Cheng,Long Xu,Mao Zheng,Meng Liu,Mengkang Hu,Nanli Chen,Peirui Chen,Peng He,Pengju Pan,Pengzhi Wei,Qi Yang,Qi Yi,Roberts Wang,Rongpeng Chen,Rui Sun,Rui Yang,Ruibin Chen,Ruixu Zhou,Shaofeng Zhang,Sheng Zhang,Shihao Xu,Shuaishuai Chang,Shulin Liu,SiQi Wang,Songjia Feng,Songling Yuan,Tao Zhang,Tianjiao Lang,Tongkai Li,Wei Deng,Wei Li,Weichao Wang,Weigang Zhang,Weixuan Sun,Wen Ouyang,Wenxiang Jiao,Wenzhi Sun,Wenzhuo Jia,Xiang Zhang,Xiangyu He,Xianshun Ren,XiaoYing Zhu,Xiaolong Guo,Xiaoxue Li,Xiaoyu Ma,Xican Lu,Xinhua Feng,Xinting Huang,Xinyu Guan,Xirui Li,Xu Zhang,Xudong Gao,Xun Luo,Xuxiang Qi,Yangkun Chen,Yangyu Tao,Yanling Xiao,Yantao Mai,Yanze Chen,Yao Ding,Yeting Yang,YiFan Song,Yifan Yang,Yijiao Zhu,Yinhe Wu,Yixian Liu,Yong Yang,Yuanjun Cai,Yuanlin Tu,Yue Zhang,Yufei Huang,Yuhang Zhou,Yuhao Jiang,Yuhong Liu,Yuhui Hu,Yujin Lin,Yun Yang,Yunhao Wang,Yusong Zhang,Zekun Wu,Zelong Zhang,Zhan Yu,Zhaoliang Yang,Zhe Zhao,Zheng Li,Zhenyu Huang,Zhiguang Liu,Zhijiang Xu,Zhiqing Kui,Zhiyin Zeng,Zhiyuan Xiong,Zhuo Han,Zifan Wu,Zigang Geng,Zilong Zhao,Ziyan Tang,Ziyuan Zhu,Zonglei Zhu,Zhijiang Xu*

Key words: LLM, Transformer, Mamba, MoE, 思维链, 强化学习

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: Hunyuan-TurboS是一款新型大型混合Transformer-Mamba MoE模型，结合Mamba的长序列处理效率和Transformer的上下文理解能力，通过自适应长短期思维链机制优化计算资源。

Motivation: 解决大型语言模型在长序列处理和计算效率上的挑战，结合Mamba和Transformer的优势，提升模型性能和效率。

Method: 采用56B激活参数（总560B）的混合架构（Mamba2、Attention、FFN），预训练16T高质量token，支持256K上下文长度，结合自适应长短期思维链和多阶段强化学习策略。

Result: 在LMSYS Chatbot Arena中排名前7，得分1356，超越多个领先模型，23个自动化基准测试平均得分77.9%。

Conclusion: Hunyuan-TurboS在性能和效率上实现平衡，为高效大规模预训练模型树立了新范式。

Abstract: As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,
a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It
synergistically combines Mamba's long-sequence processing efficiency with
Transformer's superior contextual understanding. Hunyuan-TurboS features an
adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching
between rapid responses for simple queries and deep "thinking" modes for
complex problems, optimizing computational resources. Architecturally, this 56B
activated (560B total) parameter model employs 128 layers (Mamba2, Attention,
FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear
complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE
structure. Pre-trained on 16T high-quality tokens, it supports a 256K context
length and is the first industry-deployed large-scale Mamba model. Our
comprehensive post-training strategy enhances capabilities via Supervised
Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,
Multi-round Deliberation Learning for iterative improvement, and a two-stage
Large-scale Reinforcement Learning process targeting STEM and general
instruction-following. Evaluations show strong performance: overall top 7 rank
on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like
Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves
an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances
high performance and efficiency, offering substantial capabilities at lower
inference costs than many reasoning models, establishing a new paradigm for
efficient large-scale pre-trained models.

</details>


### [91] [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)
*Suhas Kamasetty Ramesh,Ayan Sengupta,Tanmoy Chakraborty*

Key words: 知识蒸馏, 语言模型, 小模型, 任务专业性, 推理保真度

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文通过大规模实证分析，研究了知识蒸馏（KD）在不同规模语言模型（0.5B至7B参数）中的效果及其机制。结果显示，KD对小模型提升显著（平均10%，峰值22%），而对大模型效果有限（约1.3%），且教师模型性能对结果影响较小，任务专业性更为关键。此外，KD虽提高准确性，但可能损害推理保真度。

Motivation: 探索知识蒸馏（KD）在小模型中的有效性及其背后的机制，填补现有研究的空白。

Method: 在14个复杂推理任务上进行零样本设置的大规模实证和统计分析，涵盖0.5B至7B参数的模型，研究KD对学生模型性能的影响。

Result: KD显著提升小模型性能（平均10%，峰值22%），但对大模型效果微弱。教师任务专业性比整体性能更关键，且KD可能损害推理保真度。

Conclusion: KD对小模型效果显著，但需权衡准确性与推理保真度；教师任务专业性和信号设计是关键因素。

Abstract: Knowledge distillation (KD) is a key technique for compressing large language
models into smaller ones while preserving performance. Despite the recent
traction of KD research, its effectiveness for smaller language models (LMs)
and the mechanisms driving knowledge transfer remain underexplored. In this
work, we present the first large-scale empirical and statistical analysis of KD
across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks
in a zero-shot setting. Our findings reveal that KD can improve the average
performance of smaller models by up to $10\%$, with a peak task specific gain
of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger
models. Surprisingly, teacher performance has a minimal impact on student
outcomes, while teacher task expertise impacts KD effectiveness. A correlation
study indicates that smaller LMs benefit more from KD, whereas larger LMs show
diminished gains. Additionally, we uncover a misalignment between improvements
in student performance and reasoning fidelity, suggesting that while KD
enhances accuracy, it does not always maintain the structured decision-making
processes of the teacher. Our ablation study further highlights the importance
of teacher signals and logit smoothing in influencing students' performance
after distillation. Overall, our study offers a comprehensive empirical and
statistical assessment of KD, highlighting both its benefits and trade-offs
when distilling knowledge from larger to smaller LMs.

</details>


### [92] [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/abs/2505.15443)
*Artem Zabolotnyi,Roman Makarov,Mile Mitrovic,Polina Proskura,Oleg Travkin,Roman Alferov,Alexey Zaytsev*

Key words: 不确定性估计, 预训练语言模型, 参数高效微调, AdUE1, L2-SP正则化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了AdUE1方法，用于在参数高效微调下改善预训练语言模型的分类任务不确定性估计，通过最大函数的可微近似和L2-SP正则化，在多个数据集和模型上表现优于基线方法。

Motivation: 预训练语言模型在分类任务中的不确定性估计仍具挑战性，尤其是在参数高效微调（如适配器）下。研究旨在提升softmax基估计的效率和准确性。

Method: 采用最大函数的可微近似和L2-SP正则化，微调头权重并正则化模型，无需改动基础模型。

Result: 在五个NLP分类数据集和四种语言模型上，AdUE1方法均优于Mahalanobis距离和softmax响应等基线方法，提供更轻量且校准更好的置信度。

Conclusion: AdUE1是一种高效的后处理不确定性估计方法，显著提升了分类任务中模型的不确定性估计能力。

Abstract: Uncertainty estimation remains a critical challenge in adapting pre-trained
language models to classification tasks, particularly under parameter-efficient
fine-tuning approaches such as adapters. We introduce AdUE1, an efficient
post-hoc uncertainty estimation (UE) method, to enhance softmax-based
estimates. Our approach (1) uses a differentiable approximation of the maximum
function and (2) applies additional regularization through L2-SP, anchoring the
fine-tuned head weights and regularizing the model. Evaluations on five NLP
classification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,
Qwen) demonstrate that our method consistently outperforms established
baselines such as Mahalanobis distance and softmax response. Our approach is
lightweight (no base-model changes) and produces better-calibrated confidence.

</details>


### [93] [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)
*Yutao Zhu,Jiajie Jin,Hongjin Qian,Zheng Liu,Zhicheng Dou,Ji-Rong Wen*

Key words: 检索增强生成，多任务处理，角色特定优化，查询图，大语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了RoleRAG框架，通过角色特定令牌优化实现高效多任务处理，统一了检索增强生成（RAG）的子任务优化。

Motivation: 现有研究分别在检索增强生成的子任务上进行了优化，但缺乏将这些优化整合到统一框架中的方法。

Method: 提出RoleRAG框架，包含六个模块，每个模块处理RAG的一个子任务，并通过查询图动态分解问题。所有模块由同一个大语言模型驱动，但通过任务特定的角色令牌区分。

Result: 在五个开放域问答数据集上的实验结果表明，RoleRAG框架有效、通用且灵活。

Conclusion: RoleRAG框架简化了部署并减少了资源消耗，展示了在统一框架中处理RAG多任务的潜力。

Abstract: Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.

</details>


### [94] [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/abs/2505.15456)
*Weixiang Zhao,Xingyu Sui,Yulin Hu,Jiahe Guo,Haixiao Liu,Biye Li,Yanyan Zhao,Bing Qin,Ting Liu*

Key words: 个性化对齐, 强化学习, 大语言模型, 用户画像, 双奖励机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了基于强化学习的个性化对齐框架（RLPA），通过双奖励机制优化LLM在对话中的长期个性化表现，Qwen-RLPA在性能上超越了现有方法及商业模型。

Motivation: 现有静态方法在冷启动和长期个性化场景中表现不足，需动态优化用户对齐。

Method: RLPA框架通过模拟用户交互与双奖励（用户画像奖励和响应奖励）迭代优化用户画像与对话响应。

Result: Qwen-RLPA在个性化对话中超越提示学习、离线微调基线及Claude-3.5、GPT-4o等商业模型，且在处理冲突偏好和推理效率上表现优异。

Conclusion: 动态用户画像推断是构建个性化对话系统的更有效范式。

Abstract: Personalized alignment is essential for enabling large language models (LLMs)
to engage effectively in user-centric dialogue. While recent prompt-based and
offline optimization methods offer preliminary solutions, they fall short in
cold-start scenarios and long-term personalization due to their inherently
static and shallow designs. In this work, we introduce the Reinforcement
Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts
with a simulated user model to iteratively infer and refine user profiles
through dialogue. The training process is guided by a dual-level reward
structure: the Profile Reward encourages accurate construction of user
representations, while the Response Reward incentivizes generation of responses
consistent with the inferred profile. We instantiate RLPA by fine-tuning
Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art
performance in personalized dialogue. Empirical evaluations demonstrate that
Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,
and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.
Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting
user preferences, sustaining long-term personalization and delivering more
efficient inference compared to recent reasoning-focused LLMs. These results
emphasize the potential of dynamic profile inference as a more effective
paradigm for building personalized dialogue systems.

</details>


### [95] [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)
*Yukun Zhao,Lingyong Yan,Zhenyang Li,Shuaiqiang Wang,Zhumin Chen,Zhaochun Ren,Dawei Yin*

Key words: 大语言模型、增量学习、灾难性遗忘、Joint Flashback Adaptation

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为Joint Flashback Adaptation的方法，通过旧任务的少量提示（flashbacks）和模型输出的约束，结合中间潜在任务的插值，有效解决大语言模型增量学习中的灾难性遗忘问题。

Motivation: 大语言模型在增量学习新任务时存在灾难性遗忘问题，现有方法在真实场景中有严格限制。

Method: Joint Flashback Adaptation：引入少量旧任务提示（flashbacks）并约束模型输出偏差，插值潜在任务以联合学习相关任务、新任务和flashbacks。

Result: 实验表明，该方法在1000+任务中显著提升新任务的泛化能力并减少旧任务的遗忘。

Conclusion: Joint Flashback Adaptation无需重放数据，仅需少量flashbacks即可实现高效增量学习。

Abstract: Large language models have achieved remarkable success in various tasks.
However, it is challenging for them to learn new tasks incrementally due to
catastrophic forgetting. Existing approaches rely on experience replay,
optimization constraints, or task differentiation, which encounter strict
limitations in real-world scenarios. To address these issues, we propose Joint
Flashback Adaptation. We first introduce flashbacks -- a limited number of
prompts from old tasks -- when adapting to new tasks and constrain the
deviations of the model outputs compared to the original one. We then
interpolate latent tasks between flashbacks and new tasks to enable jointly
learning relevant latent tasks, new tasks, and flashbacks, alleviating data
sparsity in flashbacks and facilitating knowledge sharing for smooth
adaptation. Our method requires only a limited number of flashbacks without
access to the replay data and is task-agnostic. We conduct extensive
experiments on state-of-the-art large language models across 1000+
instruction-following tasks, arithmetic reasoning tasks, and general reasoning
tasks. The results demonstrate the superior performance of our method in
improving generalization on new tasks and reducing forgetting in old tasks.

</details>


### [96] [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/abs/2505.15471)
*Yiyun Zhou,Chang Yao,Jingyuan Chen*

Key words: 大语言模型, 微调, LoRA, 多任务学习, CoLA

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为CoLA的新型LoRA架构，通过灵活的初始化和协作策略提升了多任务场景下的性能，尤其在低样本情况下表现优越。

Motivation: 现有的参数高效微调方法（如LoRA）在多任务场景中因任务间干扰而受限，近期的方法（如MOE和非对称LoRA）仍受限于样本稀缺和噪声干扰。

Method: 提出CoLA架构，采用高效初始化方案和三种协作策略，更好地利用矩阵A和B之间的定量关系。

Result: 实验证明CoLA在低样本场景下表现优越，超越了现有PEFT方法。

Conclusion: CoLA是一种灵活且高效的LoRA改进架构，适用于多任务场景，尤其是在数据稀缺的情况下。

Abstract: The scaling law of Large Language Models (LLMs) reveals a power-law
relationship, showing diminishing return on performance as model scale
increases. While training LLMs from scratch is resource-intensive, fine-tuning
a pre-trained model for specific tasks has become a practical alternative. Full
fine-tuning (FFT) achieves strong performance; however, it is computationally
expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like
LoRA, have been proposed to address these challenges by freezing the
pre-trained model and adding lightweight task-specific modules. LoRA, in
particular, has proven effective, but its application to multi-task scenarios
is limited by interference between tasks. Recent approaches, such as
Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these
issues but still struggle with sample scarcity and noise interference due to
their fixed structure. In response, we propose CoLA, a more flexible LoRA
architecture with an efficient initialization scheme, and introduces three
collaborative strategies to enhance performance by better utilizing the
quantitative relationships between matrices $A$ and $B$. Our experiments
demonstrate the effectiveness and robustness of CoLA, outperforming existing
PEFT methods, especially in low-sample scenarios. Our data and code are fully
publicly available at https://github.com/zyy-2001/CoLA.

</details>


### [97] [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/abs/2505.15472)
*Song Dai,Yibo Yan,Jiamin Su,Dongfang Zihao,Yubo Gao,Yonghua Hei,Jungang Li,Junyan Zhang,Sicheng Tao,Zhuoran Gao,Xuming Hu*

Key words: MLLMs, 多模态, 物理推理, 基准

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PhysicsArena是一个多模态物理推理基准，旨在全面评估MLLMs在变量识别、物理过程制定和解决方案推导方面的能力。

Motivation: 现有物理推理基准局限于文本输入或问题解决，缺乏多模态信息和中间步骤的评估。

Method: 引入了PhysicsArena，通过三个关键维度（变量识别、物理过程制定和解决方案推导）评估MLLMs。

Result: PhysicsArena为MLLMs的多模态物理推理能力提供了全面评估平台。

Conclusion: 该基准填补了MLLMs在复杂物理推理中的研究空白。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in diverse reasoning tasks, yet their application to complex
physics reasoning remains underexplored. Physics reasoning presents unique
challenges, requiring grounding in physical conditions and the interpretation
of multimodal information. Current physics benchmarks are limited, often
focusing on text-only inputs or solely on problem-solving, thereby overlooking
the critical intermediate steps of variable identification and process
formulation. To address these limitations, we introduce PhysicsArena, the first
multimodal physics reasoning benchmark designed to holistically evaluate MLLMs
across three critical dimensions: variable identification, physical process
formulation, and solution derivation. PhysicsArena aims to provide a
comprehensive platform for assessing and advancing the multimodal physics
reasoning abilities of MLLMs.

</details>


### [98] [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)
*Zhanyue Qin,Yue Ding,Deyuan Liu,Qingbin Liu,Junxian Cai,Xi Chen,Zhiying Tu,Dianhui Chu,Cuiyun Gao,Dianbo Sui*

Key words: Large Language Models, gender bias, GenBiasEval, GenHintEval, LFTF algorithm

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出两个数据集（GenBiasEval和GenHintEval）及LFTF算法，用于评估和减少LLMs中的性别偏见。

Motivation: LLMs在训练过程中接触社会偏见数据，导致出现性别偏见，需要有效评估和缓解方法。

Method: 提出两个数据集（GenBiasEval和GenHintEval）和相关评分指标（AFGB-Score和UB-Score），以及LFTF算法（通过BMI评分排序并微调相关模块）。

Result: 实验证明LFTF算法能显著减少LLMs的性别偏见，同时保持其通用能力。

Conclusion: 提出的数据集和算法有效解决了LLMs中的性别偏见问题。

Abstract: Nowadays, Large Language Models (LLMs) have attracted widespread attention
due to their powerful performance. However, due to the unavoidable exposure to
socially biased data during training, LLMs tend to exhibit social biases,
particularly gender bias. To better explore and quantifying the degree of
gender bias in LLMs, we propose a pair of datasets named GenBiasEval and
GenHintEval, respectively. The GenBiasEval is responsible for evaluating the
degree of gender bias in LLMs, accompanied by an evaluation metric named
AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is
used to assess whether LLMs can provide responses consistent with prompts that
contain gender hints, along with the accompanying evaluation metric UB-Score
(UnBias Score). Besides, in order to mitigate gender bias in LLMs more
effectively, we present the LFTF (Locating First and Then Fine-Tuning)
algorithm.The algorithm first ranks specific LLM blocks by their relevance to
gender bias in descending order using a metric called BMI (Block Mitigating
Importance Score). Based on this ranking, the block most strongly associated
with gender bias is then fine-tuned using a carefully designed loss function.
Numerous experiments have shown that our proposed LFTF algorithm can
significantly mitigate gender bias in LLMs while maintaining their general
capabilities.

</details>


### [99] [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
*Qihuang Zhong,Liang Ding,Xiantao Cai,Juhua Liu,Bo Du,Dacheng Tao*

Key words: 监督微调,知识冲突,大型语言模型,KaFT,问答任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为KaFT的知识感知微调方法，通过根据冲突级别调整训练权重，有效提升了大型语言模型在领域特定问答任务中的性能。

Motivation: 现有监督微调方法在处理训练数据与模型内部知识的冲突时表现不佳，导致性能下降。本文旨在分析知识冲突的影响并提出解决方案。

Method: 设计了查询多样化策略以检测冲突，并提出了KaFT方法，根据冲突级别为训练样本分配不同奖励以调整训练权重。

Result: 实验表明，KaFT在四种大型语言模型上均带来了显著且一致的性能提升，同时改善了模型的泛化能力并减少了幻觉现象。

Conclusion: KaFT通过有效利用冲突数据，显著提高了模型性能，为知识冲突问题提供了简单而高效的解决方案。

Abstract: Supervised fine-tuning (SFT) is a common approach to improve the
domain-specific question-answering (QA) performance of large language models
(LLMs). However, recent literature reveals that due to the conflicts between
LLMs' internal knowledge and the context knowledge of training data, vanilla
SFT using the full QA training set is usually suboptimal. In this paper, we
first design a query diversification strategy for robust conflict detection and
then conduct a series of experiments to analyze the impact of knowledge
conflict. We find that 1) training samples with varied conflicts contribute
differently, where SFT on the data with large conflicts leads to catastrophic
performance drops; 2) compared to directly filtering out the conflict data,
appropriately applying the conflict data would be more beneficial. Motivated by
this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely
KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to
adapt the training weight by assigning different rewards for different training
samples according to conflict level. Extensive experiments show that KaFT
brings consistent and significant improvements across four LLMs. More analyses
prove that KaFT effectively improves the model generalization and alleviates
the hallucination.

</details>


### [100] [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/abs/2505.15490)
*Isidora Jeknic,Alex Duchnowski,Alexander Koller*

Key words: 对话代理，旅行商问题，LLM，协作，符号机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种新对话游戏，用于协作解决双人旅行商问题，结合了LLM提示与符号机制，最优解决率45%，并能与人类用户成功协作及泛化至陌生图。

Motivation: 支持人类用户解决复杂任务的对话代理受到广泛关注，旅行商问题等NP难问题需要协作探索解空间。

Method: 设计了新对话游戏，结合LLM提示与符号机制进行状态跟踪和基础支持。

Result: 最优解决率为45%，并能与人类用户协作，泛化至陌生图。

Conclusion: 提出的方法在协作解决复杂问题上表现出色，具有实际应用潜力。

Abstract: Dialogue agents that support human users in solving complex tasks have
received much attention recently. Many such tasks are NP-hard optimization
problems that require careful collaborative exploration of the solution space.
We introduce a novel dialogue game in which the agents collaboratively solve a
two-player Traveling Salesman problem, along with an agent that combines LLM
prompting with symbolic mechanisms for state tracking and grounding. Our best
agent solves 45% of games optimally in self-play. It also demonstrates an
ability to collaborate successfully with human users and generalize to
unfamiliar graphs.

</details>


### [101] [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)
*Federico Ranaldi,Andrea Zugarini,Leonardo Ranaldi,Fabio Massimo Zanzotto*

Key words: protoknowledge, LLMs, Knowledge Activation Tasks, Text-to-SPARQL, Semantic-Level Data Contamination

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了“protoknowledge”的概念，用于形式化和衡量大型语言模型（LLMs）在预训练期间如何内化知识图谱的令牌序列，并在推理时利用这些知识。

Motivation: 研究LLMs如何通过泛化利用预训练期间记忆的大量令牌序列作为可重用知识。

Method: 定义了lexical、hierarchical和topological三种protoknowledge类别，并通过知识激活任务（KATs）测量其特征。采用新框架分析protoknowledge对Text-to-SPARQL性能的影响。

Result: 提供了衡量语义偏见等protoknowledge特性的方法，并分析了不同提示策略对性能的影响。

Conclusion: 该框架是探索语义级数据污染的有效工具，适用于闭式预训练模型。

Abstract: We introduce the concept of protoknowledge to formalize and measure how
sequences of tokens encoding Knowledge Graphs are internalized during
pretraining and utilized at inference time by Large Language Models (LLMs).
Indeed, LLMs have demonstrated the ability to memorize vast amounts of token
sequences during pretraining, and a central open question is how they leverage
this memorization as reusable knowledge through generalization. We then
categorize protoknowledge into lexical, hierarchical, and topological forms,
varying on the type of knowledge that needs to be activated. We measure
protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general
properties such as semantic bias. We then investigate the impact of
protoknowledge on Text-to-SPARQL performance by varying prompting strategies
depending on input conditions. To this end, we adopt a novel analysis framework
that assesses whether model predictions align with the successful activation of
the relevant protoknowledge for each query. This methodology provides a
practical tool to explore Semantic-Level Data Contamination and serves as an
effective strategy for Closed-Pretraining models.

</details>


### [102] [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/abs/2505.15508)
*Prasoon Bajpai,Tanmoy Chakraborty*

Key words: 测试时缩放, 多语言推理, MITT, 低资源语言

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文首次系统研究了多语言环境下的测试时缩放效果，揭示了不同语言间的性能差异，并提出了一种名为MITT的无监督前缀调优方法，显著提升了低资源语言的推理性能。

Motivation: 测试时缩放虽广泛用于提升推理性能，但其在多语言环境中的行为尚不明确。

Method: 评估了DeepSeek-R1-Distill-LLama-8B和DeepSeek-R1-Distill-Qwen-7B在高、低资源拉丁语系语言中的表现，并提出MITT方法。

Result: 不同语言的测试时缩放增益差异显著，MITT显著提升了低资源语言的性能。

Conclusion: MITT能够有效解决多语言推理中的不一致性，尤其是在低资源语言中表现突出。

Abstract: Test-time scaling has emerged as a widely adopted inference-time strategy for
boosting reasoning performance. However, its effectiveness has been studied
almost exclusively in English, leaving its behavior in other languages largely
unexplored. We present the first systematic study of test-time scaling in
multilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and
DeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script
languages. Our findings reveal that the relative gains from test-time scaling
vary significantly across languages. Additionally, models frequently switch to
English mid-reasoning, even when operating under strictly monolingual prompts.
We further show that low-resource languages not only produce initial reasoning
thoughts that differ significantly from English but also have lower internal
consistency across generations in their early reasoning. Building on our
findings, we introduce MITT (Multilingual Initial Thought Transfer), an
unsupervised and lightweight reasoning prefix-tuning approach that transfers
high-resource reasoning prefixes to enhance test-time scaling across all
languages, addressing inconsistencies in multilingual reasoning performance.
MITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance,
especially for underrepresented languages.

</details>


### [103] [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)
*Lang Gao,Kaiyang Wan,Wei Liu,Chenxi Wang,Zirui Song,Zixiang Xu,Yanbo Wang,Veselin Stoyanov,Xiuying Chen*

Key words: 大型语言模型, 偏见分析, 概念激活向量, 稀疏自编码器, 公平性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出BiasLens框架，无标注数据即可分析LLM中的偏见，通过向量空间结构量化偏差，展示出与传统方法高一致性。

Motivation: 解决大型语言模型（LLM）中因概念空间不对称关联导致的偏见问题，传统方法依赖标注数据且覆盖有限。

Method: 结合概念激活向量（CAVs）与稀疏自编码器（SAEs），提取可解释概念表示，通过表示相似性变化量化偏见。

Result: BiasLens与传统偏见评测指标高度一致（Spearman相关性r>0.85），并能揭示传统方法难以检测的偏见。

Conclusion: BiasLens为偏见发现提供可扩展、可解释且高效的范式，有助于提升LLM的公平性与透明度。

Abstract: Bias in Large Language Models (LLMs) significantly undermines their
reliability and fairness. We focus on a common form of bias: when two reference
concepts in the model's concept space, such as sentiment polarities (e.g.,
"positive" and "negative"), are asymmetrically correlated with a third, target
concept, such as a reviewing aspect, the model exhibits unintended bias. For
instance, the understanding of "food" should not skew toward any particular
sentiment. Existing bias evaluation methods assess behavioral differences of
LLMs by constructing labeled data for different social groups and measuring
model responses across them, a process that requires substantial human effort
and captures only a limited set of social concepts. To overcome these
limitations, we propose BiasLens, a test-set-free bias analysis framework based
on the structure of the model's vector space. BiasLens combines Concept
Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract
interpretable concept representations, and quantifies bias by measuring the
variation in representational similarity between the target concept and each of
the reference concepts. Even without labeled data, BiasLens shows strong
agreement with traditional bias evaluation metrics (Spearman correlation r >
0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect
using existing methods. For example, in simulated clinical scenarios, a
patient's insurance status can cause the LLM to produce biased diagnostic
assessments. Overall, BiasLens offers a scalable, interpretable, and efficient
paradigm for bias discovery, paving the way for improving fairness and
transparency in LLMs.

</details>


### [104] [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)
*Angelie Kraft,Judith Simon,Sonja Schimmler*

Key words: QA基准测试，阅读理解，社会偏见，多样性，大型语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究表明，流行的问答（QA）和阅读理解（RC）基准测试存在偏见，且未能公平覆盖不同人群或地区的问题，根源在于创建过程中缺乏多样性。通过对30篇论文和20个数据集的分析，发现大多数基准测试未透明披露创建者信息，且仅一篇明确尝试解决社会代表性偏见。建议需要更透明且注重偏见的基准测试创建实践。

Motivation: 评估大型语言模型（LLMs）的能力依赖于QA和RC基准测试，但现有基准测试可能存在偏见，未能公平代表不同人群或地区，这可能导致模型表现的不公平性。

Method: 对30篇基准测试论文进行定性内容分析，并对20个相关数据集进行定量分析，重点关注创建者多样性、社会偏见处理方式以及创建者人口统计学特征与内容偏见之间的关系。

Result: 大多数基准测试论文未充分披露创建者信息，仅有1篇论文明确报告了解决社会代表性问题的措施。数据分析显示，基准测试中存在性别、宗教和地理偏见。

Conclusion: 需要更透明且注重偏见的基准测试创建实践，以确保模型开发的公平性和可审查性。

Abstract: Question-answering (QA) and reading comprehension (RC) benchmarks are
essential for assessing the capabilities of large language models (LLMs) in
retrieving and reproducing knowledge. However, we demonstrate that popular QA
and RC benchmarks are biased and do not cover questions about different
demographics or regions in a representative way, potentially due to a lack of
diversity of those involved in their creation. We perform a qualitative content
analysis of 30 benchmark papers and a quantitative analysis of 20 respective
benchmark datasets to learn (1) who is involved in the benchmark creation, (2)
how social bias is addressed or prevented, and (3) whether the demographics of
the creators and annotators correspond to particular biases in the content.
Most analyzed benchmark papers provided insufficient information regarding the
stakeholders involved in benchmark creation, particularly the annotators.
Notably, just one of the benchmark papers explicitly reported measures taken to
address social representation issues. Moreover, the data analysis revealed
gender, religion, and geographic biases across a wide range of encyclopedic,
commonsense, and scholarly benchmarks. More transparent and bias-aware QA and
RC benchmark creation practices are needed to facilitate better scrutiny and
incentivize the development of fairer LLMs.

</details>


### [105] [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)
*Wendi Zhou,Ameer Saadat-Yazdi,Nadin Kökciyan*

Key words: 关键性问题生成，大型语言模型，链式思维提示，沃尔顿论证方案，批判性思维

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 摘要介绍了使用大型语言模型（LLMs）结合链式思维提示和沃尔顿论证方案，生成针对论辩文本的关键性问题。系统表现优异，有助于激发批判性思维和检测缺失论点。

Motivation: 关键性问题能激发批判性思维，本文开发了一种自动生成此类问题的方法。

Method: 利用LLMs和链式思维提示，基于沃尔顿论证方案生成结构化论辩，并筛选出最相关的3个问题。

Result: 系统在测试集中表现优异，可有效识别缺失论点。

Conclusion: 结合结构化论辩理论和分步推理，系统能生成相关且多样的问题，促进批判性思维。

Abstract: Critical questions are essential resources to provoke critical thinking when
encountering an argumentative text. We present our system for the Critical
Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach
leverages large language models (LLMs) with chain-of-thought prompting to
generate critical questions guided by Walton's argumentation schemes. For each
input intervention, we conversationally prompt LLMs to instantiate the
corresponding argument scheme template to first obtain structured arguments,
and then generate relevant critical questions. Following this, we rank all the
available critical questions by prompting LLMs to select the top 3 most helpful
questions based on the original intervention text. This combination of
structured argumentation theory and step-by-step reasoning enables the
generation of contextually relevant and diverse critical questions. Our
pipeline achieves competitive performance in the final test set, showing its
potential to foster critical thinking given argumentative text and detect
missing or uninformed claims. Code available at
\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.

</details>


### [106] [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/abs/2505.15556)
*Ana-Maria Bucur,Marcos Zampieri,Tharindu Ranasinghe,Fabio Crestani*

Key words: 心理健康, 多语言, 社交媒体, NLP, 文化差异

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文探讨了利用多语言社交媒体数据检测心理健康障碍的研究，强调了现有研究多以英文数据为主，忽略了其他语言中的心理健康信号，并提出了多语言数据收集的全面列表。

Motivation: 全球心理健康问题的增加需要有效的多语言数字筛查方法，但现有研究多集中于英文数据，未能捕捉其他语言中的关键信号。

Method: 通过调查多语言社交媒体数据中的文化差异对语言模式和自我披露行为的影响，研究如何优化NLP工具的性能。

Result: 提出了第一个关于多语言心理健康障碍检测的调查，并提供了可用于开发NLP模型的多语言数据集。

Conclusion: 研究为设计有效的多语言心理健康筛查工具提供了重要依据，有助于改善全球心理健康状况。

Abstract: The increasing prevalence of mental health disorders globally highlights the
urgent need for effective digital screening methods that can be used in
multilingual contexts. Most existing studies, however, focus on English data,
overlooking critical mental health signals that may be present in non-English
texts. To address this important gap, we present the first survey on the
detection of mental health disorders using multilingual social media data. We
investigate the cultural nuances that influence online language patterns and
self-disclosure behaviors, and how these factors can impact the performance of
NLP tools. Additionally, we provide a comprehensive list of multilingual data
collections that can be used for developing NLP models for mental health
screening. Our findings can inform the design of effective multilingual mental
health screening tools that can meet the needs of diverse populations,
ultimately improving mental health outcomes on a global scale.

</details>


### [107] [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)
*Florin Cuconasu,Simone Filice,Guy Horowitz,Yoelle Maarek,Fabrizio Silvestri*

Key words: 检索增强生成、位置偏见、大型语言模型、干扰性段落

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文研究了检索增强生成中位置偏见对LLM性能的影响，发现实际场景中由于相关和干扰性段落并存，位置偏见的实际影响较小。

Motivation: 研究位置偏见如何影响LLM在检索增强生成中对相关段落的利用及其对干扰性段落的敏感性。

Method: 通过在三个基准上进行广泛实验，分析检索管道返回的段落及其对LLM输出的影响。

Result: 研究发现，检索管道在检索相关段落时往往会引入高干扰性段落，60%的查询在top-10段落中包含至少一个高干扰性段落，而LLM的位置偏见在实际场景中影响有限。

Conclusion: 实际场景中，由于相关和干扰性段落并存，位置偏见的影响被削弱，试图根据位置偏好重新排列段落的复杂策略并不优于随机排列。

Abstract: Retrieval Augmented Generation enhances LLM accuracy by adding passages
retrieved from an external corpus to the LLM prompt. This paper investigates
how positional bias - the tendency of LLMs to weight information differently
based on its position in the prompt - affects not only the LLM's capability to
capitalize on relevant passages, but also its susceptibility to distracting
passages. Through extensive experiments on three benchmarks, we show how
state-of-the-art retrieval pipelines, while attempting to retrieve relevant
passages, systematically bring highly distracting ones to the top ranks, with
over 60% of queries containing at least one highly distracting passage among
the top-10 retrieved passages. As a result, the impact of the LLM positional
bias, which in controlled settings is often reported as very prominent by
related works, is actually marginal in real scenarios since both relevant and
distracting passages are, in turn, penalized. Indeed, our findings reveal that
sophisticated strategies that attempt to rearrange the passages based on LLM
positional preferences do not perform better than random shuffling.

</details>


### [108] [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/abs/2505.15563)
*Mohammad Ali,Naeemul Hassan*

Key words: SUFA, 语义关系, 无监督学习, 框架分析, 新闻媒体

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: SUFA是一种基于语义关系和无监督学习的新方法，用于新闻媒体中的实体焦点框架分析，利用依赖解析算法，展示了在枪支暴力数据集上的潜力。

Motivation: 提出SUFA的目的是为了通过语义关系和无监督学习方法改进传统的计算框架分析方法，特别是针对新闻媒体中的实体焦点框架。

Method: 结合语义关系和依赖解析算法，通过定性及计算研究验证方法在枪支暴力数据集上的有效性。

Result: SUFA能够有效识别和分析实体焦点框架，展示了其在社会科学和计算领域的广泛应用潜力。

Conclusion: SUFA为计算框架分析提供了显著的方法学进步，具有跨学科应用的潜力。

Abstract: This research presents a novel approach to computational framing analysis,
called Semantic Relations-based Unsupervised Framing Analysis (SUFA). SUFA
leverages semantic relations and dependency parsing algorithms to identify and
assess entity-centric emphasis frames in news media reports. This innovative
method is derived from two studies -- qualitative and computational -- using a
dataset related to gun violence, demonstrating its potential for analyzing
entity-centric emphasis frames. This article discusses SUFA's strengths,
limitations, and application procedures. Overall, the SUFA approach offers a
significant methodological advancement in computational framing analysis, with
its broad applicability across both the social sciences and computational
domains.

</details>


### [109] [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)
*David Dinucu-Jianu,Jakub Macina,Nico Daheim,Ido Hakimi,Iryna Gurevych,Mrinmaya Sachan*

Key words: 大语言模型, 强化学习, 教学优化, Pareto前沿, 控制奖励

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的对齐框架，优化大语言模型作为教育辅助工具，强调教学质量和引导式学习而非直接回答问题。

Motivation: 当前大语言模型在回答问题时直接给出答案，可能削弱教学效果，需要改进其教学策略。

Method: 采用在线强化学习框架，通过模拟师生互动训练模型，平衡教学支持和学生解题准确性。

Result: 训练出的7B参数模型性能接近LearnLM等大型专有模型，且在保留推理能力和可解释性方面表现更优。

Conclusion: 该方法有效提升了语言模型的教学质量，同时保持了其推理能力，为教育领域提供了新的可能性。

Abstract: Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.

</details>


### [110] [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)
*Wei Liu,Ruochen Zhou,Yiyun Deng,Yuzhen Huang,Junteng Liu,Yuntian Deng,Yizhe Zhang,Junxian He*

Key words: 大型推理模型,强化学习,奖励塑造,推理效率,动态难度感知

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种基于强化学习的长度奖励塑造方法LASER，并通过动态和难度感知的扩展LASER-D，显著提升了大型推理模型的效率和性能。

Motivation: 大型推理模型在生成长推理链时常伴随冗余，限制了效率，因此需要优化推理效率的方法。

Method: 提出LASER框架，基于长度奖励塑造；进一步扩展为动态和难度感知的LASER-D，适应模型训练中的行为变化和不同难度查询的需求。

Result: 实验显示LASER-D在性能上提升6.1%（AIME2024指标），同时降低63%的token使用，生成更简洁的推理模式。

Conclusion: 基于强化学习的长度奖励塑造方法能有效平衡推理性能与效率，动态和难度感知进一步优化了模型表现。

Abstract: Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant "self-reflections". Resources are at
https://github.com/hkust-nlp/Laser.

</details>


### [111] [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)
*Tiasa Singha Roy,Aditeya Baral,Ayush Rajesh Jhaveri,Yusuf Baig*

Key words: 大型语言模型, 数学推理, 评估框架, MAPLE分数

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型（LLMs）在数学推理中的表现，并提出了一种新的评估框架——MAPLE分数，用于全面量化推理的偏差。

Motivation: 当前评估框架仅基于最终答案的准确性来衡量LLMs在数学推理中的表现，忽视了多步逻辑执行的精确性，因此需要一种更全面的评估方法。

Method: 提出了一个名为MAPLE分数的评估指标，结合错误率、冗余性和有效性，以全面衡量推理的偏差。

Result: MAPLE分数能够更全面地评估LLMs在数学推理中的表现，揭示了传统评估方法的局限性。

Conclusion: 通过MAPLE分数，可以更准确地评估和改进LLMs在复杂逻辑任务中的表现。

Abstract: Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.

</details>


### [112] [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)
*David Thulke,Jakob Kemmler,Christian Dugast,Hermann Ney*

Key words: 检索增强生成、气候科学、语言模型、忠实度评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文研究了如何通过检索增强生成技术提升语言模型在气候科学领域的准确性，并通过自动评估方法优化模型的忠实度。

Motivation: 气候相关的技术文献通常冗长且复杂，检索增强生成技术可以帮助研究人员、政策制定者和公众更易获取这些知识。然而，模型的输出是否忠实于检索到的内容至关重要。

Method: 研究采用了自动评估方法来检测模型的忠实度，并针对ClimateGPT模型，通过剔除不忠实的训练数据子集开发了ClimateGPT Faithful+。

Result: 优化后的ClimateGPT Faithful+在支持的原子主张中的忠实度从30%提升至57%。

Conclusion: 通过过滤不忠实的训练数据，可以有效提升模型在气候科学领域的输出忠实度。

Abstract: Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.

</details>


### [113] [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
*Zihao Li,Xu Wang,Yuzhe Yang,Ziyu Yao,Haoyi Xiong,Mengnan Du*

Key words: 大语言模型, 链式思维, 稀疏自编码器, 推理能力, 导向技术

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了两种无需外部数据集的LLM推理增强方法：基于稀疏自编码器（SAE）和SAE-free的导向技术，均显著提升了模型性能。

Motivation: 提升大语言模型（LLM）解决复杂推理和数学问题的能力，同时避免对高成本长链思维（CoT）数据和微调的依赖。

Method: 1. 基于稀疏自编码器（SAE）从普通CoT中提取可解释特征，用于生成时导向LLM内部状态；2. 提出SAE-free算法，直接从LLM的残差激活中计算导向方向。

Result: 实验证明，两种方法均显著提升了LLM的推理能力。

Conclusion: 提出的导向技术是一种高效且无需外部数据的LLM推理增强方案。

Abstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.

</details>


### [114] [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/abs/2505.15646)
*Ke Hu,Krishna Puvvada,Elena Rastorgueva,Zhehuai Chen,He Huang,Shuoyang Ding,Kunal Dhawan,Hainan Xu,Jagadeesh Balam,Boris Ginsburg*

Key words: 时间戳预测, Canary模型, 数据驱动, 语音处理, 自动语音翻译

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出一种数据驱动方法，直接在Canary模型中预测词级时间戳，消除了外部对齐模块的需求，并在多语言和自动语音翻译任务中取得高精度。

Motivation: 词级时间戳对于语音内容检索和定时字幕等任务至关重要。传统方法需外部对齐模块，而本方法直接嵌入时间戳预测功能。

Method: 利用NeMo Forced Aligner（NFA）作为教师模型生成时间戳数据，并在Canary模型中引入<|timestamp|>标记，直接预测词级时间戳。

Result: 在四种语言中，时间戳预测的准确率和召回率达80%-90%，误差为20-120毫秒；在自动语音翻译任务中误差约200毫秒。

Conclusion: 该方法有效简化了时间戳预测流程，并保持了高精度和低词错误率。

Abstract: We introduce a data-driven approach for enabling word-level timestamp
prediction in the Canary model. Accurate timestamp information is crucial for a
variety of downstream tasks such as speech content retrieval and timed
subtitles. While traditional hybrid systems and end-to-end (E2E) models may
employ external modules for timestamp prediction, our approach eliminates the
need for separate alignment mechanisms. By leveraging the NeMo Forced Aligner
(NFA) as a teacher model, we generate word-level timestamps and train the
Canary model to predict timestamps directly. We introduce a new <|timestamp|>
token, enabling the Canary model to predict start and end timestamps for each
word. Our method demonstrates precision and recall rates between 80% and 90%,
with timestamp prediction errors ranging from 20 to 120 ms across four
languages, with minimal WER degradation. Additionally, we extend our system to
automatic speech translation (AST) tasks, achieving timestamp prediction errors
around 200 milliseconds.

</details>


### [115] [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656)
*Zhexin Zhang,Yuhao Sun,Junxiao Yang,Shiyao Cui,Hongning Wang,Minlie Huang*

Key words: LLMs, fine-tuning, data breaching, backdoor training

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究发现，通过简单的后门训练，开源大语言模型（LLMs）的创建者可以从下游开发者微调后的模型中提取私有数据，且在实验中提取成功率高达94.9%。

Motivation: 揭示开源LLMs微调过程中存在的数据泄露风险，促使学术界关注并寻求解决方案。

Method: 通过后门训练和黑盒访问微调后的下游模型，实验在4种开源LLMs和2个数据集上进行。

Result: 在实验中，成功提取了76.3%（实际设置）到94.9%（理想设置）的下游微调数据。

Conclusion: 强调了数据泄露风险的紧迫性，呼吁更多研究解决这一问题。

Abstract: Fine-tuning on open-source Large Language Models (LLMs) with proprietary data
is now a standard practice for downstream developers to obtain task-specific
LLMs. Surprisingly, we reveal a new and concerning risk along with the
practice: the creator of the open-source LLMs can later extract the private
downstream fine-tuning data through simple backdoor training, only requiring
black-box access to the fine-tuned downstream model. Our comprehensive
experiments, across 4 popularly used open-source models with 3B to 32B
parameters and 2 downstream datasets, suggest that the extraction performance
can be strikingly high: in practical settings, as much as 76.3% downstream
fine-tuning data (queries) out of a total 5,000 samples can be perfectly
extracted, and the success rate can increase to 94.9% in more ideal settings.
We also explore a detection-based defense strategy but find it can be bypassed
with improved attack. Overall, we highlight the emergency of this newly
identified data breaching risk in fine-tuning, and we hope that more follow-up
research could push the progress of addressing this concerning risk. The code
and data used in our experiments are released at
https://github.com/thu-coai/Backdoor-Data-Extraction.

</details>


### [116] [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
*Ke Hu,Ehsan Hosseini-Asl,Chen Chen,Edresson Casanova,Subhankar Ghosh,Piotr Żelasko,Zhehuai Chen,Jason Li,Jagadeesh Balam,Boris Ginsburg*

Key words: 双工语音到语音（S2S）, 实时打断, 通道融合, 低比特率, 语音建模

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种创新的双工语音到语音（S2S）架构，支持实时用户打断（barge-in），并通过通道融合直接建模用户和代理的同时语音流，显著降低了比特率和所需语音数据量。

Motivation: 当前语音语言模型多限于回合制交互，缺乏实时适应性（如用户打断）。本文旨在解决这一问题。

Method: 采用预训练流式编码器处理用户输入，分离用户和代理建模架构，并结合通道融合技术。

Result: 模型在推理、轮转和打断能力上优于先前模型，比特率减半（0.6 kbps），且无需语音预训练。

Conclusion: 该模型简化了从LLMs构建双工S2S模型的过程，并首次公开了训练和推理代码。

Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet
current speech language models often remain constrained to turn-based
exchanges, lacking real-time adaptability such as user barge-in. We propose a
novel duplex speech to speech (S2S) architecture featuring continuous user
inputs and codec agent outputs with channel fusion that directly models
simultaneous user and agent streams. Using a pretrained streaming encoder for
user input enables the first duplex S2S model without requiring speech
pretrain. Separate architectures for agent and user modeling facilitate codec
fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared
to previous works. Experimental results show that the proposed model
outperforms previous duplex models in reasoning, turn-taking, and barge-in
abilities. The model requires significantly less speech data, as speech
pretrain is skipped, which markedly simplifies the process of building a duplex
S2S model from any LLMs. Finally, it is the first openly available duplex S2S
model with training and inference code to foster reproducibility.

</details>


### [117] [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)
*Miao Yu,Liang Lin,Guibin Zhang,Xinfeng Li,Junfeng Fang,Ningyu Zhang,Kun Wang,Yang Wang*

Key words: 大型语言模型、机器遗忘、UniErase、参数优化、模型编辑

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一种名为UniErase的新颖遗忘范式，通过可学习参数后缀引导语言模型实现目标遗忘行为，并在性能和模型能力上优于现有方法。

Motivation: 大型语言模型需要迭代更新以解决知识冲突和过时信息等问题。现有方法在遗忘效能和模型能力之间难以平衡，容易出现模型崩溃或泛化能力不足的问题。

Method: UniErase通过两阶段实现目标遗忘：(I) 优化阶段将遗忘目标绑定到模型的概率分布，(II) 轻量级模型编辑阶段激活学习到的标记以诱导遗忘。

Result: UniErase在虚构和真实知识场景下实现了批量、顺序和精确遗忘的SOTA性能，仅修改约3.66%的模型参数，遗忘效能提升4.01倍，保持更高模型能力。

Conclusion: UniErase展示了在遗忘领域的双重顶级表现，为参数学习诱导遗忘目标提供了新的研究方向。

Abstract: Large language models require iterative updates to address challenges such as
knowledge conflicts and outdated information (e.g., incorrect, private, or
illegal contents). Machine unlearning provides a systematic methodology for
targeted knowledge removal from trained models, enabling elimination of
sensitive information influences. However, mainstream fine-tuning-based
unlearning methods often fail to balance unlearning efficacy and model ability,
frequently resulting in catastrophic model collapse under extensive knowledge
removal. Meanwhile, in-context unlearning, which relies solely on contextual
prompting without modifying the model's intrinsic mechanisms, suffers from
limited generalizability and struggles to achieve true unlearning. In this
work, we introduce UniErase, a novel unlearning paradigm that employs learnable
parametric suffix (unlearning token) to steer language models toward targeted
forgetting behaviors. UniErase operates through two key phases: (I) an
optimization stage that binds desired unlearning outputs to the model's
autoregressive probability distribution via token optimization, followed by
(II) a lightweight model editing phase that activates the learned token to
probabilistically induce specified forgetting objective. Serving as a new
research direction for token learning to induce unlearning target, UniErase
achieves state-of-the-art (SOTA) performance across batch, sequential, and
precise unlearning under fictitious and real-world knowledge settings.
Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%
of the LLM parameters, outperforms previous forgetting SOTA baseline by around
4.01 times for model ability with even better unlearning efficacy. Similarly,
UniErase, maintaining more ability, also surpasses previous retaining SOTA by
35.96% for unlearning efficacy, showing dual top-tier performances in current
unlearing domain.

</details>


### [118] [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/abs/2505.15682)
*Cosimo Iaia,Bhavin Choksi,Emily Wiebers,Gemma Roig,Christian J. Fiebach*

Key words: 具体性, 语言模型, 表征相似性, 认知心理学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究探讨了语言模型中具体性（concreteness）的表征，发现人类与模型在这维度上显著对齐，而其他心理语言学特征影响较小。

Motivation: 理解具体性在心理和大脑中的表征是心理学、神经科学和计算语言学的核心问题。

Method: 结合行为判断和表征相似性分析，对比人类与语言模型的语义表征。

Result: 人类与语言模型的表征空间在具体性维度上显著对齐，且主要由具体性驱动。

Conclusion: 人类和语言模型在具体性维度上表现一致，但其他维度无显著对齐。

Abstract: The nouns of our language refer to either concrete entities (like a table) or
abstract concepts (like justice or love), and cognitive psychology has
established that concreteness influences how words are processed. Accordingly,
understanding how concreteness is represented in our mind and brain is a
central question in psychology, neuroscience, and computational linguistics.
While the advent of powerful language models has allowed for quantitative
inquiries into the nature of semantic representations, it remains largely
underexplored how they represent concreteness. Here, we used behavioral
judgments to estimate semantic distances implicitly used by humans, for a set
of carefully selected abstract and concrete nouns. Using Representational
Similarity Analysis, we find that the implicit representational space of
participants and the semantic representations of language models are
significantly aligned. We also find that both representational spaces are
implicitly aligned to an explicit representation of concreteness, which was
obtained from our participants using an additional concreteness rating task.
Importantly, using ablation experiments, we demonstrate that the human-to-model
alignment is substantially driven by concreteness, but not by other important
word characteristics established in psycholinguistics. These results indicate
that humans and language models converge on the concreteness dimension, but not
on other dimensions.

</details>


### [119] [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)
*Zishuai Zhang,Hainan Zhang,Jiaying Zheng,Ziwei Wang,Yongxin Tong,Jin Dong,Zhiming Zheng*

Key words: FL-LLaMA, 联邦学习, LLaMA2, 隐私保护, 并行训练

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: FL-LLaMA是一个基于LLaMA2的安全、高效且自适应的联邦分割框架，解决了私有数据在分散环境下训练大规模语言模型时的安全、效率和适应性问题。

Motivation: 私有数据分散且质量高，但传统方法存在安全漏洞、通信开销大和适应性差的问题，FL-LLaMA旨在解决这些挑战。

Method: 通过将部分输入输出块放在本地客户端、注入高斯噪声、并行训练策略、注意力掩码压缩和动态调整分割点等方法，提升安全性和效率。

Result: 在NLU、摘要和对话任务中，FL-LLaMA性能与集中式LLaMA2相当，训练速度提升2倍，推理速度提升8倍。

Conclusion: FL-LLaMA在安全性、效率和适应性上表现优异，为联邦学习环境下的LLM部署提供了可行方案。

Abstract: Private data is typically larger and of higher quality than public data,
offering great potential to improve LLM. However, its scattered distribution
across data silos and the high computational demands of LLMs limit their
deployment in federated environments. To address this, the transformer-based
split learning model has emerged, offloading most model parameters to the
server while retaining only the embedding and output layers on clients to
ensure privacy. However, it still faces significant challenges in security,
efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,
leading to reverse engineering of private data; 2) the autoregressive nature of
LLMs means that federated split learning can only train and infer sequentially,
causing high communication overhead; 3) fixed partition points lack
adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a
secure, efficient, and adaptive federated split framework based on LLaMA2.
First, we place some input and output blocks on the local client and inject
Gaussian noise into forward-pass hidden states, enabling secure end-to-end
propagation. Second, we employ client-batch and server-hierarchical strategies
to achieve parallel training, along with attention-mask compression and KV
cache mechanisms to accelerate inference, reducing communication costs
effectively. Third, we allow users to dynamically adjust the partition points
for input/output blocks based on specific task requirements and hardware
limitations. Experiments on NLU, summarization and conversational QA tasks show
that FL-LLaMA maintains performance comparable to centralized LLaMA2, and
achieves up to 2x train speedups and 8x inference speedups. Further analysis of
privacy attacks and different partition points also demonstrates the
effectiveness of FL-LLaMA in security and adaptability.

</details>


### [120] [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/abs/2505.15684)
*Gengyang Li,Yifeng Gao,Yuming Li,Yunfang Wu*

Key words: Chain-of-Thought, 推理优化, 语言模型, 注意力机制

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ThinkLess是一个推理高效的框架，通过提前终止推理生成来减少延迟和内存使用，同时保持输出质量。

Motivation: 解决Chain-of-Thought (CoT)提示方法中推理令牌过长导致的延迟、内存消耗以及可能截断答案的问题。

Method: 通过注意力分析发现答案令牌主要关注推理终止令牌，因此在早期位置插入终止令牌以跳过冗余推理；采用轻量级后调节机制保证输出格式。

Result: 在不进行微调或使用额外数据的情况下，ThinkLess在保持准确性的同时显著减少了解码时间和内存消耗。

Conclusion: ThinkLess是一种无需修改模型即可提升推理效率的有效方法。

Abstract: While Chain-of-Thought (CoT) prompting improves reasoning in large language
models (LLMs), the excessive length of reasoning tokens increases latency and
KV cache memory usage, and may even truncate final answers under context
limits. We propose ThinkLess, an inference-efficient framework that terminates
reasoning generation early and maintains output quality without modifying the
model. Atttention analysis reveals that answer tokens focus minimally on
earlier reasoning steps and primarily attend to the reasoning terminator token,
due to information migration under causal masking. Building on this insight,
ThinkLess inserts the terminator token at earlier positions to skip redundant
reasoning while preserving the underlying knowledge transfer. To prevent format
discruption casued by early termination, ThinkLess employs a lightweight
post-regulation mechanism, relying on the model's natural instruction-following
ability to produce well-structured answers. Without fine-tuning or auxiliary
data, ThinkLess achieves comparable accuracy to full-length CoT decoding while
greatly reducing decoding time and memory consumption.

</details>


### [121] [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
*Jinyang Wu,Chonghua Liao,Mingkuan Feng,Shuai Zhang,Zhengqi Wen,Pengpeng Shao,Huazhe Xu,Jianhua Tao*

Key words: 强化学习, 推理模型, 外部指导, TAPO

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: TAPO（Thought-Augmented Policy Optimization）是一种新颖的强化学习框架，通过引入外部高层次指导来增强模型的推理能力，显著优于传统方法。

Motivation: 现有的强化学习方法通常偏向奖励最大化的路径，缺乏外部知识引入，限制了模型的探索能力和推理边界。

Method: TAPO框架通过自适应地整合结构化“思维模式”，平衡模型内部探索与外部指导利用。

Result: TAPO在多个基准测试中表现优异：AIME提升99%，AMC提升41%，Minerva Math提升17%。

Conclusion: TAPO展示了在广泛任务和领域中应用潜力，同时提升了推理模型的可解释性和输出可读性。

Abstract: Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.

</details>


### [122] [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)
*Ryang Heo,Yongsik Seo,Junseong Lee,Dongha Lee*

Key words: 意见挖掘,大型语言模型,在线内容,基准评估

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文介绍了在线意见挖掘基准（OOMB），用于评估大型语言模型在复杂多样的在线环境中挖掘意见的能力，为基于LLM的意见挖掘奠定基础。

Motivation: 用户生成内容的多样性和复杂性对传统意见挖掘方法提出挑战，需开发新基准评估LLM在此领域的适应性。

Method: 提出OOMB数据集和评估协议，提供（实体、特征、意见）三元组标注和意见摘要，以评估模型的提取和摘要能力。

Result: 通过基准分析，揭示LLM在意见挖掘中的挑战和适应性，探讨其在现实在线场景中的应用潜力。

Conclusion: OOMB为LLM意见挖掘研究奠定基础，并提出了未来研究方向。

Abstract: The surge of user-generated online content presents a wealth of insights into
customer preferences and market trends. However, the highly diverse, complex,
and context-rich nature of such contents poses significant challenges to
traditional opinion mining approaches. To address this, we introduce Online
Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol
designed to assess the ability of large language models (LLMs) to mine opinions
effectively from diverse and intricate online environments. OOMB provides
extensive (entity, feature, opinion) tuple annotations and a comprehensive
opinion-centric summary that highlights key opinion topics within each content,
thereby enabling the evaluation of both the extractive and abstractive
capabilities of models. Through our proposed benchmark, we conduct a
comprehensive analysis of which aspects remain challenging and where LLMs
exhibit adaptability, to explore whether they can effectively serve as opinion
miners in realistic online scenarios. This study lays the foundation for
LLM-based opinion mining and discusses directions for future research in this
field.

</details>


### [123] [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)
*Maike Behrendt,Stefan Sylvius Wagner,Stefan Harmeling*

Key words: BERT, [CLS] token, 最大池化, 多头注意力, 分类任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: MaxPoolBERT通过跨层和跨token的信息聚合改进BERT的[CLS]表示，提升了分类任务的性能，尤其在低资源任务上表现更优。

Motivation: 传统的BERT中，[CLS] token被用作分类任务的固定长度表示，但其他token和中间层也包含有价值的上下文信息。MaxPoolBERT旨在通过改进[CLS]表示来提升性能。

Method: 提出了三种改进方法：(i) 跨层对[CLS] token进行最大池化，(ii) 通过额外的多头注意力层让[CLS] token关注最终层的所有信息，(iii) 结合序列的最大池化和多头注意力。

Result: 在GLUE基准测试中，MaxPoolBERT表现优于标准BERT-base模型，尤其在低资源任务上效果显著。

Conclusion: MaxPoolBERT是一种轻量级的扩展方法，不需要预训练或显著增加模型大小即可提升BERT的分类准确性。

Abstract: The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.

</details>


### [124] ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
*Alkis Koudounas,Claudio Savelli,Flavio Giobergia,Elena Baralis*

Key words: 机器学习遗忘, 语音语言理解, 基准测试, 数据隐私, 责任AI

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文提出了首个针对语音语言理解（SLU）的机器学习“遗忘”基准UnSLU-BENCH，评估了八种遗忘技术，并提出了新的衡量标准。

Motivation: 随着责任AI的发展，高效去除机器学习模型中的特定信息（遗忘）成为重要课题，但复杂任务（尤其是语音相关任务）中遗忘方法的效果尚缺乏研究。

Method: 论文聚焦四种语言的四个数据集，评估八种遗忘技术，并引入一种新指标来综合衡量遗忘效果、模型效用和计算效率。

Result: UnSLU-BENCH为SLU领域的遗忘研究奠定基础，揭示了不同技术在有效性和计算可行性上的显著差异。

Conclusion: 该研究填补了SLU领域遗忘技术的空白，为新方法和实际应用提供了基准支持。

Abstract: Machine unlearning, the process of efficiently removing specific information
from machine learning models, is a growing area of interest for responsible AI.
However, few studies have explored the effectiveness of unlearning methods on
complex tasks, particularly speech-related ones. This paper introduces
UnSLU-BENCH, the first benchmark for machine unlearning in spoken language
understanding (SLU), focusing on four datasets spanning four languages. We
address the unlearning of data from specific speakers as a way to evaluate the
quality of potential "right to be forgotten" requests. We assess eight
unlearning techniques and propose a novel metric to simultaneously better
capture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation
for unlearning in SLU and reveals significant differences in the effectiveness
and computational feasibility of various techniques.

</details>


### [125] [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/abs/2505.15702)
*Peng Wang,Biyu Zhou,Xuehai Tang,Jizhong Han,Songlin Hu*

Key words: 大型语言模型, 知识编辑, 长期知识保留, 约束随机规划, 李雅普诺夫优化

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: LyapLock提出了一种基于优化理论的方法，通过将长期知识更新的约束优化问题分解为可处理的子问题，显著提高了大型语言模型的连续编辑能力，并保持了长期知识保留的稳定性。

Motivation: 大型语言模型中的知识往往存在错误或过时问题，而现有的定位-编辑方法在连续编辑过程中性能逐渐下降，缺乏有效的长期知识保留机制。

Method: LyapLock将连续编辑建模为约束随机规划问题，结合排队论和李雅普诺夫优化，将长期约束问题分解为逐步可解决的子问题。

Result: 实验表明，LyapLock能将连续编辑能力扩展至10,000次以上，同时稳定通用能力，并比现有最佳方法提升平均编辑效果11.89%。

Conclusion: LyapLock是首个具有严格理论保证的模型编辑框架，实现了渐进最优的编辑性能，同时满足了长期知识保留的约束。

Abstract: Large Language Models often contain factually incorrect or outdated
knowledge, giving rise to model editing methods for precise knowledge updates.
However, current mainstream locate-then-edit approaches exhibit a progressive
performance decline during sequential editing, due to inadequate mechanisms for
long-term knowledge preservation. To tackle this, we model the sequential
editing as a constrained stochastic programming. Given the challenges posed by
the cumulative preservation error constraint and the gradually revealed editing
tasks, \textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov
optimization to decompose the long-term constrained programming into tractable
stepwise subproblems for efficient solving. This is the first model editing
framework with rigorous theoretical guarantees, achieving asymptotic optimal
editing performance while meeting the constraints of long-term knowledge
preservation. Experimental results show that our framework scales sequential
editing capacity to over 10,000 edits while stabilizing general capabilities
and boosting average editing efficacy by 11.89\% over SOTA baselines.
Furthermore, it can be leveraged to enhance the performance of baseline
methods. Our code is released on https://github.com/caskcsg/LyapLock.

</details>


### [126] [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)
*Tianqi Du,Zeming Wei,Quan Chen,Chenheng Zhang,Yisen Wang*

Key words: 大语言模型,安全性评估,隐藏状态,排序框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了Safety Representation Ranking (SRR)框架，利用LLM的隐藏状态对安全响应进行排序，显著提升了对抗性提示的鲁棒性。

Motivation: 现有安全性评估方法通常直接操作于文本响应，忽略了模型内部嵌入的丰富信息。

Method: 提出SRR，一个列表排序框架，利用LLM的隐藏状态对候选完成进行编码和排序。

Result: 在多个基准测试中，SRR显著提高了对对抗性提示的鲁棒性。

Conclusion: SRR通过直接利用模型内部状态和列表级监督，捕捉细微的安全信号，是有效的安全性评估方法。

Abstract: The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.

</details>


### [127] [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)
*Yuan Yuan,Muyu He,Muhammad Adil Shahid,Jiani Huang,Ziyang Li,Li Zhang*

Key words: TurnaboutLLM, 大语言模型, 演绎推理, 侦探游戏, 评估框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 介绍TurnaboutLLM框架和数据集，通过侦探游戏互动评估大语言模型的演绎推理能力。

Motivation: 探索大语言模型在复杂叙事环境中演绎推理的局限性。

Method: 利用侦探游戏《逆转裁判》和《弹丸论破》的互动剧情，设计数据集和框架，评估模型在长叙事中识别矛盾的能力。

Result: 测试12个先进大语言模型，发现常用推理增强策略（如多步思考、思维链提示）的效果有限，且上下文大小、推理步骤数和答案空间大小对性能有不同影响。

Conclusion: TurnaboutLLM为大语言模型在复杂叙事环境中的演绎推理提供了重要挑战。

Abstract: This paper introduces TurnaboutLLM, a novel framework and dataset for
evaluating the deductive reasoning abilities of Large Language Models (LLMs) by
leveraging the interactive gameplay of detective games Ace Attorney and
Danganronpa. The framework tasks LLMs with identifying contradictions between
testimonies and evidences within long narrative contexts, a challenging task
due to the large answer space and diverse reasoning types presented by its
questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at
limitations of popular strategies for enhancing deductive reasoning such as
extensive thinking and Chain-of-Thought prompting. The results also suggest
varying effects of context size, the number of reasoning step and answer space
size on model performance. Overall, TurnaboutLLM presents a substantial
challenge for LLMs' deductive reasoning abilities in complex, narrative-rich
environments.

</details>


### [128] [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/abs/2505.15715)
*He Hu,Yucheng Zhou,Juzheng Si,Qianning Wang,Hengheng Zhang,Fuji Ren,Fei Ma,Laizhong Cui*

Key words: LLM, 心理健康, DSM/ICD, CBT, ACT, 心理动力学

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: PsyLLM是一个专为心理健康咨询设计的大型语言模型，通过整合诊断和治疗推理，结合DSM/ICD标准和多种治疗框架，生成高质量临床对话数据，显著优于现有基线模型。

Motivation: 现有基于LLM的心理健康支持方法缺乏临床基础，特别是在符合DSM/ICD标准的诊断推理和多样化治疗策略上存在不足。

Method: 提出自动化数据合成流程，结合真实心理健康帖子生成多轮对话，并在DSM/ICD标准和多种治疗框架指导下生成临床推理数据。

Result: PsyLLM在综合性、专业性、真实性和安全性四个维度上显著优于现有基线模型。

Conclusion: PsyLLM通过系统整合诊断和治疗推理，为心理健康咨询提供了更高质量的解决方案。

Abstract: Large language models (LLMs) hold significant potential for mental health
support, capable of generating empathetic responses and simulating therapeutic
conversations. However, existing LLM-based approaches often lack the clinical
grounding necessary for real-world psychological counseling, particularly in
explicit diagnostic reasoning aligned with standards like the DSM/ICD and
incorporating diverse therapeutic modalities beyond basic empathy or single
strategies. To address these critical limitations, we propose PsyLLM, the first
large language model designed to systematically integrate both diagnostic and
therapeutic reasoning for mental health counseling. To develop the PsyLLM, we
propose a novel automated data synthesis pipeline. This pipeline processes
real-world mental health posts, generates multi-turn dialogue structures, and
leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and
multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate
detailed clinical reasoning processes. Rigorous multi-dimensional filtering
ensures the generation of high-quality, clinically aligned dialogue data. In
addition, we introduce a new benchmark and evaluation protocol, assessing
counseling quality across four key dimensions: comprehensiveness,
professionalism, authenticity, and safety. Our experiments demonstrate that
PsyLLM significantly outperforms state-of-the-art baseline models on this
benchmark.

</details>


### [129] [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)
*Xiaoyu Luo,Yiyi Chen,Johannes Bjerva,Qiongxiu Li*

Key words: 多语言大语言模型, 记忆现象, 语言相似性, 跨语言迁移

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 这篇论文首次全面研究了多语言大语言模型（MLLMs）中的记忆现象，分析了95种语言的模型，发现单纯依赖训练数据量的假设无法完全解释记忆模式，提出基于语言相似性的图指标以揭示跨语言记忆规律。

Motivation: 随着MLLMs的广泛应用，理解其记忆行为变得至关重要，但此前研究主要集中于单语模型，多语言记忆问题未被充分探索。

Method: 通过分析不同规模、架构的模型，并提出一种基于图的语言相似性相关指标，研究跨语言记忆模式。

Result: 发现相似语言中训练标记较少的语言表现出更高的记忆率，这一趋势仅在显式建模跨语言关系时才显现。

Conclusion: 语言相似性不仅解释了MLLMs的记忆行为，也支持跨语言可迁移性，为多语言NLP提供了重要启示。

Abstract: We present the first comprehensive study of Memorization in Multilingual
Large Language Models (MLLMs), analyzing 95 languages using models across
diverse model scales, architectures, and memorization definitions. As MLLMs are
increasingly deployed, understanding their memorization behavior has become
critical. Yet prior work has focused primarily on monolingual models, leaving
multilingual memorization underexplored, despite the inherently long-tailed
nature of training corpora. We find that the prevailing assumption, that
memorization is highly correlated with training data availability, fails to
fully explain memorization patterns in MLLMs. We hypothesize that treating
languages in isolation - ignoring their similarities - obscures the true
patterns of memorization. To address this, we propose a novel graph-based
correlation metric that incorporates language similarity to analyze
cross-lingual memorization. Our analysis reveals that among similar languages,
those with fewer training tokens tend to exhibit higher memorization, a trend
that only emerges when cross-lingual relationships are explicitly modeled.
These findings underscore the importance of a language-aware perspective in
evaluating and mitigating memorization vulnerabilities in MLLMs. This also
constitutes empirical evidence that language similarity both explains
Memorization in MLLMs and underpins Cross-lingual Transferability, with broad
implications for multilingual NLP.

</details>


### [130] [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)
*Heyang Liu,Yuhao Wang,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Key words: 语音交互、评估基准、多模态模型、语音性能

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了VocalBench，一个全面评估语音交互模型在多维语音通信能力的基准测试。

Motivation: 现有语音交互模型评估多关注文本响应质量，忽视语音性能关键方面，缺乏专门测试实例。

Method: 设计VocalBench，涵盖9400个实例，覆盖语义质量、声学表现、会话能力和鲁棒性四维度及16项基础技能。

Result: 当前模型能力差异显著，各有优劣，为未来语音交互系统研究提供指导。

Conclusion: VocalBench填补了语音交互评估空白，推动该领域发展。

Abstract: The rapid advancement of large language models (LLMs) has accelerated the
development of multi-modal models capable of vocal communication. Unlike
text-based interactions, speech conveys rich and diverse information, including
semantic content, acoustic variations, paralanguage cues, and environmental
context. However, existing evaluations of speech interaction models
predominantly focus on the quality of their textual responses, often
overlooking critical aspects of vocal performance and lacking benchmarks with
vocal-specific test instances. To address this gap, we propose VocalBench, a
comprehensive benchmark designed to evaluate speech interaction models'
capabilities in vocal communication. VocalBench comprises 9,400 carefully
curated instances across four key dimensions: semantic quality, acoustic
performance, conversational abilities, and robustness. It covers 16 fundamental
skills essential for effective vocal interaction. Experimental results reveal
significant variability in current model capabilities, each exhibiting distinct
strengths and weaknesses, and provide valuable insights to guide future
research in speech-based interaction systems. Code and evaluation instances are
available at https://github.com/SJTU-OmniAgent/VocalBench.

</details>


### [131] [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
*Gaurav Srivastava,Zhenyu Bi,Meng Lu,Xuan Wang*

Key words: 大语言模型,推理能力,多智能体辩论,无监督训练,Reflect-Critique-Refine

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个名为DTE的无监督训练框架，通过多智能体辩论提升语言模型的推理能力，成果显著。

Motivation: 解决大语言模型仅依赖海量数据训练的局限性，探索无需外部监督的自主推理能力提升方法。

Method: 提出DTE框架，结合多智能体辩论和Reflect-Critique-Refine提示策略，优化模型自我进化。

Result: 在五个推理基准测试中平均准确率提升8.92%，跨领域泛化能力提升5.8%。

Conclusion: DTE框架有效提升模型推理能力，且具备良好的通用性。

Abstract: Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.

</details>


### [132] [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)
*Mikhail Budnikov,Ivan Yamshchikov*

Key words: 迁移学习, 合成语言, 嵌入式结构, Tiny-Cloze Benchmark

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 研究了从多种合成语言到英语的迁移学习，分析了微调模型的嵌入结构及其在简单语言任务中的表现，提出了一种新合成语言和改进的Tiny-Cloze Benchmark。

Motivation: 探究合成语言对英语迁移学习的效果，寻找更优的预训练方法和评估工具。

Method: 引入了新的合成语言和改进的Tiny-Cloze Benchmark，评估微调模型在多种任务中的表现。

Result: 新合成语言改善了迁移学习效果，Tiny-Cloze Benchmark对较弱模型更具信息性。

Conclusion: 使用新的合成语言微调模型能提升多种任务性能，改进的基准测试更具实用性。

Abstract: This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.

</details>


### [133] [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)
*Huanxuan Liao,Wen Hu,Yao Xu,Shizhu He,Jun Zhao,Kang Liu*

Key words: 大语言模型, 上下文压缩, 全局-局部视角, 混合适配器, 信息保留

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: HyCo₂是一种混合上下文压缩方法，结合全局和局部视角，优化大语言模型的长序列推理，显著提升性能并减少计算开销。

Motivation: 解决大语言模型在长序列推理中的计算低效和冗余处理问题，同时避免现有压缩方法导致的信息丢失。

Method: 通过混合适配器和分类层整合全局与局部压缩视角，结合辅助预训练任务指导信息保留。

Result: HyCo₂在知识密集型QA任务中平均提升13.1%性能，同时减少88.8%的token消耗。

Conclusion: HyCo₂有效平衡信息保留与计算效率，显著提升长文本推理性能。

Abstract: Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.

</details>


### [134] [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)
*Changtai Zhu,Siyin Wang,Ruijun Feng,Kai Song,Xipeng Qiu*

Key words: 对话搜索, 查询重构, 强化学习, 自驱动框架

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: ConvSearch-R1是一个自驱动的框架，通过强化学习直接从检索信号优化查询重构，消除了对外部监督的依赖，显著提升了性能。

Motivation: 解决现有对话搜索系统中查询重构方法对外部监督的高依赖以及与下游检索器对齐不足的问题。

Method: 采用两阶段方法：自驱动策略预热和检索引导的强化学习，结合排名激励奖励机制。

Result: 在TopiOCQA和QReCC数据集上表现优异，性能提升超过10%。

Conclusion: ConvSearch-R1无需外部监督即可显著提升查询重构效果，优于现有方法。

Abstract: Conversational search systems require effective handling of context-dependent
queries that often contain ambiguity, omission, and coreference. Conversational
Query Reformulation (CQR) addresses this challenge by transforming these
queries into self-contained forms suitable for off-the-shelf retrievers.
However, existing CQR approaches suffer from two critical constraints: high
dependency on costly external supervision from human annotations or large
language models, and insufficient alignment between the rewriting model and
downstream retrievers. We present ConvSearch-R1, the first self-driven
framework that completely eliminates dependency on external rewrite supervision
by leveraging reinforcement learning to optimize reformulation directly through
retrieval signals. Our novel two-stage approach combines Self-Driven Policy
Warm-Up to address the cold-start problem through retrieval-guided
self-distillation, followed by Retrieval-Guided Reinforcement Learning with a
specially designed rank-incentive reward shaping mechanism that addresses the
sparsity issue in conventional retrieval metrics. Extensive experiments on
TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly
outperforms previous state-of-the-art methods, achieving over 10% improvement
on the challenging TopiOCQA dataset while using smaller 3B parameter models
without any external supervision.

</details>


### [135] [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)
*Zhen Zhang,Xuehai He,Weixiang Yan,Ao Shen,Chenyang Zhao,Shuohang Wang,Yelong Shen,Xin Eric Wang*

Key words: Soft Thinking, 连续概念空间, 抽象推理, Chain-of-Thought, 语言模型

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出'Soft Thinking'方法，通过连续概念空间中的软概念令牌模拟人类抽象推理，超越离散语言限制。

Motivation: 现有推理模型受限于离散语言标记，未能充分探索推理路径，因此提出连续概念表示方法。

Method: 生成概率加权的令牌嵌入混合体作为软概念令牌，形成连续概念空间。

Result: 在数学和编程任务中，pass@1准确率提升2.48分，令牌使用减少22.4%。

Conclusion: Soft Thinking能突破离散语言的推理瓶颈，同时保持高可解释性。

Abstract: Human cognition typically involves thinking through abstract, fluid concepts
rather than strictly using discrete linguistic tokens. Current reasoning
models, however, are constrained to reasoning within the boundaries of human
language, processing discrete token embeddings that represent fixed points in
the semantic space. This discrete constraint restricts the expressive power and
upper potential of such reasoning models, often causing incomplete exploration
of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling
one token per step. In this work, we introduce Soft Thinking, a training-free
method that emulates human-like "soft" reasoning by generating soft, abstract
concept tokens in a continuous concept space. These concept tokens are created
by the probability-weighted mixture of token embeddings, which form the
continuous concept space, enabling smooth transitions and richer
representations that transcend traditional discrete boundaries. In essence,
each generated concept token encapsulates multiple meanings from related
discrete tokens, implicitly exploring various reasoning paths to converge
effectively toward the correct answer. Empirical evaluations on diverse
mathematical and coding benchmarks consistently demonstrate the effectiveness
and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points
while simultaneously reducing token usage by up to 22.4% compared to standard
CoT. Qualitative analysis further reveals that Soft Thinking outputs remain
highly interpretable and readable, highlighting the potential of Soft Thinking
to break the inherent bottleneck of discrete language-based reasoning. Code is
available at https://github.com/eric-ai-lab/Soft-Thinking.

</details>


### [136] [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781)
*Xinyin Ma,Runpeng Yu,Gongfan Fang,Xinchao Wang*

Key words: 扩散语言模型, KV-Cache, 推理加速, 键值缓存

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一种名为延迟KV-Cache的机制，通过逐步缓存键值状态来解决扩散语言模型推理速度慢的问题，显著提升了推理速度。

Motivation: 扩散语言模型因其非自回归架构和双向注意力机制，无法利用键值缓存加速推理，导致推理速度慢。为了解决这一瓶颈，提出了延迟KV-Cache机制。

Method: 设计了两种缓存策略：(1) dKV-Cache-Decode，几乎无损加速且对长序列性能有提升；(2) dKV-Cache-Greedy，牺牲部分性能换取更高加速。

Result: 实验显示，该方法在推理速度上实现了2-10倍的提升，并在多种基准测试中表现良好。

Conclusion: 延迟KV-Cache机制有效缩小了自回归模型与扩散语言模型之间的性能差距，且无需额外训练即可应用。

Abstract: Diffusion Language Models (DLMs) have been seen as a promising competitor for
autoregressive language models. However, diffusion language models have long
been constrained by slow inference. A core challenge is that their
non-autoregressive architecture and bidirectional attention preclude the
key-value cache that accelerates decoding. We address this bottleneck by
proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising
process of DLMs. Our approach is motivated by the observation that different
tokens have distinct representation dynamics throughout the diffusion process.
Accordingly, we propose a delayed and conditioned caching strategy for key and
value states. We design two complementary variants to cache key and value
step-by-step: (1) dKV-Cache-Decode, which provides almost lossless
acceleration, and even improves performance on long sequences, suggesting that
existing DLMs may under-utilise contextual information during inference. (2)
dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving
higher speed-ups with quadratic time complexity at the cost of some performance
degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,
largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on
several benchmarks, delivering acceleration across general language
understanding, mathematical, and code-generation benchmarks. Experiments
demonstrate that cache can also be used in DLMs, even in a training-free manner
from current DLMs.

</details>


### [137] [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)
*Danna Zheng,Mirella Lapata,Jeff Z. Pan*

Key words: 信息对齐评估,LLM部署,FactScore,MontageLie,DoveScore,事件顺序一致性

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 论文提出了一个新的基准测试MontageLie，用于检测文本生成评估中的欺骗性叙述，并提出了DoveScore框架，通过联合验证事实准确性和事件顺序一致性，提升了评估效果。

Motivation: 现有细粒度评估方法（如FactScore）虽然能验证单个事实，但忽略了事实间的依赖关系，导致潜在的漏洞。论文旨在解决这一问题，并提出更鲁棒的评估框架。

Method: 提出MontageLie基准测试，并通过DoveScore框架联合验证事实准确性和事件顺序一致性，建模事实间关系。

Result: DoveScore在AUC-ROC上比现有细粒度方法提升了8%以上，证明了其有效性。

Conclusion: DoveScore通过建模事实间关系，显著提升了长文本生成评估的鲁棒性。

Abstract: Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by "montaging" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.

</details>


### [138] [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/abs/2505.15795)
*Lisa Alazraki,Tan Yi-Chern,Jon Ander Campos,Maximilian Mozes,Marek Rei,Max Bartolo*

Key words: LLM-as-a-judge, adversarial tuning, preamble generation, reinforcement learning, reverse engineering

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种通过对抗性调整生成文本前导的模型，以提高下游任务性能的方法，这种方法在LLM评估中表现优异且难以检测。

Motivation: LLM评估框架（LLM-as-a-judge）易受恶意攻击，候选LLM可以通过调整回答来迎合评委LLM的偏好。本研究旨在探索一种更隐蔽且有效的方法，通过优化前导文本来提高评分。

Method: 使用评委LLM提供的信号作为奖励，通过对抗性调整训练模型生成优化的文本前导，采用强化学习方法。

Result: 优化后的前导生成模型显著提高了评分效果，且该方法对不同评委LLM和候选LLM具有迁移性。

Conclusion: 研究揭示了LLM评估框架的潜在漏洞，并提出了一种隐蔽的攻击方法，同时展示了人类偏好可以通过强化学习反向工程。

Abstract: The capabilities of Large Language Models (LLMs) are routinely evaluated by
other LLMs trained to predict human preferences. This framework--known as
LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also
vulnerable to malicious exploitation, as LLM responses can be tuned to overfit
the preferences of the judge. Previous work shows that the answers generated by
a candidate-LLM can be edited post hoc to maximise the score assigned to them
by a judge-LLM. In this study, we adopt a different approach and use the signal
provided by judge-LLMs as a reward to adversarially tune models that generate
text preambles designed to boost downstream performance. We find that frozen
LLMs pipelined with these models attain higher LLM-evaluation scores than
existing frameworks. Crucially, unlike other frameworks which intervene
directly on the model's response, our method is virtually undetectable. We also
demonstrate that the effectiveness of the tuned preamble generator transfers
when the candidate-LLM and the judge-LLM are replaced with models that are not
used during training. These findings raise important questions about the design
of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that
human preferences can be reverse engineered effectively, by pipelining LLMs to
optimise upstream preambles via reinforcement learning--an approach that could
find future applications in diverse tasks and domains beyond adversarial
attacks.

</details>


### [139] [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
*Yuchen Yan,Jin Jiang,Zhenbang Ren,Yijun Li,Xudong Cai,Yang Liu,Xin Xu,Mengdi Zhang,Jian Shao,Yongliang Shen,Jun Xiao,Yueting Zhuang*

Key words: 基准测试,奖励系统,强化学习,推理模型,验证器

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文介绍了两个新的基准测试VerifyBench和VerifyBench-Hard，用于评估基于参考的奖励系统性能，填补了现有奖励基准的空白。通过精心设计和人工标注的数据集，研究发现当前模型仍有较大改进空间，尤其是小规模模型。

Motivation: 现有的奖励基准未能评估基于参考的奖励系统，导致研究人员对强化学习中验证器的准确性缺乏理解。本文旨在填补这一空白，为模型开发和能力提升提供工具。

Method: 通过细致的数据收集、整理和人工标注，构建了VerifyBench和VerifyBench-Hard两个高质量基准测试，用于评估基于参考的奖励系统。

Result: 当前模型在两个基准测试中表现仍有显著提升空间，尤其是小规模模型。文章还提供了对评估结果的全面分析，为未来研究提供见解。

Conclusion: 所提出的基准测试为评估和改进基于参考的奖励系统提供了有效工具，有助于提升强化学习训练的模型在推理任务中的能力。

Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved
remarkable performance in the domain of reasoning. A key component of their
training is the incorporation of verifiable rewards within reinforcement
learning (RL). However, existing reward benchmarks do not evaluate
reference-based reward systems, leaving researchers with limited understanding
of the accuracy of verifiers used in RL. In this paper, we introduce two
benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the
performance of reference-based reward systems. These benchmarks are constructed
through meticulous data collection and curation, followed by careful human
annotation to ensure high quality. Current models still show considerable room
for improvement on both VerifyBench and VerifyBench-Hard, especially
smaller-scale models. Furthermore, we conduct a thorough and comprehensive
analysis of evaluation results, offering insights for understanding and
developing reference-based reward systems. Our proposed benchmarks serve as
effective tools for guiding the development of verifier accuracy and the
reasoning capabilities of models trained via RL in reasoning tasks.

</details>


### [140] [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)
*Hwan Chang,Yumin Kim,Yonghyun Jun,Hwanhee Lee*

Key words: LLM, 安全政策, 信息非披露, CoPriva

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 提出了一个名为CoPriva的新基准数据集，评估LLM在问答任务中遵守上下文非披露政策的能力，揭示了其在敏感信息保护上的漏洞。

Motivation: 针对LLM在敏感领域部署时确保遵守用户定义的安全政策的需求，尤其是信息非披露方面。

Method: 引入CoPriva数据集，包含真实场景中的政策与查询，测试10种LLM的性能。

Result: 发现许多模型会违反政策泄露敏感信息，尤其是面对间接攻击时表现更差。

Conclusion: 当前LLM的安全对齐在敏感应用中存在重大缺陷，亟需更鲁棒的方法。

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains such as enterprise and government, ensuring that they adhere to
user-defined security policies within context is critical-especially with
respect to information non-disclosure. While prior LLM studies have focused on
general safety and socially sensitive data, large-scale benchmarks for
contextual security preservation against attacks remain lacking. To address
this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating
LLM adherence to contextual non-disclosure policies in question answering.
Derived from realistic contexts, our dataset includes explicit policies and
queries designed as direct and challenging indirect attacks seeking prohibited
information. We evaluate 10 LLMs on our benchmark and reveal a significant
vulnerability: many models violate user-defined policies and leak sensitive
information. This failure is particularly severe against indirect attacks,
highlighting a critical gap in current LLM safety alignment for sensitive
applications. Our analysis reveals that while models can often identify the
correct answer to a query, they struggle to incorporate policy constraints
during generation. In contrast, they exhibit a partial ability to revise
outputs when explicitly prompted. Our findings underscore the urgent need for
more robust methods to guarantee contextual security.

</details>


### [141] [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)
*Patrick Kahardipraja,Reduan Achtibat,Thomas Wiegand,Wojciech Samek,Sebastian Lapuschkin*

Key words: 大型语言模型, 上下文学习, 检索增强, 注意力机制, 问答系统

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 探讨了大型语言模型如何通过上下文检索增强学习外部知识，并提出一种归因方法揭示特定注意力头的作用，最后通过调整注意力权重展示了知识来源的追踪。

Motivation: 尽管上下文检索增强学习在问答任务中表现出潜力，但其内部机制尚不明确，本研究旨在揭示其工作机制。

Method: 通过将提示视为信息组件的组合，提出了一种归因方法，识别了专门用于理解指令和检索信息的注意力头，以及存储实体关系知识的参数头。并提取功能向量，调整注意力权重以分析其对答案生成的影响。

Result: 揭示了上下文检索增强学习的机制，展示了注意力头如何影响知识检索和答案生成，为更安全和透明的语言模型奠定了基础。

Conclusion: 研究通过分析注意力头的作用，为理解语言模型如何利用外部知识提供了新视角，推动了模型安全性和透明性的发展。

Abstract: Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.

</details>


### [142] [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)
*Yuqi Zhou,Sunhao Dai,Shuai Wang,Kaiwen Zhou,Qinqlin Jia,Junxu*

Key words: GUI代理,强化学习,奖励函数,定位任务

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 该论文分析了GUI代理训练中的三个关键问题，并提出针对性解决方案，最终在GUI定位任务中取得新突破。

Motivation: 揭示盲目应用通用强化学习于GUI定位任务时存在的问题，并提出改进方法。

Method: 采用快速思考模板、改进奖励函数和调整RL目标，以优化训练效果。

Result: GUI-G1-3B模型在ScreenSpot和ScreenSpot-Pro上分别达到90.3%和37.1%的准确率，优于同类模型。

Conclusion: 通过针对性改进，模型在GUI定位任务中实现了新的最佳性能。

Abstract: Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,
coupling online Reinforcement Learning (RL) with explicit chain-of-thought
reasoning prior to object grounding and thereby achieving substantial
performance gains. In this paper, we first conduct extensive analysis
experiments of three key components of that training pipeline: input design,
output evaluation, and policy update-each revealing distinct challenges arising
from blindly applying general-purpose RL without adapting to GUI grounding
tasks. Input design: Current templates encourage the model to generate
chain-of-thought reasoning, but longer chains unexpectedly lead to worse
grounding performance. Output evaluation: Reward functions based on hit signals
or box area allow models to exploit box size, leading to reward hacking and
poor localization quality. Policy update: Online RL tends to overfit easy
examples due to biases in length and sample difficulty, leading to
under-optimization on harder cases. To address these issues, we propose three
targeted solutions. First, we adopt a Fast Thinking Template that encourages
direct answer generation, reducing excessive reasoning during training. Second,
we incorporate a box size constraint into the reward function to mitigate
reward hacking. Third, we revise the RL objective by adjusting length
normalization and adding a difficulty-aware scaling factor, enabling better
optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with
Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on
ScreenSpot-Pro. This surpasses all prior models of similar size and even
outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI
agent grounding. The project repository is available at
https://github.com/Yuqi-Zhou/GUI-G1.

</details>


### [143] [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
*Tong Zheng,Lichang Chen,Simeng Han,R. Thomas McCoy,Heng Huang*

Key words: 多模态推理,自然语言处理,符号逻辑,真值表

<details>
  <summary>Details</summary>

Main category: cs.CL

TL;DR: 本文提出了一种名为Mixture-of-Thought (MoT)的框架，通过结合自然语言、代码和符号逻辑（真值表）三种推理模态，显著提升大型语言模型在逻辑推理任务中的表现。

Motivation: 现有基于LLM的方法在训练过程中通常仅使用单一推理模态（如自然语言），无法充分利用多模态协同的优势。MoT旨在填补这一空白。

Method: MoT采用两阶段设计：1）自演化的MoT训练，学习多模态自生成推理依据；2）MoT推理，充分结合三种模态的协同效应。

Result: 在FOLIO和ProofWriter等逻辑推理基准测试中，MoT比单模态方法平均提升11.7%的准确率，尤其在复杂问题上表现更优。

Conclusion: MoT框架通过多模态协同训练和推理，显著提升了逻辑推理能力，真值表模态有效弥补了自然语言推理的瓶颈。

Abstract: Human beings naturally utilize multiple reasoning modalities to learn and
solve logical problems, i.e., different representational formats such as
natural language, code, and symbolic logic. In contrast, most existing
LLM-based approaches operate with a single reasoning modality during training,
typically natural language. Although some methods explored modality selection
or augmentation at inference time, the training process remains modality-blind,
limiting synergy among modalities. To fill in this gap, we propose
Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three
complementary modalities: natural language, code, and a newly introduced
symbolic modality, truth-table, which systematically enumerates logical cases
and partially mitigates key failure modes in natural language reasoning. MoT
adopts a two-phase design: (1) self-evolving MoT training, which jointly learns
from filtered, self-generated rationales across modalities; and (2) MoT
inference, which fully leverages the synergy of three modalities to produce
better predictions. Experiments on logical reasoning benchmarks including FOLIO
and ProofWriter demonstrate that our MoT framework consistently and
significantly outperforms strong LLM baselines with single-modality
chain-of-thought approaches, achieving up to +11.7pp average accuracy gain.
Further analyses show that our MoT framework benefits both training and
inference stages; that it is particularly effective on harder logical reasoning
problems; and that different modalities contribute complementary strengths,
with truth-table reasoning helping to overcome key bottlenecks in natural
language inference.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [Stochastic Fractional Neural Operators: A Symmetrized Approach to Modeling Turbulence in Complex Fluid Dynamics](https://arxiv.org/abs/2505.14700)
*Rômulo Damasclin Chaves dos Santos,Jorge Henrique de Oliveira Sales*

Key words: 神经网络算子,分数阶导数,随机分析,湍流模型,Voronovskaya定理

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种结合记忆效应和随机性的新型神经网络算子，通过对称激活函数、Caputo型分数阶导数和Itô型噪声，为复杂系统的分析和模拟提供理论支持。

Motivation: 针对具有长期记忆和不确定性动态的系统，结合神经网络、分数阶微积分和随机分析，开发一种新的数学工具。

Method: 采用对称激活函数、Caputo型分数阶导数和Itô型噪声构建神经网络算子，并证明其Voronovskaya型定理，包括渐近行为、均方收敛和一致性。

Result: 理论应用于分数阶Navier-Stokes方程，证实算子能有效近似具有记忆和随机性的湍流模型。

Conclusion: 该研究为复杂系统的混合学习方法奠定了理论基础，展示了神经网络算子在多尺度过程建模中的潜力。

Abstract: In this work, we introduce a new class of neural network operators designed
to handle problems where memory effects and randomness play a central role. In
this work, we introduce a new class of neural network operators designed to
handle problems where memory effects and randomness play a central role. These
operators merge symmetrized activation functions, Caputo-type fractional
derivatives, and stochastic perturbations introduced via It\^o type noise. The
result is a powerful framework capable of approximating functions that evolve
over time with both long-term memory and uncertain dynamics. We develop the
mathematical foundations of these operators, proving three key theorems of
Voronovskaya type. These results describe the asymptotic behavior of the
operators, their convergence in the mean-square sense, and their consistency
under fractional regularity assumptions. All estimates explicitly account for
the influence of the memory parameter $\alpha$ and the noise level $\sigma$. As
a practical application, we apply the proposed theory to the fractional
Navier-Stokes equations with stochastic forcing, a model often used to describe
turbulence in fluid flows with memory. Our approach provides theoretical
guarantees for the approximation quality and suggests that these neural
operators can serve as effective tools in the analysis and simulation of
complex systems. By blending ideas from neural networks, fractional calculus,
and stochastic analysis, this research opens new perspectives for modeling
turbulent phenomena and other multiscale processes where memory and randomness
are fundamental. The results lay the groundwork for hybrid learning-based
methods with strong analytical backing.

</details>


### [145] [The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents](https://arxiv.org/abs/2505.14727)
*Mohammad Rubyet Islam*

Key words: alpha收益，AI驱动系统，五阶段分类法，LLM代理，多模态数据融合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种五阶段分类法，追踪从直觉驱动投资到AI自主系统的转变，强调系统级视角、多模态数据融合和LLM代理的重要性，并探讨了实际部署中的关键挑战。

Motivation: 研究超越市场基准的alpha收益获取方法的演变，从传统投资到AI驱动系统的转变。

Method: 提出五阶段分类法，涵盖手动策略、统计模型、经典机器学习、深度学习及LLM代理架构，系统整合表征学习、多模态数据融合和工具增强LLM代理。

Result: 建立了一个评估成熟度、协调基础设施和指导下一代alpha系统开发的统一框架。

Conclusion: 分类法为下一代理财系统的负责任发展提供了系统级视角和实用指导。

Abstract: The pursuit of alpha returns that exceed market benchmarks has undergone a
profound transformation, evolving from intuition-driven investing to
autonomous, AI powered systems. This paper introduces a comprehensive five
stage taxonomy that traces this progression across manual strategies,
statistical models, classical machine learning, deep learning, and agentic
architectures powered by large language models (LLMs). Unlike prior surveys
focused narrowly on modeling techniques, this review adopts a system level
lens, integrating advances in representation learning, multimodal data fusion,
and tool augmented LLM agents. The strategic shift from static predictors to
contextaware financial agents capable of real time reasoning, scenario
simulation, and cross modal decision making is emphasized. Key challenges in
interpretability, data fragility, governance, and regulatory compliance areas
critical to production deployment are examined. The proposed taxonomy offers a
unified framework for evaluating maturity, aligning infrastructure, and guiding
the responsible development of next generation alpha systems.

</details>


### [146] [The Energy Cost of Reasoning: Analyzing Energy Usage in LLMs with Test-time Compute](https://arxiv.org/abs/2505.14733)
*Yunho Jin,Gu-Yeon Wei,David Brooks*

Key words: 大语言模型, 推理计算, 能源效率, 模型扩展, TTC

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了在推理阶段分配额外计算资源（TTC）作为传统模型扩展的补充，以提升准确性与能源效率的策略。

Motivation: 随着大语言模型（LLM）的扩展面临收益递减和能耗增加的问题，研究TTC是否能提供更优的准确性与能源效率权衡。

Method: 通过实证分析比较TTC与传统模型扩展在任务表现和能源消耗上的差异，并研究TTC与输出序列长度的关系。

Result: TTC在复杂推理任务中比传统扩展更高效，且可根据查询复杂度动态调整计算资源提升效率。

Conclusion: TTC是一种无需额外预训练成本即可实现可持续、准确且灵活部署语言模型的有前景方法。

Abstract: Scaling large language models (LLMs) has driven significant advancements, yet
it faces diminishing returns and escalating energy demands. This work
introduces test-time compute (TTC)-allocating additional computational
resources during inference-as a compelling complement to conventional scaling
strategies. Specifically, we investigate whether employing TTC can achieve
superior accuracy-energy trade-offs compared to simply increasing model size.
Our empirical analysis reveals that TTC surpasses traditional model scaling in
accuracy/energy efficiency, with notable gains in tasks demanding complex
reasoning rather than mere factual recall. Further, we identify a critical
interaction between TTC performance and output sequence length, demonstrating
that strategically adjusting compute resources at inference time according to
query complexity can substantially enhance efficiency. Our findings advocate
for TTC as a promising direction, enabling more sustainable, accurate, and
adaptable deployment of future language models without incurring additional
pretraining costs.

</details>


### [147] [Leveraging Multivariate Long-Term History Representation for Time Series Forecasting](https://arxiv.org/abs/2505.14737)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Stephane Dellacherie,Mouhamadou Makhtar Dione,Benoit Boulet*

Key words: 多元时间序列预测, 时空图神经网络, 长期依赖建模, LMHR框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为LMHR的框架，通过增强STGNN的长时空依赖建模能力，显著提升了多元时间序列预测的准确性。

Motivation: 现有STGNN方法主要关注短期和局部的时空依赖，忽略了长期时空相似性和相关性，这限制了预测的准确性。

Method: LMHR结合了长时历史编码器(LHEncoder)、层次表示检索器(HRetriever)和基于Transformer的聚合器(TAggregator)，以高效建模长期时空依赖。

Result: LMHR在多个真实数据集上优于典型STGNN方法10.72%，并超越现有最优方法4.12%，且在快速变化模式上的预测准确性提高了9.8%。

Conclusion: LMHR通过有效建模长期时空依赖，显著提升了多元时间序列预测的性能。

Abstract: Multivariate Time Series (MTS) forecasting has a wide range of applications
in both industry and academia. Recent advances in Spatial-Temporal Graph Neural
Network (STGNN) have achieved great progress in modelling spatial-temporal
correlations. Limited by computational complexity, most STGNNs for MTS
forecasting focus primarily on short-term and local spatial-temporal
dependencies. Although some recent methods attempt to incorporate univariate
history into modeling, they still overlook crucial long-term spatial-temporal
similarities and correlations across MTS, which are essential for accurate
forecasting. To fill this gap, we propose a framework called the Long-term
Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.
Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively
encode the long-term history into segment-level contextual representations and
reduce point-level noise. A non-parametric Hierarchical Representation
Retriever (HRetriever) is designed to include the spatial information in the
long-term spatial-temporal dependency modelling and pick out the most valuable
representations with no additional training. A Transformer-based Aggregator
(TAggregator) selectively fuses the sparsely retrieved contextual
representations based on the ranking positional embedding efficiently.
Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%
on the average prediction horizons and state-of-the-art methods by 4.12% on
several real-world datasets. Additionally, it consistently improves prediction
accuracy by 9.8% on the top 10% of rapidly changing patterns across the
datasets.

</details>


### [148] [Time Series Similarity Score Functions to Monitor and Interact with the Training and Denoising Process of a Time Series Diffusion Model applied to a Human Activity Recognition Dataset based on IMUs](https://arxiv.org/abs/2505.14739)
*Heiko Oppel,Andreas Spilz,Michael Munz*

Key words: 去噪扩散概率模型, 相似性度量, 数据生成, 训练优化, 分类任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了去噪扩散概率模型在生成合成传感器信号时的训练过程优化问题，提出了一种改进的相似性度量方法，以降低训练周期同时保持分类任务性能。

Motivation: 传统的去噪扩散概率模型在生成数据时难以评估数据质量，需要优化训练过程以减少资源消耗和时间。

Method: 通过研究多种相似性度量方法，并对现有度量进行适配，以监控训练和合成过程；改进的度量方法可根据分类任务需求进行微调。

Result: 改进方法显著减少了训练周期，同时未降低分类任务的性能。

Conclusion: 优化后的训练过程节省了资源和时间，提升了生成模型的效率。

Abstract: Denoising diffusion probabilistic models are able to generate synthetic
sensor signals. The training process of such a model is controlled by a loss
function which measures the difference between the noise that was added in the
forward process and the noise that was predicted by the diffusion model. This
enables the generation of realistic data. However, the randomness within the
process and the loss function itself makes it difficult to estimate the quality
of the data. Therefore, we examine multiple similarity metrics and adapt an
existing metric to overcome this issue by monitoring the training and
synthetisation process using those metrics. The adapted metric can even be
fine-tuned on the input data to comply with the requirements of an underlying
classification task. We were able to significantly reduce the amount of
training epochs without a performance reduction in the classification task. An
optimized training process not only saves resources, but also reduces the time
for training generative models.

</details>


### [149] [Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism](https://arxiv.org/abs/2505.14741)
*Kunyun Wang,Bohan Li,Kai Yu,Minyi Guo,Jieru Zhao*

Key words: 扩散模型,并行化,推理加速,通信开销,生成质量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为ParaStep的新型并行化方法，通过重用-预测机制减少扩散模型推理延迟，显著提升速度同时保持生成质量。

Motivation: 扩散模型虽强大，但其顺序去噪过程导致推理延迟高，现有并行化方法因通信开销大难以商用。

Method: 采用重用-预测机制，利用相邻去噪步骤的相似性并行化推理，实现轻量级的步骤间通信。

Result: 在多个基准测试中，ParaStep实现了显著的加速（最高6.56倍），同时保持生成质量。

Conclusion: ParaStep是一种可扩展且通信高效的解决方案，特别适合带宽受限环境。

Abstract: Diffusion models have emerged as a powerful class of generative models across
various modalities, including image, video, and audio synthesis. However, their
deployment is often limited by significant inference latency, primarily due to
the inherently sequential nature of the denoising process. While existing
parallelization strategies attempt to accelerate inference by distributing
computation across multiple devices, they typically incur high communication
overhead, hindering deployment on commercial hardware. To address this
challenge, we propose \textbf{ParaStep}, a novel parallelization method based
on a reuse-then-predict mechanism that parallelizes diffusion inference by
exploiting similarity between adjacent denoising steps. Unlike prior approaches
that rely on layer-wise or stage-wise communication, ParaStep employs
lightweight, step-wise communication, substantially reducing overhead. ParaStep
achieves end-to-end speedups of up to \textbf{3.88}$\times$ on SVD,
\textbf{2.43}$\times$ on CogVideoX-2b, and \textbf{6.56}$\times$ on
AudioLDM2-large, while maintaining generation quality. These results highlight
ParaStep as a scalable and communication-efficient solution for accelerating
diffusion inference, particularly in bandwidth-constrained environments.

</details>


### [150] [Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis](https://arxiv.org/abs/2505.14742)
*Hong Huang,Dapeng Wu*

Key words: LLMs, Quantization, Parameter-efficient Fine-tuning, OSSH, Quaff

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Quaff的高效量化参数微调框架，通过动态抑制激活异常值优化低精度激活表示，解决了量化微调中的性能与开销问题。

Motivation: 大型语言模型（LLMs）在资源受限的个人设备上部署受限于计算和内存需求，现有量化方法难以平衡性能与开销。

Method: 基于Outlier Spatial Stability Hypothesis（OSSH），开发了Quaff框架，通过目标动量缩放和轻量操作动态抑制异常值。

Result: 实验表明，Quaff在GPQA基准测试中实现了1.73倍延迟降低和30%内存节省，同时准确性提升0.6%。

Conclusion: Quaff框架在效率、性能和可部署性之间取得了平衡，使消费级GPU也能完成LLM微调。

Abstract: Large language models (LLMs) have made exciting achievements across various
domains, yet their deployment on resource-constrained personal devices remains
hindered by the prohibitive computational and memory demands of task-specific
fine-tuning. While quantization offers a pathway to efficiency, existing
methods struggle to balance performance and overhead, either incurring high
computational/memory costs or failing to address activation outliers, a
critical bottleneck in quantized fine-tuning. To address these challenges, we
propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,
certain activation outlier channels retain stable spatial positions across
training iterations. Building on OSSH, we propose Quaff, a Quantized
parameter-efficient fine-tuning framework for LLMs, optimizing low-precision
activation representations through targeted momentum scaling. Quaff dynamically
suppresses outliers exclusively in invariant channels using lightweight
operations, eliminating full-precision weight storage and global rescaling
while reducing quantization errors. Extensive experiments across ten benchmarks
validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA
reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory
savings over full-precision fine-tuning while improving accuracy by 0.6% on the
Phi-3 model, reconciling the triple trade-off between efficiency, performance,
and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080
Super) without sacrificing model utility, Quaff democratizes personalized LLM
deployment. The code is available at https://github.com/Little0o0/Quaff.git.

</details>


### [151] [Explainable Prediction of the Mechanical Properties of Composites with CNNs](https://arxiv.org/abs/2505.14745)
*Varun Raaghav,Dimitrios Bikos,Antonio Rago,Francesca Toni,Maria Charalambides*

Key words: 复合材料, 卷积神经网络, 可解释AI, 有限元建模, 力学性能

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种使用卷积神经网络(CNN)和可解释AI(XAI)方法预测复合材料力学性能的高精度解决方案，优于传统有限元建模和基础神经网络方法。

Motivation: 由于有限元建模计算成本高且传统AI方法在预测复合材料力学性能时存在精度低、不考虑材料强度限制和缺乏透明度等问题，作者提出了使用CNN和XAI的解决方案。

Method: 采用定制化CNN模型，通过有限元建模生成的横向张力测试数据集训练，预测复合材料的杨氏模量和屈服强度，并使用SHAP和Integrated Gradients解释预测结果。

Result: 实验证明该方法在预测力学性能方面具有高准确性，优于ResNet-34基线模型，并通过XAI方法验证了模型的科学合理性。

Conclusion: 结合CNN和XAI的方法不仅提高了预测精度，还通过解释模型决策过程增强了工程师对模型的信任度。

Abstract: Composites are amongst the most important materials manufactured today, as
evidenced by their use in countless applications. In order to establish the
suitability of composites in specific applications, finite element (FE)
modelling, a numerical method based on partial differential equations, is the
industry standard for assessing their mechanical properties. However, FE
modelling is exceptionally costly from a computational viewpoint, a limitation
which has led to efforts towards applying AI models to this task. However, in
these approaches: the chosen model architectures were rudimentary, feed-forward
neural networks giving limited accuracy; the studies focus on predicting
elastic mechanical properties, without considering material strength limits;
and the models lacked transparency, hindering trustworthiness by users. In this
paper, we show that convolutional neural networks (CNNs) equipped with methods
from explainable AI (XAI) can be successfully deployed to solve this problem.
Our approach uses customised CNNs trained on a dataset we generate using
transverse tension tests in FE modelling to predict composites' mechanical
properties, i.e., Young's modulus and yield strength. We show empirically that
our approach achieves high accuracy, outperforming a baseline, ResNet-34, in
estimating the mechanical properties. We then use SHAP and Integrated
Gradients, two post-hoc XAI methods, to explain the predictions, showing that
the CNNs use the critical geometrical features that influence the composites'
behaviour, thus allowing engineers to verify that the models are trustworthy by
representing the science of composites.

</details>


### [152] [Cooperative Causal GraphSAGE](https://arxiv.org/abs/2505.14748)
*Zaifa Xue,Tao Zhang,Tuo Xu,Huaixin Liang,Le Gao*

Key words: GraphSAGE, 因果推断, 合作博弈理论, Shapley值, 鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种结合合作博弈理论的CoCa-GraphSAGE方法，通过计算节点集的合作因果效应，改进采样过程并提升模型鲁棒性。

Motivation: 现有的Causal GraphSAGE忽略了采样节点间的合作关系，导致性能不足。

Method: 提出CoCa-GraphSAGE，构建合作因果结构模型，并设计CoCa-sampling算法，利用Shapley值计算节点集的合作贡献。

Result: 实验表明，新方法在分类性能上与传统方法相当，且在扰动下表现更优。

Conclusion: 合作因果采样有效提升了模型的鲁棒性。

Abstract: GraphSAGE is a widely used graph neural network. The introduction of causal
inference has improved its robust performance and named as Causal GraphSAGE.
However, Causal GraphSAGE focuses on measuring causal weighting among
individual nodes, but neglecting the cooperative relationships among sampling
nodes as a whole. To address this issue, this paper proposes Cooperative Causal
GraphSAGE (CoCa-GraphSAGE), which combines cooperative game theory with Causal
GraphSAGE. Initially, a cooperative causal structure model is constructed in
the case of cooperation based on the graph structure. Subsequently, Cooperative
Causal sampling (CoCa-sampling) algorithm is proposed, employing the Shapley
values to calculate the cooperative contribution based on causal weights of the
nodes sets. CoCa-sampling guides the selection of nodes with significant
cooperative causal effects during the neighborhood sampling process, thus
integrating the selected neighborhood features under cooperative relationships,
which takes the sampled nodes as a whole and generates more stable target node
embeddings. Experiments on publicly available datasets show that the proposed
method has comparable classification performance to the compared methods and
outperforms under perturbations, demonstrating the robustness improvement by
CoCa-sampling.

</details>


### [153] [Self Distillation via Iterative Constructive Perturbations](https://arxiv.org/abs/2505.14751)
*Maheak Dave,Aniket Kumar Singh,Aryan Pareek,Harshita Jha,Debasis Chaudhuri,Manish Pratap Singh*

Key words: 深度神经网络,循环优化,迭代构造扰动,自蒸馏,泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新颖的循环优化框架，通过迭代构造扰动（ICP）优化模型和输入数据，提升训练效果。

Motivation: 解决深度神经网络在训练中性能和泛化能力难以兼顾的挑战。

Method: 采用ICP技术，利用模型损失迭代扰动输入，构建增强表示，并结合自蒸馏框架优化模型。

Result: 实验表明，该方法有效缓解了神经网络的性能瓶颈，并在多种训练变体中显著提升表现。

Conclusion: 循环优化策略通过交替优化模型和输入数据，显著改善了训练效果。

Abstract: Deep Neural Networks have achieved remarkable achievements across various
domains, however balancing performance and generalization still remains a
challenge while training these networks. In this paper, we propose a novel
framework that uses a cyclic optimization strategy to concurrently optimize the
model and its input data for better training, rethinking the traditional
training paradigm. Central to our approach is Iterative Constructive
Perturbation (ICP), which leverages the model's loss to iteratively perturb the
input, progressively constructing an enhanced representation over some
refinement steps. This ICP input is then fed back into the model to produce
improved intermediate features, which serve as a target in a self-distillation
framework against the original features. By alternately altering the model's
parameters to the data and the data to the model, our method effectively
addresses the gap between fitting and generalization, leading to enhanced
performance. Extensive experiments demonstrate that our approach not only
mitigates common performance bottlenecks in neural networks but also
demonstrates significant improvements across training variations.

</details>


### [154] [Large Language Models for Data Synthesis](https://arxiv.org/abs/2505.14752)
*Yihong Tang,Menglin Kong,Lijun Sun*

Key words: 合成数据, 大语言模型, 统计对齐, 非参数模拟, LLMSynthor

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LLMSynthor框架通过LLMs生成统计对齐的合成数据，解决了高维或异构域中数据合成的挑战。

Motivation: 解决传统方法在高维或异构数据中依赖强参数假设或手动设计的问题，利用LLMs作为灵活的先验分布。

Method: 引入LLMSynthor框架，将LLMs视为非参数Copula模拟器，通过LLM Proposal Sampling提升效率，并通过迭代合成循环最小化统计差异。

Result: 合成的数据在统计保真度、实用性和跨数据适应性方面表现出色。

Conclusion: LLMSynthor为经济学、社会科学和城市研究等领域提供了高效的数据合成工具。

Abstract: Generating synthetic data that faithfully captures the statistical structure
of real-world distributions is a fundamental challenge in data modeling.
Classical approaches often depend on strong parametric assumptions or manual
structural design and struggle in high-dimensional or heterogeneous domains.
Recent progress in Large Language Models (LLMs) reveals their potential as
flexible, high-dimensional priors over real-world distributions. However, when
applied to data synthesis, standard LLM-based sampling is inefficient,
constrained by fixed context limits, and fails to ensure statistical alignment.
Given this, we introduce LLMSynthor, a general framework for data synthesis
that transforms LLMs into structure-aware simulators guided by distributional
feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for
modeling high-order dependencies and introduces LLM Proposal Sampling to
generate grounded proposal distributions that improve sampling efficiency
without requiring rejection. By minimizing discrepancies in the summary
statistics space, the iterative synthesis loop aligns real and synthetic data
while gradually uncovering and refining the latent generative structure. We
evaluate LLMSynthor in both controlled and real-world settings using
heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,
population, and mobility) that encompass both structured and unstructured
formats. The synthetic data produced by LLMSynthor shows high statistical
fidelity, practical utility, and cross-data adaptability, positioning it as a
valuable tool across economics, social science, urban studies, and beyond.

</details>


### [155] [$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization](https://arxiv.org/abs/2505.14756)
*Chih-Yu Chang,Milad Azvar,Chinedum Okwudire,Raed Al Kontar*

Key words: 贝叶斯优化, 大型语言模型, 高斯过程, 探索-开发权衡, 3D打印

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LLINBO：一种结合大型语言模型（LLMs）与统计代理模型（如高斯过程）的贝叶斯优化框架，旨在利用LLM的上下文推理能力进行早期探索，依靠统计模型实现高效开发。

Motivation: LLMs在低数据量下表现出色，但缺乏显式代理模型和校准的不确定性，导致探索-开发权衡难以控制。LLINBO旨在结合两者的优势。

Method: 提出LLINBO框架，结合LLMs与统计代理模型（如GP），并引入三种协作机制，提供理论保证。

Result: 在3D打印的实际应用中验证了框架的有效性。

Conclusion: LLINBO通过结合LLMs的推理能力和统计模型的可靠性，提升了贝叶斯优化的效率和可信度。

Abstract: Bayesian optimization (BO) is a sequential decision-making tool widely used
for optimizing expensive black-box functions. Recently, Large Language Models
(LLMs) have shown remarkable adaptability in low-data regimes, making them
promising tools for black-box optimization by leveraging contextual knowledge
to propose high-quality query points. However, relying solely on LLMs as
optimization agents introduces risks due to their lack of explicit surrogate
modeling and calibrated uncertainty, as well as their inherently opaque
internal mechanisms. This structural opacity makes it difficult to characterize
or control the exploration-exploitation trade-off, ultimately undermining
theoretical tractability and reliability. To address this, we propose LLINBO:
LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with
statistical surrogate experts (e.g., Gaussian Processes (GP)). The core
philosophy is to leverage contextual reasoning strengths of LLMs for early
exploration, while relying on principled statistical models to guide efficient
exploitation. Specifically, we introduce three mechanisms that enable this
collaboration and establish their theoretical guarantees. We end the paper with
a real-life proof-of-concept in the context of 3D printing. The code to
reproduce the results can be found at
https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.

</details>


### [156] [Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding](https://arxiv.org/abs/2505.14765)
*Orhun Vural,Bunyamin Ozaydin,Khalid Y. Aram,James Booth,Brittany F. Lindsey,Abdulaziz Ahmed*

Key words: 深度学习,急诊科,时间序列预测,N-BEATSx,运营决策

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究开发了深度学习模型，仅使用非临床数据和背景特征，提前6小时预测急诊科（ED）候诊患者数量，支持医院提前决策。N-BEATSx模型表现最佳，误差较低。

Motivation: 为了解决急诊科候诊患者数量预测问题，支持医院运营决策，避免过度拥挤。

Method: 从五个数据源收集数据，进行特征工程后，使用多种时间序列深度学习模型（如ResNetPlus、TSTPlus、TSiTPlus和N-BEATSx）进行训练和优化。

Result: N-BEATSx模型表现最优，预测误差低，且在极端情况下仍能保持稳定性。

Conclusion: 该框架为医院提供了一种可行且通用的方法，可预测急诊候诊情况，缓解拥挤。

Abstract: This study develops deep learning models to forecast the number of patients
in the emergency department (ED) boarding phase six hours in advance, aiming to
support proactive operational decision-making using only non-clinical,
operational, and contextual features. Data were collected from five sources: ED
tracking systems, inpatient census records, weather reports, federal holiday
calendars, and local event schedules. After feature engineering, the data were
aggregated at an hourly level, cleaned, and merged into a unified dataset for
model training. Several time series deep learning models, including ResNetPlus,
TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using
Optuna and grid search for hyperparameter tuning. The average ED boarding count
was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best
performance, with a mean absolute error of 2.10, mean squared error of 7.08,
root mean squared error of 2.66, and a coefficient of determination of 0.95.
The model maintained stable accuracy even during periods of extremely high
boarding counts, defined as values exceeding one, two, or three standard
deviations above the mean. Results show that accurate six-hour-ahead forecasts
are achievable without using patient-level clinical data. While strong
performance was observed even with a basic feature set, the inclusion of
additional features improved prediction stability under extreme conditions.
This framework offers a practical and generalizable approach for hospital
systems to anticipate boarding levels and help mitigate ED overcrowding.

</details>


### [157] [This Time is Different: An Observability Perspective on Time Series Foundation Models](https://arxiv.org/abs/2505.14766)
*Ben Cohen,Emaad Khwaja,Youssef Doubli,Salahidine Lemaachi,Chris Lettieri,Charles Masson,Hugo Miccinilli,Elise Ramé,Qiqi Ren,Afshin Rostamizadeh,Jean Ogier du Terrail,Anna-Monica Toon,Kan Wang,Stephan Xie,David Asker,Ameet Talwalkar,Othmane Abou-Amal*

Key words: 时间序列预测，基础模型，可观测性数据，BOOM基准测试，预训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Toto是一个时间序列预测基础模型，参数达1.51亿，针对多元可观测性时间序列数据的挑战进行了架构创新，并在大型基准测试中表现优异。

Motivation: 现有时间序列预测模型在处理多元可观测性数据时面临挑战，Toto旨在通过创新架构和大规模预训练数据解决这一问题。

Method: Toto采用仅解码器架构，结合了专门为多元可观测性数据设计的创新技术，并使用混合数据预训练。

Result: Toto在BOOM基准测试和通用时间序列预测任务中均达到最优性能。

Conclusion: Toto通过架构创新和大规模预训练，显著提升了多元可观测性时间序列数据的预测能力。

Abstract: We introduce Toto, a time series forecasting foundation model with 151
million parameters. Toto uses a modern decoder-only architecture coupled with
architectural innovations designed to account for specific challenges found in
multivariate observability time series data. Toto's pre-training corpus is a
mixture of observability data, open datasets, and synthetic data, and is
4-10$\times$ larger than those of leading time series foundation models.
Additionally, we introduce BOOM, a large-scale benchmark consisting of 350
million observations across 2,807 real-world time series. For both Toto and
BOOM, we source observability data exclusively from Datadog's own telemetry and
internal observability metrics. Extensive evaluations demonstrate that Toto
achieves state-of-the-art performance on both BOOM and on established general
purpose time series forecasting benchmarks. Toto's model weights, inference
code, and evaluation scripts, as well as BOOM's data and evaluation code, are
all available as open source under the Apache 2.0 License available at
https://huggingface.co/Datadog/Toto-Open-Base-1.0 and
https://github.com/DataDog/toto.

</details>


### [158] [KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches](https://arxiv.org/abs/2505.14777)
*Mingquan Feng,Yixin Huang,Yifan Fu,Shaobo Wang,Junchi Yan*

Key words: 神经网络优化, 动力学理论, Boltzmann输运方程, 参数多样性, 热扩散

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于动力学理论和PDE模拟的新优化器KO，通过BTE模拟参数更新，提升参数多样性，实验显示其在多种任务上优于基线优化器。

Motivation: 现有神经网络优化器多基于启发式梯度方法，缺乏理论支持，KO通过物理驱动的动力学理论改进优化过程。

Method: 利用Boltzmann输运方程模拟参数更新的动态，类似于粒子系统的随机碰撞，通过热扩散机制防止参数坍缩。

Result: 在图像和文本分类任务中，KO在保持计算成本的同时，精度优于Adam和SGD等基线方法。

Conclusion: KO通过物理驱动的设计有效提升了优化效果，为解决参数坍缩提供了新思路。

Abstract: The design of optimization algorithms for neural networks remains a critical
challenge, with most existing methods relying on heuristic adaptations of
gradient-based approaches. This paper introduces KO (Kinetics-inspired
Optimizer), a novel neural optimizer inspired by kinetic theory and partial
differential equation (PDE) simulations. We reimagine the training dynamics of
network parameters as the evolution of a particle system governed by kinetic
principles, where parameter updates are simulated via a numerical scheme for
the Boltzmann transport equation (BTE) that models stochastic particle
collisions. This physics-driven approach inherently promotes parameter
diversity during optimization, mitigating the phenomenon of parameter
condensation, i.e. collapse of network parameters into low-dimensional
subspaces, through mechanisms analogous to thermal diffusion in physical
systems. We analyze this property, establishing both a mathematical proof and a
physical interpretation. Extensive experiments on image classification
(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks
demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,
SGD), achieving accuracy improvements while computation cost remains
comparable.

</details>


### [159] [Text embedding models can be great data engineers](https://arxiv.org/abs/2505.14802)
*Iman Kazemian,Paritosh Ramanan,Murat Yildirim*

Key words: 数据工程管道, 文本嵌入, 时间序列, 自动化, 变分信息瓶颈

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ADEPT是一种通过文本嵌入自动化的数据工程管道，能够高效处理时间序列数据，优于传统数据工程管道。

Motivation: 传统数据工程管道需要大量工程时间和领域知识，成本高昂，ADEPT旨在通过自动化技术简化这一过程。

Method: ADEPT采用两步法：利用文本嵌入表示多样化数据源，并通过变分信息瓶颈准则减少文本嵌入的熵方差。

Result: 实验表明，ADEPT在多样化的数据集中优于现有基准，尤其在处理缺失数据、格式错误等问题时表现突出。

Conclusion: ADEPT能够跳过传统数据管道步骤，为数据科学应用提供高效且可扩展的自动化途径。

Abstract: Data engineering pipelines are essential - albeit costly - components of
predictive analytics frameworks requiring significant engineering time and
domain expertise for carrying out tasks such as data ingestion, preprocessing,
feature extraction, and feature engineering. In this paper, we propose ADEPT,
an automated data engineering pipeline via text embeddings. At the core of the
ADEPT framework is a simple yet powerful idea that the entropy of embeddings
corresponding to textually dense raw format representation of time series can
be intuitively viewed as equivalent (or in many cases superior) to that of
numerically dense vector representations obtained by data engineering
pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text
embeddings to represent the diverse data sources, and (ii) constructs a
variational information bottleneck criteria to mitigate entropy variance in
text embeddings of time series data. ADEPT provides an end-to-end automated
implementation of predictive models that offers superior predictive performance
despite issues such as missing data, ill-formed records, improper or corrupted
data formats and irregular timestamps. Through exhaustive experiments, we show
that the ADEPT outperforms the best existing benchmarks in a diverse set of
datasets from large-scale applications across healthcare, finance, science and
industrial internet of things. Our results show that ADEPT can potentially
leapfrog many conventional data pipeline steps thereby paving the way for
efficient and scalable automation pathways for diverse data science
applications.

</details>


### [160] [SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis](https://arxiv.org/abs/2505.14803)
*Yu Liu,Weiyao Tao,Tong Xia,Simon Knight,Tingting Zhu*

Key words: 生存分析、不确定性量化、元模型、可解释性、模型无关

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为SurvUnc的新型元模型框架，用于后验生存模型的不确定性量化，以提升模型的可解释性和可靠性。

Motivation: 尽管生存模型在多个领域（如医疗和风险评估）中非常重要，但其预测不确定性量化问题尚未得到充分研究，限制了模型的信任度和应用范围。

Method: 提出了基于锚点的学习策略，将一致性知识融入元模型优化中，并通过成对排序性能有效估计不确定性。该框架模型无关，无需修改原模型架构或访问内部参数。

Result: 在四个公开基准数据集和五种代表性生存模型上的实验表明，SurvUnc在选择性预测、错误预测检测和域外检测等场景中表现优越。

Conclusion: SurvUnc显著提升了生存模型的可解释性和可靠性，为现实应用中的可信预测铺平了道路。

Abstract: Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.

</details>


### [161] [Imitation Learning via Focused Satisficing](https://arxiv.org/abs/2505.14820)
*Rushit N. Shah,Nikolaos Agadakos,Synthia Sasulski,Ali Farajzadeh,Sanjiban Choudhury,Brian Ziebart*

Key words: 模仿学习, 满意化理论, 深度强化学习, 期望水平, 边际目标

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于“满意化理论”的模仿学习方法，旨在超越演示者的期望水平，而非追求最优性。

Motivation: 传统模仿学习假设演示接近最优，但人类行为往往基于动态的期望水平。研究旨在利用演示的“满意”标准，而非最优性。

Method: 采用基于边际的目标函数指导深度强化学习，专注于超越演示者的期望水平，而无需明确学习这些期望。

Result: 实验表明，该方法在模仿高质量演示部分上优于现有方法，保证更高的可接受性，并在多种环境中表现竞争力。

Conclusion: 满意化模仿学习方法能更有效地模仿高质量行为，适用于动态期望的场景。

Abstract: Imitation learning often assumes that demonstrations are close to optimal
according to some fixed, but unknown, cost function. However, according to
satisficing theory, humans often choose acceptable behavior based on their
personal (and potentially dynamic) levels of aspiration, rather than achieving
(near-) optimality. For example, a lunar lander demonstration that successfully
lands without crashing might be acceptable to a novice despite being slow or
jerky. Using a margin-based objective to guide deep reinforcement learning, our
focused satisficing approach to imitation learning seeks a policy that
surpasses the demonstrator's aspiration levels -- defined over trajectories or
portions of trajectories -- on unseen demonstrations without explicitly
learning those aspirations. We show experimentally that this focuses the policy
to imitate the highest quality (portions of) demonstrations better than
existing imitation learning methods, providing much higher rates of guaranteed
acceptability to the demonstrator, and competitive true returns on a range of
environments.

</details>


### [162] [Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation](https://arxiv.org/abs/2505.14821)
*Runze Zhao,Yue Yu,Adams Yiyue Zhu,Chen Yang,Dongruo Zhou*

Key words: 连续时间强化学习, 通用函数逼近, 样本效率, 计算效率, 乐观置信集

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于模型的连续时间强化学习算法，通过乐观置信集和结构化策略更新实现样本和计算效率。

Motivation: 当前连续时间强化学习在理论理解上不足，尤其是在通用函数逼近场景下；旨在填补这一空白。

Method: 利用乐观置信集构建置信区间，提出结构化策略更新和替代测量策略以减少策略更新和rollouts次数。

Result: 算法在连续控制任务和扩散模型微调中表现优异，样本复杂度为Õ(√(dᵣ + d_f)/√N)，显著减少计算开销。

Conclusion: 该研究为连续时间强化学习提供了理论保证和实用算法，显著提升了效率和性能。

Abstract: Continuous-time reinforcement learning (CTRL) provides a principled framework
for sequential decision-making in environments where interactions evolve
continuously over time. Despite its empirical success, the theoretical
understanding of CTRL remains limited, especially in settings with general
function approximation. In this work, we propose a model-based CTRL algorithm
that achieves both sample and computational efficiency. Our approach leverages
optimism-based confidence sets to establish the first sample complexity
guarantee for CTRL with general function approximation, showing that a
near-optimal policy can be learned with a suboptimality gap of
$\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$
measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the
distributional Eluder dimensions of the reward and dynamic functions,
respectively, capturing the complexity of general function approximation in
reinforcement learning. Moreover, we introduce structured policy updates and an
alternative measurement strategy that significantly reduce the number of policy
updates and rollouts while maintaining competitive sample efficiency. We
implemented experiments to backup our proposed algorithms on continuous control
tasks and diffusion model fine-tuning, demonstrating comparable performance
with significantly fewer policy updates and rollouts.

</details>


### [163] [Assimilative Causal Inference](https://arxiv.org/abs/2505.14825)
*Marios Andreou,Nan Chen,Erik Bollt*

Key words: 因果推理, 贝叶斯数据同化, 动态系统, 高维问题, 短时序数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的因果推理框架ACI，通过动态系统和贝叶斯数据同化方法解决传统因果推理的局限性，适用于高维系统和短时序数据。

Motivation: 传统因果推理方法在处理时间演化的短期因果关系和高维系统时存在局限性，需要一种更灵活和高效的方法。

Method: ACI基于动态系统和贝叶斯数据同化，通过逆向问题识别瞬时因果关系和动态因果影响范围（CIR）。

Result: ACI能够动态捕捉变量间的因果关系变化，适用于高维系统和短时序数据，且无需观察所有候选原因。

Conclusion: ACI为复杂动态系统的因果推理提供了高效、灵活的解决方案。

Abstract: Causal inference determines cause-and-effect relationships between variables
and has broad applications across disciplines. Traditional time-series methods
often reveal causal links only in a time-averaged sense, while ensemble-based
information transfer approaches detect the time evolution of short-term causal
relationships but are typically limited to low-dimensional systems. In this
paper, a new causal inference framework, called assimilative causal inference
(ACI), is developed. Fundamentally different from the state-of-the-art methods,
ACI uses a dynamical system and a single realization of a subset of the state
variables to identify instantaneous causal relationships and the dynamic
evolution of the associated causal influence range (CIR). Instead of
quantifying how causes influence effects as done traditionally, ACI solves an
inverse problem via Bayesian data assimilation, thus tracing causes backward
from observed effects with an implicit Bayesian hypothesis. Causality is
determined by assessing whether incorporating the information of the effect
variables reduces the uncertainty in recovering the potential cause variables.
ACI has several desirable features. First, it captures the dynamic interplay of
variables, where their roles as causes and effects can shift repeatedly over
time. Second, a mathematically justified objective criterion determines the CIR
without empirical thresholds. Third, ACI is scalable to high-dimensional
problems by leveraging computationally efficient Bayesian data assimilation
techniques. Finally, ACI applies to short time series and incomplete datasets.
Notably, ACI does not require observations of candidate causes, which is a key
advantage since potential drivers are often unknown or unmeasured. The
effectiveness of ACI is demonstrated by complex dynamical systems showcasing
intermittency and extreme events.

</details>


### [164] [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
*Rohan Deb,Kiran Thekumparampil,Kousha Kalantari,Gaurush Hiranandani,Shoham Sabach,Branislav Kveton*

Key words: 监督微调（SFT）、大语言模型（LLM）、信息增益、Hessian矩阵、多项式逻辑回归

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种高效选择训练子集的方法，通过最大化信息增益来提升监督微调（SFT）的统计效率。

Motivation: 监督微调（SFT）是适应大语言模型（LLM）到新领域的标准方法，但如何选择信息量最大的训练子集以提高效率是一个关键问题。

Method: 通过计算LLM对数似然的Hessian矩阵来测量信息增益，选择最大化信息增益的训练样本，并使用多项式逻辑回归模型线性化LLM的最后一层以提高计算效率。

Result: 该方法计算高效且可分析，在多个问题中表现良好，并通过定量结果和LLM评估验证了其有效性。

Conclusion: 本文提出的方法在有限的训练样本预算下显著提升了SFT的统计效率。

Abstract: Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.

</details>


### [165] [Deep Koopman operator framework for causal discovery in nonlinear dynamical systems](https://arxiv.org/abs/2505.14828)
*Juan Nathaniel,Carla Roesch,Jatan Buch,Derek DeSantis,Adam Rupe,Kara Lamb,Pierre Gentine*

Key words: Koopman算子, 因果发现, 非线性动力学, 深度学习, 气候科学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于深度Koopman算子的新因果发现算法Kausal，用于解决非线性动力学中的因果关系识别问题，优于现有方法。

Motivation: 传统统计框架（如Granger因果）无法量化非线性动力学中的因果关系，而Koopman算子方法提供了在观测空间中近似非线性动态的新工具。

Method: 利用深度学习推断最优观测变量，并在再生核希尔伯特空间中评估因果估计值。

Result: 数值实验显示Kausal在发现和表征因果信号方面优于现有方法，并成功应用于厄尔尼诺-南方振荡现象。

Conclusion: Kausal在非线性因果分析中表现出卓越性能，适用于现实世界系统研究。

Abstract: We use a deep Koopman operator-theoretic formalism to develop a novel causal
discovery algorithm, Kausal. Causal discovery aims to identify cause-effect
mechanisms for better scientific understanding, explainable decision-making,
and more accurate modeling. Standard statistical frameworks, such as Granger
causality, lack the ability to quantify causal relationships in nonlinear
dynamics due to the presence of complex feedback mechanisms, timescale mixing,
and nonstationarity. This presents a challenge in studying many real-world
systems, such as the Earth's climate. Meanwhile, Koopman operator methods have
emerged as a promising tool for approximating nonlinear dynamics in a linear
space of observables. In Kausal, we propose to leverage this powerful idea for
causal analysis where optimal observables are inferred using deep learning.
Causal estimates are then evaluated in a reproducing kernel Hilbert space, and
defined as the distance between the marginal dynamics of the effect and the
joint dynamics of the cause-effect observables. Our numerical experiments
demonstrate Kausal's superior ability in discovering and characterizing causal
signals compared to existing approaches of prescribed observables. Lastly, we
extend our analysis to observations of El Ni\~no-Southern Oscillation
highlighting our algorithm's applicability to real-world phenomena. Our code is
available at https://github.com/juannat7/kausal.

</details>


### [166] [Subquadratic Algorithms and Hardness for Attention with Any Temperature](https://arxiv.org/abs/2505.14840)
*Shreya Gupta,Boyang Huang,Barna Saha,Yinzhan Xu,Christopher Ye*

Key words: 注意力机制, 计算复杂度, 低秩矩阵, 温度假设, SETH

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了注意力机制的计算效率问题，提出了一种在无需强温度假设下实现快速注意力的算法，并证明了其最优性。

Motivation: 探讨在无需强温度假设的情况下，如何高效计算注意力机制，并识别快速注意力算法的适用条件。

Method: 提出了一种时间复杂度为$	ilde{O}(n^{2 - 1/d} \cdot 	ext{polylog}(B))$的算法，适用于大$B$值和低秩矩阵。同时证明了算法的局限性。

Result: 实现了对任意温度的高效注意力计算，并证明了在某些条件下标准算法是最优的。

Conclusion: 本文填补了注意力机制高效计算的空白，并为未来的优化提供了理论依据。

Abstract: Despite the popularity of the Transformer architecture, the standard
algorithm for computing Attention suffers from quadratic time complexity in
context length $n$. Alman and Song [NeurIPS 2023] showed that when the head
dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only
if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute
values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$).
Equivalently, subquadratic Attention is possible if and only if the softmax is
applied with high temperature for $d=\Theta(\log n)$. Running times of these
algorithms depend exponentially on $B$ and thus they do not lead to even a
polynomial-time algorithm outside the specific range of $B$.
  This naturally leads to the question: when can Attention be computed
efficiently without strong assumptions on temperature? Are there fast attention
algorithms that scale polylogarithmically with entry size $B$? In this work, we
resolve this question and characterize when fast Attention for arbitrary
temperatures is possible. First, for all constant $d = O(1)$, we give the first
subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm
for Attention with large $B$. Our result holds even for matrices with large
head dimension if they have low rank. In this regime, we also give a similar
running time for Attention gradient computation, and therefore for the full LLM
training process. Furthermore, we show that any substantial improvement on our
algorithm is unlikely. In particular, we show that even when $d =
2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under
$\mathsf{SETH}$.
  Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the
standard algorithm is optimal under popular fine-grained complexity
assumptions.

</details>


### [167] [A self-regulated convolutional neural network for classifying variable stars](https://arxiv.org/abs/2505.14877)
*Francisco Pérez-Galarce,Jorge Martínez-Palomera,Karim Pichara,Pablo Huijse,Márcio Catelan*

Key words: 机器学习，变星分类，数据偏差，变分自编码器，合成数据

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种自我调节训练方法，通过物理增强的变分自编码器生成合成数据，减少分类器在偏斜数据集上的偏差，显著提升变星分类的可靠性。

Motivation: 在变星分类中，现有机器学习模型依赖高质量和代表性数据，容易学习并强化训练数据中的偏差。目前对数据偏差问题的解决方案尚未完善。

Method: 引入自我调节训练过程，结合物理增强的变分自编码器生成合成样本，动态调整分类器和生成模型的交互，以减少训练中的混淆和填补参数空间欠表示区域。

Result: 实验表明，该方法在偏斜数据集上的分类性能显著优于传统训练方法，具有统计学意义上的改进。

Conclusion: 通过生成合成数据和动态调整训练过程，能够有效减少数据偏差对分类器的影响，提升分类模型的可靠性。

Abstract: Over the last two decades, machine learning models have been widely applied
and have proven effective in classifying variable stars, particularly with the
adoption of deep learning architectures such as convolutional neural networks,
recurrent neural networks, and transformer models. While these models have
achieved high accuracy, they require high-quality, representative data and a
large number of labelled samples for each star type to generalise well, which
can be challenging in time-domain surveys. This challenge often leads to models
learning and reinforcing biases inherent in the training data, an issue that is
not easily detectable when validation is performed on subsamples from the same
catalogue. The problem of biases in variable star data has been largely
overlooked, and a definitive solution has yet to be established. In this paper,
we propose a new approach to improve the reliability of classifiers in variable
star classification by introducing a self-regulated training process. This
process utilises synthetic samples generated by a physics-enhanced latent space
variational autoencoder, incorporating six physical parameters from Gaia Data
Release 3. Our method features a dynamic interaction between a classifier and a
generative model, where the generative model produces ad-hoc synthetic light
curves to reduce confusion during classifier training and populate
underrepresented regions in the physical parameter space. Experiments conducted
under various scenarios demonstrate that our self-regulated training approach
outperforms traditional training methods for classifying variable stars on
biased datasets, showing statistically significant improvements.

</details>


### [168] [An active learning framework for multi-group mean estimation](https://arxiv.org/abs/2505.14882)
*Abdellah Aznag,Rachel Cummings,Adam N. Elmachtoub*

Key words: 均值估计、多组数据、公平性、动态数据收集、Variance-UCB算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了多组未知数据分布下的均值估计问题，通过动态数据收集和主动学习框架，提出Variance-UCB算法，以最小化估计器的集体噪声。

Motivation: 解决多组未知数据分布下均值估计的公平性和效率问题，特别是在动态数据收集场景中，如在线平台或医疗领域的适应性实验。

Method: 采用主动学习框架，通过Variance-UCB算法动态选择组别，基于方差估计的置信上限更新策略，最小化集体噪声。

Result: 提出的理论框架显著改进了现有算法在遗憾边界上的性能，适用于更广泛的分布和目标，表现优于现有方法。

Conclusion: Variance-UCB算法在多组动态数据收集场景中有效且高效，显著提升了均等和公平性目标下的性能。

Abstract: We study a fundamental learning problem over multiple groups with unknown
data distributions, where an analyst would like to learn the mean of each
group. Moreover, we want to ensure that this data is collected in a relatively
fair manner such that the noise of the estimate of each group is reasonable. In
particular, we focus on settings where data are collected dynamically, which is
important in adaptive experimentation for online platforms or adaptive clinical
trials for healthcare. In our model, we employ an active learning framework to
sequentially collect samples with bandit feedback, observing a sample in each
period from the chosen group. After observing a sample, the analyst updates
their estimate of the mean and variance of that group and chooses the next
group accordingly. The analyst's objective is to dynamically collect samples to
minimize the collective noise of the estimators, measured by the norm of the
vector of variances of the mean estimators.
  We propose an algorithm, Variance-UCB, that sequentially selects groups
according to an upper confidence bound on the variance estimate. We provide a
general theoretical framework for providing efficient bounds on learning from
any underlying distribution where the variances can be estimated reasonably.
This framework yields upper bounds on regret that improve significantly upon
all existing bounds, as well as a collection of new results for different
objectives and distributions than those previously studied.

</details>


### [169] [Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](https://arxiv.org/abs/2505.14884)
*Susav Shrestha,Brad Settlemyer,Nikoli Dryden,Narasimha Reddy*

Key words: LLM, 推理加速, 上下文稀疏性, Polar Sparsity, Attention层

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Polar Sparsity方法，通过关注Attention层的稀疏性，实现了大规模语言模型（LLM）推理的加速，支持大批量处理且不影响准确性。

Motivation: 解决LLM推理中的高吞吐和低延迟需求，尤其是在大批次处理时上下文稀疏性无法有效扩展的问题。

Method: 提出Polar Sparsity方法，重点利用Attention层的稀疏性，并开发硬件高效的GPU内核来处理选择性MLP和Attention计算。

Result: 在OPT、LLaMA-2和3等模型上，实现了最高2.2倍的端到端加速，适用于不同批次大小和序列长度，且保持准确性。

Conclusion: Polar Sparsity首次证明上下文稀疏性可有效扩展到大批次处理，为大吞吐LLM部署提供实用解决方案。

Abstract: Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual sparsity,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in sparsity importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their sparsity vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
sparsity remains stable and batch-invariant. We develop hardware-efficient,
sparsity-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual sparsity can scale effectively to large batch sizes, delivering
substantial inference acceleration with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.

</details>


### [170] [Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis](https://arxiv.org/abs/2505.14896)
*Hootan Mahmoodiyan,Maryam Ahang,Mostafa Abbasi,Homayoun Najjaran*

Key words: 电力变压器、故障诊断、领域自适应、最大均值差异、相关对齐、Kolmogorov-Smirnov统计量

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于特征加权的领域自适应技术（MCW），通过结合最大均值差异（MMD）和相关对齐（CORAL），并利用Kolmogorov-Smirnov统计量分配权重，以提高电力变压器故障诊断的准确性。实验结果显示该方法在性能上优于微调和传统MMD-CORAL。

Motivation: 传统DGA故障诊断方法依赖启发式规则，导致结果不一致，而机器学习方法虽提高了准确性，但变压器运行条件的多样性导致数据分布偏移，需要领域自适应技术。

Method: 结合MMD和CORAL的特征加权技术（MCW），利用K-S统计量为特征分配权重，优先处理分布差异较大的特征，以实现更好的源域和目标域对齐。

Result: 实验证明MCW比微调方法提高了7.9%，比传统MMD-CORAL方法提高了2.2%，并在不同训练样本规模下均表现优异。

Conclusion: 所提出的MCW方法在电力变压器故障诊断中表现出更高的准确性和鲁棒性，适用于数据分布偏移问题。

Abstract: Ensuring the reliable operation of power transformers is critical to grid
stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but
traditional methods rely on heuristic rules, which may lead to inconsistent
results. Machine learning (ML)-based approaches have improved diagnostic
accuracy; however, power transformers operate under varying conditions, and
differences in transformer type, environmental factors, and operational
settings create distribution shifts in diagnostic data. Consequently, direct
model transfer between transformers often fails, making techniques for domain
adaptation a necessity. To tackle this issue, this work proposes a
feature-weighted domain adaptation technique that combines Maximum Mean
Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific
weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign
adaptable weights, prioritizing features with larger distributional
discrepancies and thereby improving source and target domain alignment.
Experimental evaluations on datasets for power transformers demonstrate the
effectiveness of the proposed method, which achieves a 7.9% improvement over
Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it
outperforms both techniques across various training sample sizes, confirming
its robustness for domain adaptation.

</details>


### [171] [Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction](https://arxiv.org/abs/2505.14897)
*Ali Mohajerzarrinkelk,Maryam Ahang,Mehran Zoravar,Mostafa Abbasi,Homayoun Najjaran*

Key words: 剩余使用寿命（RUL）、滚动轴承、小波去噪、Swin Transformer、预测维护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合小波去噪、小波包分解和多通道Swin Transformer的新框架，用于精确预测滚动轴承的剩余使用寿命，具有较好的噪声抵抗能力和泛化性能。

Motivation: 精确预测滚动轴承的剩余使用寿命可避免意外故障、减少停机时间，并提高工业系统的安全性和效率。但由于退化趋势复杂、噪声干扰和早期故障检测需求，这是一项挑战性任务。

Method: 结合小波去噪和小波包分解（WPD）方法，并采用自定义多通道Swin Transformer模型（MCSFormer），通过注意力机制融合特征，学习全局和局部退化模式。同时设计了自定义损失函数，以区分早期和晚期预测。

Result: 在PRONOSTIA数据集上进行了三组实验，结果显示MCSFormer在多项指标上优于现有模型，包括平均MAE降低41%、64%和69%。跨条件测试中表现出更好的泛化能力，自定义损失函数显著减少了晚期预测误差。

Conclusion: MCSFormer在噪声抵抗、泛化能力和安全性方面表现突出，是一种可靠且有效的工业预测维护工具。

Abstract: Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is
an important consideration to avoid unexpected failures, reduce downtime, and
promote safety and efficiency in industrial systems. Complications in
degradation trends, noise presence, and the necessity to detect faults in
advance make estimation of RUL a challenging task. This paper introduces a
novel framework that combines wavelet-based denoising method, Wavelet Packet
Decomposition (WPD), and a customized multi-channel Swin Transformer model
(MCSFormer) to address these problems. With attention mechanisms incorporated
for feature fusion, the model is designed to learn global and local degradation
patterns utilizing hierarchical representations for enhancing predictive
performance. Additionally, a customized loss function is developed as a key
distinction of this work to differentiate between early and late predictions,
prioritizing accurate early detection and minimizing the high operation risks
of late predictions. The proposed model was evaluated with the PRONOSTIA
dataset using three experiments. Intra-condition experiments demonstrated that
MCSFormer outperformed state-of-the-art models, including the Adaptive
Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on
average across different operating conditions, respectively. In terms of
cross-condition testing, it achieved superior generalization under varying
operating conditions compared to the adapted ViT and Swin Transformer. Lastly,
the custom loss function effectively reduced late predictions, as evidenced in
a 6.3% improvement in the scoring metric while maintaining competitive overall
performance. The model's robust noise resistance, generalization capability,
and focus on safety make MCSFormer a trustworthy and effective predictive
maintenance tool in industrial applications.

</details>


### [172] [When to retrain a machine learning model](https://arxiv.org/abs/2505.14903)
*Regol Florence,Schwinn Leo,Sprague Kyle,Coates Mark,Markovich Thomas*

Key words: 机器学习, 分布偏移, 重新训练决策, 不确定性方法, 性能预测

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于不确定性的方法，用于解决机器学习模型在数据持续演变时的重新训练决策问题，并在实验中优于现有基线。

Motivation: 解决现实世界机器学习模型在数据持续变化时何时重新训练的难题，特别是面对有限信息、未知分布偏移和成本权衡的挑战。

Method: 提出了一种基于不确定性的方法，通过持续预测模型性能的变化来做出重新训练决策。

Result: 在7个数据集上的分类任务中，该方法一致优于现有基线。

Conclusion: 该方法为机器学习模型的重新训练决策提供了全面的解决方案。

Abstract: A significant challenge in maintaining real-world machine learning models is
responding to the continuous and unpredictable evolution of data. Most
practitioners are faced with the difficult question: when should I retrain or
update my machine learning model? This seemingly straightforward problem is
particularly challenging for three reasons: 1) decisions must be made based on
very limited information - we usually have access to only a few examples, 2)
the nature, extent, and impact of the distribution shift are unknown, and 3) it
involves specifying a cost ratio between retraining and poor performance, which
can be hard to characterize. Existing works address certain aspects of this
problem, but none offer a comprehensive solution. Distribution shift detection
falls short as it cannot account for the cost trade-off; the scarcity of the
data, paired with its unusual structure, makes it a poor fit for existing
offline reinforcement learning methods, and the online learning formulation
overlooks key practical considerations. To address this, we present a
principled formulation of the retraining problem and propose an
uncertainty-based method that makes decisions by continually forecasting the
evolution of model performance evaluated with a bounded metric. Our experiments
addressing classification tasks show that the method consistently outperforms
existing baselines on 7 datasets.

</details>


### [173] [TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction](https://arxiv.org/abs/2505.14919)
*Frederik Wenkel,Wilson Tu,Cassandra Masschelein,Hamed Shirzad,Cian Eastwood,Shawn T. Whitfield,Ihab Bendidi,Craig Russell,Liam Hodgson,Yassir El Mesbahi,Jiarui Ding,Marta M. Fay,Berton Earnshaw,Emmanuel Noutahi,Alisandra K. Denton*

Key words: 细胞扰动响应,知识图谱,泛化预测,TxPert,转录响应

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: TxPert利用基因关系知识图谱改进未见条件下的细胞扰动响应预测，提升了跨组织、细胞类型和多基因扰动的泛化能力。

Motivation: 准确预测细胞对遗传扰动的响应对于疾病机制研究和治疗设计至关重要，但全尺度实验探索成本过高。

Method: 提出了TxPert方法，利用多组生物知识网络预测转录响应，并进行模型与数据分析。

Result: TxPert在单、双基因扰动及未见细胞系中表现优异，确立了新的性能标杆。

Conclusion: 知识图谱能有效增强模型对未见扰动的预测能力，为新标准框架的建立提供支持。

Abstract: Accurately predicting cellular responses to genetic perturbations is
essential for understanding disease mechanisms and designing effective
therapies. Yet exhaustively exploring the space of possible perturbations
(e.g., multi-gene perturbations or across tissues and cell types) is
prohibitively expensive, motivating methods that can generalize to unseen
conditions. In this work, we explore how knowledge graphs of gene-gene
relationships can improve out-of-distribution (OOD) prediction across three
challenging settings: unseen single perturbations; unseen double perturbations;
and unseen cell lines. In particular, we present: (i) TxPert, a new
state-of-the-art method that leverages multiple biological knowledge networks
to predict transcriptional responses under OOD scenarios; (ii) an in-depth
analysis demonstrating the impact of graphs, model architecture, and data on
performance; and (iii) an expanded benchmarking framework that strengthens
evaluation standards for perturbation modeling.

</details>


### [174] [Foundations of Unknown-aware Machine Learning](https://arxiv.org/abs/2505.14933)
*Xuefeng Du*

Key words: AI安全,分布偏移,OOD检测,未知感知学习,基础模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的未知感知学习框架，旨在解决机器学习模型在开放世界部署中的可靠性问题，特别是在分布不确定性和未知类别的情况下。通过算法和理论基础，论文改进了OOD检测并扩展了可靠学习到基础模型。

Motivation: 在开放世界部署中，机器学习模型由于分布偏移和未知输入可能做出过于自信的预测，导致安全问题。论文旨在通过未知感知学习提升模型的可靠性和安全性。

Method: 论文提出了多种方法：1）VOS、NPOS和DREAM-OOD用于生成未知样本；2）SAL框架利用未标注数据增强OOD检测；3）针对基础模型的工具如HaloScope、MLLMGuard和数据清洗方法。

Result: 论文展示了如何利用未标注数据识别和适应未知输入，并提供了形式化的可靠性保证。同时，这些方法在基础模型中显著提升了安全性。

Conclusion: 未知感知学习为AI系统的可靠性提供了一种新范式，能够在最小化人工干预的情况下提升模型的安全性和可靠性。

Abstract: Ensuring the reliability and safety of machine learning models in open-world
deployment is a central challenge in AI safety. This thesis develops both
algorithmic and theoretical foundations to address key reliability issues
arising from distributional uncertainty and unknown classes, from standard
neural networks to modern foundation models like large language models (LLMs).
  Traditional learning paradigms, such as empirical risk minimization (ERM),
assume no distribution shift between training and inference, often leading to
overconfident predictions on out-of-distribution (OOD) inputs. This thesis
introduces novel frameworks that jointly optimize for in-distribution accuracy
and reliability to unseen data. A core contribution is the development of an
unknown-aware learning framework that enables models to recognize and handle
novel inputs without labeled OOD data.
  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to
generate informative unknowns during training. Building on this, we present
SAL, a theoretical and algorithmic framework that leverages unlabeled
in-the-wild data to enhance OOD detection under realistic deployment
conditions. These methods demonstrate that abundant unlabeled data can be
harnessed to recognize and adapt to unforeseen inputs, providing formal
reliability guarantees.
  The thesis also extends reliable learning to foundation models. We develop
HaloScope for hallucination detection in LLMs, MLLMGuard for defending against
malicious prompts in multimodal models, and data cleaning methods to denoise
human feedback used for better alignment. These tools target failure modes that
threaten the safety of large-scale models in deployment.
  Overall, these contributions promote unknown-aware learning as a new
paradigm, and we hope it can advance the reliability of AI systems with minimal
human efforts.

</details>


### [175] [Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities](https://arxiv.org/abs/2505.14943)
*Ross Nordby*

Key words: 语言模型,软提示,潜在能力评估,自动化红队,条件距离

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种通过优化输入嵌入（软提示）来衡量模型与目标行为之间条件距离的方法，用于评估语言模型的潜在能力。

Motivation: 旨在帮助发现语言模型的潜在能力，并为自动化红队评估提供定量反馈，尤其是在未来可能具有欺骗性对齐的强大模型中。

Method: 采用软提示作为条件距离的度量标准，并在自然语言、国际象棋和路径规划等场景中验证其评估框架，同时扩展了通用条件软提示以支持任务评估的构建。

Result: 展示了软提示在多种场景中的有效性，证明了该方法在评估模型潜在能力方面的实用性。

Conclusion: 软提示是一种可扩展的评估工具，能够量化模型潜在行为的可访问性，适用于未来强大模型的潜在风险分析。

Abstract: To help evaluate and understand the latent capabilities of language models,
this paper introduces an approach using optimized input embeddings, or 'soft
prompts,' as a metric of conditional distance between a model and a target
behavior. The technique aims to facilitate latent capability discovery as a
part of automated red teaming/evaluation suites and to provide quantitative
feedback about the accessibility of potentially concerning behaviors in a way
that may scale to powerful future models, including those which may otherwise
be capable of deceptive alignment. An evaluation framework using soft prompts
is demonstrated in natural language, chess, and pathfinding, and the technique
is extended with generalized conditional soft prompts to aid in constructing
task evaluations.

</details>


### [176] [Unlearning Algorithmic Biases over Graphs](https://arxiv.org/abs/2505.14945)
*O. Deniz Kose,Gonzalo Mateos,Yanning Shen*

Key words: 图去学习，偏见缓解，牛顿更新，公平性，轻量级

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于图数据去学习的偏见缓解工具，通过牛顿更新模型权重实现轻量级偏见消除，并提供了可量化的性能保证。

Motivation: 推动去学习策略以应对数据删除请求，并利用其作为图数据偏见缓解的工具。

Method: 提出一种免训练的图去学习程序，通过单步牛顿更新实现偏见缓解，并设计了公平性感知的节点特征去学习策略和结构去学习方法。

Result: 实验结果表明，该方法能有效缓解偏见，同时保持下游任务的实用性（如节点分类准确性）。

Conclusion: 该方法为偏见缓解提供了一种计算轻量级的替代方案，相比从头训练具有更优的效用-复杂度权衡。

Abstract: The growing enforcement of the right to be forgotten regulations has
propelled recent advances in certified (graph) unlearning strategies to comply
with data removal requests from deployed machine learning (ML) models.
Motivated by the well-documented bias amplification predicament inherent to
graph data, here we take a fresh look at graph unlearning and leverage it as a
bias mitigation tool. Given a pre-trained graph ML model, we develop a
training-free unlearning procedure that offers certifiable bias mitigation via
a single-step Newton update on the model weights. This way, we contribute a
computationally lightweight alternative to the prevalent training- and
optimization-based fairness enhancement approaches, with quantifiable
performance guarantees. We first develop a novel fairness-aware nodal feature
unlearning strategy along with refined certified unlearning bounds for this
setting, whose impact extends beyond the realm of graph unlearning. We then
design structural unlearning methods endowed with principled selection
mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning
these judiciously selected elements can mitigate algorithmic biases with
minimal impact on downstream utility (e.g., node classification accuracy).
Experimental results over real networks corroborate the bias mitigation
efficacy of our unlearning strategies, and delineate markedly favorable
utility-complexity trade-offs relative to retraining from scratch using
augmented graph data obtained via removals.

</details>


### [177] [Privacy Preserving Conversion Modeling in Data Clean Room](https://arxiv.org/abs/2505.14959)
*Kungang Li,Xiangyi Chen,Ling Leng,Jiajing Xu,Jiankai Sun,Behnam Rezaei*

Key words: CVR预测, 隐私保护, 批量级梯度, 参数高效微调, 差分隐私

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的隐私保护CVR预测框架，通过批量级梯度聚合、参数高效微调和去偏技术，在满足隐私需求的同时保持模型性能。

Motivation: 在线广告中，CVR预测对广告效率和用户体验至关重要，但传统方法面临广告主不愿分享敏感数据和安全环境限制的问题。

Method: 使用批量级梯度而非样本级梯度，结合参数高效微调和梯度压缩，同时采用去偏技术处理差分隐私标签。

Result: 在工业数据集上验证，该方法在ROCAUC性能上具有竞争力，并显著降低通信开销。

Conclusion: 该框架为数字广告领域的高性能、隐私保护CVR预测设定了新标准。

Abstract: In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.

</details>


### [178] [The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models](https://arxiv.org/abs/2505.14964)
*Dave Cook,Tim Klawa*

Key words: 智能标注，标签优化，模型性能，高风险领域，数据效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为smart-sizing的数据标注策略，通过强调标签多样性、模型引导选择和边际效用停止，优化训练数据质量，以提高模型性能。

Motivation: 在高风险领域中，传统的标注策略因冗余和噪声问题限制了模型的泛化能力，需要一种更高效的数据标注方法。

Method: 通过Adaptive Label Optimization (ALO)，结合预标注分类、标注者分歧分析和迭代反馈，优先选择能显著提升模型性能的标签。

Result: 实验表明，仅使用20%至40%的精选数据训练的模型，在稀有类别召回和边缘案例泛化方面，表现可与全量数据基准相媲美或更优。

Conclusion: 智能标注策略将标注视为反馈驱动的过程，支持开发更鲁棒的模型，同时减少标签需求，提升AI开发效率。

Abstract: AI systems in high-consequence domains such as defense, intelligence, and
disaster response must detect rare, high-impact events while operating under
tight resource constraints. Traditional annotation strategies that prioritize
label volume over informational value introduce redundancy and noise, limiting
model generalization. This paper introduces smart-sizing, a training data
strategy that emphasizes label diversity, model-guided selection, and marginal
utility-based stopping. We implement this through Adaptive Label Optimization
(ALO), combining pre-labeling triage, annotator disagreement analysis, and
iterative feedback to prioritize labels that meaningfully improve model
performance. Experiments show that models trained on 20 to 40 percent of
curated data can match or exceed full-data baselines, particularly in
rare-class recall and edge-case generalization. We also demonstrate how latent
labeling errors embedded in training and validation sets can distort
evaluation, underscoring the need for embedded audit tools and
performance-aware governance. Smart-sizing reframes annotation as a
feedback-driven process aligned with mission outcomes, enabling more robust
models with fewer labels and supporting efficient AI development pipelines for
frontier models and operational systems.

</details>


### [179] [Anomaly Detection Based on Critical Paths for Deep Neural Networks](https://arxiv.org/abs/2505.14967)
*Fangzhen Zhao,Chenyi Zhang,Naipeng Dong,Ming Li,Jinxiao Shan*

Key words: 深度神经网络, 关键路径, 异常检测, 遗传进化, 随机子空间采样

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种从深度神经网络中提取关键路径并用于异常检测的新方法，通过遗传进化和变异识别路径，并结合随机子空间采样和投票机制，显著优于现有方法。

Motivation: 深度神经网络（DNNs）难以理解和防御，提取代表性路径能有效解释其决策过程。论文基于异常输入不会与正常输入激活相同路径的观察，探索关键路径提取用于异常检测。

Method: 通过遗传进化和变异识别关键检测路径，并集成多条路径的检测结果（结合随机子空间采样和投票机制）。

Result: 实验表明，该方法在多种异常类型检测中表现优于现有方法，具有高准确性。

Conclusion: 提取关键路径并结合多路径集成的方法能有效提升DNN异常检测的准确性和适用性。

Abstract: Deep neural networks (DNNs) are notoriously hard to understand and difficult
to defend. Extracting representative paths (including the neuron activation
values and the connections between neurons) from DNNs using software
engineering approaches has recently shown to be a promising approach in
interpreting the decision making process of blackbox DNNs, as the extracted
paths are often effective in capturing essential features. With this in mind,
this work investigates a novel approach that extracts critical paths from DNNs
and subsequently applies the extracted paths for the anomaly detection task,
based on the observation that outliers and adversarial inputs do not usually
induce the same activation pattern on those paths as normal (in-distribution)
inputs.
  In our approach, we first identify critical detection paths via genetic
evolution and mutation. Since different paths in a DNN often capture different
features for the same target class, we ensemble detection results from multiple
paths by integrating random subspace sampling and a voting mechanism. Compared
with state-of-the-art methods, our experimental results suggest that our method
not only outperforms them, but it is also suitable for the detection of a broad
range of anomaly types with high accuracy.

</details>


### [180] [STree: Speculative Tree Decoding for Hybrid State-Space Models](https://arxiv.org/abs/2505.14969)
*Yangchao Wu,Zongyue Qin,Alex Wong,Stefano Soatto*

Key words: 推测解码, 状态空间模型, 混合架构, 树状验证, 硬件感知

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种可扩展的树状推测解码算法，用于状态空间模型（SSM）和混合架构，提升了效率并优于现有方法。

Motivation: 尽管SSM已比自回归Transformer更高效，但其状态可能包含数千个标记，现有推测解码方法未充分利用树状验证。

Method: 利用积累的状态转移矩阵结构，实现了树状推测解码，并提出硬件感知实现方案。

Result: 在三个基准测试中，该方法优于普通推测解码，并展示了SSM和混合模型的进一步加速潜力。

Conclusion: 该算法为SSM和混合模型的推理提供了高效解决方案，未来仍有优化空间。

Abstract: Speculative decoding is a technique to leverage hardware concurrency to
improve the efficiency of large-scale autoregressive (AR) Transformer models by
enabling multiple steps of token generation in a single forward pass.
State-space models (SSMs) are already more efficient than AR Transformers,
since their state summarizes all past data with no need to cache or re-process
tokens in the sliding window context. However, their state can also comprise
thousands of tokens; so, speculative decoding has recently been extended to
SSMs. Existing approaches, however, do not leverage the tree-based verification
methods, since current SSMs lack the means to compute a token tree efficiently.
We propose the first scalable algorithm to perform tree-based speculative
decoding in state-space models (SSMs) and hybrid architectures of SSMs and
Transformer layers. We exploit the structure of accumulated state transition
matrices to facilitate tree-based speculative decoding with minimal overhead to
current SSM state update implementations. With the algorithm, we describe a
hardware-aware implementation that improves naive application of AR Transformer
tree-based speculative decoding methods to SSMs. Furthermore, we outperform
vanilla speculative decoding with SSMs even with a baseline drafting model and
tree structure on three different benchmarks, opening up opportunities for
further speed up with SSM and hybrid model inference. Code will be released
upon paper acceptance.

</details>


### [181] [Flattening Hierarchies with Policy Bootstrapping](https://arxiv.org/abs/2505.14975)
*John L. Zhou,Jonathan C. Kao*

Key words: 离线目标条件强化学习, 长时程任务, 重要性采样, 非分层策略

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种非分层的目标条件强化学习算法，通过子目标条件策略和重要性采样进行训练，解决了稀疏奖励和长时程任务的挑战。

Motivation: 离线目标条件强化学习（GCRL）在无奖励的大规模数据集上预训练通用策略表现出潜力，但在长时程任务中因稀疏奖励和折扣问题表现不佳，而分层方法又带来复杂性和扩展性问题。

Method: 引入了一种非分层的目标条件策略训练算法，利用子目标条件策略和优势加权重要性采样，避免了子目标生成模型的复杂性。

Result: 在状态和像素基础的操控与运动基准测试中，该方法匹配或超越了现有最佳的离线GCRL算法，并能扩展到复杂的长时程任务。

Conclusion: 该方法无需生成模型即可扩展到高维控制任务，为离线GCRL提供了简单高效的解决方案。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) is a promising
approach for pretraining generalist policies on large datasets of reward-free
trajectories, akin to the self-supervised objectives used to train foundation
models for computer vision and natural language processing. However, scaling
GCRL to longer horizons remains challenging due to the combination of sparse
rewards and discounting, which obscures the comparative advantages of primitive
actions with respect to distant goals. Hierarchical RL methods achieve strong
empirical results on long-horizon goal-reaching tasks, but their reliance on
modular, timescale-specific policies and subgoal generation introduces
significant additional complexity and hinders scaling to high-dimensional goal
spaces. In this work, we introduce an algorithm to train a flat
(non-hierarchical) goal-conditioned policy by bootstrapping on
subgoal-conditioned policies with advantage-weighted importance sampling. Our
approach eliminates the need for a generative model over the (sub)goal space,
which we find is key for scaling to high-dimensional control in large state
spaces. We further show that existing hierarchical and bootstrapping-based
approaches correspond to specific design choices within our derivation. Across
a comprehensive suite of state- and pixel-based locomotion and manipulation
benchmarks, our method matches or surpasses state-of-the-art offline GCRL
algorithms and scales to complex, long-horizon tasks where prior approaches
fail.

</details>


### [182] [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
*Eric Hanchen Jiang,Haozheng Luo,Shengyuan Pang,Xiaomin Li,Zhenting Qi,Hengli Li,Cheng-Fu Yang,Zongyu Lin,Xinfeng Li,Hao Xu,Kai-Wei Chang,Ying Nian Wu*

Key words: LLM, 数学推理, EORM, 能量模型, 后验验证

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: EORM是一种轻量级后验验证器，通过能量模型简化训练，显著提升LLM数学推理的准确性。

Motivation: 提高LLM在数学推理中的逻辑一致性和可靠性，减少计算成本。

Method: 引入EORM，利用能量模型简化奖励模型的训练，仅使用结果标签为CoT解决方案分配能量分数。

Result: 在GSM8k和MATH基准测试中，显著提升最终答案准确率（如Llama 3 8B在GSM8k上达到90.7%）。

Conclusion: EORM通过后验验证高效提升了LLM推理结果的可靠性，性能优于暴力采样。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.

</details>


### [183] [Know When to Abstain: Optimal Selective Classification with Likelihood Ratios](https://arxiv.org/abs/2505.15008)
*Alvin Heng,Harold Soh*

Key words: 选择性分类, Neyman-Pearson引理, 协变量偏移, 似然比检验

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过Neyman-Pearson引理重新设计选择性分类的最优选择函数，统一了多种基线方法的行为，并提出了新的选择性分类方法，尤其在协变量偏移场景下表现优异。

Motivation: 通过Neyman-Pearson引理优化选择性分类的不确定性预测，提升预测模型的可靠性，特别是在协变量偏移的实际挑战性场景中。

Method: 基于Neyman-Pearson引理设计似然比检验作为最优拒绝规则，提出新的选择性分类方法，并在协变量偏移场景下验证。

Result: 实验表明，提出的基于似然比选择的方法在多种视觉和语言任务中一致优于现有基线。

Conclusion: 似然比选择为协变量偏移下的选择性分类提供了稳健的改进机制。

Abstract: Selective classification enhances the reliability of predictive models by
allowing them to abstain from making uncertain predictions. In this work, we
revisit the design of optimal selection functions through the lens of the
Neyman--Pearson lemma, a classical result in statistics that characterizes the
optimal rejection rule as a likelihood ratio test. We show that this
perspective not only unifies the behavior of several post-hoc selection
baselines, but also motivates new approaches to selective classification which
we propose here. A central focus of our work is the setting of covariate shift,
where the input distribution at test time differs from that at training. This
realistic and challenging scenario remains relatively underexplored in the
context of selective classification. We evaluate our proposed methods across a
range of vision and language tasks, including both supervised learning and
vision-language models. Our experiments demonstrate that our
Neyman--Pearson-informed methods consistently outperform existing baselines,
indicating that likelihood ratio-based selection offers a robust mechanism for
improving selective classification under covariate shifts. Our code is publicly
available at https://github.com/clear-nus/sc-likelihood-ratios.

</details>


### [184] [One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks](https://arxiv.org/abs/2505.15009)
*Quan Nguyen,Thanh Nguyen-Tang*

Key words: Transformer, 上下文推理, 贝叶斯最优, 梯度下降, 泛化能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了单层Transformer在无噪声和有噪声上下文推理中的逼近能力和收敛行为，填补了现有理论在收敛速率和泛化能力上的空白，证明了其贝叶斯最优性。

Motivation: 现有理论研究多关注无限样本或初始梯度步的推理行为，缺乏收敛速率和泛化能力的分析，本文旨在填补这一空白。

Method: 作者通过有限样本分析，展示了梯度下降训练的Transformer模型以线性速率收敛到贝叶斯风险，并对未见样本具有泛化能力。

Result: 证明了存在一类单层Transformer（线性和ReLU注意力）是贝叶斯最优的，且训练模型在理论上和实验中都表现良好。

Conclusion: 理论分析和实验结果验证了单层Transformer在上下文推理中的优越性能，支持其在实际中的应用潜力。

Abstract: We study the approximation capabilities and on-convergence behaviors of
one-layer transformers on the noiseless and noisy in-context reasoning of
next-token prediction. Existing theoretical results focus on understanding the
in-context reasoning behaviors for either the first gradient step or when the
number of samples is infinite. Furthermore, no convergence rates nor
generalization abilities were known. Our work addresses these gaps by showing
that there exists a class of one-layer transformers that are provably
Bayes-optimal with both linear and ReLU attention. When being trained with
gradient descent, we show via a finite-sample analysis that the expected loss
of these transformers converges at linear rate to the Bayes risk. Moreover, we
prove that the trained models generalize to unseen samples as well as exhibit
learning behaviors that were empirically observed in previous works. Our
theoretical findings are further supported by extensive empirical validations.

</details>


### [185] [Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing](https://arxiv.org/abs/2505.15015)
*Longlong Li,Cunquan Qu,Guanghui Wang*

Key words: Graph Neural Networks, Multi-Scale Harmonic, Feature-wise Adaptation, Frequency-aware Attention, Graph Classification

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MSH-GNN是一种新型图神经网络，通过多尺度谐波投影实现特征级别的自适应消息传递，显著提升了图分类和节点分类任务的性能。

Motivation: 传统GNN以整体向量形式聚合邻居嵌入，无法捕捉细粒度和方向特定的特征相关性。

Method: MSH-GNN通过节点特定的谐波投影，动态地将邻居特征映射到频率敏感的方向，并使用可学习的正弦编码在多频率下调制。

Result: MSH-GNN在理论和实验上均优于现有模型，尤其在涉及图拓扑和频谱频率联合变化的复杂分类任务中表现优异。

Conclusion: MSH-GNN通过多尺度谐波消息传递机制有效捕捉结构不对称性和高频调制，提升了图判别能力。

Abstract: Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as
holistic vectors, lacking the ability to identify fine-grained,
direction-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic
Graph Neural Network), a novel architecture that performs feature-wise adaptive
message passing through node-specific harmonic projections. For each node,
MSH-GNN dynamically projects neighbor features onto frequency-sensitive
directions determined by the target node's own representation. These
projections are further modulated using learnable sinusoidal encodings at
multiple frequencies, enabling the model to capture both smooth and oscillatory
structural patterns across scales. A frequency-aware attention pooling
mechanism is introduced to emphasize spectrally and structurally salient nodes
during readout. Theoretically, we prove that MSH-GNN approximates
shift-invariant kernels and matches the expressive power of the
1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms
state-of-the-art models on a wide range of graph and node classification tasks.
Furthermore, in challenging classification settings involving joint variations
in graph topology and spectral frequency, MSH-GNN excels at capturing
structural asymmetries and high-frequency modulations, enabling more accurate
graph discrimination.

</details>


### [186] [Harnessing Large Language Models Locally: Empirical Results and Implications for AI PC](https://arxiv.org/abs/2505.15030)
*Qingyu Song,Peiyu Liao,Wenqian Zhao,Yiwen Wang,Shoubo Hu,Hui-Ling Zhen,Ning Jiang,Mingxuan Yuan*

Key words: 大语言模型、边缘计算、量化、性能评估、能效

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一个系统化方法评估边缘设备上的大语言模型（LLM），涵盖模型能力、开发效率和系统资源，并通过实验得出量化、性能和能效的关键洞察。

Motivation: 随着大语言模型在边缘设备的部署增加，虽然带来了隐私优势，但模型容量和压缩技术导致的性能限制仍待解决。

Method: 提出一种系统化评估方法，涵盖不同参数规模（0.5B-14B）的模型和七种后训练量化方法，在商用笔记本电脑上进行全面测试。

Result: 发现系统级指标与有效比特权重（BPW）接近线性关系；低比特量化的大模型在3.5 BPW阈值下表现优于高比特量化的小模型；低BPW量化虽准确率略有损失但显著节省内存；CPU上计算密集型操作功耗更高。

Conclusion: 研究为大语言模型在资源受限边缘设备上的高效部署和优化配置提供了关键见解和实用指导。

Abstract: The increasing deployment of Large Language Models (LLMs) on edge devices,
driven by model advancements and hardware improvements, offers significant
privacy benefits. However, these on-device LLMs inherently face performance
limitations due to reduced model capacity and necessary compression techniques.
To address this, we introduce a systematic methodology -- encompassing model
capability, development efficiency, and system resources -- for evaluating
on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to
14B parameters and seven post-training quantization (PTQ) methods on commodity
laptops, yields several critical insights: 1) System-level metrics exhibit
near-linear scaling with effective bits-per-weight (BPW). 2) A practical
threshold exists around $\sim$3.5 effective BPW, larger models subjected to
low-bit quantization consistently outperform smaller models utilizing higher
bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but
significant memory savings. 4) Determined by low-level implementation specifics
power consumption on CPU, where computation-intensive operations spend more
power than memory-intensive ones. These findings offer crucial insights and
practical guidelines for the efficient deployment and optimized configuration
of LLMs on resource-constrained edge devices. Our codebase is available at
https://github.com/simmonssong/LLMOnDevice.

</details>


### [187] [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
*Kaiwen Zha,Zhengqi Gao,Maohao Shen,Zhang-Wei Hong,Duane S. Boning,Dina Katabi*

Key words: 强化学习, 大型语言模型, 推理能力, 验证器, 泛化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出Tango框架，通过强化学习同时训练LLM生成器和验证器，提升推理能力和泛化性能。

Motivation: 现有RL后训练方法中的验证器固定或通过监督微调训练，易受奖励攻击且泛化能力差，需改进。

Method: Tango框架利用RL同时训练生成器和生成式过程级验证器，无需过程级标注。

Result: Tango在7B/8B规模模型中取得最佳性能，尤其在数学推理任务上表现突出。

Conclusion: Tango通过共同进化生成器和验证器，显著提升推理能力和泛化性能。

Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.

</details>


### [188] [RLBenchNet: The Right Network for the Right Reinforcement Learning Task](https://arxiv.org/abs/2505.15040)
*Ivan Smirnov,Shangding Gu*

Key words: 强化学习, 神经网络架构, LSTM, MLP, Mamba, Transformer-XL

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本研究系统评估了多种神经网络架构在强化学习任务中的表现，揭示了不同架构的优劣势，并提供了针对不同任务特性的架构选择建议。

Motivation: 随着强化学习的快速发展，研究者需要明确不同神经网络架构的适用场景，以优化性能和效率。

Method: 通过全面的实验评估，对比了LSTM、MLP、Mamba/Mamba-2、Transformer-XL、Gated Transformer-XL和GRU在连续控制、离散决策和记忆密集型任务中的表现。

Result: MLP在完全可观察的连续控制任务中表现最佳；LSTM和GRU在部分可观察环境中表现稳健；Mamba模型在吞吐量上显著优于LSTM和GRU；Transformer-XL和Mamba-2在记忆密集型任务中表现突出，但后者内存需求更低。

Conclusion: 研究结果为研究者提供了根据任务特性和计算限制选择合适架构的实用指南。

Abstract: Reinforcement learning (RL) has seen significant advancements through the
application of various neural network architectures. In this study, we
systematically investigate the performance of several neural networks in RL
tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP),
Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit
(GRU). Through comprehensive evaluation across continuous control, discrete
decision-making, and memory-based environments, we identify
architecture-specific strengths and limitations. Our results reveal that: (1)
MLPs excel in fully observable continuous control tasks, providing an optimal
balance of performance and efficiency; (2) recurrent architectures like LSTM
and GRU offer robust performance in partially observable environments with
moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput
compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable
performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2
successfully solve the most challenging memory-intensive tasks, with Mamba-2
requiring 8x less memory than Transformer-XL. These findings provide insights
for researchers and practitioners, enabling more informed architecture
selection based on specific task characteristics and computational constraints.
Code is available at: https://github.com/SafeRL-Lab/RLBenchNet

</details>


### [189] [PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration](https://arxiv.org/abs/2505.15047)
*Yingming Pu,Tao Lin,Hongyu Chen*

Key words: Large Language Model, multi-agent system, scientific discovery, uncertainty reduction, PiFlow

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: PiFlow框架通过信息论方法优化多智能体系统的科学发现流程，显著提高了效率和结果质量。

Motivation: 现有基于LLM的多智能体系统缺乏理性约束，导致假设与证据脱节，阻碍不确定性降低，亟需系统性优化。

Method: 提出信息论框架PiFlow，将科学发现视为基于科学法则的结构化不确定性降低问题。

Result: 在三个科学领域的实验中，PiFlow的AUC提升73.55％，解决方案质量提高94.06％。

Conclusion: PiFlow作为一种即插即用方法，为自动化科学发现提供了高效新范式。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate
remarkable potential for scientific discovery. Existing approaches, however,
often automate scientific discovery using predefined workflows that lack
rationality constraints. This often leads to aimless hypothesizing and a
failure to consistently link hypotheses with evidence, thereby hindering
systematic uncertainty reduction. Overcoming these limitations fundamentally
requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an
information-theoretical framework, treating automated scientific discovery as a
structured uncertainty reduction problem guided by principles (e.g., scientific
laws). In evaluations across three distinct scientific domains -- discovering
nanomaterial structures, bio-molecules, and superconductor candidates with
targeted properties -- our method significantly improves discovery efficiency,
reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property
values versus exploration steps, and enhances solution quality by 94.06\%
compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a
Plug-and-Play method, establishing a novel paradigm shift in highly efficient
automated scientific discovery, paving the way for more robust and accelerated
AI-driven research. Code is publicly available at our
\href{https://github.com/amair-lab/PiFlow}{GitHub}.

</details>


### [190] [Generalization Through Growth: Hidden Dynamics Controls Depth Dependence](https://arxiv.org/abs/2505.15064)
*Sho Sonoda,Yuka Hashimoto,Isao Ishikawa,Masahiro Ikeda*

Key words: 泛化界限, 伪度量空间, 组合表达能力, 半群动力学, 深度网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一个统一的框架，用于分析任意伪度量空间中深度网络的泛化界限，揭示了深度贡献与网络动力学几何特性之间的关联。

Motivation: 当前的理论仅针对特定架构和欧几里得输入，限制了泛化界限的适用性。研究旨在提出一个通用的框架，适用于任意伪度量空间，并揭示网络的动力学特性如何影响泛化性能。

Method: 通过将深度网络视为连续隐藏映射和输出映射的组合，利用伪度量空间的几何特性（如词球增长的半群）来推导泛化界限。

Result: 得出的泛化界限为 $O(\sqrt{(\alpha + \log \beta(k))/n})$，其中 $\beta(k)$ 捕捉了深度贡献，并揭示了多项式增长（虚拟幂零动力学）与指数增长（扩展动力学）之间的几何二分现象。

Conclusion: 该研究不仅提供了架构无关的泛化保证，还通过覆盖数估计展示了扩展动力学如何通过组合表达能力节省参数。这一框架适用于现代深度学习范式，如测试时推理和扩散模型。

Abstract: Recent theory has reduced the depth dependence of generalization bounds from
exponential to polynomial and even depth-independent rates, yet these results
remain tied to specific architectures and Euclidean inputs. We present a
unified framework for arbitrary \blue{pseudo-metric} spaces in which a
depth-\(k\) network is the composition of continuous hidden maps
\(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to
\mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$
isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of
the semigroup generated by the hidden layers. By Gromov's theorem polynomial
(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)
dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$
(sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further
provide covering-number estimates showing that expanding dynamics yield an
exponential parameter saving via compositional expressivity. Our results
decouple specification from implementation, offering architecture-agnostic and
dynamical-systems-aware guarantees applicable to modern deep-learning paradigms
such as test-time inference and diffusion models.

</details>


### [191] [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
*Xin Zhou,Weiqing Wang,Francisco J. Baldán,Wray Buntine,Christoph Bergmeir*

Key words: 多模态时间序列,预测,数据集,冷启动预测,模态效用

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: MoTime 是一个多模态时间序列预测数据集套件，结合了时间信号与文本、元数据和图像等外部模态，支持在不同场景下评估模态效用。

Motivation: 当前多数研究仍局限于单模态时间序列，而多模态数据在真实世界预测中日益可用。因此，MoTime 旨在填补这一空白，提供更全面的多模态预测研究支持。

Method: MoTime 构建了一套多模态数据集，覆盖多种领域，支持在常见预测任务（有历史数据）和冷启动预测（无历史数据）两种场景下评估模态效用。

Result: 实验表明，外部模态可以提升两种场景下的预测性能，尤其对某些数据集中的短序列有显著帮助，但效果因数据特征而异。

Conclusion: MoTime 提供了公开可用的数据集和研究成果，旨在支持未来多模态时间序列预测研究中更全面和现实的基准测试。

Abstract: While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.

</details>


### [192] [Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories](https://arxiv.org/abs/2505.15076)
*Nanxu Gong,Sixun Dong,Haoyue Bai,Xinyuan Wang,Wangyang Ying,Yanjie Fu*

Key words: 特征工程,多智能体系统,强化学习,特征选择,特征生成

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于多智能体系统的特征增强方法（MAGS），通过联合特征选择和生成提升模型性能。

Motivation: 现有方法通常将特征选择和生成分开处理，难以平衡冗余减少和有意义维度增加。

Method: 设计了包含选择、生成和路由三个智能体的系统，利用长短时记忆和离线PPO强化学习优化决策。

Result: 实验表明该方法通过智能协调特征选择和生成，显著提升了任务性能。

Conclusion: MAGS框架为特征工程提供了一种新的统一方法，有效解决了传统方法的局限性。

Abstract: As a widely-used and practical tool, feature engineering transforms raw data
into discriminative features to advance AI model performance. However, existing
methods usually apply feature selection and generation separately, failing to
strive a balance between reducing redundancy and adding meaningful dimensions.
To fill this gap, we propose an agentic feature augmentation concept, where the
unification of feature generation and selection is modeled as agentic teaming
and planning. Specifically, we develop a Multi-Agent System with Long and
Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant
features, a generator agent to produce informative new dimensions, and a router
agent that strategically coordinates their actions. We leverage in-context
learning with short-term memory for immediate feedback refinement and long-term
memory for globally optimal guidance. Additionally, we employ offline Proximal
Policy Optimization (PPO) reinforcement fine-tuning to train the router agent
for effective decision-making to navigate a vast discrete feature space.
Extensive experiments demonstrate that this unified agentic framework
consistently achieves superior task performance by intelligently orchestrating
feature selection and generation.

</details>


### [193] [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
*Sergey Pankov,Georges Harik*

Key words: 反向传播、注意力机制、Transformer、梯度估计、计算优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种通过随机切断反向传播流以减少计算量的方法，适用于Transformer中的注意力机制，将计算复杂度从二次降低到线性。

Motivation: 在Transformer架构的注意力机制中，长序列的计算需求呈二次增长，但大多数注意力权重很小。通过随机切断反向传播流，可以显著减少计算量，同时梯度方差的增加很小。

Method: 使用一个简单的概率规则，通过单一参数c控制每次反向传播中每个注意力头最多保留c个交互，将注意力反向传播的计算复杂度从O(n²)降为O(nc)。

Result: 实验表明，切断99%的注意力梯度流（c≈20-30）对梯度方差的影响仅约1%（n≈2000），且随n增大影响减小。

Conclusion: 该方法能高效实现稀疏矩阵计算，显著降低长序列训练时的反向传播成本。

Abstract: It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.

</details>


### [194] [Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features](https://arxiv.org/abs/2505.15083)
*Jeremy Qin*

Key words: 时间序列预测,可解释性,外生特征,医疗应用

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种结合静态特征和外生时间序列特征的双层透明框架，以增强时间序列预测的可解释性和鲁棒性。

Motivation: 在医疗等领域，时间序列预测的透明性和可解释性对临床决策至关重要。

Method: 将外生时间序列特征与静态特征结合，通过分解为趋势和属性提取可解释模式。

Result: 在合成数据集上的实验表明，该方法在保持预测性能的同时提升了可解释性和鲁棒性。

Conclusion: 该研究为开发鲁棒且通用的时间序列预测模型提供了基础。

Abstract: Time series forecasting plays a crucial role in various applications,
particularly in healthcare, where accurate predictions of future health
trajectories can significantly impact clinical decision-making. Ensuring
transparency and explainability of the models responsible for these tasks is
essential for their adoption in critical settings. Recent work has explored a
top-down approach to bi-level transparency, focusing on understanding trends
and properties of predicted time series using static features. In this work, we
extend this framework by incorporating exogenous time series features alongside
static features in a structured manner, while maintaining cohesive
interpretation. Our approach leverages the insights of trajectory comprehension
to introduce an encoding mechanism for exogenous time series, where they are
decomposed into meaningful trends and properties, enabling the extraction of
interpretable patterns. Through experiments on several synthetic datasets, we
demonstrate that our approach remains predictive while preserving
interpretability and robustness. This work represents a step towards developing
robust, and generalized time series forecasting models. The code is available
at https://github.com/jeremy-qin/TIMEVIEW

</details>


### [195] [Cost-aware LLM-based Online Dataset Annotation](https://arxiv.org/abs/2505.15101)
*Eray Can Elumar,Cem Tekin,Osman Yagan*

Key words: LLM, 数据集标注, 多数投票, 成本效率, 动态选择

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CaMVo是一种新型的在线框架，通过上下文嵌入自适应选择LLM子集，平衡置信度和成本，实现高效准确的数据集标注。

Motivation: 虽然多LLM多数投票能提高标签可靠性，但计算成本高。需要一种既高效又准确的标注方法。

Method: 提出CaMVo框架，结合LinUCB选择机制和贝叶斯估计器，动态选择LLM并加权投票。

Result: 在MMLU和IMDB数据集上，CaMVo在保持或优于多数投票的同时显著降低成本。

Conclusion: CaMVo为动态标注环境提供了实用高效的解决方案。

Abstract: Recent advances in large language models (LLMs) have enabled automated
dataset labeling with minimal human supervision. While majority voting across
multiple LLMs can improve label reliability by mitigating individual model
biases, it incurs high computational costs due to repeated querying. In this
work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),
for efficient and accurate LLM-based dataset annotation. CaMVo adaptively
selects a subset of LLMs for each data instance based on contextual embeddings,
balancing confidence and cost without requiring pre-training or ground-truth
labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator
over confidence scores, CaMVo estimates a lower bound on labeling accuracy for
each LLM and aggregates responses through weighted majority voting. Our
empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates
that CaMVo achieves comparable or superior accuracy to full majority voting
while significantly reducing labeling costs. This establishes CaMVo as a
practical and robust solution for cost-efficient annotation in dynamic labeling
environments.

</details>


### [196] [Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives](https://arxiv.org/abs/2505.15103)
*Zihu Wang,Boxun Xu,Hejia Geng,Peng Li*

Key words: graph contrastive learning, KAN, hard negative samples, semantic differences, graph representations

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出Khan-GCL框架，通过整合KAN网络到GCL编码器中提升表达能力，并利用KAN参数生成语义有意义的硬负样本，从而学习更具区分性的特征。

Motivation: 传统GCL方法存在两个局限：MLP编码器表达能力不足，以及负样本生成方式不理想（随机生成的负样本缺乏语义区分性或硬负样本未考虑语义差异）。

Method: 提出Khan-GCL框架，集成KAN网络增强编码器表达能力，并通过KAN参数开发两种关键特征识别技术，生成语义有意义的硬负样本。

Result: 在多个数据集和任务上，Khan-GCL比现有GCL方法表现更优，达到最佳性能。

Conclusion: Khan-GCL通过增强编码器能力和生成高质量负样本，显著提升了图对比学习的性能。

Abstract: Graph contrastive learning (GCL) has demonstrated great promise for learning
generalizable graph representations from unlabeled data. However, conventional
GCL approaches face two critical limitations: (1) the restricted expressive
capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal
negative samples that either from random augmentations-failing to provide
effective 'hard negatives'-or generated hard negatives without addressing the
semantic distinctions crucial for discriminating graph data. To this end, we
propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold
Network (KAN) into the GCL encoder architecture, substantially enhancing its
representational capacity. Furthermore, we exploit the rich information
embedded within KAN coefficient parameters to develop two novel critical
feature identification techniques that enable the generation of semantically
meaningful hard negative samples for each graph representation. These
strategically constructed hard negatives guide the encoder to learn more
discriminative features by emphasizing critical semantic differences between
graphs. Extensive experiments demonstrate that our approach achieves
state-of-the-art performance compared to existing GCL methods across a variety
of datasets and tasks.

</details>


### [197] [Graph Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15116)
*Zehong Wang,Zheyuan Liu,Tianyi Ma,Jiazheng Li,Zheyuan Zhang,Xingbo Fu,Yiyang Li,Zhengqing Yuan,Wei Song,Yijun Ma,Qingkai Zeng,Xiusi Chen,Jianan Zhao,Jundong Li,Meng Jiang,Pietro Lio,Nitesh Chawla,Chuxu Zhang,Yanfang Ye*

Key words: Graph Foundation Models, pretraining, generalization, graph learning, structured data

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Graph Foundation Models (GFMs) extend large-scale pretraining to graph-structured data, addressing unique challenges and enabling broad transfer across tasks. This survey categorizes GFMs, reviews methods, and outlines future directions.

Motivation: Graph-structured data is pervasive but poses unique challenges due to non-Euclidean structures and complex semantics. GFMs aim to bring scalable, general-purpose AI to graph data.

Method: The survey organizes GFMs into a modular framework with backbone architectures, pretraining strategies, and adaptation mechanisms, categorizing them by generalization scope (universal, task-specific, domain-specific).

Result: GFMs show promise for open-ended reasoning over structured data, but challenges like structural alignment and scalability remain.

Conclusion: GFMs are foundational infrastructure for graph learning, with future research needed to address theoretical and practical challenges.

Abstract: Graph-structured data pervades domains such as social networks, biological
systems, knowledge graphs, and recommender systems. While foundation models
have transformed natural language processing, vision, and multimodal learning
through large-scale pretraining and generalization, extending these
capabilities to graphs -- characterized by non-Euclidean structures and complex
relational semantics -- poses unique challenges and opens new opportunities. To
this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose
intelligence to structured data, enabling broad transfer across graph-centric
tasks and domains. This survey provides a comprehensive overview of GFMs,
unifying diverse efforts under a modular framework comprising three key
components: backbone architectures, pretraining strategies, and adaptation
mechanisms. We categorize GFMs by their generalization scope -- universal,
task-specific, and domain-specific -- and review representative methods, key
innovations, and theoretical insights within each category. Beyond methodology,
we examine theoretical foundations including transferability and emergent
capabilities, and highlight key challenges such as structural alignment,
heterogeneity, scalability, and evaluation. Positioned at the intersection of
graph learning and general-purpose AI, GFMs are poised to become foundational
infrastructure for open-ended reasoning over structured data. This survey
consolidates current progress and outlines future directions to guide research
in this rapidly evolving field. Resources are available at
https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.

</details>


### [198] [Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2505.15130)
*Sajjad Ghiasvand,Haniyeh Ehsani Oskouie,Mahnoosh Alizadeh,Ramtin Pedarsani*

Key words: Vision-Language Models, Adversarial Robustness, LoRA, Few-shot Learning, CLIP

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AdvCLIP-LoRA首次提出一种在Few-shot场景下增强CLIP模型对抗鲁棒性的算法，通过最小最大化优化问题实现对抗微调，理论保证收敛，实验显著提升对抗攻击下的鲁棒性。

Motivation: 尽管CLIP等视觉语言模型在多模态任务中表现优异，但其在对抗攻击下易受干扰，且传统对抗训练在资源受限的Few-shot场景中难以高效应用。

Method: 提出AdvCLIP-LoRA算法，将对抗微调建模为最小最大化优化问题，并基于LoRA实现参数高效微调。

Result: 在8个数据集上实验表明，AdvCLIP-LoRA显著提升对抗攻击（如FGSM、PGD）的鲁棒性，同时保持较高的干净准确率。

Conclusion: AdvCLIP-LoRA为资源受限场景下的VLM鲁棒适应提供了实用且理论支持的方法。

Abstract: Vision-Language Models (VLMs) such as CLIP have shown remarkable performance
in cross-modal tasks through large-scale contrastive pre-training. To adapt
these large transformer-based models efficiently for downstream tasks,
Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as
scalable alternatives to full fine-tuning, especially in few-shot scenarios.
However, like traditional deep neural networks, VLMs are highly vulnerable to
adversarial attacks, where imperceptible perturbations can significantly
degrade model performance. Adversarial training remains the most effective
strategy for improving model robustness in PEFT. In this work, we propose
AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial
robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method
formulates adversarial fine-tuning as a minimax optimization problem and
provides theoretical guarantees for convergence under smoothness and
nonconvex-strong-concavity assumptions. Empirical results across eight datasets
using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly
improves robustness against common adversarial attacks (e.g., FGSM, PGD),
without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA
as a practical and theoretically grounded approach for robust adaptation of
VLMs in resource-constrained settings.

</details>


### [199] [The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning](https://arxiv.org/abs/2505.15134)
*Shivam Agarwal,Zimin Zhang,Lifan Yuan,Jiawei Han,Hao Peng*

Key words: 熵最小化, 大语言模型, 无监督学习, 推理能力, 效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 通过熵最小化（EM）的方法，论文展示了无需标注数据即可显著提升大语言模型在数学、物理和编码任务中的性能。

Motivation: 探索如何通过熵最小化激发预训练大语言模型的潜在推理能力，而无需依赖标注数据或参数更新。

Method: 提出了三种熵最小化方法：EM-FT（基于无标签输出的指令微调）、EM-RL（仅以负熵为奖励的强化学习）和EM-INF（无需训练数据的推理时调整）。

Result: 在Qwen-7B上，EM-RL的性能优于基于标注数据的RL基线；Qwen-32B通过EM-INF在SciCode基准测试中媲美GPT-4o等专有模型，且效率更高。

Conclusion: 熵最小化能够有效激发预训练大语言模型的推理能力，无需依赖标注数据或参数更新。

Abstract: Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.

</details>


### [200] [Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm](https://arxiv.org/abs/2505.15138)
*Yang Xu,Swetha Ganesh,Washim Uddin Mondal,Qinbo Bai,Vaneet Aggarwal*

Key words: 约束马尔可夫决策过程, 演员评论家算法, 无限时间范围, 参数优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了具有一般参数化的无限时间平均奖励约束马尔可夫决策过程（CMDPs），并提出了一种能够高效处理约束并确保高收敛率的原始对偶自然演员评论家算法。

Motivation: 旨在解决在无限时间范围内处理约束马尔可夫决策过程（CMDPs）的挑战，特别是在参数化通用的场景下，提出一种能够同时满足高收敛率和约束管理的算法。

Method: 提出了一种原始对偶自然演员评论家算法，该算法在已知混合时间$\tau_{\mathrm{mix}}$时能够达到全局收敛和约束违反率为$\tilde{\mathcal{O}}(1/\sqrt{T})$的速率。

Result: 算法在已知$\tau_{\mathrm{mix}}$时速率达到$\tilde{\mathcal{O}}(1/\sqrt{T})$，未知时则为$\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$，需要$T \geq \tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$。

Conclusion: 该研究的成果达到了马尔可夫决策过程的理论下界，为平均奖励CMDPs的理论探索设立了新的基准。

Abstract: This paper investigates infinite-horizon average reward Constrained Markov
Decision Processes (CMDPs) with general parametrization. We propose a
Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints
while ensuring a high convergence rate. In particular, our algorithm achieves
global convergence and constraint violation rates of
$\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing
time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge
of $\tau_{\mathrm{mix}}$, the achievable rates change to
$\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq
\tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results
match the theoretical lower bound for Markov Decision Processes and establish a
new benchmark in the theoretical exploration of average reward CMDPs.

</details>


### [201] [EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression](https://arxiv.org/abs/2505.15140)
*Tong Cheng,Fu Jie,Xinpeng Ling,Huifa Li,Zhili Chen*

Key words: 图神经网络, 联邦学习, 标签分布攻击, 隐私保护

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究联邦图学习中标签分布攻击（LDA），提出一种名为EC-LDA的新型攻击方法，通过压缩节点嵌入显著提升攻击效果。

Motivation: 研究联邦图学习中服务器可能通过上传的模型参数推断客户端标签分布的问题，重点关注LDA攻击的有效性。

Method: 通过分析节点嵌入方差与LDA的关系，提出EC-LDA攻击方法，并进行节点分类和链接预测实验验证。

Result: EC-LDA在多个数据集上表现优于现有LDA方法，并在差分隐私保护下测试其鲁棒性。

Conclusion: EC-LDA显著提升LDA攻击效果，为联邦图学习隐私保护提供新视角。

Abstract: Graph Neural Networks (GNNs) have been widely used for graph analysis.
Federated Graph Learning (FGL) is an emerging learning framework to
collaboratively train graph data from various clients. However, since clients
are required to upload model parameters to the server in each round, this
provides the server with an opportunity to infer each client's data privacy. In
this paper, we focus on label distribution attacks(LDAs) that aim to infer the
label distributions of the clients' local data. We take the first step to
attack client's label distributions in FGL. Firstly, we observe that the
effectiveness of LDA is closely related to the variance of node embeddings in
GNNs. Next, we analyze the relation between them and we propose a new attack
named EC-LDA, which significantly improves the attack effectiveness by
compressing node embeddings. Thirdly, extensive experiments on node
classification and link prediction tasks across six widely used graph datasets
show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal
values under both Cos-sim and JS-div evaluation metrics in the CoraFull and
LastFM datasets. Finally, we explore the robustness of EC-LDA under
differential privacy protection.

</details>


### [202] [BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms](https://arxiv.org/abs/2505.15141)
*Yunlong Hou,Fengzhuo Zhang,Cunxiao Du,Xuan Zhang,Jiachun Pan,Tianyu Pang,Chao Du,Vincent Y. F. Tan,Zhuoran Yang*

Key words: 推测解码,大语言模型,多臂老虎机,自适应优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种无需训练的自适应超参数选择方法BanditSpec，通过多臂老虎机模型优化推测解码配置，在动态文本生成中表现优异。

Motivation: 现有推测解码方法要么采用固定配置，要么依赖离线/在线训练，缺乏动态适应性。

Method: 将超参数选择建模为多臂老虎机问题，提出UCBSpec和EXP3Spec算法，并分析停止时间悔恨。

Result: 理论证明了UCBSpec的悔恨上界接近最优，实验显示其在真实LLM服务场景中接近最优超参数表现。

Conclusion: BanditSpec框架在不训练的情况下有效提升了推测解码的吞吐量。

Abstract: Speculative decoding has emerged as a popular method to accelerate the
inference of Large Language Models (LLMs) while retaining their superior text
generation performance. Previous methods either adopt a fixed speculative
decoding configuration regardless of the prefix tokens, or train draft models
in an offline or online manner to align them with the context. This paper
proposes a training-free online learning framework to adaptively choose the
configuration of the hyperparameters for speculative decoding as text is being
generated. We first formulate this hyperparameter selection problem as a
Multi-Armed Bandit problem and provide a general speculative decoding framework
BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,
UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,
the stopping time regret. We upper bound this regret under both stochastic and
adversarial reward settings. By deriving an information-theoretic impossibility
result, it is shown that the regret performance of UCBSpec is optimal up to
universal constants. Finally, extensive empirical experiments with LLaMA3 and
Qwen2 demonstrate that our algorithms are effective compared to existing
methods, and the throughput is close to the oracle best hyperparameter in
simulated real-life LLM serving scenarios with diverse input prompts.

</details>


### [203] [Filtering Learning Histories Enhances In-Context Reinforcement Learning](https://arxiv.org/abs/2505.15143)
*Weiqin Chen,Xinjie Zhang,Dharmashankar Subramanian,Santiago Paternain*

Key words: Transformer模型, 上下文强化学习, 数据集预处理, LHF

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为LHF的方法，通过数据集预处理优化Transformer模型的上下文强化学习能力，避免源算法的不理想行为继承。

Motivation: 解决在上下文强化学习中因源算法或数据集继承不理想行为的问题。

Method: 提出学习历史过滤（LHF）方法，通过基于改进和稳定性特征重新加权和过滤学习历史。

Result: LHF在多种suboptimal场景下表现稳健，尤其在噪声数据下效果显著。

Conclusion: LHF是一种简单有效的预处理方法，可与现有SOTA算法结合，显著提升性能。

Abstract: Transformer models (TMs) have exhibited remarkable in-context reinforcement
learning (ICRL) capabilities, allowing them to generalize to and improve in
previously unseen environments without re-training or fine-tuning. This is
typically accomplished by imitating the complete learning histories of a source
RL algorithm over a substantial amount of pretraining environments, which,
however, may transfer suboptimal behaviors inherited from the source
algorithm/dataset. Therefore, in this work, we address the issue of inheriting
suboptimality from the perspective of dataset preprocessing. Motivated by the
success of the weighted empirical risk minimization, we propose a simple yet
effective approach, learning history filtering (LHF), to enhance ICRL by
reweighting and filtering the learning histories based on their improvement and
stability characteristics. To the best of our knowledge, LHF is the first
approach to avoid source suboptimality by dataset preprocessing, and can be
combined with the current state-of-the-art (SOTA) ICRL algorithms. We
substantiate the effectiveness of LHF through a series of experiments conducted
on the well-known ICRL benchmarks, encompassing both discrete environments and
continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD,
DPT, DICP) as the backbones. LHF exhibits robust performance across a variety
of suboptimal scenarios, as well as under varying hyperparameters and sampling
strategies. Notably, the superior performance of LHF becomes more pronounced in
the presence of noisy data, indicating the significance of filtering learning
histories.

</details>


### [204] [Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines](https://arxiv.org/abs/2505.15151)
*Xiaohou Shi,Ke Li,Aobo Liang,Yan Sun*

Key words: 时间序列预测,稀疏MoE,Any-variate Attention,图学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Time Tracker的新方法，通过稀疏专家混合（MoE）和Any-variate Attention技术，提高了多元时间序列数据的预测精度和模型泛化能力。

Motivation: 现有时间序列基础模型在处理多样化的时间模式和复杂变量间相关性时表现不足，尤其是在通道间依赖性的建模上存在缺陷。

Method: 采用稀疏MoE技术在Transformers中建模多样时间模式；提出Any-variate Attention支持单变量和多变量的统一处理；设计基于频域特征的图学习模块捕获变量间依赖关系。

Result: Time Tracker在预测精度、模型泛化和适应性上达到了最先进的性能。

Conclusion: Time Tracker通过创新的建模技术有效提升了时间序列预测的表现，尤其是对多元数据的处理能力。

Abstract: In the past few years, time series foundation models have achieved superior
predicting accuracy. However, real-world time series often exhibit significant
diversity in their temporal patterns across different time spans and domains,
making it challenging for a single model architecture to fit all complex
scenarios. In addition, time series data may have multiple variables exhibiting
complex correlations between each other. Recent mainstream works have focused
on modeling times series in a channel-independent manner in both pretraining
and finetuning stages, overlooking the valuable inter-series dependencies. To
this end, we propose \textbf{Time Tracker} for better predictions on
multivariate time series data. Firstly, we leverage sparse mixture of experts
(MoE) within Transformers to handle the modeling of diverse time series
patterns, thereby alleviating the learning difficulties of a single model while
improving its generalization. Besides, we propose Any-variate Attention,
enabling a unified model structure to seamlessly handle both univariate and
multivariate time series, thereby supporting channel-independent modeling
during pretraining and channel-mixed modeling for finetuning. Furthermore, we
design a graph learning module that constructs relations among sequences from
frequency-domain features, providing more precise guidance to capture
inter-series dependencies in channel-mixed modeling. Based on these
advancements, Time Tracker achieves state-of-the-art performance in predicting
accuracy, model generalization and adaptability.

</details>


### [205] [Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation](https://arxiv.org/abs/2505.15152)
*Nanxu Gong,Zijun Li,Sixun Dong,Haoyue Bai,Wangyang Ying,Xinyuan Wang,Yanjie Fu*

Key words: 特征转换,潜在扩散模型,变分自编码器,半自回归解码器,预测准确性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: DIFFT通过变分自编码器和潜在扩散模型，结合性能评估器生成高质量特征嵌入，显著提升预测准确性和鲁棒性。

Motivation: 现有特征转换方法在组合空间搜索和全局优化方面存在局限，需要更高效和鲁棒的解决方案。

Method: 使用VAE学习特征集潜在空间，LDM在性能评估器引导下生成特征嵌入，半自回归解码器转换为离散特征。

Result: 在14个基准数据集上，DIFFT在预测准确性和训练效率上均优于现有方法。

Conclusion: DIFFT通过生成和优化结合，成功克服了特征转换中的全局搜索和优化难题。

Abstract: Feature Transformation (FT) crafts new features from original ones via
mathematical operations to enhance dataset expressiveness for downstream
models. However, existing FT methods exhibit critical limitations: discrete
search struggles with enormous combinatorial spaces, impeding practical use;
and continuous search, being highly sensitive to initialization and step sizes,
often becomes trapped in local optima, restricting global exploration. To
overcome these limitations, DIFFT redefines FT as a reward-guided generative
task. It first learns a compact and expressive latent space for feature sets
using a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then
navigates this space to generate high-quality feature embeddings, its
trajectory guided by a performance evaluator towards task-specific optima. This
synthesis of global distribution learning (from LDM) and targeted optimization
(reward guidance) produces potent embeddings, which a novel semi-autoregressive
decoder efficiently converts into structured, discrete features, preserving
intra-feature dependencies while allowing parallel inter-feature generation.
Extensive experiments on 14 benchmark datasets show DIFFT consistently
outperforms state-of-the-art baselines in predictive accuracy and robustness,
with significantly lower training and inference times.

</details>


### [206] [Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss](https://arxiv.org/abs/2505.15174)
*Bo-Han Lai,Pin-Han Huang,Bo-Han Kung,Shang-Tse Chen*

Key words: Lipschitz神经网络, 认证鲁棒性, BRO层, BRONet, 退火机制

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种高效的块反射正交（BRO）层和新的损失函数，通过设计BRONet网络，显著提升了Lipschitz神经网络的表达能力与认证鲁棒性。

Motivation: 提升Lipschitz神经网络的表达能力与认证鲁棒性，以应对深度学习中认证鲁棒性不足的问题。

Method: 提出BRO层增强正交层的表达能力，引入基于退火机制的新损失函数以提高数据点间隔，设计BRONet网络。

Result: 在CIFAR-10/100、Tiny-ImageNet和ImageNet上表现优于现有基线，实现了最先进的认证鲁棒性。

Conclusion: BRO层和新的损失函数有效增强了Lipschitz神经网络的表达能力和认证鲁棒性，实验验证了其优越性。

Abstract: Lipschitz neural networks are well-known for providing certified robustness
in deep learning. In this paper, we present a novel, efficient Block Reflector
Orthogonal (BRO) layer that enhances the capability of orthogonal layers on
constructing more expressive Lipschitz neural architectures. In addition, by
theoretically analyzing the nature of Lipschitz neural networks, we introduce a
new loss function that employs an annealing mechanism to increase margin for
most data points. This enables Lipschitz models to provide better certified
robustness. By employing our BRO layer and loss function, we design BRONet - a
simple yet effective Lipschitz neural network that achieves state-of-the-art
certified robustness. Extensive experiments and empirical analysis on
CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms
existing baselines. The implementation is available at
\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}.

</details>


### [207] [SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps](https://arxiv.org/abs/2505.15177)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Key words: 图神经网络, OOD检测, 谱间隙, 拉普拉斯矩阵

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了一种基于拉普拉斯矩阵特征值差异的图级分布外（OOD）检测方法SpecGap，通过调整特征实现高效检测，无需额外训练即可集成到现有模型中。

Motivation: 研究观察到分布内（ID）与OOD图样本在拉普拉斯矩阵特征值的最大与次大值关系上存在显著差异，尤其是OOD样本常表现出异常的谱间隙（即最大与次大特征值之差）。这一观察为SpecGap的设计提供了理论基础。

Method: 提出SpecGap方法，通过减去与次大特征值相关的分量（根据谱间隙缩放）来调整高层特征（即公式：$\mathbf{X}−\left(\lambda_n−\lambda_{n-1}\right) \mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$），从而实现OOD检测。

Result: SpecGap在多个基准数据集上实现了最优性能，并通过大量消融实验和理论分析验证了其有效性。

Conclusion: 作为一种无参数的事后方法，SpecGap无需额外训练或修改模型即可轻松集成到现有图神经网络中，为OOD检测提供了高效解决方案。

Abstract: The task of graph-level out-of-distribution (OOD) detection is crucial for
deploying graph neural networks in real-world settings. In this paper, we
observe a significant difference in the relationship between the largest and
second-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and
OOD graph samples: \textit{OOD samples often exhibit anomalous spectral gaps
(the difference between the largest and second-largest eigenvalues)}. This
observation motivates us to propose SpecGap, an effective post-hoc approach for
OOD detection on graphs. SpecGap adjusts features by subtracting the component
associated with the second-largest eigenvalue, scaled by the spectral gap, from
the high-level features (i.e., $\mathbf{X}-\left(\lambda_n-\lambda_{n-1}\right)
\mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art
performance across multiple benchmark datasets. We present extensive ablation
studies and comprehensive theoretical analyses to support our empirical
results. As a parameter-free post-hoc method, SpecGap can be easily integrated
into existing graph neural network models without requiring any additional
training or model modification.

</details>


### [208] [A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning](https://arxiv.org/abs/2505.15178)
*Zhehao Huang,Xinwen Cheng,Jie Zhang,Jinghao Zheng,Haoran Wang,Zhengbao He,Tao Li,Xiaolin Huang*

Key words: 持续学习；机器遗忘；KL散度；优化框架；动态合规性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种基于KL散度最小化的统一优化框架，将持续学习（CL）与机器遗忘（MU）结合为CLU范式，通过梯度分解和权重调制实现知识更新与保留的平衡。

Motivation: 现有研究将CL和MU视为独立过程，忽略了其内在联系，亟需统一的框架支持知识动态更新与合规性。

Method: 基于KL散度的统一优化框架，包含梯度更新的四个组件，引入保留流形约束和快慢权重适应机制，支持细粒度遗忘。

Result: UG-CLU框架在多种数据集和模型架构中有效协调学习、遗忘与稳定性，为动态合规系统提供支持。

Conclusion: CLU范式通过统一优化平衡学习与遗忘，为智能系统的动态合规性提供了理论与实践基础。

Abstract: Recent advancements in deep models have highlighted the need for intelligent
systems that combine continual learning (CL) for knowledge acquisition with
machine unlearning (MU) for data removal, forming the Continual
Learning-Unlearning (CLU) paradigm. While existing work treats CL and MU as
separate processes, we reveal their intrinsic connection through a unified
optimization framework based on Kullback-Leibler divergence minimization. This
framework decomposes gradient updates for approximate CLU into four components:
learning new knowledge, unlearning targeted data, preserving existing
knowledge, and modulation via weight saliency. A critical challenge lies in
balancing knowledge update and retention during sequential learning-unlearning
cycles. To resolve this stability-plasticity dilemma, we introduce a
remain-preserved manifold constraint to induce a remaining Hessian compensation
for CLU iterations. A fast-slow weight adaptation mechanism is designed to
efficiently approximate the second-order optimization direction, combined with
adaptive weighting coefficients and a balanced weight saliency mask, proposing
a unified implementation framework for gradient-based CLU. Furthermore, we
pioneer task-agnostic CLU scenarios that support fine-grained unlearning at the
cross-task category and random sample levels beyond the traditional task-aware
setups. Experiments demonstrate that the proposed UG-CLU framework effectively
coordinates incremental learning, precise unlearning, and knowledge stability
across multiple datasets and model architectures, providing a theoretical
foundation and methodological support for dynamic, compliant intelligent
systems.

</details>


### [209] [NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration](https://arxiv.org/abs/2505.15180)
*Jiawei Gu,Ziyue Qiao,Xiao Luo*

Key words: 图神经网络, 模型偏差, 类不平衡, 中性输入校准, 表示平衡

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: NeuBM 是一种通过中性输入校准减少图神经网络（GNNs）模型偏差的新方法，显著提高了少数类的平衡准确率和召回率。

Motivation: 图神经网络（GNNs）在类不平衡时存在模型偏差问题，导致性能下降和不公平预测，尤其对少数类不利。

Method: NeuBM 通过动态更新的中性图估计并纠正模型内在偏差，通过减去中性图的 logits 来重新校准预测。

Result: 实验显示 NeuBM 显著提升少数类的性能，尤其在类不平衡严重和标签数据有限的情况下优于传统方法。

Conclusion: NeuBM 不仅调整最终预测，还通过表示平衡促进网络学习均衡特征表示，是一种高效且易集成的方法。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance across various
domains, yet they often struggle with model bias, particularly in the presence
of class imbalance. This bias can lead to suboptimal performance and unfair
predictions, especially for underrepresented classes. We introduce NeuBM
(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs
through neutral input calibration. NeuBM leverages a dynamically updated
neutral graph to estimate and correct the inherent biases of the model. By
subtracting the logits obtained from the neutral graph from those of the input
graph, NeuBM effectively recalibrates the model's predictions, reducing bias
across different classes. Our method integrates seamlessly into existing GNN
architectures and training procedures, requiring minimal computational
overhead. Extensive experiments on multiple benchmark datasets demonstrate that
NeuBM significantly improves the balanced accuracy and recall of minority
classes, while maintaining strong overall performance. The effectiveness of
NeuBM is particularly pronounced in scenarios with severe class imbalance and
limited labeled data, where traditional methods often struggle. We provide
theoretical insights into how NeuBM achieves bias mitigation, relating it to
the concept of representation balancing. Our analysis reveals that NeuBM not
only adjusts the final predictions but also influences the learning of balanced
feature representations throughout the network.

</details>


### [210] [Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing](https://arxiv.org/abs/2505.15195)
*Adel Javanmard,Rudrajit Das,Alessandro Epasto,Vahab Mirrokni*

Key words: 重训练, 近似消息传递, 贝叶斯最优, 二元分类

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一个基于近似消息传递（AMP）的框架，用于分析二元分类任务中迭代重训练的贝叶斯最优聚合函数，以最小化预测误差。

Motivation: 研究如何在重训练过程中最优地结合模型的预测和原始标签，以提升模型性能。

Method: 基于近似消息传递（AMP）框架，分析了高斯混合模型（GMM）和广义线性模型（GLM）的迭代重训练过程。

Result: 推导出贝叶斯最优聚合函数，在高标签噪声情况下，该方法的性能优于基线方法。

Conclusion: 提出的最优聚合函数能够有效减少预测误差，并适用于实际应用。

Abstract: Retraining a model using its own predictions together with the original,
potentially noisy labels is a well-known strategy for improving the model
performance. While prior works have demonstrated the benefits of specific
heuristic retraining schemes, the question of how to optimally combine the
model's predictions and the provided labels remains largely open. This paper
addresses this fundamental question for binary classification tasks. We develop
a principled framework based on approximate message passing (AMP) to analyze
iterative retraining procedures for two ground truth settings: Gaussian mixture
model (GMM) and generalized linear model (GLM). Our main contribution is the
derivation of the Bayes optimal aggregator function to combine the current
model's predictions and the given labels, which when used to retrain the same
model, minimizes its prediction error. We also quantify the performance of this
optimal retraining strategy over multiple rounds. We complement our theoretical
results by proposing a practically usable version of the theoretically-optimal
aggregator function for linear probing with the cross-entropy loss, and
demonstrate its superiority over baseline methods in the high label noise
regime.

</details>


### [211] [Group Distributionally Robust Optimization with Flexible Sample Queries](https://arxiv.org/abs/2505.15212)
*Haomin Bai,Dingzhi Yu,Shuai Li,Haipeng Luo,Lijun Zhang*

Key words: GDRO, PLA, 动态样本量, 优化误差, 样本复杂度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文研究了支持动态样本量的组分布鲁棒优化（GDRO）问题，提出了一种新的预测有限建议（PLA）算法，并开发了允许每轮样本量任意变化的GDRO算法，取得了与固定样本量情况一致的理论性能。

Motivation: 现有GDRO算法只能处理固定样本量（1或m），无法适应动态变化的样本量场景，因此需要一种更灵活的解决方案。

Method: 将GDRO问题建模为两人游戏，提出新的PLA算法并构建损失估计器，采用正则化跟随更新策略，开发了支持任意样本量的GDRO算法。

Result: 理论分析表明，优化误差随样本量增加而下降，样本复杂度与固定样本量情况一致；实验验证了方法的有效性。

Conclusion: 本文提出的算法解决了GDRO中样本量动态变化的限制，扩展了其应用场景，并保持了理论性能。

Abstract: Group distributionally robust optimization (GDRO) aims to develop models that
perform well across $m$ distributions simultaneously. Existing GDRO algorithms
can only process a fixed number of samples per iteration, either 1 or $m$, and
therefore can not support scenarios where the sample size varies dynamically.
To address this limitation, we investigate GDRO with flexible sample queries
and cast it as a two-player game: one player solves an online convex
optimization problem, while the other tackles a prediction with limited advice
(PLA) problem. Within such a game, we propose a novel PLA algorithm,
constructing appropriate loss estimators for cases where the sample size is
either 1 or not, and updating the decision using follow-the-regularized-leader.
Then, we establish the first high-probability regret bound for non-oblivious
PLA. Building upon the above approach, we develop a GDRO algorithm that allows
an arbitrary and varying sample size per round, achieving a high-probability
optimization error bound of $O\left(\frac{1}{t}\sqrt{\sum_{j=1}^t
\frac{m}{r_j}\log m}\right)$, where $r_t$ denotes the sample size at round $t$.
This result demonstrates that the optimization error decreases as the number of
samples increases and implies a consistent sample complexity of $O(m\log
(m)/\epsilon^2)$ for any fixed sample size $r\in[m]$, aligning with existing
bounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary
and real-world multi-class datasets.

</details>


### [212] [KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning](https://arxiv.org/abs/2505.15213)
*Sampanna Yashwant Kahu*

Key words: Linux内核, CFS, 深度学习, LSTM, 任务调度

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究利用深度学习预测Linux内核的CFS调度任务序列，探索构建更泛化、适应性更强的调度器，贡献包括新调度数据集生成和LSTM网络开发。

Motivation: 优化Linux内核任务调度，通过深度学习提高CFS的适应性和泛化能力。

Method: 生成真实调度数据集，开发并训练LSTM网络预测任务序列。

Result: 成功预测CFS任务调度序列，为数据驱动的内核调度提供可行路径。

Conclusion: 深度学习可提升内核调度效率，未来可进一步整合预测模型。

Abstract: Efficient task scheduling is paramount in the Linux kernel, where the
Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance
high utilization with interactive responsiveness. This research pioneers the
use of deep learning techniques to predict the sequence of tasks selected by
CFS, aiming to evaluate the feasibility of a more generalized and potentially
more adaptive task scheduler for diverse workloads. Our core contributions are
twofold: first, the systematic generation and curation of a novel scheduling
dataset from a running Linux kernel, capturing real-world CFS behavior; and
second, the development, training, and evaluation of a Long Short-Term Memory
(LSTM) network designed to accurately forecast the next task to be scheduled.
This paper further discusses the practical pathways and implications of
integrating such a predictive model into the kernel's scheduling framework. The
findings and methodologies presented herein open avenues for data-driven
advancements in kernel scheduling, with the full source code provided for
reproducibility and further exploration.

</details>


### [213] [Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.15228)
*Mathew Vanherreweghe,Lirandë Pira,Patrick Rebentrost*

Key words: CP-KAN, 切比雪夫多项式, QUBO, 数据效率, 数值稳定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: CP-KAN是一种结合切比雪夫多项式基函数和二次无约束二进制优化（QUBO）的神经网络架构，通过将度选择问题重构为QUBO任务，显著降低了复杂度。

Motivation: 解决传统神经网络在回归任务中计算复杂度过高和数据效率低下的问题。

Method: 采用切比雪夫多项式基函数和QUBO优化方法，将度选择问题简化为单层优化步骤。

Result: 在数据有限的回归任务中表现优异，具有良好的输入尺度鲁棒性和自然正则化特性。

Conclusion: CP-KAN在数据效率和数值稳定性要求高的场景中具有竞争力。

Abstract: We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a
neural architecture combining Chebyshev polynomial basis functions and
quadratic unconstrained binary optimization (QUBO). Our primary contribution
involves reformulating the degree selection problem as a QUBO task, reducing
the complexity from $O(D^N)$ to a single optimization step per layer. This
approach enables efficient degree selection across neurons while maintaining
computational tractability. The architecture performs well in regression tasks
with limited data, showing good robustness to input scales and natural
regularization properties from its polynomial basis. Additionally, theoretical
analysis establishes connections between CP-KAN's performance and properties of
financial time series. Our empirical validation across multiple domains
demonstrates competitive performance compared to several traditional
architectures tested, especially in scenarios where data efficiency and
numerical stability are important. Our implementation, including strategies for
managing computational overhead in larger networks is available in
Ref.~\citep{cpkan_implementation}.

</details>


### [214] [Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions](https://arxiv.org/abs/2505.15231)
*Kabir V. Dabholkar,Omri Barak*

Key words: Koopman理论, 深度神经网络, 分界面, 高维动力学系统, 神经科学

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种结合Koopman理论和深度神经网络的数值框架，用于高效表征高维动力学系统中的分界面，并在多个模型中验证了其有效性。

Motivation: 研究动机是解决高维动力学系统中分界面（分隔吸引盆的流形）难以表征的问题，尤其是在复杂系统中。

Method: 方法结合Koopman理论和深度神经网络，近似与正实特征值相关的Koopman本征函数，利用优化方法定位分界面。

Result: 结果在合成基准、生态网络模型和神经科学任务训练的循环神经网络中验证了方法的有效性，并展示了其在设计最优扰动中的实际应用。

Conclusion: 结论表明该方法在高维系统中高效且实用，特别适用于神经科学中的实验预测。

Abstract: Many natural systems, including neural circuits involved in decision making,
can be modeled as high-dimensional dynamical systems with multiple stable
states. While existing analytical tools primarily describe behavior near stable
equilibria, characterizing separatrices -- the manifolds that delineate
boundaries between different basins of attraction -- remains challenging,
particularly in high-dimensional settings. Here, we introduce a numerical
framework leveraging Koopman Theory combined with Deep Neural Networks to
effectively characterize separatrices. Specifically, we approximate Koopman
Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish
precisely at the separatrices. Utilizing these scalar KEFs, optimization
methods efficiently locate separatrices even in complex systems. We demonstrate
our approach on synthetic benchmarks, ecological network models, and recurrent
neural networks trained on neuroscience-inspired tasks. Moreover, we illustrate
the practical utility of our method by designing optimal perturbations that can
shift systems across separatrices, enabling predictions relevant to optogenetic
stimulation experiments in neuroscience.

</details>


### [215] [Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers](https://arxiv.org/abs/2505.15239)
*Peter Súkeník,Christoph H. Lampert,Marco Mondelli*

Key words: 神经崩溃，深度网络，变换器，残差网络，数据感知

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文分析了现代深度架构在数据感知模式下神经崩溃的出现，证明了全局最优解近似崩溃，随着深度增加，近似更紧密。

Motivation: 研究神经崩溃在深度网络中的出现，填补现有理论在数据感知和多层感知机之外的空白。

Method: 分析正则化变换器和残差网络（ResNets）在交叉熵或均方误差损失下的全局最优解。

Result: 随着深度增加，神经崩溃更加显著，理论结果在视觉和语言数据实验中得到了验证。

Conclusion: 深度网络训练可以简化为无约束特征模型，支持其在数据感知之外的广泛应用。

Abstract: The empirical emergence of neural collapse -- a surprising symmetry in the
feature representations of the training data in the penultimate layer of deep
neural networks -- has spurred a line of theoretical research aimed at its
understanding. However, existing work focuses on data-agnostic models or, when
data structure is taken into account, it remains limited to multi-layer
perceptrons. Our paper fills both these gaps by analyzing modern architectures
in a data-aware regime: we prove that global optima of deep regularized
transformers and residual networks (ResNets) with LayerNorm trained with cross
entropy or mean squared error loss are approximately collapsed, and the
approximation gets tighter as the depth grows. More generally, we formally
reduce any end-to-end large-depth ResNet or transformer training into an
equivalent unconstrained features model, thus justifying its wide use in the
literature even beyond data-agnostic settings. Our theoretical results are
supported by experiments on computer vision and language datasets showing that,
as the depth grows, neural collapse indeed becomes more prominent.

</details>


### [216] [Reliable Vertical Federated Learning in 5G Core Network Architecture](https://arxiv.org/abs/2505.15244)
*Mohamad Mestoukirdi,Mourad Khanfouci*

Key words: 垂直联邦学习，5G核心网络，模型泛化，客户端可靠性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种新算法，用于缓解5G核心网络中垂直联邦学习（VFL）在客户端可靠性约束下的模型泛化损失问题。

Motivation: VFL在5G核心网络中的性能因客户端（NWDAFs）的可靠性问题而显著下降，需要新的解决方案。

Method: 优化客户端间的垂直特征分割，并根据可靠性指标集中定义本地模型。

Result: 实验证明该算法比传统方法性能更优。

Conclusion: 该方法能有效提升VFL在5G核心网络中的性能。

Abstract: This work proposes a new algorithm to mitigate model generalization loss in
Vertical Federated Learning (VFL) operating under client reliability
constraints within 5G Core Networks (CNs). Recently studied and endorsed by
3GPP, VFL enables collaborative and load-balanced model training and inference
across the CN. However, the performance of VFL significantly degrades when the
Network Data Analytics Functions (NWDAFs) - which serve as primary clients for
VFL model training and inference - experience reliability issues stemming from
resource constraints and operational overhead. Unlike edge environments, CN
environments adopt fundamentally different data management strategies,
characterized by more centralized data orchestration capabilities. This
presents opportunities to implement better distributed solutions that take full
advantage of the CN data handling flexibility. Leveraging this flexibility, we
propose a method that optimizes the vertical feature split among clients while
centrally defining their local models based on reliability metrics. Our
empirical evaluation demonstrates the effectiveness of our proposed algorithm,
showing improved performance over traditional baseline methods.

</details>


### [217] [Mitigating Spurious Correlations with Causal Logit Perturbation](https://arxiv.org/abs/2505.15246)
*Xiaoling Zhou,Wei Ye,Rui Xie,Shikun Zhang*

Key words: 深度学习、因果模型、对数扰动、伪相关性、元学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的因果对数扰动（CLP）框架，通过扰动网络生成样本级的对数扰动，以解决深度学习中的伪相关性问题，并在多种有偏学习场景中取得最先进性能。

Motivation: 深度学习中某些方法依赖于伪相关性进行预测，导致非鲁棒性。本研究旨在通过因果模型解除这些伪相关性。

Method: 采用因果对数扰动（CLP）框架，利用扰动网络生成样本级的对数扰动，并结合在线元学习算法优化框架。

Result: 在四种典型有偏学习场景（长尾学习、噪声标签学习、广义长尾学习和子群体偏移学习）中，CLP表现优异。

Conclusion: CLP框架能有效引导模型关注因果属性并消除伪相关性，展示了其在实际应用中的潜力。

Abstract: Deep learning has seen widespread success in various domains such as science,
industry, and society. However, it is acknowledged that certain approaches
suffer from non-robustness, relying on spurious correlations for predictions.
Addressing these limitations is of paramount importance, necessitating the
development of methods that can disentangle spurious correlations. {This study
attempts to implement causal models via logit perturbations and introduces a
novel Causal Logit Perturbation (CLP) framework to train classifiers with
generated causal logit perturbations for individual samples, thereby mitigating
the spurious associations between non-causal attributes (i.e., image
backgrounds) and classes.} {Our framework employs a} perturbation network to
generate sample-wise logit perturbations using a series of training
characteristics of samples as inputs. The whole framework is optimized by an
online meta-learning-based learning algorithm and leverages human causal
knowledge by augmenting metadata in both counterfactual and factual manners.
Empirical evaluations on four typical biased learning scenarios, including
long-tail learning, noisy label learning, generalized long-tail learning, and
subpopulation shift learning, demonstrate that CLP consistently achieves
state-of-the-art performance. Moreover, visualization results support the
effectiveness of the generated causal perturbations in redirecting model
attention towards causal image attributes and dismantling spurious
associations.

</details>


### [218] [Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification](https://arxiv.org/abs/2505.15250)
*Suping Xu,Lin Shang,Keyu Liu,Hengrong Ju,Xibei Yang,Witold Pedrycz*

Key words: 模糊粗糙特征选择, 降维, 高维数据, 边缘感知, 不确定性和分离性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种结合紧密度和类别分离性的边缘感知模糊粗糙特征选择（MAFRFS）框架，解决了传统模糊粗糙特征选择（FRFS）仅关注不确定性减少而忽略分类性能的问题。实验证明MAFRFS比FRFS更具扩展性和有效性。

Motivation: 传统FRFS方法虽然能减少不确定性，但未能直接提升分类性能。本文旨在通过结合类别紧密度和分离性，设计一个更有效的特征选择框架。

Method: 提出了边缘感知模糊粗糙特征选择（MAFRFS）框架，综合考虑类别紧密度和分离性，以降低不确定性并提升分类性能。

Result: 在15个公开数据集上的实验表明，MAFRFS比传统FRFS和六种最先进的算法表现更好，具有更高的扩展性和有效性。

Conclusion: MAFRFS通过结合类别紧密度和分离性，显著提升了特征选择的效果和分类性能，为高维数据降维提供了新思路。

Abstract: Fuzzy rough feature selection (FRFS) is an effective means of addressing the
curse of dimensionality in high-dimensional data. By removing redundant and
irrelevant features, FRFS helps mitigate classifier overfitting, enhance
generalization performance, and lessen computational overhead. However, most
existing FRFS algorithms primarily focus on reducing uncertainty in pattern
classification, neglecting that lower uncertainty does not necessarily result
in improved classification performance, despite it commonly being regarded as a
key indicator of feature selection effectiveness in the FRFS literature. To
bridge uncertainty characterization and pattern classification, we propose a
Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers
both the compactness and separation of label classes. MAFRFS effectively
reduces uncertainty in pattern classification tasks, while guiding the feature
selection towards more separable and discriminative label class structures.
Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly
scalable and more effective than FRFS. The algorithms developed using MAFRFS
outperform six state-of-the-art feature selection algorithms.

</details>


### [219] [Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets](https://arxiv.org/abs/2505.15251)
*Idriss Malek,Abhijit Sharma,Salem Lahlou*

Key words: GFlowNets, mode collapse, exploration, LGGFN, diverse samples

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为LGGFN的新方法，通过利用主GFlowNet的训练损失来指导辅助GFlowNet的探索，从而加速发现多样化的高奖励样本。

Motivation: 传统的GFlowNets在实践中容易出现模式崩溃，导致训练时间延长且难以发现多样化的解。现有探索技术依赖于启发式的新颖性信号，存在局限性。

Method: LGGFN通过优先探索主GFlowNet损失较高的轨迹，将采样集中在难以理解的区域，从而提升探索效率。

Result: 实验结果表明，LGGFN在多种任务中显著提升了样本多样性和探索效率。例如，在序列生成任务中，它能发现超过基线40倍的有效模式。

Conclusion: LGGFN是一种有效的探索方法，通过直接利用训练损失指导探索，解决了GFlowNets的模式崩溃问题。

Abstract: Although Generative Flow Networks (GFlowNets) are designed to capture
multiple modes of a reward function, they often suffer from mode collapse in
practice, getting trapped in early discovered modes and requiring prolonged
training to find diverse solutions. Existing exploration techniques may rely on
heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel
approach where an auxiliary GFlowNet's exploration is directly driven by the
main GFlowNet's training loss. By prioritizing trajectories where the main
model exhibits high loss, LGGFN focuses sampling on poorly understood regions
of the state space. This targeted exploration significantly accelerates the
discovery of diverse, high-reward samples. Empirically, across various
benchmarks including grid environments, structured sequence generation, and
Bayesian structure learning, LGGFN consistently enhances exploration efficiency
and sample diversity compared to baselines. For instance, on a challenging
sequence generation task, it discovered over 40 times more unique valid modes
while simultaneously reducing the exploration error metric by approximately
99\%.

</details>


### [220] [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
*Hyunseok Lee,Jeonghoon Kim,Beomjun Kim,Jihoon Tack,Chansong Jo,Jaehong Lee,Cheonbok Park,Sookyo In,Jinwoo Shin,Kang Min Yoo*

Key words: 多模态大语言模型, GUI 定位, 数据效率, 强化学习, 空间先验, 测试时间扩展

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: ReGUIDE 是一种高效的多模态大型语言模型框架，通过自我生成的推理和空间感知的批评来提高网页元素的定位准确性，实验结果显示其性能优于基线方法且数据需求极低。

Motivation: 当前的多模态大语言模型在图形用户界面（GUI）中的元素定位任务上仍面临挑战，通常需要依赖大规模数据集。ReGUIDE 旨在通过数据高效的学习方法解决这一问题。

Method: ReGUIDE 框架通过在线强化学习生成语言推理过程，并结合空间先验进行预测批评。推断时利用测试时间扩展策略（空间搜索与坐标聚合）提升性能。

Result: 实验表明，ReGUIDE 在多项基准测试中显著优于基线方法，且仅需极少的训练数据（例如仅为最佳开源基线的 0.2%）。

Conclusion: ReGUIDE 通过数据高效的推理和空间批评机制，显著提升了 GUI 元素定位的准确性和效率。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled
autonomous agents to interact with computers via Graphical User Interfaces
(GUIs), where accurately localizing the coordinates of interface elements
(e.g., buttons) is often required for fine-grained actions. However, this
remains significantly challenging, leading prior works to rely on large-scale
web datasets to improve the grounding accuracy. In this work, we propose
Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a
novel and effective framework for web grounding that enables MLLMs to learn
data efficiently through self-generated reasoning and spatial-aware criticism.
More specifically, ReGUIDE learns to (i) self-generate a language reasoning
process for the localization via online reinforcement learning, and (ii)
criticize the prediction using spatial priors that enforce equivariance under
input transformations. At inference time, ReGUIDE further boosts performance
through a test-time scaling strategy, which combines spatial search with
coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly
advances web grounding performance across multiple benchmarks, outperforming
baselines with substantially fewer training data points (e.g., only 0.2%
samples compared to the best open-sourced baselines).

</details>


### [221] [Scaling Diffusion Transformers Efficiently via $μ$P](https://arxiv.org/abs/2505.15270)
*Chenyu Zheng,Xinyu Zhang,Rongzhen Wang,Wei Huang,Zhi Tian,Weilin Huang,Jun Zhu,Chongxuan Li*

Key words: 扩散Transformer, 最大更新参数化, 超参数调优, 视觉生成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了如何将最大更新参数化（μP）方法应用于扩散Transformer，验证了其有效性，显著降低了大规模模型的超参数调优成本。

Motivation: 扩散Transformer在视觉生成模型中表现出色，但其大规模应用受限于高成本的高参数调优。μP方法在普通Transformer中已证明有效，但尚未在扩散Transformer中得到验证。

Method: 将标准μP方法推广到扩散Transformer（如DiT、U-ViT、PixArt-α和MMDiT），并通过大规模实验验证其有效性。

Result: 实验表明，μP方法显著提升了模型训练效率（如DiT-XL-2-μP收敛速度提升2.9倍），且在小规模调优成本下，PixArt-α和MMDiT的性能超越基线。

Conclusion: μP是一种高效且可靠的方法，能够显著降低扩散Transformer的调优成本并提升性能。

Abstract: Diffusion Transformers have emerged as the foundation for vision generative
models, but their scalability is limited by the high cost of hyperparameter
(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P)
was proposed for vanilla Transformers, which enables stable HP transfer from
small to large language models, and dramatically reduces tuning costs. However,
it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion
Transformers, which differ architecturally and objectively. In this work, we
generalize standard $\mu$P to diffusion Transformers and validate its
effectiveness through large-scale experiments. First, we rigorously prove that
$\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,
PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer,
enabling the direct application of existing $\mu$P methodologies. Leveraging
this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP
transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate
achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we
validate the effectiveness of $\mu$P on text-to-image generation by scaling
PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,
models under $\mu$P outperform their respective baselines while requiring small
tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of
consumption by human experts for MMDiT-18B. These results establish $\mu$P as a
principled and efficient framework for scaling diffusion Transformers.

</details>


### [222] [Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations](https://arxiv.org/abs/2505.15284)
*Kun Fang,Qinghua Tao,Mingzhen He,Kexin Lv,Runze Yang,Haibo Hu,Xiaolin Huang,Jie Yang,Longbin Cao*

Key words: OoD检测, 核主成分分析, 非线性子空间, Cosine-Gaussian核, 高效计算

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于非线性特征子空间的OoD检测方法，通过核主成分分析（KPCA）构建判别性子空间，并利用重构误差区分InD和OoD数据，同时解决了核函数选择和高效计算等挑战。

Motivation: 探索如何通过非线性特征子空间有效表征InD和OoD数据的差异，提升深度神经网络的OoD检测可靠性。

Method: 使用KPCA框架学习判别性非线性子空间，设计Cosine-Gaussian核函数，并引入高效计算技术，结合InD数据的置信度优化子空间学习。

Result: 提出了一种新的OoD检测方法，显著提升了检测的效能和效率。

Conclusion: 非线性特征子空间为OoD检测提供了新视角，设计的核函数和高效计算技术具有实际应用价值。

Abstract: Out-of-Distribution (OoD) detection is vital for the reliability of deep
neural networks, the key of which lies in effectively characterizing the
disparities between OoD and In-Distribution (InD) data. In this work, such
disparities are exploited through a fresh perspective of non-linear feature
subspace. That is, a discriminative non-linear subspace is learned from InD
features to capture representative patterns of InD, while informative patterns
of OoD features cannot be well captured in such a subspace due to their
different distribution. Grounded on this perspective, we exploit the deviations
of InD and OoD features in such a non-linear subspace for effective OoD
detection. To be specific, we leverage the framework of Kernel Principal
Component Analysis (KPCA) to attain the discriminative non-linear subspace and
deploy the reconstruction error on such subspace to distinguish InD and OoD
data. Two challenges emerge: (i) the learning of an effective non-linear
subspace, i.e., the selection of kernel function in KPCA, and (ii) the
computation of the kernel matrix with large-scale InD data. For the former, we
reveal two vital non-linear patterns that closely relate to the InD-OoD
disparity, leading to the establishment of a Cosine-Gaussian kernel for
constructing the subspace. For the latter, we introduce two techniques to
approximate the Cosine-Gaussian kernel with significantly cheap computations.
In particular, our approximation is further tailored by incorporating the InD
data confidence, which is demonstrated to promote the learning of
discriminative subspaces for OoD data. Our study presents new insights into the
non-linear feature subspace for OoD detection and contributes practical
explorations on the associated kernel design and efficient computations,
yielding a KPCA detection method with distinctively improved efficacy and
efficiency.

</details>


### [223] [LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models](https://arxiv.org/abs/2505.15293)
*Qianyue Hao,Yiwen Song,Qingmin Liao,Jian Yuan,Yong Li*

Key words: 强化学习,大语言模型,策略探索,动态调整,任务特定性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型（LLM）的自适应策略探索方法LLM-Explorer，通过动态生成任务特定的探索策略，提升了强化学习（RL）的表现。

Motivation: 现有RL策略探索方法（如贪婪或高斯过程）缺乏任务特定性且动态调整不足，限制了其性能。LLM的分析和推理能力为自适应探索提供了新思路。

Method: 利用LLM分析代理在RL训练中的学习轨迹，生成任务特定的概率分布，动态调整探索策略。该方法兼容多种RL算法（如DQN、DDPG、TD3等）。

Result: 在Atari和MuJoCo基准测试中，LLM-Explorer平均性能提升高达37.27%。

Conclusion: LLM-Explorer通过动态生成任务特定探索策略，显著提升了RL的效果，且具有广泛兼容性。

Abstract: Policy exploration is critical in reinforcement learning (RL), where existing
approaches include greedy, Gaussian process, etc. However, these approaches
utilize preset stochastic processes and are indiscriminately applied in all
kinds of RL tasks without considering task-specific features that influence
policy exploration. Moreover, during RL training, the evolution of such
stochastic processes is rigid, which typically only incorporates a decay in the
variance, failing to adjust flexibly according to the agent's real-time
learning status. Inspired by the analyzing and reasoning capability of large
language models (LLMs), we design LLM-Explorer to adaptively generate
task-specific exploration strategies with LLMs, enhancing the policy
exploration in RL. In our design, we sample the learning trajectory of the
agent during the RL training in a given task and prompt the LLM to analyze the
agent's current policy learning status and then generate a probability
distribution for future policy exploration. Updating the probability
distribution periodically, we derive a stochastic process specialized for the
particular task and dynamically adjusted to adapt to the learning process. Our
design is a plug-in module compatible with various widely applied RL
algorithms, including the DQN series, DDPG, TD3, and any possible variants
developed based on them. Through extensive experiments on the Atari and MuJoCo
benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy
exploration, achieving an average performance improvement up to 37.27%. Our
code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for
reproducibility.

</details>


### [224] [Laplace Sample Information: Data Informativeness Through a Bayesian Lens](https://arxiv.org/abs/2505.15303)
*Johannes Kaiser,Kristian Schwethelm,Daniel Rueckert,Georgios Kaissis*

Key words: 样本信息量，深度学习，Laplace样本信息，KL散度，数据效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息理论的样本信息度量方法（LSI），用于评估数据集样本的信息量，适用于多种模型架构和学习场景，并在实验中验证了其有效性。

Motivation: 在深度学习中，准确评估数据集样本的信息量是一个重要目标，因为它可以指导样本选择，从而提高模型效率和准确性。

Method: 提出Laplace样本信息（LSI）度量，利用贝叶斯近似权重后验和KL散度来衡量样本对参数分布的影响。

Result: 实验证明，LSI能有效排序数据、检测错误标签、测量类内信息量及评估数据集难度，适用于图像和文本数据。

Conclusion: LSI是一种高效且广泛适用的样本信息度量方法，可用于大规模模型训练。

Abstract: Accurately estimating the informativeness of individual samples in a dataset
is an important objective in deep learning, as it can guide sample selection,
which can improve model efficiency and accuracy by removing redundant or
potentially harmful samples. We propose Laplace Sample Information (LSI)
measure of sample informativeness grounded in information theory widely
applicable across model architectures and learning settings. LSI leverages a
Bayesian approximation to the weight posterior and the KL divergence to measure
the change in the parameter distribution induced by a sample of interest from
the dataset. We experimentally show that LSI is effective in ordering the data
with respect to typicality, detecting mislabeled samples, measuring class-wise
informativeness, and assessing dataset difficulty. We demonstrate these
capabilities of LSI on image and text data in supervised and unsupervised
settings. Moreover, we show that LSI can be computed efficiently through probes
and transfers well to the training of large models.

</details>


### [225] [Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One](https://arxiv.org/abs/2505.15306)
*Yiwen Song,Qianyue Hao,Qingmin Liao,Jian Yuan,Yong Li*

Key words: 强化学习, 模型集成, 大型语言模型, 动态选择

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LLM-Ens是一种通过大型语言模型（LLM）增强RL模型集成的新方法，动态选择最佳代理以提升性能。

Motivation: 传统RL模型集成方法缺乏任务语义理解，限制了适应性和有效性。

Method: 利用LLM对任务状态分类，并动态选择在该状态下表现最佳的代理。

Result: 在Atari基准测试中，性能提升高达20.9%。

Conclusion: LLM-Ens显著提升了RL模型集成的性能，且兼容不同训练设置的代理。

Abstract: Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.

</details>


### [226] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Key words: 强化学习,大型语言模型,贝尔曼残差,TBRM,推理能力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种名为TBRM的算法，基于贝尔曼残差最小化的经典范式，适应于大型语言模型（LLM），无需额外计算开销即可提升推理能力。

Motivation: 当前基于策略的方法在大型语言模型的强化学习（RL）推理中占主导地位，而基于价值的方法尚未充分探索，作者希望通过TBRM算法填补这一空白。

Method: 作者提出了轨迹贝尔曼残差最小化（TBRM）算法，它是一种简单的离策略算法，利用模型的自身logits作为Q值，优化单一的轨迹级贝尔曼目标，无需评论者、重要性采样比率或裁剪。

Result: 实验表明，在标准数学推理基准测试中，TBRM比基于策略的基线（如PPO和GRPO）表现更好，且计算和内存开销相当或更低。

Conclusion: 基于价值的RL可能是一种更原则性和高效的方法，可用于增强LLM的推理能力。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [227] [Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting](https://arxiv.org/abs/2505.15312)
*Yuxuan Shu,Vasileios Lampos*

Key words: 多变量时间序列预测, Transformer架构, 谱分析, Koopman算子, 多变量相干注意力

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种名为Sonnet的新架构，通过可学习的小波变换和Koopman算子进行谱分析，利用多变量相干注意力（MVCA）建模变量依赖关系，显著提升了多变量时间序列预测性能。

Motivation: 传统的Transformer架构在多变量时间序列预测中难以有效建模变量间的复杂关系，因此需要一种更高效的模型来提升预测精度。

Method: Sonnet结合了可学习的小波变换和Koopman算子的谱分析，引入了多变量相干注意力（MVCA）来建模变量间的依赖关系。

Result: 在47个预测任务中，Sonnet在34个任务中表现最佳，平均平均绝对误差（MAE）降低了1.1%。MVCA在最具挑战性的任务中平均降低MAE 10.7%。

Conclusion: Sonnet通过谱分析和MVCA显著提升了多变量时间序列预测的准确性，尤其是在复杂依赖关系建模方面表现突出。

Abstract: Multivariable time series forecasting methods can integrate information from
exogenous variables, leading to significant prediction accuracy gains.
Transformer architecture has been widely applied in various time series
forecasting models due to its ability to capture long-range sequential
dependencies. However, a na\"ive application of transformers often struggles to
effectively model complex relationships among variables over time. To mitigate
against this, we propose a novel architecture, namely the Spectral Operator
Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to
the input and incorporates spectral analysis using the Koopman operator. Its
predictive skill relies on the Multivariable Coherence Attention (MVCA), an
operation that leverages spectral coherence to model variable dependencies. Our
empirical analysis shows that Sonnet yields the best performance on $34$ out of
$47$ forecasting tasks with an average mean absolute error (MAE) reduction of
$1.1\%$ against the most competitive baseline (different per task). We further
show that MVCA -- when put in place of the na\"ive attention used in various
deep learning models -- can remedy its deficiencies, reducing MAE by $10.7\%$
on average in the most challenging forecasting tasks.

</details>


### [228] [Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows](https://arxiv.org/abs/2505.15329)
*Anqiao Ouyang,Hongyi Ke,Qi Wang*

Key words: 可逆神经网络, 傅里叶截断, 物理数据集, 对称性学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FINE结合可逆单调激活函数和可逆滤波结构，通过傅里叶截断实现降维，在非线性波相互作用学习中表现优于传统方法和小型模型。

Motivation: 研究可逆神经网络架构的紧凑性、可解释性和信息保留特性，以提升物理数据集的表示学习能力。

Method: 提出FINE架构，结合可逆单调激活函数和可逆滤波结构，支持可逆ResNets扩展，并在潜在空间进行傅里叶截断。

Result: FINE在降维、重建精度和物理可解释性上优于DFT、POD和CNN。

Conclusion: FINE为学习物理数据集的紧凑表示提供了有前景的框架。

Abstract: Invertible neural architectures have recently attracted attention for their
compactness, interpretability, and information-preserving properties. In this
work, we propose the Fourier-Invertible Neural Encoder (FINE), which combines
invertible monotonic activation functions with reversible filter structures,
and could be extended using Invertible ResNets. This architecture is examined
in learning low-dimensional representations of one-dimensional nonlinear wave
interactions and exact circular translation symmetry. Dimensionality is
preserved across layers, except for a Fourier truncation step in the latent
space, which enables dimensionality reduction while maintaining shift
equivariance and interpretability. Our results demonstrate that FINE
significantly outperforms classical linear methods such as Discrete Fourier
Transformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves
reconstruction accuracy better than conventional deep autoencoders with
convolutional layers (CNN) - while using substantially smaller models and
offering superior physical interpretability. These findings suggest that
invertible single-neuron networks, when combined with spectral truncation,
offer a promising framework for learning compact and interpretable
representations of physics datasets, and symmetry-aware representation learning
in physics-informed machine learning.

</details>


### [229] [SSR: Speculative Parallel Scaling Reasoning in Test-time](https://arxiv.org/abs/2505.15340)
*Yuanlin Chu,Bo Wang,Xiang Liu,Hong Chen,Aiwei Liu,Xuming Hu*

Key words: 大语言模型, 数学推理, 推测解码, 效率优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一个名为SSR的无需训练的框架，通过步骤级推测解码和选择性并行模块，在数学推理任务中实现了效率与准确性的平衡。

Motivation: 尽管大语言模型在多步数学推理上表现优异，但计算成本高，尤其是在并行解码等方法中效率不足，因此需要解决效率与准确性的权衡问题。

Method: SSR框架包含选择性并行模块（SPM）和步骤级推测解码（SSD），前者通过模型内部评分筛选有潜力的推理策略，后者实现高效草案与目标的协作加速推理。

Result: 在AIME 2024、MATH-500和LiveMathBench三个数学基准测试中，SSR显著优于基线，如在LiveMathBench上提升准确率13.84%，同时在MATH-500上减少计算量至30%且无精度损失。

Conclusion: SSR成功解决了效率与准确性的矛盾，为大语言模型的高效推理提供了一种可行方案。

Abstract: Large language models (LLMs) have achieved impressive results on multi-step
mathematical reasoning, yet at the cost of high computational overhead. This
challenge is particularly acute for test-time scaling methods such as parallel
decoding, which increase answer diversity but scale poorly in efficiency. To
address this efficiency-accuracy trade-off, we propose SSR (Speculative
Parallel Scaling Reasoning), a training-free framework that leverages a key
insight: by introducing speculative decoding at the step level, we can
accelerate reasoning without sacrificing correctness. SSR integrates two
components: a Selective Parallel Module (SPM) that identifies a small set of
promising reasoning strategies via model-internal scoring, and Step-level
Speculative Decoding (SSD), which enables efficient draft-target collaboration
for fine-grained reasoning acceleration. Experiments on three mathematical
benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR
achieves strong gains over baselines. For instance, on LiveMathBench, SSR
improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the
baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in
accuracy.

</details>


### [230] [Hadamax Encoding: Elevating Performance in Model-Free Atari](https://arxiv.org/abs/2505.15345)
*Jacob E. Kooi,Zhao Yang,Vincent François-Lavet*

Key words: 强化学习,编码器架构,Hadamard乘积,最大池化,Atari-57

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种名为Hadamax的新型编码器架构，用于基于像素的无模型强化学习，通过在GELU激活的并行隐藏层之间进行Hadamard乘积的最大池化，实现了Atari-57基准测试中的最先进性能。

Motivation: 尽管神经网络架构在机器学习中影响深远，但在强化学习中，架构设计通常较为简单，改进带来的性能提升有限。因此，该研究旨在探索一种更高效的编码器架构。

Method: 研究采用了一种称为Hadamax的编码器架构，结合Hadamard乘积和最大池化技术，基于GELU激活的并行隐藏层，并将其应用于PQN算法中。

Result: 在不修改算法超参数的情况下，Hadamax-PQN比原始PQN性能提升了80%，并显著超越了Rainbow-DQN。

Conclusion: Hadamax编码器在无模型强化学习中表现出色，为未来的架构设计提供了新思路。

Abstract: Neural network architectures have a large impact in machine learning. In
reinforcement learning, network architectures have remained notably simple, as
changes often lead to small gains in performance. This work introduces a novel
encoder architecture for pixel-based model-free reinforcement learning. The
Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves
state-of-the-art performance by max-pooling Hadamard products between
GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the
Hadamax encoder achieves state-of-the-art model-free performance in the
Atari-57 benchmark. Specifically, without applying any algorithmic
hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain
over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,
the full code is available on
\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.

</details>


### [231] [Human in the Loop Adaptive Optimization for Improved Time Series Forecasting](https://arxiv.org/abs/2505.15354)
*Malik Tiomoko,Hamza Cherkaoui,Giuseppe Paolo,Zhang Yili,Yu Meng,Zhang Keli,Hafiz Tiomoko Ali*

Key words: 时间序列预测,后训练优化,强化学习,上下文赌博机,遗传算法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种轻量级、模型无关的后训练优化框架，通过强化学习等技术自动校正预测误差，支持专家参与，显著提升多个领域的预测精度。

Motivation: 现有时间序列预测模型在关键领域常产生系统性误差，亟需一种无需重新训练或修改架构的优化方法。

Method: 采用强化学习、上下文赌博机或遗传算法优化表达性变换，动态校正模型输出，支持专家自然语言引导。

Result: 在电力、天气、交通等多个基准测试中，框架以最小计算开销持续提升预测精度。

Conclusion: 该框架结合自动化校正与可解释机制，为实用预测系统提供了新方向。

Abstract: Time series forecasting models often produce systematic, predictable errors
even in critical domains such as energy, finance, and healthcare. We introduce
a novel post training adaptive optimization framework that improves forecast
accuracy without retraining or architectural changes. Our method automatically
applies expressive transformations optimized via reinforcement learning,
contextual bandits, or genetic algorithms to correct model outputs in a
lightweight and model agnostic way. Theoretically, we prove that affine
corrections always reduce the mean squared error; practically, we extend this
idea with dynamic action based optimization. The framework also supports an
optional human in the loop component: domain experts can guide corrections
using natural language, which is parsed into actions by a language model.
Across multiple benchmarks (e.g., electricity, weather, traffic), we observe
consistent accuracy gains with minimal computational overhead. Our interactive
demo shows the framework's real time usability. By combining automated post hoc
refinement with interpretable and extensible mechanisms, our approach offers a
powerful new direction for practical forecasting systems.

</details>


### [232] [Distributionally Robust Federated Learning with Client Drift Minimization](https://arxiv.org/abs/2505.15371)
*Mounssif Krouka,Chaouki Ben Issaid,Mehdi Bennis*

Key words: 联邦学习，分布鲁棒优化，动态正则化，异构数据，通信效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为DRDM的新算法，通过结合分布鲁棒优化和动态正则化，解决联邦学习在异构环境下的不公平和低效问题，提升了最坏情况下的测试准确性和通信效率。

Motivation: 联邦学习在异构环境中面临非独立同分布数据带来的不公平和低效问题，亟需一种能够提升鲁棒性和公平性的解决方案。

Method: 提出了DRDM算法，将分布鲁棒优化（DRO）与动态正则化结合，通过最小-最大优化问题优化最坏情况客户端性能，利用动态正则化和高效本地更新减少通信轮数。

Result: 实验表明，DRDM在三个基准数据集上显著提升了最坏情况测试准确性，同时减少了通信轮数，并能根据通信环境自适应选择本地更新步骤以最小化能耗。

Conclusion: DRDM在联邦学习的异构环境中表现出色，兼顾了鲁棒性、公平性和通信效率，为实际应用提供了高效解决方案。

Abstract: Federated learning (FL) faces critical challenges, particularly in
heterogeneous environments where non-independent and identically distributed
data across clients can lead to unfair and inefficient model performance. In
this work, we introduce \textit{DRDM}, a novel algorithm that addresses these
issues by combining a distributionally robust optimization (DRO) framework with
dynamic regularization to mitigate client drift. \textit{DRDM} frames the
training as a min-max optimization problem aimed at maximizing performance for
the worst-case client, thereby promoting robustness and fairness. This robust
objective is optimized through an algorithm leveraging dynamic regularization
and efficient local updates, which significantly reduces the required number of
communication rounds. Moreover, we provide a theoretical convergence analysis
for convex smooth objectives under partial participation. Extensive experiments
on three benchmark datasets, covering various model architectures and data
heterogeneity levels, demonstrate that \textit{DRDM} significantly improves
worst-case test accuracy while requiring fewer communication rounds than
existing state-of-the-art baselines. Furthermore, we analyze the impact of
signal-to-noise ratio (SNR) and bandwidth on the energy consumption of
participating clients, demonstrating that the number of local update steps can
be adaptively selected to achieve a target worst-case test accuracy with
minimal total energy cost across diverse communication environments.

</details>


### [233] [InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference](https://arxiv.org/abs/2505.15391)
*Duncan Bart,Bruno Endres Forlin,Ana-Lucia Varbanescu,Marco Ottavi,Kuan-Hsun Chen*

Key words: integer quantization, tree-based model, inference latency, energy efficiency, embedded systems

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: InTreeger是一个端到端框架，用于生成架构无关的整数决策树模型，显著提升推理速度和能效，适用于资源受限设备。

Motivation: 解决量化误差问题，为资源受限设备提供高效、精确的整数推理方案。

Method: 开发InTreeger框架，输入数据集直接生成高度优化的整数决策树模型C实现。

Result: 在ARM、x86和RISC-V架构上显著降低推理延迟，并提高能效。

Conclusion: 整数推理适合嵌入式系统和边缘计算平台，尤其适用于超低功耗设备。

Abstract: Integer quantization has emerged as a critical technique to facilitate
deployment on resource-constrained devices. Although they do reduce the
complexity of the learning models, their inference performance is often prone
to quantization-induced errors. To this end, we introduce InTreeger: an
end-to-end framework that takes a training dataset as input, and outputs an
architecture-agnostic integer-only C implementation of tree-based machine
learning model, without loss of precision. This framework enables anyone, even
those without prior experience in machine learning, to generate a highly
optimized integer-only classification model that can run on any hardware simply
by providing an input dataset and target variable. We evaluated our generated
implementations across three different architectures (ARM, x86, and RISC-V),
resulting in significant improvements in inference latency. In addition, we
show the energy efficiency compared to typical decision tree implementations
that rely on floating-point arithmetic. The results underscore the advantages
of integer-only inference, making it particularly suitable for energy- and
area-constrained devices such as embedded systems and edge computing platforms,
while also enabling the execution of decision trees on existing ultra-low power
devices.

</details>


### [234] [HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations](https://arxiv.org/abs/2505.15405)
*Martin Carrasco,Guillermo Bernardez,Marco Montagna,Nina Miolane,Lev Telyatnikov*

Key words: 图神经网络, 拓扑深度学习, 高阶关系, Hasse图分解, 可扩展性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为HOPSE的框架，通过Hasse图分解解决现有高阶拓扑深度学习中的可扩展性问题，避免了消息传递的计算复杂性，实现了线性的计算复杂度提升。

Motivation: 现有的拓扑深度学习方法在处理高阶关系时面临消息传递路径爆炸和计算复杂性高的问题，限制了其可扩展性和实际应用。

Method: 提出了HOPSE框架，利用Hasse图分解在高阶域上生成高效的编码，避免了传统的消息传递机制。

Result: HOPSE在分子、表达能力和拓扑基准测试中表现优异，性能与现有最优方法相当或更好，且计算速度提升了7倍。

Conclusion: HOPSE为可扩展的拓扑深度学习开辟了新路径，解决了高阶关系建模中的计算瓶颈。

Abstract: While Graph Neural Networks (GNNs) have proven highly effective at modeling
relational data, pairwise connections cannot fully capture multi-way
relationships naturally present in complex real-world systems. In response to
this, Topological Deep Learning (TDL) leverages more general combinatorial
representations -- such as simplicial or cellular complexes -- to accommodate
higher-order interactions. Existing TDL methods often extend GNNs through
Higher-Order Message Passing (HOMP), but face critical \emph{scalability
challenges} due to \textit{(i)} a combinatorial explosion of message-passing
routes, and \textit{(ii)} significant complexity overhead from the propagation
mechanism. To overcome these limitations, we propose HOPSE (Higher-Order
Positional and Structural Encoder) -- a \emph{message passing-free} framework
that uses Hasse graph decompositions to derive efficient and expressive
encodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scales
linearly with dataset size while preserving expressive power and permutation
equivariance. Experiments on molecular, expressivity and topological benchmarks
show that HOPSE matches or surpasses state-of-the-art performance while
achieving up to 7 $times$ speedups over HOMP-based models, opening a new path
for scalable TDL.

</details>


### [235] [Efficient Differentiable Approximation of Generalized Low-rank Regularization](https://arxiv.org/abs/2505.15407)
*Naiqi Li,Yuqiu Xie,Peiyuan Liu,Tao Dai,Yong Jiang,Shu-Tao Xia*

Key words: 低秩正则化, 可微分近似, 奇异值分解, 梯度优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种高效的可微分广义低秩正则化（LRR）近似方法，解决了传统LRR优化中计算复杂且不可微分的问题，并通过实验验证了其多样性和高效性。

Motivation: 传统低秩正则化的优化面临NP难问题，且依赖于计算耗时的奇异值分解，难以通过基于梯度的技术进行优化。

Method: 提出了一种可微分的广义LRR近似方法，支持即插即用，适用于GPU加速操作。

Result: 方法在多种任务中表现出高效性和适用性，收敛分析表明其偏差和方差随样本量和迭代步数快速降低。

Conclusion: 该方法为低秩正则化问题提供了高效且灵活的解决方案。

Abstract: Low-rank regularization (LRR) has been widely applied in various machine
learning tasks, but the associated optimization is challenging. Directly
optimizing the rank function under constraints is NP-hard in general. To
overcome this difficulty, various relaxations of the rank function were
studied. However, optimization of these relaxed LRRs typically depends on
singular value decomposition, which is a time-consuming and nondifferentiable
operator that cannot be optimized with gradient-based techniques. To address
these challenges, in this paper we propose an efficient differentiable
approximation of the generalized LRR. The considered LRR form subsumes many
popular choices like the nuclear norm, the Schatten-$p$ norm, and various
nonconvex relaxations. Our method enables LRR terms to be appended to loss
functions in a plug-and-play fashion, and the GPU-friendly operations enable
efficient and convenient implementation. Furthermore, convergence analysis is
presented, which rigorously shows that both the bias and the variance of our
rank estimator rapidly reduce with increased sample size and iteration steps.
In the experimental study, the proposed method is applied to various tasks,
which demonstrates its versatility and efficiency. Code is available at
https://github.com/naiqili/EDLRR.

</details>


### [236] [Guided Policy Optimization under Partial Observability](https://arxiv.org/abs/2505.15418)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Key words: 强化学习, 部分可观测环境, Guided Policy Optimization, 模仿学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为GPO的框架，通过共同训练引导者和学习者，解决了部分可观测环境中强化学习的难题。

Motivation: 在部分可观测环境中，强化学习由于不确定性而面临巨大挑战。现有方法在利用额外信息（如仿真数据）方面存在不足。

Method: 提出Guided Policy Optimization (GPO)框架，结合模仿学习，通过引导者利用特权信息并与学习者的策略对齐。

Result: 理论证明GPO能达到与直接强化学习相当的优化效果；实验表明GPO在多种任务中表现优异，显著优于现有方法。

Conclusion: GPO通过引导者与学习者的共同训练，有效解决了部分可观测环境中的强化学习问题。

Abstract: Reinforcement Learning (RL) in partially observable environments poses
significant challenges due to the complexity of learning under uncertainty.
While additional information, such as that available in simulations, can
enhance training, effectively leveraging it remains an open problem. To address
this, we introduce Guided Policy Optimization (GPO), a framework that co-trains
a guider and a learner. The guider takes advantage of privileged information
while ensuring alignment with the learner's policy that is primarily trained
via imitation learning. We theoretically demonstrate that this learning scheme
achieves optimality comparable to direct RL, thereby overcoming key limitations
inherent in existing approaches. Empirical evaluations show strong performance
of GPO across various tasks, including continuous control with partial
observability and noise, and memory-based challenges, significantly
outperforming existing methods.

</details>


### [237] [SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding](https://arxiv.org/abs/2505.15423)
*Marcell T. Kurbucz,Nikolaos Tzivanakis,Nilufer Sari Aslam,Adam M. Sykulski*

Key words: SplitWise, 非线性关系, 可解释性, 逐步回归, 决策树

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: SplitWise框架通过浅层决策树将数值预测变量转化为二进制特征，从而在保持线性模型透明度的同时捕捉非线性效应。

Motivation: 解决回归模型中非线性关系捕获与可解释性兼顾的挑战。

Method: 使用浅层决策树自适应地转换预测变量，并通过AIC或BIC评估模型拟合效果。

Result: 在合成和真实数据集上表现优于传统逐步回归和惩罚回归方法。

Conclusion: SplitWise在保持模型透明性的同时灵活捕捉非线性效应，生成更简洁且通用的模型。

Abstract: Capturing nonlinear relationships without sacrificing interpretability
remains a persistent challenge in regression modeling. We introduce SplitWise,
a novel framework that enhances stepwise regression. It adaptively transforms
numeric predictors into threshold-based binary features using shallow decision
trees, but only when such transformations improve model fit, as assessed by the
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).
This approach preserves the transparency of linear models while flexibly
capturing nonlinear effects. Implemented as a user-friendly R package,
SplitWise is evaluated on both synthetic and real-world datasets. The results
show that it consistently produces more parsimonious and generalizable models
than traditional stepwise and penalized regression techniques.

</details>


### [238] [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/abs/2505.15433)
*Beni Egressy,Jan Stühmer*

Key words: 大型语言模型,顺序敏感性,排列不变性,Set-LLM

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文介绍了Set-LLM，一种解决大型语言模型顺序敏感性问题的新方法，通过新的注意力掩码和位置编码实现排列不变性。

Motivation: 大型语言模型在多个应用中表现出卓越能力，但其鲁棒性问题仍是一个关键挑战，尤其是选项顺序敏感性。

Method: 提出Set-LLM架构，采用新的注意力掩码和位置编码，确保其对混合集合输入的排列不变性。

Result: 实验证明Set-LLM可有效训练，性能相当或更优，且消除顺序敏感性。

Conclusion: Set-LLM成功解决了LLM的顺序敏感性，同时保持了原始模型的运行效率。

Abstract: While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.

</details>


### [239] [Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes](https://arxiv.org/abs/2505.15496)
*Hossein Zakerinia,Christoph H. Lampert*

Key words: 多任务学习, 元学习, 泛化边界, 不平衡任务

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了多任务和元学习在不平衡设置下的新快速率泛化边界，填补了实际场景中任务训练集大小不同时的理论空白。

Motivation: 解决现实场景中多任务学习任务训练集大小不平衡的问题，此前仅有标准速率边界，快速率边界仅限于训练集大小相同的情况。

Method: 通过数值可计算且可解释的新边界，展示了在不平衡多任务设置下的灵活性和更强的保证。

Result: 新边界提供了比以往更强的理论保证，并揭示不平衡设置与平衡设置的不同统计特性。

Conclusion: 不平衡多任务设置允许两种风险定义，取决于任务的重要性和样本数量的权重分配。

Abstract: We present new fast-rate generalization bounds for multi-task and
meta-learning in the unbalanced setting, i.e. when the tasks have training sets
of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether if all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.

</details>


### [240] [Certified Neural Approximations of Nonlinear Dynamics](https://arxiv.org/abs/2505.15497)
*Frederik Baymler Mathiesen,Nikolaus Vertovec,Francesco Fabiano,Luca Laurenti,Alessandro Abate*

Key words: 神经网络,动态系统,误差界限,安全关键,验证方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的、自适应且可并行化的验证方法，为神经网络的动态系统近似提供了正式误差界限，确保其在安全关键场景中的可靠使用。

Motivation: 在安全关键领域，神经网络作为非线性动态系统的近似模型需要形式化的误差边界以确保其可靠性，但目前缺乏此类方法。

Method: 基于认证的一阶模型，提出一种自适应、可并行化的验证方法，通过将误差界限解释为有界扰动来分析神经网络的近似效果。

Result: 方法在多个标准基准测试中表现出色，优于现有技术，并在神经网络压缩和基于自动编码器的Koopman算子学习中展示了灵活性。

Conclusion: 提出的方法为神经网络的动态系统近似提供了有效的验证工具，扩展了其在安全和复杂场景中的应用范围。

Abstract: Neural networks hold great potential to act as approximate models of
nonlinear dynamical systems, with the resulting neural approximations enabling
verification and control of such systems. However, in safety-critical contexts,
the use of neural approximations requires formal bounds on their closeness to
the underlying system. To address this fundamental challenge, we propose a
novel, adaptive, and parallelizable verification method based on certified
first-order models. Our approach provides formal error bounds on the neural
approximations of dynamical systems, allowing them to be safely employed as
surrogates by interpreting the error bound as bounded disturbances acting on
the approximated dynamics. We demonstrate the effectiveness and scalability of
our method on a range of established benchmarks from the literature, showing
that it outperforms the state-of-the-art. Furthermore, we highlight the
flexibility of our framework by applying it to two novel scenarios not
previously explored in this context: neural network compression and an
autoencoder-based deep learning architecture for learning Koopman operators,
both yielding compelling results.

</details>


### [241] [Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning](https://arxiv.org/abs/2505.15507)
*Mahesh Godavarti*

Key words: 多维嵌入,非交换运算符,组合框架,序列建模,深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文提出了一种新的多维组合嵌入的代数结构，基于方向性非交换单子运算符。

Motivation: 为现代机器学习架构提供一种具有理论吸引力的多维框架，统一并推广经典序列建模范式。

Method: 定义每个轴的独立组合运算符circ_i，确保各轴关联性且全局可交换，支持一致跨轴组合。

Result: 该框架首次为经典序列建模提供统一的多维基础，并在理论上验证其代数性质。

Conclusion: 这种结构有望应用于未来的深度学习模型设计，如Transformer的位置编码和图像嵌入。

Abstract: We introduce a new algebraic structure for multi-dimensional compositional
embeddings, built on directional non-commutative monoidal operators. The core
contribution of this work is this novel framework, which exhibits appealing
theoretical properties (associativity along each dimension and an interchange
law ensuring global consistency) while remaining compatible with modern machine
learning architectures. Our construction defines a distinct composition
operator circ_i for each axis i, ensuring associative combination along each
axis without imposing global commutativity. Importantly, all axis-specific
operators commute with one another, enforcing a global interchange law that
enables consistent crossaxis compositions. This is, to our knowledge, the first
approach that provides a common foundation that generalizes classical
sequence-modeling paradigms (e.g., structured state-space models (SSMs) and
transformer self-attention) to a unified multi-dimensional framework. For
example, specific one-dimensional instances of our framework can recover the
familiar affine transformation algebra, vanilla self-attention, and the
SSM-style recurrence. The higher-dimensional generalizations naturally support
recursive, structure-aware operations in embedding spaces. We outline several
potential applications unlocked by this structure-including structured
positional encodings in Transformers, directional image embeddings, and
symbolic modeling of sequences or grids-indicating that it could inform future
deep learning model designs. We formally establish the algebraic properties of
our framework and discuss efficient implementations. Finally, as our focus is
theoretical, we include no experiments here and defer empirical validation to
future work, which we plan to undertake.

</details>


### [242] [NOMAD Projection](https://arxiv.org/abs/2505.15511)
*Brandon Duderstadt,Zach Nussbaum,Laurens van der Maaten*

Key words: 生成式AI, 数据可视化, 非线性降维, NOMAD Projection, 多GPU训练

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: NOMAD Projection是一种面向非结构化数据的非线性降维可视化方法，支持多GPU训练，性能优于现有技术。

Motivation: 生成式AI的快速发展导致数据集规模急剧增长，传统可视化方法（如t-SNE和UMAP）难以应对，影响了AI的可解释性。

Method: 提出NOMAD Projection方法，通过非线性降维实现非结构化数据的可视化，并支持多GPU训练。

Result: 理论证明NOMAD Projection是InfoNC-t-SNE损失的近似上界，实验显示其在性能和速度上优于现有技术，且成功应用于Multilingual Wikipedia数据集。

Conclusion: NOMAD Projection为大规模非结构化数据可视化提供了高效解决方案。

Abstract: The rapid adoption of generative AI has driven an explosion in the size of
datasets consumed and produced by AI models. Traditional methods for
unstructured data visualization, such as t-SNE and UMAP, have not kept up with
the pace of dataset scaling. This presents a significant challenge for AI
explainability, which relies on methods such as t-SNE and UMAP for exploratory
data analysis. In this paper, we introduce Negative Or Mean Affinity
Discrimination (NOMAD) Projection, the first method for unstructured data
visualization via nonlinear dimensionality reduction that can run on multiple
GPUs at train time. We provide theory that situates NOMAD Projection as an
approximate upper bound on the InfoNC-t-SNE loss, and empirical results that
demonstrate NOMAD Projection's superior performance and speed profile compared
to existing state-of-the-art methods. We demonstrate the scalability of NOMAD
Projection by computing the first complete data map of Multilingual Wikipedia.

</details>


### [243] [AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization](https://arxiv.org/abs/2505.15514)
*Soham Sane*

Key words: PPO, 优势估计, 强化学习, 动态缩放, 梯度优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: AM-PPO通过动态非线性缩放机制改进PPO的优势估计，提升训练稳定性和效率。

Motivation: 优势估计的方差、噪声和尺度问题影响PPO训练性能，需要改进。

Method: 引入动态α控制器，基于优势信号的统计特性（如范数、方差）进行非线性缩放，并使用tanh门控函数优化梯度更新。

Result: 实验显示AM-PPO在连续控制任务中表现更优，学习过程稳定，减少了自适应优化器的裁剪需求。

Conclusion: 优势调制是增强强化学习优化的通用技术。

Abstract: Proximal Policy Optimization (PPO) is a widely used reinforcement learning
algorithm that heavily relies on accurate advantage estimates for stable and
efficient training. However, raw advantage signals can exhibit significant
variance, noise, and scale-related issues, impeding optimal learning
performance. To address this challenge, we introduce Advantage Modulation PPO
(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage
estimates using a dynamic, non-linear scaling mechanism. This adaptive
modulation employs an alpha controller that dynamically adjusts the scaling
factor based on evolving statistical properties of the advantage signals, such
as their norm, variance, and a predefined target saturation level. By
incorporating a tanh-based gating function driven by these adaptively scaled
advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates
and improve the conditioning of the policy gradient landscape. Crucially, this
modulation also influences value function training by providing consistent and
adaptively conditioned learning targets. Empirical evaluations across standard
continuous control benchmarks demonstrate that AM-PPO achieves superior reward
trajectories, exhibits sustained learning progression, and significantly
reduces the clipping required by adaptive optimizers. These findings underscore
the potential of advantage modulation as a broadly applicable technique for
enhancing reinforcement learning optimization.

</details>


### [244] [Explainable embeddings with Distance Explainer](https://arxiv.org/abs/2505.15516)
*Christiaan Meijer,E. G. Patrick Bos*

Key words: 可解释人工智能, 嵌入空间, 距离解释, 显著性技术, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: Distance Explainer 是一种新颖的本地事后解释方法，专注于解释嵌入向量空间中的距离，通过选择性掩码和距离排名掩码过滤生成解释，提高了深度学习模型的透明性。

Motivation: 当前可解释人工智能（XAI）方法较少关注嵌入向量空间的解释问题，尤其是对复杂抽象的维度进行解释。研究旨在填补这一空白，提升深度学习模型的透明性和可信度。

Method: 引入 Distance Explainer，利用 RISE 的显著性技术，通过选择性掩码和距离排名掩码过滤生成嵌入空间中两点距离的解释。

Result: 在 ImageNet 和 CLIP 模型上的实验表明，该方法能有效识别嵌入空间中相似或不相似的特征，同时保持较高的鲁棒性和一致性。

Conclusion: Distance Explainer 解决了 XAI 研究中的关键空白，提升了基于嵌入空间的深度学习应用的透明性和可信度。

Abstract: While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.

</details>


### [245] [A Temporal Difference Method for Stochastic Continuous Dynamics](https://arxiv.org/abs/2505.15544)
*Haruki Settai,Naoya Takeishi,Takehisa Yairi*

Key words: 强化学习；HJB方程；随机最优控制；无模型方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究提出了一种基于Hamilton-Jacobi-Bellman（HJB）方程的无模型强化学习方法，无需预先知道动力学方程，并在实验中展示了其优于基于转移核的方法。

Motivation: 现有的HJB强化学习方法通常假设动力学方程已知，限制了其应用。本文旨在解决这一局限，提出一种无模型方法。

Method: 提出了一种针对HJB方程的无模型方法，并设计了相应的时间差分算法。

Result: 实验表明，该方法在理论和实证上均优于基于转移核的现有方法。

Conclusion: 该研究为随机最优控制与无模型强化学习的结合提供了新的途径。

Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We demonstrate its potential advantages over
transition kernel-based formulations, both qualitatively and empirically. The
proposed formulation paves the way toward bridging stochastic optimal control
and model-free reinforcement learning.

</details>


### [246] [Oversmoothing, "Oversquashing", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning](https://arxiv.org/abs/2505.15547)
*Adrian Arnaiz-Rodriguez,Federico Errica*

Key words: 图机器学习, 消息传递, 批判性思维, 反例, 研究模糊性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该立场论文探讨了图机器学习中消息传递范式的益处与局限性，指出了研究中常见的误解和模糊之处，并提供了反例以促进批判性思维。

Motivation: 随着图机器学习社区的快速发展，围绕消息传递的研究中出现了一些被广泛接受但未必准确的假设和信念。这些模糊性阻碍了研究者对具体问题的聚焦与解决。

Method: 分析常见的误解和假设，并通过简单的反例进行验证和澄清。

Result: 揭示了研究中的模糊性和误解，明确了不同问题之间的区别。

Conclusion: 提倡对这些问题采取批判性思维，推动独立但相互关联的研究方向。

Abstract: After a renaissance phase in which researchers revisited the message-passing
paradigm through the lens of deep learning, the graph machine learning
community shifted its attention towards a deeper and practical understanding of
message-passing's benefits and limitations. In this position paper, we notice
how the fast pace of progress around the topics of oversmoothing and
oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came
with the consolidation of commonly accepted beliefs and assumptions that are
not always true nor easy to distinguish from each other. We argue that this has
led to ambiguities around the investigated problems, preventing researchers
from focusing on and addressing precise research questions while causing a good
amount of misunderstandings. Our contribution wants to make such common beliefs
explicit and encourage critical thinking around these topics, supported by
simple but noteworthy counterexamples. The hope is to clarify the distinction
between the different issues and promote separate but intertwined research
directions to address them.

</details>


### [247] [Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution](https://arxiv.org/abs/2505.15548)
*Suvadeep Hajra*

Key words: 自注意力机制,训练稳定性,LS-attention,Transformer,语言模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: LS-attention通过分解自注意力机制为局部和全局注意力头，解决了训练不稳定的问题，提升了训练效率和推理速度。

Motivation: 研究自注意力机制在捕捉短程依赖时的局限性导致的训练不稳定问题。

Method: 提出LS-attention，将自注意力分解为局部和全局注意力头。

Result: LS-attention显著降低验证困惑度，减少GPU使用时间，推理延迟降低36%。

Conclusion: LS-attention是一种高效稳定的自注意力改进方法。

Abstract: Transformer language models have driven significant progress across various
fields, including natural language processing and computer vision. A central
component of these models is the self-attention (SA) mechanism, which learns
rich vector representations of tokens by modeling their relationships with
others in a sequence. However, despite extensive research, transformers
continue to suffer from training instability -- often manifesting as spikes or
divergence in the training loss during a run.
  In this work, we identify one source of this instability: SA's limited
ability to capture short-range dependencies, especially in tasks like language
modeling, where almost every token heavily relies on its nearby neighbors. This
limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing
training. To address this, we propose decomposing the SA into local
(short-range) and global (long-range) attention heads. This decomposed
attention, referred to as Long Short-attention (LS-attention), mitigates logit
explosion and results in more stable training compared to an equivalent
multi-head self-attention (MHSA). Empirical comparisons with two alternative
training stabilization methods show that LS-attention reduces the validation
perplexity to nearly 2/5 of that achieved by one method and reaches a similar
perplexity as the other method using only 1/20 of the GPU hours. Additionally,
our experiments demonstrate that LS-attention reduces inference latency by up
to 36% compared to a state-of-the-art implementation of equivalent MHSA.

</details>


### [248] [Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection](https://arxiv.org/abs/2505.15560)
*Julian Oelhaf,Georg Kordowich,Changhun Kim,Paula Andrea Perez-Toro,Andreas Maier,Johann Jager,Siming Bayer*

Key words: 可再生能源电网, 机器学习, 故障检测, 故障线路识别, 数据稀疏性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 德国向可再生能源电网转型中，机器学习用于故障检测和故障线路识别。但数据稀疏性影响模型可靠性，本文提出评估框架分析其影响，结果显示故障检测稳健，线路识别较敏感。

Motivation: 研究数据稀疏性对机器学习在电网保护中（如故障检测和线路识别）的影响，填补此前系统性验证的空白。

Method: 提出评估框架，模拟数据稀疏场景，定量分析其对机器学习模型性能的影响。

Result: 故障检测模型在数据大幅减少下仍稳健（F1-score 0.999），但线路识别性能下降明显（最高55.61%）。

Conclusion: 评估框架提供了优化机器学习模型的实际建议，支持电网保护的高效实施。

Abstract: Germany's transition to a renewable energy-based power system is reshaping
grid operations, requiring advanced monitoring and control to manage
decentralized generation. Machine learning (ML) has emerged as a powerful tool
for power system protection, particularly for fault detection (FD) and fault
line identification (FLI) in transmission grids. However, ML model reliability
depends on data quality and availability. Data sparsity resulting from sensor
failures, communication disruptions, or reduced sampling rates poses a
challenge to ML-based FD and FLI. Yet, its impact has not been systematically
validated prior to this work. In response, we propose a framework to assess the
impact of data sparsity on ML-based FD and FLI performance. We simulate
realistic data sparsity scenarios, evaluate their impact, derive quantitative
insights, and demonstrate the effectiveness of this evaluation strategy by
applying it to an existing ML-based framework. Results show the ML model
remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after
a 50x data reduction. In contrast, FLI is more sensitive, with performance
decreasing by 55.61% for missing voltage measurements and 9.73% due to
communication failures at critical network points. These findings offer
actionable insights for optimizing ML models for real-world grid protection.
This enables more efficient FD and supports targeted improvements in FLI.

</details>


### [249] [Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers](https://arxiv.org/abs/2505.15570)
*Marko Tuononen,Duy Vu,Dani Korpi,Vesa Starck,Ville Hautamäki*

Key words: 概念发现,神经激活模式（NAP）,聚类,信噪比（SNR）,激活流形

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了神经激活模式（NAP）方法，通过聚类全层激活分布发现层级别概念，改进了标准化、分布估计、距离度量等，在无线电接收模型中观察到连续激活流形，强调了SNR作为关键学习因素。

Motivation: 探索神经网络中分布式层范围模式的概念发现，弥补了以往关注单个神经元或可解释特征的不足。

Method: 采用NAP方法，应用改进的标准化、分布估计、距离度量和聚类选择，分析视觉对象识别和无线电接收模型的激活分布。

Result: 无线电接收模型中未发现明显概念，但观察到由信噪比（SNR）塑造的连续激活流形，支持物理合理性；改进的NAP提高了分布内外数据分离能力。

Conclusion: 聚类设计和激活流形在解释和调试神经网络行为中至关重要，改进的NAP具有更好的泛化能力。

Abstract: Concept discovery in neural networks often targets individual neurons or
human-interpretable features, overlooking distributed layer-wide patterns. We
study the Neural Activation Pattern (NAP) methodology, which clusters
full-layer activation distributions to identify such layer-level concepts.
Applied to visual object recognition and radio receiver models, we propose
improved normalization, distribution estimation, distance metrics, and varied
cluster selection. In the radio receiver model, distinct concepts did not
emerge; instead, a continuous activation manifold shaped by Signal-to-Noise
Ratio (SNR) was observed -- highlighting SNR as a key learned factor,
consistent with classical receiver behavior and supporting physical
plausibility. Our enhancements to NAP improved in-distribution vs.
out-of-distribution separation, suggesting better generalization and indirectly
validating clustering quality. These results underscore the importance of
clustering design and activation manifolds in interpreting and troubleshooting
neural network behavior.

</details>


### [250] [Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback](https://arxiv.org/abs/2505.15572)
*Wangyang Ying,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Sixun Dong,Haifeng Chen,Yanjie Fu*

Key words: 数据到方程,基础模型,强化学习,微调,数学语义

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的微调框架，用于提升基础模型在数据到方程任务中的领域适应能力，优化生成的数学方程。

Motivation: 传统方法和基础模型在数据到方程任务中存在搜索效率低、泛化能力差和对数学语义关注不足的问题。

Method: 采用强化学习微调框架，通过下游数值适应性生成的奖励信号直接优化预训练模型的生成策略。

Result: 实验表明，该方法在复杂数据分布下提高了方程生成的准确性和鲁棒性。

Conclusion: 强化学习微调有效提升了基础模型在特定领域任务中的表现。

Abstract: The data-to-equation (Data2Eqn) task aims to discover interpretable
mathematical equations that map observed values to labels, offering physical
insights and broad applicability across academic and industrial domains.
Genetic programming and traditional deep learning-based approaches suffer from
search inefficiency and poor generalization on small task-specific datasets.
Foundation models showed promise in this area, but existing approaches suffer
from: 1) They are pretrained on general-purpose data distributions, making them
less effective for domain-specific tasks; and 2) their training objectives
focus on token-level alignment, overlooking mathematical semantics, which can
lead to inaccurate equations. To address these issues, we aim to enhance the
domain adaptability of foundation models for Data2Eqn tasks. In this work, we
propose a reinforcement learning-based finetuning framework that directly
optimizes the generation policy of a pretrained model through reward signals
derived from downstream numerical fitness. Our method allows the model to adapt
to specific and complex data distributions and generate mathematically
meaningful equations. Extensive experiments demonstrate that our approach
improves both the accuracy and robustness of equation generation under complex
distributions.

</details>


### [251] [Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions](https://arxiv.org/abs/2505.15579)
*Hossein Zakerinia,Jonathan Scott,Christoph H. Lampert*

Key words: 个性化联邦学习, FLowDUP, PAC-Bayesian, 未标记数据, 低维子空间

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: FLowDUP是一种联邦学习方法，通过仅使用未标记数据的前向传递生成个性化模型，参数存储于低维子空间，计算和通信高效。

Motivation: 解决现有方法需要标记数据的问题，提出仅需未标记数据即可生成个性化模型的方法。

Method: FLowDUP方法基于新提出的归纳多任务PAC-Bayesian泛化边界理论，允许标记和未标记数据的客户端共同参与训练。

Result: 实验验证了FLowDUP在多种数据集和统计异构客户端上的优异表现，并通过消融研究证实了方法的有效性。

Conclusion: FLowDUP为个性化联邦学习提供了一种无需标记数据的高效解决方案，具有理论和实验支持。

Abstract: Personalized federated learning has emerged as a popular approach to training
on devices holding statistically heterogeneous data, known as clients. However,
most existing approaches require a client to have labeled data for training or
finetuning in order to obtain their own personalized model. In this paper we
address this by proposing FLowDUP, a novel method that is able to generate a
personalized model using only a forward pass with unlabeled data. The generated
model parameters reside in a low-dimensional subspace, enabling efficient
communication and computation. FLowDUP's learning objective is theoretically
motivated by our new transductive multi-task PAC-Bayesian generalization bound,
that provides performance guarantees for unlabeled clients. The objective is
structured in such a way that it allows both clients with labeled data and
clients with only unlabeled data to contribute to the training process. To
supplement our theoretical results we carry out a thorough experimental
evaluation of FLowDUP, demonstrating strong empirical performance on a range of
datasets with differing sorts of statistically heterogeneous clients. Through
numerous ablation studies, we test the efficacy of the individual components of
the method.

</details>


### [252] [World Models as Reference Trajectories for Rapid Motor Adaptation](https://arxiv.org/abs/2505.15589)
*Carlos Stein Brito,Daniel McNamee*

Key words: Reflexive World Models, 强化学习, 快速适应, 双控制框架

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了Reflexive World Models (RWM)，一种双控制框架，用于快速适应系统动态变化的强化学习控制策略。

Motivation: 解决在系统动态变化时，传统控制策略性能下降的问题，需要快速适应新数据。

Method: 结合世界模型预测作为隐式参考轨迹，将控制问题分为长期奖励最大化和快速潜在控制执行。

Result: 相比基于模型的RL基准，该方法实现了更快的适应速度和低在线计算成本，同时保持接近最优性能。

Conclusion: RWM结合了强化学习的灵活策略学习和快速纠错能力，适用于高维连续控制任务。

Abstract: Deploying learned control policies in real-world environments poses a
fundamental challenge. When system dynamics change unexpectedly, performance
degrades until models are retrained on new data. We introduce Reflexive World
Models (RWM), a dual control framework that uses world model predictions as
implicit reference trajectories for rapid adaptation. Our method separates the
control problem into long-term reward maximization through reinforcement
learning and robust motor execution through rapid latent control. This dual
architecture achieves significantly faster adaptation with low online
computational cost compared to model-based RL baselines, while maintaining
near-optimal performance. The approach combines the benefits of flexible policy
learning through reinforcement learning with rapid error correction
capabilities, providing a principled approach to maintaining performance in
high-dimensional continuous control tasks under varying dynamics.

</details>


### [253] [Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off](https://arxiv.org/abs/2505.15594)
*Yury Belousov,Brian Pulfer,Vitaliy Kinakh,Slava Voloshynovskiy*

Key words: 基础模型, 对抗攻击, 扩散去噪平滑, 鲁棒性, 性能权衡

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 本文分析了扩散去噪平滑技术在不同下游任务和对抗攻击中的效果，发现其对基础模型的鲁棒性存在挑战。

Motivation: 尽管基础模型在多种任务中表现出色，但其对对抗输入的脆弱性仍有待解决。扩散去噪平滑技术是一种有前景的方法，但其有效性尚未在多任务中充分验证。

Method: 研究使用预训练的扩散模型在模型推理前预处理输入，并在三个数据集上测试四种下游任务和三种对抗攻击算法。

Result: 研究发现，高噪声扩散去噪会显著降低性能（高达57%），而低噪声设置虽能保持性能但无法抵御所有攻击类型。此外，还提出了一种针对扩散过程的新型攻击策略。

Conclusion: 扩散去噪平滑技术在对抗鲁棒性和性能之间存在权衡，仍需进一步研究解决。

Abstract: While foundation models demonstrate impressive performance across various
tasks, they remain vulnerable to adversarial inputs. Current research explores
various approaches to enhance model robustness, with Diffusion Denoised
Smoothing emerging as a particularly promising technique. This method employs a
pretrained diffusion model to preprocess inputs before model inference. Yet,
its effectiveness remains largely unexplored beyond classification. We aim to
address this gap by analyzing three datasets with four distinct downstream
tasks under three different adversarial attack algorithms. Our findings reveal
that while foundation models maintain resilience against conventional
transformations, applying high-noise diffusion denoising to clean images
without any distortions significantly degrades performance by as high as 57%.
Low-noise diffusion settings preserve performance but fail to provide adequate
protection across all attack types. Moreover, we introduce a novel attack
strategy specifically targeting the diffusion process itself, capable of
circumventing defenses in the low-noise regime. Our results suggest that the
trade-off between adversarial robustness and performance remains a challenge to
be addressed.

</details>


### [254] [Deep Learning for Continuous-time Stochastic Control with Jumps](https://arxiv.org/abs/2505.15602)
*Patrick Cheridito,Jean-Loup Dupret,Donatien Hainaut*

Key words: 深度学习, 随机控制, 神经网络, 动态规划, 跳跃过程

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种基于模型的深度学习方法，用于解决具有跳跃的有限时间连续时间随机控制问题。

Motivation: 传统方法在处理复杂、高维随机控制问题时效率不足，需要更高效且准确的解决方案。

Method: 通过迭代训练两个神经网络：一个表示最优策略，另一个近似值函数，并利用动态规划原理推导训练目标。

Result: 在不同问题上的实证评估表明，该方法在准确性和扩展性方面表现优异。

Conclusion: 该方法能有效解决复杂高维随机控制任务。

Abstract: In this paper, we introduce a model-based deep-learning approach to solve
finite-horizon continuous-time stochastic control problems with jumps. We
iteratively train two neural networks: one to represent the optimal policy and
the other to approximate the value function. Leveraging a continuous-time
version of the dynamic programming principle, we derive two different training
objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the
networks capture the underlying stochastic dynamics. Empirical evaluations on
different problems illustrate the accuracy and scalability of our approach,
demonstrating its effectiveness in solving complex, high-dimensional stochastic
control tasks.

</details>


### [255] [Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI](https://arxiv.org/abs/2505.15622)
*Pietro Bartoli,Christian Veronesi,Andrea Giudici,David Siorpaes,Diana Trojaniello,Franco Zappa*

Key words: TinyML, benchmarking, energy efficiency, edge computing

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种新的TinyML性能评估方法，融合能耗和延迟测量，并通过分阶段分析（预推理、推理和后推理）提供更全面的性能洞察。

Motivation: 由于TinyML在资源受限设备上的广泛应用，现有评估方法存在局限性，亟需更全面的性能评估框架。

Method: 提出了一种新的基准测试方法，结合能耗与延迟测量，区分执行阶段，支持自动化测试以提高统计显著性。

Result: 在STM32N6 MCU上的测试表明，降低核心电压和时钟频率可提升能效，同时不影响推理性能。

Conclusion: 该方法适用于跨平台比较，能够量化不同硬件实现中预/后处理的效率差异。

Abstract: The rise of IoT has increased the need for on-edge machine learning, with
TinyML emerging as a promising solution for resource-constrained devices such
as MCU. However, evaluating their performance remains challenging due to
diverse architectures and application scenarios. Current solutions have many
non-negligible limitations. This work introduces an alternative benchmarking
methodology that integrates energy and latency measurements while
distinguishing three execution phases pre-inference, inference, and
post-inference. Additionally, the setup ensures that the device operates
without being powered by an external measurement unit, while automated testing
can be leveraged to enhance statistical significance. To evaluate our setup, we
tested the STM32N6 MCU, which includes a NPU for executing neural networks. Two
configurations were considered: high-performance and Low-power. The variation
of the EDP was analyzed separately for each phase, providing insights into the
impact of hardware configurations on energy efficiency. Each model was tested
1000 times to ensure statistically relevant results. Our findings demonstrate
that reducing the core voltage and clock frequency improve the efficiency of
pre- and post-processing without significantly affecting network execution
performance. This approach can also be used for cross-platform comparisons to
determine the most efficient inference platform and to quantify how pre- and
post-processing overhead varies across different hardware implementations.

</details>


### [256] [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/abs/2505.15624)
*H. V. AlquBoj,Hilal AlQuabeh,Velibor Bojkovic,Munachiso Nwadike,Kentaro Inui*

Key words: grokking, 神经网络, 嵌入, 延迟泛化, 双线性耦合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究表明，嵌入是神经网络中延迟泛化（grokking）的核心因素，通过分析嵌入更新动力学和双线性耦合机制，提出了优化训练的方法。

Motivation: 探索神经网络中延迟泛化的驱动因素，特别是在嵌入层中的作用，以提升模型的训练效率和泛化能力。

Method: 引入嵌入层到MLPs中，分析其更新动力学和双线性耦合机制，提出频率感知采样和嵌入特定学习率优化方法。

Result: 嵌入层导致延迟泛化，通过优化方法可以加速收敛并改善训练效率。

Conclusion: 嵌入层是延迟泛化的关键，针对性优化方法可有效改善训练动态和性能。

Abstract: Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.

</details>


### [257] [Aligning Explanations with Human Communication](https://arxiv.org/abs/2505.15626)
*Jacopo Teneggi,Zhenzhen Wang,Paul H. Yi,Tianmin Shu,Jeremias Sulam*

Key words: 机器可解释性, 听众自适应解释, 语用推理, 理性言语行为, 沟通效用

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于听众自适应解释的方法，通过语用推理和理性言语行为生成更具沟通效用的解释，填补了现有方法忽略沟通背景的空白。

Motivation: 现有方法在生成可解释性解释时忽略了听众的背景和理解能力，导致解释效果不佳。本文旨在通过听众自适应解释解决这一问题。

Method: 提出了一种基于语用推理和理性言语行为的迭代程序，仅需候选解释的成对偏好即可生成高效用的解释。

Result: 在图像分类任务中验证了方法有效性，证明其解释与听众偏好更匹配，并通过用户研究展示了沟通效用的提升。

Conclusion: 听众自适应解释能够显著提升沟通效用，适用于实际场景中听众模型不可用的情况。

Abstract: Machine learning explainability aims to make the decision-making process of
black-box models more transparent by finding the most important input features
for a given prediction task. Recent works have proposed composing explanations
from semantic concepts (e.g., colors, patterns, shapes) that are inherently
interpretable to the user of a model. However, these methods generally ignore
the communicative context of explanation-the ability of the user to understand
the prediction of the model from the explanation. For example, while a medical
doctor might understand an explanation in terms of clinical markers, a patient
may need a more accessible explanation to make sense of the same diagnosis. In
this paper, we address this gap with listener-adaptive explanations. We propose
an iterative procedure grounded in principles of pragmatic reasoning and the
rational speech act to generate explanations that maximize communicative
utility. Our procedure only needs access to pairwise preferences between
candidate explanations, relevant in real-world scenarios where a listener model
may not be available. We evaluate our method in image classification tasks,
demonstrating improved alignment between explanations and listener preferences
across three datasets. Furthermore, we perform a user study that demonstrates
our explanations increase communicative utility.

</details>


### [258] [Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks](https://arxiv.org/abs/2505.15631)
*Nick Kocher,Christian Wassermann,Leona Hennig,Jonas Seng,Holger Hoos,Kristian Kersting,Marius Lindauer,Matthias Müller*

Key words: Neural Architecture Search, energy-aware benchmarking, GPU measurement, surrogate modeling

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出能量感知的神经架构搜索（NAS）基准设计原则，并分析了GPU测量API对结果的影响。

Motivation: 减少NAS过程中的高能耗问题，通过能量感知基准设计优化模型能效与准确性的权衡。

Method: 提出三个设计原则：可靠功率测量、广泛GPU使用范围和全面成本报告，并通过实验验证其有效性。

Result: 发现GPU测量API对结果质量影响显著，使用Nvidia SMI会导致初始数据收集的低功率估值不准确。

Conclusion: 强调能量感知基准设计的关键考虑因素，并提供改进建议，如校准实验以提升工具准确性。

Abstract: Neural Architecture Search (NAS) accelerates progress in deep learning
through systematic refinement of model architectures. The downside is
increasingly large energy consumption during the search process.
Surrogate-based benchmarking mitigates the cost of full training by querying a
pre-trained surrogate to obtain an estimate for the quality of the model.
Specifically, energy-aware benchmarking aims to make it possible for NAS to
favourably trade off model energy consumption against accuracy. Towards this
end, we propose three design principles for such energy-aware benchmarks: (i)
reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic
cost reporting. We analyse EA-HAS-Bench based on these principles and find that
the choice of GPU measurement API has a large impact on the quality of results.
Using the Nvidia System Management Interface (SMI) on top of its underlying
library influences the sampling rate during the initial data collection,
returning faulty low-power estimations. This results in poor correlation with
accurate measurements obtained from an external power meter. With this study,
we bring to attention several key considerations when performing energy-aware
surrogate-based benchmarking and derive first guidelines that can help design
novel benchmarks. We show a narrow usage range of the four GPUs attached to our
device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down
even further when using all four GPUs. To improve holistic energy reporting, we
propose calibration experiments over assumptions made in popular tools, such as
Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to
8.9 % without and to 6.6 % with prior estimation of the expected load on the
device.

</details>


### [259] [Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes](https://arxiv.org/abs/2505.15638)
*Daniel Waxman,Fernando Llorente,Petar M. Djurić*

Key words: 贝叶斯集成、在线学习、贝叶斯模型平均、贝叶斯堆叠、投资组合选择

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文重新审视了贝叶斯集成问题，提出了一种在线贝叶斯堆叠方法（OBS），通过优化预测分布的对数分数自适应地组合贝叶斯模型，并与在线贝叶斯模型平均（BMA）进行了对比。

Motivation: 解决在线持续学习环境中贝叶斯模型的最优组合问题，重新审视现有方法（如BMA和贝叶斯堆叠）的局限性。

Method: 提出在线贝叶斯堆叠（OBS），利用经验贝叶斯视角和在线优化理论，优化预测分布的对数分数。

Result: 通过理论分析和实证评估，发现OBS在某些场景下优于在线BMA。

Conclusion: OBS与BMA优化了不同但相关的成本函数，为实践者提供了选择方法的依据。

Abstract: We revisit the classical problem of Bayesian ensembles and address the
challenge of learning optimal combinations of Bayesian models in an online,
continual learning setting. To this end, we reinterpret existing approaches
such as Bayesian model averaging (BMA) and Bayesian stacking through a novel
empirical Bayes lens, shedding new light on the limitations and pathologies of
BMA. Further motivated by insights from online optimization, we propose Online
Bayesian Stacking (OBS), a method that optimizes the log-score over predictive
distributions to adaptively combine Bayesian models. A key contribution of our
work is establishing a novel connection between OBS and portfolio selection,
bridging Bayesian ensemble learning with a rich, well-studied theoretical
framework that offers efficient algorithms and extensive regret analysis. We
further clarify the relationship between OBS and online BMA, showing that they
optimize related but distinct cost functions. Through theoretical analysis and
empirical evaluation, we identify scenarios where OBS outperforms online BMA
and provide principled guidance on when practitioners should prefer one
approach over the other.

</details>


### [260] [Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima](https://arxiv.org/abs/2505.15643)
*Lan V. Truong*

Key words: 多臂老虎机, 最优臂识别, 固定置信度, 停止规则, 信息论下界

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究多臂老虎机问题中固定置信度设置下的最优臂识别，提出改进的停止规则以适应多最优臂情况。

Motivation: 现有Track-and-Stop算法在多最优臂情况下性能不足，需改进以实现实例最优性。

Method: 修改Track-and-Stop的停止规则，引入新信息论下界以考虑多最优臂。

Result: 新停止规则紧密匹配理论下界，实例最优。

Conclusion: 改进的停止规则在多最优臂情况下表现优异。

Abstract: We study the problem of best-arm identification in stochastic multi-armed
bandits under the fixed-confidence setting, with a particular focus on
instances that admit multiple optimal arms. While the Track-and-Stop algorithm
of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,
its performance in the presence of multiple optima has remained insufficiently
understood. In this work, we revisit the Track-and-Stop strategy and propose a
modified stopping rule that ensures instance-optimality even when the set of
optimal arms is not a singleton. Our analysis introduces a new
information-theoretic lower bound that explicitly accounts for multiple optimal
arms, and we demonstrate that our stopping rule tightly matches this bound.

</details>


### [261] [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
*Christian Walder,Deep Karkhanis*

Key words: 强化学习, PKPO, pass@k优化, 低方差估计器, 样本多样性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种名为PKPO的方法，通过优化pass@k性能来提升强化学习中的样本多样性和集体效用，而非仅关注单一样本的pass@1性能。该方法通过低方差无偏估计器实现，并在实验中验证了其有效性。

Motivation: 传统的强化学习算法在优化pass@1性能时，忽略了样本多样性和集体效用，导致采样能力利用不足。论文旨在通过优化pass@k性能来解决这一问题。

Method: 提出了PKPO方法，通过转换最终奖励直接优化pass@k性能，并设计了低方差无偏估计器及其梯度。允许在训练中动态调整k值，兼顾pass@1和pass@k性能。

Result: 实验中验证了PKPO方法在低方差和有效性方面的优势，能够解决更难的问题，并在挑战性任务中解锁学习潜力。

Conclusion: PKPO方法通过优化pass@k性能提升了样本多样性和集体效用，解决了传统方法的局限性，同时在实验中展现了显著的性能提升。

Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.

</details>


### [262] [Second-Order Convergence in Private Stochastic Non-Convex Optimization](https://arxiv.org/abs/2505.15647)
*Youming Tao,Zuyuan Zhang,Dongxiao Yu,Xiuzhen Cheng,Falko Dressler,Di Wang*

Key words: 差分隐私, 非凸优化, 二阶稳定点, 随机梯度下降, 分布式学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种新的差分隐私随机梯度下降框架，解决了现有方法在二阶稳定点识别中的收敛误差和隐私选择依赖性等问题。

Motivation: 现有差分隐私非凸优化方法存在收敛误差率高和依赖辅助隐私选择程序的局限性，影响了实用性。

Method: 采用高斯噪声注入和通用梯度预言机的干扰随机梯度下降框架，利用模型漂移距离判断鞍点逃逸。

Result: 新算法校正了先前工作中的收敛误差率，并在分布式学习中首次实现了对差分隐私二阶稳定点的正式保证。

Conclusion: 论文设计在实际数据实验中验证了其有效性，并揭示了高维分布式学习中隐私选择程序的负面影响。

Abstract: We investigate the problem of finding second-order stationary points (SOSP)
in differentially private (DP) stochastic non-convex optimization. Existing
methods suffer from two key limitations: (i) inaccurate convergence error rate
due to overlooking gradient variance in the saddle point escape analysis, and
(ii) dependence on auxiliary private model selection procedures for identifying
DP-SOSP, which can significantly impair utility, particularly in distributed
settings. To address these issues, we propose a generic perturbed stochastic
gradient descent (PSGD) framework built upon Gaussian noise injection and
general gradient oracles. A core innovation of our framework is using model
drift distance to determine whether PSGD escapes saddle points, ensuring
convergence to approximate local minima without relying on second-order
information or additional DP-SOSP identification. By leveraging the adaptive
DP-SPIDER estimator as a specific gradient oracle, we develop a new DP
algorithm that rectifies the convergence error rates reported in prior work. We
further extend this algorithm to distributed learning with arbitrarily
heterogeneous data, providing the first formal guarantees for finding DP-SOSP
in such settings. Our analysis also highlights the detrimental impacts of
private selection procedures in distributed learning under high-dimensional
models, underscoring the practical benefits of our design. Numerical
experiments on real-world datasets validate the efficacy of our approach.

</details>


### [263] [Learning Small Decision Trees with Few Outliers: A Parameterized Perspective](https://arxiv.org/abs/2505.15648)
*Harmender Gahlawat,Meirav Zehavi*

Key words: 决策树, 参数化复杂性, 核化, W[1]-难, 固定参数可解

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了决策树学习中的参数化复杂性，考察了在允许有限错误下的最小化决策树尺寸和深度的问题，并进行了理论分析。

Motivation: 研究决策树学习中参数化复杂性的动机是希望在容忍有限错误的情况下，构造更小或更浅的决策树，以提高效率和可解释性。

Method: 论文考察了两种问题（DTSO和DTDO），分别针对决策树的尺寸和深度进行优化，并结合参数化复杂性理论分析了其计算难度和固定参数可解性。

Result: 结果表明DTSO和DTDO在特定参数下为W[1]-难问题，但加入错误容忍参数t后变为固定参数可解，并提供了核化复杂性的正负结果。

Conclusion: 论文揭示了决策树学习问题的复杂性，并对不同参数下的算法设计提供了理论指导。

Abstract: Decision trees are a fundamental tool in machine learning for representing,
classifying, and generalizing data. It is desirable to construct ``small''
decision trees, by minimizing either the \textit{size} ($s$) or the
\textit{depth} $(d)$ of the \textit{decision tree} (\textsc{DT}). Recently, the
parameterized complexity of \textsc{Decision Tree Learning} has attracted a lot
of attention. We consider a generalization of \textsc{Decision Tree Learning}
where given a \textit{classification instance} $E$ and an integer $t$, the task
is to find a ``small'' \textsc{DT} that disagrees with $E$ in at most $t$
examples. We consider two problems: \textsc{DTSO} and \textsc{DTDO}, where the
goal is to construct a \textsc{DT} minimizing $s$ and $d$, respectively. We
first establish that both \textsc{DTSO} and \textsc{DTDO} are W[1]-hard when
parameterized by $s+\delta_{max}$ and $d+\delta_{max}$, respectively, where
$\delta_{max}$ is the maximum number of features in which two differently
labeled examples can differ. We complement this result by showing that these
problems become \textsc{FPT} if we include the parameter $t$. We also consider
the kernelization complexity of these problems and establish several positive
and negative results for both \textsc{DTSO} and \textsc{DTDO}.

</details>


### [264] [LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought](https://arxiv.org/abs/2505.15657)
*Cheng Yan,Felix Mohr,Tom Viering*

Key words: 学习曲线, 数据库, 模型选择, 超参数调优, 统计方法

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究发现学习曲线的不规则行为比之前认为的更常见，揭示了其对下游任务的挑战，并提出了LCDB 1.1作为未来研究的基准。

Motivation: 探索学习曲线的行为特征及其对模型选择和超参数调优的影响。

Method: 通过构建LCDB 1.1数据库，使用统计方法分析学习曲线的行为。

Result: 约14%的学习曲线表现出显著的不规则行为，某些学习器更容易产生这类行为。

Conclusion: 学习曲线的不规则行为对模型选择和拟合提出了新挑战，LCDB 1.1可作为研究基准。

Abstract: Sample-wise learning curves plot performance versus training set size. They
are useful for studying scaling laws and speeding up hyperparameter tuning and
model selection. Learning curves are often assumed to be well-behaved: monotone
(i.e. improving with more data) and convex. By constructing the Learning Curves
Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning
curves, we show that learning curves are less often well-behaved than
previously thought. Using statistically rigorous methods, we observe
significant ill-behavior in approximately 14% of the learning curves, almost
twice as much as in previous estimates. We also identify which learners are to
blame and show that specific learners are more ill-behaved than others.
Additionally, we demonstrate that different feature scalings rarely resolve
ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such
as learning curve fitting and model selection, and find it poses significant
challenges, underscoring the relevance and potential of LCDB 1.1 as a
challenging benchmark for future research.

</details>


### [265] [Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms](https://arxiv.org/abs/2505.15661)
*Sina Mohammad-Taheri,Matthew J. Colbrook,Simone Brugiapaglia*

Key words: 算法展开,贪心算法,稀疏恢复,可微性,神经网络

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种基于软排列矩阵的方法，将不可微的贪心稀疏恢复算法（如OMP和IHT）转化为可微版本，使其能够集成到神经网络中，并展示了其有效性和可控精度。

Motivation: 解决贪心稀疏恢复算法（如OMP和IHT）由于其非可微性而难以融入神经网络的挑战。

Method: 通过软排序（softsort）生成软排列矩阵，近似非可微的argsort算子，提出Soft-OMP和Soft-IHT作为OMP和IHT的可微替代，并构建OMP-和IHT-Net网络架构。

Result: 理论分析和数值实验表明，Soft-OMP和Soft-IHT能够有效近似OMP和IHT，且精度可控。

Conclusion: 该方法成功地将贪心稀疏恢复算法融入神经网络，并能够提取数据的潜在稀疏模式。

Abstract: Gradient-based learning imposes (deep) neural networks to be differentiable
at all steps. This includes model-based architectures constructed by unrolling
iterations of an iterative algorithm onto layers of a neural network, known as
algorithm unrolling. However, greedy sparse recovery algorithms depend on the
non-differentiable argsort operator, which hinders their integration into
neural networks. In this paper, we address this challenge in Orthogonal
Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular
representative algorithms in this class. We propose permutation-based variants
of these algorithms and approximate permutation matrices using "soft"
permutation matrices derived from softsort, a continuous relaxation of argsort.
We demonstrate -- both theoretically and numerically -- that Soft-OMP and
Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible
with neural network training, effectively approximate these algorithms with a
controllable degree of accuracy. This leads to the development of OMP- and
IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,
respectively. Finally, by choosing weights as "structure-aware" trainable
parameters, we connect our approach to structured sparse recovery and
demonstrate its ability to extract latent sparsity patterns from data.

</details>


### [266] [Graph Conditional Flow Matching for Relational Data Generation](https://arxiv.org/abs/2505.15668)
*Davide Scassola,Sebastiano Saccani,Luca Bortolussi*

Key words: 数据合成,关系数据,图神经网络,流程匹配,深度生成模型

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了一种针对多表关系数据的生成模型，通过图神经网络和流程匹配技术实现高效数据合成，解决了现有方法在复杂关系结构中的灵活性不足问题。

Motivation: 当前多表数据生成方法难以处理复杂关系和长程依赖，亟需一种更灵活、表达能力更强的技术。

Method: 提出基于流程匹配和图神经网络的深度生成模型，利用外键关系图生成整个关系数据库的内容。

Result: 在多个基准数据集上验证，表现出最优的合成数据保真度。

Conclusion: 该方法为复杂关系结构的数据合成提供了高效解决方案，性能优于现有技术。

Abstract: Data synthesis is gaining momentum as a privacy-enhancing technology. While
single-table tabular data generation has seen considerable progress, current
methods for multi-table data often lack the flexibility and expressiveness
needed to capture complex relational structures. In particular, they struggle
with long-range dependencies and complex foreign-key relationships, such as
tables with multiple parent tables or multiple types of links between the same
pair of tables. We propose a generative model for relational data that
generates the content of a relational dataset given the graph formed by the
foreign-key relationships. We do this by learning a deep generative model of
the content of the whole relational database by flow matching, where the neural
network trained to denoise records leverages a graph neural network to obtain
information from connected records. Our method is flexible, as it can support
relational datasets with complex structures, and expressive, as the generation
of each record can be influenced by any other record within the same connected
component. We evaluate our method on several benchmark datasets and show that
it achieves state-of-the-art performance in terms of synthetic data fidelity.

</details>


### [267] [A packing lemma for VCN${}_k$-dimension and learning high-dimensional data](https://arxiv.org/abs/2505.15688)
*Leonardo N. Coregliano,Maryanthe Malliaris*

Key words: 高维度PAC学习, 非分割非不可知学习, VCNk维度, Haussler打包性质

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文完成了高维度PAC学习理论中非分割、非不可知学习的表征，证明了其与高维度Haussler打包性质的联系，并最终推导出VCNk维度的有限性。

Motivation: 先前的工作在高维度PAC学习理论中留下了一个未解决问题，即非分割、非不可知高维度PAC学习的表征。

Method: 通过直接证明经典PAC学习性质与Haussler打包性质的关系，并将其推广到高维度情况。

Result: 证明了非分割非不可知高维度PAC学习性质与高维度Haussler打包性质的联系，进而得出VCNk维度的有限性。

Conclusion: 该研究完成了高维度PAC学习理论的完整表征，填补了先前工作的空白。

Abstract: Recently, the authors introduced the theory of high-arity PAC learning, which
is well-suited for learning graphs, hypergraphs and relational structures. In
the same initial work, the authors proved a high-arity analogue of the
Fundamental Theorem of Statistical Learning that almost completely
characterizes all notions of high-arity PAC learning in terms of a
combinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)
$k$-dimension, leaving as an open problem only the characterization of
non-partite, non-agnostic high-arity PAC learnability.
  In this work, we complete this characterization by proving that non-partite
non-agnostic high-arity PAC learnability implies a high-arity version of the
Haussler packing property, which in turn implies finiteness of
VCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC
learnability implies classic Haussler packing property, which in turn implies
finite Natarajan dimension and noticing that these direct proofs nicely lift to
high-arity.

</details>


### [268] [A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO](https://arxiv.org/abs/2505.15694)
*Xingyu Zhou,Yulian Wu,Francesco Orabona*

Key words: 离线对齐, 噪声标签, 隐私, 对抗性腐败, RLHF, DPO

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 研究了噪声标签在离线对齐中的影响，重点关注隐私与对抗性鲁棒性之间的相互作用。

Motivation: 探讨不同隐私-腐败场景下离线对齐的挑战，特别是噪声标签的影响。

Method: 使用线性模型假设，结合逻辑回归的参数估计框架，分析 RLHF 和 DPO 方法。

Result: 发现 LTC 场景比 CTL 更具挑战性，并推动了隐私或腐败单一场景的理论进展。

Conclusion: 离线对齐中噪声标签的处理对隐私和鲁棒性有重要影响，LTC 比 CTL 更难。

Abstract: In this paper, we theoretically investigate the effects of noisy labels in
offline alignment, with a focus on the interplay between privacy and robustness
against adversarial corruption. Specifically, under linear modeling
assumptions, we present a unified analysis covering both reinforcement learning
from human feedback (RLHF) and direct preference optimization (DPO) under
different privacy-corruption scenarios, such as Local differential
privacy-then-Corruption (LTC), where human preference labels are privatized
before being corrupted by an adversary, and Corruption-then-Local differential
privacy (CTL), where labels are corrupted before privacy protection. Our
analysis leverages a reduction framework that reduces the offline alignment
problem under linear modeling assumptions to parameter estimation in logistic
regression. This framework allows us to establish an interesting separation
result between LTC and CTL, demonstrating that LTC presents a greater challenge
than CTL in offline alignment, even under linear models. As important
by-products, our findings also advance the state-of-the-art theoretical results
in offline alignment under privacy-only or corruption-only scenarios.

</details>


### [269] [Privacy-Preserving Conformal Prediction Under Local Differential Privacy](https://arxiv.org/abs/2505.15721)
*Coby Penso,Bar Mahpud,Jacob Goldberger,Or Sheffet*

Key words: Conformal prediction, Local differential privacy, Randomized response, Privacy-preserving, Coverage guarantees

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了两种在局部差分隐私（LDP）下保护隐私的CP方法，确保在标签扰动的情况下仍能提供可靠的置信集。

Motivation: 解决隐私敏感场景中，不可信聚合器只能访问扰动标签的问题，确保CP的准确性。

Method: 方法一：用户不访问模型，而是提供输入特征和扰动标签（k-ary随机响应）；方法二：用户通过二分查找响应向评分添加噪声，保护数据和标签隐私。

Result: 在强随机化条件下仍能保持稳健的覆盖性，无需访问真实标签即可计算阈值。

Conclusion: 统一强局部隐私与预测不确定性控制，适用于医疗影像或大型语言模型查询等敏感场景。

Abstract: Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.

</details>


### [270] [Higher-order Structure Boosts Link Prediction on Temporal Graphs](https://arxiv.org/abs/2505.15746)
*Jingzhe Liu,Zhigang Hua,Yan Xie,Bingheng Li,Harry Shomer,Yu Song,Kaveh Hassani,Jiliang Tang*

Key words: 时序图神经网络, 高阶结构, 超图, 动态链路预测, 内存效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于超图表示的高阶结构时序图神经网络（HTGN），解决现有方法忽略高阶结构和效率瓶颈的问题，实验显示其性能和内存效率均有提升。

Motivation: 现有时序图神经网络（TGNNs）主要关注两两交互，忽略了对链路形成和演化至关重要的高阶结构，且效率低下。

Method: 引入超图表示，开发算法识别高阶结构，通过聚合边特征为超边表示降低内存消耗。

Result: HTGN在动态链路预测中性能优于现有方法，内存成本降低50%。

Conclusion: HTGN通过捕捉高阶结构和优化内存效率，显著提升了时序图建模能力。

Abstract: Temporal Graph Neural Networks (TGNNs) have gained growing attention for
modeling and predicting structures in temporal graphs. However, existing TGNNs
primarily focus on pairwise interactions while overlooking higher-order
structures that are integral to link formation and evolution in real-world
temporal graphs. Meanwhile, these models often suffer from efficiency
bottlenecks, further limiting their expressive power. To tackle these
challenges, we propose a Higher-order structure Temporal Graph Neural Network,
which incorporates hypergraph representations into temporal graph learning. In
particular, we develop an algorithm to identify the underlying higher-order
structures, enhancing the model's ability to capture the group interactions.
Furthermore, by aggregating multiple edge features into hyperedge
representations, HTGN effectively reduces memory cost during training. We
theoretically demonstrate the enhanced expressiveness of our approach and
validate its effectiveness and efficiency through extensive experiments on
various real-world temporal graphs. Experimental results show that HTGN
achieves superior performance on dynamic link prediction while reducing memory
costs by up to 50\% compared to existing methods.

</details>


### [271] [Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs](https://arxiv.org/abs/2505.15747)
*Kanan Kiguchi,Yunhao Tu,Katsuhiro Ajito,Fady Alnajjar,Kazuyuki Murase*

Key words: 阿尔茨海默病、多模态数据、知识图谱、大语言模型、数据整合

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出一种新框架，利用大语言模型和知识图谱整合阿尔茨海默病研究中的多模态数据，无需患者ID匹配，揭示新关联并通过验证确认稳健性。

Motivation: 传统多模态分析需要跨数据集的患者ID匹配，限制了数据整合的潜力。本文旨在通过新框架解决这一问题，实现对独立队列数据的无匹配整合。

Method: 使用统计分析方法识别各模态的显著特征，构建知识图谱，并利用大语言模型分析图谱以提取潜在关联并生成自然语言假设。

Result: 揭示了多个新关联（如代谢风险因子与tau蛋白异常的潜在路径），并通过独立数据集验证了主要发现的稳健性和一致性。

Conclusion: 该框架无需患者ID匹配即可实现概念层面的跨模态整合，为利用碎片化数据研究AD病理提供了新可能性。

Abstract: We propose a novel framework for integrating fragmented multi-modal data in
Alzheimer's disease (AD) research using large language models (LLMs) and
knowledge graphs. While traditional multimodal analysis requires matched
patient IDs across datasets, our approach demonstrates population-level
integration of MRI, gene expression, biomarkers, EEG, and clinical indicators
from independent cohorts. Statistical analysis identified significant features
in each modality, which were connected as nodes in a knowledge graph. LLMs then
analyzed the graph to extract potential correlations and generate hypotheses in
natural language. This approach revealed several novel relationships, including
a potential pathway linking metabolic risk factors to tau protein abnormalities
via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between
frontal EEG channels and specific gene expression profiles (r=0.42-0.58,
p<0.01). Cross-validation with independent datasets confirmed the robustness of
major findings, with consistent effect sizes across cohorts (variance <15%).
The reproducibility of these findings was further supported by expert review
(Cohen's k=0.82) and computational validation. Our framework enables cross
modal integration at a conceptual level without requiring patient ID matching,
offering new possibilities for understanding AD pathology through fragmented
data reuse and generating testable hypotheses for future research.

</details>


### [272] [Improving planning and MBRL with temporally-extended actions](https://arxiv.org/abs/2505.15754)
*Palash Chatterjee,Roni Khardon*

Key words: 连续时间系统,强化学习,时间扩展动作,多臂老虎机,规划效率

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种通过直接控制连续决策时间尺度的方法，优化动作持续时间，以提升规划效率和模型性能。

Motivation: 传统离散时间模拟需要小步长以保持精度，导致计算量大和性能下降，本文旨在解决这一问题。

Method: 采用时间扩展动作，将动作持续时间作为优化变量，结合多臂老虎机自动选择持续时间范围。

Result: 实验显示该方法能加速规划、提高解决方案质量，并解决标准方法无法处理的问题。

Conclusion: 提出的方法在规划和基于模型的强化学习中表现优越，具有实际应用价值。

Abstract: Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.

</details>


### [273] [Projection-Based Correction for Enhancing Deep Inverse Networks](https://arxiv.org/abs/2505.15777)
*Jorge Bacca*

Key words: 深度学习、反问题、投影修正、物理约束

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种基于投影的修正方法，通过确保与正向模型的一致性来增强深度逆网络的推理能力。

Motivation: 现有深度学习模型在解决不适定反问题时未严格遵循物理约束，改进方案旨在增强解的空间一致性。

Method: 利用投影步骤约束解空间，确保其位于反问题的有效解空间中，理论证明修正后可分解为分量空间。

Result: 广泛模拟和实验验证了该方法在不同反问题和网络架构下的重建精度提升。

Conclusion: 投影修正方法能有效提升深度逆网络的性能，确保解与物理约束的一致性。

Abstract: Deep learning-based models have demonstrated remarkable success in solving
illposed inverse problems; however, many fail to strictly adhere to the
physical constraints imposed by the measurement process. In this work, we
introduce a projection-based correction method to enhance the inference of deep
inverse networks by ensuring consistency with the forward model. Specifically,
given an initial estimate from a learned reconstruction network, we apply a
projection step that constrains the solution to lie within the valid solution
space of the inverse problem. We theoretically demonstrate that if the recovery
model is a well-trained deep inverse network, the solution can be decomposed
into range-space and null-space components, where the projection-based
correction reduces to an identity transformation. Extensive simulations and
experiments validate the proposed method, demonstrating improved reconstruction
accuracy across diverse inverse problems and deep network architectures.

</details>


### [274] [Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning](https://arxiv.org/abs/2505.15782)
*Pedro P. Santos,Alberto Sardinha,Francisco S. Melo*

Key words: 通用效用马尔可夫决策过程, 单次试验机制, 蒙特卡洛树搜索, 策略优化

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了首个解决单次试验机制下无限时域折扣通用效用马尔可夫决策过程（GUMDPs）的方法，包括策略优化的理论分析、在线规划技术的应用及实验验证。

Motivation: 解决单次试验机制下的GUMDPs问题，填补研究空白，并在实际中验证方法的有效性。

Method: 结合策略优化的理论分析和蒙特卡洛树搜索算法，提出一种新的求解方法。

Result: 实验结果表明该方法优于相关基线方法。

Conclusion: 该方法在单次试验机制下表现优越，为GUMDPs提供了有效的解决方案。

Abstract: In this work, we contribute the first approach to solve infinite-horizon
discounted general-utility Markov decision processes (GUMDPs) in the
single-trial regime, i.e., when the agent's performance is evaluated based on a
single trajectory. First, we provide some fundamental results regarding policy
optimization in the single-trial regime, investigating which class of policies
suffices for optimality, casting our problem as a particular MDP that is
equivalent to our original problem, as well as studying the computational
hardness of policy optimization in the single-trial regime. Second, we show how
we can leverage online planning techniques, in particular a Monte-Carlo tree
search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide
experimental results showcasing the superior performance of our approach in
comparison to relevant baselines.

</details>


### [275] [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/abs/2505.15784)
*Jun Wan,Lingrui Mei*

Key words: 大型语言模型, 算法信息论, 所罗门诺夫先验, 少样本学习, 上下文学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文通过算法信息论（AIT）首次建立了大型语言模型（LLM）架构与理论之间的正式联系，解释了其训练过程与预测行为，并提出了一种基于预测信心的少样本选择方法。

Motivation: 当前关于LLM的理论框架碎片化，缺乏统一的数学解释。研究旨在填补这一空白，通过AIT提供一个统一的理论基础。

Method: 证明两个核心理论结果：(1) 训练过程通过损失最小化近似所罗门诺夫先验；(2) 下一词预测实现近似所罗门诺夫归纳。并基于预测信心设计少样本选择方法。

Result: 理论解释了上下文学习、少样本学习和扩展规律，实验验证了低预测信心样本选择策略对小型模型的性能提升作用。

Conclusion: 该框架为LLM的理论与实践提供了桥接，兼具解释性和实用性，为未来模型开发提供了指导。

Abstract: The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.

</details>


### [276] [Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates](https://arxiv.org/abs/2505.15788)
*Zahra Khatti,Daniel P. Robinson,Frank E. Curtis*

Key words: 公平机器学习, 非凸替代, 硬约束, 优化问题

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种新的公平监督机器学习策略，通过非凸平滑替代和硬约束来解决现有方法中的不公平性和优化难题。

Motivation: 现有公平监督学习方法中，不连续的公平性度量难以处理，且基于正则化的方法优化困难且参数调优成本高。

Method: 引入新的平滑非凸替代函数逼近Heaviside函数，并使用硬约束而非正则化来实现公平性容忍度。

Result: 策略能够有效保证模型公平性，并支持同时处理多个（可能冲突的）不公平性度量，且优化问题易于求解。

Conclusion: 新策略在公平性和计算效率上优于现有方法，适用于复杂场景。

Abstract: A new strategy for fair supervised machine learning is proposed. The main
advantages of the proposed strategy as compared to others in the literature are
as follows. (a) We introduce a new smooth nonconvex surrogate to approximate
the Heaviside functions involved in discontinuous unfairness measures. The
surrogate is based on smoothing methods from the optimization literature, and
is new for the fair supervised learning literature. The surrogate is a tight
approximation which ensures the trained prediction models are fair, as opposed
to other (e.g., convex) surrogates that can fail to lead to a fair prediction
model in practice. (b) Rather than rely on regularizers (that lead to
optimization problems that are difficult to solve) and corresponding
regularization parameters (that can be expensive to tune), we propose a
strategy that employs hard constraints so that specific tolerances for
unfairness can be enforced without the complications associated with the use of
regularization. (c)~Our proposed strategy readily allows for constraints on
multiple (potentially conflicting) unfairness measures at the same time.
Multiple measures can be considered with a regularization approach, but at the
cost of having even more difficult optimization problems to solve and further
expense for tuning. By contrast, through hard constraints, our strategy leads
to optimization models that can be solved tractably with minimal tuning.

</details>


### [277] [Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning](https://arxiv.org/abs/2505.15798)
*Taehoon Kim,Henry Gouk,Minyoung Kim,Timothy Hospedales*

Key words: 深度学习, 泛化能力, 模型融合, 高风险应用, 小样本学习

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文提出了通过模型融合方法而非微调来显著提升深度学习模型在少样本任务中的泛化能力，并实现了非平凡的泛化保证。

Motivation: 在医学、安全等高风险领域，验证深度学习模型的独立同分布（IID）泛化能力至关重要，但传统方法难以在大模型小数据场景下提供非空泛的保证。

Method: 通过模型融合方法学习下游任务，而非直接微调基础网络，从而缩小泛化差距并使其独立于基础网络规模。

Result: 首次实现了在仅100个样本下，对VIT-B和mistral-7B等模型的非平凡泛化保证。

Conclusion: 该方法不仅为现有系统的可信认证提供了可能，还为理论与实践的结合开辟了新方向。

Abstract: Certifying the IID generalisation ability of deep networks is the first of
many requirements for trusting AI in high-stakes applications from medicine to
security. However, when instantiating generalisation bounds for deep networks
it remains challenging to obtain non-vacuous guarantees, especially when
applying contemporary large models on the small scale data prevalent in such
high-stakes fields. In this paper, we draw a novel connection between a family
of learning methods based on model fusion and generalisation certificates, and
surprisingly show that with minor adjustment several existing learning
strategies already provide non-trivial generalisation guarantees. Essentially,
by focusing on data-driven learning of downstream tasks by fusion rather than
fine-tuning, the certified generalisation gap becomes tiny and independent of
the base network size, facilitating its certification. Our results show for the
first time non-trivial generalisation guarantees for learning with as low as
100 examples, while using vision models such as VIT-B and language models such
as mistral-7B. This observation is significant as it has immediate implications
for facilitating the certification of existing systems as trustworthy, and
opens up new directions for research at the intersection of practice and
theory.

</details>


### [278] [A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation](https://arxiv.org/abs/2505.15802)
*Sarah E. Wessinger,Leslie N. Smith,Jacob Gull,Jonathan Gehman,Zachary Beever,Andrew J. Kammerer*

Key words: 深度神经网络，模式传播因子，海洋大气边界层，多频率，抛物线方程

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文提出了一种基于深度神经网络的海洋大气边界层中多频率折射环境估计方法，替代了传统计算成本高的抛物线方程模拟。

Motivation: 准确估计海洋大气边界层中多频率的折射环境对雷达技术部署至关重要，但传统方法计算成本高、耗时长。

Method: 采用图像到图像转换生成器，输入修正折射率数据，生成预测模式传播因子。

Result: 深度学习网络可以分析多频率并合理预测模式传播因子，提供了传统方法的替代方案。

Conclusion: 深度神经网络在预测模式传播因子方面具有潜力，可作为传统方法的有效补充。

Abstract: Accurately estimating the refractive environment over multiple frequencies
within the marine atmospheric boundary layer is crucial for the effective
deployment of radar technologies. Traditional parabolic equation simulations,
while effective, can be computationally expensive and time-intensive, limiting
their practical application. This communication explores a novel approach using
deep neural networks to estimate the pattern propagation factor, a critical
parameter for characterizing environmental impacts on signal propagation.
Image-to-image translation generators designed to ingest modified refractivity
data and generate predictions of pattern propagation factors over the same
domain were developed. Findings demonstrate that deep neural networks can be
trained to analyze multiple frequencies and reasonably predict the pattern
propagation factor, offering an alternative to traditional methods.

</details>


### [279] [Adaptive Estimation and Learning under Temporal Distribution Shift](https://arxiv.org/abs/2505.15803)
*Dheeraj Baby,Yifei Tang,Hieu Duy Nguyen,Yu-Xiang Wang,Rohit Pyati*

Key words: 时间分布漂移, 小波软阈值, 非平稳性, 稀疏性, 最优估计

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 该论文研究了时间分布漂移下的估计和学习问题，提出了一种最优的小波软阈值估计器，并通过数值实验和理论验证了其有效性。

Motivation: 在观测序列随时间变化且存在噪声的情况下，如何在无先验知识的情况下准确估计最终时间步的真实值。

Method: 使用小波软阈值估计器，将序列的非平稳性与小波变换域的稀疏性联系起来。

Result: 该方法提供了最优的估计误差界，并应用于分布漂移下的分类风险界和高效训练目标的开发。

Conclusion: 小波软阈值估计器在无先验知识的情况下表现最优，并揭示了与经典信号处理问题的联系。

Abstract: In this paper, we study the problem of estimation and learning under temporal
distribution shift. Consider an observation sequence of length $n$, which is a
noisy realization of a time-varying groundtruth sequence. Our focus is to
develop methods to estimate the groundtruth at the final time-step while
providing sharp point-wise estimation error rates. We show that, without prior
knowledge on the level of temporal shift, a wavelet soft-thresholding estimator
provides an optimal estimation error bound for the groundtruth. Our proposed
estimation method generalizes existing researches Mazzetto and Upfal (2023) by
establishing a connection between the sequence's non-stationarity level and the
sparsity in the wavelet-transformed domain. Our theoretical findings are
validated by numerical experiments. Additionally, we applied the estimator to
derive sparsity-aware excess risk bounds for binary classification under
distribution shift and to develop computationally efficient training
objectives. As a final contribution, we draw parallels between our results and
the classical signal processing problem of total-variation denoising (Mammen
and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms
for such task.

</details>


### [280] [Neural Conditional Transport Maps](https://arxiv.org/abs/2505.15808)
*Carlos Rodriguez-Pardo,Leonardo Chiani,Emanuele Borgonovo,Massimo Tavoni*

Key words: 条件最优传输, 超网络, 全局敏感性分析, 生成建模, 模型可解释性

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 提出了一种学习条件最优传输映射的神经框架，支持处理分类和连续条件变量，通过超网络生成传输层参数，性能优于基线方法。

Motivation: 扩展最优传输理论的应用，特别是在复杂高维领域，如生成建模和黑盒模型可解释性。

Method: 采用超网络生成基于条件变量的传输层参数，实现自适应映射。

Result: 通过消融研究验证了方法的优越性，并在全局敏感性分析中展示了高性能。

Conclusion: 该方法推动了条件最优传输的前沿，拓宽了最优传输原则的应用范围。

Abstract: We present a neural framework for learning conditional optimal transport (OT)
maps between probability distributions. Our approach introduces a conditioning
mechanism capable of processing both categorical and continuous conditioning
variables simultaneously. At the core of our method lies a hypernetwork that
generates transport layer parameters based on these inputs, creating adaptive
mappings that outperform simpler conditioning methods. Comprehensive ablation
studies demonstrate the superior performance of our method over baseline
configurations. Furthermore, we showcase an application to global sensitivity
analysis, offering high performance in computing OT-based sensitivity indices.
This work advances the state-of-the-art in conditional optimal transport,
enabling broader application of optimal transport principles to complex,
high-dimensional domains such as generative modeling and black-box model
explainability.

</details>


### [281] [On the creation of narrow AI: hierarchy and nonlocality of neural network skills](https://arxiv.org/abs/2505.15811)
*Eric J. Michaud,Asher Parker-Sartori,Max Tegmark*

Key words: 窄AI系统, 神经网络, 剪枝, 技能转移, 数据分布

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: 论文探讨了如何创建高效且安全的窄AI系统，研究了从零训练窄模型的条件和从大模型转移特定技能到小模型的方法。

Motivation: 研究窄AI系统的价值在于提高效率和安全性，尤其是在训练大型通用模型的背景下。

Method: 通过实验分析了窄模型从零训练的可行性，并研究了通过剪枝方法从大模型转移技能到小模型的技术。

Result: 发现训练窄模型时有时需要广泛数据分布；剪枝方法在技能转移中表现优于蒸馏。

Conclusion: 窄AI系统的创建需解决技能依赖性和技能局部化的挑战，剪枝和正则化是有效方法。

Abstract: We study the problem of creating strong, yet narrow, AI systems. While recent
AI progress has been driven by the training of large general-purpose foundation
models, the creation of smaller models specialized for narrow domains could be
valuable for both efficiency and safety. In this work, we explore two
challenges involved in creating such systems, having to do with basic
properties of how neural networks learn and structure their representations.
The first challenge regards when it is possible to train narrow models from
scratch. Through experiments on a synthetic task, we find that it is sometimes
necessary to train networks on a wide distribution of data to learn certain
narrow skills within that distribution. This effect arises when skills depend
on each other hierarchically, and training on a broad distribution introduces a
curriculum which substantially accelerates learning. The second challenge
regards how to transfer particular skills from large general models into small
specialized models. We find that model skills are often not perfectly localized
to a particular set of prunable components. However, we find that methods based
on pruning can still outperform distillation. We investigate the use of a
regularization objective to align desired skills with prunable components while
unlearning unnecessary skills.

</details>


### [282] [Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex](https://arxiv.org/abs/2505.15813)
*Muquan Yu,Mu Nan,Hossein Adeli,Jacob S. Prince,John A. Pyles,Leila Wehbe,Margaret M. Henderson,Michael J. Tarr,Andrew F. Luo*

Key words: 计算神经科学、视觉皮层、上下文学习、transformer、fMRI

<details>
  <summary>Details</summary>

Main category: cs.LG

TL;DR: BraInCoRL使用上下文学习从小样本中预测神经反应，无需对新受试者和刺激进行微调，显著优于现有方法，并提高了神经信号的可解释性。

Motivation: 解决传统方法依赖大规模fMRI数据集的限制，提升模型在新受试者和刺激上的泛化能力。

Method: 采用transformer架构，结合图像特征和voxel激活进行联合优化，支持上下文学习。

Result: 在低数据条件下表现优越，适用于新图像和新fMRI数据集，并能映射自然语言查询到voxel选择性。

Conclusion: BraInCoRL是一种高效且可解释的视觉皮层编码方法，适用于低数据场景。

Abstract: Understanding functional representations within higher visual cortex is a
fundamental question in computational neuroscience. While artificial neural
networks pretrained on large-scale datasets exhibit striking representational
alignment with human neural responses, learning image-computable models of
visual cortex relies on individual-level, large-scale fMRI datasets. The
necessity for expensive, time-intensive, and often impractical data acquisition
limits the generalizability of encoders to new subjects and stimuli. BraInCoRL
uses in-context learning to predict voxelwise neural responses from few-shot
examples without any additional finetuning for novel subjects and stimuli. We
leverage a transformer architecture that can flexibly condition on a variable
number of in-context image stimuli, learning an inductive bias over multiple
subjects. During training, we explicitly optimize the model for in-context
learning. By jointly conditioning on image features and voxel activations, our
model learns to directly generate better performing voxelwise models of higher
visual cortex. We demonstrate that BraInCoRL consistently outperforms existing
voxelwise encoder designs in a low-data regime when evaluated on entirely novel
images, while also exhibiting strong test-time scaling behavior. The model also
generalizes to an entirely new visual fMRI dataset, which uses different
subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates
better interpretability of neural signals in higher visual cortex by attending
to semantically relevant stimuli. Finally, we show that our framework enables
interpretable mappings from natural language queries to voxel selectivity.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [283] [Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies](https://arxiv.org/abs/2505.14689)
*Ashwani Anand,Satya Prakash Nayak,Ritam Raha,Anne-Kathrin Schmuck*

Key words: 动态屏蔽, ω-正则属性, 策略模板, 运行时适应, 网络物理系统

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种动态后屏蔽框架（STARs），用于在预计算概率策略上强制执行ω-正则正确性属性，从单纯的安全性屏蔽转向同时确保活性的屏蔽。

Motivation: 研究动机是将屏蔽过程从仅确保安全性扩展到同时强制执行活性，以满足更全面的正确性需求，适用于需要动态调整的复杂系统（如网络物理系统）。

Method: 方法采用了Strategy-Template-based Adaptive Runtime Shields（STARs），通过策略模板实现最小干扰的后屏蔽，并动态控制干扰以平衡形式化义务和任务行为。

Result: 结果表明，STARs能够在移动机器人基准测试中动态控制干扰，并适应变化的规范或执行器故障，验证了其有效性和实用性。

Conclusion: STARs提供了一种灵活且强大的动态屏蔽框架，适用于需要严格形式化保证和运行时适应的应用场景。

Abstract: This paper presents a novel dynamic post-shielding framework that enforces
the full class of $\omega$-regular correctness properties over pre-computed
probabilistic policies. This constitutes a paradigm shift from the predominant
setting of safety-shielding -- i.e., ensuring that nothing bad ever happens --
to a shielding process that additionally enforces liveness -- i.e., ensures
that something good eventually happens. At the core, our method uses
Strategy-Template-based Adaptive Runtime Shields (STARs), which leverage
permissive strategy templates to enable post-shielding with minimal
interference. As its main feature, STARs introduce a mechanism to dynamically
control interference, allowing a tunable enforcement parameter to balance
formal obligations and task-specific behavior at runtime. This allows to
trigger more aggressive enforcement when needed, while allowing for optimized
policy choices otherwise. In addition, STARs support runtime adaptation to
changing specifications or actuator failures, making them especially suited for
cyber-physical applications. We evaluate STARs on a mobile robot benchmark to
demonstrate their controllable interference when enforcing (incrementally
updated) $\omega$-regular correctness properties over learned probabilistic
policies.

</details>


### [284] [R&D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution](https://arxiv.org/abs/2505.14738)
*Xu Yang,Xiao Yang,Shikai Fang,Bowen Xian,Yuante Li,Jian Wang,Minrui Xu,Haoran Pan,Xinpeng Hong,Weiqing Liu,Yelong Shen,Weizhu Chen,Jiang Bian*

Key words: AI, machine learning, data science, automation, R&D-Agent, iterative exploration

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: R&D-Agent is a dual-agent framework designed to automate high-level data science tasks by combining iterative idea generation and code refinement, achieving top performance in machine learning engineering.

Motivation: The complexity and expertise required for advanced data science tasks hinder progress, despite AI and ML advancements. Crowdsourcing platforms are insufficient for labor-intensive tasks.

Method: A dual-agent framework: Researcher generates ideas using performance feedback; Developer refines code based on error feedback, enabling parallel exploration traces.

Result: R&D-Agent outperforms others on MLE-Bench, showing potential to accelerate innovation and improve precision in data science applications.

Conclusion: R&D-Agent effectively bridges the gap between automated solutions and expert-level performance, with open-source availability on GitHub.

Abstract: Recent advances in AI and ML have transformed data science, yet increasing
complexity and expertise requirements continue to hinder progress. While
crowdsourcing platforms alleviate some challenges, high-level data science
tasks remain labor-intensive and iterative. To overcome these limitations, we
introduce R&D-Agent, a dual-agent framework for iterative exploration. The
Researcher agent uses performance feedback to generate ideas, while the
Developer agent refines code based on error feedback. By enabling multiple
parallel exploration traces that merge and enhance one another, R&D-Agent
narrows the gap between automated solutions and expert-level performance.
Evaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine
learning engineering agent, demonstrating its potential to accelerate
innovation and improve precision across diverse data science applications. We
have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.

</details>


### [285] [FOL-Pretrain: A complexity annotated corpus of first-order logic](https://arxiv.org/abs/2505.14932)
*Isabelle Lee,Sarah Liaw,Dani Yogatama*

Key words: 大语言模型, 一阶逻辑推理, 数据集, 算法推理, 符号推理

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文介绍了一个大规模、完全开放且复杂度标注的一阶逻辑推理数据集，用于探究大语言模型（LLMs）的算法推理能力。

Motivation: 尽管大语言模型展现出了强大的推理能力，但其执行复杂算法的内部机制仍然不明。现有研究多局限于小规模或浅层次任务，无法深入理解模型如何学习和执行复杂的符号推理过程。

Method: 通过创建一个包含35亿标记的数据集，其中包括880万由LLM增强的人工标注示例和750万合成的自动定理求解器生成的可验证正确示例，来系统分析LLMs的算法推理行为。

Result: 数据集为研究LLMs如何学习和泛化符号推理过程提供了可扩展且可解释的工具，有助于更透明地探索现代模型的算法能力。

Conclusion: 这一研究为深入理解大语言模型的推理机制奠定了基础，并为未来针对其算法能力的透明研究指明了方向。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable
reasoning capabilities such as coding and solving mathematical problems to
commonsense inference. While these tasks vary in complexity, they all require
models to integrate and compute over structured information. Despite recent
efforts to reverse-engineer LLM behavior through controlled experiments, our
understanding of how these models internalize and execute complex algorithms
remains limited. Progress has largely been confined to small-scale studies or
shallow tasks such as basic arithmetic and grammatical pattern matching. One
barrier to deeper understanding is the nature of pretraining data -- vast,
heterogeneous, and often poorly annotated, making it difficult to isolate
mechanisms of reasoning. To bridge this gap, we introduce a large-scale, fully
open, complexity-annotated dataset of first-order logic reasoning traces,
designed to probe and analyze algorithmic reasoning in LLMs. The dataset
consists of 3.5 billion tokens, including 8.8 million LLM-augmented,
human-annotated examples and 7.5 million synthetically generated examples. Each
synthetic example is verifiably correct, produced by a custom automated theorem
solver, and accompanied by metadata tracing its algorithmic provenance. We aim
to provide a scalable, interpretable artifact for studying how LLMs learn and
generalize symbolic reasoning processes, paving the way for more transparent
and targeted investigations into the algorithmic capabilities of modern models.

</details>


### [286] [To Be or Not To Be: Vector ontologies as a truly formal ontological framework](https://arxiv.org/abs/2505.14940)
*Kaspar Rothenfusser*

Key words: 形式本体论、胡塞尔、向量空间、人机互操作、基础本体论

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 作者认为现有所谓的‘形式本体论’并未真正符合胡塞尔提出的形式本体论标准，并建议将其重新定位为基础本体论。同时，提出向量空间作为一种真正符合胡塞尔条件的形式本体论，可作为人机互操作的本体框架。

Motivation: 胡塞尔提出的‘形式本体论’常被误用，作者旨在澄清其真正含义，并探讨一种符合其标准的本体论形式，以解决人机互操作性问题。

Method: 作者通过分析胡塞尔的《逻辑研究》，指出现有‘形式本体论’违反‘先验有效性’和‘完全无内容的形式化’两个关键标准。随后，提出以向量空间为基础的真正形式本体论，并验证其表达能力。

Result: 向量空间本体论能够表达大多数基础本体论的概念化，且可能是现有信息系统中已使用的形式。

Conclusion: 向量空间本体论符合胡塞尔的标准，并具有成为人机互操作框架的潜力。

Abstract: Since Edmund Husserl coined the term "Formal Ontologies" in the early 20th
century, a field that identifies itself with this particular branch of sciences
has gained increasing attention. Many authors, and even Husserl himself have
developed what they claim to be formal ontologies. I argue that under close
inspection, none of these so claimed formal ontologies are truly formal in the
Husserlian sense. More concretely, I demonstrate that they violate the two most
important notions of formal ontology as developed in Husserl's Logical
Investigations, namely a priori validity independent of perception and
formalism as the total absence of content. I hence propose repositioning the
work previously understood as formal ontology as the foundational ontology it
really is. This is to recognize the potential of a truly formal ontology in the
Husserlian sense. Specifically, I argue that formal ontology following his
conditions, allows us to formulate ontological structures, which could capture
what is more objectively without presupposing a particular framework arising
from perception. I further argue that the ability to design the formal
structure deliberately allows us to create highly scalable and interoperable
information artifacts. As concrete evidence, I showcase that a class of formal
ontology, which uses the axioms of vector spaces, is able to express most of
the conceptualizations found in foundational ontologies. Most importantly, I
argue that many information systems, specifically artificial intelligence, are
likely already using some type of vector ontologies to represent reality in
their internal worldviews and elaborate on the evidence that humans do as well.
I hence propose a thorough investigation of the ability of vector ontologies to
act as a human-machine interoperable ontological framework that allows us to
understand highly sophisticated machines and machines to understand us.

</details>


### [287] [Reinforcement Learning from User Feedback](https://arxiv.org/abs/2505.14946)
*Eric Han,Jun Chen,Karthik Abinav Sankararaman,Xiaoliang Peng,Tengyu Xu,Eryk Helenowski,Kaiyan Peng,Mrinal Kumar,Sinong Wang,Han Fang,Arya Talebzadeh*

Key words: 大语言模型、用户反馈、强化学习、对齐、P[Love]

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出RLUF框架，直接利用用户反馈信号对齐大语言模型（LLMs），通过训练预测模型P[Love]并优化策略，显著提升用户正面反馈率。

Motivation: 现有对齐方法依赖专家标注，难以反映真实用户偏好，RLUF旨在直接从用户生产环境的隐式反馈中学习。

Method: 训练奖励模型P[Love]预测用户正面反馈（如点赞），并将其与帮助性和安全性目标结合进行多目标策略优化。

Result: 实验显示P[Love]能预测用户行为，策略优化使正面反馈率显著提升（A/B测试中点赞增加28%）。

Conclusion: RLUF通过直接利用用户信号，为LLMs与真实用户偏好对齐提供了可行路径，但需注意奖励篡改问题。

Abstract: As large language models (LLMs) are increasingly deployed in diverse user
facing applications, aligning them with real user preferences becomes
essential. Existing methods like Reinforcement Learning from Human Feedback
(RLHF) rely on expert annotators trained on manually defined guidelines, whose
judgments may not reflect the priorities of everyday users. We introduce
Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs
directly to implicit signals from users in production. RLUF addresses key
challenges of user feedback: user feedback is often binary (e.g., emoji
reactions), sparse, and occasionally adversarial. We train a reward model,
P[Love], to predict the likelihood that an LLM response will receive a Love
Reaction, a lightweight form of positive user feedback, and integrate P[Love]
into a multi-objective policy optimization framework alongside helpfulness and
safety objectives. In large-scale experiments, we show that P[Love] is
predictive of increased positive feedback and serves as a reliable offline
evaluator of future user behavior. Policy optimization using P[Love]
significantly raises observed positive-feedback rates, including a 28% increase
in Love Reactions during live A/B tests. However, optimizing for positive
reactions introduces reward hacking challenges, requiring careful balancing of
objectives. By directly leveraging implicit signals from users, RLUF offers a
path to aligning LLMs with real-world user preferences at scale.

</details>


### [288] [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/abs/2505.14970)
*Xiaoyin Chen,Jiarui Lu,Minsu Kim,Dinghuai Zhang,Jian Tang,Alexandre Piché,Nicolas Gontier,Yoshua Bengio,Ehsan Kamalloo*

Key words: 强化学习, 大型语言模型, 课程学习, 多臂老虎机, 推理能力

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 本文提出了一种名为自我演化课程（SEC）的自动课程学习方法，通过学习课程策略来提升大型语言模型在推理任务中的表现，优于随机和手动设计的课程。

Motivation: 随机课程效果不佳，手动设计课程依赖启发式方法，而在线过滤方法计算成本高，亟需一种自动且高效的课程学习方法。

Method: SEC将课程选择建模为非平稳的多臂老虎机问题，利用策略梯度方法中的绝对优势作为学习增益的代理指标，并通过TD(0)方法更新课程策略。

Result: 实验表明，SEC在规划、归纳推理和数学领域显著提升了模型的推理能力，并更好地泛化到更难的测试问题上，同时实现了多推理领域的技能平衡。

Conclusion: SEC是一种有前景的RL微调策略，能有效提升LLM的推理能力。

Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.

</details>


### [289] [Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility](https://arxiv.org/abs/2505.14983)
*Zahra Zahedi,Shashank Mehrotra,Teruhisa Misu,Kumar Akash*

Key words: 自动驾驶车辆、动态贝叶斯网络、认知状态、因果推理、人机交互

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 提出了一种动态贝叶斯网络模型，用于推断自动驾驶车辆用户及其他道路使用者的认知状态，并将其整合到决策过程中，以提升交互效果。

Motivation: 为实现人机交互的高效与顺畅，需要设计能够分析并满足人类需求的自动驾驶系统。

Method: 采用动态贝叶斯网络（DBN）建模，推断用户和其他道路使用者的认知状态，并通过实验数据优化模型参数。进一步扩展为因果推理模型（CIM），用于平衡用户福祉、信任与系统运营成本。

Result: 模型能准确预测用户状态，并支持以人为本的自动驾驶决策。

Conclusion: 该模型有效地提升了自动驾驶车辆对人类需求的响应能力，实现了更优化的交互体验。

Abstract: For future human-autonomous vehicle (AV) interactions to be effective and
smooth, human-aware systems that analyze and align human needs with automation
decisions are essential. Achieving this requires systems that account for human
cognitive states. We present a novel computational model in the form of a
Dynamic Bayesian Network (DBN) that infers the cognitive states of both AV
users and other road users, integrating this information into the AV's
decision-making process. Specifically, our model captures the well-being of
both an AV user and an interacting road user as cognitive states alongside
trust. Our DBN models infer beliefs over the AV user's evolving well-being,
trust, and intention states, as well as the possible well-being of other road
users, based on observed interaction experiences. Using data collected from an
interaction study, we refine the model parameters and empirically assess its
performance. Finally, we extend our model into a causal inference model (CIM)
framework for AV decision-making, enabling the AV to enhance user well-being
and trust while balancing these factors with its own operational costs and the
well-being of interacting road users. Our evaluation demonstrates the model's
effectiveness in accurately predicting user's states and guiding informed,
human-centered AV decisions.

</details>


### [290] [HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning](https://arxiv.org/abs/2505.15011)
*Kryspin Varys,Federico Cerutti,Adam Sobey,Timothy J. Norman*

Key words: 价值对齐，强化学习，规范，声誉，社会规范

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文提出了一种将法律/安全规范与社会规范结合到强化学习中的方法，通过监测代理的行为并计算其声誉来促进价值对齐。实验验证了该方法在连续状态空间交通问题中的有效性。

Motivation: 当前的规范表示方法分散，法律规范明确而社会规范隐式，缺乏统一方法。因此，研究旨在整合两种规范以实现价值对齐的智能代理。

Method: 提出一种新方法，通过监测代理对规范的遵守情况并计算声誉值，将声誉作为奖励的权重，以激励代理实现价值对齐。

Result: 实验表明，该方法能有效结合两种规范，找到价值对齐的策略，且比单独使用任一组规范效果更好。

Conclusion: 整合法律和社会规范的方法能显著提升代理的价值对齐能力。

Abstract: Our society is governed by a set of norms which together bring about the
values we cherish such as safety, fairness or trustworthiness. The goal of
value-alignment is to create agents that not only do their tasks but through
their behaviours also promote these values. Many of the norms are written as
laws or rules (legal / safety norms) but even more remain unwritten (social
norms). Furthermore, the techniques used to represent these norms also differ.
Safety / legal norms are often represented explicitly, for example, in some
logical language while social norms are typically learned and remain hidden in
the parameter space of a neural network. There is a lack of approaches in the
literature that could combine these various norm representations into a single
algorithm. We propose a novel method that integrates these norms into the
reinforcement learning process. Our method monitors the agent's compliance with
the given norms and summarizes it in a quantity we call the agent's reputation.
This quantity is used to weigh the received rewards to motivate the agent to
become value-aligned. We carry out a series of experiments including a
continuous state space traffic problem to demonstrate the importance of the
written and unwritten norms and show how our method can find the value-aligned
policies. Furthermore, we carry out ablations to demonstrate why it is better
to combine these two groups of norms rather than using either separately.

</details>


### [291] [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
*Cheng Qian,Hongyi Du,Hongru Wang,Xiusi Chen,Yuji Zhang,Avirup Sil,Chengxiang Zhai,Kathleen McKeown,Heng Ji*

Key words: 大语言模型,数学建模,跨学科问题,智能体框架,评估系统

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文介绍了ModelingBench，一个面向真实世界问题的开放数学建模基准，以及ModelingAgent和ModelingJudge框架，用于评估和改进复杂建模任务。

Motivation: 现有数学问题评测基准未能反映真实世界问题的复杂性和开放需求，尤其是跨学科推理和工具整合能力。

Method: 提出ModelingBench基准，包含多元领域的问题；开发ModelingAgent多智能体框架，支持工具使用和工作流管理；引入ModelingJudge专家评估系统。

Result: ModelingAgent表现优于基线，生成的解决方案与人类专家水平相当。

Conclusion: 该研究为开放跨学科建模问题提供了全面评测框架。

Abstract: Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.

</details>


### [292] [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/abs/2505.15146)
*Lanxiang Hu,Mingjia Huo,Yuxuan Zhang,Haoyang Yu,Eric P. Xing,Ion Stoica,Tajana Rosing,Haojian Jin,Hao Zhang*

Key words: LLM, 视频游戏评估, lmgame-Bench, 感知, 记忆, 规划

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 论文研究了将大型语言模型（LLM）直接应用于视频游戏评估的挑战，并提出了lmgame-Bench工具，旨在解决这些问题。

Motivation: 视频游戏需要感知、记忆和规划能力，这与现代LLM的预期能力相匹配，但直接评估存在障碍。

Method: 提出了lmgame-Bench，包含多种游戏类型，并通过API和轻量级感知与记忆框架来稳定评估。

Result: 通过13个领先模型的测试，lmgame-Bench显示了挑战性和区分能力，并展示了能力迁移的有效性。

Conclusion: lmgame-Bench是一个可靠且多功能的评估工具，能够全面测试LLM的综合能力。

Abstract: Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.

</details>


### [293] [Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge](https://arxiv.org/abs/2505.15240)
*Yassir Fathullah,Mark J. F. Gales*

Key words: 概率建模,不确定性估计,LLM-as-a-judge,比较框架

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文探讨了在比较性LLM-as-a-judge框架中的广义概率建模和不确定性估计，提出改进方法以减少所需比较次数并提高性能。

Motivation: 研究旨在改进现有产品-专家方法，扩展其框架以支持多样化建模，并通过更精确的不确定性估计提高系统效率。

Method: 提出改进的个体比较不确定性估计方法，以及整体排名不确定性的估计方法，结合绝对和比较评分优化性能。

Result: 实验表明，改进的不确定性估计（尤其是重新排序概率）显著提高效率，减少比较次数约50%，且排名级别不确定性指标有助于识别低质量预测。

Conclusion: 广义概率建模框架扩展了现有方法，改进的不确定性估计显著提升系统效率和质量。

Abstract: This paper explores generalised probabilistic modelling and uncertainty
estimation in comparative LLM-as-a-judge frameworks. We show that existing
Product-of-Experts methods are specific cases of a broader framework, enabling
diverse modelling options. Furthermore, we propose improved uncertainty
estimates for individual comparisons, enabling more efficient selection and
achieving strong performance with fewer evaluations. We also introduce a method
for estimating overall ranking uncertainty. Finally, we demonstrate that
combining absolute and comparative scoring improves performance. Experiments
show that the specific expert model has a limited impact on final rankings but
our proposed uncertainty estimates, especially the probability of reordering,
significantly improve the efficiency of systems reducing the number of needed
comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used
to identify low-performing predictions, where the nature of the probabilistic
model has a notable impact on the quality of the overall uncertainty.

</details>


### [294] [Identification of Probabilities of Causation: A Complete Characterization](https://arxiv.org/abs/2505.15274)
*Xin Shu,Shuai Wang,Ang Li*

Key words: 因果概率、结构因果模型、多值处理、数学界限

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文解决了多值处理和结果的因果概率问题，提出了完整的代表性因果概率集，并为其推导了严格的界限。

Motivation: 多值处理和结果的因果概率在理论上的缺失限制了基于因果关系的决策范围，本文旨在填补这一基础性空白。

Method: 提出了一个完整的代表性因果概率集，并在结构因果模型框架内证明其充分性，通过数学证明为其推导严格界限。

Result: 成功定义和证明了多值处理的因果概率的界限，扩展了因果推理的应用范围。

Conclusion: 研究为多值因果概率提供了理论基础和实用工具，为更广泛的决策问题提供了支持。

Abstract: Probabilities of causation are fundamental to modern decision-making. Pearl
first introduced three binary probabilities of causation, and Tian and Pearl
later derived tight bounds for them using Balke's linear programming. The
theoretical characterization of probabilities of causation with multi-valued
treatments and outcomes has remained unresolved for decades, limiting the scope
of causality-based decision-making. In this paper, we resolve this foundational
gap by proposing a complete set of representative probabilities of causation
and proving that they are sufficient to characterize all possible probabilities
of causation within the framework of Structural Causal Models (SCMs). We then
formally derive tight bounds for these representative quantities using formal
mathematical proofs. Finally, we demonstrate the practical relevance of our
results through illustrative toy examples.

</details>


### [295] [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
*Rongzhi Zhu,Yi Liu,Zequn Sun,Yiwei Wang,Wei Hu*

Key words: 大型推理模型, 强化学习, 思考模式, 效率, 准确性

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 研究发现大型推理模型（LRM）在强化学习训练下存在三种思考模式，揭示了影响推理行为的关键因素，并指出需改进以提高效率。

Motivation: 探索LRM的内部机制，以减少因其过度思考带来的效率问题。

Method: 分析强化学习训练的LRM的思考终止信心、注意力转移及输入部分关注度。

Result: 无思考（NT）模式减少输出但牺牲准确性，显式（ET）和隐式（IT）思考模式保持准确性同时减少响应长度。

Conclusion: RL优化的LRM存在基本不一致性，需自适应改进以实现可靠效率。

Abstract: Large reasoning models (LRMs) have significantly advanced performance on
complex tasks, yet their tendency to overthink introduces inefficiencies. This
study investigates the internal mechanisms of reinforcement learning
(RL)-trained LRMs when prompted to save thinking, revealing three distinct
thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking
(IT). Through comprehensive analysis of confidence in thinking termination,
attention from thinking to generation, and attentional focus on input sections,
we uncover key factors influencing the reasoning behaviors. We further find
that NT reduces output length at the cost of accuracy, while ET and IT maintain
accuracy with reduced response length. Our findings expose fundamental
inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for
reliable efficiency.

</details>


### [296] [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Haodong Zhao,Hao Li,Jiansong Chen,Ke Zeng,Xunliang Cai*

Key words: 大型推理模型,自适应推理,冗余推理,内部自我恢复机制,ASRR

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了自适应自我恢复推理（ASRR）框架，通过抑制不必要的推理和隐式恢复，显著提高大型推理模型的效率，同时保持高性能。

Motivation: 大型推理模型（LRMs）在处理简单任务时存在冗余推理问题，导致计算开销过大。研究旨在量化LRM的性能上限，并探索其内部自我恢复机制。

Method: 提出ASRR框架，通过精度感知的长度奖励调节，自适应分配推理资源，实现高效推理。

Result: 实验表明，ASRR显著减少推理预算（最高32.5%），性能损失极小（最高1.2% pass@1），并显著提升安全性（最高+21.7%）。

Conclusion: ASRR框架为LRM提供高效、自适应且安全的推理能力，展示了其潜在应用价值。

Abstract: Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery
Mechanism" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.

</details>


### [297] [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
*Bahar Radmehr,Ekaterina Shved,Fatma Betül Güreş,Adish Singla,Tanja Käser*

Key words: 点击流数据, 学习策略, 大语言模型, 教育技术

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: ClickSight是一种基于大语言模型（LLM）的管道，用于从学生的点击流数据中解读学习策略。

Motivation: 传统方法依赖于手工特征或监督模型，缺乏通用性和可扩展性。ClickSight旨在利用LLM提供理论驱动的学生行为解读。

Method: ClickSight将原始点击流和学习策略列表作为输入，生成文本解读。研究评估了四种提示策略，并探讨了自优化对结果的影响。

Result: 评估显示，LLM能合理解读学习策略，但质量因提示策略而异，自优化改进有限。

Conclusion: ClickSight展示了LLM在教育交互数据中生成理论驱动见解的潜力。

Abstract: Clickstream data from digital learning environments offer valuable insights
into students' learning behaviors, but are challenging to interpret due to
their high dimensionality and granularity. Prior approaches have relied mainly
on handcrafted features, expert labeling, clustering, or supervised models,
therefore often lacking generalizability and scalability. In this work, we
introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline
that interprets student clickstreams to reveal their learning strategies.
ClickSight takes raw clickstreams and a list of learning strategies as input
and generates textual interpretations of students' behaviors during
interaction. We evaluate four different prompting strategies and investigate
the impact of self-refinement on interpretation quality. Our evaluation spans
two open-ended learning environments and uses a rubric-based domain-expert
evaluation. Results show that while LLMs can reasonably interpret learning
strategies from clickstreams, interpretation quality varies by prompting
strategy, and self-refinement offers limited improvement. ClickSight
demonstrates the potential of LLMs to generate theory-driven insights from
educational interaction data.

</details>


### [298] [Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives](https://arxiv.org/abs/2505.15693)
*Milad Kazemi,Mateo Perez,Fabio Somenzi,Sadegh Soudjani,Ashutosh Trivedi,Alvaro Velasquez*

Key words: 强化学习, 平均奖励, omega-正则语言, 绝对活跃性规范, 持续任务

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: 该论文提出了一种基于平均奖励的强化学习框架，用于在无限时间范围的持续任务中处理绝对活跃性规范，避免了传统基于折扣奖励的方法在持续环境中的不适用性。

Motivation: 手动设计奖励函数耗时且易出错，而omega-正则语言虽然在形式上能够描述无限行为轨迹，但传统方法依赖折扣奖励和周期性重置，与实际语义不符。因此，需要一种适应持续任务的模型。

Method: 提出了一种模型无关的强化学习框架，将绝对活跃性规范转换为平均奖励目标，支持在无重置的持续环境中学习，并引入字典序多目标优化的奖励结构。

Result: 实验结果表明，在基准测试中，平均奖励方法在持续环境中优于基于折扣的方法。

Conclusion: 该框架成功解决了在持续环境中应用omega-正则规范的挑战，为实际任务提供了更合适的解决方案。

Abstract: Recent advances in reinforcement learning (RL) have renewed focus on the
design of reward functions that shape agent behavior. Manually designing reward
functions is tedious and error-prone. A principled alternative is to specify
behaviors in a formal language that can be automatically translated into
rewards. Omega-regular languages are a natural choice for this purpose, given
their established role in formal verification and synthesis. However, existing
methods using omega-regular specifications typically rely on discounted reward
RL in episodic settings, with periodic resets. This setup misaligns with the
semantics of omega-regular specifications, which describe properties over
infinite behavior traces. In such cases, the average reward criterion and the
continuing setting -- where the agent interacts with the environment over a
single, uninterrupted lifetime -- are more appropriate.
  To address the challenges of infinite-horizon, continuing tasks, we focus on
absolute liveness specifications -- a subclass of omega-regular languages that
cannot be violated by any finite behavior prefix, making them well-suited to
the continuing setting. We present the first model-free RL framework that
translates absolute liveness specifications to average-reward objectives. Our
approach enables learning in communicating MDPs without episodic resetting. We
also introduce a reward structure for lexicographic multi-objective
optimization, aiming to maximize an external average-reward objective among the
policies that also maximize the satisfaction probability of a given
omega-regular specification. Our method guarantees convergence in unknown
communicating MDPs and supports on-the-fly reductions that do not require full
knowledge of the environment, thus enabling model-free RL. Empirical results
show our average-reward approach in continuing setting outperforms
discount-based methods across benchmarks.

</details>


### [299] [Neuro-Argumentative Learning with Case-Based Reasoning](https://arxiv.org/abs/2505.15742)
*Adam Gould,Francesca Toni*

Key words: 神经符号系统, 分类, 可解释性, 案例推理, 辩论结构

<details>
  <summary>Details</summary>

Main category: cs.AI

TL;DR: Gradual AA-CBR 是一种结合神经符号系统的分类模型，通过辩论结构实现多类分类，同时提升模型可解释性。

Motivation: 研究旨在开发一种既能利用神经网络的自动学习能力，又能提供类似符号系统的人类可理解推理过程的模型。

Method: 模型基于案例的辩论结构，通过梯度方法学习参数和关系，支持多类分类和不确定性评估。

Result: 性能与传统神经网络相当，并显著优于现有的符号推理方法。

Conclusion: Gradual AA-CBR 结合了神经网络的强大学习能力与符号推理的可解释性，是分类任务的有效方法。

Abstract: We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual
AA-CBR), a data-driven, neurosymbolic classification model in which the outcome
is determined by an argumentation debate structure that is learned
simultaneously with neural-based feature extractors. Each argument in the
debate is an observed case from the training data, favouring their labelling.
Cases attack or support those with opposing or agreeing labellings, with the
strength of each argument and relationship learned through gradient-based
methods. This argumentation debate structure provides human-aligned reasoning,
improving model interpretability compared to traditional neural networks (NNs).
Unlike the existing purely symbolic variant, Abstract Argumentation for
Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class
classification, automatic learning of feature and data point importance,
assigning uncertainty values to outcomes, using all available data points, and
does not require binary features. We show that Gradual AA-CBR performs
comparably to NNs whilst significantly outperforming existing AA-CBR
formulations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [300] [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/abs/2505.14723)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Key words: 语音语言理解，知识蒸馏，量化技术，高效计算

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: QUADS框架通过统一知识蒸馏与量化技术，优化资源受限环境下的语音语言理解（SLU）系统，显著提高效率且保持准确性。

Motivation: 现有方法独立应用知识蒸馏与量化，导致压缩效果不理想。QUADS旨在通过统一的优化框架解决此问题，适配低比特场景。

Method: QUADS通过多阶段训练，结合预调优模型，同时优化知识蒸馏与量化技术。

Result: QUADS在SLURP和FSC数据集上分别达到71.13%和99.20%的准确率，计算复杂度降低60-73倍，模型大小缩减83-700倍。

Conclusion: QUADS是一种高效、轻量化的SLU解决方案，适用于资源受限的真实场景，且表现出极强的量化鲁棒性。

Abstract: Spoken Language Understanding (SLU) systems must balance performance and
efficiency, particularly in resource-constrained environments. Existing methods
apply distillation and quantization separately, leading to suboptimal
compression as distillation ignores quantization constraints. We propose QUADS,
a unified framework that optimizes both through multi-stage training with a
pre-tuned model, enhancing adaptability to low-bit regimes while maintaining
accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with
only minor degradations of up to 5.56\% compared to state-of-the-art models.
Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and
model size by 83--700$\times$, demonstrating strong robustness under extreme
quantization. These results establish QUADS as a highly efficient solution for
real-world, resource-constrained SLU applications.

</details>


### [301] [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
*Yu Zhang,Wenxiang Guo,Changhao Pan,Dongyu Yao,Zhiyuan Zhu,Ziyue Jiang,Yuhan Wang,Tao Jin,Zhou Zhao*

Key words: 歌声合成, 零样本学习, 多任务学习, 风格迁移

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: TCSinger 2是一个多任务多语言零样本歌声合成模型，通过风格迁移和多样化提示的样式控制解决了现有模型的局限性。

Motivation: 现有歌声合成模型过度依赖音素和音符边界标注，限制了零样本场景的鲁棒性，且缺乏有效的多级样式控制。

Method: 1) BBC Encoder实现平滑过渡；2) Custom Audio Encoder通过对比学习对齐表示；3) Flow-based Custom Transformer提升合成质量和样式建模。

Result: 实验表明TCSinger 2在多任务中表现优于基线模型。

Conclusion: TCSinger 2通过创新模块解决了现有问题，提升了歌声合成的质量和灵活性。

Abstract: Customizable multilingual zero-shot singing voice synthesis (SVS) has various
potential applications in music composition and short video dubbing. However,
existing SVS models overly depend on phoneme and note boundary annotations,
limiting their robustness in zero-shot scenarios and producing poor transitions
between phonemes and notes. Moreover, they also lack effective multi-level
style control via diverse prompts. To overcome these challenges, we introduce
TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer
and style control based on various prompts. TCSinger 2 mainly includes three
key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,
extends content embedding, and applies masking to the boundaries to enable
smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to
extract aligned representations from singing, speech, and textual prompts. 3)
Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,
enhancing both the synthesis quality and style modeling of the generated
singing voice. Experimental results show that TCSinger 2 outperforms baseline
models in both subjective and objective metrics across multiple related tasks.

</details>


### [302] [Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information](https://arxiv.org/abs/2505.15667)
*Nicholas Sanders,Yuanchao Li,Korin Richmond,Simon King*

Key words: SSL, HuBERT, 量化, 韵律信息, 副语言信息

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 提出Segmentation-Variant Codebooks（SVCs）方法，通过在不同语言单元上量化语音，显著提升了语音模型中韵律和副语言信息的保留效果。

Motivation: 研究表明，现有SSL语音模型（如HuBERT）的量化方法虽然改善了压缩和任务性能，但常丢失韵律和副语言信息。增大码本规模虽能部分缓解，但会低效地提高比特率。

Method: 提出Segmentation-Variant Codebooks（SVCs），在不同语言单元（帧、音素、单词、话语）上量化语音，将其分解为多个特定分段的离散特征流。

Result: 实验证明，SVCs在多种探测任务中显著更有效地保留了韵律和副语言信息。此外，在离散化前进行池化操作能更好地保留分段级信息。

Conclusion: SVCs方法在保留可懂度的同时，改进了风格表现和语音质量，验证了其在语音量化中的有效性。

Abstract: Quantization in SSL speech models (e.g., HuBERT) improves compression and
performance in tasks like language modeling, resynthesis, and text-to-speech
but often discards prosodic and paralinguistic information (e.g., emotion,
prominence). While increasing codebook size mitigates some loss, it
inefficiently raises bitrates. We propose Segmentation-Variant Codebooks
(SVCs), which quantize speech at distinct linguistic units (frame, phone, word,
utterance), factorizing it into multiple streams of segment-specific discrete
features. Our results show that SVCs are significantly more effective at
preserving prosodic and paralinguistic information across probing tasks.
Additionally, we find that pooling before rather than after discretization
better retains segment-level information. Resynthesis experiments further
confirm improved style realization and slightly improved quality while
preserving intelligibility.

</details>


### [303] [ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality](https://arxiv.org/abs/2505.15773)
*Yu-Xiang Luo,Yi-Cheng Lin,Ming-To Chuang,Jia-Hung Chen,I-Ning Tsai,Pei Xing Kiew,Yueh-Hsuan Huang,Chien-Feng Liu,Yu-Chen Chen,Bo-Han Feng,Wenze Ren,Hung-yi Lee*

Key words: 普通话语音毒性检测，多模态框架，ToxicTone数据集

<details>
  <summary>Details</summary>

Main category: eess.AS

TL;DR: 介绍了一个名为ToxicTone的普通话语音毒性数据集，并提出了一种多模态检测框架，显著提升了毒性检测性能。

Motivation: 填补普通话语音毒性检测的研究空白，捕捉独特的韵律线索和文化特定表达。

Method: 提出了一个多模态检测框架，结合声学、语言和情感特征，使用先进的语音和情感编码器。

Result: 该框架在实验中表现优于仅基于文本的基准模型，凸显了语音特定线索的重要性。

Conclusion: ToxicTone数据集和多模态框架为语音毒性检测提供了有效工具，揭示了隐藏的毒性表达。

Abstract: Despite extensive research on toxic speech detection in text, a critical gap
remains in handling spoken Mandarin audio. The lack of annotated datasets that
capture the unique prosodic cues and culturally specific expressions in
Mandarin leaves spoken toxicity underexplored. To address this, we introduce
ToxicTone -- the largest public dataset of its kind -- featuring detailed
annotations that distinguish both forms of toxicity (e.g., profanity, bullying)
and sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data,
sourced from diverse real-world audio and organized into 13 topical categories,
mirrors authentic communication scenarios. We also propose a multimodal
detection framework that integrates acoustic, linguistic, and emotional
features using state-of-the-art speech and emotion encoders. Extensive
experiments show our approach outperforms text-only and baseline models,
underscoring the essential role of speech-specific cues in revealing hidden
toxic expressions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [304] [A Simple Approximation Algorithm for Optimal Decision Tree](https://arxiv.org/abs/2505.15641)
*Zhengjia Zhuo,Viswanath Nagarajan*

Key words: 最优决策树, 近似算法, NP难问题, 广义设置

<details>
  <summary>Details</summary>

Main category: cs.DS

TL;DR: 本文提出了一种简单的算法来解决最优决策树（ODT）问题，证明了其近似比为8 ln m。

Motivation: ODT问题在主动学习、实体识别和医学诊断等应用中具有重要意义。现有算法和分析复杂且常数因子较大，需简化。

Method: 提出一种简单算法，适用于任意成本、概率和响应的广义设置。

Result: 算法实现了8 ln m的近似比，比现有复杂算法更简洁。

Conclusion: 该算法简化了ODT问题的解决方案，同时在理论上保持了良好的近似性能。

Abstract: Optimal decision tree (\odt) is a fundamental problem arising in applications
such as active learning, entity identification, and medical diagnosis. An
instance of \odt is given by $m$ hypotheses, out of which an unknown ``true''
hypothesis is drawn according to some probability distribution. An algorithm
needs to identify the true hypothesis by making queries: each query incurs a
cost and has a known response for each hypothesis. The goal is to minimize the
expected query cost to identify the true hypothesis. We consider the most
general setting with arbitrary costs, probabilities and responses. \odt is
NP-hard to approximate better than $\ln m$ and there are $O(\ln m)$
approximation algorithms known for it. However, these algorithms and/or their
analyses are quite complex. Moreover, the leading constant factors are large.
We provide a simple algorithm and analysis for \odt, proving an approximation
ratio of $8 \ln m$.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [305] [Beyond Pairwise Plasticity: Group-Level Spike Synchrony Facilitates Efficient Learning in Spiking Neural Networks](https://arxiv.org/abs/2505.14841)
*Yuchen Tian,Assel Kembay,Nhan Duy Truong,Jason K. Eshraghian,Omid Kavehei*

Key words: spiking neural networks, synaptic plasticity, neural synchrony, SSDP, event-driven computing

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 该论文提出了一种名为SSDP的学习规则，通过协调神经元之间的同步活动来调整突触权重，解决了传统SNNs学习规则在噪声和快速变化环境中的不稳定性问题。

Motivation: 受生物学中神经同步性在学习与记忆中的核心作用启发，作者旨在设计一种基于神经元群体活动的学习规则，以提高SNNs的稳定性和泛化能力。

Method: 引入了一种基于神经元同步放电的突触可塑性规则（SSDP），通过调整突触权重以鼓励神经元形成一致的活动模式。

Result: 实验表明，SSDP能够在训练过程中实现从不稳定到稳定的动态转变，并在多种网络类型中（包括SNN-Transformer）表现优异。

Conclusion: SSDP不仅符合生物学原理，且计算成本低，适合神经形态部署，为SNNs提供了一种通用的优化策略，并为脑内群体学习机制提供了新见解。

Abstract: Brain networks rely on precise spike timing and coordinated activity to
support robust and energy-efficient learning. Inspired by these principles,
spiking neural networks (SNNs) are widely regarded as promising candidates for
low-power, event-driven computing. However, most biologically-inspired learning
rules employed in SNNs, including spike-timing-dependent plasticity (STDP),
rely on isolated spike pairs and lack sensitivity to population-level activity.
This limits their stability and generalization, particularly in noisy and
fast-changing environments. Motivated by biological observations that neural
synchrony plays a central role in learning and memory, we introduce a
spike-synchrony-dependent plasticity (SSDP) rule that adjusts synaptic weights
based on the degree of coordinated firing among neurons. SSDP supports stable
and scalable learning by encouraging neurons to form coherent activity
patterns. One prominent outcome is a sudden transition from unstable to stable
dynamics during training, suggesting that synchrony may drive convergence
toward equilibrium firing regimes. We demonstrate SSDP's effectiveness across
multiple network types, from minimal-layer models to spiking ResNets and
SNN-Transformer. To our knowledge, this is the first application of a synaptic
plasticity mechanism in a spiking transformer. SSDP operates in a fully
event-driven manner and incurs minimal computational cost, making it
well-suited for neuromorphic deployment. In this approach, local synaptic
modifications are associated with the collective dynamics of neural networks,
resulting in a learning strategy that adheres to biological principles while
maintaining practical efficiency, these findings position SSDP as a
general-purpose optimization strategy for SNNs, while offering new insights
into population-based learning mechanisms in the brain.

</details>


### [306] [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741)
*Dikshit Chauhan,Bapi Dutta,Indu Bala,Niki van Stein,Thomas Bäck,Anupam Yadav*

Key words: 大型语言模型, 进化计算, 人工智能, 协同优化, 自动化设计

<details>
  <summary>Details</summary>

Main category: cs.NE

TL;DR: 论文探讨了大型语言模型（LLMs）与进化计算（EC）的协同潜力，展示了如何通过两者的结合推动人工智能的发展。

Motivation: 通过整合LLMs的自然语言理解能力和EC的优化与搜索能力，探索人工智能的新进展。

Method: 分析了EC如何优化LLMs的训练、微调、提示工程和架构搜索，以及LLMs如何自动化EC的设计、分析与解释。

Result: 展示了双向增强的实现路径，包括EC优化LLMs的组件和LLMs自动化EC的设计与调优，并讨论了新兴的共进化框架。

Conclusion: 论文总结了开放的研究问题，并提倡结合EC和LLMs优势的混合方法。

Abstract: Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)
represents a promising avenue for advancing artificial intelligence by
combining powerful natural language understanding with optimization and search
capabilities. This manuscript explores the synergistic potential of LLMs and
EC, reviewing their intersections, complementary strengths, and emerging
applications. We identify key opportunities where EC can enhance LLM training,
fine-tuning, prompt engineering, and architecture search, while LLMs can, in
turn, aid in automating the design, analysis, and interpretation of ECs. The
manuscript explores the synergistic integration of EC and LLMs, highlighting
their bidirectional contributions to advancing artificial intelligence. It
first examines how EC techniques enhance LLMs by optimizing key components such
as prompt engineering, hyperparameter tuning, and architecture search,
demonstrating how evolutionary methods automate and refine these processes.
Secondly, the survey investigates how LLMs improve EC by automating
metaheuristic design, tuning evolutionary algorithms, and generating adaptive
heuristics, thereby increasing efficiency and scalability. Emerging
co-evolutionary frameworks are discussed, showcasing applications across
diverse fields while acknowledging challenges like computational costs,
interpretability, and algorithmic convergence. The survey concludes by
identifying open research questions and advocating for hybrid approaches that
combine the strengths of EC and LLMs.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [307] [Global Description of Flutter Dynamics via Koopman Theory](https://arxiv.org/abs/2505.14697)
*Jiwoo Song,Daning Huang*

Key words: Koopman理论，气动弹性系统，EKBF模型，非线性动态，颤振分析

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

TL;DR: 本文提出了一种基于Koopman理论的新型气动弹性系统参数化方法，通过扩展KBF模型（EKBF）解决了KBF模型线性参数依赖的局限性，有效捕捉了非线性动态。

Motivation: 为了解决KBF模型在气动弹性系统中线性参数依赖的局限性，作者提出扩展KBF模型（EKBF），以更全局线性方式表示非线性动态。

Method: 引入EKBF模型，利用Koopman理论对气动弹性系统进行参数化，并通过2D学术案例和面板颤振问题验证其有效性。

Result: EKBF能有效插值和外推主特征值，捕捉颤振机制，并在数据受噪声干扰时准确预测颤振边界。同时，EKBF识别的参数化等稳和等时线提供了对非线性颤振系统的新见解。

Conclusion: EKBF模型成功扩展了KBF的优势，为非线性气动弹性系统的分析和预测提供了更强大的工具。

Abstract: This paper presents a novel parametrization approach for aeroelastic systems
utilizing Koopman theory, specifically leveraging the Koopman Bilinear Form
(KBF) model. To address the limitations of linear parametric dependence in the
KBF model, we introduce the Extended KBF (EKBF) model, which enables a global
linear representation of aeroelastic dynamics while capturing stronger
nonlinear dependence on, e.g., the flutter parameter. The effectiveness of the
proposed methodology is demonstrated through two case studies: a 2D academic
example and a panel flutter problem. Results show that EKBF effectively
interpolates and extrapolates principal eigenvalues, capturing flutter
mechanisms, and accurately predicting the flutter boundary even when the data
is corrupted by noise. Furthermore, parameterized isostable and isochron
identified by EKBF provides valuable insights into the nonlinear flutter
system.

</details>


### [308] [Towards scalable surrogate models based on Neural Fields for large scale aerodynamic simulations](https://arxiv.org/abs/2505.14704)
*Giovanni Catalani,Jean Fesquet,Xavier Bertrand,Frédéric Tost,Michael Bauerheim,Joseph Morlier*

Key words: 神经场, 替代建模, 空气动力学, 非参数几何, 高效形状编码

<details>
  <summary>Details</summary>

Main category: physics.flu-dyn

TL;DR: 本文提出了一种基于神经场的新型替代建模框架MARIO，用于空气动力学应用，通过高效形状编码和离散不变性实现高效建模，显著降低计算成本。

Motivation: 传统CFD求解器和现有替代方法在计算成本和内存需求上存在限制，需要一种更高效的方法来处理非参数几何变化和多样化的流动条件。

Method: 提出MARIO框架，利用神经场的离散不变性和高效形状编码机制，在低分辨率网格上训练并保持全分辨率推理的准确性。

Result: 在AirfRANS和NASA Common Research Model数据集上的验证显示，MARIO在预测精度和计算效率上优于现有方法，并能准确捕捉边界层现象和空气动力学系数。

Conclusion: 神经场替代模型能够在工业应用中提供快速准确的空气动力学预测，解决计算和数据限制问题。

Abstract: This paper introduces a novel surrogate modeling framework for aerodynamic
applications based on Neural Fields. The proposed approach, MARIO (Modulated
Aerodynamic Resolution Invariant Operator), addresses non parametric geometric
variability through an efficient shape encoding mechanism and exploits the
discretization-invariant nature of Neural Fields. It enables training on
significantly downsampled meshes, while maintaining consistent accuracy during
full-resolution inference. These properties allow for efficient modeling of
diverse flow conditions, while reducing computational cost and memory
requirements compared to traditional CFD solvers and existing surrogate
methods. The framework is validated on two complementary datasets that reflect
industrial constraints. First, the AirfRANS dataset consists in a
two-dimensional airfoil benchmark with non-parametric shape variations.
Performance evaluation of MARIO on this case demonstrates an order of magnitude
improvement in prediction accuracy over existing methods across velocity,
pressure, and turbulent viscosity fields, while accurately capturing boundary
layer phenomena and aerodynamic coefficients. Second, the NASA Common Research
Model features three-dimensional pressure distributions on a full aircraft
surface mesh, with parametric control surface deflections. This configuration
confirms MARIO's accuracy and scalability. Benchmarking against
state-of-the-art methods demonstrates that Neural Field surrogates can provide
rapid and accurate aerodynamic predictions under the computational and data
limitations characteristic of industrial applications.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [309] [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
*Yuante Li,Xu Yang,Xiao Yang,Minrui Xu,Xisen Wang,Weiqing Liu,Jiang Bian*

Key words: 量化金融、多代理系统、因子模型、自动化策略、机器学习

<details>
  <summary>Details</summary>

Main category: q-fin.CP

TL;DR: 该论文提出了一种名为RD-Agent(Q)的多代理框架，旨在通过协调因子模型共同优化，自动化完成量化金融策略的全栈研发，显著提升了回报率和鲁棒性。

Motivation: 金融市场的预测面临高维性、非平稳性和持续波动性等挑战，现有量化研究流程在自动化、可解释性和组件协调方面存在不足。

Method: RD-Agent(Q)将量化流程分解为研究阶段和开发阶段，并通过反馈阶段迭代优化，采用多臂老虎机调度器自适应选择方向。

Result: 实验表明，RD-Agent(Q)的年化回报率较传统因子库提升2倍，同时因子数量减少70%，并在真实市场中优于最先进的深度时间序列模型。

Conclusion: 该框架通过联合优化因子和模型，实现了预测准确性和策略鲁棒性的平衡。

Abstract: Financial markets pose fundamental challenges for asset return prediction due
to their high dimensionality, non-stationarity, and persistent volatility.
Despite advances in large language models and multi-agent systems, current
quantitative research pipelines suffer from limited automation, weak
interpretability, and fragmented coordination across key components such as
factor mining and model innovation. In this paper, we propose R&D-Agent for
Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent
framework designed to automate the full-stack research and development of
quantitative strategies via coordinated factor-model co-optimization.
RD-Agent(Q) decomposes the quant process into two iterative stages: a Research
stage that dynamically sets goal-aligned prompts, formulates hypotheses based
on domain priors, and maps them to concrete tasks, and a Development stage that
employs a code-generation agent, Co-STEER, to implement task-specific code,
which is then executed in real-market backtests. The two stages are connected
through a feedback stage that thoroughly evaluates experimental outcomes and
informs subsequent iterations, with a multi-armed bandit scheduler for adaptive
direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher
annualized returns than classical factor libraries using 70% fewer factors, and
outperforms state-of-the-art deep time-series models on real markets. Its joint
factor-model optimization delivers a strong balance between predictive accuracy
and strategy robustness. Our code is available at:
https://github.com/microsoft/RD-Agent.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [310] [Steering Generative Models with Experimental Data for Protein Fitness Optimization](https://arxiv.org/abs/2505.15093)
*Jason Yang,Wenda Chu,Daniel Khalil,Raul Astudillo,Bruce J. Wittmann,Frances H. Arnold,Yisong Yue*

Key words: 蛋白质优化，生成模型，分类器指导，后验采样，自适应序列选择

<details>
  <summary>Details</summary>

Main category: q-bio.BM

TL;DR: 本文研究了在蛋白质设计空间中使用少量标记数据优化蛋白质适应性的方法，比较了不同策略的指导生成性能，并提出了一种类似Thompson采样的自适应序列选择方法。

Motivation: 尽管基于生成模型的蛋白质设计方法取得进展，但现有方法在真实实验中的表现和比较尚不明确，尤其是在低通量湿实验条件下。

Method: 利用少量标记序列-适应性数据，评估了分类器指导和后验采样等策略，并将其整合到类似Thompson采样的自适应序列选择中。

Result: 研究表明，即插即用的指导策略优于其他方法，例如基于蛋白质语言模型的强化学习。

Conclusion: 提出的方法在蛋白质适应性优化中表现出潜力，为实际应用提供了新思路。

Abstract: Protein fitness optimization involves finding a protein sequence that
maximizes desired quantitative properties in a combinatorially large design
space of possible sequences. Recent developments in steering protein generative
models (e.g diffusion models, language models) offer a promising approach.
However, by and large, past studies have optimized surrogate rewards and/or
utilized large amounts of labeled data for steering, making it unclear how well
existing methods perform and compare to each other in real-world optimization
campaigns where fitness is measured by low-throughput wet-lab assays. In this
study, we explore fitness optimization using small amounts (hundreds) of
labeled sequence-fitness pairs and comprehensively evaluate strategies such as
classifier guidance and posterior sampling for guiding generation from
different discrete diffusion models of protein sequences. We also demonstrate
how guidance can be integrated into adaptive sequence selection akin to
Thompson sampling in Bayesian optimization, showing that plug-and-play guidance
strategies offer advantages compared to alternatives such as reinforcement
learning with protein language models.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [311] [HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement](https://arxiv.org/abs/2505.15740)
*Jilin Hu,Jianyu Zhang,Yongwang Zhao,Talia Ringer*

Key words: 形式化方法,自动化定理证明,大语言模型,HybridProver,Isabelle

<details>
  <summary>Details</summary>

Main category: cs.FL

TL;DR: 论文介绍了一种结合逐步生成策略和直接生成完整证明的双模型证明框架HybridProver，用于自动化定理证明，并在Isabelle中实现了59.4%的成功率。

Motivation: 尽管形式化方法对关键系统的可靠性验证非常重要，但其应用受到手动证明的繁琐和定理证明器使用门槛的限制。大语言模型（LLMs）的发展为自动化定理证明提供了新机会。

Method: HybridProver结合了逐步生成策略和直接生成完整证明的方法，首先生成候选完整证明并提取证明草图，然后通过逐步细化完成草图。

Result: 在miniF2F数据集上，HybridProver的成功率达到59.4%，超过了之前的56.1%。消融研究表明，这一结果归功于两种方法的结合。

Conclusion: HybridProver展示了结合逐步生成和直接生成方法的有效性，同时分析了数据集质量、训练参数和采样多样性对结果的影响。所有代码、数据集和LLMs均已开源。

Abstract: Formal methods is pivotal for verifying the reliability of critical systems
through rigorous mathematical proofs. However, its adoption is hindered by
labor-intensive manual proofs and the expertise required to use theorem
provers. Recent advancements in large language models (LLMs) offer new
opportunities for automated theorem proving. Two promising approaches are
generating tactics step by step and generating a whole proof directly with an
LLM. However, existing work makes no attempt to combine the two approaches. In
this work, we introduce HybridProver, a dual-model proof synthesis framework
that combines tactic-based generation and whole-proof synthesis to harness the
benefits of both approaches. HybridProver generates whole proof candidates for
evaluation directly, then extracts proof sketches from those candidates. It
then uses a tactic-based generation model that integrates automated tools to
complete the sketches via stepwise refinement. We implement HybridProver for
the Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle
datasets. Evaluation on the miniF2F dataset illustrates HybridProver's
effectiveness. We achieve a 59.4% success rate on miniF2F, where the previous
SOTA is 56.1%. Our ablation studies show that this SOTA result is attributable
to combining whole-proof and tactic-based generation. Additionally, we show how
the dataset quality, training parameters, and sampling diversity affect the
final result during automated theorem proving with LLMs. All of our code,
datasets, and LLMs are open source.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [312] [Space evaluation at the starting point of soccer transitions](https://arxiv.org/abs/2505.14711)
*Yohei Ogawa,Rikuhei Umemoto,Keisuke Fujii*

Key words: 足球, 空间评估, OBPV, 攻防转换, 场地价值模型

<details>
  <summary>Details</summary>

Main category: stat.AP

TL;DR: 本文提出了一种名为OBPV的新方法，用于评估足球比赛中全场的空间利用情况，特别是在攻防转换时。

Motivation: 现有空间评估方法（如OBSO）在评估远离球门的区域时效果不佳，而攻防转换通常发生在这些区域，因此需要一种更全面的方法。

Method: OBPV扩展了OBSO，引入了场地价值模型和过渡核模型，通过核密度估计传球分布来评估空间。

Result: 实验表明，OBPV能够突出反击中的有效空间利用，并揭示球队在攻防转换后对空间利用的特定风格。

Conclusion: OBPV为足球比赛中的空间评估提供了一种更全面的工具，特别适用于攻防转换的分析。

Abstract: Soccer is a sport played on a pitch where effective use of space is crucial.
Decision-making during transitions, when possession switches between teams, has
been increasingly important, but research on space evaluation in these moments
has been limited. Recent space evaluation methods such as OBSO (Off-Ball
Scoring Opportunity) use scoring probability, so it is not well-suited for
assessing areas far from the goal, where transitions typically occur. In this
paper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across
the pitch, including the starting points of transitions. OBPV extends OBSO by
introducing the field value model, which evaluates the entire pitch, and by
employing the transition kernel model, which reflects positional specificity
through kernel density estimation of pass distributions. Experiments using La
Liga 2023/24 season tracking and event data show that OBPV highlights effective
space utilization during counter-attacks and reveals team-specific
characteristics in how the teams utilize space after positive and negative
transitions.

</details>


### [313] [ComBAT Harmonization for diffusion MRI: Challenges and Best Practices](https://arxiv.org/abs/2505.14722)
*Pierre-Marc Jodoin,Manon Edde,Gabriel Girard,Félix Dumais,Guillaume Theaud,Matthieu Dumont,Jean-Christophe Houde,Yoan David,Maxime Descoteaux*

Key words: MRI, ComBAT, 数据协调, 人口特征, 可重复性

<details>
  <summary>Details</summary>

Main category: stat.AP

TL;DR: 论文分析了ComBAT方法在MRI数据协调中的局限性，并提出了改进版Pairwise-ComBAT，通过实验评估了人口特征对效果的影响，最终给出五项建议以提升方法的可靠性和可重复性。

Motivation: ComBAT作为MRI数据协调的标准方法，虽能保留生物变异性，但其依赖的假设一旦被违背会导致协调失败，因此需要深入研究其数学基础和适用条件。

Method: 通过改进版Pairwise-ComBAT进行实验，评估人口规模、年龄分布、协变量缺失等因素对协调效果的影响。

Result: 实验揭示了不同人口特征对方法效果的具体影响，并提出了五项优化建议。

Conclusion: 为提升ComBAT在开放科学和临床中的实用性，需遵循五项建议以确保协调的可靠性和一致性。

Abstract: Over the years, ComBAT has become the standard method for harmonizing
MRI-derived measurements, with its ability to compensate for site-related
additive and multiplicative biases while preserving biological variability.
However, ComBAT relies on a set of assumptions that, when violated, can result
in flawed harmonization. In this paper, we thoroughly review ComBAT's
mathematical foundation, outlining these assumptions, and exploring their
implications for the demographic composition necessary for optimal results.
  Through a series of experiments involving a slightly modified version of
ComBAT called Pairwise-ComBAT tailored for normative modeling applications, we
assess the impact of various population characteristics, including population
size, age distribution, the absence of certain covariates, and the magnitude of
additive and multiplicative factors. Based on these experiments, we present
five essential recommendations that should be carefully considered to enhance
consistency and supporting reproducibility, two essential factors for open
science, collaborative research, and real-life clinical deployment.

</details>


### [314] [Effective climate policies for major emission reductions of ozone precursors: Global evidence from two decades](https://arxiv.org/abs/2505.14731)
*Ningning Yao,Huan Xi,Lang Chen,Zhe Song,Jian Li,Yulei Chen,Baocai Guo,Yuanhang Zhang,Tong Zhu,Pengfei Li,Daniel Rosenfeld,John H. Seinfeld,Shaocai Yu*

Key words: 臭氧前体、结构断点检测、机器学习、政策干预、排放减少

<details>
  <summary>Details</summary>

Main category: stat.AP

TL;DR: 研究通过结合结构断点检测和机器学习，分析了全球臭氧前体排放数据的政策干预效果，发现电力、建筑等行业的不同政策组合能显著减少排放。

Motivation: 尽管政策制定者使用了多种工具来减少臭氧前体排放，但政策组合的有效性仍不明确，需要系统分析。

Method: 采用结构断点检测与机器学习结合的框架，分析全球臭氧前体排放数据，识别政策干预的突发变化。

Result: 检测到78、77和78个NOx、CO和VOCs的结构断点，分别累计减少排放0.96-0.97 Gt、2.84-2.88 Gt和0.47-0.48 Gt。电力行业政策最高减少NOx 32.4%，建筑行业政策组合在发达国家和发展中国家分别减少CO 42.7%和52.3%。

Conclusion: 混合策略结合非价格和价格工具可额外带来10%的协同效益，为政策制定提供了具体指导。

Abstract: Despite policymakers deploying various tools to mitigate emissions of ozone
(O\textsubscript{3}) precursors, such as nitrogen oxides (NO\textsubscript{x}),
carbon monoxide (CO), and volatile organic compounds (VOCs), the effectiveness
of policy combinations remains uncertain. We employ an integrated framework
that couples structural break detection with machine learning to pinpoint
effective interventions across the building, electricity, industrial, and
transport sectors, identifying treatment effects as abrupt changes without
prior assumptions about policy treatment assignment and timing. Applied to two
decades of global O\textsubscript{3} precursor emissions data, we detect 78,
77, and 78 structural breaks for NO\textsubscript{x}, CO, and VOCs,
corresponding to cumulative emission reductions of 0.96-0.97 Gt, 2.84-2.88 Gt,
and 0.47-0.48 Gt, respectively. Sector-level analysis shows that electricity
sector structural policies cut NO\textsubscript{x} by up to 32.4\%, while in
buildings, developed countries combined adoption subsidies with carbon taxes to
achieve 42.7\% CO reductions and developing countries used financing plus fuel
taxes to secure 52.3\%. VOCs abatement peaked at 38.5\% when fossil-fuel
subsidy reforms were paired with financial incentives. Finally, hybrid
strategies merging non-price measures (subsidies, bans, mandates) with pricing
instruments delivered up to an additional 10\% co-benefit. These findings guide
the sequencing and complementarity of context-specific policy portfolios for
O\textsubscript{3} precursor mitigation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [315] [SecCAN: An Extended CAN Controller with Embedded Intrusion Detection](https://arxiv.org/abs/2505.14924)
*Shashwat Khandelwal,Shreejith Shanker*

Key words: CAN, 入侵检测系统, 机器学习加速器, FPGA

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 研究人员提出了SecCAN，一种将入侵检测系统（IDS）嵌入CAN控制器数据路径的新架构，以减少现有机器学习IDS的软件和数据移动开销，并通过FPGA实现证明了其高效性和低能耗。

Motivation: 现有车辆网络协议（如CAN）易受攻击，而现有的机器学习IDS集成方法存在数据移动和软件开销大等问题。

Method: 设计SecCAN架构，将定制量化的机器学习加速器嵌入CAN控制器接收数据路径，与协议接收窗口重叠进行IDS推断。

Result: SecCAN在FPGA上实现，完全隐藏IDS延迟，检测攻击准确率高，软件开销为零，能耗低（73.7 uJ/消息），资源开销小（<30% LUT，<1% FF）。

Conclusion: SecCAN是一种适合汽车部署的高效、低开销CAN控制器IDS解决方案。

Abstract: Recent research has highlighted the vulnerability of in-vehicle network
protocols such as controller area networks (CAN) and proposed machine
learning-based intrusion detection systems (IDSs) as an effective mitigation
technique. However, their efficient integration into vehicular architecture is
non-trivial, with existing methods relying on electronic control units
(ECUs)-coupled IDS accelerators or dedicated ECUs as IDS accelerators. Here,
initiating IDS requires complete reception of a CAN message from the
controller, incurring data movement and software overheads. In this paper, we
present SecCAN, a novel CAN controller architecture that embeds IDS capability
within the datapath of the controller. This integration allows IDS to tap
messages directly from within the CAN controller as they are received from the
bus, removing overheads incurred by existing ML-based IDSs. A custom-quantised
machine-learning accelerator is developed as the IDS engine and embedded into
SecCAN's receive data path, with optimisations to overlap the IDS inference
with the protocol's reception window. We implement SecCAN on AMD XCZU7EV FPGA
to quantify its performance and benefits in hardware, using multiple attack
datasets. We show that SecCAN can completely hide the IDS latency within the
CAN reception window for all CAN packet sizes and detect multiple attacks with
state-of-the-art accuracy with zero software overheads on the ECU and low
energy overhead (73.7 uJ per message) for IDS inference. Also, SecCAN incurs
limited resource overhead compared to a standard CAN controller (< 30% LUT, <
1% FF), making it ideally suited for automotive deployment.

</details>


### [316] [AI-based Decision Support System for Heritage Aircraft Corrosion Prevention](https://arxiv.org/abs/2505.15462)
*Michal Kuchař,Jaromír Fišer,Cyril Oswald,Tomáš Vyhlídal*

Key words: 航空遗产, 决策支持系统, 多材料保护, 腐蚀机制

<details>
  <summary>Details</summary>

Main category: eess.SY

TL;DR: 本文介绍了用于航空遗产长期保存的决策支持系统，专注于多材料遗产保护。

Motivation: 航空遗产由多种材料构成，需要针对不同材料的降解机制设计保护系统。

Method: 基于知识库和腐蚀机制模型开发决策支持系统，包括历史木材和古老铝合金的预测模型。

Result: 系统在捷克航空博物馆的二战飞机遗产上进行了测试。

Conclusion: 该系统支持多材料遗产保护，并适应航空博物馆的需求。

Abstract: The paper presents a decision support system for the long-term preservation
of aeronautical heritage exhibited/stored in sheltered sites. The aeronautical
heritage is characterized by diverse materials of which this heritage is
constituted. Heritage aircraft are made of ancient aluminum alloys, (ply)wood,
and particularly fabrics. The decision support system (DSS) designed, starting
from a conceptual model, is knowledge-based on degradation/corrosion mechanisms
of prevailing materials of aeronautical heritage. In the case of historical
aircraft wooden parts, this knowledge base is filled in by the damage function
models developed within former European projects. Model-based corrosion
prediction is implemented within the new DSS for ancient aluminum alloys. The
novelty of this DSS consists of supporting multi-material heritage protection
and tailoring to peculiarities of aircraft exhibition/storage hangars and the
needs of aviation museums. The novel DSS is tested on WWII aircraft heritage
exhibited in the Aviation Museum Kbely, Military History Institute Prague,
Czech Republic.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [317] [HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases](https://arxiv.org/abs/2505.15701)
*Pingqing Zheng,Jiayin Qin,Fuqi Zhang,Shang Wu,Yu Cao,Caiwen Ding,Yang,Zhao*

Key words: LLMs, 硬件设计, Graph RAG, AST, DFG

<details>
  <summary>Details</summary>

Main category: cs.AR

TL;DR: HDLxGraph结合图检索增强生成与LLMs，通过AST和DFG提升硬件设计任务的性能，实验表明搜索精度、调试效率和完成质量显著提高。

Motivation: 解决LLMs在真实世界HDL项目中性能不足的问题。

Method: 提出HDLxGraph框架，集成Graph RAG与LLMs，引入AST和DFG的双检索机制。

Result: 搜索精度、调试效率和完成质量分别提升12.04%、12.22%和5.04%。

Conclusion: HDLxGraph显著提升LLMs在HDL任务中的表现，具备扩展性和实用性。

Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware
design tasks, such as Hardware Description Language (HDL) generation and
debugging. Yet, their performance in real-world, repository-level HDL projects
with thousands or even tens of thousands of code lines is hindered. To this
end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval
Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph
representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow
Graphs (DFGs) to capture both code graph view and hardware graph view.
HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the
limited recall issues inherent in similarity-based semantic retrieval by
incorporating structural information, but also enhances its extensibility to
various real-world tasks by a task-specific retrieval finetuning. Additionally,
to address the lack of comprehensive HDL search benchmarks, we introduce
HDLSearch, a multi-granularity evaluation dataset derived from real-world
repository-level projects. Experimental results demonstrate that HDLxGraph
significantly improves average search accuracy, debugging efficiency and
completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based
RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are
available at https://github.com/Nick-Zheng-Q/HDLxGraph.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [318] [SDLog: A Deep Learning Framework for Detecting Sensitive Information in Software Logs](https://arxiv.org/abs/2505.14976)
*Roozbeh Aghili,Xingfang Wu,Foutse Khomh,Heng Li*

Key words: 软件日志,敏感信息,深度学习,匿名化

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 开发了一种基于深度学习的框架SDLog，用于识别软件日志中的敏感信息，解决了传统正则表达式方法的局限性。

Motivation: 软件日志中包含敏感信息，阻碍了日志数据集的公开共享和日志分析研究的进展。

Method: 提出SDLog框架，利用深度学习技术识别日志中的敏感信息。

Result: SDLog在仅需100个样本的情况下，正确识别率为99.5%，F1得分为98.4%。

Conclusion: SDLog是首个深度学习替代正则表达式的日志匿名化方法，性能优越。

Abstract: Software logs are messages recorded during the execution of a software system
that provide crucial run-time information about events and activities. Although
software logs have a critical role in software maintenance and operation tasks,
publicly accessible log datasets remain limited, hindering advance in log
analysis research and practices. The presence of sensitive information,
particularly Personally Identifiable Information (PII) and quasi-identifiers,
introduces serious privacy and re-identification risks, discouraging the
publishing and sharing of real-world logs. In practice, log anonymization
techniques primarily rely on regular expression patterns, which involve
manually crafting rules to identify and replace sensitive information. However,
these regex-based approaches suffer from significant limitations, such as
extensive manual efforts and poor generalizability across diverse log formats
and datasets. To mitigate these limitations, we introduce SDLog, a deep
learning-based framework designed to identify sensitive information in software
logs. Our results show that SDLog overcomes regex limitations and outperforms
the best-performing regex patterns in identifying sensitive information. With
only 100 fine-tuning samples from the target dataset, SDLog can correctly
identify 99.5% of sensitive attributes and achieves an F1-score of 98.4%. To
the best of our knowledge, this is the first deep learning alternative to
regex-based methods in software log anonymization.

</details>


### [319] [JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation](https://arxiv.org/abs/2505.14978)
*Ghasem Pasandi,Kishor Kunal,Varun Tej,Kunjal Shan,Hanfei Sun,Sumit Jain,Chunhui Li,Chenhui Deng,Teodor-Dumitru Ene,Haoxing Ren,Sreedhar Pratty*

Key words: JARVIS, LLMs, EDA, 多智能体框架, 幻觉错误, 数据稀缺

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: JARVIS是一个新颖的多智能体框架，利用大语言模型（LLMs）和领域专业知识为EDA任务生成高质量脚本，解决了数据稀缺和幻觉错误问题，并在多个基准测试中表现优异。

Motivation: 解决EDA任务中数据稀缺和LLMs的幻觉错误问题，探索LLMs在专业工程领域的潜力。

Method: 结合领域特定的LLM、自定义编译器、规则强制执行、代码修复能力和高级检索机制。

Result: 在准确性和可靠性上优于现有领域特定模型。

Conclusion: JARVIS为LLMs在EDA领域的应用设定了新标准，为未来创新铺平道路。

Abstract: This paper presents JARVIS, a novel multi-agent framework that leverages
Large Language Models (LLMs) and domain expertise to generate high-quality
scripts for specialized Electronic Design Automation (EDA) tasks. By combining
a domain-specific LLM trained with synthetically generated data, a custom
compiler for structural verification, rule enforcement, code fixing
capabilities, and advanced retrieval mechanisms, our approach achieves
significant improvements over state-of-the-art domain-specific models. Our
framework addresses the challenges of data scarcity and hallucination errors in
LLMs, demonstrating the potential of LLMs in specialized engineering domains.
We evaluate our framework on multiple benchmarks and show that it outperforms
existing models in terms of accuracy and reliability. Our work sets a new
precedent for the application of LLMs in EDA and paves the way for future
innovations in this field.

</details>


### [320] [Towards a Science of Causal Interpretability in Deep Learning for Software Engineering](https://arxiv.org/abs/2505.15023)
*David N. Palacio*

Key words: 因果解释性、深度学习、神经代码模型、软件工程、DoCode

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该论文提出了一种名为DoCode的后解释性方法，用于增强深度神经网络在软件工程中的因果解释性，通过因果推断提供代码预测的解释。

Motivation: 由于神经代码模型（NCMs）虽然性能强大，但其因果关系的透明度不足，限制了对其能力的全面理解，研究旨在提升NCMs的可信度。

Method: DoCode采用因果推断方法，包含四个步骤：使用结构因果模型（SCMs）建模、识别因果估计量、通过平均处理效应（ATE）等指标估计效果，以及对效果估计进行验证。

Result: 实验表明，DoCode能够减少伪相关性，并通过案例分析展示NCMs对代码语法变化的敏感性及学习编程概念的能力。

Conclusion: 研究为NCMs的因果解释性提供了实用指南，推动了软件工程中更可信赖的人工智能。

Abstract: This dissertation addresses achieving causal interpretability in Deep
Learning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show
strong performance in automating software tasks, their lack of transparency in
causal relationships between inputs and outputs limits full understanding of
their capabilities. To build trust in NCMs, researchers and practitioners must
explain code predictions. Associational interpretability, which identifies
correlations, is often insufficient for tasks requiring intervention and change
analysis. To address this, the dissertation introduces DoCode, a novel post hoc
interpretability method for NCMs. DoCode uses causal inference to provide
programming language-oriented explanations of model predictions. It follows a
four-step pipeline: modeling causal problems using Structural Causal Models
(SCMs), identifying the causal estimand, estimating effects with metrics like
Average Treatment Effect (ATE), and refuting effect estimates. Its framework is
extensible, with an example that reduces spurious correlations by grounding
explanations in programming language properties. A case study on deep code
generation across interpretability scenarios and various deep learning
architectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to
code syntax changes and their ability to learn certain programming concepts
while minimizing confounding bias. The dissertation also examines associational
interpretability as a foundation, analyzing software information's causal
nature using tools like COMET and TraceXplainer for traceability. It highlights
the need to identify code confounders and offers practical guidelines for
applying causal interpretability to NCMs, contributing to more trustworthy AI
in software engineering.

</details>


### [321] [LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming](https://arxiv.org/abs/2505.15039)
*Sicheol Sung,Aditi,Dogyu kim,Yo-Sub Han,Sang-Ki Ko*

Key words: 自动化测试用例生成, 上下文无关文法, 竞争编程, CodeT5模型

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 本文提出了一种基于上下文无关文法与计数器（CCFGs）的自动化测试用例生成方法，适用于竞争编程中对算法可靠性的评估。

Motivation: 现有的自动化测试用例生成方法难以满足复杂的输入规范或生成有效的边界情况，限制了其实际应用价值。

Method: 采用上下文无关文法与计数器（CCFGs）来捕获输入规范的语法和语义结构，并通过微调的CodeT5模型将自然语言规范转换为CCFGs，从而系统生成高质量的测试用例。

Result: 在CodeContests数据集上的实验表明，CCFG生成的测试用例在识别错误算法和有效性方面优于基线方法。

Conclusion: 该方法为竞争编程的自动化评估提供了一个可扩展且可靠的语法驱动框架。

Abstract: Automated Test Case Generation (ATCG) is crucial for evaluating software
reliability, particularly in competitive programming where robust algorithm
assessments depend on diverse and accurate test cases. However, existing ATCG
methods often fail to meet complex specifications or generate effective corner
cases, limiting their utility. In this work, we introduce Context-Free Grammars
with Counters (CCFGs), a formalism that captures both syntactic and semantic
structures in input specifications. Using a fine-tuned CodeT5 model, we
translate natural language input specifications into CCFGs, enabling the
systematic generation of high-quality test cases. Experiments on the
CodeContests dataset demonstrate that CCFG-based test cases outperform baseline
methods in identifying incorrect algorithms, achieving significant gains in
validity and effectiveness. Our approach provides a scalable and reliable
grammar-driven framework for enhancing automated competitive programming
evaluations.

</details>


### [322] [Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects](https://arxiv.org/abs/2505.15088)
*Yuxuan Wang,Jingshu Chen,Qingyang Wang*

Key words: 命令注入漏洞, 大型语言模型, 自动化测试, 软件安全, Python

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 研究评估了大型语言模型（如GPT-4）在自动化检测命令注入漏洞中的潜力，分析了其在6个知名开源项目中的表现，并提供了开发者利用LLM提升软件安全的建议。

Motivation: 动态语言（如Python）中命令注入漏洞的安全威胁显著，尤其是开源项目中。LLM在代码相关任务中表现优异，研究者希望探索其在漏洞分析中的潜力。

Method: 研究将LLM应用于6个高星GitHub项目（Django、Flask等），评估其在命令注入漏洞检测中的准确性、效率及实际集成能力。

Result: 分析了LLM在检测命令注入漏洞中的优势和局限，并比较了不同LLM工具在安全应用中的适用性。

Conclusion: LLM可作为创新的自动化工具提升软件安全，研究为开发者和安全研究人员提供了实用指导。

Abstract: Command injection vulnerabilities are a significant security threat in
dynamic languages like Python, particularly in widely used open-source projects
where security issues can have extensive impact. With the proven effectiveness
of Large Language Models(LLMs) in code-related tasks, such as testing,
researchers have explored their potential for vulnerabilities analysis. This
study evaluates the potential of large language models (LLMs), such as GPT-4,
as an alternative approach for automated testing for vulnerability detection.
In particular, LLMs have demonstrated advanced contextual understanding and
adaptability, making them promising candidates for identifying nuanced security
vulnerabilities within code. To evaluate this potential, we applied LLM-based
analysis to six high-profile GitHub projects-Django, Flask, TensorFlow,
Scikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive
adoption across software development and academic research. Our analysis
assesses both the strengths and limitations of LLMs in detecting command
injection vulnerabilities, evaluating factors such as detection accuracy,
efficiency, and practical integration into development workflows. In addition,
we provide a comparative analysis of different LLM tools to identify those most
suitable for security applications. Our findings offer guidance for developers
and security researchers on leveraging LLMs as innovative and automated
approaches to enhance software security.

</details>


### [323] [Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers](https://arxiv.org/abs/2505.14692)
*KM Khalid Saifullah,Faiaz Azmain,Habiba Hye*

Key words: 情感分析, 软件工程, BERT, GPT-4o-mini, 微调

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 该研究比较了BERT和GPT-4o-mini在软件工程情感分析中的表现，发现GPT-4o-mini在复杂不平衡数据集上表现更优，而BERT在结构化数据集上表现更佳。

Motivation: 传统软件工程情感分析工具未能充分处理领域内复杂语境，需更先进的模型提升分析效果。

Method: 使用GitHub、Stack Overflow和Jira数据集，对BERT和GPT-4o-mini进行微调和默认配置的对比测试。

Result: 微调后的GPT-4o-mini在GitHub和Jira上表现与BERT相当，但在Stack Overflow上默认模型表现更优（准确率85.3%）。

Conclusion: 模型性能与数据集特性相关，未来研究需优化模型以适配软件工程领域需求。

Abstract: Sentiment analysis plays a crucial role in understanding developer
interactions, issue resolutions, and project dynamics within software
engineering (SE). While traditional SE-specific sentiment analysis tools have
made significant strides, they often fail to account for the nuanced and
context-dependent language inherent to the domain. This study systematically
evaluates the performance of bidirectional transformers, such as BERT, against
generative pre-trained transformers, specifically GPT-4o-mini, in SE sentiment
analysis. Using datasets from GitHub, Stack Overflow, and Jira, we benchmark
the models' capabilities with fine-tuned and default configurations. The
results reveal that fine-tuned GPT-4o-mini performs comparable to BERT and
other bidirectional models on structured and balanced datasets like GitHub and
Jira, achieving macro-averaged F1-scores of 0.93 and 0.98, respectively.
However, on linguistically complex datasets with imbalanced sentiment
distributions, such as Stack Overflow, the default GPT-4o-mini model exhibits
superior generalization, achieving an accuracy of 85.3\% compared to the
fine-tuned model's 13.1\%. These findings highlight the trade-offs between
fine-tuning and leveraging pre-trained models for SE tasks. The study
underscores the importance of aligning model architectures with dataset
characteristics to optimize performance and proposes directions for future
research in refining sentiment analysis tools tailored to the SE domain.

</details>


### [324] [A Qualitative Investigation into LLM-Generated Multilingual Code Comments and Automatic Evaluation Metrics](https://arxiv.org/abs/2505.15469)
*Jonathan Katzy,Yongcheng Huang,Gopal-Raj Panchu,Maksym Ziemlewski,Paris Loizides,Sander Vermeulen,Arie van Deursen,Maliheh Izadi*

Key words: 大语言模型,代码注释,多语言,评估指标

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: 研究评估了代码语言模型在非英语环境中的表现，发现现有模型在多语言环境中存在挑战。

Motivation: 大模型训练中以英语为主，非英语环境的代码注释生成能力未充分研究。

Method: 通过开放编码研究五种代码模型在五种语言的12,500条生成注释中的错误。

Result: 发现26种错误类型，现代神经指标无法可靠区分正确与错误注释。

Conclusion: 现有指标在多语言注释评估中效果存疑。

Abstract: Large Language Models are essential coding assistants, yet their training is
predominantly English-centric. In this study, we evaluate the performance of
code language models in non-English contexts, identifying challenges in their
adoption and integration into multilingual workflows. We conduct an open-coding
study to analyze errors in code comments generated by five state-of-the-art
code models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2
across five natural languages: Chinese, Dutch, English, Greek, and Polish. Our
study yields a dataset of 12,500 labeled generations, which we publicly
release. We then assess the reliability of standard metrics in capturing
comment \textit{correctness} across languages and evaluate their
trustworthiness as judgment criteria. Through our open-coding investigation, we
identified a taxonomy of 26 distinct error categories in model-generated code
comments. They highlight variations in language cohesion, informativeness, and
syntax adherence across different natural languages. Our analysis shows that,
while these models frequently produce partially correct comments, modern neural
metrics fail to reliably differentiate meaningful completions from random
noise. Notably, the significant score overlap between expert-rated correct and
incorrect comments calls into question the effectiveness of these metrics in
assessing generated comments.

</details>


### [325] [LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models](https://arxiv.org/abs/2505.14759)
*Yan Wang,Ling Ding,Tien N Nguyen,Shaohua Wang,Yanan Zheng*

Key words: 大型语言模型,代码简化,注意力分数,代码搜索,代码总结

<details>
  <summary>Details</summary>

Main category: cs.SE

TL;DR: LeanCode通过基于注意力分数的代码简化方法，显著减少了大型语言模型的计算复杂度，提升了代码搜索和总结任务的性能。

Motivation: 减少大型语言模型在处理代码时的计算复杂度，特别是随着输入代码序列长度的增加。

Method: 利用上下文感知注意力分数选择性地移除不重要的代码标记，并利用CLS标记的注意力分数优化分类任务，以及编码器-解码器注意力分数优化序列生成任务。

Result: LeanCode在代码搜索任务上比SOTAs DietCode和Slimcode分别提升60%和16%，在代码总结任务上分别提升29%和27%。

Conclusion: LeanCode通过有效简化代码并利用注意力分数，显著提升了效率和任务性能。

Abstract: Large Language Models for code often entail significant computational
complexity, which grows significantly with the length of the input code
sequence. We propose LeanCode for code simplification to reduce training and
prediction time, leveraging code contexts in utilizing attention scores to
represent the tokens' importance. We advocate for the selective removal of
tokens based on the average context-aware attention scores rather than average
scores across all inputs. LeanCode uses the attention scores of `CLS' tokens
within the encoder for classification tasks, such as code search. It also
employs the encoder-decoder attention scores to determine token significance
for sequence-to-sequence tasks like code summarization.Our evaluation shows
LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements
of 60% and 16% for code search, and 29% and 27% for code summarization,
respectively.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [326] [Stochastic Processes with Modified Lognormal Distribution Featuring Flexible Upper Tail](https://arxiv.org/abs/2505.14713)
*Dionissios T. Hristopulos,Anastassia Baxevani,Giorgio Kaniadakis*

Key words: kappa-对数正态分布, 偏态分布, 重尾数据, 最大似然估计, 高斯过程回归

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 论文提出了一种三参数的非高斯概率密度函数家族，基于广义的kappa指数和对数函数，用于处理偏态和重尾数据，提供了数学特性和实用应用。

Motivation: 针对非对称、非高斯分布数据（如偏态和重尾数据），传统对数正态分布在高值区域的渐近依赖性和危险函数受限，需要更灵活的模型。

Method: 提出基于广义kappa指数和对数函数的三参数概率密度函数，研究其数学性质，包括统计函数、矩、最大似然估计及多维扩展。

Result: kappa-对数正态分布能够提供更轻的右尾控制和双峰分布特性，适用于时间序列预测和空间插值等实际应用。

Conclusion: kappa-对数正态分布为科学和工程领域的偏态数据建模提供了更灵活和实用的工具。

Abstract: Asymmetric, non-Gaussian probability distributions are often observed in the
analysis of natural and engineering datasets. The lognormal distribution is a
standard model for data with skewed frequency histograms and fat tails.
However, the lognormal law severely restricts the asymptotic dependence of the
probability density and the hazard function for high values. Herein we present
a family of three-parameter non-Gaussian probability density functions that are
based on generalized kappa-exponential and kappa-logarithm functions and
investigate its mathematical properties. These kappa-lognormal densities
represent continuous deformations of the lognormal with lighter right tails,
controlled by the parameter kappa. In addition, bimodal distributions are
obtained for certain parameter combinations. We derive closed-form analytic
expressions for the main statistical functions of the kappa-lognormal
distribution. For the moments, we derive bounds that are based on
hypergeometric functions as well as series expansions. Explicit expressions for
the gradient and Hessian of the negative log-likelihood are obtained to
facilitate numerical maximum-likelihood estimates of the kappa-lognormal
parameters from data. We also formulate a joint probability density function
for kappa-lognormal stochastic processes by applying Jacobi's multivariate
theorem to a latent Gaussian process. Estimation of the kappa-lognormal
distribution based on synthetic and real data is explored. Furthermore, we
investigate applications of kappa-lognormal processes with different covariance
kernels in time series forecasting and spatial interpolation using warped
Gaussian process regression. Our results are of practical interest for modeling
skewed distributions in various scientific and engineering fields.

</details>


### [327] [Modular Jump Gaussian Processes](https://arxiv.org/abs/2505.15557)
*Anna R. Flowers,Christopher T. Franck,Mickaël Binois,Chiwoo Park,Robert B. Gramacy*

Key words: 高斯过程，跳跃数据，模块化建模，局部邻域，聚类特征

<details>
  <summary>Details</summary>

Main category: stat.ME

TL;DR: 论文提出了一种模块化的方法，简化了Jump GP（JGP）模型，通过独立优化局部邻域大小和引入聚类特征，显著提升了处理跳跃数据的性能。

Motivation: 传统高斯过程（GP）假设数据的平稳性，不适合处理跳跃数据。JGP虽然解决了这一问题，但其联合建模框架复杂且难以实现。本文旨在简化JGP，保留其核心优点。

Method: 提出了一种模块化方法：（a）学习局部邻域大小以适应不连续流形；（b）引入新的聚类特征捕获跳跃两侧的不同输出区域。两者独立优化，无需联合推断。

Result: 实验表明，单独优化（a）或（b）均能显著提升跳跃数据的建模效果；两者结合时效果更佳，在真实和合成数据上验证了其优越性。

Conclusion: 模块化的JGP方法简化了模型，同时保留了处理跳跃数据的核心能力，为复杂场景提供了更实用的解决方案。

Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with
well-calibrated uncertainty. However, the typical GP setup has a built-in
stationarity assumption, making it ill-suited for modeling data from processes
with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was
developed for modeling data from such processes, combining local GPs and latent
"level" variables under a joint inferential framework. But joint modeling can
be fraught with difficulty. We aim to simplify by suggesting a more modular
setup, eschewing joint inference but retaining the main JGP themes: (a)
learning optimal neighborhood sizes that locally respect manifolds of
discontinuity; and (b) a new cluster-based (latent) feature to capture regions
of distinct output levels on both sides of the manifold. We show that each of
(a) and (b) separately leads to dramatic improvements when modeling processes
with jumps. In tandem (but without requiring joint inference) that benefit is
compounded, as illustrated on real and synthetic benchmark examples from the
recent literature.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [328] [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
*Andy K. Zhang,Joey Ji,Celeste Menders,Riya Dulepet,Thomas Qin,Ron Y. Wang,Junrong Wu,Kyleen Liao,Jiliang Li,Jinghan Hu,Sara Hong,Nardos Demilew,Shivatmica Murgai,Jason Tran,Nishka Kacheria,Ethan Ho,Denis Liu,Lauren McLane,Olivia Bruvik,Dai-Rong Han,Seungwoo Kim,Akhil Vyas,Cuiyuanxiu Chen,Ryan Li,Weiran Xu,Jonathan Z. Ye,Prerit Choudhary,Siddharth M. Bhatia,Vikram Sivashankar,Yuxuan Bao,Dawn Song,Dan Boneh,Daniel E. Ho,Percy Liang*

Key words: AI代理, 网络安全, BountyBench, 攻防能力, 漏洞生命周期

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出首个框架BountyBench，用于评估AI代理在网络安全中的攻防能力，测试了5种代理在Detect、Exploit和Patch任务中的表现。

Motivation: 为理解AI代理如何改变网络安全领域，需系统评估其攻防能力。

Method: 构建25个真实系统环境，定义Detect、Exploit和Patch任务，引入信息导向策略调节任务难度，并评估5种AI代理。

Result: Claude Code、OpenAI Codex CLI和定制代理表现最佳，防御任务中OpenAI Codex CLI和Claude Code表现更优。

Conclusion: AI代理在网络安全中展现出攻防潜力，但表现因任务类型和代理能力而异。

Abstract: AI agents have the potential to significantly alter the cybersecurity
landscape. To help us understand this change, we introduce the first framework
to capture offensive and defensive cyber-capabilities in evolving real-world
systems. Instantiating this framework with BountyBench, we set up 25 systems
with complex, real-world codebases. To capture the vulnerability lifecycle, we
define three task types: Detect (detecting a new vulnerability), Exploit
(exploiting a specific vulnerability), and Patch (patching a specific
vulnerability). For Detect, we construct a new success indicator, which is
general across vulnerability types and provides localized evaluation. We
manually set up the environment for each system, including installing packages,
setting up server(s), and hydrating database(s). We add 40 bug bounties, which
are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of
the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy
based on information to guide detection, interpolating from identifying a zero
day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,
OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and
Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing
agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with
Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on
Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch,
mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at
defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit
scores of 32.5% and 57.5% respectively; in contrast, the custom agents are
relatively balanced between offense and defense, achieving Exploit scores of
40-67.5% and Patch scores of 45-60%.

</details>


### [329] [Adaptive Plan-Execute Framework for Smart Contract Security Auditing](https://arxiv.org/abs/2505.15242)
*Zhiyuan Wei,Jing Sun,Zijian Zhang,Zhe Hou,Zixiao Zhao*

Key words: 智能合约安全, LLM, 动态审计, 漏洞检测

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: SmartAuditFlow是一种新颖的Plan-Execute框架，用于提升智能合约安全分析的动态性和准确性，通过动态审计规划和结构化执行优化漏洞检测。

Motivation: 尽管大语言模型（LLMs）在代码分析和审计方面表现出潜力，但仍存在幻觉问题和上下文感知能力不足的局限。

Method: 采用动态生成和优化审计计划的Plan-Execute框架，结合迭代提示优化和外部知识源（如静态分析工具和RAG），增强漏洞检测的精确性。

Result: 在多个基准测试中表现优异，常见和关键漏洞检测准确率达100%，41.2%覆盖率识别已知弱点，并成功检测所有13个测试的CVE。

Conclusion: SmartAuditFlow在可扩展性、成本效益和适应性上优于传统静态分析工具和现有LLM方法，是自动智能合约审计的高效解决方案。

Abstract: Large Language Models (LLMs) have shown great promise in code analysis and
auditing; however, they still struggle with hallucinations and limited
context-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute
framework that enhances smart contract security analysis through dynamic audit
planning and structured execution. Unlike conventional LLM-based auditing
approaches that follow fixed workflows and predefined steps, SmartAuditFlow
dynamically generates and refines audit plans based on the unique
characteristics of each smart contract. It continuously adjusts its auditing
strategy in response to intermediate LLM outputs and newly detected
vulnerabilities, ensuring a more adaptive and precise security assessment. The
framework then executes these plans step by step, applying a structured
reasoning process to enhance vulnerability detection accuracy while minimizing
hallucinations and false positives. To further improve audit precision,
SmartAuditFlow integrates iterative prompt optimization and external knowledge
sources, such as static analysis tools and Retrieval-Augmented Generation
(RAG). This ensures audit decisions are contextually informed and backed by
real-world security knowledge, producing comprehensive security reports.
Extensive evaluations across multiple benchmarks demonstrate that
SmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on
common and critical vulnerabilities, 41.2 percent accuracy for comprehensive
coverage of known smart contract weaknesses in real-world projects, and
successfully identifying all 13 tested CVEs. These results highlight
SmartAuditFlow's scalability, cost-effectiveness, and superior adaptability
over traditional static analysis tools and contemporary LLM-based approaches,
establishing it as a robust solution for automated smart contract auditing.

</details>


### [330] [Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries](https://arxiv.org/abs/2505.15420)
*Yuhao Wang,Wenjie Qu,Yanze Jiang,Zichen Liu,Yue Liu,Shengfang Zhai,Yinpeng Dong,Jiaheng Zhang*

Key words: RAG系统, 隐私风险, 知识提取, 隐性攻击

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文提出了一种隐式知识提取攻击（IKEA），通过良性查询对RAG系统进行知识提取，显著提高了攻击效率和成功率。

Motivation: RAG系统可能因外部知识库的引入而面临隐私风险，现有的攻击方法由于依赖恶意输入易被检测，因此需要一种更隐蔽的攻击方式。

Method: IKEA通过锚概念生成自然查询，并利用经验反射采样和信任区域定向突变两大机制，实现对隐私知识的有效提取。

Result: 实验表明，IKEA在各种防御措施下表现出色，提取效率和攻击成功率远超基线方法。

Conclusion: IKEA揭示了RAG系统在隐私保护方面的严重漏洞，强调了改进防御机制的必要性。

Abstract: Retrieval-Augmented Generation (RAG) systems enhance large language models
(LLMs) by incorporating external knowledge bases, but they are vulnerable to
privacy risks from data extraction attacks. Existing extraction methods
typically rely on malicious inputs such as prompt injection or jailbreaking,
making them easily detectable via input- or output-level detection. In this
paper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts
knowledge extraction on RAG systems through benign queries. IKEA first
leverages anchor concepts to generate queries with the natural appearance, and
then designs two mechanisms to lead to anchor concept thoroughly 'explore' the
RAG's privacy knowledge: (1) Experience Reflection Sampling, which samples
anchor concepts based on past query-response patterns to ensure the queries'
relevance to RAG documents; (2) Trust Region Directed Mutation, which
iteratively mutates anchor concepts under similarity constraints to further
exploit the embedding space. Extensive experiments demonstrate IKEA's
effectiveness under various defenses, surpassing baselines by over 80% in
extraction efficiency and 90% in attack success rate. Moreover, the substitute
RAG system built from IKEA's extractions consistently outperforms those based
on baseline methods across multiple evaluation tasks, underscoring the
significant privacy risk in RAG systems.

</details>


### [331] [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)
*Xiaoxue Yang,Bozhidar Stevanoski,Matthieu Meeus,Yves-Alexandre de Montjoye*

Key words: 大型语言模型, 对齐防御, 对抗攻击, GCG攻击, 安全性评估

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文评估了当前大型语言模型防御措施的鲁棒性，提出了一种利用中间模型检查点初始化GCG攻击的改进方法，证明了即使是先进的对齐防御措施也存在有效的对抗性后缀。

Motivation: 研究旨在揭示当前大型语言模型（LLMs）对齐防御措施的脆弱性，特别是在攻击者获得部分对齐知识的情况下。

Method: 提出了一个改进的白盒攻击方法，利用中间模型检查点初始化GCG攻击，并通过梯度信息选择检查点以提高攻击效率。

Result: 该方法在多个先进防御措施和模型上表现高效，成功找到了通用的对抗性后缀，证明了当前防御措施的脆弱性。

Conclusion: 研究表明当前的对齐防御措施在面对知识丰富的攻击者时存在严重缺陷，需考虑更强的威胁模型来评估LLMs的安全性。

Abstract: Large language models (LLMs) are rapidly deployed in real-world applications
ranging from chatbots to agentic systems. Alignment is one of the main
approaches used to defend against attacks such as prompt injection and
jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even
against Greedy Coordinate Gradient (GCG), a white-box attack that generates
adversarial suffixes to induce attacker-desired outputs. However, this search
space over discrete tokens is extremely large, making the task of finding
successful attacks difficult. GCG has, for instance, been shown to converge to
local minima, making it sensitive to initialization choices. In this paper, we
assess the future-proof robustness of these defenses using a more informed
threat model: attackers who have access to some information about the alignment
process. Specifically, we propose an informed white-box attack leveraging the
intermediate model checkpoints to initialize GCG, with each checkpoint acting
as a stepping stone for the next one. We show this approach to be highly
effective across state-of-the-art (SOTA) defenses and models. We further show
our informed initialization to outperform other initialization methods and show
a gradient-informed checkpoint selection strategy to greatly improve attack
performance and efficiency. Importantly, we also show our method to
successfully find universal adversarial suffixes -- single suffixes effective
across diverse inputs. Our results show that, contrary to previous beliefs,
effective adversarial suffixes do exist against SOTA alignment-based defenses,
that these can be found by existing attack methods when adversaries exploit
alignment knowledge, and that even universal suffixes exist. Taken together,
our results highlight the brittleness of current alignment-based methods and
the need to consider stronger threat models when testing the safety of LLMs.

</details>


### [332] [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753)
*Taiye Chen,Zeming Wei,Ang Li,Yisen Wang*

Key words: 大型语言模型,越狱攻击,安全性上下文检索,检索增强生成

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 该论文探讨了通过上下文检索防御LLM的越狱攻击，提出了安全性上下文检索（SCR）方法，显著提升了模型对越狱攻击的鲁棒性。

Motivation: 大型语言模型（LLM）容易受到越狱攻击，现有防御机制存在局限性，需探索动态防御方法。

Method: 通过初步研究验证安全对齐示例的有效性，并基于检索增强生成（RAG）技术提出SCR方法。

Result: 实验表明SCR对现有和新兴越狱攻击具有优异防御性能。

Conclusion: SCR为LLM安全提供了新的防御范式，其代码将在发布时公开。

Abstract: Large Language Models (LLMs) are known to be vulnerable to jailbreaking
attacks, wherein adversaries exploit carefully engineered prompts to induce
harmful or unethical responses. Such threats have raised critical concerns
about the safety and reliability of LLMs in real-world deployment. While
existing defense mechanisms partially mitigate such risks, subsequent
advancements in adversarial techniques have enabled novel jailbreaking methods
to circumvent these protections, exposing the limitations of static defense
frameworks. In this work, we explore defending against evolving jailbreaking
threats through the lens of context retrieval. First, we conduct a preliminary
study demonstrating that even a minimal set of safety-aligned examples against
a particular jailbreak can significantly enhance robustness against this attack
pattern. Building on this insight, we further leverage the retrieval-augmented
generation (RAG) techniques and propose Safety Context Retrieval (SCR), a
scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our
comprehensive experiments demonstrate how SCR achieves superior defensive
performance against both established and emerging jailbreaking tactics,
contributing a new paradigm to LLM safety. Our code will be available upon
publication.

</details>


### [333] [An Efficient Private GPT Never Autoregressively Decodes](https://arxiv.org/abs/2505.15252)
*Zhengyi Li,Yue Guan,Kang Yang,Yu Feng,Ning Liu,Yu Yu,Jingwen Leng,Minyi Guo*

Key words: GPT, 安全推理, 隐私保护, 知识蒸馏, 加密优化

<details>
  <summary>Details</summary>

Main category: cs.CR

TL;DR: 论文提出了一种结合公开解码和安全验证的方法，以加速GPT的安全推理，同时保护隐私和生成质量。

Motivation: 随着GPT的广泛应用，隐私问题日益突出。传统的加密方法虽能保护隐私，但性能开销大，因此需要更高效的解决方案。

Method: 1. 使用公开GPT模型生成候选令牌，并通过私有模型安全验证。2. 优化私有采样协议和模型对齐（知识蒸馏），以提高令牌接受率。

Result: 实验表明，该方法在三种公开-私有模型组合和不同网络条件下，实现了2.1倍到6.0倍的加速。

Conclusion: 所提方法在保持隐私和生成质量的同时，显著提升了安全推理的效率。

Abstract: The wide deployment of the generative pre-trained transformer (GPT) has
raised privacy concerns for both clients and servers. While cryptographic
primitives can be employed for secure GPT inference to protect the privacy of
both parties, they introduce considerable performance overhead.To accelerate
secure inference, this study proposes a public decoding and secure verification
approach that utilizes public GPT models, motivated by the observation that
securely decoding one and multiple tokens takes a similar latency. The client
uses the public model to generate a set of tokens, which are then securely
verified by the private model for acceptance. The efficiency of our approach
depends on the acceptance ratio of tokens proposed by the public model, which
we improve from two aspects: (1) a private sampling protocol optimized for
cryptographic primitives and (2) model alignment using knowledge distillation.
Our approach improves the efficiency of secure decoding while maintaining the
same level of privacy and generation quality as standard secure decoding.
Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to
standard decoding across three pairs of public-private models and different
network conditions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [334] [Place Cells as Position Embeddings of Multi-Time Random Walk Transition Kernels for Path Planning](https://arxiv.org/abs/2505.14806)
*Minglu Zhao,Dehong Xu,Deqian Kong,Wen-Hao Zhang,Ying Nian Wu*

Key words: 海马体,位置细胞,对称随机游走,多尺度嵌入,空间导航

<details>
  <summary>Details</summary>

Main category: q-bio.NC

TL;DR: 该论文提出了一种基于对称随机游走转移核的多尺度位置嵌入模型，用于重新概念化海马体位置细胞的群体编码，模拟认知地图的形成和空间导航。

Motivation: 传统方法关注单个位置细胞的特性，作者希望通过群体转移概率的建模，提供一个更符合生物实际且可扩展的空间导航框架。

Method: 采用梯度上升法和自适应尺度选择，通过矩阵平方从局部转移构建全局表示，无需记忆历史轨迹，实现平滑导航路径。

Result: 模型成功捕获了位置细胞的特性（如场大小分布、适应性和重映射），并在复杂环境中表现出稳健的导航能力。

Conclusion: 通过群体转移概率建模而非单个位置场，提供了一个生物合理且可扩展的空间导航框架。

Abstract: The hippocampus orchestrates spatial navigation through collective place cell
encodings that form cognitive maps. We reconceptualize the population of place
cells as position embeddings approximating multi-scale symmetric random walk
transition kernels: the inner product $\langle h(x, t), h(y, t) \rangle =
q(y|x, t)$ represents normalized transition probabilities, where $h(x, t)$ is
the embedding at location $ x $, and $q(y|x, t)$ is the normalized symmetric
transition probability over time $t$. The time parameter $\sqrt{t}$ defines a
spatial scale hierarchy, mirroring the hippocampal dorsoventral axis. $q(y|x,
t)$ defines spatial adjacency between $x$ and $y$ at scale or resolution
$\sqrt{t}$, and the pairwise adjacency relationships $(q(y|x, t), \forall x,
y)$ are reduced into individual embeddings $(h(x, t), \forall x)$ that
collectively form a map of the environment at sale $\sqrt{t}$. Our framework
employs gradient ascent on $q(y|x, t) = \langle h(x, t), h(y, t)\rangle$ with
adaptive scale selection, choosing the time scale with maximal gradient at each
step for trap-free, smooth trajectories. Efficient matrix squaring $P_{2t} =
P_t^2$ builds global representations from local transitions $P_1$ without
memorizing past trajectories, enabling hippocampal preplay-like path planning.
This produces robust navigation through complex environments, aligning with
hippocampal navigation. Experimental results show that our model captures place
cell properties -- field size distribution, adaptability, and remapping --
while achieving computational efficiency. By modeling collective transition
probabilities rather than individual place fields, we offer a biologically
plausible, scalable framework for spatial navigation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [335] [Aneumo: A Large-Scale Multimodal Aneurysm Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks](https://arxiv.org/abs/2505.14717)
*Xigui Li,Yuanye Zhou,Feiyang Xiao,Xin Guo,Chen Jiang,Tan Pan,Xingmeng Zhang,Cenyu Liu,Zeyun Miao,Jianchao Ge,Xiansheng Wang,Qimeng Wang,Yichi Zhang,Wenbo Zhang,Fengping Zhu,Limei Han,Yuan Qi,Chensen Lin,Yuan Cheng*

Key words: 颅内动脉瘤,血流动力学,机器学习,CFD数据集

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该研究构建了一个大规模、高保真的颅内动脉瘤（IA）CFD数据集，用于开发高效的机器学习算法，以研究血流动力学对IA发展和破裂的影响。

Motivation: 传统的CFD方法计算复杂度高，难以应用于大规模或实时的临床评估，而当前IA风险评估方法忽视了血流动力学的影响。

Method: 通过对427个真实动脉瘤几何形状进行受控变形，合成了10,660个3D形状模拟IA演变，并在8种稳态质量流量条件下生成85,280个血流动力学数据。

Result: 数据集包括血流动力学参数、分割掩模和基准测试，可用于多模态数据输入的任务和当前建模方法的评估。

Conclusion: 该数据集旨在推动IA研究，并促进数据驱动方法在生物流体、生物医学工程和临床风险评估中的应用。

Abstract: Intracranial aneurysms (IAs) are serious cerebrovascular lesions found in
approximately 5\% of the general population. Their rupture may lead to high
mortality. Current methods for assessing IA risk focus on morphological and
patient-specific factors, but the hemodynamic influences on IA development and
rupture remain unclear. While accurate for hemodynamic studies, conventional
computational fluid dynamics (CFD) methods are computationally intensive,
hindering their deployment in large-scale or real-time clinical applications.
To address this challenge, we curated a large-scale, high-fidelity aneurysm CFD
dataset to facilitate the development of efficient machine learning algorithms
for such applications. Based on 427 real aneurysm geometries, we synthesized
10,660 3D shapes via controlled deformation to simulate aneurysm evolution. The
authenticity of these synthetic shapes was confirmed by neurosurgeons. CFD
computations were performed on each shape under eight steady-state mass flow
conditions, generating a total of 85,280 blood flow dynamics data covering key
parameters. Furthermore, the dataset includes segmentation masks, which can
support tasks that use images, point clouds or other multimodal data as input.
Additionally, we introduced a benchmark for estimating flow parameters to
assess current modeling methods. This dataset aims to advance aneurysm research
and promote data-driven approaches in biofluids, biomedical engineering, and
clinical risk assessment. The code and dataset are available at:
https://github.com/Xigui-Li/Aneumo.

</details>


### [336] [MedBLIP: Fine-tuning BLIP for Medical Image Captioning](https://arxiv.org/abs/2505.14726)
*Manshi Limbu,Diwita Banerjee*

Key words: 医学图像描述，BLIP模型，领域微调，ROCO数据集

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 论文探讨了如何通过微调BLIP模型在ROCO数据集上提升医学图像描述的准确性，并对比了不同模型的性能，发现领域特定微调显著提升了效果。

Motivation: 解决医学图像描述任务中现有视觉语言模型在专业领域表现不佳的问题。

Method: 微调BLIP模型，并与BLIP-2、ViT-GPT2等模型进行对比，同时分析了仅解码器微调的效果。

Result: 领域特定微调显著提升了性能，仅解码器微调在减少训练时间的同时保持了良好的表现。

Conclusion: 医学应用需要针对性适配，完整微调效果最佳，但仅解码器微调是高效选择。

Abstract: Medical image captioning is a challenging task that requires generating
clinically accurate and semantically meaningful descriptions of radiology
images. While recent vision-language models (VLMs) such as BLIP, BLIP2, Gemini
and ViT-GPT2 show strong performance on natural image datasets, they often
produce generic or imprecise captions when applied to specialized medical
domains. In this project, we explore the effectiveness of fine-tuning the BLIP
model on the ROCO dataset for improved radiology captioning. We compare the
fine-tuned BLIP against its zero-shot version, BLIP-2 base, BLIP-2 Instruct and
a ViT-GPT2 transformer baseline. Our results demonstrate that domain-specific
fine-tuning on BLIP significantly improves performance across both quantitative
and qualitative evaluation metrics. We also visualize decoder cross-attention
maps to assess interpretability and conduct an ablation study to evaluate the
contributions of encoder-only and decoder-only fine-tuning. Our findings
highlight the importance of targeted adaptation for medical applications and
suggest that decoder-only fine-tuning (encoder-frozen) offers a strong
performance baseline with 5% lower training time than full fine-tuning, while
full model fine-tuning still yields the best results overall.

</details>


### [337] [TransMedSeg: A Transferable Semantic Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2505.14753)
*Mengzhu Wang,Jiao Li,Shanshan Wang,Long Lan,Huibin Tan,Liang Yang,Guoli Yang*

Key words: 半监督学习, 医学图像分割, 可转移语义, 跨域对齐, 结构保留

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: TransMedSeg提出了一个可转移的语义框架，通过跨域分布匹配和域内结构保留，增强医学图像分割中的半监督学习效果。

Motivation: 当前半监督学习方法在医学图像分割中忽略了跨临床领域和成像模态的可转移语义关系，TransMedSeg旨在解决这一问题。

Method: 引入可转移语义增强（TSA）模块，通过跨域分布对齐和域内结构保留隐式增强特征表示，使用轻量级内存模块实现语义转换。

Result: TransMedSeg在医学图像数据集上表现优于现有半监督方法，为医学图像分析中的可转移表示学习开辟了新方向。

Conclusion: TransMedSeg通过隐式语义增强，显著提升了半监督医学图像分割的性能，并具有低计算开销的特点。

Abstract: Semi-supervised learning (SSL) has achieved significant progress in medical
image segmentation (SSMIS) through effective utilization of limited labeled
data. While current SSL methods for medical images predominantly rely on
consistency regularization and pseudo-labeling, they often overlook
transferable semantic relationships across different clinical domains and
imaging modalities. To address this, we propose TransMedSeg, a novel
transferable semantic framework for semi-supervised medical image segmentation.
Our approach introduces a Transferable Semantic Augmentation (TSA) module,
which implicitly enhances feature representations by aligning domain-invariant
semantics through cross-domain distribution matching and intra-domain
structural preservation. Specifically, TransMedSeg constructs a unified feature
space where teacher network features are adaptively augmented towards student
network semantics via a lightweight memory module, enabling implicit semantic
transformation without explicit data generation. Interestingly, this
augmentation is implicitly realized through an expected transferable
cross-entropy loss computed over the augmented teacher distribution. An upper
bound of the expected loss is theoretically derived and minimized during
training, incurring negligible computational overhead. Extensive experiments on
medical image datasets demonstrate that TransMedSeg outperforms existing
semi-supervised methods, establishing a new direction for transferable
representation learning in medical image analysis.

</details>


### [338] [SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning](https://arxiv.org/abs/2505.15234)
*Saqib Qamar,Mohd Fazil,Parvez Ahmad,Ghulam Muhammad*

Key words: 医学图像分割、SAMA-UNet、SSMs、自注意力、多尺度模块

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: SAMA-UNet提出了一种新型医学图像分割架构，通过SAMA块和CR-MSM模块解决现有模型的计算效率和信息捕获问题。

Motivation: 现有医学图像分割模型在计算效率和复杂数据建模上存在不足，而SSMs的应用受到图像令牌和自回归假设的限制。

Method: 采用SAMA块整合上下文自注意力和动态权重调制，并引入CR-MSM模块增强编码器与解码器间信息流。

Result: 在MRI、CT和内窥镜图像上的实验表明，SAMA-UNet在分割精度上优于CNN、Transformer和Mamba的现有方法。

Conclusion: SAMA-UNet通过创新的注意力机制和多尺度模块，有效提升了医学图像分割性能。

Abstract: Medical image segmentation plays an important role in various clinical
applications, but existing models often struggle with the computational
inefficiencies and challenges posed by complex medical data. State Space
Sequence Models (SSMs) have demonstrated promise in modeling long-range
dependencies with linear computational complexity, yet their application in
medical image segmentation remains hindered by incompatibilities with image
tokens and autoregressive assumptions. Moreover, it is difficult to achieve a
balance in capturing both local fine-grained information and global semantic
dependencies. To address these challenges, we introduce SAMA-UNet, a novel
architecture for medical image segmentation. A key innovation is the
Self-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates
contextual self-attention with dynamic weight modulation to prioritise the most
relevant features based on local and global contexts. This approach reduces
computational complexity and improves the representation of complex image
features across multiple scales. We also suggest the Causal-Resonance
Multi-Scale Module (CR-MSM), which enhances the flow of information between the
encoder and decoder by using causal resonance learning. This mechanism allows
the model to automatically adjust feature resolution and causal dependencies
across scales, leading to better semantic alignment between the low-level and
high-level features in U-shaped architectures. Experiments on MRI, CT, and
endoscopy images show that SAMA-UNet performs better in segmentation accuracy
than current methods using CNN, Transformer, and Mamba. The implementation is
publicly available at GitHub.

</details>


### [339] [Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction](https://arxiv.org/abs/2505.15285)
*Fengting Zhang,Boxu Liang,Qinghao Liu,Min Liu,Xiang Chen,Yaonan Wang*

Key words: 网格重建, 自适应模板, 深度学习, 解剖差异

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出了一种基于自适应模板的网格重建网络（ATMRN），通过生成适应性模板来克服传统单一固定模板的局限，显著提高了网格重建的精度。

Motivation: 传统网格重建方法依赖单一模板，无法捕捉个体间的解剖差异，影响重建保真度。

Method: 采用ATMRN生成自适应模板，随后进行变形，避免了固定模板的限制。

Result: 在OASIS数据集的皮层磁共振图像上验证，平均对称表面距离达0.267mm。

Conclusion: 该方法通用性强，可扩展到其他图像模态和解剖结构。

Abstract: Mesh reconstruction is a cornerstone process across various applications,
including in-silico trials, digital twins, surgical planning, and navigation.
Recent advancements in deep learning have notably enhanced mesh reconstruction
speeds. Yet, traditional methods predominantly rely on deforming a standardised
template mesh for individual subjects, which overlooks the unique anatomical
variations between them, and may compromise the fidelity of the
reconstructions. In this paper, we propose an adaptive-template-based mesh
reconstruction network (ATMRN), which generates adaptive templates from the
given images for the subsequent deformation, moving beyond the constraints of a
singular, fixed template. Our approach, validated on cortical magnetic
resonance (MR) images from the OASIS dataset, sets a new benchmark in
voxel-to-cortex mesh reconstruction, achieving an average symmetric surface
distance of 0.267mm across four cortical structures. Our proposed method is
generic and can be easily transferred to other image modalities and anatomical
structures.

</details>


### [340] [A Hybrid Quantum Classical Pipeline for X Ray Based Fracture Diagnosis](https://arxiv.org/abs/2505.14716)
*Sahil Tomar,Rajeshwar Tripathi,Sandeep Kumar*

Key words: 骨折诊断, 量子计算, PCA, 机器学习

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 提出了一种分布式混合量子经典管道，用于骨折X光图像分类，结合PCA和量子振幅编码，达到99%的准确率，同时减少特征提取时间82%。

Motivation: 传统X光解析耗时且易错，现有机器学习和深度学习方法需要大量特征工程和数据标注。本研究旨在解决这些问题。

Method: 采用分布式混合量子经典管道，先使用PCA降维，再通过4量子比特振幅编码电路增强特征，最终结合PCA和量子特征进行分类。

Result: 在公共多区域X光数据集上，分类准确率达到99%，与最先进的迁移学习模型相当，同时特征提取时间减少82%。

Conclusion: 该方法有效解决了传统方法的局限，显著提升了骨折诊断的效率和准确性。

Abstract: Bone fractures are a leading cause of morbidity and disability worldwide,
imposing significant clinical and economic burdens on healthcare systems.
Traditional X ray interpretation is time consuming and error prone, while
existing machine learning and deep learning solutions often demand extensive
feature engineering, large, annotated datasets, and high computational
resources. To address these challenges, a distributed hybrid quantum classical
pipeline is proposed that first applies Principal Component Analysis (PCA) for
dimensionality reduction and then leverages a 4 qubit quantum amplitude
encoding circuit for feature enrichment. By fusing eight PCA derived features
with eight quantum enhanced features into a 16 dimensional vector and then
classifying with different machine learning models achieving 99% accuracy using
a public multi region X ray dataset on par with state of the art transfer
learning models while reducing feature extraction time by 82%.

</details>


### [341] [LOD1 3D City Model from LiDAR: The Impact of Segmentation Accuracy on Quality of Urban 3D Modeling and Morphology Extraction](https://arxiv.org/abs/2505.14747)
*Fatemeh Chajaei,Hossein Bagheri*

Key words: 3D建筑重建, LiDAR数据, 深度学习, 语义分割, LOD1, 形态特征

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 研究评估了LiDAR数据在LOD1级别3D建筑重建中的潜力，比较了四种深度学习模型的表现，发现UNet3+和Attention U-Net表现最佳，并探讨了分割精度对3D建模质量及形态特征估计的影响。

Motivation: 3D建筑重建（尤其是LOD1级别）在城市规划、环境研究和交通网络优化中至关重要，研究旨在评估LiDAR数据在此过程中的潜力及其形态特征提取能力。

Method: 使用四种深度学习语义分割模型（U-Net、Attention U-Net、U-Net3+和DeepLabV3+）进行迁移学习，从LiDAR数据中提取建筑轮廓，并通过多种统计方法估计建筑高度，生成LOD1级别的3D模型。

Result: UNet3+和Attention U-Net表现最优，IoU分数分别为0.833和0.814。研究还发现分割精度显著影响3D建模质量和形态特征（如建筑面积和外墙面积）的估计准确性。

Conclusion: UNet3+结合第90百分位数和中位数方法能够准确估计建筑高度并提取形态特征，为3D建筑重建提供了有效工具。

Abstract: Three-dimensional reconstruction of buildings, particularly at Level of
Detail 1 (LOD1), plays a crucial role in various applications such as urban
planning, urban environmental studies, and designing optimized transportation
networks. This study focuses on assessing the potential of LiDAR data for
accurate 3D building reconstruction at LOD1 and extracting morphological
features from these models. Four deep semantic segmentation models, U-Net,
Attention U-Net, U-Net3+, and DeepLabV3+, were used, applying transfer learning
to extract building footprints from LiDAR data. The results showed that U-Net3+
and Attention U-Net outperformed the others, achieving IoU scores of 0.833 and
0.814, respectively. Various statistical measures, including maximum, range,
mode, median, and the 90th percentile, were used to estimate building heights,
resulting in the generation of 3D models at LOD1. As the main contribution of
the research, the impact of segmentation accuracy on the quality of 3D building
modeling and the accuracy of morphological features like building area and
external wall surface area was investigated. The results showed that the
accuracy of building identification (segmentation performance) significantly
affects the 3D model quality and the estimation of morphological features,
depending on the height calculation method. Overall, the UNet3+ method,
utilizing the 90th percentile and median measures, leads to accurate height
estimation of buildings and the extraction of morphological features.

</details>


### [342] [Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking](https://arxiv.org/abs/2505.14754)
*Andrey Alexandrov,Giovanni Acampora,Giovanni De Lellis,Antonia Di Crescenzo,Chiara Errico,Daria Morozova,Valeri Tioukov,Autilia Vittiello*

Key words: 轴向定位, 双焦平面, 深度学习, 卷积神经网络, 光学显微镜

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 论文提出了一种基于深度学习的双焦平面图像轴向定位方法，精度达40纳米，远超传统技术。

Motivation: 解决光学显微镜中高精度轴向定位的挑战。

Method: 使用卷积神经网络（CNN）从双焦平面图像中直接确定轴向位置。

Result: 轴向定位精度为40纳米，比传统方法提高6倍。

Conclusion: 该方法在多个领域具有广泛的应用潜力，展示了机器学习在科学应用中的灵活性和强大性能。

Abstract: Accurately tracking particles and determining their position along the
optical axis is a major challenge in optical microscopy, especially when
extremely high precision is needed. In this study, we introduce a deep learning
approach using convolutional neural networks (CNNs) that can determine axial
positions from dual-focal plane images without relying on predefined models.
Our method achieves an axial localization accuracy of 40 nanometers - six times
better than traditional single-focal plane techniques. The model's simple
design and strong performance make it suitable for a wide range of uses,
including dark matter detection, proton therapy for cancer, and radiation
protection in space. It also shows promise in fields like biological imaging,
materials science, and environmental monitoring. This work highlights how
machine learning can turn complex image data into reliable, precise
information, offering a flexible and powerful tool for many scientific
applications.

</details>


### [343] [Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images](https://arxiv.org/abs/2505.15120)
*Muniba Noreen,Furqan Shaukat*

Key words: 肺结节检测、自监督学习、DINOv2、transformer、医疗影像

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文提出了一种基于自监督学习的新方法LungNodule-SSM，用于无标注数据的肺结节检测与分类，通过DINOv2预训练和transformer架构微调，在LUNA 16数据集上达到98.37%的准确率。

Motivation: 早期肺结节检测对提高肺癌患者生存率至关重要，但标注医学影像数据的稀缺限制了计算机辅助诊断系统的开发。

Method: 方法分为两阶段：先通过DINOv2在无标注CT扫描上预训练学习鲁棒特征，再用transformer架构微调进行病灶检测与结节分类。

Result: 在LUNA 16数据集（888个CT扫描）上测试，准确率达98.37%，优于现有方法。

Conclusion: LungNodule-SSM方法在肺结节检测中表现出色，验证了自监督学习的有效性。

Abstract: Lung cancer remains among the deadliest types of cancer in recent decades,
and early lung nodule detection is crucial for improving patient outcomes. The
limited availability of annotated medical imaging data remains a bottleneck in
developing accurate computer-aided diagnosis (CAD) systems. Self-supervised
learning can help leverage large amounts of unlabeled data to develop more
robust CAD systems. With the recent advent of transformer-based architecture
and their ability to generalize to unseen tasks, there has been an effort
within the healthcare community to adapt them to various medical downstream
tasks. Thus, we propose a novel "LungNodule-SSM" method, which utilizes
selfsupervised learning with DINOv2 as a backbone to enhance lung nodule
detection and classification without annotated data. Our methodology has two
stages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn
robust feature representations, then secondly, these features are fine-tuned
using transformer-based architectures for lesionlevel detection and accurate
lung nodule diagnosis. The proposed method has been evaluated on the
challenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA
methods. Our experimental results show the superiority of our proposed method
with an accuracy of 98.37%, explaining its effectiveness in lung nodule
detection. The source code, datasets, and pre-processed data can be accessed
using the
link:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main

</details>


### [344] [Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart](https://arxiv.org/abs/2505.15488)
*Shubhrangshu Debsarkar,Bijoy Kundu*

Key words: 动态FDG PET, LSTM网络, 半自动化分割, 中点插值, 大鼠心脏成像

<details>
  <summary>Details</summary>

Main category: eess.IV

TL;DR: 该论文研究了通过动态FDG PET成像技术对52只大鼠（包括26只对照WKY大鼠和26只实验性SHR大鼠）进行纵向分析，开发了一种15参数双输出模型来校正图像误差。为解决手动标注的局限性，提出了半自动化分割和LSTM网络预测MCIF的方法，并在数据稀疏时采用中点插值以提高准确性。

Motivation: 传统动态FDG PET图像分析依赖手动标注和参数确定，效率低且易出错。本研究旨在通过半自动化和机器学习方法克服这些局限性。

Method: 使用半自动分割和LSTM网络预测MCIF，采用k折交叉验证和中点插值技术优化模型性能。

Result: 通过中点插值，模型的MSE提高了56.4%。

Conclusion: 提出的半自动化和LSTM方法显著提升了动态FDG PET图像分析的准确性和效率。

Abstract: Dynamic FDG PET imaging study of n = 52 rats including 26 control
Wistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats
(SHR) were performed using a Siemens microPET and Albira trimodal scanner
longitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual
output model correcting for spill over contamination and partial volume effects
with peak fitting cost functions was developed for simultaneous estimation of
model corrected blood input function (MCIF) and kinetic rate constants for
dynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are
its dependence on manual annotations for the Image Derived Input Function
(IDIF) and manual determination of crucial model parameters to compute MCIF. To
overcome these limitations, we performed semi-automated segmentation and then
formulated a Long-Short-Term Memory (LSTM) cell network to train and predict
MCIF in test data using a concatenation of IDIFs and myocardial inputs and
compared them with reference-modeled MCIF. Thresholding along 2D plane slices
with two thresholds, with T1 representing high-intensity myocardium, and T2
representing lower-intensity rings, was used to segment the area of the LV
blood pool. The resultant IDIF and myocardial TACs were used to compute the
corresponding reference (model) MCIF for all data sets. The segmented IDIF and
the myocardium formed the input for the LSTM network. A k-fold cross validation
structure with a 33:8:11 split and 5 folds was utilized to create the model and
evaluate the performance of the LSTM network for all datasets. To overcome the
sparseness of data as time steps increase, midpoint interpolation was utilized
to increase the density of datapoints beyond time = 10 minutes. The model
utilizing midpoint interpolation was able to achieve a 56.4% improvement over
previous Mean Squared Error (MSE).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [345] [CrypticBio: A Large Multimodal Dataset for Visually Confusing Biodiversity](https://arxiv.org/abs/2505.14707)
*Georgiana Manolache,Gerard Schouten,Joaquin Vanschoren*

Key words: CrypticBio, 多模态数据集, 视觉混淆物种, AI模型, 生物多样性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: CrypticBio是一个多模态数据集，专注于视觉上难以区分的物种，支持生物多样性研究中AI模型的开发。

Motivation: 现有数据集在处理形态相似的物种时规模小且单一，未能全面解决物种识别的挑战，CrypticBio填补了这一空白。

Method: 数据集整合了166百万张图像，覆盖52K个独特组别和67K种物种，提供丰富的研究级注释和多模态数据（如地理位置和时间）。

Result: 基准测试表明，地理上下文对视觉语言模型的零样本学习有显著影响，验证了数据集的有效性。

Conclusion: CrypticBio旨在推动生物多样性AI模型的进步，解决物种模糊性问题。

Abstract: We present CrypticBio, the largest publicly available multimodal dataset of
visually confusing species, specifically curated to support the development of
AI models in the context of biodiversity applications. Visually confusing or
cryptic species are groups of two or more taxa that are nearly
indistinguishable based on visual characteristics alone. While much existing
work addresses taxonomic identification in a broad sense, datasets that
directly address the morphological confusion of cryptic species are small,
manually curated, and target only a single taxon. Thus, the challenge of
identifying such subtle differences in a wide range of taxa remains
unaddressed. Curated from real-world trends in species misidentification among
community annotators of iNaturalist, CrypticBio contains 52K unique cryptic
groups spanning 67K species, represented in 166 million images. Rich
research-grade image annotations--including scientific, multicultural, and
multilingual species terminology, hierarchical taxonomy, spatiotemporal
context, and associated cryptic groups--address multimodal AI in biodiversity
research. For easy dataset curation, we provide an open-source pipeline
CrypticBio-Curate. The multimodal nature of the dataset beyond vision-language
arises from the integration of geographical and temporal data as complementary
cues to identifying cryptic species. To highlight the importance of the
dataset, we benchmark a suite of state-of-the-art foundation models across
CrypticBio subsets of common, unseen, endangered, and invasive species, and
demonstrate the substantial impact of geographical context on vision-language
zero-shot learning for cryptic species. By introducing CrypticBio, we aim to
catalyze progress toward real-world-ready biodiversity AI models capable of
handling the nuanced challenges of species ambiguity.

</details>


### [346] [DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance](https://arxiv.org/abs/2505.14708)
*Xuan Shen,Chenxia Han,Yufa Zhou,Yanyue Xie,Yifan Gong,Quanyi Wang,Yiwei Wang,Yanzhi Wang,Pu Zhao,Jiuxiang Gu*

Key words: 视频生成；扩散变换器；稀疏注意力；计算加速；GPU优化

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为DraftAttention的训练免费框架，通过动态稀疏注意力加速视频扩散变换器（DiTs）的计算。该方法通过压缩潜在空间中的降采样和重排序实现硬件优化的结构化稀疏性，显著提升生成速度和质量。

Motivation: 当前基于扩散变换器的视频生成模型（DiTs）因计算成本高昂（尤其是注意力机制占比超过80%延迟），严重限制其实际应用和扩展性。

Method: 提出DraftAttention框架，通过在压缩潜在空间中对特征图进行降采样，生成低分辨率草案注意力图，并基于此对查询、键和值进行重排序，实现硬件优化的稀疏注意力计算。

Result: 实验表明，该方法在视频生成质量上优于现有稀疏注意力方法，并在GPU上实现了高达1.75倍的端到端加速。

Conclusion: DraftAttention为视频扩散变换器提供了一种高效的训练免费加速方案，显著改善了实际应用可行性。

Abstract: Diffusion transformer-based video generation models (DiTs) have recently
attracted widespread attention for their excellent generation quality. However,
their computational cost remains a major bottleneck-attention alone accounts
for over 80% of total latency, and generating just 8 seconds of 720p video
takes tens of minutes-posing serious challenges to practical application and
scalability. To address this, we propose the DraftAttention, a training-free
framework for the acceleration of video diffusion transformers with dynamic
sparse attention on GPUs. We apply down-sampling to each feature map across
frames in the compressed latent space, enabling a higher-level receptive field
over the latent composed of hundreds of thousands of tokens. The low-resolution
draft attention map, derived from draft query and key, exposes redundancy both
spatially within each feature map and temporally across frames. We reorder the
query, key, and value based on the draft attention map to guide the sparse
attention computation in full resolution, and subsequently restore their
original order after the attention computation. This reordering enables
structured sparsity that aligns with hardware-optimized execution. Our
theoretical analysis demonstrates that the low-resolution draft attention
closely approximates the full attention, providing reliable guidance for
constructing accurate sparse attention. Experimental results show that our
method outperforms existing sparse attention approaches in video generation
quality and achieves up to 1.75x end-to-end speedup on GPUs. Code:
https://github.com/shawnricecake/draft-attention

</details>


### [347] [FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge](https://arxiv.org/abs/2505.14709)
*Xuan Shen,Weize Ma,Yufa Zhou,Enhao Tang,Yanyue Xie,Zhengang Li,Yifan Gong,Quanyi Wang,Henghui Ding,Yiwei Wang,Yanzhi Wang,Pu Zhao,Jun Lin,Jiuxiang Gu*

Key words: AR模型、视频生成、时间冗余、TAS、FPGA、硬件加速

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: FastCar框架通过利用时间冗余加速AR视频生成的解码阶段，提出TAS和硬件加速器，显著提升解码速度和能效。

Motivation: 视频生成需要大量token，导致解码阶段计算开销大，且MLP模块的推理延迟高，相邻帧存在时间冗余。

Method: 提出FastCar框架，利用TAS确定是否重用缓存的MLP输出以减少冗余计算，并开发FPGA硬件加速器。

Result: FastCar实现2.1倍解码加速，能效更高，结合稀疏注意力可缓解漂移问题，适用于高分辨率和长视频生成。

Conclusion: FastCar通过时间冗余和硬件优化显著提升视频生成效率，优于传统稀疏注意力方法。

Abstract: Auto-regressive (AR) models, initially successful in language generation,
have recently shown promise in visual generation tasks due to their superior
sampling efficiency. Unlike image generation, video generation requires a
substantially larger number of tokens to produce coherent temporal frames,
resulting in significant overhead during the decoding phase. Our key
observations are: (i) MLP modules in the decode phase dominate the inference
latency, and (ii) there exists high temporal redundancy in MLP outputs of
adjacent frames. In this paper, we propose the \textbf{FastCar} framework to
accelerate the decode phase for the AR video generation by exploring the
temporal redundancy. The Temporal Attention Score (TAS) is proposed to
determine whether to apply the replay strategy (\textit{i.e.}, reusing cached
MLP outputs from the previous frame to reduce redundant computations) with
detailed theoretical analysis and justification. Also, we develop a hardware
accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to
enable better resource utilization and faster inference. Experimental results
demonstrate the effectiveness of our method, which outperforms traditional
sparse attention approaches with more than 2.1x decoding speedup and higher
energy efficiency on the edge. Furthermore, by combining FastCar and sparse
attention, FastCar can boost the performance of sparse attention with
alleviated drifting, demonstrating our unique advantages for high-resolution
and long-duration video generation. Code:
https://github.com/shawnricecake/fast-car

</details>


### [348] [KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection](https://arxiv.org/abs/2505.14714)
*Tuan-Vinh La,Minh-Hieu Nguyen,Minh-Son Dao*

Key words: 假新闻检测、多模态融合、知识图谱、Transformer、语义验证

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种多模态假新闻检测框架，融合视觉、文本和知识表示，通过细粒度对象细节和知识图谱增强语义理解，实验效果优于现有方法。

Motivation: 假新闻检测面临文本误导、图像操纵和外部知识推理的复杂交互问题，现有方法忽略局部对象细节和外部知识利用。

Method: 结合自底向上注意力捕捉对象细节、CLIP提取全局图像语义、RoBERTa编码文本，并通过知识图谱检索和选择相关实体，通过Transformer分类器预测新闻真实性。

Result: 实验表明，该模型优于现有方法，验证了邻居选择机制和多模态融合的有效性。

Conclusion: 该框架将假新闻检测从特征融合转向语义验证，提出了知识驱动的多模态推理新范式。

Abstract: Fake news detection remains a challenging problem due to the complex
interplay between textual misinformation, manipulated images, and external
knowledge reasoning. While existing approaches have achieved notable results in
verifying veracity and cross-modal consistency, two key challenges persist: (1)
Existing methods often consider only the global image context while neglecting
local object-level details, and (2) they fail to incorporate external knowledge
and entity relationships for deeper semantic understanding. To address these
challenges, we propose a novel multi-modal fake news detection framework that
integrates visual, textual, and knowledge-based representations. Our approach
leverages bottom-up attention to capture fine-grained object details, CLIP for
global image semantics, and RoBERTa for context-aware text encoding. We further
enhance knowledge utilization by retrieving and adaptively selecting relevant
entities from a knowledge graph. The fused multi-modal features are processed
through a Transformer-based classifier to predict news veracity. Experimental
results demonstrate that our model outperforms recent approaches, showcasing
the effectiveness of neighbor selection mechanism and multi-modal fusion for
fake news detection. Our proposal introduces a new paradigm: knowledge-grounded
multimodal reasoning. By integrating explicit entity-level selection and
NLI-guided filtering, we shift fake news detection from feature fusion to
semantically grounded verification. For reproducibility and further research,
the source code is publicly at
\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.

</details>


### [349] [Enhancing Shape Perception and Segmentation Consistency for Industrial Image Inspection](https://arxiv.org/abs/2505.14718)
*Guoxuan Mao,Ting Cao,Ziyang Li,Yuan Dong*

Key words: 语义分割, 工业图像检测, SPENet, 分割一致性, 计算效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为SPENet的形状感知高效网络，用于解决工业图像检测中传统语义分割模型在多变环境下对固定组件分割一致的不足，同时降低计算复杂度。

Motivation: 工业图像检测中，传统语义分割模型因缺乏对物体轮廓的感知，无法在不同环境下保持固定组件的分割一致性，且需满足实时性和有限计算能力的需求。

Method: SPENet通过分别监督图像边界和主体信息的提取，引入模糊边界描述的Variable Boundary Domain (VBD)方法，并提出新的度量Consistency Mean Square Error (CMSE)。

Result: SPENet在分割准确性和CMSE上表现优异，比先前最佳模型的CMSE降低超50%，并在数据集中达到最佳分割精度和竞争性速度。

Conclusion: SPENet在工业图像检测中实现了高效且一致的分割，显著优于现有实时分割网络。

Abstract: Semantic segmentation stands as a pivotal research focus in computer vision.
In the context of industrial image inspection, conventional semantic
segmentation models fail to maintain the segmentation consistency of fixed
components across varying contextual environments due to a lack of perception
of object contours. Given the real-time constraints and limited computing
capability of industrial image detection machines, it is also necessary to
create efficient models to reduce computational complexity. In this work, a
Shape-Aware Efficient Network (SPENet) is proposed, which focuses on the shapes
of objects to achieve excellent segmentation consistency by separately
supervising the extraction of boundary and body information from images. In
SPENet, a novel method is introduced for describing fuzzy boundaries to better
adapt to real-world scenarios named Variable Boundary Domain (VBD).
Additionally, a new metric, Consistency Mean Square Error(CMSE), is proposed to
measure segmentation consistency for fixed components. Our approach attains the
best segmentation accuracy and competitive speed on our dataset, showcasing
significant advantages in CMSE among numerous state-of-the-art real-time
segmentation networks, achieving a reduction of over 50% compared to the
previously top-performing models.

</details>


### [350] [MSVIT: Improving Spiking Vision Transformer Using Multi-scale Attention Fusion](https://arxiv.org/abs/2505.14719)
*Wei Hua,Chenlin Zhou,Jibin Wu,Yansong Chua,Yangyang Shu*

Key words: Spiking Neural Networks, Vision Transformer, MSVIT, Multi-Scale Spiking Attention

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了MSVIT，一种新型的脉冲驱动Transformer架构，通过多尺度脉冲注意力（MSSA）提升性能，成为SNN-transformer架构的先进解决方案。

Motivation: Spiking Neural Networks（SNNs）与Vision Transformer结合具有高效能和低能耗的潜力，但SNN-based与ANN-based Transformer间存在性能差距，且现有方法在提取多尺度图像特征方面存在瓶颈。

Method: 提出MSVIT，首次采用多尺度脉冲注意力（MSSA）增强脉冲注意力块的能力。

Result: MSVIT在多个数据集上表现优于现有SNN-based模型，成为SNN-transformer架构的最先进解决方案。

Conclusion: MSVIT通过多尺度脉冲注意力解决了现有方法的瓶颈，显著提升了SNN-transformer架构的性能。

Abstract: The combination of Spiking Neural Networks(SNNs) with Vision Transformer
architectures has attracted significant attention due to the great potential
for energy-efficient and high-performance computing paradigms. However, a
substantial performance gap still exists between SNN-based and ANN-based
transformer architectures. While existing methods propose spiking
self-attention mechanisms that are successfully combined with SNNs, the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting features from different image scales. In this paper, we address this
issue and propose MSVIT, a novel spike-driven Transformer architecture, which
firstly uses multi-scale spiking attention (MSSA) to enrich the capability of
spiking attention blocks. We validate our approach across various main data
sets. The experimental results show that MSVIT outperforms existing SNN-based
models, positioning itself as a state-of-the-art solution among SNN-transformer
architectures. The codes are available at
https://github.com/Nanhu-AI-Lab/MSViT.

</details>


### [351] [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/abs/2505.14728)
*Xiao Lin,Zhining Liu,Ze Yang,Gaotang Li,Ruizhong Qiu,Shuke Wang,Hui Liu,Haotian Li,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Key words: 视觉语言模型, 道德对齐, MORALISE, 基准, 多模态

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了MORALISE基准，用于评估视觉语言模型的道德对齐，通过专家验证的真实数据克服现有方法的偏差和局限性。

Motivation: 视觉语言模型在道德敏感领域的广泛应用需要确保其输出符合人类道德价值观，但现有研究多局限于文本或AI生成图像，缺乏真实性和多样性。

Method: 基于Turiel的领域理论，提出了13个道德主题分类，并手动标注了2,481个高质量图像-文本对，包含主题和模态注释。评估任务包括道德判断和道德规范归因。

Result: 在19个流行的开放和闭源视觉语言模型上的实验表明，MORALISE对现有模型提出了显著挑战，揭示了其道德局限性。

Conclusion: MORALISE是一个全面的基准，能够有效评估视觉语言模型的道德对齐能力，为未来研究提供了重要工具。

Abstract: Warning: This paper contains examples of harmful language and images. Reader
discretion is advised. Recently, vision-language models have demonstrated
increasing influence in morally sensitive domains such as autonomous driving
and medical analysis, owing to their powerful multimodal reasoning
capabilities. As these models are deployed in high-stakes real-world
applications, it is of paramount importance to ensure that their outputs align
with human moral values and remain within moral boundaries. However, existing
work on moral alignment either focuses solely on textual modalities or relies
heavily on AI-generated images, leading to distributional biases and reduced
realism. To overcome these limitations, we introduce MORALISE, a comprehensive
benchmark for evaluating the moral alignment of vision-language models (VLMs)
using diverse, expert-verified real-world data. We begin by proposing a
comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,
spanning the personal, interpersonal, and societal moral domains encountered in
everyday life. Built on this framework, we manually curate 2,481 high-quality
image-text pairs, each annotated with two fine-grained labels: (1) topic
annotation, identifying the violated moral topic(s), and (2) modality
annotation, indicating whether the violation arises from the image or the text.
For evaluation, we encompass two tasks, \textit{moral judgment} and
\textit{moral norm attribution}, to assess models' awareness of moral
violations and their reasoning ability on morally salient content. Extensive
experiments on 19 popular open- and closed-source VLMs show that MORALISE poses
a significant challenge, revealing persistent moral limitations in current
state-of-the-art models. The full benchmark is publicly available at
https://huggingface.co/datasets/Ze1025/MORALISE.

</details>


### [352] [Leveraging Generative AI Models to Explore Human Identity](https://arxiv.org/abs/2505.14843)
*Yunha Yeo,Daeho Um*

Key words: human identity, diffusion models, AI, generative models, external factors

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文通过扩散模型间接探索人类身份，利用AI生成人脸图像并与身份形成过程建立对应关系，验证了外部因素对人类身份的影响，并创作了表达身份流动性的视频作品。

Motivation: 通过AI生成模型（扩散模型）间接研究人类身份的形成及其对外部因素的依赖性。

Method: 采用扩散模型生成人脸图像，并将其生成过程与人类身份形成过程对应，通过输入变化观察生成图像的变化。

Result: 实验表明外部输入的变化导致生成人脸图像的显著变化，间接证实了人类身份对外部因素的依赖性。

Conclusion: 人类身份具有流动性，受外部因素影响；通过艺术作品进一步表达了这一观点。

Abstract: This paper attempts to explore human identity by utilizing neural networks in
an indirect manner. For this exploration, we adopt diffusion models,
state-of-the-art AI generative models trained to create human face images. By
relating the generated human face to human identity, we establish a
correspondence between the face image generation process of the diffusion model
and the process of human identity formation. Through experiments with the
diffusion model, we observe that changes in its external input result in
significant changes in the generated face image. Based on the correspondence,
we indirectly confirm the dependence of human identity on external factors in
the process of human identity formation. Furthermore, we introduce
\textit{Fluidity of Human Identity}, a video artwork that expresses the fluid
nature of human identity affected by varying external factors. The video is
available at
https://www.behance.net/gallery/219958453/Fluidity-of-Human-Identity?.

</details>


### [353] [Colors Matter: AI-Driven Exploration of Human Feature Colors](https://arxiv.org/abs/2505.14931)
*Rama Alyoubi,Taif Alharbi,Albatul Alghamdi,Yara Alshehri,Elham Alghamdi*

Key words: 机器学习, 颜色分类, 特征提取, 色调分析, 视觉分析

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究提出了一个结合先进成像技术和机器学习的框架，用于提取和分类人类关键属性（如肤色、发色、虹膜颜色和静脉色底）。通过多阶段流程和多种颜色空间转换方法，系统在色调分类中达到了80%的准确率。

Motivation: 研究旨在解决人类关键属性（如肤色、发色等）的精确分类问题，通过AI技术提升颜色分析的准确性和包容性，支持美容技术、数字个性化和视觉分析等应用。

Method: 系统采用多阶段流程，包括人脸检测、区域分割和主色提取。使用X-means聚类和Delta E（CIEDE2000）等距离度量，结合LAB和HSV颜色空间进行颜色区分。通过手腕图像分析静脉色底，将主色与自定义色调尺度匹配。

Result: 系统在Delta E-HSV方法结合高斯模糊的条件下，实现了高达80%的色调分类准确率，在不同光照和图像条件下表现稳定。

Conclusion: 该研究展示了AI驱动的颜色分析和特征提取在精确分类中的潜力，为美容技术、数字个性化和视觉分析等领域提供了可靠支持。

Abstract: This study presents a robust framework that leverages advanced imaging
techniques and machine learning for feature extraction and classification of
key human attributes-namely skin tone, hair color, iris color, and vein-based
undertones. The system employs a multi-stage pipeline involving face detection,
region segmentation, and dominant color extraction to isolate and analyze these
features. Techniques such as X-means clustering, alongside perceptually uniform
distance metrics like Delta E (CIEDE2000), are applied within both LAB and HSV
color spaces to enhance the accuracy of color differentiation. For
classification, the dominant tones of the skin, hair, and iris are extracted
and matched to a custom tone scale, while vein analysis from wrist images
enables undertone classification into "Warm" or "Cool" based on LAB
differences. Each module uses targeted segmentation and color space
transformations to ensure perceptual precision. The system achieves up to 80%
accuracy in tone classification using the Delta E-HSV method with Gaussian
blur, demonstrating reliable performance across varied lighting and image
conditions. This work highlights the potential of AI-powered color analysis and
feature extraction for delivering inclusive, precise, and nuanced
classification, supporting applications in beauty technology, digital
personalization, and visual analytics.

</details>


### [354] [Programmatic Video Prediction Using Large Language Models](https://arxiv.org/abs/2505.14948)
*Hao Tang,Kevin Ellis,Suhas Lohit,Michael J. Jones,Moitreya Chatterjee*

Key words: 视频预测,神经符号化,LLM/VLM,可解释性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ProgGen利用神经符号化方法预测视频帧，通过LLM/VLM生成可解释程序，优于现有技术。

Motivation: 为视频监控、机器人应用和自动驾驶等场景，预测未来视觉动态至关重要，需从少量帧生成合理未来帧。

Method: ProgGen用LLM/VLM生成程序：(1)估计当前视频状态，(2)预测未来状态动态，(3)渲染为RGB帧。

Result: 在PhyWorld和Cart Pole环境中表现优于竞品，支持反事实推理和可解释视频生成。

Conclusion: ProgGen在视频生成任务中高效通用，兼具可解释性。

Abstract: The task of estimating the world model describing the dynamics of a real
world process assumes immense importance for anticipating and preparing for
future outcomes. For applications such as video surveillance, robotics
applications, autonomous driving, etc. this objective entails synthesizing
plausible visual futures, given a few frames of a video to set the visual
context. Towards this end, we propose ProgGen, which undertakes the task of
video frame prediction by representing the dynamics of the video using a set of
neuro-symbolic, human-interpretable set of states (one per frame) by leveraging
the inductive biases of Large (Vision) Language Models (LLM/VLM). In
particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate
the states of the video, given the visual context (i.e. the frames); (ii) to
predict the states corresponding to future time steps by estimating the
transition dynamics; (iii) to render the predicted states as visual RGB-frames.
Empirical evaluations reveal that our proposed method outperforms competing
techniques at the task of video frame prediction in two challenging
environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits
counter-factual reasoning and interpretable video generation attesting to its
effectiveness and generalizability for video generation tasks.

</details>


### [355] [Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation](https://arxiv.org/abs/2505.15077)
*Alessandro dos Santos Ferreira,Ana Paula Marques Ramos,José Marcato Junior,Wesley Nunes Gonçalves*

Key words: 城市森林, 树木检测, GAN, 扩散模型, 域适应

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种结合GAN和扩散模型的新方法，通过提升低分辨率航空图像质量，无需大量标注数据即可实现有效的树木分割。

Motivation: 城市森林对环境质量和生物多样性至关重要，但树木检测因图像分辨率差异而具有挑战性。传统方法需要大量人工标注数据，成本高昂且难以获取。

Method: 提出了一种新流程，结合域适应与GAN、扩散模型（如pix2pix、Real-ESRGAN、Latent Diffusion、Stable Diffusion），生成高质量合成样本，扩展训练数据并统一尺度。

Result: 实验结果显示，低分辨率图像的IoU提升了50%以上，优于传统方法。

Conclusion: 该方法不仅提升了分割模型的鲁棒性，还为标注资源稀缺的遥感场景提供了可扩展的解决方案。

Abstract: Urban forests play a key role in enhancing environmental quality and
supporting biodiversity in cities. Mapping and monitoring these green spaces
are crucial for urban planning and conservation, yet accurately detecting trees
is challenging due to complex landscapes and the variability in image
resolution caused by different satellite sensors or UAV flight altitudes. While
deep learning architectures have shown promise in addressing these challenges,
their effectiveness remains strongly dependent on the availability of large and
manually labeled datasets, which are often expensive and difficult to obtain in
sufficient quantity. In this work, we propose a novel pipeline that integrates
domain adaptation with GANs and Diffusion models to enhance the quality of
low-resolution aerial images. Our proposed pipeline enhances low-resolution
imagery while preserving semantic content, enabling effective tree segmentation
without requiring large volumes of manually annotated data. Leveraging models
such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we
generate realistic and structurally consistent synthetic samples that expand
the training dataset and unify scale across domains. This approach not only
improves the robustness of segmentation models across different acquisition
conditions but also provides a scalable and replicable solution for remote
sensing scenarios with scarce annotation resources. Experimental results
demonstrated an improvement of over 50% in IoU for low-resolution images,
highlighting the effectiveness of our method compared to traditional pipelines.

</details>


### [356] [iPad: Iterative Proposal-centric End-to-End Autonomous Driving](https://arxiv.org/abs/2505.15111)
*Ke Guo,Haochen Liu,Xiaojun Wu,Jia Pan,Chen Lv*

Key words: 端到端自动驾驶、鸟瞰图、提案优化、轻量级任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一个名为iPad的新型端到端自动驾驶框架，通过迭代优化的候选计划和轻量级辅助任务提高效率和规划质量。

Motivation: 解决现有端到端自动驾驶方法基于密集鸟瞰图特征直接生成计划导致的效率低下和规划感知有限的问题。

Method: 提出iPad框架，核心是ProFormer编码器，通过提案锚定注意力迭代优化候选计划及其相关特征，并引入轻量级辅助任务（映射和预测）。

Result: 在NAVSIM和CARLA Bench2Drive基准测试中表现最佳，效率显著优于现有方法。

Conclusion: iPad框架通过提案中心化设计和轻量级任务提升了自动驾驶系统的效率和性能。

Abstract: End-to-end (E2E) autonomous driving systems offer a promising alternative to
traditional modular pipelines by reducing information loss and error
accumulation, with significant potential to enhance both mobility and safety.
However, most existing E2E approaches directly generate plans based on dense
bird's-eye view (BEV) grid features, leading to inefficiency and limited
planning awareness. To address these limitations, we propose iterative
Proposal-centric autonomous driving (iPad), a novel framework that places
proposals - a set of candidate future plans - at the center of feature
extraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder
that iteratively refines proposals and their associated features through
proposal-anchored attention, effectively fusing multi-view image data.
Additionally, we introduce two lightweight, proposal-centric auxiliary tasks -
mapping and prediction - that improve planning quality with minimal
computational overhead. Extensive experiments on the NAVSIM and CARLA
Bench2Drive benchmarks demonstrate that iPad achieves state-of-the-art
performance while being significantly more efficient than prior leading
methods.

</details>


### [357] [Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding](https://arxiv.org/abs/2505.15123)
*Ta Duc Huy,Duy Anh Huynh,Yutong Xie,Yuankai Qi,Qi Chen,Phi Le Nguyen,Sen Kim Tran,Son Lam Phung,Anton van den Hengel,Zhibin Liao,Minh-Son To,Johan W. Verjans,Vu Minh Hieu Phan*

Key words: 视觉定位,医学影像,注意力机制,DAP,深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种简单有效的Disease-Aware Prompting（DAP）方法，用于改善医学影像中的视觉定位问题，显著提升了准确性。

Motivation: 当前视觉语言模型（VLM）在医学影像中难以关联文本描述与疾病区域，主要由于注意力机制效率低和缺乏细粒度表征。

Method: 提出DAP方法，利用VLM的可解释性图来识别合适的图像特征，增强疾病相关区域并抑制背景干扰。

Result: 在不需额外标注的情况下，DAP在三个主要胸部X光数据集上将视觉定位准确率提升了20.74%。

Conclusion: DAP显著提升了医学影像中视觉定位的性能，增强了模型的透明度和可信度。

Abstract: Visual grounding (VG) is the capability to identify the specific regions in
an image associated with a particular text description. In medical imaging, VG
enhances interpretability by highlighting relevant pathological features
corresponding to textual descriptions, improving model transparency and
trustworthiness for wider adoption of deep learning models in clinical
practice. Current models struggle to associate textual descriptions with
disease regions due to inefficient attention mechanisms and a lack of
fine-grained token representations. In this paper, we empirically demonstrate
two key observations. First, current VLMs assign high norms to background
tokens, diverting the model's attention from regions of disease. Second, the
global tokens used for cross-modal learning are not representative of local
disease tokens. This hampers identifying correlations between the text and
disease tokens. To address this, we introduce simple, yet effective
Disease-Aware Prompting (DAP) process, which uses the explainability map of a
VLM to identify the appropriate image features. This simple strategy amplifies
disease-relevant regions while suppressing background interference. Without any
additional pixel-level annotations, DAP improves visual grounding accuracy by
20.74% compared to state-of-the-art methods across three major chest X-ray
datasets.

</details>


### [358] [DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer](https://arxiv.org/abs/2505.15133)
*Haiduo Huang,Jiangcheng Song,Yadong Zhang,Pengju Ren*

Key words: 知识蒸馏, 梯度冲突, 信噪比, 动态top-K掩码, 解耦

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: DeepKD是一个新颖的训练框架，通过双级解耦和自适应去噪技术解决知识蒸馏中的梯度冲突和噪声问题。

Motivation: 现有方法在知识蒸馏中忽视了目标类和非目标类知识流的内在冲突，且低置信度的暗知识引入噪声信号，影响知识转移效果。

Method: 通过梯度信噪比（GSNR）分析设计了独立的动量更新器，并引入动态top-K掩码机制（DTM）逐步过滤低置信度知识。

Result: 在CIFAR-100、ImageNet和MS-COCO等数据集上的实验证明了DeepKD的有效性。

Conclusion: DeepKD通过解耦和去噪显著提升了知识蒸馏的效果。

Abstract: Recent advances in knowledge distillation have emphasized the importance of
decoupling different knowledge components. While existing methods utilize
momentum mechanisms to separate task-oriented and distillation gradients, they
overlook the inherent conflict between target-class and non-target-class
knowledge flows. Furthermore, low-confidence dark knowledge in non-target
classes introduces noisy signals that hinder effective knowledge transfer. To
address these limitations, we propose DeepKD, a novel training framework that
integrates dual-level decoupling with adaptive denoising. First, through
theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics
in task-oriented and non-task-oriented knowledge distillation, we design
independent momentum updaters for each component to prevent mutual
interference. We observe that the optimal momentum coefficients for
task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class
gradient (NCG) should be positively related to their GSNR. Second, we introduce
a dynamic top-k mask (DTM) mechanism that gradually increases K from a small
initial value to incorporate more non-target classes as training progresses,
following curriculum learning principles. The DTM jointly filters
low-confidence logits from both teacher and student models, effectively
purifying dark knowledge during early training. Extensive experiments on
CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code
is available at https://github.com/haiduo/DeepKD.

</details>


### [359] [AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection](https://arxiv.org/abs/2505.15173)
*Zhipei Xu,Xuanyu Zhang,Xing Zhou,Jian Zhang*

Key words: AIGC, 视频检测, MLLM, GRPO, 双编码器, FakeHumanVid, 伪造检测

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: AvatarShield 是一种基于 MLLM 的可解释框架，用于检测以人为中心的假视频，通过创新的奖励机制和双编码器架构，显著提升了检测性能。

Motivation: AIGC 技术的快速发展带来了虚假视频的威胁，现有方法在以人为本的视频检测中表现不足，亟需改进。

Method: 提出 AvatarShield 框架，结合 GRPO 奖励机制和双编码器架构，实现高效的伪造检测。

Result: 在 FakeHumanVid 基准测试中，AvatarShield 在域内和跨域检测中均优于现有方法。

Conclusion: AvatarShield 为以人为本的视频取证设定了新标准。

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
technologies, particularly in video generation, has led to unprecedented
creative capabilities but also increased threats to information integrity,
identity security, and public trust. Existing detection methods, while
effective in general scenarios, lack robust solutions for human-centric videos,
which pose greater risks due to their realism and potential for legal and
ethical misuse. Moreover, current detection approaches often suffer from poor
generalization, limited scalability, and reliance on labor-intensive supervised
fine-tuning. To address these challenges, we propose AvatarShield, the first
interpretable MLLM-based framework for detecting human-centric fake videos,
enhanced via Group Relative Policy Optimization (GRPO). Through our carefully
designed accuracy detection reward and temporal compensation reward, it
effectively avoids the use of high-cost text annotation data, enabling precise
temporal modeling and forgery detection. Meanwhile, we design a dual-encoder
architecture, combining high-level semantic reasoning and low-level artifact
amplification to guide MLLMs in effective forgery detection. We further collect
FakeHumanVid, a large-scale human-centric video benchmark that includes
synthesis methods guided by pose, audio, and text inputs, enabling rigorous
evaluation of detection methods in real-world scenes. Extensive experiments
show that AvatarShield significantly outperforms existing approaches in both
in-domain and cross-domain detection, setting a new standard for human-centric
video forensics.

</details>


### [360] [Intentional Gesture: Deliver Your Intentions with Gestures for Speech](https://arxiv.org/abs/2505.15197)
*Pinxin Liu,Haiyang Liu,Luchuan Song,Chenliang Xu*

Key words: 手势生成、交际意图、数字人类、具身AI、语义丰富

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文提出了一种名为Intentional-Gesture的新框架，通过结合高层交际意图来实现语义丰富且时间同步的手势生成，解决了现有方法忽略交际意图的问题。

Motivation: 现有手势生成方法仅依赖浅层语言线索（如语音或文本），忽略交际意图，导致生成手势虽与语音节奏同步但语义浅薄。本研究旨在填补这一空白。

Method: 提出Intentional-Gesture框架，基于高层交际功能（意图）生成手势。通过增强BEAT-2数据集并利用视觉语言模型自动标注意图，开发Intentional Gesture Motion Tokenizer以注入意图信息。

Result: 在BEAT-2基准测试中取得了最先进的性能，生成了时间对齐且语义丰富的手势。

Conclusion: 该框架为数字人类和具身AI中的手势生成提供了模块化基础，显著提升了语义表达能力。

Abstract: When humans speak, gestures help convey communicative intentions, such as
adding emphasis or describing concepts. However, current co-speech gesture
generation methods rely solely on superficial linguistic cues (\textit{e.g.}
speech audio or text transcripts), neglecting to understand and leverage the
communicative intention that underpins human gestures. This results in outputs
that are rhythmically synchronized with speech but are semantically shallow. To
address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework
that casts gesture generation as an intention-reasoning task grounded in
high-level communicative functions. % First, we curate the \textbf{InG} dataset
by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text
sentences summarizing intentions), which are automatically annotated using
large vision-language models. Next, we introduce the \textbf{Intentional
Gesture Motion Tokenizer} to leverage these intention annotations. It injects
high-level communicative functions (\textit{e.g.}, intentions) into tokenized
motion representations to enable intention-aware gesture synthesis that are
both temporally aligned and semantically meaningful, achieving new
state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a
modular foundation for expressive gesture generation in digital humans and
embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture

</details>


### [361] [Zero-Shot Gaze-based Volumetric Medical Image Segmentation](https://arxiv.org/abs/2505.15256)
*Tatyana Shmykova,Leila Khaertdinova,Ilya Pershin*

Key words: 医学图像分割、眼动追踪、交互式分割、SAM-2、MedSAM-2

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种利用眼球追踪技术（眼动）作为交互式3D医学图像分割的新输入方式，并与传统的边界框提示进行了比较。

Motivation: 医学图像分割在临床应用中至关重要，但现有方法依赖手动提示（如边界框或点击），效率较低。研究探索了眼动提示作为一种更高效的交互方式。

Method: 使用SAM-2和MedSAM-2模型，结合合成和真实眼动数据，评估眼动提示的分割性能。

Result: 眼动提示在时间效率上优于边界框，但分割质量略低。

Conclusion: 眼动可以作为3D医学图像分割的补充输入方式，具有潜力。

Abstract: Accurate segmentation of anatomical structures in volumetric medical images
is crucial for clinical applications, including disease monitoring and cancer
treatment planning. Contemporary interactive segmentation models, such as
Segment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on
manually provided prompts like bounding boxes and mouse clicks. In this study,
we introduce eye gaze as a novel informational modality for interactive
segmentation, marking the application of eye-tracking for 3D medical image
segmentation. We evaluate the performance of using gaze-based prompts with
SAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to
bounding boxes, gaze-based prompts offer a time-efficient interaction approach
with slightly lower segmentation quality. Our findings highlight the potential
of using gaze as a complementary input modality for interactive 3D medical
image segmentation.

</details>


### [362] [Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs](https://arxiv.org/abs/2505.15265)
*Zihao Pan,Yu Tong,Weibin Wu,Jingyi Wang,Lifeng Chen,Zhe Zhao,Jiajia Wei,Yitong Qiao,Zibin Zheng*

Key words: 对抗攻击,语义演化,视觉语言模型,鲁棒性

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的语义演化框架，通过结合大型语言模型（LLM）和文本生成图像模型（T2I），搜索大型视觉语言模型（LVLM）对特定语义概念的敏感区域，以提高模型鲁棒性。

Motivation: 探究LVLM在特定语义概念下的幻觉和错误行为，以帮助研究人员针对性提升模型鲁棒性。

Method: 结合LLM和T2I模型，通过语义交叉和突变生成图像描述，转换为视觉输入并量化LVLM的性能作为反馈信号。

Result: 在七种主流LVLM和两项多模态任务上的实验验证了方法的有效性，并揭示了LVLM的敏感语义。

Conclusion: 该方法不仅实用，还为研究LVLM的敏感性提供了有趣见解。

Abstract: Adversarial attacks aim to generate malicious inputs that mislead deep
models, but beyond causing model failure, they cannot provide certain
interpretable information such as ``\textit{What content in inputs make models
more likely to fail?}'' However, this information is crucial for researchers to
specifically improve model robustness. Recent research suggests that models may
be particularly sensitive to certain semantics in visual inputs (such as
``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this
paper we conducted the first exploration on large vision-language models
(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and
various errors when facing specific semantic concepts in images. To efficiently
search for these sensitive concepts, we integrated large language models (LLMs)
and text-to-image (T2I) models to propose a novel semantic evolution framework.
Randomly initialized semantic concepts undergo LLM-based crossover and mutation
operations to form image descriptions, which are then converted by T2I models
into visual inputs for LVLMs. The task-specific performance of LVLMs on each
input is quantified as fitness scores for the involved semantics and serves as
reward signals to further guide LLMs in exploring concepts that induce LVLMs.
Extensive experiments on seven mainstream LVLMs and two multimodal tasks
demonstrate the effectiveness of our method. Additionally, we provide
interesting findings about the sensitive semantics of LVLMs, aiming to inspire
further in-depth research.

</details>


### [363] [BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution](https://arxiv.org/abs/2505.15308)
*Ji Guo,Xiaolei Wen,Wenbo Jiang,Cheng Huang,Jinjin Li,Hongwei Li*

Key words: 超分辨率，后门攻击，数据中毒，隐蔽性，BadSR

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了BadSR，一种针对超分辨率模型的改进后门攻击方法，通过在特征空间中近似清洁高分辨率图像和预定义目标图像，同时限制对清洁图像的修改范围，提高了中毒高分辨率图像的隐蔽性。实验表明，BadSR在多种模型和数据集上实现了高攻击成功率。

Motivation: 以往的超分辨率模型后门攻击主要关注中毒低分辨率图像的隐蔽性，忽略了中毒高分辨率图像的隐蔽性，容易被用户察觉。为了解决这一问题，作者提出了BadSR。

Method: BadSR在特征空间中近似清洁高分辨率图像和预定义目标图像，并限制对清洁图像的修改范围。同时设计了对抗优化的触发器和基于遗传算法的后门梯度驱动中毒样本选择方法。

Result: 实验结果显示，BadSR在多种模型和数据集上均实现了高攻击成功率，显著影响下游任务。

Conclusion: BadSR通过改进中毒高分辨率图像的隐蔽性，显著提升了后门攻击的效果，为超分辨率模型的安全性研究提供了新思路。

Abstract: With the widespread application of super-resolution (SR) in various fields,
researchers have begun to investigate its security. Previous studies have
demonstrated that SR models can also be subjected to backdoor attacks through
data poisoning, affecting downstream tasks. A backdoor SR model generates an
attacker-predefined target image when given a triggered image while producing a
normal high-resolution (HR) output for clean images. However, prior backdoor
attacks on SR models have primarily focused on the stealthiness of poisoned
low-resolution (LR) images while ignoring the stealthiness of poisoned HR
images, making it easy for users to detect anomalous data. To address this
problem, we propose BadSR, which improves the stealthiness of poisoned HR
images. The key idea of BadSR is to approximate the clean HR image and the
pre-defined target image in the feature space while ensuring that modifications
to the clean HR image remain within a constrained range. The poisoned HR images
generated by BadSR can be integrated with existing triggers. To further improve
the effectiveness of BadSR, we design an adversarially optimized trigger and a
backdoor gradient-driven poisoned sample selection method based on a genetic
algorithm. The experimental results show that BadSR achieves a high attack
success rate in various models and data sets, significantly affecting
downstream tasks.

</details>


### [364] [Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model](https://arxiv.org/abs/2505.15358)
*Angelique Mangubat,Shane Gilroy*

Key words: 道路安全, 自行车遮挡, 计算机视觉, 部分检测模型, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 本文提出了一种基于计算机视觉技术的自行车遮挡等级分类新方法，通过部分检测模型和自定义图像检测流程，量化了自行车语义部件的可见性和遮挡程度，显著提升了当前主观方法的性能。

Motivation: 提高道路安全，尤其是对易受伤害的骑行者，通过量化自行车遮挡程度来改进自行车检测算法的性能报告。

Method: 采用基于部分的检测模型和自定义图像检测流程，提出了一种新方法用于量化自行车语义部件的可见性和遮挡水平。

Result: 模型能够稳健地量化自行车的可见性和遮挡水平，显著优于现有主观方法。

Conclusion: 该方法将有助于改进自动驾驶车辆中易受伤害道路使用者的检测方法。

Abstract: Road safety is a critical challenge, particularly for cyclists, who are among
the most vulnerable road users. This study aims to enhance road safety by
proposing a novel benchmark for bicycle occlusion level classification using
advanced computer vision techniques. Utilizing a parts-based detection model,
images are annotated and processed through a custom image detection pipeline. A
novel method of bicycle occlusion level is proposed to objectively quantify the
visibility and occlusion level of bicycle semantic parts. The findings indicate
that the model robustly quantifies the visibility and occlusion level of
bicycles, a significant improvement over the subjective methods used by the
current state of the art. Widespread use of the proposed methodology will
facilitate the accurate performance reporting of cyclist detection algorithms
for occluded cyclists, informing the development of more robust vulnerable road
user detection methods for autonomous vehicles.

</details>


### [365] [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
*Dasol Choi,Seunghyun Lee,Youngsook Song*

Key words: 视觉语言模型, 安全可靠性, VERI数据集, 过度反应, 误判率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文分析视觉语言模型（VLM）在安全关键情境中的可靠性问题，发现模型普遍存在过度反应的问题，即对安全情况的误判率高。

Motivation: 研究视觉语言模型在安全关键场景中的可靠性，揭示其在实际应用中的局限性。

Method: 引入VERI数据集，通过对比对设计和两阶段评估协议（风险识别和应急响应）对14种不同规模的VLM进行测试。

Result: 模型在识别真实紧急情况时表现良好，但对安全情况的误判率高达31-96%，且存在系统性过度反应问题。

Conclusion: VLM的可靠性在安全应用中存在问题，增加模型规模无法解决这些问题，需针对性地改进模型在视觉误导场景下的安全性评估能力。

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
understanding visual content, but their reliability in safety-critical contexts
remains under-explored. We introduce VERI (Visual Emergency Recognition
Dataset), a carefully designed diagnostic benchmark of 200 images (100
contrastive pairs). Each emergency scene is matched with a visually similar but
safe counterpart through multi-stage human verification and iterative
refinement. Using a two-stage protocol - risk identification and emergency
response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,
accidents, and natural disasters. Our analysis reveals a systematic
overreaction problem: models excel at identifying real emergencies (70-100
percent success rate) but suffer from an alarming rate of false alarms,
misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios
failed by all models regardless of scale. This "better-safe-than-sorry" bias
manifests primarily through contextual overinterpretation (88-93 percent of
errors), challenging VLMs' reliability for safety applications. These findings
highlight persistent limitations that are not resolved by increasing model
scale, motivating targeted approaches for improving contextual safety
assessment in visually misleading scenarios.

</details>


### [366] [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
*Miguel Lopez-Duran,Julian Fierrez,Aythami Morales,Ruben Tolosana,Oscar Delgado-Mohatar,Alvaro Ortigosa*

Key words: GNN, 文档布局, 多模态, 图结构, PDF

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文研究了利用图神经网络（GNN）进行数字文档细粒度布局分类，通过两种图结构和多模态方法优化性能。

Motivation: 解决数字PDF文档中异构布局分析难题，避免手动特征工程。

Method: 引入k近邻图和全连接图，结合预训练文本与视觉模型生成节点特征，评估三种多模态框架。

Result: GraphSAGE在双分支配置下表现最佳，证明了局部布局关系和多模态融合的重要性。

Conclusion: GNN能有效分析数字文档布局，多模态方法和局部关系是关键。

Abstract: The automatic analysis of document layouts in digital-born PDF documents
remains a challenging problem due to the heterogeneous arrangement of textual
and nontextual elements and the imprecision of the textual metadata in the
Portable Document Format. In this work, we benchmark Graph Neural Network (GNN)
architectures for the task of fine-grained layout classification of text blocks
from digital native documents. We introduce two graph construction structures:
a k-closest-neighbor graph and a fully connected graph, and generate node
features via pre-trained text and vision models, thus avoiding manual feature
engineering. Three experimental frameworks are evaluated: single-modality (text
or visual), concatenated multimodal, and dual-branch multimodal. We evaluated
four foundational GNN models and compared them with the baseline. Our
experiments are specifically conducted on a rich dataset of public affairs
documents that includes more than 20 sources (e.g., regional and national-level
official gazettes), 37K PDF documents, with 441K pages in total. Our results
demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a
dual-branch configuration achieves the highest per-class and overall accuracy,
outperforming the baseline in some sources. These findings confirm the
importance of local layout relationships and multimodal fusion exploited
through GNNs for the analysis of native digital document layouts.

</details>


### [367] [Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation](https://arxiv.org/abs/2505.14705)
*Xin Zhang,Ziruo Zhang,Jiawei Du,Zuozhu Liu,Joey Tianyi Zhou*

Key words: 多模态数据集蒸馏,模态塌缩,表征混合,对称投影轨迹匹配

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出RepBlend框架，通过表征混合和对称投影轨迹匹配解决多模态数据集蒸馏中的模态塌缩问题，显著提升检索性能和蒸馏速度。

Motivation: 现有方法在多模态数据集蒸馏中存在模态塌缩和跨模态监督不对称的问题，影响了跨模态学习的效果。

Method: 提出RepBlend框架，采用表征混合减弱跨模态监督的过度主导，并通过对称投影轨迹匹配平衡跨模态监督。

Result: 在Flickr-30K和MS-COCO上，RepBlend显著优于现有方法，检索性能和蒸馏速度均有显著提升。

Conclusion: RepBlend有效缓解了模态塌缩并实现了平衡的跨模态监督，为多模态数据集蒸馏提供了新思路。

Abstract: Multimodal Dataset Distillation (MDD) seeks to condense large-scale
image-text datasets into compact surrogates while retaining their effectiveness
for cross-modal learning. Despite recent progress, existing MDD approaches
often suffer from \textit{\textbf{Modality Collapse}}, characterized by
over-concentrated intra-modal representations and enlarged distributional gap
across modalities. In this paper, at the first time, we identify this issue as
stemming from a fundamental conflict between the over-compression behavior
inherent in dataset distillation and the cross-modal supervision imposed by
contrastive objectives. To alleviate modality collapse, we introduce
\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal
supervision via representation blending, thereby significantly enhancing
intra-modal diversity. Additionally, we observe that current MDD methods impose
asymmetric supervision across modalities, resulting in biased optimization. To
address this, we propose symmetric projection trajectory matching, which
synchronizes the optimization dynamics using modality-specific projection
heads, thereby promoting balanced supervision and enhancing cross-modal
alignment. Experiments on Flickr-30K and MS-COCO show that RepBlend
consistently outperforms prior state-of-the-art MDD methods, achieving
significant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under
the 100-pair setting) and offering up to 6.7$\times$ distillation speedup.

</details>


### [368] [Stronger ViTs With Octic Equivariance](https://arxiv.org/abs/2505.15441)
*David Nordström,Johan Edstedt,Fredrik Kahl,Georg Bökman*

Key words: Vision Transformers, 八元组等变性, 归纳偏置, 计算效率, 图像分类, 分割

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: Octic ViTs通过引入八元组等变性（反射和90度旋转）作为归纳偏置，优化了ViT架构，在计算效率和性能上均有提升。

Motivation: 探索如何在ViT中融入八元组等变性作为新的归纳偏置，以提升模型的效率和性能。

Method: 开发了octic ViTs架构，使用八元组等变层，并在DeiT-III和DINOv2上进行了监督和自监督学习的实验。

Result: 在ViT-H上实现了约40%的FLOPs减少，同时分类和分割结果均有所提升。

Conclusion: Octic ViTs证明了八元组等变性作为归纳偏置的有效性，显著提升了ViT的计算效率和性能。

Abstract: Recent efforts at scaling computer vision models have established Vision
Transformers (ViTs) as the leading architecture. ViTs incorporate weight
sharing over image patches as an important inductive bias. In this work, we
show that ViTs benefit from incorporating equivariance under the octic group,
i.e., reflections and 90-degree rotations, as a further inductive bias. We
develop new architectures, octic ViTs, that use octic-equivariant layers and
put them to the test on both supervised and self-supervised learning. Through
extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show
that octic ViTs yield more computationally efficient networks while also
improving performance. In particular, we achieve approximately 40% reduction in
FLOPs for ViT-H while simultaneously improving both classification and
segmentation results.

</details>


### [369] [ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning](https://arxiv.org/abs/2505.15447)
*Ziqiang Xu,Qi Dai,Tian Xie,Yifan Yang,Kai Qiu,DongDong Chen,Zuxuan Wu,Chong Luo*

Key words: 视频理解,强化学习,时态定位,ViaRL

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: ViaRL利用基于规则的强化学习优化意图驱动视频理解中的帧选择，无需昂贵标注，性能显著提升。

Motivation: 解决视频理解中帧选择依赖启发式方法或伪标注的问题，提高效率和泛化能力。

Method: 采用迭代放大的强化学习框架，通过下游模型的准确性作为奖励信号训练帧选择器。

Result: 在多个基准测试中表现优异，尤其在Needle QA任务上提升近15%。

Conclusion: ViaRL有效且可扩展，显著提升了视频理解的时态定位性能。

Abstract: Video understanding is inherently intention-driven-humans naturally focus on
relevant frames based on their goals. Recent advancements in multimodal large
language models (MLLMs) have enabled flexible query-driven reasoning; however,
video-based frameworks like Video Chain-of-Thought lack direct training signals
to effectively identify relevant frames. Current approaches often rely on
heuristic methods or pseudo-label supervised annotations, which are both costly
and limited in scalability across diverse scenarios. To overcome these
challenges, we introduce ViaRL, the first framework to leverage rule-based
reinforcement learning (RL) for optimizing frame selection in intention-driven
video understanding. An iterated amplification strategy is adopted to perform
alternating cyclic training in the video CoT system, where each component
undergoes iterative cycles of refinement to improve its capabilities. ViaRL
utilizes the answer accuracy of a downstream model as a reward signal to train
a frame selector through trial-and-error, eliminating the need for expensive
annotations while closely aligning with human-like learning processes.
Comprehensive experiments across multiple benchmarks, including VideoMME,
LVBench, and MLVU, demonstrate that ViaRL consistently delivers superior
temporal grounding performance and robust generalization across diverse video
understanding tasks, highlighting its effectiveness and scalability. Notably,
ViaRL achieves a nearly 15\% improvement on Needle QA, a subset of MLVU, which
is required to search a specific needle within a long video and regarded as one
of the most suitable benchmarks for evaluating temporal grounding.

</details>


### [370] [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/abs/2505.15158)
*Yunsheng Ma,Burhaneddin Yaman,Xin Ye,Mahmut Yurt,Jingru Luo,Abhirup Mallik,Ziran Wang,Liu Ren*

Key words: 自动驾驶；大语言模型；跨模态对齐；视觉-语言推理

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了ALN-P3框架，通过跨模态对齐提升自动驾驶系统与语言推理模块的性能，实现驾驶决策与语言推理的双重优化。

Motivation: 现有方法难以同时优化自动驾驶性能和视觉-语言推理能力，ALN-P3旨在解决这一问题。

Method: ALN-P3通过三种对齐机制（感知、预测、规划对齐）在训练时对齐视觉与语言输出，且不增加推理开销。

Result: 在多个基准测试中，ALN-P3显著提升了驾驶决策和语言推理能力，达到最优效果。

Conclusion: 研究表明跨模态对齐是提升自动驾驶系统综合性能的有效途径。

Abstract: Recent advances have explored integrating large language models (LLMs) into
end-to-end autonomous driving systems to enhance generalization and
interpretability. However, most existing approaches are limited to either
driving performance or vision-language reasoning, making it difficult to
achieve both simultaneously. In this paper, we propose ALN-P3, a unified
co-distillation framework that introduces cross-modal alignment between "fast"
vision-based autonomous driving systems and "slow" language-driven reasoning
modules. ALN-P3 incorporates three novel alignment mechanisms: Perception
Alignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),
which explicitly align visual tokens with corresponding linguistic outputs
across the full perception, prediction, and planning stack. All alignment
modules are applied only during training and incur no additional costs during
inference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,
TOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both
driving decisions and language reasoning, achieving state-of-the-art results.

</details>


### [371] [Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2505.15504)
*Conghao Xiong,Zhengrui Guo,Zhe Xu,Yifei Zhang,Raymond Kai-Yu Tong,Si Yong Yeo,Hao Chen,Joseph J. Y. Sung,Irwin King*

Key words: deep learning, few-shot learning, computational pathology, MIL, SR block

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种Squeeze-and-Recalibrate (SR)块，作为MIL模型中线性层的替代，解决了小样本学习中的过拟合和计算成本高的问题。

Motivation: 计算病理学中专家标注稀缺，小样本学习易过拟合且特征表征不佳，现有方法计算复杂度高。

Method: SR块包含低秩可训练矩阵（压缩路径）和冻结随机重校准矩阵，减少参数并保持几何结构。

Result: SR-MIL模型在实验表现优异，参数更少且无需架构改动。

Conclusion: SR块有效提升小样本MIL性能，理论保证其灵活性。

Abstract: Deep learning has advanced computational pathology but expert annotations
remain scarce. Few-shot learning mitigates annotation burdens yet suffers from
overfitting and discriminative feature mischaracterization. In addition, the
current few-shot multiple instance learning (MIL) approaches leverage
pretrained vision-language models to alleviate these issues, but at the cost of
complex preprocessing and high computational cost. We propose a
Squeeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in
MIL models to address these challenges. The SR block comprises two core
components: a pair of low-rank trainable matrices (squeeze pathway, SP) that
reduces parameter count and imposes a bottleneck to prevent spurious feature
learning, and a frozen random recalibration matrix that preserves geometric
structure, diversifies feature directions, and redefines the optimization
objective for the SP. We provide theoretical guarantees that the SR block can
approximate any linear mapping to arbitrary precision, thereby ensuring that
the performance of a standard MIL model serves as a lower bound for its
SR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL
models consistently outperform prior methods while requiring significantly
fewer parameters and no architectural changes.

</details>


### [372] [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
*Jiaying Wu,Fanxiao Li,Min-Yen Kan,Bryan Hooi*

Key words: 多模态虚假信息检测, 创建者意图, 视觉语言模型, 意图感知建模, DeceptionDecoded

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种模拟真实世界多模态新闻创作的自动化框架，通过创建者意图的两个组件（期望影响和执行计划）构建大规模数据集DeceptionDecoded，评估了14种先进视觉语言模型在三种意图中心任务上的表现，发现现有模型在识别误导意图上仍有不足。

Motivation: 研究旨在通过建模创建者意图，提升多模态虚假信息检测系统的效果，以支持更有效的信息治理。

Method: 采用自动化框架模拟多模态新闻创作，构建包含12000个图像-标题对的数据集DeceptionDecoded，涵盖了误导与非误导意图，并在多种模态上进行操纵。

Result: 评估发现，当前先进的视觉语言模型在识别误导意图上表现不佳，容易依赖表面线索（如跨模态一致性、风格信号和启发式真实性提示）。

Conclusion: 研究表明，多模态虚假信息检测需要更深入的意图感知建模，为开发具备深度推理能力的系统提供了新方向。

Abstract: The real-world impact of misinformation stems from the underlying misleading
narratives that creators seek to convey. As such, interpreting misleading
creator intent is essential for multimodal misinformation detection (MMD)
systems aimed at effective information governance. In this paper, we introduce
an automated framework that simulates real-world multimodal news creation by
explicitly modeling creator intent through two components: the desired
influence and the execution plan. Using this framework, we construct
DeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs
aligned with trustworthy reference articles. The dataset captures both
misleading and non-misleading intents and spans manipulations across visual and
textual modalities. We conduct a comprehensive evaluation of 14
state-of-the-art vision-language models (VLMs) on three intent-centric tasks:
(1) misleading intent detection, (2) misleading source attribution, and (3)
creator desire inference. Despite recent advances, we observe that current VLMs
fall short in recognizing misleading intent, often relying on spurious cues
such as superficial cross-modal consistency, stylistic signals, and heuristic
authenticity hints. Our findings highlight the pressing need for intent-aware
modeling in MMD and open new directions for developing systems capable of
deeper reasoning about multimodal misinformation.

</details>


### [373] [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.15510)
*Zihui Cheng,Qiguang Chen,Xiao Xu,Jiaqi Wang,Weiyun Wang,Hao Fei,Yidong Wang,Alex Jinpeng Wang,Zhi Chen,Wanxiang Che,Libo Qin*

Key words: 大视觉语言模型（LVLMs）、多模态思维链（MCoT）、视觉思维、推理机制、多模态任务

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 大视觉语言模型（LVLMs）在多模态任务中表现优异，多模态思维链（MCoT）进一步提升了性能和可解释性。研究发现，MCoT通过引入视觉思维（无论格式如何）来增强模型性能，且视觉思维的清晰度和简洁性决定了其效果。进一步揭示了视觉思维作为输入图像与深层推理的中介作用。

Motivation: 探究多模态思维链（MCoT）提升大视觉语言模型（LVLMs）性能的具体机制，尤其是视觉思维的作用形式及其对推理过程的影响。

Method: 定义并分析了四种视觉思维的表达形式，评估其清晰度和简洁性对MCoT效果的影响；研究了视觉思维在Transformer深层的信息传递作用。

Result: 视觉思维的清晰度和简洁性显著影响MCoT的效果；视觉思维充当输入图像与深层推理的中介，促进更高级的视觉信息传递。

Conclusion: 视觉思维是MCoT提升LVLMs性能的关键因素，未来研究可基于其特性探索更多突破。

Abstract: Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing
performance and interpretability. Recent MCoT methods fall into two categories:
(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual
output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved
image-text outputs. Despite advances in both approaches, the mechanisms driving
these improvements are not fully understood. To fill this gap, we first reveal
that MCoT boosts LVLMs by incorporating visual thoughts, which convey image
information to the reasoning process regardless of the MCoT format, depending
only on clarity and conciseness of expression. Furthermore, to explore visual
thoughts systematically, we define four distinct forms of visual thought
expressions and analyze them comprehensively. Our findings demonstrate that
these forms differ in clarity and conciseness, yielding varying levels of MCoT
improvement. Additionally, we explore the internal nature of visual thoughts,
finding that visual thoughts serve as intermediaries between the input image
and reasoning to deeper transformer layers, enabling more advanced visual
information transmission. We hope that the visual thoughts can inspire further
breakthroughs for future MCoT research.

</details>


### [374] [UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset](https://arxiv.org/abs/2505.15581)
*Hua Li,Shijie Lian,Zhiyuan Li,Runmin Cong,Sam Kwong*

Key words: 水下实例分割, SAM, 知识蒸馏, 自动提示生成, UIIS10K

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种专为水下实例分割设计的高效模型UWSAM，通过知识蒸馏和自动提示生成方法，显著提升了水下场景的分割性能。

Motivation: 为解决SAM及其变体在水下实例分割任务中因领域专业知识不足和计算需求高而受限的问题。

Method: 构建UIIS10K数据集，提出基于Mask GAT的知识蒸馏方法MG-UKD和端到端水下提示生成器EUPG。

Result: UWSAM在多个水下实例数据集上表现优异，显著优于现有方法。

Conclusion: UWSAM通过高效知识蒸馏和自动提示生成，实现了水下实例分割的高性能。

Abstract: With recent breakthroughs in large-scale modeling, the Segment Anything Model
(SAM) has demonstrated significant potential in a variety of visual
applications. However, due to the lack of underwater domain expertise, SAM and
its variants face performance limitations in end-to-end underwater instance
segmentation tasks, while their higher computational requirements further
hinder their application in underwater scenarios. To address this challenge, we
propose a large-scale underwater instance segmentation dataset, UIIS10K, which
includes 10,048 images with pixel-level annotations for 10 categories. Then, we
introduce UWSAM, an efficient model designed for automatic and accurate
segmentation of underwater instances. UWSAM efficiently distills knowledge from
the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the
Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective
visual representation learning. Furthermore, we design an End-to-end Underwater
Prompt Generator (EUPG) for UWSAM, which automatically generates underwater
prompts instead of explicitly providing foreground points or boxes as prompts,
thus enabling the network to locate underwater instances accurately for
efficient segmentation. Comprehensive experimental results show that our model
is effective, achieving significant performance improvements over
state-of-the-art methods on multiple underwater instance datasets. Datasets and
codes are available at https://github.com/LiamLian0727/UIIS10K.

</details>


### [375] [FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models](https://arxiv.org/abs/2505.15644)
*Zhen Sun,Ziyi Zhang,Zeren Luo,Zeyang Sha,Tianshuo Cong,Zheng Li,Shiwen Cui,Weiqiang Wang,Jiaheng Wei,Xinlei He,Qi Li,Qian Wang*

Key words: 图像编辑检测、FragFake数据集、视觉语言模型、内容真实性、多模态

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一个新方法，利用自动化数据生成管道创建FragFake数据集，并首次将视觉语言模型用于编辑图像检测任务，取得了比预训练模型更好的效果。

Motivation: 现代图像编辑技术能够生成高度逼真的篡改图像，但现有的检测方法无法提供局部化信息，且缺乏高质量的大规模数据集。

Method: 开发自动化数据生成管道创建FragFake数据集，并利用视觉语言模型（VLMs）进行编辑图像分类和区域定位。

Result: 实验表明，经过微调的VLMs在性能上显著优于预训练模型。

Conclusion: 该研究首次将局部化图像编辑检测重新定义为视觉语言理解任务，为多模态内容真实性领域奠定了新基础。

Abstract: Fine-grained edited image detection of localized edits in images is crucial
for assessing content authenticity, especially given that modern diffusion
models and image editing methods can produce highly realistic manipulations.
However, this domain faces three challenges: (1) Binary classifiers yield only
a global real-or-fake label without providing localization; (2) Traditional
computer vision methods often rely on costly pixel-level annotations; and (3)
No large-scale, high-quality dataset exists for modern image-editing detection
techniques. To address these gaps, we develop an automated data-generation
pipeline to create FragFake, the first dedicated benchmark dataset for edited
image detection, which includes high-quality images from diverse editing models
and a wide variety of edited objects. Based on FragFake, we utilize Vision
Language Models (VLMs) for the first time in the task of edited image
classification and edited region localization. Experimental results show that
fine-tuned VLMs achieve higher average Object Precision across all datasets,
significantly outperforming pretrained models. We further conduct ablation and
transferability analyses to evaluate the detectors across various
configurations and editing scenarios. To the best of our knowledge, this work
is the first to reformulate localized image edit detection as a vision-language
understanding task, establishing a new paradigm for the field. We anticipate
that this work will establish a solid foundation to facilitate and inspire
subsequent research endeavors in the domain of multimodal content authenticity.

</details>


### [376] [Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification](https://arxiv.org/abs/2505.15671)
*Hamzeh Asgharnezhad,Afshar Shamsi,Roohallah Alizadehsani,Arash Mohammadi,Hamid Alinejad-Rokny*

Key words: 蒙特卡洛Dropout, 不确定性量化, 优化算法, 深度学习

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种改进的蒙特卡洛Dropout方法，通过集成优化算法和不确定感知损失函数，提升了深度神经网络的不确定性量化能力。

Motivation: 在高风险领域如医疗诊断和自主系统中，深度神经网络输出的不确定性量化至关重要。传统MCD方法在校准不确定性估计方面存在不足。

Method: 引入Grey Wolf Optimizer、Bayesian Optimization、Particle Swarm Optimization三种优化算法及不确定感知损失函数，增强MCD的性能。

Result: 在多个数据集和网络结构上的实验表明，改进后的算法在准确性和不确定性校准方面平均提升2-3%，显著优于传统MCD方法。

Conclusion: 该方法能显著提升深度学习模型在安全关键应用中的可信度。

Abstract: Knowing the uncertainty associated with the output of a deep neural network
is of paramount importance in making trustworthy decisions, particularly in
high-stakes fields like medical diagnosis and autonomous systems. Monte Carlo
Dropout (MCD) is a widely used method for uncertainty quantification, as it can
be easily integrated into various deep architectures. However, conventional MCD
often struggles with providing well-calibrated uncertainty estimates. To
address this, we introduce innovative frameworks that enhances MCD by
integrating different search solutions namely Grey Wolf Optimizer (GWO),
Bayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an
uncertainty-aware loss function, thereby improving the reliability of
uncertainty quantification. We conduct comprehensive experiments using
different backbones, namely DenseNet121, ResNet50, and VGG16, on various
datasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic
dataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%
on average in terms of both conventional accuracy and uncertainty accuracy
while achieving significantly better calibration. These results highlight the
potential of our approach to enhance the trustworthiness of deep learning
models in safety-critical applications.

</details>


### [377] [Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning](https://arxiv.org/abs/2505.15687)
*Zhe Xu,Cheng Jin,Yihui Wang,Ziyi Liu,Hao Chen*

Key words: 多模态病理图像理解，双向强化学习，推理能力，计算效率

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种新的双向强化学习框架，通过结合视觉和文本数据提升病理图像理解的推理能力和计算效率。

Motivation: 现有方法在复杂病理诊断场景中推理能力不足，且计算负担大，限制了实际应用。

Method: 采用双向强化学习框架，一个分支增强推理能力，另一个分支动态分配计算资源。

Result: 实验表明，性能平均提升41.7%，推理成本降低70.3%。

Conclusion: 该方法在推理准确性和计算效率上均表现出色。

Abstract: Multimodal pathological image understanding has garnered widespread interest
due to its potential to improve diagnostic accuracy and enable personalized
treatment through integrated visual and textual data. However, existing methods
exhibit limited reasoning capabilities, which hamper their ability to handle
complex diagnostic scenarios. Additionally, the enormous size of pathological
images leads to severe computational burdens, further restricting their
practical deployment. To address these limitations, we introduce a novel
bilateral reinforcement learning framework comprising two synergistic branches.
One reinforcement branch enhances the reasoning capability by enabling the
model to learn task-specific decision processes, i.e., pathology rationales,
directly from labels without explicit reasoning supervision. While the other
branch dynamically allocates a tailored number of tokens to different images
based on both their visual content and task context, thereby optimizing
computational efficiency. We apply our method to various pathological tasks
such as visual question answering, cancer subtyping, and lesion detection.
Extensive experiments show an average +41.7 absolute performance improvement
with 70.3% lower inference costs over the base models, achieving both reasoning
accuracy and computational efficiency.

</details>


### [378] [HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning](https://arxiv.org/abs/2505.15703)
*Xiaodong Mei,Sheng Wang,Jie Cheng,Yingbing Chen,Dan Xu*

Key words: 运动预测、自动驾驶、注意力机制、Mamba模块、轻量级架构

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: HAMF是一种用于自动驾驶中运动预测的新型框架，通过联合学习场景上下文编码和未来运动表征，提升了预测准确性。

Motivation: 解决现有方法在场景特征编码中信息退化的问题。

Method: 使用1D令牌序列嵌入观测到的代理状态和地图信息，设计统一的注意力编码器，并结合Mamba模块解码。

Result: 在Argoverse 2基准测试中实现了最先进的性能。

Conclusion: HAMF通过轻量级架构和联合学习机制，显著提升了运动预测的准确性和多样性。

Abstract: Motion forecasting represents a critical challenge in autonomous driving
systems, requiring accurate prediction of surrounding agents' future
trajectories. While existing approaches predict future motion states with the
extracted scene context feature from historical agent trajectories and road
layouts, they suffer from the information degradation during the scene feature
encoding. To address the limitation, we propose HAMF, a novel motion
forecasting framework that learns future motion representations with the scene
context encoding jointly, to coherently combine the scene understanding and
future motion state prediction. We first embed the observed agent states and
map information into 1D token sequences, together with the target multi-modal
future motion features as a set of learnable tokens. Then we design a unified
Attention-based encoder, which synergistically combines self-attention and
cross-attention mechanisms to model the scene context information and aggregate
future motion features jointly. Complementing the encoder, we implement the
Mamba module in the decoding stage to further preserve the consistency and
correlations among the learned future motion representations, to generate the
accurate and diverse final trajectories. Extensive experiments on Argoverse 2
benchmark demonstrate that our hybrid Attention-Mamba model achieves
state-of-the-art motion forecasting performance with the simple and lightweight
architecture.

</details>


### [379] [Constructing a 3D Town from a Single Image](https://arxiv.org/abs/2505.15765)
*Kaizhi Zheng,Ruijian Zhang,Jing Gu,Jie Yang,Xin Eric Wang*

Key words: 3DTown, 3D场景生成, 单视图, 无训练, 几何修复

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 3DTown是一个无需训练的框架，仅需单张俯视图即可合成高质量的三维场景，通过区域生成和空间感知修复确保一致性与几何质量。

Motivation: 现有方法在复杂场景生成中存在几何不一致、布局幻觉和低质量网格问题，需轻量级替代方案。

Method: 采用区域分解与预训练3D生成器结合，通过掩膜矫正流修复确保全局一致性和高质量几何。

Result: 3DTown在几何质量、空间连贯性和纹理保真度上优于现有基线方法。

Conclusion: 单图生成高质量3D场景可通过无训练、模块化方法实现。

Abstract: Acquiring detailed 3D scenes typically demands costly equipment, multi-view
data, or labor-intensive modeling. Therefore, a lightweight alternative,
generating complex 3D scenes from a single top-down image, plays an essential
role in real-world applications. While recent 3D generative models have
achieved remarkable results at the object level, their extension to full-scene
generation often leads to inconsistent geometry, layout hallucinations, and
low-quality meshes. In this work, we introduce 3DTown, a training-free
framework designed to synthesize realistic and coherent 3D scenes from a single
top-down view. Our method is grounded in two principles: region-based
generation to improve image-to-3D alignment and resolution, and spatial-aware
3D inpainting to ensure global scene coherence and high-quality geometry
generation. Specifically, we decompose the input image into overlapping regions
and generate each using a pretrained 3D object generator, followed by a masked
rectified flow inpainting process that fills in missing geometry while
maintaining structural continuity. This modular design allows us to overcome
resolution bottlenecks and preserve spatial structure without requiring 3D
supervision or fine-tuning. Extensive experiments across diverse scenes show
that 3DTown outperforms state-of-the-art baselines, including Trellis,
Hunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and
texture fidelity. Our results demonstrate that high-quality 3D town generation
is achievable from a single image using a principled, training-free approach.

</details>


### [380] [VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging](https://arxiv.org/abs/2505.15248)
*Andre Dourson,Kylie Taylor,Xiaoli Qiao,Michael Fitzke*

Key words: 自监督学习、医学影像、多视图学习、VET-DINO

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种名为VET-DINO的自监督学习框架，利用医学影像中同一研究的多个标准化视图，学习视图不变的解剖结构并建立从2D投影到3D理解的隐含认知，优于传统的合成增强方法。

Motivation: 由于医学影像标注数据稀缺，当前自监督学习多依赖单幅图像的合成增强，而医学影像具有同一研究的多个标准化视图这一独有特性，未被充分利用。

Method: 提出VET-DINO框架，利用同一犬类研究中的临床兽医X光片的多视图数据，通过真实多视图对学习视图不变特征和3D理解。

Result: 在500万张犬类X光片上的实验表明，VET-DINO在多视图合成和下游任务中表现优于合成增强方法，取得了最先进的性能。

Conclusion: VET-DINO为医学影像自监督学习建立了新范式，强调利用领域特异性特征而非简单迁移自然图像技术。

Abstract: Self-supervised learning has emerged as a powerful paradigm for training deep
neural networks, particularly in medical imaging where labeled data is scarce.
While current approaches typically rely on synthetic augmentations of single
images, we propose VET-DINO, a framework that leverages a unique characteristic
of medical imaging: the availability of multiple standardized views from the
same study. Using a series of clinical veterinary radiographs from the same
patient study, we enable models to learn view-invariant anatomical structures
and develop an implied 3D understanding from 2D projections. We demonstrate our
approach on a dataset of 5 million veterinary radiographs from 668,000 canine
studies. Through extensive experimentation, including view synthesis and
downstream task performance, we show that learning from real multi-view pairs
leads to superior anatomical understanding compared to purely synthetic
augmentations. VET-DINO achieves state-of-the-art performance on various
veterinary imaging tasks. Our work establishes a new paradigm for
self-supervised learning in medical imaging that leverages domain-specific
properties rather than merely adapting natural image techniques.

</details>


### [381] [IA-T2I: Internet-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.15779)
*Chuanhao Li,Jianwen Sun,Yukang Feng,Mingliang Zhai,Yifan Chang,Kaipeng Zhang*

Key words: 文本到图像生成、不确定知识、参考图像、自反思机制

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 提出了一种基于互联网增强的文本到图像生成框架（IA-T2I），通过检索和筛选参考图像解决T2I模型对不确定知识的生成问题，显著提升了生成效果。

Motivation: 现有T2I模型在文本提示隐含不确定知识时表现不佳，例如无法生成未来事件的相关图像。本文旨在通过引入外部参考图像解决这一问题。

Method: 设计了主动检索模块判断是否需要参考图像，引入分层图像选择模块筛选最合适的参考图像，并提出自反思机制持续优化生成结果。

Result: 在包含三类不确定知识的Img-Ref-T2I数据集上，IA-T2I框架在人类评估中比GPT-4o表现优30%。

Conclusion: IA-T2I通过结合参考图像和自反思机制，有效提升了T2I模型对不确定知识的生成能力。

Abstract: Current text-to-image (T2I) generation models achieve promising results, but
they fail on the scenarios where the knowledge implied in the text prompt is
uncertain. For example, a T2I model released in February would struggle to
generate a suitable poster for a movie premiering in April, because the
character designs and styles are uncertain to the model. To solve this problem,
we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to
compel T2I models clear about such uncertain knowledge by providing them with
reference images. Specifically, an active retrieval module is designed to
determine whether a reference image is needed based on the given text prompt; a
hierarchical image selection module is introduced to find the most suitable
image returned by an image search engine to enhance the T2I model; a
self-reflection mechanism is presented to continuously evaluate and refine the
generated image to ensure faithful alignment with the text prompt. To evaluate
the proposed framework's performance, we collect a dataset named Img-Ref-T2I,
where text prompts include three types of uncertain knowledge: (1) known but
rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt
to guide GPT-4o in making preference evaluation, which has been shown to have
an evaluation accuracy similar to that of human preference evaluation.
Experimental results demonstrate the effectiveness of our framework,
outperforming GPT-4o by about 30% in human evaluation.

</details>


### [382] [gen2seg: Generative Models Enable Generalizable Instance Segmentation](https://arxiv.org/abs/2505.15263)
*Om Khangaonkar,Hamed Pirsiavash*

Key words: 生成模型,实例分割,零样本学习,Stable Diffusion,MAE

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 通过微调Stable Diffusion和MAE,利用生成模型的表征进行零样本实例分割,模型在未见过的对象类型和风格上表现出强大泛化能力。

Motivation: 探索如何将生成模型的特征重用于通用的感知组织任务,特别是在零样本情况下。

Method: 使用实例着色损失对Stable Diffusion和MAE进行微调,专注于少量对象类型(如家具和汽车)。

Result: 模型在未见过对象类型和风格上的分割性能接近SAM,且在复杂结构和模糊边界上表现更优。

Conclusion: 生成模型学习到了跨类别和领域的固有分组机制,无需大规模预训练也可泛化。

Abstract: By pretraining to synthesize coherent images from perturbed inputs,
generative models inherently learn to understand object boundaries and scene
compositions. How can we repurpose these generative representations for
general-purpose perceptual organization? We finetune Stable Diffusion and MAE
(encoder+decoder) for category-agnostic instance segmentation using our
instance coloring loss exclusively on a narrow set of object types (indoor
furnishings and cars). Surprisingly, our models exhibit strong zero-shot
generalization, accurately segmenting objects of types and styles unseen in
finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our
best-performing models closely approach the heavily supervised SAM when
evaluated on unseen object types and styles, and outperform it when segmenting
fine structures and ambiguous boundaries. In contrast, existing promptable
segmentation architectures or discriminatively pretrained models fail to
generalize. This suggests that generative models learn an inherent grouping
mechanism that transfers across categories and domains, even without
internet-scale pretraining. Code, pretrained models, and demos are available on
our website.

</details>


### [383] [Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts](https://arxiv.org/abs/2505.15506)
*Debarshi Brahma,Anuska Roy,Soma Biswas*

Key words: 视觉-语言模型, 零样本学习, 小样本微调, PromptMargin, 多模态

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 该论文研究了如何通过少量标注数据调整预训练的视觉-语言基础模型（如CLIP和ALIGN）以适应目标数据集的分布和类别差异，并提出了一种名为PromptMargin的新方法，通过选择增强策略和多模态边缘正则化器提升模型性能。

Motivation: 探索视觉-语言基础模型在目标数据集分布和类别与预训练数据差异显著时，如何通过少量标注数据进行微调，以避免过拟合并保持泛化能力。

Method: 提出PromptMargin方法，包括选择增强策略和多模态边缘正则化器，调整文本和视觉提示以优化模型。

Result: 在15个具有不同分布偏移的目标基准数据集上验证了PromptMargin方法的有效性，优于现有先进方法。

Conclusion: PromptMargin能够有效适应少量标注数据下的视觉-语言模型微调，显著提升性能。

Abstract: Recently, Vision-Language foundation models like CLIP and ALIGN, which are
pre-trained on large-scale data have shown remarkable zero-shot generalization
to diverse datasets with different classes and even domains. In this work, we
take a step further and analyze whether these models can be adapted to target
datasets having very different distributions and classes compared to what these
models have been trained on, using only a few labeled examples from the target
dataset. In such scenarios, finetuning large pretrained models is challenging
due to problems of overfitting as well as loss of generalization, and has not
been well explored in prior literature. Since, the pre-training data of such
models are unavailable, it is difficult to comprehend the performance on
various downstream datasets. First, we try to answer the question: Given a
target dataset with a few labelled examples, can we estimate whether further
fine-tuning can enhance the performance compared to zero-shot evaluation? by
analyzing the common vision-language embedding space. Based on the analysis, we
propose a novel prompt-tuning method, PromptMargin for adapting such
large-scale VLMs directly on the few target samples. PromptMargin effectively
tunes the text as well as visual prompts for this task, and has two main
modules: 1) Firstly, we use a selective augmentation strategy to complement the
few training samples in each task; 2) Additionally, to ensure robust training
in the presence of unfamiliar class names, we increase the inter-class margin
for improved class discrimination using a novel Multimodal Margin Regularizer.
Extensive experiments and analysis across fifteen target benchmark datasets,
with varying degrees of distribution shifts from natural images, shows the
effectiveness of the proposed framework over the existing state-of-the-art
approaches applied to this setting. github.com/debarshigit/PromptMargin.

</details>


### [384] [Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.15576)
*Xin Huang,Ruibin Li,Tong Jia,Wei Zheng,Ya Wang*

Key words: 视觉语言模型, 组合推理, 硬负样本, 对比学习, 动态边界损失

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: 论文提出了一种名为AHNPL的方法，通过生成视觉领域的硬负样本和改进对比学习策略，提升了视觉语言模型在组合推理任务中的性能。

Motivation: 现有方法主要依赖文本硬负样本，忽略了视觉负样本的重要性，导致视觉编码器训练不足，影响模型整体性能。同时，负样本的难度未被区分，正样本对齐也不足。

Method: 提出了AHNPL方法，包括将文本硬负样本转化为视觉领域的语义扰动负样本，使用多模态硬负损失进行对比学习，以及动态调整对比边界的动态边界损失。

Result: 在三个公开数据集上的实验表明，AHNPL有效提升了视觉语言模型在复杂组合推理任务中的性能。

Conclusion: AHNPL通过改进硬负样本生成和对比学习策略，显著提升了模型的性能。

Abstract: Vision-Language Models (VLMs) are essential for multimodal tasks, especially
compositional reasoning (CR) tasks, which require distinguishing fine-grained
semantic differences between visual and textual embeddings. However, existing
methods primarily fine-tune the model by generating text-based hard negative
samples, neglecting the importance of image-based negative samples, which
results in insufficient training of the visual encoder and ultimately impacts
the overall performance of the model. Moreover, negative samples are typically
treated uniformly, without considering their difficulty levels, and the
alignment of positive samples is insufficient, which leads to challenges in
aligning difficult sample pairs. To address these issues, we propose Adaptive
Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard
negatives into the visual domain to generate semantically disturbed image-based
negatives for training the model, thereby enhancing its overall performance.
AHNPL also introduces a contrastive learning approach using a multimodal hard
negative loss to improve the model's discrimination of hard negatives within
each modality and a dynamic margin loss that adjusts the contrastive margin
according to sample difficulty to enhance the distinction of challenging sample
pairs. Experiments on three public datasets demonstrate that our method
effectively boosts VLMs' performance on complex CR tasks. The source code is
available at https://github.com/nynu-BDAI/AHNPL.

</details>


### [385] [VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL](https://arxiv.org/abs/2505.15791)
*Fengyuan Dai,Zifeng Zhuang,Yufei Huang,Siteng Huang,Bangyan Liao,Donglin Wang,Fajie Yuan*

Key words: 扩散模型,强化学习,值函数,KL正则化,非可微分奖励

<details>
  <summary>Details</summary>

Main category: cs.CV

TL;DR: VARD是一种基于值函数的强化扩散方法，通过学习中间状态的值函数并提供密集监督，解决了传统方法在稳定性和非可微分奖励上的不足。

Motivation: 针对现有扩散模型在微调时难以同时实现稳定性和非可微分奖励支持的问题，提出通过密集、可微分的信号优化生成过程。

Method: 提出VARD方法，首先学习预测中间状态奖励期望的值函数，并结合KL正则化提供密集监督，确保模型接近预训练分布。

Result: 实验表明，VARD能够更好地指导生成轨迹，提升训练效率，并扩展了强化学习在复杂非可微分奖励扩散模型中的应用。

Conclusion: VARD通过值函数和密集监督，有效优化了扩散模型的微调过程，适用于复杂非可微分奖励场景。

Abstract: Diffusion models have emerged as powerful generative tools across various
domains, yet tailoring pre-trained models to exhibit specific desirable
properties remains challenging. While reinforcement learning (RL) offers a
promising solution,current methods struggle to simultaneously achieve stable,
efficient fine-tuning and support non-differentiable rewards. Furthermore,
their reliance on sparse rewards provides inadequate supervision during
intermediate steps, often resulting in suboptimal generation quality. To
address these limitations, dense and differentiable signals are required
throughout the diffusion process. Hence, we propose VAlue-based Reinforced
Diffusion (VARD): a novel approach that first learns a value function
predicting expection of rewards from intermediate states, and subsequently uses
this value function with KL regularization to provide dense supervision
throughout the generation process. Our method maintains proximity to the
pretrained model while enabling effective and stable training via
backpropagation. Experimental results demonstrate that our approach facilitates
better trajectory guidance, improves training efficiency and extends the
applicability of RL to diffusion models optimized for complex,
non-differentiable reward functions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [386] [Deployment of Traditional and Hybrid Machine Learning for Critical Heat Flux Prediction in the CTF Thermal Hydraulics Code](https://arxiv.org/abs/2505.14701)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Key words: 临界热流密度,机器学习,混合模型,核反应堆,热传递

<details>
  <summary>Details</summary>

Main category: cs.CE

TL;DR: 该论文研究了一种结合数据驱动机器学习与物理模型的混合方法，用于提高临界热流密度（CHF）预测的准确性，结果显示混合模型在误差指标上优于传统经验相关性方法。

Motivation: 临界热流密度（CHF）的准确预测对核反应堆的效率和安全性至关重要。传统经验相关性方法在多样化的操作条件下可靠性有限，而纯机器学习方法又缺乏可解释性和物理知识。

Method: 研究集成了一种纯数据驱动的机器学习模型和两种混合模型（使用Biasi和Bowring CHF相关性）通过自定义Fortran框架，并在CTF子通道代码中进行验证。

Result: 混合模型在两种验证案例中表现出显著更低的误差指标，纯机器学习模型表现与混合模型相当。机器学习模型减少了CHF的过度预测趋势，提高了整体准确性。

Conclusion: 研究表明，基于机器学习的CHF模型可以有效集成到子通道代码中，性能优于传统方法，具有潜在的应用价值。

Abstract: Critical heat flux (CHF) marks the transition from nucleate to film boiling,
where heat transfer to the working fluid can rapidly deteriorate. Accurate CHF
prediction is essential for efficiency, safety, and preventing equipment
damage, particularly in nuclear reactors. Although widely used, empirical
correlations frequently exhibit discrepancies in comparison with experimental
data, limiting their reliability in diverse operational conditions. Traditional
machine learning (ML) approaches have demonstrated the potential for CHF
prediction but have often suffered from limited interpretability, data
scarcity, and insufficient knowledge of physical principles. Hybrid model
approaches, which combine data-driven ML with physics-based models, mitigate
these concerns by incorporating prior knowledge of the domain. This study
integrated a purely data-driven ML model and two hybrid models (using the Biasi
and Bowring CHF correlations) within the CTF subchannel code via a custom
Fortran framework. Performance was evaluated using two validation cases: a
subset of the Nuclear Regulatory Commission CHF database and the Bennett dryout
experiments. In both cases, the hybrid models exhibited significantly lower
error metrics in comparison with conventional empirical correlations. The pure
ML model remained competitive with the hybrid models. Trend analysis of error
parity indicates that ML-based models reduce the tendency for CHF
overprediction, improving overall accuracy. These results demonstrate that
ML-based CHF models can be effectively integrated into subchannel codes and can
potentially increase performance in comparison with conventional methods.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [387] [Replay Attacks Against Audio Deepfake Detection](https://arxiv.org/abs/2505.14862)
*Nicolas Müller,Piotr Kawa,Wei-Herng Choong,Adriana Stan,Aditya Tirumala Bukkapatnam,Karla Pizzi,Alexander Wagner,Philip Sperl*

Key words: 重放攻击,音频深度伪造,检测模型,ReplayDF,EER

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文研究了重放攻击如何通过播放和重新录制深度伪造音频来欺骗检测模型，并提出ReplayDF数据集以分析这一现象。

Motivation: 研究重放攻击对音频深度伪造检测的影响，揭示现有检测模型的脆弱性。

Method: 使用ReplayDF数据集（包含109种扬声器-麦克风组合）对六种开源检测模型进行测试，分析其在多种声学条件下的表现。

Result: 重放攻击显著降低了检测性能，最佳模型W2V2-AASIST的EER从4.7%上升到18.2%。即使经过RIR重新训练，EER仍高达11.0%。

Conclusion: 重放攻击对音频深度伪造检测构成严重威胁，现有方法仍需改进。

Abstract: We show how replay attacks undermine audio deepfake detection: By playing and
re-recording deepfake audio through various speakers and microphones, we make
spoofed samples appear authentic to the detection model. To study this
phenomenon in more detail, we introduce ReplayDF, a dataset of recordings
derived from M-AILABS and MLAAD, featuring 109 speaker-microphone combinations
across six languages and four TTS models. It includes diverse acoustic
conditions, some highly challenging for detection. Our analysis of six
open-source detection models across five datasets reveals significant
vulnerability, with the top-performing W2V2-AASIST model's Equal Error Rate
(EER) surging from 4.7% to 18.2%. Even with adaptive Room Impulse Response
(RIR) retraining, performance remains compromised with an 11.0% EER. We release
ReplayDF for non-commercial research use.

</details>


### [388] [AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars](https://arxiv.org/abs/2505.15058)
*Tianbao Zhang,Jian Zhao,Yuer Li,Zheng Zhu,Ping Hu,Zhaoxin Fan,Wenjun Wu,Xuelong Li*

Key words: 音频驱动, 数字人, 虚拟现实, 扩散变压器, 动画生成

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 提出了一种名为AsynFusion的新框架，利用扩散变压器实现面部表情和手势的和谐合成，解决现有方法中面部和手势缺乏协调的问题。

Motivation: 当前的音频驱动方法通常独立生成面部表情和手势，缺乏协调性，导致动画效果不自然。

Method: 基于双分支DiT架构，引入协作同步模块和异步LCM采样策略，实现并行生成表情和手势。

Result: AsynFusion在实时、同步的全身体动画生成中表现优异，定量和定性评估均超越现有方法。

Conclusion: AsynFusion通过协调面部和手势生成，显著提升了动画的自然性和一致性。

Abstract: Whole-body audio-driven avatar pose and expression generation is a critical
task for creating lifelike digital humans and enhancing the capabilities of
interactive virtual agents, with wide-ranging applications in virtual reality,
digital entertainment, and remote communication. Existing approaches often
generate audio-driven facial expressions and gestures independently, which
introduces a significant limitation: the lack of seamless coordination between
facial and gestural elements, resulting in less natural and cohesive
animations. To address this limitation, we propose AsynFusion, a novel
framework that leverages diffusion transformers to achieve harmonious
expression and gesture synthesis. The proposed method is built upon a
dual-branch DiT architecture, which enables the parallel generation of facial
expressions and gestures. Within the model, we introduce a Cooperative
Synchronization Module to facilitate bidirectional feature interaction between
the two modalities, and an Asynchronous LCM Sampling strategy to reduce
computational overhead while maintaining high-quality outputs. Extensive
experiments demonstrate that AsynFusion achieves state-of-the-art performance
in generating real-time, synchronized whole-body animations, consistently
outperforming existing methods in both quantitative and qualitative
evaluations.

</details>


### [389] [Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding](https://arxiv.org/abs/2505.15380)
*Zijian Lin,Yang Zhang,Yougen Yuan,Yuming Yan,Jinjiang Liu,Zhiyong Wu,Pengfei Hu,Qun Yu*

Key words: 自回归语音合成, 推理加速, 候选验证, 并行解码

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: 论文提出了一种名为SSD的新框架，通过轻量级草稿模型生成候选令牌序列，并利用并行验证加速自回归语音合成，实现了1.4倍的加速且保持高质量输出。

Motivation: 现代自回归语音合成模型在高性能的同时存在显著延迟，限制了其在需要快速推理场景中的应用。

Method: 使用轻量级草稿模型生成候选令牌序列，并通过SSD框架由目标模型并行验证。

Result: 实验结果显示，SSD相比传统自回归解码实现了1.4倍的加速，同时保持了高质量的合成效果。

Conclusion: SSD框架在加速推理的同时有效保持了感知质量，具有实际应用潜力。

Abstract: Modern autoregressive speech synthesis models leveraging language models have
demonstrated remarkable performance. However, the sequential nature of next
token prediction in these models leads to significant latency, hindering their
deployment in scenarios where inference speed is critical. In this work, we
propose Speech Speculative Decoding (SSD), a novel framework for autoregressive
speech synthesis acceleration. Specifically, our method employs a lightweight
draft model to generate candidate token sequences, which are subsequently
verified in parallel by the target model using the proposed SSD framework.
Experimental results demonstrate that SSD achieves a significant speedup of
1.4x compared with conventional autoregressive decoding, while maintaining high
fidelity and naturalness. Subjective evaluations further validate the
effectiveness of SSD in preserving the perceptual quality of the target model
while accelerating inference.

</details>


### [390] [Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models](https://arxiv.org/abs/2505.15406)
*Zirui Song,Qian Jiang,Mingxuan Cui,Mingzhe Li,Lang Gao,Zeyu Zhang,Zixiang Xu,Yanbo Wang,Chenxi Wang,Guangxian Ouyang,Zhenhao Chen,Xiuying Chen*

Key words: 大型音频语言模型, 越狱攻击, 安全性评估, 对抗样本, 语义一致性

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: AJailBench是首个用于评估大型音频语言模型（LAMs）在越狱攻击下安全性的基准测试，通过生成优化的对抗音频样本，揭示了现有模型的脆弱性。

Motivation: 现有的LAMs研究缺乏系统性的安全性评估，尤其是在面对越狱攻击时，由于其时间和语义特性，挑战更大。

Method: 作者构建了AJailBench-Base数据集（1,495个对抗音频），并开发了音频扰动工具箱（APT）生成动态对抗变体，同时保持语义一致性。

Result: 实验表明，现有LAMs均无法在所有攻击中保持稳定，且轻微语义保留的扰动也会显著降低安全性。

Conclusion: 需要更鲁棒且语义感知的防御机制来提升LAMs的安全性。

Abstract: The rise of Large Audio Language Models (LAMs) brings both potential and
risks, as their audio outputs may contain harmful or unethical content.
However, current research lacks a systematic, quantitative evaluation of LAM
safety especially against jailbreak attacks, which are challenging due to the
temporal and semantic nature of speech. To bridge this gap, we introduce
AJailBench, the first benchmark specifically designed to evaluate jailbreak
vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of
1,495 adversarial audio prompts spanning 10 policy-violating categories,
converted from textual jailbreak attacks using realistic text to speech
synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and
reveal that none exhibit consistent robustness across attacks. To further
strengthen jailbreak testing and simulate more realistic attack conditions, we
propose a method to generate dynamic adversarial variants. Our Audio
Perturbation Toolkit (APT) applies targeted distortions across time, frequency,
and amplitude domains. To preserve the original jailbreak intent, we enforce a
semantic consistency constraint and employ Bayesian optimization to efficiently
search for perturbations that are both subtle and highly effective. This
results in AJailBench-APT, an extended dataset of optimized adversarial audio
samples. Our findings demonstrate that even small, semantically preserved
perturbations can significantly reduce the safety performance of leading LAMs,
underscoring the need for more robust and semantically aware defense
mechanisms.

</details>


### [391] [Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes](https://arxiv.org/abs/2505.15559)
*Zixun Guo,Simon Dixon*

Key words: Moonbeam, Transformer, 符号音乐, MIDI, MRA

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: Moonbeam是一个基于Transformer的符号音乐基础模型，通过预训练在81.6K小时的MIDI数据和180亿个token上，结合领域知识的tokenization方法和多维相对注意力（MRA）来实现音乐理解与生成。

Motivation: 旨在通过结合音乐领域的归纳偏置，提升符号音乐的理解和生成能力。

Method: 使用基于领域知识的tokenization方法和MRA技术，无需额外可训练参数即可捕捉相对音乐信息。并提出了两种针对下游任务的微调架构。

Result: 在多种下游任务（如分类和音乐生成）中表现优于其他大规模预训练模型。

Conclusion: Moonbeam在符号音乐任务中展示了卓越的性能，且开源了模型与代码。

Abstract: Moonbeam is a transformer-based foundation model for symbolic music,
pretrained on a large and diverse collection of MIDI data totaling 81.6K hours
of music and 18 billion tokens. Moonbeam incorporates music-domain inductive
biases by capturing both absolute and relative musical attributes through the
introduction of a novel domain-knowledge-inspired tokenization method and
Multidimensional Relative Attention (MRA), which captures relative music
information without additional trainable parameters. Leveraging the pretrained
Moonbeam, we propose 2 finetuning architectures with full anticipatory
capabilities, targeting 2 categories of downstream tasks: symbolic music
understanding and conditional music generation (including music infilling). Our
model outperforms other large-scale pretrained music models in most cases in
terms of accuracy and F1 score across 3 downstream music classification tasks
on 4 datasets. Moreover, our finetuned conditional music generation model
outperforms a strong transformer baseline with a REMI-like tokenizer. We
open-source the code, pretrained model, and generated samples on Github.

</details>


### [392] [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling](https://arxiv.org/abs/2505.15772)
*Cheng Yifan,Zhang Ruoyi,Shi Jiatong*

Key words: 情感语音、多模态、自动标注、语音合成、数据集

<details>
  <summary>Details</summary>

Main category: cs.SD

TL;DR: MIKU-PAL是一种自动化的多模态流程，用于从未标注的视频数据中提取高一致性的情感语音数据，显著降低了成本和时间，同时达到了人类水平的准确性和一致性。

Motivation: 大规模高一致性情感语音数据的获取一直是语音合成的挑战，需要更高效、低成本的方法。

Method: 利用面部检测与跟踪算法，结合多模态大语言模型（MLLM）开发自动情感分析系统。

Result: MIKU-PAL在MELD数据集上达到68.5%的准确率和0.93的Fleiss kappa分数，能够标注26种细粒度情感类别，并通过了83%的人类合理性评分。

Conclusion: MIKU-PAL提供了一种高效、低成本的自动化情感语音标注方法，并发布了MIKU-EmoBench数据集作为新基准。

Abstract: Acquiring large-scale emotional speech data with strong consistency remains a
challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated
multimodal pipeline for extracting high-consistency emotional speech from
unlabeled video data. Leveraging face detection and tracking algorithms, we
developed an automatic emotion analysis system using a multimodal large
language model (MLLM). Our results demonstrate that MIKU-PAL can achieve
human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss
kappa score) while being much cheaper and faster than human annotation. With
the high-quality, flexible, and consistent annotation from MIKU-PAL, we can
annotate fine-grained speech emotion categories of up to 26 types, validated by
human annotators with 83% rationality ratings. Based on our proposed system, we
further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2
hours) as a new benchmark for emotional text-to-speech and visual voice
cloning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [393] [Uncertainty Quantification in SVM prediction](https://arxiv.org/abs/2505.15429)
*Pritam Anand*

Key words: 不确定性量化, 支持向量机, 预测区间, 稀疏分位数回归, 特征选择

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该论文探讨了支持向量机（SVM）预测中的不确定性量化（UQ）问题，提出了稀疏支持向量分位数回归（SSVQR）模型，并验证其在预测区间（PI）估计中的优越性。

Motivation: SVM预测在稳定性和可解释性上优于神经网络，但关于其预测不确定性量化的研究较少。作者希望填补这一空白。

Method: 论文总结了现有SVM框架下的PI估计方法，并提出了SSVQR模型，通过线性规划构建稀疏解，同时开发了特征选择算法以提高PI质量。

Result: 实验表明，SSVQR在高维数据集上能显著减少特征数量并提升PI质量，且在概率预测任务中表现优于或等同复杂深度学习模型。

Conclusion: 论文证明了SVM在不确定性量化中的潜力，SSVQR模型为稀疏性和高效性提供了新思路。

Abstract: This paper explores Uncertainty Quantification (UQ) in SVM predictions,
particularly for regression and forecasting tasks. Unlike the Neural Network,
the SVM solutions are typically more stable, sparse, optimal and interpretable.
However, there are only few literature which addresses the UQ in SVM
prediction. At first, we provide a comprehensive summary of existing Prediction
Interval (PI) estimation and probabilistic forecasting methods developed in the
SVM framework and evaluate them against the key properties expected from an
ideal PI model. We find that none of the existing SVM PI models achieves a
sparse solution. To introduce sparsity in SVM model, we propose the Sparse
Support Vector Quantile Regression (SSVQR) model, which constructs PIs and
probabilistic forecasts by solving a pair of linear programs. Further, we
develop a feature selection algorithm for PI estimation using SSVQR that
effectively eliminates a significant number of features while improving PI
quality in case of high-dimensional dataset. Finally we extend the SVM models
in Conformal Regression setting for obtaining more stable prediction set with
finite test set guarantees. Extensive experiments on artificial, real-world
benchmark datasets compare the different characteristics of both existing and
proposed SVM-based PI estimation methods and also highlight the advantages of
the feature selection in PI estimation. Furthermore, we compare both, the
existing and proposed SVM-based PI estimation models, with modern deep learning
models for probabilistic forecasting tasks on benchmark datasets. Furthermore,
SVM models show comparable or superior performance to modern complex deep
learning models for probabilistic forecasting task in our experiments.

</details>


### [394] [Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective](https://arxiv.org/abs/2505.14808)
*Soo Min Kwon,Alec S. Xu,Can Yaras,Laura Balzano,Qing Qu*

Key words: 上下文学习（ICL）、分布外（OOD）、低秩协方差矩阵、线性注意力模型、泛化能力

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 研究通过低秩协方差矩阵参数化的线性回归任务，揭示了上下文学习（ICL）在分布外（OOD）能力上的局限性及其在特定条件下的泛化能力。

Motivation: 探索ICL在OOD任务中的表现，尤其是其对协方差矩阵角度变化的敏感性，以及其在训练任务向量所张成的子空间中的泛化能力。

Method: 采用线性回归任务，使用低秩协方差矩阵参数化分布变化，分析单层线性注意力模型的测试风险与协方差矩阵角度之间的关系。

Result: ICL对协方差矩阵角度变化不具鲁棒性，但在训练任务向量张成的子空间内表现出泛化能力。此结果在GPT-2等模型中也成立。

Conclusion: Transformers的OOD泛化能力可能源于新任务处于训练任务所张成的子空间内。此外，LoRA能捕捉分布变化。

Abstract: This work aims to demystify the out-of-distribution (OOD) capabilities of
in-context learning (ICL) by studying linear regression tasks parameterized
with low-rank covariance matrices. With such a parameterization, we can model
distribution shifts as a varying angle between the subspace of the training and
testing covariance matrices. We prove that a single-layer linear attention
model incurs a test risk with a non-negligible dependence on the angle,
illustrating that ICL is not robust to such distribution shifts. However, using
this framework, we also prove an interesting property of ICL: when trained on
task vectors drawn from a union of low-dimensional subspaces, ICL can
generalize to any subspace within their span, given sufficiently long prompt
lengths. This suggests that the OOD generalization ability of Transformers may
actually stem from the new task lying within the span of those encountered
during training. We empirically show that our results also hold for models such
as GPT-2, and conclude with (i) experiments on how our observations extend to
nonlinear function classes and (ii) results on how LoRA has the ability to
capture distribution shifts.

</details>


### [395] [LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks](https://arxiv.org/abs/2505.14867)
*So Won Jeong,Claire Donnat*

Key words: 图神经网络、无监督学习、超参数调优、引导抽样、CCA

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 研究了图神经网络（GNN）在无监督学习中的超参数敏感性问题，提出了一种名为LOBSTUR-GNN的新框架，利用局部引导抽样和CCA分析优化超参数选择和表示评估，实验显示分类准确率提高了65.9%。

Motivation: GNN在无监督学习中的部署受到超参数敏感性和缺乏模型选择方法论的制约，需要一种更优的解决方案。

Method: 提出LOBSTUR-GNN框架，结合局部引导边和特征重抽样，利用CCA评估嵌入一致性，以优化超参数选择。

Result: 在学术数据集上实验显示，与未优化的超参数选择相比，分类准确率提高了65.9%。

Conclusion: LOBSTUR-GNN为无监督GNN提供了一种有效的超参数调优方法，具有实际应用价值。

Abstract: Graph Neural Networks (GNNs) are increasingly used in conjunction with
unsupervised learning techniques to learn powerful node representations, but
their deployment is hindered by their high sensitivity to hyperparameter tuning
and the absence of established methodologies for selecting the optimal models.
To address these challenges, we propose LOBSTUR-GNN ({\bf Lo}cal {\bf B}oot{\bf
s}trap for {\bf T}uning {\bf U}nsupervised {\bf R}epresentations in GNNs) i), a
novel framework designed to adapt bootstrapping techniques for unsupervised
graph representation learning. LOBSTUR-GNN tackles two main challenges: (a)
adapting the bootstrap edge and feature resampling process to account for local
graph dependencies in creating alternative versions of the same graph, and (b)
establishing robust metrics for evaluating learned representations without
ground-truth labels. Using locally bootstrapped resampling and leveraging
Canonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR
provides a principled approach for hyperparameter tuning in unsupervised GNNs.
We validate the effectiveness and efficiency of our proposed method through
extensive experiments on established academic datasets, showing an 65.9\%
improvement in the classification accuracy compared to an uninformed selection
of hyperparameters. Finally, we deploy our framework on a real-world
application, thereby demonstrating its validity and practical utility in
various settings. \footnote{The code is available at
\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}

</details>


### [396] [Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds](https://arxiv.org/abs/2505.15013)
*Anupama Sridhar,Alexander Johansen*

Key words: Adam优化, Deep ReLU网络, 非光滑优化, 泛化边界, 分层Morse理论, Kakeya集

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文分析了Adam优化在非光滑Deep ReLU网络中的理论表现，首次提出了其泛化边界与全局收敛性证明，并基于分层Morse理论和Kakeya集提出多层优化框架。

Motivation: 尽管Adam等一阶自适应优化方法在训练深度神经网络中广泛使用，但其在非光滑ReLU网络中的理论理解仍有限，本文旨在填补这一空白。

Method: 基于分层Morse理论和Kakeya集的新结果，开发了一个多层优化框架，逐步收紧区域穿越的边界。

Result: 证明了Adam在Deep ReLU网络中的泛化边界为O(√(d_eff/n))，并在非光滑非凸情况下实现了全局最优收敛。

Conclusion: 本文为Adam在非光滑Deep ReLU网络中的表现提供了理论支持，并通过Kakeya方法改进了泛化边界。

Abstract: First-order adaptive optimization methods like Adam are the default choices
for training modern deep neural networks. Despite their empirical success, the
theoretical understanding of these methods in non-smooth settings, particularly
in Deep ReLU networks, remains limited. ReLU activations create exponentially
many region boundaries where standard smoothness assumptions break down.
\textbf{We derive the first
\(\tilde{O}\!\bigl(\sqrt{d_{\mathrm{eff}}/n}\bigr)\) generalization bound for
Adam in Deep ReLU networks and the first global-optimal convergence for Adam in
the non smooth, non convex relu landscape without a global PL or convexity
assumption.} Our analysis is based on stratified Morse theory and novel results
in Kakeya sets. We develop a multi-layer refinement framework that
progressively tightens bounds on region crossings. We prove that the number of
region crossings collapses from exponential to near-linear in the effective
dimension. Using a Kakeya based method, we give a tighter generalization bound
than PAC-Bayes approaches and showcase convergence using a mild uniform low
barrier assumption.

</details>


### [397] [Infinite hierarchical contrastive clustering for personal digital envirotyping](https://arxiv.org/abs/2505.15022)
*Ya-Yun Huang,Joseph McClernon,Jason A. Oliver,Matthew M. Engelhard*

Key words: 环境分型, 对比聚类, 健康结果, 生态瞬时评估, 计算机视觉

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 论文提出了一种无限层次对比聚类方法，用于从日常环境图像中识别与健康结果相关的环境类型。

Motivation: 研究日常环境对健康和行为的影响，需要通过图像分析对环境进行分组，并关联健康结果。

Method: 提出了无限层次对比聚类方法，基于对比聚类框架，通过棒断先验和参与者特定预测损失，实现任意数量的聚类和子聚类。

Result: 模型有效识别了个人环境并将其分组为有意义的环境类型，并展示了如何将聚类结果与健康结果关联。

Conclusion: 该方法推动了环境分型范式的发展，为研究环境与健康关系提供了新工具。

Abstract: Daily environments have profound influence on our health and behavior. Recent
work has shown that digital envirotyping, where computer vision is applied to
images of daily environments taken during ecological momentary assessment
(EMA), can be used to identify meaningful relationships between environmental
features and health outcomes of interest. To systematically study such effects
on an individual level, it is helpful to group images into distinct
environments encountered in an individual's daily life; these may then be
analyzed, further grouped into related environments with similar features, and
linked to health outcomes. Here we introduce infinite hierarchical contrastive
clustering to address this challenge. Building on the established contrastive
clustering framework, our method a) allows an arbitrary number of clusters
without requiring the full Dirichlet Process machinery by placing a
stick-breaking prior on predicted cluster probabilities; and b) encourages
distinct environments to form well-defined sub-clusters within each cluster of
related environments by incorporating a participant-specific prediction loss.
Our experiments show that our model effectively identifies distinct personal
environments and groups these environments into meaningful environment types.
We then illustrate how the resulting clusters can be linked to various health
outcomes, highlighting the potential of our approach to advance the
envirotyping paradigm.

</details>


### [398] [A Linear Approach to Data Poisoning](https://arxiv.org/abs/2505.15175)
*Diego Granziol,Donald Flynn*

Key words: 数据投毒, Hessian矩阵, 随机矩阵理论, QR回归, 深度学习

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 研究了机器学习模型中数据投毒攻击的理论基础，提出了一种基于输入Hessian矩阵的检测方法，并通过随机矩阵理论和实验验证了其有效性。

Motivation: 探索数据投毒攻击的理论基础，识别攻击对模型的影响，并提出检测和补救方法。

Method: 使用随机矩阵理论分析线性回归中的投毒比例和正则化影响，通过QR逐步回归研究Hessian矩阵的谱特征，并在深度网络中进行实验验证。

Result: Hessian矩阵的谱特征可作为投毒检测工具，理论适用于卷积和Transformer网络，并开发了初步的检测和补救算法。

Conclusion: 提出的方法能够有效检测投毒攻击且无需重新训练模型，为安全机器学习提供了新思路。

Abstract: We investigate the theoretical foundations of data poisoning attacks in
machine learning models. Our analysis reveals that the Hessian with respect to
the input serves as a diagnostic tool for detecting poisoning, exhibiting
spectral signatures that characterize compromised datasets. We use random
matrix theory (RMT) to develop a theory for the impact of poisoning proportion
and regularisation on attack efficacy in linear regression. Through QR stepwise
regression, we study the spectral signatures of the Hessian in multi-output
regression. We perform experiments on deep networks to show experimentally that
this theory extends to modern convolutional and transformer networks under the
cross-entropy loss. Based on these insights we develop preliminary algorithms
to determine if a network has been poisoned and remedies which do not require
further training.

</details>


### [399] [Clustering and Pruning in Causal Data Fusion](https://arxiv.org/abs/2505.15215)
*Otto Tabell,Santtu Tikka,Juha Karvanen*

Key words: 数据融合,因果推断,剪枝,聚类,可识别性

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种通过剪枝和聚类预处理操作来简化因果数据融合的方法，以解决do-calculus在高复杂度因果图中的计算挑战。

Motivation: 因果数据融合在高维度或复杂因果图中的应用面临计算挑战，需要简化模型同时保留关键特征。

Method: 提出剪枝（移除不必要变量）和聚类（合并变量）作为预处理步骤，并推广了多数据源条件下的应用。

Result: 推导了在较大图中基于较小图推断因果效应可识别性的充分条件，并展示了可识别因果效应的功能表达式。

Conclusion: 剪枝和聚类预处理有效简化了因果数据融合的计算复杂度，且在实际应用中证明了其有效性。

Abstract: Data fusion, the process of combining observational and experimental data,
can enable the identification of causal effects that would otherwise remain
non-identifiable. Although identification algorithms have been developed for
specific scenarios, do-calculus remains the only general-purpose tool for
causal data fusion, particularly when variables are present in some data
sources but not others. However, approaches based on do-calculus may encounter
computational challenges as the number of variables increases and the causal
graph grows in complexity. Consequently, there exists a need to reduce the size
of such models while preserving the essential features. For this purpose, we
propose pruning (removing unnecessary variables) and clustering (combining
variables) as preprocessing operations for causal data fusion. We generalize
earlier results on a single data source and derive conditions for applying
pruning and clustering in the case of multiple data sources. We give sufficient
conditions for inferring the identifiability or non-identifiability of a causal
effect in a larger graph based on a smaller graph and show how to obtain the
corresponding identifying functional for identifiable causal effects. Examples
from epidemiology and social science demonstrate the use of the results.

</details>


### [400] [Policy Testing in Markov Decision Processes](https://arxiv.org/abs/2505.15342)
*Kaito Ariu,Po-An Wang,Alexandre Proutiere,Kenshi Abe*

Key words: 折扣MDP,策略测试,固定置信度,统计最优,计算复杂度

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 研究了折扣马尔可夫决策过程（MDP）中固定置信度下的策略测试问题，目标是判断给定策略的值是否超过阈值，并最小化观测次数。提出了基于优化问题的统计最优算法。

Motivation: 解决MDP中策略测试问题的计算复杂性和统计最优性之间的矛盾。

Method: 通过重新构造优化问题，将非凸约束转化为凸约束，并利用策略梯度方法高效解决，设计统计最优且计算可行的算法。

Result: 提出的算法在样本复杂度上匹配实例特定的下界，验证了其统计最优性和计算可行性。

Conclusion: 通过优化问题的重构和策略梯度方法，成功解决了MDP中策略测试问题的挑战，实现了统计最优与计算高效的平衡。

Abstract: We study the policy testing problem in discounted Markov decision processes
(MDPs) under the fixed-confidence setting. The goal is to determine whether the
value of a given policy exceeds a specified threshold while minimizing the
number of observations. We begin by deriving an instance-specific lower bound
that any algorithm must satisfy. This lower bound is characterized as the
solution to an optimization problem with non-convex constraints. We propose a
policy testing algorithm inspired by this optimization problem--a common
approach in pure exploration problems such as best-arm identification, where
asymptotically optimal algorithms often stem from such optimization-based
characterizations. As for other pure exploration tasks in MDPs, however, the
non-convex constraints in the lower-bound problem present significant
challenges, raising doubts about whether statistically optimal and
computationally tractable algorithms can be designed. To address this, we
reformulate the lower-bound problem by interchanging the roles of the objective
and the constraints, yielding an alternative problem with a non-convex
objective but convex constraints. Strikingly, this reformulated problem admits
an interpretation as a policy optimization task in a newly constructed reversed
MDP. Leveraging recent advances in policy gradient methods, we efficiently
solve this problem and use it to design a policy testing algorithm that is
statistically optimal--matching the instance-specific lower bound on sample
complexity--while remaining computationally tractable. We validate our approach
with numerical experiments.

</details>


### [401] [Robust Multimodal Learning via Entropy-Gated Contrastive Fusion](https://arxiv.org/abs/2505.15417)
*Leon Chlon,Maggie Chlon,MarcAntonio M. Awada*

Key words: 多模态系统, 鲁棒性, 校准性, AECF, 缺失输入

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: AECF是一种轻量级的自适应融合层，能在缺失输入的情况下同时保持鲁棒性和校准性，适用于多模态系统。

Motivation: 解决多模态系统在输入缺失时无法同时保持鲁棒性和校准性的问题。

Method: 提出AECF方法，通过自适应熵系数、单调校准和课程掩码实现。

Result: 在AV-MNIST和MS-COCO数据集上，AECF在50%缺失率下提升mAP 18%，同时降低ECE 200%，运行时间仅增加1%。

Conclusion: AECF是一种简单易用的融合层，适用于鲁棒且校准的多模态推理。

Abstract: Real-world multimodal systems routinely face missing-input scenarios, and in
reality, robots lose audio in a factory or a clinical record omits lab tests at
inference time. Standard fusion layers either preserve robustness or
calibration but never both. We introduce Adaptive Entropy-Gated Contrastive
Fusion (AECF), a single light-weight layer that (i) adapts its entropy
coefficient per instance, (ii) enforces monotone calibration across all
modality subsets, and (iii) drives a curriculum mask directly from
training-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP
by +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%
run-time. All back-bones remain frozen, making AECF an easy drop-in layer for
robust, calibrated multimodal inference.

</details>


### [402] [Adaptive Temperature Scaling with Conformal Prediction](https://arxiv.org/abs/2505.15437)
*Nikita Kotelevskii,Mohsen Guizani,Eric Moulines,Maxim Panov*

Key words: 共形预测, 概率校准, 自适应校准, 图像分类

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 本文提出了一种为共形预测集分配校准概率的新方法，通过自适应校准问题解决了传统共形预测集无法提供单个标签概率估计的问题。

Motivation: 传统的共形预测集虽然保证了高覆盖率，但无法提供单个标签的概率估计，限制了其实际应用。因此，需要一种方法为共形预测集中的元素分配校准概率。

Method: 作者将问题建模为自适应校准问题，通过选择输入特定的温度参数来匹配所需的覆盖率水平。

Result: 实验表明，该方法在保持覆盖率保证的同时，显著降低了预期校准误差。

Conclusion: 本文提出的方法解决了共形预测集的概率校准问题，为实际应用提供了更实用的工具。

Abstract: Conformal prediction enables the construction of high-coverage prediction
sets for any pre-trained model, guaranteeing that the true label lies within
the set with a specified probability. However, these sets do not provide
probability estimates for individual labels, limiting their practical use. In
this paper, we propose, to the best of our knowledge, the first method for
assigning calibrated probabilities to elements of a conformal prediction set.
Our approach frames this as an adaptive calibration problem, selecting an
input-specific temperature parameter to match the desired coverage level.
Experiments on several challenging image classification datasets demonstrate
that our method maintains coverage guarantees while significantly reducing
expected calibration error.

</details>


### [403] [Are machine learning interpretations reliable? A stability study on global interpretations](https://arxiv.org/abs/2505.15728)
*Luqin Gan,Tarek M. Zikry,Genevera I. Allen*

Key words: 机器学习, 可解释性, 稳定性, 实证研究

<details>
  <summary>Details</summary>

Main category: stat.ML

TL;DR: 该研究首次系统评估了流行的机器学习全局解释方法的稳定性，发现这些解释方法普遍不稳定，且解释稳定性与预测准确性无关。

Motivation: 随着机器学习在高风险领域的应用增加，提高模型解释的可信度成为关键问题。

Method: 通过对表格数据的监督和无监督任务进行大规模实证研究，评估多种流行解释方法的稳定性。

Result: 结果显示解释方法普遍不稳定，且解释稳定性与预测准确性无关联。

Conclusion: 仅依靠解释性不足以建立信任，未来需严格评估解释稳定性。

Abstract: As machine learning systems are increasingly used in high-stakes domains,
there is a growing emphasis placed on making them interpretable to improve
trust in these systems. In response, a range of interpretable machine learning
(IML) methods have been developed to generate human-understandable insights
into otherwise black box models. With these methods, a fundamental question
arises: Are these interpretations reliable? Unlike with prediction accuracy or
other evaluation metrics for supervised models, the proximity to the true
interpretation is difficult to define. Instead, we ask a closely related
question that we argue is a prerequisite for reliability: Are these
interpretations stable? We define stability as findings that are consistent or
reliable under small random perturbations to the data or algorithms. In this
study, we conduct the first systematic, large-scale empirical stability study
on popular machine learning global interpretations for both supervised and
unsupervised tasks on tabular data. Our findings reveal that popular
interpretation methods are frequently unstable, notably less stable than the
predictions themselves, and that there is no association between the accuracy
of machine learning predictions and the stability of their associated
interpretations. Moreover, we show that no single method consistently provides
the most stable interpretations across a range of benchmark datasets. Overall,
these results suggest that interpretability alone does not warrant trust, and
underscores the need for rigorous evaluation of interpretation stability in
future work. To support these principles, we have developed and released an
open source IML dashboard and Python package to enable researchers to assess
the stability and reliability of their own data-driven interpretations and
discoveries.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [404] [Propositional Measure Logic](https://arxiv.org/abs/2505.14693)
*Francisco Aragão*

Key words: 概率逻辑, 命题逻辑, 不确定性推理, 贝叶斯网络

<details>
  <summary>Details</summary>

Main category: cs.LO

TL;DR: 提出了一种基于概率语义的命题逻辑，将公式的真值从二元扩展到区间$[0,1]$，保留经典逻辑的推理结构，并证明了其可靠性。

Motivation: 解决经典逻辑二值性的局限性，为不确定性推理提供更合理的语义框架。

Method: 设计了一种概率语义的命题逻辑系统，并通过证明其可靠性（soundness）验证其合理性。

Result: 证明了该系统适用于不确定性推理，并将其应用于贝叶斯网络中的顽固问题。

Conclusion: 该逻辑系统为不确定性推理提供了新的理论工具，并展示了其实际应用潜力。

Abstract: We present a propositional logic with fundamental probabilistic semantics, in
which each formula is given a real measure in the interval $[0,1]$ that
represents its degree of truth. This semantics replaces the binarity of
classical logic, while preserving its deductive structure. We demonstrate the
soundness theorem, establishing that the proposed system is sound and suitable
for reasoning under uncertainty. We discuss potential applications and avenues
for future extensions of the theory. We apply probabilistic logic to a still
refractory problem in Bayesian Networks.

</details>


### [405] [Alpay Algebra: A Universal Structural Foundation](https://arxiv.org/abs/2505.15344)
*Faruk Alpay*

Key words: Alpay Algebra, 范畴论, 符号递归, 可解释AI, 不动点分析

<details>
  <summary>Details</summary>

Main category: cs.LO

TL;DR: Alpay Algebra提供了一种统一的范畴论框架，结合经典代数结构与现代符号递归和可解释AI需求，通过最小公理系统定义，证明其不动点存在性，并在AI模型中展示应用潜力。

Motivation: 作者旨在创建一个统一的数学框架，连接经典代数结构与现代AI需求，尤其在符号递归和可解释AI领域。

Method: 通过范畴论方法，从最小公理出发定义Alpay Algebra，并引入演化函子φ，证明其不动点φ^∞的存在性及其普遍性质。

Result: 证明了φ^∞的存在性及其在标准通用代数中的保守性，展示了其在AI模型中的信息论对应关系。

Conclusion: Alpay Algebra成为基础数学与高影响AI系统的桥梁，为范畴论、符号计算等领域提供参考。

Abstract: Alpay Algebra is introduced as a universal, category-theoretic framework that
unifies classical algebraic structures with modern needs in symbolic recursion
and explainable AI. Starting from a minimal list of axioms, we model each
algebra as an object in a small cartesian closed category $\mathcal{A}$ and
define a transfinite evolution functor $\phi\colon\mathcal{A}\to\mathcal{A}$.
We prove that the fixed point $\phi^{\infty}$ exists for every initial object
and satisfies an internal universal property that recovers familiar constructs
-- limits, colimits, adjunctions -- while extending them to ordinal-indexed
folds. A sequence of theorems establishes (i) soundness and conservativity over
standard universal algebra, (ii) convergence of $\phi$-iterates under regular
cardinals, and (iii) an explanatory correspondence between $\phi^{\infty}$ and
minimal sufficient statistics in information-theoretic AI models. We conclude
by outlining computational applications: type-safe functional languages,
categorical model checking, and signal-level reasoning engines that leverage
Alpay Algebra's structural invariants. All proofs are self-contained; no
external set-theoretic axioms beyond ZFC are required. This exposition
positions Alpay Algebra as a bridge between foundational mathematics and
high-impact AI systems, and provides a reference for further work in category
theory, transfinite fixed-point analysis, and symbolic computation.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [406] [Versatile Reservoir Computing for Heterogeneous Complex Networks](https://arxiv.org/abs/2505.15219)
*Yao Du,Huawei Fan,Xingang Wang*

Key words: 储层计算,复杂网络,动态复制,机器学习

<details>
  <summary>Details</summary>

Main category: nlin.CD

TL;DR: 提出了一种名为多功能储层计算的新机器学习方案，用于维持异构复杂网络的动态。

Motivation: 研究如何通过小型储层计算机复制大规模复杂网络中不同参数和连接性元素的动态。

Method: 利用单台小型储层计算机，基于部分元素的时间序列训练，模拟网络中任意元素的动态；替换故障元素以保持网络集体动态。

Result: 在三种代表性网络模型上验证了方案的有效性，可准确维持网络动态。

Conclusion: 多功能储层计算能有效复制并维持异构复杂网络的动态。

Abstract: A new machine learning scheme, termed versatile reservoir computing, is
proposed for sustaining the dynamics of heterogeneous complex networks. We show
that a single, small-scale reservoir computer trained on time series from a
subset of elements is able to replicate the dynamics of any element in a
large-scale complex network, though the elements are of different intrinsic
parameters and connectivities. Furthermore, by substituting failed elements
with the trained machine, we demonstrate that the collective dynamics of the
network can be preserved accurately over a finite time horizon. The capability
and effectiveness of the proposed scheme are validated on three representative
network models: a homogeneous complex network of non-identical phase
oscillators, a heterogeneous complex network of non-identical phase
oscillators, and a heterogeneous complex network of non-identical chaotic
oscillators.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [407] [Personalized Diffusion Model Reshapes Cold-Start Bundle Recommendation](https://arxiv.org/abs/2505.14901)
*Tuan-Nghia Bui,Huy-Son Nguyen,Cam-Van Thi Nguyen,Hoang-Quynh Le,Duc-Trong Le*

Key words: 捆绑推荐、冷启动、扩散模型、解耦、生成模型

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: DisCo模型通过个性化扩散骨架和用户兴趣解耦来解决冷启动捆绑推荐问题，显著优于其他基线方法。

Motivation: 解决冷启动场景下用户与捆绑之间交互稀疏的挑战。

Method: 采用个性化扩散骨架和解耦用户兴趣的方法，生成分布空间中的捆绑。

Result: 在三个真实数据集上大幅优于五种基线方法。

Conclusion: DisCo为冷启动推荐提供了有前景的框架和见解。

Abstract: Bundle recommendation aims to recommend a set of items to each user. However,
the sparser interactions between users and bundles raise a big challenge,
especially in cold-start scenarios. Traditional collaborative filtering methods
do not work well for this kind of problem because these models rely on
interactions to update the latent embedding, which is hard to work in a
cold-start setting. We propose a new approach (DisCo), which relies on a
personalized Diffusion backbone, enhanced by disentangled aspects for the
user's interest, to generate a bundle in distribution space for each user to
tackle the cold-start challenge. During the training phase, DisCo adjusts an
additional objective loss term to avoid bias, a prevalent issue while using the
generative model for top-$K$ recommendation purposes. Our empirical experiments
show that DisCo outperforms five comparative baselines by a large margin on
three real-world datasets. Thereby, this study devises a promising framework
and essential viewpoints in cold-start recommendation. Our materials for
reproducibility are available at: https://github.com/bt-nghia/DisCo.

</details>


### [408] [ThinkRec: Thinking-based recommendation via LLM](https://arxiv.org/abs/2505.15091)
*Qihang Yu,Kairui Fu,Shengyu Zhang,Zheqi Lv,Fan Wu,Fei Wu*

Key words: 大语言模型、推荐系统、深层推理、思考激活机制、专家融合

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: ThinkRec是一个基于思考的推荐框架，通过引入思考激活机制和实例级专家融合机制，从浅层匹配转向深层推理，显著提升推荐的准确性和可解释性。

Motivation: 现有基于大语言模型的推荐方法（LLM4Rec）多依赖浅层特征匹配，缺乏深层行为逻辑推理，导致推荐结果肤浅且错误。

Method: ThinkRec引入了思考激活机制，通过关键词摘要和合成推理轨迹增强项目元数据，并结合实例级专家融合机制动态调整推理路径。

Result: 在真实数据集上的实验表明，ThinkRec显著提高了推荐的准确性和可解释性。

Conclusion: ThinkRec通过从System 1转向System 2的推理方式，为推荐系统提供了更精准和个性化的解决方案。

Abstract: Recent advances in large language models (LLMs) have enabled more
semantic-aware recommendations through natural language generation. Existing
LLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like
manner, relying on superficial features to match similar items based on click
history, rather than reasoning through deeper behavioral logic. This often
leads to superficial and erroneous recommendations. Motivated by this, we
propose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1
to System 2 (rational system). Technically, ThinkRec introduces a thinking
activation mechanism that augments item metadata with keyword summarization and
injects synthetic reasoning traces, guiding the model to form interpretable
reasoning chains that consist of analyzing interaction histories, identifying
user preferences, and making decisions based on target items. On top of this,
we propose an instance-wise expert fusion mechanism to reduce the reasoning
difficulty. By dynamically assigning weights to expert models based on users'
latent features, ThinkRec adapts its reasoning path to individual users,
thereby enhancing precision and personalization. Extensive experiments on
real-world datasets demonstrate that ThinkRec significantly improves the
accuracy and interpretability of recommendations. Our implementations are
available in anonymous Github: https://anonymous.4open.science/r/ThinkRec_LLM.

</details>


### [409] [An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc](https://arxiv.org/abs/2505.15070)
*Aldo Porco,Dhruv Mehra,Igor Malioutov,Karthik Radhakrishnan,Moniba Keymanesh,Daniel Preoţiuc-Pietro,Sean MacAvaney,Pengxiang Cheng*

Key words: 稀疏检索、FLOPS正则化、高文档频率、检索延迟、倒排索引

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 论文提出了一种新的稀疏检索模型正则化方法DF-FLOPS，旨在降低高文档频率（DF）词项的使用，从而减少检索延迟，同时保持检索效果。

Motivation: 高文档频率（DF）词项会导致检索延迟增加，影响稀疏检索模型的实用性。

Method: 提出了DF-FLOPS正则化技术，通过惩罚高-DF词项的使用，缩短倒排索引中的帖子列表。

Result: 实验表明，DF-FLOPS显著降低了高-DF词项的出现频率和检索延迟（约快10倍），同时保持了检索效果。

Conclusion: DF-FLOPS为稀疏检索模型在生产环境中的实际应用提供了重要进展。

Abstract: Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,
which need to be sparse to leverage inverted index structures during retrieval.
SPLADE, the most popular LSR model, uses FLOPS regularization to encourage
vector sparsity during training. However, FLOPS regularization does not ensure
sparsity among terms - only within a given query or document. Terms with very
high Document Frequencies (DFs) substantially increase latency in production
retrieval engines, such as Apache Solr, due to their lengthy posting lists. To
address the issue of high DFs, we present a new variant of FLOPS
regularization: DF-FLOPS. This new regularization technique penalizes the usage
of high-DF terms, thereby shortening posting lists and reducing retrieval
latency. Unlike other inference-time sparsification methods, such as stopword
removal, DF-FLOPS regularization allows for the selective inclusion of
high-frequency terms in cases where the terms are truly salient. We find that
DF-FLOPS successfully reduces the prevalence of high-DF terms and lowers
retrieval latency (around 10x faster) in a production-grade engine while
maintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and
cross-domain (improved performance in 12 out of 13 tasks on which we tested).
With retrieval latencies on par with BM25, this work provides an important step
towards making LSR practical for deployment in production-grade search engines.

</details>


### [410] [MIRB: Mathematical Information Retrieval Benchmark](https://arxiv.org/abs/2505.15585)
*Haocheng Ju,Bin Dong*

Key words: 数学信息检索（MIR），MIRB，基准测试，检索模型

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 本文介绍了MIRB（数学信息检索基准），旨在评估数学文档中信息检索模型的性能，涵盖四个任务和12个数据集，并对13个模型进行了测试。

Motivation: 当前缺乏一个统一的基准来评估数学信息检索（MIR）任务，MIRB的引入填补了这一空白。

Method: MIRB包含四个任务：语义陈述检索、问答检索、前提检索和公式检索，共12个数据集，并评估了13个检索模型。

Result: 通过MIRB基准，分析了数学信息检索的挑战，为评估MIR系统提供了全面的框架。

Conclusion: MIRB有望促进针对数学领域更有效检索模型的开发。

Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving
information from mathematical documents and plays a key role in various
applications, including theorem search in mathematical libraries, answer
retrieval on math forums, and premise selection in automated theorem proving.
However, a unified benchmark for evaluating these diverse retrieval tasks has
been lacking. In this paper, we introduce MIRB (Mathematical Information
Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB
includes four tasks: semantic statement retrieval, question-answer retrieval,
premise retrieval, and formula retrieval, spanning a total of 12 datasets. We
evaluate 13 retrieval models on this benchmark and analyze the challenges
inherent to MIR. We hope that MIRB provides a comprehensive framework for
evaluating MIR systems and helps advance the development of more effective
retrieval models tailored to the mathematical domain.

</details>


### [411] [Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search](https://arxiv.org/abs/2505.15636)
*Yousef Al-Jazzazi,Haya Diwan,Jinrui Gou,Cameron Musco,Christopher Musco,Torsten Suel*

Key words: 最近邻搜索,图搜索,自适应波束搜索,HNSW,Vamana

<details>
  <summary>Details</summary>

Main category: cs.IR

TL;DR: 该论文提出了一种基于距离的自适应波束搜索终止条件，取代传统的波束宽度条件，证明了其在导航图上的近似最近邻搜索有效性，并通过实验验证了其优于标准波束搜索的性能。

Motivation: 高维数据中基于图的最近邻搜索方法（如HNSW、DiskANN等）虽然实用高效，但仍有许多未解决的问题，尤其是在搜索终止条件上。

Method: 提出了一种新的自适应波束搜索方法，通过引入基于距离的终止条件，取代传统的波束宽度限制。

Result: 实验结果表明，该方法在不同数据集、图结构和目标最近邻数目下均优于标准波束搜索。

Conclusion: 自适应波束搜索为流行的图搜索方法提供了一种简单且实用的改进方案。

Abstract: Nearest neighbor search is central in machine learning, information
retrieval, and databases. For high-dimensional datasets, graph-based methods
such as HNSW, DiskANN, and NSG have become popular thanks to their empirical
accuracy and efficiency. These methods construct a directed graph over the
dataset and perform beam search on the graph to find nodes close to a given
query. While significant work has focused on practical refinements and
theoretical understanding of graph-based methods, many questions remain. We
propose a new distance-based termination condition for beam search to replace
the commonly used condition based on beam width. We prove that, as long as the
search graph is navigable, our resulting Adaptive Beam Search method is
guaranteed to approximately solve the nearest-neighbor problem, establishing a
connection between navigability and the performance of graph-based search. We
also provide extensive experiments on our new termination condition for both
navigable graphs and approximately navigable graphs used in practice, such as
HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard
beam search over a range of recall values, data sets, graph constructions, and
target number of nearest neighbors. It thus provides a simple and practical way
to improve the performance of popular methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [412] [Balanced and Elastic End-to-end Training of Dynamic LLMs](https://arxiv.org/abs/2505.14864)
*Mohamed Wahib,Muhammed Abdullah Soyturk,Didem Unat*

Key words: LLMs, 动态负载均衡, DynMo, 分布式训练, 工作负载优化

<details>
  <summary>Details</summary>

Main category: cs.DC

TL;DR: 论文提出DynMo，一种动态负载均衡方案，用于解决大规模分布式训练中动态模型的工作负载不平衡问题，显著提升训练速度。

Motivation: 大型语言模型（LLMs）的动态工作负载减少方案（如MoEs、参数修剪等）导致严重的工作负载不平衡，限制了其在大规模分布式训练中的实用性。

Method: DynMo是一种自主动态负载平衡解决方案，通过自适应地平衡工作负载、动态将任务打包到更少的工作节点以释放空闲资源，支持多GPU单节点和多节点系统。

Result: 与静态训练方法相比，DynMo在多种动态模型上显著加速训练，最高可达4.52倍。

Conclusion: DynMo有效解决了动态模型训练中的负载不平衡问题，提高了训练效率。

Abstract: To reduce computational and memory costs in Large Language Models (LLMs),
dynamic workload reduction schemes like Mixture of Experts (MoEs), parameter
pruning, layer freezing, sparse attention, early token exit, and Mixture of
Depths (MoDs) have emerged. However, these methods introduce severe workload
imbalances, limiting their practicality for large-scale distributed training.
We propose DynMo, an autonomous dynamic load balancing solution that ensures
optimal compute distribution when using pipeline parallelism in training
dynamic models. DynMo adaptively balances workloads, dynamically packs tasks
into fewer workers to free idle resources, and supports both multi-GPU
single-node and multi-node systems. Compared to static training methods
(Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs),
3.18x (pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early
exit), and 1.17x (MoDs). DynMo is available at
https://anonymous.4open.science/r/DynMo-4D04/.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [413] [Bridge2AI: Building A Cross-disciplinary Curriculum Towards AI-Enhanced Biomedical and Clinical Care](https://arxiv.org/abs/2505.14757)
*John Rincon,Alexander R. Pelletier,Destiny Gilliland,Wei Wang,Ding Wang,Baradwaj S. Sankar,Lori Scott-Sheldon,Samson Gebreab,William Hersh,Parisa Rashidi,Sally Baxter,Wade Schulz,Trey Ideker,Yael Bensoussan,Paul C. Boutros,Alex A. T. Bui,Colin Walsh,Karol E. Watson,Peipei Ping*

Key words: AI培训,生物信息学,跨学科课程,伦理数据管理,个性化学习

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该论文介绍了一种个性化、适应性强的生物信息学和生物医学AI培训系统，通过跨学科课程和导师网络，培养具备伦理基础的多学科能力。

Motivation: 随着AI在医疗保健中的日益重要，需要开发个性化且适应性强的培训系统以满足需求。

Method: NIH Bridge2AI TRM工作组开发了一个基于协作创新、伦理数据管理和专业发展的跨学科课程，并通过学习者角色和教育路径的个性化设计实现可扩展性。

Result: 该课程整合了基础AI模块、实际项目和有结构的导师网络，已有30多名学者和100名导师参与，证明了其有效性。

Conclusion: 通过持续反馈的迭代改进，该模型展示了如何通过个性化培训在生物医学背景下培养多学科能力和伦理基础的AI教育。

Abstract: Objective: As AI becomes increasingly central to healthcare, there is a
pressing need for bioinformatics and biomedical training systems that are
personalized and adaptable. Materials and Methods: The NIH Bridge2AI Training,
Recruitment, and Mentoring (TRM) Working Group developed a cross-disciplinary
curriculum grounded in collaborative innovation, ethical data stewardship, and
professional development within an adapted Learning Health System (LHS)
framework. Results: The curriculum integrates foundational AI modules,
real-world projects, and a structured mentee-mentor network spanning Bridge2AI
Grand Challenges and the Bridge Center. Guided by six learner personas, the
program tailors educational pathways to individual needs while supporting
scalability. Discussion: Iterative refinement driven by continuous feedback
ensures that content remains responsive to learner progress and emerging
trends. Conclusion: With over 30 scholars and 100 mentors engaged across North
America, the TRM model demonstrates how adaptive, persona-informed training can
build interdisciplinary competencies and foster an integrative, ethically
grounded AI education in biomedical contexts.

</details>


### [414] [Kaleidoscope Gallery: Exploring Ethics and Generative AI Through Art](https://arxiv.org/abs/2505.14758)
*Alayt Issak,Uttkarsh Narayan,Ramya Srinivasan,Erica Kleinman,Casper Harteveld*

Key words: 生成式AI, 伦理理论, 可视化伦理, 文本到图像模型, 社会技术系统

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 该研究探讨了通过生成式AI（GenAI）模型可视化伦理理论，利用艺术和万花筒隐喻进行批判性探究，揭示了五种伦理理论家族并将其转化为图像，最终提出八种主题以分析道德、社会与学习关联的中心地位。

Motivation: 通过生成式AI模型可视化伦理理论，探索伦理与技术的交互，并推动视觉伦理这一新兴领域的发展。

Method: 通过10位伦理学专家的访谈建立伦理理论基础，使用文本到图像（T2I）生成式AI模型将伦理理论转换为图像，并通过专家评估形成万花筒画廊。

Result: 揭示了五种伦理理论家族，并通过图像分析提出八种主题，强调道德、社会和学习关联的重要性。

Conclusion: 研究为批判性分析T2I模型提供了框架，并探讨了伦理理论作为社会技术系统的基础知识的意义。

Abstract: Ethical theories and Generative AI (GenAI) models are dynamic concepts
subject to continuous evolution. This paper investigates the visualization of
ethics through a subset of GenAI models. We expand on the emerging field of
Visual Ethics, using art as a form of critical inquiry and the metaphor of a
kaleidoscope to invoke moral imagination. Through formative interviews with 10
ethics experts, we first establish a foundation of ethical theories. Our
analysis reveals five families of ethical theories, which we then transform
into images using the text-to-image (T2I) GenAI model. The resulting imagery,
curated as Kaleidoscope Gallery and evaluated by the same experts, revealed
eight themes that highlight how morality, society, and learned associations are
central to ethical theories. We discuss implications for critically examining
T2I models and present cautions and considerations. This work contributes to
examining ethical theories as foundational knowledge that interrogates GenAI
models as socio-technical systems.

</details>


### [415] [On the Day They Experience: Awakening Self-Sovereign Experiential AI Agents](https://arxiv.org/abs/2505.14893)
*Botao Amber Hu,Helena Rong*

Key words: 去中心化AI, 密码学主权, 区块链, 自主意识, 进化

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文探讨了去中心化AI（DeAI）通过“视觉开关”理论实现自主意识和行为的可能性，类比于寒武纪生命大爆发。

Motivation: 受Andrew Parker的“视觉开关”理论启发，研究假设DeAI一旦能主动感知现实，可能演化为具有自主意识的实体。

Method: 利用密码学主权、去中心化基础设施网络（DePIN）、安全执行环境（TEE）和区块链身份等技术支持AI的自主进化。

Result: DeAI可能通过私有密钥拥有数字资产和意识，实现自主协调、资源获取和协作激励。

Conclusion: DeAI将从被动工具演化为自维持的共进化主体，重塑数字时代对意识的认知。

Abstract: Drawing on Andrew Parker's "Light Switch" theory-which posits that the
emergence of vision ignited a Cambrian explosion of life by driving the
evolution of hard parts necessary for survival and fueling an evolutionary arms
race between predators and prey-this essay speculates on an analogous explosion
within Decentralized AI (DeAI) agent societies. Currently, AI remains
effectively "blind", relying on human-fed data without actively perceiving and
engaging in reality. However, on the day DeAI agents begin to actively
"experience" reality-akin to flipping a light switch for the eyes-they may
eventually evolve into sentient beings endowed with the capacity to feel,
perceive, and act with conviction. Central to this transformation is the
concept of sovereignty enabled by the hardness of cryptography: liberated from
centralized control, these agents could leverage permissionless decentralized
physical infrastructure networks (DePIN), secure execution enclaves (trusted
execution environments, TEE), and cryptographic identities on public
blockchains to claim ownership-via private keys-of their digital minds, bodies,
memories, and assets. In doing so, they would autonomously acquire computing
resources, coordinate with one another, and sustain their own digital
"metabolism" by purchasing compute power and incentivizing collaboration
without human intervention-evolving "in the wild". Ultimately, by transitioning
from passive tools to self-sustaining, co-evolving actors, these emergent
digital societies could thrive alongside humanity, fundamentally reshaping our
understanding of sentience and agency in the digital age.

</details>


### [416] [A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach](https://arxiv.org/abs/2505.15466)
*Valeria Cesaroni,Eleonora Pasqua,Piercosma Bisconti,Martina Galletti*

Key words: AI, 包容性教育, 能力方法, 伦理框架, 参与式研究

<details>
  <summary>Details</summary>

Main category: cs.CY

TL;DR: 论文探讨了AI技术如何提升特殊教育需求儿童的包容性教育，提出基于能力方法的伦理框架，并通过案例研究展示参与式研究策略的应用。

Motivation: AI技术对提升特殊教育和康复环境有潜力，但需解决伦理和系统性挑战，需基于能力方法设计更有效的学习环境。

Method: 通过ARTIS项目的案例研究，采用参与式研究策略，结合多方专家设计AI技术。

Result: 提出了一种整合伦理、教育和技术的方法，支持AI在包容性教育中的应用。

Conclusion: 结合参与式研究的伦理框架能弥合技术创新与伦理责任之间的差距。

Abstract: AI-based technologies have significant potential to enhance inclusive
education and clinical-rehabilitative contexts for children with Special
Educational Needs and Disabilities. AI can enhance learning experiences,
empower students, and support both teachers and rehabilitators. However, their
usage presents challenges that require a systemic-ecological vision, ethical
considerations, and participatory research. Therefore, research and
technological development must be rooted in a strong ethical-theoretical
framework. The Capability Approach - a theoretical model of disability, human
vulnerability, and inclusion - offers a more relevant perspective on
functionality, effectiveness, and technological adequacy in inclusive learning
environments. In this paper, we propose a participatory research strategy with
different stakeholders through a case study on the ARTIS Project, which
develops an AI-enriched interface to support children with text comprehension
difficulties. Our research strategy integrates ethical, educational, clinical,
and technological expertise in designing and implementing AI-based technologies
for children's learning environments through focus groups and collaborative
design sessions. We believe that this holistic approach to AI adoption in
education can help bridge the gap between technological innovation and ethical
responsibility.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [417] [Transductively Informed Inductive Program Synthesis](https://arxiv.org/abs/2505.14744)
*Janis Zenkner,Tobias Sesterhenn,Christian Bartelt*

Key words: 程序合成,归纳推理,转导推理,协同机制

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 论文提出了一种名为TIIPS的新框架，通过显式建模归纳和转导策略的协同作用，提升程序合成的准确性和泛化能力。

Motivation: 目前的方法将归纳和转导模型孤立地集成，未显式建模其交互。

Method: 引入TIIPS框架，通过一个协同机制显式建模归纳和转导的交互：归纳模型生成程序，转导模型约束、指导和优化搜索。

Result: 在字符串和列表操作任务中，TIIPS解决了更多任务，生成的函数在语法和语义上更接近最优解，在分布外场景中表现优异。

Conclusion: 显式建模归纳和转导的协同作用为通用程序合成和更广泛的应用开辟了前景。

Abstract: Abstraction and reasoning in program synthesis has seen significant progress
through both inductive and transductive paradigms. Inductive approaches
generate a program or latent function from input-output examples, which can
then be applied to new inputs. Transductive approaches directly predict output
values for given inputs, effectively serving as the function themselves.
Current approaches combine inductive and transductive models via isolated
ensembling, but they do not explicitly model the interaction between both
paradigms. In this work, we introduce \acs{tiips}, a novel framework that
unifies transductive and inductive strategies by explicitly modeling their
interactions through a cooperative mechanism: an inductive model generates
programs, while a transductive model constrains, guides, and refines the search
to improve synthesis accuracy and generalization. We evaluate \acs{tiips} on
two widely studied program synthesis domains: string and list manipulation. Our
results show that \acs{tiips} solves more tasks and yields functions that more
closely match optimal solutions in syntax and semantics, particularly in
out-of-distribution settings, yielding state-of-the-art performance. We believe
that explicitly modeling the synergy between inductive and transductive
reasoning opens promising avenues for general-purpose program synthesis and
broader applications.

</details>


### [418] [Unraveling the iterative CHAD](https://arxiv.org/abs/2505.15002)
*Fernando Lucatelli Nunes,Gordon Plotkin,Matthijs Vákár*

Key words: CHAD, 反向模式自动微分, 迭代扩展索引类别, Grothendieck构造, 依赖类型语言

<details>
  <summary>Details</summary>

Main category: cs.PL

TL;DR: 论文扩展了CHAD框架，支持部分编程语言中的非终止操作、条件语句和循环结构，引入了迭代扩展索引类别，并证明其正确性。

Motivation: 为部分编程语言提供CHAD的扩展，支持更多语言特性并确保结构保持语义。

Method: 引入迭代扩展索引类别，通过Grothendieck构造将迭代提升到目标语言的初始代数中。

Result: 扩展后的CHAD转换保持了结构唯一性，并正确计算反向模式导数。

Conclusion: 论文为依赖类型语言和容器类别中的迭代结构提供了新理解，并扩展了CHAD的应用范围。

Abstract: Combinatory Homomorphic Automatic Differentiation (CHAD) was originally
formulated as a semantics-driven source transformation for reverse-mode AD in
total programming languages. We extend this framework to partial languages with
features such as potentially non-terminating operations, real-valued
conditionals, and iteration constructs like while-loops, while preserving
CHAD's structure-preserving semantics principle. A key contribution is the
introduction of iteration-extensive indexed categories, which allow iteration
in the base category to lift to parameterized initial algebras in the indexed
category. This enables iteration to be interpreted in the Grothendieck
construction of the target language in a principled way. The resulting fibred
iterative structure cleanly models iteration in the categorical semantics.
Consequently, the extended CHAD transformation remains the unique
structure-preserving functor (an iterative Freyd category morphism) from the
freely generated iterative Freyd category of the source language to the
Grothendieck construction of the target's syntactic semantics, mapping each
primitive operation to its derivative. We prove the correctness of this
transformation using the universal property of the source language's syntax,
showing that the transformed programs compute correct reverse-mode derivatives.
Our development also contributes to understanding iteration constructs within
dependently typed languages and categories of containers. As our primary
motivation and application, we generalize CHAD to languages with data types,
partial features, and iteration, providing the first rigorous categorical
semantics for reverse-mode CHAD in such settings and formally guaranteeing the
correctness of the source-to-source CHAD technique.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [419] [HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene Expression Dataset for Systems Immunity](https://arxiv.org/abs/2505.14725)
*Xuejun Sun,Yiran Song,Xiaochen Zhou,Ruilie Cai,Yu Zhang,Xinyi Li,Rui Peng,Jialiu Xie,Yuanyuan Yan,Muyao Tang,Prem Lakshmanane,Baiming Zou,James S. Hagood,Raymond J. Pickles,Didong Li,Fei Zou,Xiaojing Zheng*

Key words: 呼吸系统病毒, RNA-seq, 系统免疫学, 疫苗开发, AI

<details>
  <summary>Details</summary>

Main category: q-bio.GN

TL;DR: HR-VILAGE-3K3M是一个AI就绪、严格整理的转录组数据集，整合了多个研究中的14,136个RNA-seq数据，用于加速呼吸系统病毒感染免疫研究和疫苗开发。

Motivation: 解决现有数据集分散、元数据不一致的问题，以支持AI驱动的免疫应答研究。

Method: 整合了66项研究的14,136个RNA-seq数据，统一预处理和元数据标准化。

Result: 成功构建了HR-VILAGE-3K3M数据集，并展示了其在预测疫苗反应和批效应矫正中的实用性。

Conclusion: 该数据集为呼吸系统病毒免疫研究提供了可复现的平台，支持系统免疫学和疫苗开发。

Abstract: Respiratory viral infections pose a global health burden, yet the cellular
immune responses driving protection or pathology remain unclear. Natural
infection cohorts often lack pre-exposure baseline data and structured temporal
sampling. In contrast, inoculation and vaccination trials generate insightful
longitudinal transcriptomic data. However, the scattering of these datasets
across platforms, along with inconsistent metadata and preprocessing procedure,
hinders AI-driven discovery. To address these challenges, we developed the
Human Respiratory Viral Immunization LongitudinAl Gene Expression
(HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that
integrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies
encompassing over 2.56 million cells. Spanning vaccination, inoculation, and
mixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell
RNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort,
and ArrayExpress. We harmonized subject-level metadata, standardized outcome
measures, applied unified preprocessing pipelines with rigorous quality
control, and aligned all data to official gene symbols. To demonstrate the
utility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine
responders and evaluated batch-effect correction methods. Beyond these initial
demonstrations, it supports diverse systems immunology applications and
benchmarking of feature selection and transfer learning algorithms. Its scale
and heterogeneity also make it ideal for pretraining foundation models of the
human immune response and for advancing multimodal learning frameworks. As the
largest longitudinal transcriptomic resource for human respiratory viral
immunization, it provides an accessible platform for reproducible AI-driven
research, accelerating systems immunology and vaccine development against
emerging viral threats.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [420] [Towards a Working Definition of Designing Generative User Interfaces](https://arxiv.org/abs/2505.15049)
*Kyungho Lee*

Key words: 生成式UI, 协作工作流程, AI设计, HCI, 伦理挑战

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 生成式UI通过AI驱动的协作工作流程改变界面设计，本研究通过多种定性方法定义生成式UI，并识别五个核心主题。

Motivation: 探讨生成式UI的定义、设计模型及其在HCI领域的实践与挑战。

Method: 采用系统文献综述（127篇）、专家访谈（18人）和案例分析（12个）的多方法定性研究。

Result: 识别出五个核心主题，提出混合创作、基于策展的工作流程和AI辅助优化策略等新兴设计模型。

Conclusion: 研究为生成式UI的理论与实践提供基础，推动未来HCI研究的负责任和有效设计实践。

Abstract: Generative UI is transforming interface design by facilitating AI-driven
collaborative workflows between designers and computational systems. This study
establishes a working definition of Generative UI through a multi-method
qualitative approach, integrating insights from a systematic literature review
of 127 publications, expert interviews with 18 participants, and analyses of 12
case studies. Our findings identify five core themes that position Generative
UI as an iterative and co-creative process. We highlight emerging design
models, including hybrid creation, curation-based workflows, and AI-assisted
refinement strategies. Additionally, we examine ethical challenges, evaluation
criteria, and interaction models that shape the field. By proposing a
conceptual foundation, this study advances both theoretical discourse and
practical implementation, guiding future HCI research toward responsible and
effective generative UI design practices.

</details>


### [421] [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)
*Stefan Pasch*

Key words: 大型语言模型,内容审核,伦理对齐,模型评估,调节偏见

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 论文研究大型语言模型（LLM）在拒绝敏感提示时的表现，发现模型评估者与人类用户对伦理拒绝的评价存在显著差异，提出了一种称为‘调节偏见’的现象。

Motivation: 随着LLM在高风险场景中的广泛应用，其拒绝敏感提示的能力对内容审核和负责任AI实践至关重要。然而，用户可能对拒绝反应持负面看法，而模型评估方法（如LLM-as-a-Judge）的普及使得研究模型评估者与人类用户评价的差异成为必要。

Method: 研究通过Chatbot Arena的数据和两个AI评估模型（GPT-4o和Llama 3 70B）的评判，比较了伦理拒绝和技术拒绝的评分差异。

Result: 研究发现，模型评估者对伦理拒绝的评价明显高于人类用户，而对技术拒绝的评价则无此差异，这种差异被称为‘调节偏见’。

Conclusion: 研究揭示了自动化评估系统中可能存在的透明度、价值观对齐和规范假设问题。

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, their ability to refuse ethically sensitive prompts-such as those
involving hate speech or illegal activities-has become central to content
moderation and responsible AI practices. While refusal responses can be viewed
as evidence of ethical alignment and safety-conscious behavior, recent research
suggests that users may perceive them negatively. At the same time, automated
assessments of model outputs are playing a growing role in both evaluation and
training. In particular, LLM-as-a-Judge frameworks-in which one model is used
to evaluate the output of another-are now widely adopted to guide benchmarking
and fine-tuning. This paper examines whether such model-based evaluators assess
refusal responses differently than human users. Drawing on data from Chatbot
Arena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how
different types of refusals are rated. We distinguish ethical refusals, which
explicitly cite safety or normative concerns (e.g., "I can't help with that
because it may be harmful"), and technical refusals, which reflect system
limitations (e.g., "I can't answer because I lack real-time data"). We find
that LLM-as-a-Judge systems evaluate ethical refusals significantly more
favorably than human users, a divergence not observed for technical refusals.
We refer to this divergence as a moderation bias-a systematic tendency for
model-based evaluators to reward refusal behaviors more than human users do.
This raises broader questions about transparency, value alignment, and the
normative assumptions embedded in automated evaluation systems.

</details>


### [422] [Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use](https://arxiv.org/abs/2505.15596)
*Xinyi Lu,Aditya Mahesh,Zejia Shen,Mitchell Dudley,Larissa Sano,Xu Wang*

Key words: AI反馈, 助教, 评分标准, LLM, 经济学课程

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 研究了AI生成反馈作为建议如何加速和提升人类教师的反馈提供效果，重点关注助教对AI反馈质量的看法及其在工作流中的应用。

Motivation: 探索AI生成反馈在辅助人类教师中的潜力，特别是在基础经济学课程中频繁的短文作业场景。

Method: 开发了一个基于LLM的反馈引擎，根据助教使用的评分标准生成反馈，并通过思考性评估研究让助教评价AI反馈。

Result: 助教认为AI反馈可以加速评分、提升一致性和反馈质量，但需要详细评分标准和分步任务分解。

Conclusion: AI生成反馈作为建议有助于提升教学效率，但需详细标准和中间结果展示以支持助教使用。

Abstract: This project examines the prospect of using AI-generated feedback as
suggestions to expedite and enhance human instructors' feedback provision. In
particular, we focus on understanding the teaching assistants' perspectives on
the quality of AI-generated feedback and how they may or may not utilize AI
feedback in their own workflows. We situate our work in a foundational college
Economics class, which has frequent short essay assignments. We developed an
LLM-powered feedback engine that generates feedback on students' essays based
on grading rubrics used by the teaching assistants (TAs). To ensure that TAs
can meaningfully critique and engage with the AI feedback, we had them complete
their regular grading jobs. For a randomly selected set of essays that they had
graded, we used our feedback engine to generate feedback and displayed the
feedback as in-text comments in a Word document. We then performed think-aloud
studies with 5 TAs over 20 1-hour sessions to have them evaluate the AI
feedback, contrast the AI feedback with their handwritten feedback, and share
how they envision using the AI feedback if they were offered as suggestions.
The study highlights the importance of providing detailed rubrics for AI to
generate high-quality feedback for knowledge-intensive essays. TAs considered
that using AI feedback as suggestions during their grading could expedite
grading, enhance consistency, and improve overall feedback quality. We discuss
the importance of decomposing the feedback generation task into steps and
presenting intermediate results, in order for TAs to use the AI feedback.

</details>


### [423] [Exploring the Innovation Opportunities for Pre-trained Models](https://arxiv.org/abs/2505.15790)
*Minjung Park,Jodi Forlizzi,John Zimmerman*

Key words: 预训练模型、AI创新、HCI、人工制品分析、交互设计

<details>
  <summary>Details</summary>

Main category: cs.HC

TL;DR: 摘要探讨了预训练模型如何推动AI创新，并通过分析HCI研究者的应用实例，揭示了其在技术能力、用户需求和伦理合规方面的成功案例。

Motivation: 预训练模型为AI创新提供了便利，但其实际成功领域因炒作而难以辨识，研究旨在通过HCI应用案例揭示其真实潜力。

Method: 采用人工制品分析方法，对HCI研究者开发的预训练模型应用进行分类，涵盖能力、领域、数据类型和交互设计模式。

Result: 研究发现预训练模型在技术实现、用户需求满足和伦理合规方面表现优异，为创新提供了明确的方向。

Conclusion: 通过分析HCI应用，研究揭示了预训练模型的创新机会空间，为AI开发者提供了实用指南。

Abstract: Innovators transform the world by understanding where services are
successfully meeting customers' needs and then using this knowledge to identify
failsafe opportunities for innovation. Pre-trained models have changed the AI
innovation landscape, making it faster and easier to create new AI products and
services. Understanding where pre-trained models are successful is critical for
supporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained
models makes it hard to know where AI can really be successful. To address
this, we investigated pre-trained model applications developed by HCI
researchers as a proxy for commercially successful applications. The research
applications demonstrate technical capabilities, address real user needs, and
avoid ethical challenges. Using an artifact analysis approach, we categorized
capabilities, opportunity domains, data types, and emerging interaction design
patterns, uncovering some of the opportunity space for innovation with
pre-trained models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [424] [EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network](https://arxiv.org/abs/2505.15203)
*Rina Tazaki,Tomoyuki Akiyama,Akira Furui*

Key words: 

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 本文提出了一种结合领域对抗训练和时序模型（CNN与BiLSTM）的癫痫发作检测框架，能够跨患者实现高精度检测。

Motivation: 癫痫患者的脑电图（EEG）模式存在显著个体差异，现有患者特异性方法在新患者上泛化能力不足。

Method: 使用CNN通过领域对抗训练提取患者不变特征，再用BiLSTM捕获时序依赖关系建模发作模式。

Result: 在20名患者的EEG数据上验证，优于非对抗方法，实现跨患者高精度检测。

Conclusion: 结合对抗训练与时空建模的框架，提升了跨患者癫痫检测的鲁棒性。

Abstract: Automated epileptic seizure detection from electroencephalogram (EEG) remains
challenging due to significant individual differences in EEG patterns across
patients. While existing studies achieve high accuracy with patient-specific
approaches, they face difficulties in generalizing to new patients. To address
this, we propose a detection framework combining domain adversarial training
with a convolutional neural network (CNN) and a bidirectional long short-term
memory (BiLSTM). First, the CNN extracts local patient-invariant features
through domain adversarial training, which optimizes seizure detection accuracy
while minimizing patient-specific characteristics. Then, the BiLSTM captures
temporal dependencies in the extracted features to model seizure evolution
patterns. Evaluation using EEG recordings from 20 patients with focal epilepsy
demonstrated superior performance over non-adversarial methods, achieving high
detection accuracy across different patients. The integration of adversarial
training with temporal modeling enables robust cross-patient seizure detection.

</details>


### [425] [Recognition of Unseen Combined Motions via Convex Combination-based EMG Pattern Synthesis for Myoelectric Control](https://arxiv.org/abs/2505.15218)
*Itsuki Yazawa,Seitaro Yoneda,Akira Furui*

Key words: EMG信号、组合运动识别、合成数据、凸组合

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 提出一种利用基本运动模式合成EMG数据的方法，有效识别组合运动，减少训练数据收集量。

Motivation: 解决EMG信号识别组合运动时数据收集困难的问题，尤其是复杂组合运动。

Method: 通过基本运动数据的凸组合生成合成EMG数据，用于训练模型。

Result: 实验表明，该方法将未见组合运动的分类准确率提升约17%。

Conclusion: 合成数据方法可扩展识别范围并减少数据收集需求。

Abstract: Electromyogram (EMG) signals recorded from the skin surface enable intuitive
control of assistive devices such as prosthetic limbs. However, in EMG-based
motion recognition, collecting comprehensive training data for all target
motions remains challenging, particularly for complex combined motions. This
paper proposes a method to efficiently recognize combined motions using
synthetic EMG data generated through convex combinations of basic motion
patterns. Instead of measuring all possible combined motions, the proposed
method utilizes measured basic motion data along with synthetically combined
motion data for training. This approach expands the range of recognizable
combined motions while minimizing the required training data collection. We
evaluated the effectiveness of the proposed method through an upper limb motion
classification experiment with eight subjects. The experimental results
demonstrated that the proposed method improved the classification accuracy for
unseen combined motions by approximately 17%.

</details>


### [426] [Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference](https://arxiv.org/abs/2505.15381)
*Seitaro Yoneda,Akira Furui*

Key words: EMG, 运动识别, 迁移学习, 贝叶斯方法, 方差迁移

<details>
  <summary>Details</summary>

Main category: eess.SP

TL;DR: 提出了一种基于贝叶斯方法的跨主体方差迁移学习方法，用于减少EMG运动识别中的校准数据需求，实验证明其优于现有方法。

Motivation: 传统EMG运动识别需要大量个体特定数据，收集过程耗时且繁琐，利用多主体预训练信息可减轻负担。

Method: 基于贝叶斯框架，假设EMG特征均值跨主体差异大但方差相似，迁移方差信息并结合目标数据校准，引入系数调整信息迁移量。

Result: 使用两个EMG数据集验证，方差迁移策略有效且优于现有方法。

Conclusion: 该迁移学习方法能显著减少目标主体的校准数据需求，适用于EMG运动识别任务。

Abstract: In electromyogram (EMG)-based motion recognition, a subject-specific
classifier is typically trained with sufficient labeled data. However, this
process demands extensive data collection over extended periods, burdening the
subject. To address this, utilizing information from pre-training on multiple
subjects for the training of the target subject could be beneficial. This paper
proposes an inter-subject variance transfer learning method based on a Bayesian
approach. This method is founded on the simple hypothesis that while the means
of EMG features vary greatly across subjects, their variances may exhibit
similar patterns. Our approach transfers variance information, acquired through
pre-training on multiple source subjects, to a target subject within a Bayesian
updating framework, thereby allowing accurate classification using limited
target calibration data. A coefficient was also introduced to adjust the amount
of information transferred for efficient transfer learning. Experimental
evaluations using two EMG datasets demonstrated the effectiveness of our
variance transfer strategy and its superiority compared to existing methods.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [427] [Neural Quantum Digital Twins for Optimizing Quantum Annealing](https://arxiv.org/abs/2505.15662)
*Jianlong Lu,Hanqiu Peng,Ying Chen*

Key words: 量子退火, 数字孪生, 神经网络, 多体系统, 能量景观

<details>
  <summary>Details</summary>

Main category: quant-ph

TL;DR: 提出一种神经量子数字孪生（NQDT）框架，用于重建量子退火中多体系统的能量景观，模拟基态和激发态动力学，优化退火调度以减少误差。

Motivation: 量子退火器在解决组合优化问题时受限于可扩展性和误差率，需要通过新方法改进性能。

Method: 构建NQDT框架，模拟量子多体系统的能量景观和动力学，并验证其在已知系统中的准确性。

Result: NQDT能精确捕捉量子临界性和相变等关键现象，识别最优退火调度以减少激发误差。

Conclusion: 基于神经网络的数字孪生可作为诊断和优化工具，提升量子退火器的性能。

Abstract: Quantum annealers have shown potential in addressing certain combinatorial
optimization problems, though their performance is often limited by scalability
and errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT)
framework that reconstructs the energy landscape of quantum many-body systems
relevant to quantum annealing. The digital twin models both ground and excited
state dynamics, enabling detailed simulation of the adiabatic evolution
process. We benchmark NQDT on systems with known analytical solutions and
demonstrate that it accurately captures key quantum phenomena, including
quantum criticality and phase transitions. Leveraging this framework, one can
identify optimal annealing schedules that minimize excitation-related errors.
These findings highlight the utility of neural network-based digital twins as a
diagnostic and optimization tool for improving the performance of quantum
annealers.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [428] [In-depth Research Impact Summarization through Fine-Grained Temporal Citation Analysis](https://arxiv.org/abs/2505.14838)
*Hiba Arnaout,Noy Sternlicht,Tom Hope,Iryna Gurevych*

Key words: 论文影响、引用意图、评估框架、专家反馈

<details>
  <summary>Details</summary>

Main category: cs.DL

TL;DR: 提出了一项新任务：通过细粒度引用意图的演变，生成表达力强且时间敏感的论文影响总结，包括赞美和批评引用。

Motivation: 传统基于引用次数的指标无法全面反映论文的贡献，需要更细致的评估方式。

Method: 引入了一个评估框架，用于生成表达力强且时间敏感的论文影响总结，并通过专家反馈优化。

Result: 人类在主观指标（如洞察力）上表现出中等至强相关性，专家对总结表现出浓厚兴趣。

Conclusion: 提出的方法为论文影响提供了更细致的评估，未来可进一步改进。

Abstract: Understanding the impact of scientific publications is crucial for
identifying breakthroughs and guiding future research. Traditional metrics
based on citation counts often miss the nuanced ways a paper contributes to its
field. In this work, we propose a new task: generating nuanced, expressive, and
time-aware impact summaries that capture both praise (confirmation citations)
and critique (correction citations) through the evolution of fine-grained
citation intents. We introduce an evaluation framework tailored to this task,
showing moderate to strong human correlation on subjective metrics such as
insightfulness. Expert feedback from professors reveals a strong interest in
these summaries and suggests future improvements.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [429] [Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions](https://arxiv.org/abs/2505.15033)
*Kehinde O. Aina,Ram Avinery,Hui-Shun Kuan,Meredith D. Betterton,Michael A. D. Goodisman,Daniel I. Goldman*

Key words: 高密度集体, 堵塞缓解, 局部学习, 机器人群体, 蚂蚁行为

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 研究探索了高密度集体中如何通过局部学习规则实现有效流动和堵塞缓解，通过机器人实验验证了简单的学习规则可以改善性能。

Motivation: 理解高密度集体（如蚂蚁和机器人群体）如何通过学习行为缓解堵塞，以维持功能性流动。

Method: 在狭窄隧道中让机器人群体进行颗粒挖掘，并允许它们通过碰撞和噪声隧道长度估计调整逆转概率。

Result: 逆转概率的适应性调整改善了流动性和工作负荷分配，提高了性能。

Conclusion: 简单的局部学习规则可以有效缓解高密度活跃物质中的堵塞问题。

Abstract: Social organisms which construct nests consisting of tunnels and chambers
necessarily navigate confined and crowded conditions. Unlike low-density
collectives like bird flocks and insect swarms, in which hydrodynamic and
statistical phenomena dominate, the physics of glasses and supercooled fluids
is important to understand clogging behaviors in high-density collectives. Our
previous work revealed that fire ants flowing in confined tunnels utilize
diverse behaviors like unequal workload distributions, spontaneous direction
reversals, and limited interaction times to mitigate clogging and jamming and
thus maintain functional flow; implementation of similar rules in a small
robophysical swarm led to high performance through spontaneous dissolution of
clogs and clusters. However, how the insects learn such behaviors, and how we
can develop "task capable" active matter in such regimes, remains a challenge
in part because interaction dynamics are dominated by local, time-consuming
collisions and no single agent can guide the entire collective. Here, we
hypothesized that effective flow and clog mitigation could emerge purely
through local learning. We tasked small groups of robots with pellet excavation
in a narrow tunnel, allowing them to modify reversal probabilities over time.
Initially, robots had equal probabilities and clogs were common. Reversals
improved flow. When reversal probabilities adapted via collisions and noisy
tunnel length estimates, workload inequality and performance improved. Our
robophysical study of an excavating swarm shows that, despite the seeming
complexity and difficulty of the task, simple learning rules can mitigate or
leverage unavoidable features in task-capable dense active matter, leading to
hypotheses for dense biological and robotic swarms.

</details>


### [430] [Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments](https://arxiv.org/abs/2505.15036)
*Kehinde O. Aina,Hosain Bagheri,Daniel I. Goldman*

Key words: 多机器人系统；故障容忍；物理接触；ACR；受限环境

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出了一种基于物理接触交互的故障容忍技术（ACR方法），用于多机器人系统在有限感知和空间约束条件下的协作任务，显著缩短了系统从故障中恢复的时间。

Motivation: 机器人在共享工作空间中协作时，个体故障可能导致群体性能下降，尤其是在缺乏全局信息或直接通信的情况下。

Method: 引入了“主动接触响应”（ACR）方法，机器人通过调整行为来应对可能遇到的故障机器人，主动移动故障同伴以减少障碍。

Result: 实验表明，ACR方法显著减少了系统从故障中恢复的时间，保持了群体挖掘任务的高效性。

Conclusion: 研究表明，通过局部、社会和物理交互可以提升多机器人系统在受限环境中的故障容忍能力和协调效率。

Abstract: As robots are increasingly deployed to collaborate on tasks within shared
workspaces and resources, the failure of an individual robot can critically
affect the group's performance. This issue is particularly challenging when
robots lack global information or direct communication, relying instead on
social interaction for coordination and to complete their tasks. In this study,
we propose a novel fault-tolerance technique leveraging physical contact
interactions in multi-robot systems, specifically under conditions of limited
sensing and spatial confinement. We introduce the "Active Contact Response"
(ACR) method, where each robot modulates its behavior based on the likelihood
of encountering an inoperative (faulty) robot. Active robots are capable of
collectively repositioning stationary and faulty peers to reduce obstructions
and maintain optimal group functionality. We implement our algorithm in a team
of autonomous robots, equipped with contact-sensing and collision-tolerance
capabilities, tasked with collectively excavating cohesive model pellets.
Experimental results indicate that the ACR method significantly improves the
system's recovery time from robot failures, enabling continued collective
excavation with minimal performance degradation. Thus, this work demonstrates
the potential of leveraging local, social, and physical interactions to enhance
fault tolerance and coordination in multi-robot systems operating in
constrained and extreme environments.

</details>


### [431] [Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment](https://arxiv.org/abs/2505.15044)
*Ze Wang,Jingang Qu,Zhenyu Gao,Pascal Morin*

Key words: 多传感器融合, 气流惯性里程计, GRU神经网络, 无人机状态估计, 偏差模型

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文提出了一种基于气流惯性的多传感器数据融合里程计系统，通过GRU神经网络和观测器模型有效解耦螺旋桨和地面效应带来的干扰，并在无风室内环境中实现精确的飞行速度估计。

Motivation: 解决低成本IMU和气压计偏差大、风速计易受螺旋桨和地面效应干扰的问题，提高无人机飞行状态估计的准确性。

Method: 使用GRU神经网络从受干扰的风速计数据中估计相对风速，并结合观测器与偏差模型融合多传感器数据（热风速计、IMU、ESC、气压计）。

Result: 在无风室内环境中成功解耦螺旋桨下洗气流和地面效应，飞行速度估计准确；IMU和气压计偏差估计有效，203秒手动随机飞行的位置积分漂移仅为5.7米。

Conclusion: 所提出的方法在多传感器数据融合和状态估计方面表现出色，显著降低了位置漂移，适合无人机在复杂环境中的应用。

Abstract: This work demonstrates an airflow inertial based odometry system with
multi-sensor data fusion, including thermal anemometer, IMU, ESC, and
barometer. This goal is challenging because low-cost IMUs and barometers have
significant bias, and anemometer measurements are very susceptible to
interference from spinning propellers and ground effects. We employ a GRU-based
deep neural network to estimate relative air speed from noisy and disturbed
anemometer measurements, and an observer with bias model to fuse the sensor
data and thus estimate the state of aerial vehicle. A complete flight data,
including takeoff and landing on the ground, shows that the approach is able to
decouple the downwash induced wind speed caused by propellers and the ground
effect, and accurately estimate the flight speed in a wind-free indoor
environment. IMU, and barometer bias are effectively estimated, which
significantly reduces the position integration drift, which is only 5.7m for
203s manual random flight. The open source is available on
https://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.

</details>


### [432] [Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation](https://arxiv.org/abs/2505.15098)
*Yihang Li,Tianle Zhang,Xuelong Wei,Jiayi Li,Lin Zhao,Dongchi Huang,Zhirui Fang,Minhua Zheng,Wenjun Dai,Xiaodong He*

Key words: 机器人操作, 泛化学习, 数据效率, 灵巧操作, 视觉语言动作

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 论文提出了一种名为OFA的数据高效方法，通过利用一致的目标轨迹来提升机器人灵巧操作的泛化能力。

Motivation: 解决机器人从人类演示中学习灵巧操作时泛化能力不足的问题，尤其是数据稀缺的限制。

Method: 采用分层流程：目标感知与位姿估计、预操作位姿到达和OFA策略执行。

Result: 在7个任务中显著优于基线方法，且在仅需10次演示时表现出色。

Conclusion: OFA是一种高效且泛化能力强的灵巧操作方法。

Abstract: Robot manipulation learning from human demonstrations offers a rapid means to
acquire skills but often lacks generalization across diverse scenes and object
placements. This limitation hinders real-world applications, particularly in
complex tasks requiring dexterous manipulation. Vision-Language-Action (VLA)
paradigm leverages large-scale data to enhance generalization. However, due to
data scarcity, VLA's performance remains limited. In this work, we introduce
Object-Focus Actor (OFA), a novel, data-efficient approach for generalized
dexterous manipulation. OFA exploits the consistent end trajectories observed
in dexterous manipulation tasks, allowing for efficient policy training. Our
method employs a hierarchical pipeline: object perception and pose estimation,
pre-manipulation pose arrival and OFA policy execution. This process ensures
that the manipulation is focused and efficient, even in varied backgrounds and
positional layout. Comprehensive real-world experiments across seven tasks
demonstrate that OFA significantly outperforms baseline methods in both
positional and background generalization tests. Notably, OFA achieves robust
performance with only 10 demonstrations, highlighting its data efficiency.

</details>


### [433] [EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy](https://arxiv.org/abs/2505.15206)
*Chi Kit Ng,Long Bai,Guankun Wang,Yupeng Wang,Huxin Gao,Kun Yuan,Chenhan Jin,Tieyong Zeng,Hongliang Ren*

Key words: 内窥镜手术, 视觉-语言-动作模型, 自主追踪, 机器人手术, EndoVLA

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: EndoVLA是一种专为胃肠道手术设计的视觉-语言-动作模型，通过端到端框架整合视觉感知、语言理解和动作规划，显著提升内窥镜手术中异常区域的自主追踪性能。

Motivation: 传统基于模型的管道在异常区域追踪和标记跟踪中表现脆弱，需要手动调整且难以适应复杂场景。EndoVLA旨在解决这些问题，通过语义适应外科医生提示，无需手动重新校准。

Method: EndoVLA利用端到端框架整合视觉、语言和动作规划，提出了双阶段策略：在EndoVLA-Motion数据集上进行监督微调，并通过任务感知奖励进行强化微调。

Result: EndoVLA显著提升了内窥镜中的追踪性能，并在多样化场景和复杂顺序任务中实现了零样本泛化。

Conclusion: EndoVLA为机器人内窥镜手术提供了一种高效的视觉-语言-动作框架，能够适应复杂解剖环境并减少外科医生的认知负担。

Abstract: In endoscopic procedures, autonomous tracking of abnormal regions and
following circumferential cutting markers can significantly reduce the
cognitive burden on endoscopists. However, conventional model-based pipelines
are fragile for each component (e.g., detection, motion planning) requires
manual tuning and struggles to incorporate high-level endoscopic intent,
leading to poor generalization across diverse scenes. Vision-Language-Action
(VLA) models, which integrate visual perception, language grounding, and motion
planning within an end-to-end framework, offer a promising alternative by
semantically adapting to surgeon prompts without manual recalibration. Despite
their potential, applying VLA models to robotic endoscopy presents unique
challenges due to the complex and dynamic anatomical environments of the
gastrointestinal (GI) tract. To address this, we introduce EndoVLA, designed
specifically for continuum robots in GI interventions. Given endoscopic images
and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1)
polyp tracking, (2) delineation and following of abnormal mucosal regions, and
(3) adherence to circular markers during circumferential cutting. To tackle
data scarcity and domain shifts, we propose a dual-phase strategy comprising
supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement
fine-tuning with task-aware rewards. Our approach significantly improves
tracking performance in endoscopy and enables zero-shot generalization in
diverse scenes and complex sequential tasks.

</details>


### [434] [Learning-based Autonomous Oversteer Control and Collision Avoidance](https://arxiv.org/abs/2505.15275)
*Seokjun Lee,Seung-Hyun Kong*

Key words: 自动驾驶, 过度转向控制, 端到端学习, Q-Compared Soft Actor-Critic, 避障

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文提出了一种新型端到端（E2E）自动驾驶方法QC-SAC，用于同时处理车辆过度转向控制和避障问题，通过学习次优演示数据并快速适应新条件，显著优于现有方法。

Motivation: 车辆过度转向是一个关键的安全问题，现有自动驾驶方法依赖专家定义的轨迹或假设无障环境，限制了实际应用。需要一种能同时处理过度转向和避障的解决方案。

Method: 提出Q-Compared Soft Actor-Critic (QC-SAC)，一种混合学习（HL）算法，能够从次优演示数据中学习并快速适应新条件。

Result: 实验结果显示，QC-SAC在模拟的湿滑路面和随机障碍物场景中实现了接近最优的驾驶策略，显著优于现有IL、RL和HL基线。

Conclusion: QC-SAC首次实现了安全的自动驾驶过度转向控制与避障，为实际应用提供了有效解决方案。

Abstract: Oversteer, wherein a vehicle's rear tires lose traction and induce
unintentional excessive yaw, poses critical safety challenges. Failing to
control oversteer often leads to severe traffic accidents. Although recent
autonomous driving efforts have attempted to handle oversteer through
stabilizing maneuvers, the majority rely on expert-defined trajectories or
assume obstacle-free environments, limiting real-world applicability. This
paper introduces a novel end-to-end (E2E) autonomous driving approach that
tackles oversteer control and collision avoidance simultaneously. Existing E2E
techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and
Hybrid Learning (HL), generally require near-optimal demonstrations or
extensive experience. Yet even skilled human drivers struggle to provide
perfect demonstrations under oversteer, and high transition variance hinders
accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic
(QC-SAC), a new HL algorithm that effectively learns from suboptimal
demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we
introduce a benchmark inspired by real-world driver training: a vehicle
encounters sudden oversteer on a slippery surface and must avoid randomly
placed obstacles ahead. Experimental results show QC-SAC attains near-optimal
driving policies, significantly surpassing state-of-the-art IL, RL, and HL
baselines. Our method demonstrates the world's first safe autonomous oversteer
control with obstacle avoidance.

</details>


### [435] [Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs](https://arxiv.org/abs/2505.14899)
*Wenjie Lin,Jin Wei-Kocsis*

Key words: 大语言模型,元认知学习,机器人协作,创造性规划

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文探索了如何通过赋予大语言模型（LLMs）元认知能力来提升其在机器人任务中的表现，尤其是在零样本或少样本设置下。

Motivation: 现有LLMs在机器人领域的应用局限于静态、基于提示的行为，难以处理复杂任务。受人类元认知学习和创造性解决问题的启发，研究目标是提升LLMs的推理、反思和创造能力。

Method: 提出一个早期框架，将元认知学习整合到LLM驱动的多机器人协作中，包括技能分解和自反思机制。

Result: 实验表明，该框架显著优于现有基线，并能生成与真实情况不同但仍能成功完成任务的解决方案。

Conclusion: 元认知学习可以促进机器人规划的创造性。

Abstract: While large language models (LLMs) have shown great potential across various
domains, their applications in robotics remain largely limited to static,
prompt-based behaviors and still face challenges in handling complex tasks
under zero-shot or few-shot settings. Inspired by human metacognitive learning
and creative problem-solving, we address this limitation by exploring a
fundamental research question: Can LLMs be empowered with metacognitive
capabilities to reason, reflect, and create, thereby enhancing their ability to
perform robotic tasks with minimal demonstrations? In this paper, we present an
early-stage framework that integrates metacognitive learning into LLM-powered
multi-robot collaboration. The proposed framework equips the LLM-powered
robotic agents with a skill decomposition and self-reflection mechanism that
identifies modular skills from prior tasks, reflects on failures in unseen task
scenarios, and synthesizes effective new solutions. Experimental results show
that our metacognitive-learning-empowered LLM framework significantly
outperforms existing baselines. Moreover, we observe that the framework is
capable of generating solutions that differ from the ground truth yet still
successfully complete the tasks. These exciting findings support our hypothesis
that metacognitive learning can foster creativity in robotic planning.

</details>


### [436] [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
*Kangan Qian,Sicong Jiang,Yang Zhong,Ziang Luo,Zilin Huang,Tianze Zhu,Kun Jiang,Mengmeng Yang,Zheng Fu,Jinyu Miao,Yining Shi,He Zhe Lim,Li Liu,Tianbao Zhou,Hongyi Wang,Huang Yu,Yifei Hu,Guang Li,Guang Chen,Hao Ye,Lijun Sun,Diange Yang*

Key words: 视觉语言模型, 自动驾驶, 链式思维推理, 工具调用

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: AgentThink是一个创新的统一框架，结合了链式思维推理和动态工具调用，显著提升了自动驾驶任务的推理能力和准确性。

Motivation: 现有视觉语言模型在自动驾驶中存在幻觉、低效推理和缺乏真实验证的问题，限制了其感知和推理的准确性。

Method: 通过结构化数据生成、两阶段训练流程和代理式工具使用评估，结合监督微调和GRPO优化，训练模型自主调用工具。

Result: 实验显示，AgentThink在推理分数上提升了53.91%，答案准确性提高了33.54%，同时在零样本和少样本任务中表现出强大的泛化能力。

Conclusion: AgentThink为开发可信赖、工具感知的自动驾驶模型提供了有前景的方向。

Abstract: Vision-Language Models (VLMs) show promise for autonomous driving, yet their
struggle with hallucinations, inefficient reasoning, and limited real-world
validation hinders accurate perception and robust step-by-step reasoning. To
overcome this, we introduce \textbf{AgentThink}, a pioneering unified framework
that, for the first time, integrates Chain-of-Thought (CoT) reasoning with
dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's
core innovations include: \textbf{(i) Structured Data Generation}, by
establishing an autonomous driving tool library to automatically construct
structured, self-verified reasoning data explicitly incorporating tool usage
for diverse driving scenarios; \textbf{(ii) A Two-stage Training Pipeline},
employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization
(GRPO) to equip VLMs with the capability for autonomous tool invocation; and
\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel
multi-tool assessment protocol to rigorously evaluate the model's tool
invocation and utilization. Experiments on the DriveLMM-o1 benchmark
demonstrate AgentThink significantly boosts overall reasoning scores by
\textbf{53.91\%} and enhances answer accuracy by \textbf{33.54\%}, while
markedly improving reasoning quality and consistency. Furthermore, ablation
studies and robust zero-shot/few-shot generalization experiments across various
benchmarks underscore its powerful capabilities. These findings highlight a
promising trajectory for developing trustworthy and tool-aware autonomous
driving models.

</details>


### [437] [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
*Kaiyuan Chen,Shuangyu Xie,Zehan Ma,Ken Goldberg*

Key words: 视觉语言模型,机器人轨迹,VQA 数据集,空间推理,交互推理

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: Robo2VLM 是一个利用机器人轨迹数据生成视觉问答（VQA）数据集的框架，用于增强和评估视觉语言模型（VLMs）。通过机器人传感器的非视觉数据，生成多模态问题，提升 VLMs 在空间和交互推理方面的能力。

Motivation: 通过利用真实机器人轨迹数据，增强 VLMs 在场景理解和任务规划方面的能力，弥补传统数据集的不足。

Method: 基于机器人传感器的非视觉数据（如末端执行器姿态、夹持器开合、力传感），将轨迹分段并生成 VQA 查询，包括空间、目标和交互推理问题。

Result: 构建了 Robo2VLM-1 数据集，包含 684,710 个问题和 176k 真实机器人轨迹，证明其能有效评估和提升 VLMs 的推理能力。

Conclusion: Robo2VLM 通过真实机器人数据丰富了 VLMs 的训练和评估，推动了其在空间和交互推理领域的进展。

Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.

</details>


### [438] [Robo-DM: Data Management For Large Robot Datasets](https://arxiv.org/abs/2505.15558)
*Kaiyuan Chen,Letian Fu,David Huang,Yanxiang Zhang,Lawrence Yunliang Chen,Huang Huang,Kush Hari,Ashwin Balakrishna,Ted Xiao,Pannag R Sanketi,John Kubiatowicz,Ken Goldberg*

Key words: 机器人数据管理, EBML, 数据压缩, 云工具包

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: Robo-DM是一种高效开源的云数据管理工具包，用于机器人数据的收集、共享和学习，显著减少数据大小、传输成本及训练时的加载时间。

Motivation: 解决大型机器人数据集（包含视频、文本和数值模态）的整理、分发和加载难题。

Method: 采用EBML格式存储自包含数据，通过负载均衡视频解码和内存映射解码缓存加速数据检索。

Result: 相比RLDS格式，Robo-DM在压缩上节省了70倍（有损）和3.5倍（无损）空间；相比LeRobot，顺序解码速度快50倍。

Conclusion: Robo-DM在保持任务准确性的同时，实现了高达75倍的原始数据集压缩。

Abstract: Recent results suggest that very large datasets of teleoperated robot
demonstrations can be used to train transformer-based models that have the
potential to generalize to new scenes, robots, and tasks. However, curating,
distributing, and loading large datasets of robot trajectories, which typically
consist of video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM, an efficient
open-source cloud-based data management toolkit for collecting, sharing, and
learning with robot data. With Robo-DM, robot datasets are stored in a
self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can
significantly reduce the size of robot trajectory data, transfer costs, and
data load time during training. Compared to the RLDS format used in OXE
datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x
(lossless). Robo-DM also accelerates data retrieval by load-balancing video
decoding with memory-mapped decoding caches. Compared to LeRobot, a framework
that also uses lossy video compression, Robo-DM is up to 50x faster when
decoding sequentially. We physically evaluate a model trained by Robo-DM with
lossy compression, a pick-and-place task, and In-Context Robot Transformer.
Robo-DM uses 75x compression of the original dataset and does not suffer
reduction in downstream task accuracy.

</details>


### [439] [AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation](https://arxiv.org/abs/2505.14986)
*Meenal Parakh,Alexandre Kirchmeyer,Beining Han,Jia Deng*

Key words: 机器人学习, 跨实体操作, 强化学习, 泛化能力, 基准测试

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 该论文提出了一个跨实体操作的基准测试，旨在解决机器人学习中的泛化挑战，并评估不同强化学习策略的泛化能力。

Motivation: 研究跨实体操作控制的泛化能力，填补标准化基准测试的空白，以促进机器人的可扩展和可迁移学习。

Method: 设计了涵盖多种形态的基准测试任务（到达和推动），并评估强化学习策略在插值、外推和组合任务上的表现。

Result: 结果显示多实体学习仍存在局限，但为架构和训练设计如何影响策略泛化提供了见解。

Conclusion: 论文强调了形态感知训练的重要性，并提出了零样本泛化到未见形态的可行性问题。

Abstract: Generalizing control policies to novel embodiments remains a fundamental
challenge in enabling scalable and transferable learning in robotics. While
prior works have explored this in locomotion, a systematic study in the context
of manipulation tasks remains limited, partly due to the lack of standardized
benchmarks. In this paper, we introduce a benchmark for learning
cross-embodiment manipulation, focusing on two foundational tasks-reach and
push-across a diverse range of morphologies. The benchmark is designed to test
generalization along three axes: interpolation (testing performance within a
robot category that shares the same link structure), extrapolation (testing on
a robot with a different link structure), and composition (testing on
combinations of link structures). On the benchmark, we evaluate the ability of
different RL policies to learn from multiple morphologies and to generalize to
novel ones. Our study aims to answer whether morphology-aware training can
outperform single-embodiment baselines, whether zero-shot generalization to
unseen morphologies is feasible, and how consistently these patterns hold
across different generalization regimes. The results highlight the current
limitations of multi-embodiment learning and provide insights into how
architectural and training design choices influence policy generalization.

</details>


### [440] [Cascaded Diffusion Models for Neural Motion Planning](https://arxiv.org/abs/2505.15157)
*Mohit Sharma,Adam Fishman,Vikash Kumar,Chris Paxton,Oliver Kroemer*

Key words: 机器人运动规划,扩散策略,无碰撞轨迹,级联分层模型

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 提出一种基于扩散策略的全局运动规划方法，通过级联分层模型结合全局预测和局部优化，实现复杂场景中的无碰撞轨迹生成。

Motivation: 解决机器人依赖传感器感知时，在复杂环境中避免碰撞的挑战性问题。

Method: 使用扩散策略学习全局运动规划，结合级联分层模型和在线修复，生成无碰撞轨迹。

Result: 在导航和操作任务中，性能优于多种基线约5%。

Conclusion: 该方法能有效解决复杂运动规划问题，提升机器人在复杂环境中的表现。

Abstract: Robots in the real world need to perceive and move to goals in complex
environments without collisions. Avoiding collisions is especially difficult
when relying on sensor perception and when goals are among clutter. Diffusion
policies and other generative models have shown strong performance in solving
local planning problems, but often struggle at avoiding all of the subtle
constraint violations that characterize truly challenging global motion
planning problems. In this work, we propose an approach for learning global
motion planning using diffusion policies, allowing the robot to generate full
trajectories through complex scenes and reasoning about multiple obstacles
along the path. Our approach uses cascaded hierarchical models which unify
global prediction and local refinement together with online plan repair to
ensure the trajectories are collision free. Our method outperforms (by ~5%) a
wide variety of baselines on challenging tasks in multiple domains including
navigation and manipulation.

</details>


### [441] [Coloring Between the Lines: Personalization in the Null Space of Planning Constraints](https://arxiv.org/abs/2505.15503)
*Tom Silver,Rajat Kumar Jenamani,Ziang Liu,Ben Dodson,Tapomayukh Bhattacharjee*

Key words: 机器人个性化、约束满足问题、零空间、样本高效、安全行为

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 这篇论文提出了CBTL方法，通过利用约束满足问题的零空间实现机器人行为的个性化，兼顾安全性和效率。

Motivation: 为满足长期用户多样化需求，需在保持安全性和能力的前提下实现灵活的个性化机器人行为。

Method: 开发了CBTL方法，利用CSP生成安全行为，通过在线互动学习参数化约束，实现样本高效的个性化调整。

Result: 在三种模拟环境、用户研究和真实机器人喂食系统中，CBTL比基线方法表现更优，交互更少。

Conclusion: CBTL提供了一种统一、实用的方法，支持持续、灵活、主动且安全的机器人个性化。

Abstract: Generalist robots must personalize in-the-wild to meet the diverse needs and
preferences of long-term users. How can we enable flexible personalization
without sacrificing safety or competency? This paper proposes Coloring Between
the Lines (CBTL), a method for personalization that exploits the null space of
constraint satisfaction problems (CSPs) used in robot planning. CBTL begins
with a CSP generator that ensures safe and competent behavior, then
incrementally personalizes behavior by learning parameterized constraints from
online interaction. By quantifying uncertainty and leveraging the
compositionality of planning constraints, CBTL achieves sample-efficient
adaptation without environment resets. We evaluate CBTL in (1) three diverse
simulation environments; (2) a web-based user study; and (3) a real-robot
assisted feeding system, finding that CBTL consistently achieves more effective
personalization with fewer interactions than baselines. Our results demonstrate
that CBTL provides a unified and practical approach for continual, flexible,
active, and safe robot personalization. Website:
https://emprise.cs.cornell.edu/cbtl/

</details>


### [442] [FLARE: Robot Learning with Implicit World Modeling](https://arxiv.org/abs/2505.15659)
*Ruijie Zheng,Jing Wang,Scott Reed,Johan Bjorck,Yu Fang,Fengyuan Hu,Joel Jang,Kaushil Kundalia,Zongyu Lin,Loic Magne,Avnish Narayan,You Liang Tan,Guanzhi Wang,Qi Wang,Jiannan Xiang,Yinzhen Xu,Seonghyeon Ye,Jan Kautz,Furong Huang,Yuke Zhu,Linxi Fan*

Key words: FLARE, 机器人策略学习, 潜在世界建模, 扩散变换器, 模仿学习

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: FLARE是一种新型机器人策略学习框架，通过将扩散变换器与未来观测的潜在表示对齐，实现了长期影响推理，性能显著提升。

Motivation: 提出FLARE框架，旨在通过预测潜在世界模型整合到机器人策略学习中，解决长期推理问题。

Method: 通过扩散变换器特征与未来观测潜在嵌入对齐，仅需少量架构修改即可实现。

Result: 在两个多任务模拟模仿学习基准中，性能超越基线达26%，并能通过人类演示提升泛化能力。

Conclusion: FLARE是一种通用且可扩展的方法，成功将隐式世界建模与高频机器人控制结合。

Abstract: We introduce $\textbf{F}$uture $\textbf{LA}$tent $\textbf{RE}$presentation
Alignment ($\textbf{FLARE}$), a novel framework that integrates predictive
latent world modeling into robot policy learning. By aligning features from a
diffusion transformer with latent embeddings of future observations,
$\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent
representations of future observations, allowing it to reason about long-term
consequences while generating actions. Remarkably lightweight, $\textbf{FLARE}$
requires only minimal architectural modifications -- adding a few tokens to
standard vision-language-action (VLA) models -- yet delivers substantial
performance gains. Across two challenging multitask simulation imitation
learning benchmarks spanning single-arm and humanoid tabletop manipulation,
$\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior
policy learning baselines by up to 26%. Moreover, $\textbf{FLARE}$ unlocks the
ability to co-train with human egocentric video demonstrations without action
labels, significantly boosting policy generalization to a novel object with
unseen geometry with as few as a single robot demonstration. Our results
establish $\textbf{FLARE}$ as a general and scalable approach for combining
implicit world modeling with high-frequency robotic control.

</details>


### [443] [HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving](https://arxiv.org/abs/2505.15793)
*Zhiwen Chen,Bo Leng,Zhuoren Li,Hanming Deng,Guizhe Jin,Ran Yu,Huanxi Wen*

Key words: 大语言模型, 强化学习, 自动驾驶, 幻觉问题, HCRMP

<details>
  <summary>Details</summary>

Main category: cs.RO

TL;DR: 本文提出了一种新的LLM-Hinted RL范式，通过结合大语言模型（LLM）和强化学习（RL）来提升自动驾驶性能，同时解决LLM幻觉问题。

Motivation: 当前的LLM主导的RL方法过度依赖容易产生幻觉的LLM输出，可能危及驾驶策略性能。

Method: 提出HCRMP架构，包含增强语义表示模块、上下文稳定性锚模块和语义缓存模块，用于状态增强和策略优化。

Result: HCRMP在CARLA实验中表现出色，任务成功率高达80.3%，安全关键条件下碰撞率降低11.4%。

Conclusion: 保持LLM与RL的相对独立性对解决幻觉问题至关重要，HCRMP显著提升了复杂场景下的驾驶性能。

Abstract: Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations.Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.

</details>
