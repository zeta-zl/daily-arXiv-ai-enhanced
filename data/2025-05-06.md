<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 56]
- [cs.LG](#cs.LG) [Total: 105]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 13]
- [stat.ME](#stat.ME) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CV](#cs.CV) [Total: 29]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.OC](#math.OC) [Total: 10]
- [cs.CE](#cs.CE) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.CR](#cs.CR) [Total: 13]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456)
*Vaidehi Patil,Yi-Lin Sung,Peter Hase,Jie Peng,Tianlong Chen,Mohit Bansal*

Main category: cs.CL

TL;DR: 该论文提出了一个多模态遗忘基准UnLOK-VQA，用于评估从多模态大语言模型中删除特定知识的效果，并展示了多模态攻击的高效性以及规模对模型安全的积极影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型可能无意中学习敏感信息，现有遗忘研究主要针对文本，多模态遗忘尚未充分探索，因此需要建立高质量基准和方法。

Method: 通过自动化管道扩展视觉问答数据集生成多样样本，并手动筛选保证质量，提出攻击-防御框架评估6种防御目标和7种攻击方法。

Result: 多模态攻击效果优于纯文本或图像攻击，最有效防御方法是从内部模型状态中移除答案信息，模型规模越大安全性越高。

Conclusion: UnLOK-VQA为多模态遗忘提供了严格基准，证明了规模对模型安全的积极作用。

Abstract: LLMs trained on massive datasets may inadvertently acquire sensitive
information such as personal details and potentially harmful content. This risk
is further heightened in multimodal LLMs as they integrate information from
multiple modalities (image and text). Adversaries can exploit this knowledge
through multimodal prompts to extract sensitive details. Evaluating how
effectively MLLMs can forget such information (targeted unlearning)
necessitates the creation of high-quality, well-annotated image-text pairs.
While prior work on unlearning has focused on text, multimodal unlearning
remains underexplored. To address this gap, we first introduce a multimodal
unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as
an attack-and-defense framework to evaluate methods for deleting specific
multimodal knowledge from MLLMs. We extend a visual question-answering dataset
using an automated pipeline that generates varying-proximity samples for
testing generalization and specificity, followed by manual filtering for
maintaining high quality. We then evaluate six defense objectives against seven
attacks (four whitebox, three blackbox), including a novel whitebox method
leveraging interpretability of hidden states. Our results show multimodal
attacks outperform text- or image-only ones, and that the most effective
defense removes answer information from internal model states. Additionally,
larger models exhibit greater post-editing robustness, suggesting that scale
enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing
unlearning in MLLMs.

</details>


### [2] [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://arxiv.org/abs/2505.01459)
*Abdoul Majid O. Thiombiano,Brahim Hnich,Ali Ben Mrad,Mohamed Wiem Mkaouer*

Main category: cs.CL

TL;DR: MoxE结合xLSTM与MoE框架，提出一种新型架构，通过熵感知路由和辅助损失提升大型语言模型的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在可扩展性和效率方面的关键挑战。

Method: 结合xLSTM的内存结构和MoE的稀疏性，引入熵感知路由和辅助损失（如熵平衡和分组平衡损失）。

Result: 理论分析和实验验证显示，MoxE在效率和性能上显著优于现有方法。

Conclusion: MoxE为可扩展的大型语言模型架构提供了显著进展，具备高效和鲁棒的特点。

Abstract: This paper introduces MoxE, a novel architecture that synergistically
combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of
Experts (MoE) framework to address critical scalability and efficiency
challenges in large language models (LLMs). The proposed method effectively
leverages xLSTM's innovative memory structures while strategically introducing
sparsity through MoE to substantially reduce computational overhead. At the
heart of our approach is a novel entropy-based routing mechanism, designed to
dynamically route tokens to specialized experts, thereby ensuring efficient and
balanced resource utilization. This entropy awareness enables the architecture
to effectively manage both rare and common tokens, with mLSTM blocks being
favored to handle rare tokens. To further enhance generalization, we introduce
a suite of auxiliary losses, including entropy-based and group-wise balancing
losses, ensuring robust performance and efficient training. Theoretical
analysis and empirical evaluations rigorously demonstrate that MoxE achieves
significant efficiency gains and enhanced effectiveness compared to existing
approaches, marking a notable advancement in scalable LLM architectures.

</details>


### [3] [SymPlanner: Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
*Siheng Xiong,Jieyu Zhou,Zhangding Liu,Yusen Su*

Main category: cs.CL

TL;DR: SymPlanner是一个新框架，通过将语言模型与符号环境结合，提升规划能力，引入迭代校正和对比排序以优化规划结果。


<details>
  <summary>Details</summary>
Motivation: 语言模型在需要多步行动序列规划的领域表现不佳，缺乏对外部约束的适应性。

Method: SymPlanner结合符号环境作为世界模型，通过迭代校正和对比排序优化规划过程。

Result: 在PlanBench上验证，SymPlanner生成的规划比纯自然语言方法更连贯、多样且可验证。

Conclusion: SymPlanner通过符号环境增强了语言模型的规划能力，为复杂任务提供了更可靠的解决方案。

Abstract: Planning remains a core challenge for language models (LMs), particularly in
domains that require coherent multi-step action sequences grounded in external
constraints. We introduce SymPlanner, a novel framework that equips LMs with
structured planning capabilities by interfacing them with a symbolic
environment that serves as an explicit world model. Rather than relying purely
on natural language reasoning, SymPlanner grounds the planning process in a
symbolic state space, where a policy model proposes actions and a symbolic
environment deterministically executes and verifies their effects. To enhance
exploration and improve robustness, we introduce Iterative Correction (IC),
which refines previously proposed actions by leveraging feedback from the
symbolic environment to eliminate invalid decisions and guide the model toward
valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained
comparison of candidate plans by evaluating them jointly. We evaluate
SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,
and verifiable plans than pure natural language baselines.

</details>


### [4] [On the effectiveness of Large Language Models in the mechanical design domain](https://arxiv.org/abs/2505.01559)
*Daniele Grandi,Fabian Riquelme*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型在机械工程领域的表现，通过ABC数据集的无监督任务评估模型性能，在二进制句子对分类任务中达到0.62准确率，零样本分类任务表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在机械工程领域的表现，探索其在该领域的适用性和局限性。

Method: 使用ABC数据集，预处理后设计两个无监督任务：二进制句子对分类和零样本分类，并通过调整学习率、dropout值、序列长度和添加多头注意力层来优化模型。

Result: 二进制句子对分类任务准确率为0.62，零样本分类任务的top-1分类准确率为0.386，表现优于基线。

Conclusion: 研究揭示了大型语言模型在机械工程领域的具体失败模式，为后续研究提供了方向。

Abstract: In this work, we seek to understand the performance of large language models
in the mechanical engineering domain. We leverage the semantic data found in
the ABC dataset, specifically the assembly names that designers assigned to the
overall assemblies, and the individual semantic part names that were assigned
to each part. After pre-processing the data we developed two unsupervised tasks
to evaluate how different model architectures perform on domain-specific data:
a binary sentence-pair classification task and a zero-shot classification task.
We achieved a 0.62 accuracy for the binary sentence-pair classification task
with a fine-tuned model that focuses on fighting over-fitting: 1) modifying
learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a
multi-head attention layer. Our model on the zero-shot classification task
outperforms the baselines by a wide margin, and achieves a top-1 classification
accuracy of 0.386. The results shed some light on the specific failure modes
that arise when learning from language in this domain.

</details>


### [5] [AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains](https://arxiv.org/abs/2505.01560)
*Vicent Briva Iglesias,Gokhan Dogru*

Main category: cs.CL

TL;DR: 论文比较了五种机器翻译范式（传统NMT、通用LLM、增强推理LLM及两种多智能体流程），发现自动评分仍倾向于成熟NMT系统，但人类评估显示增强推理LLM在语义准确性上更优，尽管其计算成本显著更高。


<details>
  <summary>Details</summary>
Motivation: 验证LLM和多智能体编排在机器翻译中的实际优势，对比传统NMT的性能、质量和效率。

Method: 对五种范式（Google Translate、GPT-4o、o1-preview及两种GPT-4o智能体流程）进行自动（COMET/BLEU/chrF2/TER）和人工（专家评分）评估，测试数据涵盖法律合同和新闻文本（英西/英加泰/英土）。

Result: 自动评分中NMT领先7/12项，o1-preview多居第二；人工评估则显示o1-preview在5/6比较中更优，但多智能体流程的token消耗高达NMT的5-15倍。

Conclusion: 需多维成本感知评估，未来方向包括精简协调策略、选择性智能体激活及混合单次LLM与靶向干预的流程。

Abstract: Large language models (LLMs) and multi-agent orchestration are touted as the
next leap in machine translation (MT), but their benefits relative to
conventional neural MT (NMT) remain unclear. This paper offers an empirical
reality check. We benchmark five paradigms, Google Translate (strong NMT
baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),
and two GPT-4o-powered agentic workflows (sequential three-stage and iterative
refinement), on test data drawn from a legal contract and news prose in three
English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is
performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with
expert ratings of adequacy and fluency; efficiency with total input-plus-output
token counts mapped to April 2025 pricing.
  Automatic scores still favour the mature NMT system, which ranks first in
seven of twelve metric-language combinations; o1-preview ties or places second
in most remaining cases, while both multi-agent workflows trail. Human
evaluation reverses part of this narrative: o1-preview produces the most
adequate and fluent output in five of six comparisons, and the iterative agent
edges ahead once, indicating that reasoning layers capture semantic nuance
undervalued by surface metrics. Yet these qualitative gains carry steep costs.
The sequential agent consumes roughly five times, and the iterative agent
fifteen times, the tokens used by NMT or single-pass LLMs.
  We advocate multidimensional, cost-aware evaluation protocols and highlight
research directions that could tip the balance: leaner coordination strategies,
selective agent activation, and hybrid pipelines combining single-pass LLMs
with targeted agent intervention.

</details>


### [6] [PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents](https://arxiv.org/abs/2505.01592)
*Takyoung Kim,Janvijay Singh,Shuhaib Mehri,Emre Can Acikgoz,Sagnik Mukherjee,Nimet Beyza Bozdag,Sumuk Shashidhar,Gokhan Tur,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 论文提出了PIPA，一种基于POMDP的统一评估协议，旨在通过多维度评估任务规划代理的表现，强调用户满意度不仅取决于任务完成结果，还与代理的中间行为有关。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要基于任务完成度评估代理性能，忽视了用户满意度与代理行为的整体交互体验之间的关联。作者认为仅优化任务完成度无法最大化用户满意度。

Method: 提出PIPA协议，将交互式任务规划代理的行为过程建模为部分可观测马尔可夫决策过程（POMDP），并设计原子化评估标准以全面分析代理的决策流程。

Result: 研究发现代理在不同行为阶段表现各异，用户满意度同时受结果和中间行为影响，同时指出多代理系统和用户模拟器的局限性。

Conclusion: PIPA为任务规划代理的评估提供了更全面的框架，强调了行为过程的重要性，并提出了未来研究方向。

Abstract: The growing capabilities of large language models (LLMs) in
instruction-following and context-understanding lead to the era of agents with
numerous applications. Among these, task planning agents have become especially
prominent in realistic scenarios involving complex internal pipelines, such as
context understanding, tool management, and response generation. However,
existing benchmarks predominantly evaluate agent performance based on task
completion as a proxy for overall effectiveness. We hypothesize that merely
improving task completion is misaligned with maximizing user satisfaction, as
users interact with the entire agentic process and not only the end result. To
address this gap, we propose PIPA, a unified evaluation protocol that
conceptualizes the behavioral process of interactive task planning agents
within a partially observable Markov Decision Process (POMDP) paradigm. The
proposed protocol offers a comprehensive assessment of agent performance
through a set of atomic evaluation criteria, allowing researchers and
practitioners to diagnose specific strengths and weaknesses within the agent's
decision-making pipeline. Our analyses show that agents excel in different
behavioral stages, with user satisfaction shaped by both outcomes and
intermediate behaviors. We also highlight future directions, including systems
that leverage multiple agents and the limitations of user simulators in task
planning.

</details>


### [7] [Always Tell Me The Odds: Fine-grained Conditional Probability Estimation](https://arxiv.org/abs/2505.01595)
*Liaoyaqi Wang,Zhengping Jiang,Anqi Liu,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 这篇论文提出了一种先进的细粒度概率估计模型，通过结合人类和合成数据、扩展更大模型以及更好的监督，显著改善了LLM在不确定或部分信息下的概率预测能力，并在多项任务中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在明确任务中表现出色，但在不确定或部分信息条件下的概率预测仍存在偏差和不准确的问题。现有方法对不确定性的可靠估计研究不足，尤其是LLM的概率估计往往粗糙且偏向高频数值。

Method: 通过结合人类和合成数据的创建与评估、扩展到更大模型以及改进监督方式，提出了一组强健且精确的概率估计模型。包括系统性地评估依赖条件概率估计的任务。

Result: 研究表明，所提出的方法在条件概率估计任务中，始终优于现有的微调和基于提示的方法，且优势显著。

Conclusion: 论文通过改进数据、模型扩展和监督方法，有效提升了LLM在不确定性下的概率预测能力，为相关领域提供了更可靠的工具。

Abstract: We present a state-of-the-art model for fine-grained probability estimation
of propositions conditioned on context. Recent advances in large language
models (LLMs) have significantly enhanced their reasoning capabilities,
particularly on well-defined tasks with complete information. However, LLMs
continue to struggle with making accurate and well-calibrated probabilistic
predictions under uncertainty or partial information. While incorporating
uncertainty into model predictions often boosts performance, obtaining reliable
estimates of that uncertainty remains understudied. In particular, LLM
probability estimates tend to be coarse and biased towards more frequent
numbers. Through a combination of human and synthetic data creation and
assessment, scaling to larger models, and better supervision, we propose a set
of strong and precise probability estimation models. We conduct systematic
evaluations across tasks that rely on conditional probability estimation and
show that our approach consistently outperforms existing fine-tuned and
prompting-based methods by a large margin.

</details>


### [8] [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/abs/2505.01658)
*Sihyeong Park,Sungryeol Jeon,Chaelyn Lee,Seokhun Jeon,Byung-Soo Kim,Jemin Lee*

Main category: cs.CL

TL;DR: 本文对25种开源和商业LLM推理引擎进行了全面评估，分析了易用性、部署便捷性、通用支持、扩展性以及对吞吐和延迟敏感计算的适用性，并探讨了设计目标和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各种应用中的广泛使用，推理成本成为重要问题，但由于多样化服务需求，选择合适的优化方法困难，缺乏对推理引擎的系统研究。

Method: 评估25种推理引擎的多维度特性（如易用性、扩展性）和优化技术，并分析开源引擎生态成熟度和商业解决方案的性能成本策略。

Result: 研究为开发者和研究学者提供了实际指导，包括未来支持复杂LLM服务、各种硬件和增强安全性的研究方向。

Conclusion: 通过系统评估和探讨，本文为LLM推理引擎的选择和设计提供了实用指南，并开放公共存储库跟踪该领域快速发展的进展。

Abstract: Large language models (LLMs) are widely applied in chatbots, code generators,
and search engines. Workloads such as chain-of-thought, complex reasoning, and
agent services significantly increase the inference cost by invoking the model
repeatedly. Optimization methods such as parallelism, compression, and caching
have been adopted to reduce costs, but the diverse service requirements make it
hard to select the right method. Recently, specialized LLM inference engines
have emerged as a key component for integrating the optimization methods into
service-oriented infrastructures. However, a systematic study on inference
engines is still lacking. This paper provides a comprehensive evaluation of 25
open-source and commercial inference engines. We examine each inference engine
in terms of ease-of-use, ease-of-deployment, general-purpose support,
scalability, and suitability for throughput- and latency-aware computation.
Furthermore, we explore the design goals of each inference engine by
investigating the optimization techniques it supports. In addition, we assess
the ecosystem maturity of open source inference engines and handle the
performance and cost policy of commercial solutions. We outline future research
directions that include support for complex LLM-based services, support of
various hardware, and enhanced security, offering practical guidance to
researchers and developers in selecting and designing optimized LLM inference
engines. We also provide a public repository to continually track developments
in this fast-evolving field:
https://github.com/sihyeong/Awesome-LLM-Inference-Engine

</details>


### [9] [High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers](https://arxiv.org/abs/2505.01693)
*Brian Wong,Kaito Tanaka*

Main category: cs.CL

TL;DR: DeBERTa-RAD是一个两阶段框架，结合了先进LLM的伪标签和高效的DeBERTa知识蒸馏，用于胸部X光报告的自动化标注，显著提升了标注准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 胸部X光报告的自由文本具有高变异性、复杂性和否定、不确定性，传统NLP方法难以处理，直接使用LLM则面临计算成本和速度的限制。

Method: 通过LLM生成高质量伪标签，再用DeBERTa-Base模型在伪标签数据上进行知识蒸馏训练。

Result: 在MIMIC-500基准测试中，DeBERTa-RAD的Macro F1得分达到0.9120，优于现有方法，尤其擅长处理不确定结果。

Conclusion: 结合LLM能力和高效的学生模型，DeBERTa-RAD为克服医学文本标注瓶颈提供了可行方案。

Abstract: Automated labeling of chest X-ray reports is essential for enabling
downstream tasks such as training image-based diagnostic models, population
health studies, and clinical decision support. However, the high variability,
complexity, and prevalence of negation and uncertainty in these free-text
reports pose significant challenges for traditional Natural Language Processing
methods. While large language models (LLMs) demonstrate strong text
understanding, their direct application for large-scale, efficient labeling is
limited by computational cost and speed. This paper introduces DeBERTa-RAD, a
novel two-stage framework that combines the power of state-of-the-art LLM
pseudo-labeling with efficient DeBERTa-based knowledge distillation for
accurate and fast chest X-ray report labeling. We leverage an advanced LLM to
generate high-quality pseudo-labels, including certainty statuses, for a large
corpus of reports. Subsequently, a DeBERTa-Base model is trained on this
pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated
on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a
state-of-the-art Macro F1 score of 0.9120, significantly outperforming
established rule-based systems, fine-tuned transformer models, and direct LLM
inference, while maintaining a practical inference speed suitable for
high-throughput applications. Our analysis shows particular strength in
handling uncertain findings. This work demonstrates a promising path to
overcome data annotation bottlenecks and achieve high-performance medical text
processing through the strategic combination of LLM capabilities and efficient
student models trained via distillation.

</details>


### [10] [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
*Chuan Sun,Han Yu,Lizhen Cui*

Main category: cs.CL

TL;DR: 提出了一种基于Shapley值的非均匀剪枝方法（SVNP），用于大语言模型（LLM），通过量化各层的贡献并分配剪枝预算，显著提升了剪枝性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法对所有层采用均匀稀疏性，未考虑各层重要性差异，导致性能次优。

Method: 采用Shapley值量化每层对整体性能的贡献，结合滑动窗口近似法降低计算开销，并分配定制化剪枝预算。

Result: 在LLaMA和OPT等模型上，SVNP在70%稀疏度下显著降低困惑度（PPL），如LLaMA-7B/13B分别降低18.01%和19.55%。

Conclusion: 非均匀剪枝方法通过精细化预算分配，显著优化了剪枝模型的性能。

Abstract: Pruning large language models (LLMs) is a promising solution for reducing
model sizes and computational complexity while preserving performance.
Traditional layer-wise pruning methods often adopt a uniform sparsity approach
across all layers, which leads to suboptimal performance due to the varying
significance of individual transformer layers within the model not being
accounted for. To this end, we propose the \underline{S}hapley
\underline{V}alue-based \underline{N}on-\underline{U}niform \underline{P}runing
(\methodname{}) method for LLMs. This approach quantifies the contribution of
each transformer layer to the overall model performance, enabling the
assignment of tailored pruning budgets to different layers to retain critical
parameters. To further improve efficiency, we design the Sliding Window-based
Shapley Value approximation method. It substantially reduces computational
overhead compared to exact SV calculation methods. Extensive experiments on
various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness
of the proposed approach. The results reveal that non-uniform pruning
significantly enhances the performance of pruned models. Notably, \methodname{}
achieves a reduction in perplexity (PPL) of 18.01\% and 19.55\% on LLaMA-7B and
LLaMA-13B, respectively, compared to SparseGPT at 70\% sparsity.

</details>


### [11] [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/abs/2505.01761)
*Tobias Domhan,Dawei Zhu*

Main category: cs.CL

TL;DR: 研究发现，长文本会影响LLM对翻译质量的评估，表现为错误标注减少和系统排名准确性下降。通过优化提示和微调方法可以缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 解决长文本翻译评估中因文本长度导致的评估偏差问题。

Method: 比较了粒度对齐提示、焦点句子提示（FSP）和微调方法的效果。

Result: FSP和微调方法显著减少了长度偏差，提高了长文本翻译评估的可靠性。

Conclusion: 优化提示和微调可以有效提升LLM在长文本翻译评估中的表现。

Abstract: Accurately evaluating machine-translated text remains a long-standing
challenge, particularly for long documents. Recent work has shown that large
language models (LLMs) can serve as reliable and interpretable sentence-level
translation evaluators via MQM error span annotations. With modern LLMs
supporting larger context windows, a natural question arises: can we feed
entire document translations into an LLM for quality assessment? Ideally,
evaluation should be invariant to text length, producing consistent error spans
regardless of input granularity. However, our analysis shows that text length
significantly impacts evaluation: longer texts lead to fewer error spans and
reduced system ranking accuracy. To address this limitation, we evaluate
several strategies, including granularity-aligned prompting, Focus Sentence
Prompting (FSP), and a fine-tuning approach to better align LLMs with the
evaluation task. The latter two methods largely mitigate this length bias,
making LLMs more reliable for long-form translation evaluation.

</details>


### [12] [A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments](https://arxiv.org/abs/2505.01794)
*Jared D. T. Guerrero-Sosa,Francisco P. Romero,Víctor Hugo Menéndez-Domínguez,Jesus Serrano-Guerrero,Andres Montoro-Montarroso,Jose A. Olivas*

Main category: cs.CL

TL;DR: 本文提出了一种模糊逻辑方法，结合多模态分析，用于高等教育中学生软技能的无偏评估。通过计算感知和细粒度分析，该方法提升了评估的可解释性和可靠性。实验结果证明，整合多模态数据显著提高了评估质量。


<details>
  <summary>Details</summary>
Motivation: 在高等教育中，无偏评估软技能是一大挑战，需要解决行为的复杂性和不确定性。

Method: 采用模糊逻辑和语言现象模型，结合多模态分析（如面部表情和手势识别），开发工具评估决策力、沟通能力和创造力等软技能。

Result: 实验表明，多模态数据的整合能有效提高软技能评估的一致性和透明度。

Conclusion: 该框架为教育利益相关者提供了可解释且可靠的软技能评估工具，证明了多模态方法的优越性。

Abstract: In the rapidly evolving educational landscape, the unbiased assessment of
soft skills is a significant challenge, particularly in higher education. This
paper presents a fuzzy logic approach that employs a Granular Linguistic Model
of Phenomena integrated with multimodal analysis to evaluate soft skills in
undergraduate students. By leveraging computational perceptions, this approach
enables a structured breakdown of complex soft skill expressions, capturing
nuanced behaviours with high granularity and addressing their inherent
uncertainties, thereby enhancing interpretability and reliability. Experiments
were conducted with undergraduate students using a developed tool that assesses
soft skills such as decision-making, communication, and creativity. This tool
identifies and quantifies subtle aspects of human interaction, such as facial
expressions and gesture recognition. The findings reveal that the framework
effectively consolidates multiple data inputs to produce meaningful and
consistent assessments of soft skills, showing that integrating multiple
modalities into the evaluation process significantly improves the quality of
soft skills scores, making the assessment work transparent and understandable
to educational stakeholders.

</details>


### [13] [Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis](https://arxiv.org/abs/2505.01800)
*Chidimma Opara*

Main category: cs.CL

TL;DR: 这篇论文提出了一个结合风格计量学与心理语言学理论的框架，用于区分AI生成与人类写作的文本，通过31个风格特征映射到认知过程，以开发可靠的学术诚信工具。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成文本的日益复杂，教育场景中亟需准确透明的检测工具验证作者身份，现有研究表风格计量学结合机器学习分类器效果显著。

Method: 研究构建了一个整合风格计量学与心理语言学理论的框架，具体通过31个风格特征映射到认知过程（如词汇检索、语篇规划等），突出人类写作的独特心理语言学模式。

Result: 该框架通过计算语言学与认知科学的交叉，为开发可靠的文本检测工具提供了理论基础，有助于维护生成式AI时代的学术诚信。

Conclusion: 结合风格计量学与心理语言学的方法为区分AI与人类文本提供了可解释的路径，对学术诚信工具的发展具有重要贡献。

Abstract: The increasing sophistication of AI-generated texts highlights the urgent
need for accurate and transparent detection tools, especially in educational
settings, where verifying authorship is essential. Existing literature has
demonstrated that the application of stylometric features with machine learning
classifiers can yield excellent results. Building on this foundation, this
study proposes a comprehensive framework that integrates stylometric analysis
with psycholinguistic theories, offering a clear and interpretable approach to
distinguishing between AI-generated and human-written texts. This research
specifically maps 31 distinct stylometric features to cognitive processes such
as lexical retrieval, discourse planning, cognitive load management, and
metacognitive self-monitoring. In doing so, it highlights the unique
psycholinguistic patterns found in human writing. Through the intersection of
computational linguistics and cognitive science, this framework contributes to
the development of reliable tools aimed at preserving academic integrity in the
era of generative AI.

</details>


### [14] [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/abs/2505.01812)
*Core Francisco Park,Zechen Zhang,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 论文提出了New News数据集，展示了微调与上下文学习（ICL）之间的差距，并引入System-2 Fine-tuning（Sys2-FT）方法通过自生成数据（如重述、推理和自问自答）提升模型对新信息的权重学习。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决大语言模型（LLMs）在通过微调学习新信息（新闻）时与上下文学习（ICL）相比表现较差的问题。

Method: 研究方法包括构建New News数据集，探索自生成数据协议（重述、推理、自问自答），并提出Sys2-FT方法，同时在不同领域和模型规模上系统性评估ICL和Sys2-FT的性能。

Result: 结果显示自问自答协议的Sys2-FT显著提升了模型对新信息的权重学习，并发现了'上下文遮蔽效应'，即新闻在上下文中训练后再训练其重述或问答会削弱学习效果。

Conclusion: 结论是Sys2-FT能有效缩小微调与ICL之间的差距，初步发现其具有扩展定律的迹象。

Abstract: Humans and intelligent animals can effortlessly internalize new information
("news") and accurately extract the implications for performing downstream
tasks. While large language models (LLMs) can achieve this through in-context
learning (ICL) when the news is explicitly given as context, fine-tuning
remains challenging for the models to consolidate learning in weights. In this
paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet
plausible news spanning multiple domains (mathematics, coding, discoveries,
leaderboards, events), accompanied by downstream evaluation questions whose
correct answers critically depend on understanding and internalizing the news.
We first demonstrate a substantial gap between naive fine-tuning and in-context
learning (FT-ICL gap) on our news dataset. To address this gap, we explore a
suite of self-play data generation protocols -- paraphrases, implications and
Self-QAs -- designed to distill the knowledge from the model with context into
the weights of the model without the context, which we term $\textit{System-2
Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance
across data domains and model scales with the Qwen 2.5 family of models. Our
results demonstrate that the self-QA protocol of Sys2-FT significantly improves
models' in-weight learning of the news. Furthermore, we discover the
$\textit{contexual shadowing effect}$, where training with the news $\textit{in
context}$ followed by its rephrases or QAs degrade learning of the news.
Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.

</details>


### [15] [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
*Anthony Nguyen,Wenjun Lin*

Main category: cs.CL

TL;DR: 本文提出了一种名为层内循环（ILR）的新方法，通过在单个前向传递中选择性地对个别层应用循环，优化了Transformer架构中的循环结构。实验表明，早期层分配更多迭代次数效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在自然语言处理中设立了新标杆，但其层数增加会导致参数量大幅增长。现有的循环Transformer方法通过对整个层块进行循环来处理这一问题，但往往缺乏针对性。本研究旨在探索更精准的层内循环方法，以优化模型效率。

Method: 提出了层内循环（ILR）方法，通过在前向传递过程中针对性地选择个别层进行循环处理，而非对整个层块应用循环。实验中对不同层的迭代次数进行了分配优化。

Result: 实验结果表明，将更多迭代次数分配给早期层能够取得最佳效果。这一发现验证了ILR在优化Transformer架构循环结构方面的有效性。

Conclusion: 层内循环（ILR）为Transformer架构中的循环结构优化提供了一个有前景的方向，通过针对性地处理个别层，能够在保持模型性能的同时减少参数量的增长。

Abstract: Transformer models have established new benchmarks in natural language
processing; however, their increasing depth results in substantial growth in
parameter counts. While existing recurrent transformer methods address this
issue by reprocessing layers multiple times, they often apply recurrence
indiscriminately across entire blocks of layers. In this work, we investigate
Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence
selectively to individual layers within a single forward pass. Our experiments
show that allocating more iterations to earlier layers yields optimal results.
These findings suggest that ILR offers a promising direction for optimizing
recurrent structures in transformer architectures.

</details>


### [16] [Positional Attention for Efficient BERT-Based Named Entity Recognition](https://arxiv.org/abs/2505.01868)
*Mo Sun,Siheng Xiong,Yuankai Cai,Bowen Zuo*

Main category: cs.CL

TL;DR: 提出了一种基于BERT的命名实体识别（NER）框架，通过引入位置注意力机制和预训练参数，降低了训练成本并保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 虽然BERT在实体识别任务中表现优异，但为每个新应用从头微调既耗时又昂贵。本研究旨在提供一种更经济高效的解决方案。

Method: 结合位置注意力机制，利用预训练参数进行有效定制，减少了从头训练的需求。

Result: 在Groningen Meaning Bank数据集上表现优异，训练周期更少。

Conclusion: 该框架为降低BERT-based NER系统的训练成本提供了一种实用方案，同时保持了高准确率。

Abstract: This paper presents a framework for Named Entity Recognition (NER) leveraging
the Bidirectional Encoder Representations from Transformers (BERT) model in
natural language processing (NLP). NER is a fundamental task in NLP with broad
applicability across downstream applications. While BERT has established itself
as a state-of-the-art model for entity recognition, fine-tuning it from scratch
for each new application is computationally expensive and time-consuming. To
address this, we propose a cost-efficient approach that integrates positional
attention mechanisms into the entity recognition process and enables effective
customization using pre-trained parameters. The framework is evaluated on a
Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves
strong performance with fewer training epochs. This work contributes to the
field by offering a practical solution for reducing the training cost of
BERT-based NER systems while maintaining high accuracy.

</details>


### [17] [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/abs/2505.01877)
*Jiří Milička,Anna Marklová,Ondřej Drobil,Eva Pospíšilová*

Main category: cs.CL

TL;DR: 研究发现，提供即时反馈能帮助人们更好地区分人类写作与AI生成的文本，并校准自我认知能力。反馈还纠正了对AI文本风格的误解。


<details>
  <summary>Details</summary>
Motivation: 探讨人类是否能通过学习区分AI与人类写作，并研究其依赖的判断标准（如文本风格和可读性）。

Method: 使用GPT-4o生成文本，与人类文本对比，让255名捷克母语者随机分组（有/无反馈）进行判断，记录准确性、信心等数据。

Result: 即时反馈组在识别准确性和信心校准上显著提升，反馈纠正了最初对AI文本风格的错误假设。

Conclusion: 通过针对性训练和反馈，人们可以有效区分人类与AI文本，并改善自我评估能力，这对教育领域尤其重要。

Abstract: This study investigates whether individuals can learn to accurately
discriminate between human-written and AI-produced texts when provided with
immediate feedback, and if they can use this feedback to recalibrate their
self-perceived competence. We also explore the specific criteria individuals
rely upon when making these decisions, focusing on textual style and perceived
readability.
  We used GPT-4o to generate several hundred texts across various genres and
text types comparable to Koditex, a multi-register corpus of human-written
texts. We then presented randomized text pairs to 255 Czech native speakers who
identified which text was human-written and which was AI-generated.
Participants were randomly assigned to two conditions: one receiving immediate
feedback after each trial, the other receiving no feedback until experiment
completion. We recorded accuracy in identification, confidence levels, response
times, and judgments about text readability along with demographic data and
participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in
accuracy and confidence calibration. Participants initially held incorrect
assumptions about AI-generated text features, including expectations about
stylistic rigidity and readability. Notably, without feedback, participants
made the most errors precisely when feeling most confident -- an issue largely
resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be
effectively learned through targeted training with explicit feedback, which
helps correct misconceptions about AI stylistic features and readability, as
well as potential other variables that were not explored, while facilitating
more accurate self-assessment. This finding might be particularly important in
educational contexts.

</details>


### [18] [Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams](https://arxiv.org/abs/2505.01883)
*Yiwen Lu,Siheng Xiong,Zhaowei Li*

Main category: cs.CL

TL;DR: 提出一个用于大规模推特情感和主题分析的框架，结合多模型标注和LDA主题建模，并开发交互式可视化工具。


<details>
  <summary>Details</summary>
Motivation: 研究推特话语中的情感与上下文特征（如时间、地理位置等）的关系，支持动态地缘政治背景下的社交媒体分析。

Method: 使用冲突相关关键词收集数据，多预训练模型标注情感，LDA分析主题，并开发交互式可视化界面。

Result: 建立了可扩展的分析方法，能够探索情感趋势和主题在时间和地域上的分布。

Conclusion: 该框架为动态地缘政治环境中的社交媒体分析提供了有效的可扩展方法。

Abstract: We present a framework for large-scale sentiment and topic analysis of
Twitter discourse. Our pipeline begins with targeted data collection using
conflict-specific keywords, followed by automated sentiment labeling via
multiple pre-trained models to improve annotation robustness. We examine the
relationship between sentiment and contextual features such as timestamp,
geolocation, and lexical content. To identify latent themes, we apply Latent
Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and
metadata attributes. Finally, we develop an interactive visualization interface
to support exploration of sentiment trends and topic distributions across time
and regions. This work contributes a scalable methodology for social media
analysis in dynamic geopolitical contexts.

</details>


### [19] [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/abs/2505.01900)
*Mazal Bethany,Nishant Vishwamitra,Cho-Yu Jason Chiang,Peyman Najafirad*

Main category: cs.CL

TL;DR: CAMOUFLAGE是一种对抗性攻击方法，通过迭代优化的方式生成语义等效但能误导证据检索系统的改写，成功绕过自动化的虚假信息检测系统。


<details>
  <summary>Details</summary>
Motivation: 研究了证据驱动的虚假信息检测系统在对抗攻击下的脆弱性，发现现有方法无法有效绕过其多组件结构，故提出新方法CAMOUFLAGE。

Method: CAMOUFLAGE使用两代理系统（提示优化代理和攻击代理），通过迭代优化生成语义等效的改写，以误导检索和比较模块。

Result: 在四个系统上测试，平均攻击成功率为46.92%，同时保持文本连贯性和语义等效性。

Conclusion: CAMOUFLAGE证明了证据检测系统的对抗脆弱性，为未来防御方法的设计提供了方向。

Abstract: Automated evidence-based misinformation detection systems, which evaluate the
veracity of short claims against evidence, lack comprehensive analysis of their
adversarial vulnerabilities. Existing black-box text-based adversarial attacks
are ill-suited for evidence-based misinformation detection systems, as these
attacks primarily focus on token-level substitutions involving gradient or
logit-based optimization strategies, which are incapable of fooling the
multi-component nature of these detection systems. These systems incorporate
both retrieval and claim-evidence comparison modules, which requires attacks to
break the retrieval of evidence and/or the comparison module so that it draws
incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach
that employs a two-agent system, a Prompt Optimization Agent and an Attacker
Agent, to create adversarial claim rewritings that manipulate evidence
retrieval and mislead claim-evidence comparison, effectively bypassing the
system without altering the meaning of the claim. The Attacker Agent produces
semantically equivalent rewrites that attempt to mislead detectors, while the
Prompt Optimization Agent analyzes failed attack attempts and refines the
prompt of the Attacker to guide subsequent rewrites. This enables larger
structural and stylistic transformations of the text rather than token-level
substitutions, adapting the magnitude of changes based on previous outcomes.
Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on
binary model decisions to guide its rewriting process, eliminating the need for
classifier logits or extensive querying. We evaluate CAMOUFLAGE on four
systems, including two recent academic systems and two real-world APIs, with an
average attack success rate of 46.92\% while preserving textual coherence and
semantic equivalence to the original claims.

</details>


### [20] [Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview](https://arxiv.org/abs/2505.01967)
*Jiatao Li,Yanheng Li,Xiaojun Wan*

Main category: cs.CL

TL;DR: 论文提出了一个社会世界观分类法（SWT），用于测量LLMs在四种典型世界观中的表现，并发现社会线索可以系统性影响这些态度。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何隐式形成和表达社会认知态度或“世界观”，尤其关注以往研究中未充分探索的权威、平等、自治和命运等维度。

Method: 基于文化理论构建社会世界观分类法（SWT），并在28种LLMs上实证识别其认知特征，实验验证社会线索对认知态度的影响。

Result: 发现了LLMs之间显著且可解释的认知差异，并证实社会反馈会系统性地改变这些态度，揭示了普遍规律和模型特定变体。

Conclusion: 研究提升了LLMs的可解释性，揭示了其隐含的社会认知偏见及对社会反馈的响应，有助于开发更透明、负责任的语言技术。

Abstract: Large Language Models (LLMs) have become integral to daily life, widely
adopted in communication, decision-making, and information retrieval, raising
critical questions about how these systems implicitly form and express
socio-cognitive attitudes or "worldviews". While existing research extensively
addresses demographic and ethical biases, broader dimensions-such as attitudes
toward authority, equality, autonomy, and fate-remain under-explored. In this
paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework
grounded in Cultural Theory, operationalizing four canonical worldviews
(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable
sub-dimensions. Using SWT, we empirically identify distinct and interpretable
cognitive profiles across 28 diverse LLMs. Further, inspired by Social
Referencing Theory, we experimentally demonstrate that explicit social cues
systematically shape these cognitive attitudes, revealing both general response
patterns and nuanced model-specific variations. Our findings enhance the
interpretability of LLMs by revealing implicit socio-cognitive biases and their
responsiveness to social feedback, thus guiding the development of more
transparent and socially responsible language technologies.

</details>


### [21] [LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load](https://arxiv.org/abs/2505.01980)
*Theo Guidroz,Diego Ardila,Jimmy Li,Adam Mansour,Paul Jhun,Nina Gonzalez,Xiang Ji,Mike Sanchez,Sujay Kakarmath,Mathias MJ Bellaiche,Miguel Ángel Garrido,Faruk Ahmed,Divyansh Choudhary,Jay Hartford,Chenwei Xu,Henry Javier Serrano Echeverria,Yifan Wang,Jeff Shaffer,Eric,Cao,Yossi Matias,Avinatan Hassidim,Dale R Webster,Yun Liu,Sho Fujiwara,Peggy Bui,Quang Duong*

Main category: cs.CL

TL;DR: 该研究利用自我精炼方法开发了LLM的文本简化能力，通过随机化研究（4563名参与者）验证了简化文本能显著提高理解准确度（平均提升3.9%）。


<details>
  <summary>Details</summary>
Motivation: 解决网络信息（如科学文献和维基百科）超出用户阅读水平的问题，通过简化文本提升信息可及性。

Method: 采用自我精炼方法开发LLM文本简化技术，通过多领域（如生物医学、法律等）随机对照实验，比较原始与简化文本的理解效果。

Result: 简化文本使MCQ正确率绝对提升3.9%，生物医学领域效果最显著（14.6%）。参与者主观反馈简化文本更易读。

Conclusion: LLM文本简化能有效改善复杂信息的理解，尤其利于非专业人士获取专业知识，提升信息平等性。

Abstract: Information on the web, such as scientific publications and Wikipedia, often
surpasses users' reading level. To help address this, we used a self-refinement
approach to develop a LLM capability for minimally lossy text simplification.
To validate our approach, we conducted a randomized study involving 4563
participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical
scientific articles), biology, law, finance, literature/philosophy, and
aerospace/computer science. Participants were randomized to viewing original or
simplified texts in a subject area, and answered multiple-choice questions
(MCQs) that tested their comprehension of the text. The participants were also
asked to provide qualitative feedback such as task difficulty. Our results
indicate that participants who read the simplified text answered more MCQs
correctly than their counterparts who read the original text (3.9% absolute
increase, p<0.05). This gain was most striking with PubMed (14.6%), while more
moderate gains were observed for finance (5.5%), aerospace/computer science
(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether
participants could refer back to the text while answering MCQs. The absolute
accuracy decreased by up to ~9% for both original and simplified setups where
participants could not refer back to the text, but the ~4% overall improvement
persisted. Finally, participants' self-reported perceived ease based on a
simplified NASA Task Load Index was greater for those who read the simplified
text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,
involving an order of magnitude more participants than prior works,
demonstrates the potential of LLMs to make complex information easier to
understand. Our work aims to enable a broader audience to better learn and make
use of expert knowledge available on the web, improving information
accessibility.

</details>


### [22] [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
*Sai Krishna Mendu,Harish Yenala,Aditi Gulati,Shanu Kumar,Parag Agrawal*

Main category: cs.CL

TL;DR: 本文分析了主流大型语言模型（LLMs）预训练数据集中存在的有害内容问题，提出了一种分类方法及过滤工具，旨在提升LLMs的安全性。


<details>
  <summary>Details</summary>
Motivation: 预训练数据集中常含有仇恨言论、错误信息和偏见等有害内容，直接使用会传播毒性行为，引发伦理问题。研究动机是解决这一问题，确保LLMs更安全可靠。

Method: 通过大规模分析数据集中的不当内容，提出分类法（Topical/Toxic），开发高精度提示数据集（TTP）、基于Transformer的过滤模型（HarmFormer），并创建多毒性基准（HAVOC）。

Result: 提出了全面的有害内容分类法和高效过滤工具，为安全预训练提供了资源，并公开了模型信号以支持合规性。

Conclusion: 研究为LLMs的负责任训练提供了实用解决方案，推动了负责任AI（RAI）的发展。

Abstract: Large language models (LLMs) have become integral to various real-world
applications, leveraging massive, web-sourced datasets like Common Crawl, C4,
and FineWeb for pretraining. While these datasets provide linguistic data
essential for high-quality natural language generation, they often contain
harmful content, such as hate speech, misinformation, and biased narratives.
Training LLMs on such unfiltered data risks perpetuating toxic behaviors,
spreading misinformation, and amplifying societal biases which can undermine
trust in LLM-driven applications and raise ethical concerns about their use.
This paper presents a large-scale analysis of inappropriate content across
these datasets, offering a comprehensive taxonomy that categorizes harmful
webpages into Topical and Toxic based on their intent. We also introduce a
prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and
a transformer-based model (HarmFormer) for content filtering. Additionally, we
create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide
crucial insights into how models respond to adversarial toxic inputs. Upon
publishing, we will also opensource our model signal on the entire C4 dataset.
Our work offers insights into ensuring safer LLM pretraining and serves as a
resource for Responsible AI (RAI) compliance.

</details>


### [23] [An overview of artificial intelligence in computer-assisted language learning](https://arxiv.org/abs/2505.02032)
*Anisia Katinskaia*

Main category: cs.CL

TL;DR: 本文回顾了人工智能在计算机辅助语言学习（CALL）领域的应用，强调了其对语言学习和教学的潜在支持。尽管需求增加，但完整的CALL解决方案仍稀缺，本文旨在为开发者提供AI方法的视角，并促进跨学科工作。


<details>
  <summary>Details</summary>
Motivation: 随着语言学习需求的增长（如疫情、移民等），人工教师的时间成本高昂且难以规模化。CALL系统作为替代方案的需求日益凸显，但现有研究多为原型或部分实现，完整的商业化解决方案稀缺。本文旨在填补AI在CALL领域应用的综述空白。

Method: 从CALL系统开发者的视角，分析AI在语言学习中的方法，并联系不同学科的研究，促进跨学科协作。

Result: 指出AI在CALL领域的多样化应用潜力，但现有成果多为局部实现，完整的商业化系统仍需大量资源支持。

Conclusion: AI在CALL中的应用前景广阔，但需更多跨学科合作和资源投入以实现全面解决方案。

Abstract: Computer-assisted language learning -- CALL -- is an established research
field. We review how artificial intelligence can be applied to support language
learning and teaching. The need for intelligent agents that assist language
learners and teachers is increasing: the human teacher's time is a scarce and
costly resource, which does not scale with growing demand. Further factors
contribute to the need for CALL: pandemics and increasing demand for distance
learning, migration of large populations, the need for sustainable and
affordable support for learning, etc. CALL systems are made up of many
components that perform various functions, and AI is applied to many different
aspects in CALL, corresponding to their own expansive research areas. Most of
what we find in the research literature and in practical use are prototypes or
partial implementations -- systems that perform some aspects of the overall
desired functionality. Complete solutions -- most of them commercial -- are
few, because they require massive resources. Recent advances in AI should
result in improvements in CALL, yet there is a lack of surveys that focus on AI
in the context of this research field. This paper aims to present a perspective
on the AI methods that can be employed for language learning from a position of
a developer of a CALL system. We also aim to connect work from different
disciplines, to build bridges for interdisciplinary work.

</details>


### [24] [What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction](https://arxiv.org/abs/2505.02072)
*Eitan Wagner,Omri Abend*

Main category: cs.CL

TL;DR: 论文讨论了语言建模从有限长度字符串分布到通用预测模型的转变，分析了LLM中分布估计与响应预测的区别及其冲突目标，总结了三种不同的预期输出分布，并指出了NLP工作中常见的误解。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中分布估计与响应预测的区别，澄清不同训练阶段和用例下的输出分布差异，避免对实验结果的误解。

Method: 分析LLM的训练阶段（如预训练、上下文学习、偏好微调）及输出概率的常见用例（如补全概率、显式概率），提出三种预期输出分布。

Result: 揭示了NLP工作中常误认为这些分布应相似，导致实验结果的错误解读。

Conclusion: 为LLM的解释和诱导分布的使用奠定了更严格的形式基础。

Abstract: The notion of language modeling has gradually shifted in recent years from a
distribution over finite-length strings to general-purpose prediction models
for textual inputs and outputs, following appropriate alignment phases. This
paper analyzes the distinction between distribution estimation and response
prediction in the context of LLMs, and their often conflicting goals. We
examine the training phases of LLMs, which include pretraining, in-context
learning, and preference tuning, and also the common use cases for their output
probabilities, which include completion probabilities and explicit
probabilities as output. We argue that the different settings lead to three
distinct intended output distributions. We demonstrate that NLP works often
assume that these distributions should be similar, which leads to
misinterpretations of their experimental findings. Our work sets firmer formal
foundations for the interpretation of LLMs, which will inform ongoing work on
the interpretation and use of LLMs' induced distributions.

</details>


### [25] [LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning](https://arxiv.org/abs/2505.02078)
*Joy Lim Jia Yin,Daniel Zhang-Li,Jifan Yu,Haoxuan Li,Shangqing Tu,Yuanchun Wang,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.CL

TL;DR: 该论文介绍了LecEval，一种基于Mayer多媒体学习认知理论的自动评估指标，用于评估幻灯片学习的多模态知识获取效果。


<details>
  <summary>Details</summary>
Motivation: 传统的手动评估、基于参考的指标和大语言模型评估在可扩展性、上下文捕捉或偏置方面存在局限性，无法有效评估幻灯片教学的质量。

Method: 提出LecEval，基于四个评估维度（内容相关性、表达清晰度、逻辑结构、受众参与度），并在大规模标注数据集上训练模型。

Result: 实验表明，LecEval在准确性和适应性上优于现有指标，填补了自动与人工评估间的差距。

Conclusion: LecEval为幻灯片教学质量的自动化评估提供了有效工具，相关数据和工具已开源。

Abstract: Evaluating the quality of slide-based multimedia instruction is challenging.
Existing methods like manual assessment, reference-based metrics, and large
language model evaluators face limitations in scalability, context capture, or
bias. In this paper, we introduce LecEval, an automated metric grounded in
Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal
knowledge acquisition in slide-based learning. LecEval assesses effectiveness
using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical
Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset
of over 2,000 slides from more than 50 online course videos, annotated with
fine-grained human ratings across these rubrics. A model trained on this
dataset demonstrates superior accuracy and adaptability compared to existing
metrics, bridging the gap between automated and human assessments. We release
our dataset and toolkits at https://github.com/JoylimJY/LecEval.

</details>


### [26] [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/abs/2505.02091)
*Xinyue Peng,Yanming Liu,Yihan Cang,Chaoqun Cao,Ming Chen*

Main category: cs.CL

TL;DR: 论文提出LLM-OptiRA框架，利用大语言模型自动检测和转换非凸资源分配问题，实现全自动求解，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统优化技术难以解决无线通信系统中的非凸资源分配问题，急需一种自动化解决方案以减少对专家知识的依赖。

Method: 提出LLM-OptiRA框架，利用大语言模型自动识别并转换非凸问题为可求解形式，同时集成纠错和可行性验证机制。

Result: 实验显示LLM-OptiRA在GPT-4上实现96%的执行率和80%的成功率，在复杂优化任务中表现优越。

Conclusion: LLM-OptiRA为无线通信系统中的非凸资源分配问题提供了一种高效、自动化的解决方案，显著提升了优化效果。

Abstract: Solving non-convex resource allocation problems poses significant challenges
in wireless communication systems, often beyond the capability of traditional
optimization techniques. To address this issue, we propose LLM-OptiRA, the
first framework that leverages large language models (LLMs) to automatically
detect and transform non-convex components into solvable forms, enabling fully
automated resolution of non-convex resource allocation problems in wireless
communication systems. LLM-OptiRA not only simplifies problem-solving by
reducing reliance on expert knowledge, but also integrates error correction and
feasibility validation mechanisms to ensure robustness. Experimental results
show that LLM-OptiRA achieves an execution rate of 96% and a success rate of
80% on GPT-4, significantly outperforming baseline approaches in complex
optimization tasks across diverse scenarios.

</details>


### [27] [Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study](https://arxiv.org/abs/2505.02142)
*Xiaoyu Tian,Sitong Zhao,Haotian Wang,Shuaiting Chen,Yiping Peng,Yunjie Ji,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: 论文探讨了离线强化学习（Offline RL）方法，尤其是DPO及其变体LD-DPO，在提升大语言模型推理能力上的效果，相比在线RL更简单且经济。实验显示这些方法平均提升3.3%，在Arena-Hard基准上显著提升10.1%，并分析了输出长度对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在线RL方法虽然有效但计算成本高，而离线RL方法简单经济却研究不足。论文旨在填补这一空白，探索离线RL（如DPO和LD-DPO）对LLM推理能力的提升效果。

Method: 采用离线RL方法，包括Direct Preference Optimization (DPO)及其变体LD-DPO，通过多基准测试验证其对LLM推理能力的优化。

Result: 实验表明，离线RL方法显著提升模型性能，平均提升3.3%，在Arena-Hard基准上提升10.1%。同时发现输出长度的增加需与语义丰富度匹配。

Conclusion: 离线RL方法（如DPO）能高效提升LLM推理能力，且输出长度需合理控制。论文为开发经济高效的离线RL方法提供了实证和实践指导。

Abstract: Despite significant advances in long-context reasoning by large language
models (LLMs), primarily through Online Reinforcement Learning (RL) methods,
these approaches incur substantial computational costs and complexity. In
contrast, simpler and more economical Offline RL methods remain underexplored.
To address this gap, we investigate the effectiveness of Offline RL methods,
specifically Direct Preference Optimization (DPO) and its length-desensitized
variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive
experiments across multiple reasoning benchmarks demonstrate that these simpler
Offline RL methods substantially improve model performance, achieving an
average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on
the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity
to output length, emphasizing that increasing reasoning length should align
with semantic richness, as indiscriminate lengthening may adversely affect
model performance. We provide comprehensive descriptions of our data processing
and training methodologies, offering empirical evidence and practical insights
for developing more cost-effective Offline RL approaches.

</details>


### [28] [QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach](https://arxiv.org/abs/2505.02146)
*Shouyang Dong,Yuanbo Wen,Jun Bi,Di Huang,Jiaming Guo,Jianxing Xu,Ruibai Xu,Xinkai Song,Yifan Hao,Xuehai Zhou,Tianshi Chen,Qi Guo,Yunji Chen*

Main category: cs.CL

TL;DR: 该论文提出了一个名为QiMeng-Xpiler的新型转译器，通过结合大型语言模型（LLMs）和符号程序合成，自动将张量程序跨异构深度学习系统（DLS）翻译，显著提高编程效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决异构DLS平台上张量程序的多平台适配问题，减轻手动编程负担，实现“一次编写，随处运行”的目标，但目前技术存在效率低或功能错误的问题。

Method: 提出神经符号合成方法，利用LLM的强大代码生成能力结合符号程序合成，通过预定义元提示和分层自动调优实现高效转译。

Result: 在4种DLS平台上，QiMeng-Xpiler平均翻译准确率达95%，性能最高提升2.0倍，编程效率提升96.0倍。

Conclusion: QiMeng-Xpiler有效解决了跨平台张量程序的翻译问题，显著提升编程效率和性能，为DLS开发提供了实用工具。

Abstract: Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been
widely deployed in industrial data centers, which requires to develop multiple
low-level tensor programs for different platforms. An attractive solution to
relieve the programming burden is to transcompile the legacy code of one
platform to others. However, current transcompilation techniques struggle with
either tremendous manual efforts or functional incorrectness, rendering "Write
Once, Run Anywhere" of tensor programs an open question.
  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically
translating tensor programs across DLS via both large language models (LLMs)
and symbolic program synthesis, i.e., neural-symbolic synthesis. The key
insight is leveraging the powerful code generation ability of LLM to make
costly search-based symbolic synthesis computationally tractable. Concretely,
we propose multiple LLM-assisted compilation passes via pre-defined
meta-prompts for program transformation. During each program transformation,
efficient symbolic program synthesis is employed to repair incorrect code
snippets with a limited scale. To attain high performance, we propose a
hierarchical auto-tuning approach to systematically explore both the parameters
and sequences of transformation passes. Experiments on 4 DLS with distinct
programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,
AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler
correctly translates different tensor programs at the accuracy of 95% on
average, and the performance of translated programs achieves up to 2.0x over
vendor-provided manually-optimized libraries. As a result, the programming
productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor
programs.

</details>


### [29] [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/abs/2505.02156)
*Minzheng Wang,Yongbin Li,Haobo Wang,Xinghua Zhang,Nan Xu,Bingli Wu,Fei Huang,Haiyang Yu,Wenji Mao*

Main category: cs.CL

TL;DR: AML方法（自适应模式学习）通过实时选择四种思考模式（直觉反应→深度思考）来优化社交智能模拟，减少了不必要的大语言链推理。AMPO算法的关键创新包括多粒度思考模式、上下文感知模式切换和深度自适应处理。实验表明其性能提升15.6%，推理链缩短32.8%。


<details>
  <summary>Details</summary>
Motivation: 现有社交智能模拟方法要么缺乏动态调整推理深度的能力，要么在所有场景中强制使用长链推理，导致资源浪费和不适定模拟。AML旨在解决这一问题。

Method: 提出AML框架，其核心是AMPO算法，包含多粒度思考模式设计、上下文感知模式切换和深度自适应处理三个创新点。

Result: 实验显示AML比现有方法性能提升15.6%，推理链缩短32.8%。

Conclusion: AML通过上下文敏感的思考模式选择，实现了更接近人类的自适应推理，优于GRPO的固定深度方法。

Abstract: Effective social intelligence simulation requires language agents to
dynamically adjust reasoning depth, a capability notably absent in current
approaches. While existing methods either lack this kind of reasoning
capability or enforce uniform long chain-of-thought reasoning across all
scenarios, resulting in excessive token usage and inappropriate social
simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode
$\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four
thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on
real-time context. Our framework's core innovation, the $\textbf{A}$daptive
$\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$)
algorithm, introduces three key advancements over existing methods: (1)
Multi-granular thinking mode design, (2) Context-aware mode switching across
social interaction, and (3) Token-efficient reasoning via depth-adaptive
processing. Extensive experiments on social intelligence tasks confirm that AML
achieves 15.6% higher task performance than state-of-the-art methods. Notably,
our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These
results demonstrate that context-sensitive thinking mode selection, as
implemented in AMPO, enables more human-like adaptive reasoning than GRPO's
fixed-depth approach

</details>


### [30] [Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use](https://arxiv.org/abs/2505.02164)
*Justin Ho,Alexandra Colby,William Fisher*

Main category: cs.CL

TL;DR: 本文提出了一种针对美国版权法合理使用原则的领域特异性检索增强生成（RAG）实现，旨在解决DMCA下架通知增多和内容创作者缺乏法律援助的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是DMCA下架通知频发，而内容创作者缺乏便捷的法律支持。

Method: 方法结合了语义搜索、法律知识图谱和法院引证网络，采用链式思维推理和交错检索步骤以模拟法律推理。

Result: 初步测试表明，该方法提高了检索过程中的教义相关性。

Conclusion: 结论是为基于LLM的法律辅助工具的未来评估和部署奠定了基础。

Abstract: This paper presents a domain-specific implementation of Retrieval-Augmented
Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.
Motivated by the increasing prevalence of DMCA takedowns and the lack of
accessible legal support for content creators, we propose a structured approach
that combines semantic search with legal knowledge graphs and court citation
networks to improve retrieval quality and reasoning reliability. Our prototype
models legal precedents at the statutory factor level (e.g., purpose, nature,
amount, market effect) and incorporates citation-weighted graph representations
to prioritize doctrinally authoritative sources. We use Chain-of-Thought
reasoning and interleaved retrieval steps to better emulate legal reasoning.
Preliminary testing suggests this method improves doctrinal relevance in the
retrieval process, laying groundwork for future evaluation and deployment of
LLM-based legal assistance tools.

</details>


### [31] [A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking](https://arxiv.org/abs/2505.02171)
*Henrik Brådland,Morten Goodwin,Per-Arne Andersen,Alexander S. Nossum,Aditya Gupta*

Main category: cs.CL

TL;DR: 论文摘要


<details>
  <summary>Details</summary>
Motivation: 研究发现文档分块对检索增强生成（RAG）系统性能有重要影响，但缺乏统一的分析框架，因此需要一种新方法来评估不同分块策略的效果。

Method: 提出HOPE（全段落评估）方法，从段落内特性、段落间特性和段落-文档一致性三个层次量化分块特性，并进行自动评估。

Result: 在七个领域的实证评估中，HOPE指标与RAG性能显著相关（p>0.13）。段落间语义独立性对性能提升显著（事实正确性提升56.2%，答案正确性提升21.1%），而传统认为的段落内概念统一性影响较小。

Conclusion: HOPE方法为优化分块策略提供了可操作的见解，有助于设计更准确的RAG系统。

Abstract: Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)
by determining how source materials are segmented before indexing. Despite
evidence that Large Language Models (LLMs) are sensitive to the layout and
structure of retrieved data, there is currently no framework to analyze the
impact of different chunking methods. In this paper, we introduce a novel
methodology that defines essential characteristics of the chunking process at
three levels: intrinsic passage properties, extrinsic passage properties, and
passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a
domain-agnostic, automatic evaluation metric that quantifies and aggregates
these characteristics. Our empirical evaluations across seven domains
demonstrate that the HOPE metric correlates significantly (p > 0.13) with
various RAG performance indicators, revealing contrasts between the importance
of extrinsic and intrinsic properties of passages. Semantic independence
between passages proves essential for system performance with a performance
gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On
the contrary, traditional assumptions about maintaining concept unity within
passages show minimal impact. These findings provide actionable insights for
optimizing chunking strategies, thus improving RAG system design to produce
more factually correct responses.

</details>


### [32] [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
*Chuck Arvin*

Main category: cs.CL

TL;DR: 该研究评估不同规模的大语言模型（LLMs）在法律基准数据集CaseHOLD上的表现，发现模型性能随规模提升，且无需微调即可达到竞争性结果，同时验证了性能非因记忆训练数据。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力提升，需评估其在法律任务中的表现，尤其是在无需复杂技术调整的情况下是否能达到实用水平。

Method: 通过实验测试不同参数规模的LLMs（3B至90B+）在CaseHOLD数据集上的表现，并设计引用匿名化测试以排除记忆效应。

Result: 较大模型（如GPT4o、AmazonNovaPro）表现最佳（宏F1分别达0.744和0.720），匿名化测试下性能稳定（0.728），表明结果非因记忆。

Conclusion: LLMs在法律任务中展现潜力，但需进一步开发与评估方法以推动自动化法律分析的进步。

Abstract: As large language models (LLMs) continue to advance in capabilities, it is
essential to assess how they perform on established benchmarks. In this study,
we present a suite of experiments to assess the performance of modern LLMs
(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for
identifying case holdings. Our experiments demonstrate ``scaling effects'' -
performance on this task improves with model size, with more capable models
like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720
respectively. These scores are competitive with the best published results on
this dataset, and do not require any technically sophisticated model training,
fine-tuning or few-shot prompting. To ensure that these strong results are not
due to memorization of judicial opinions contained in the training data, we
develop and utilize a novel citation anonymization test that preserves semantic
meaning while ensuring case names and citations are fictitious. Models maintain
strong performance under these conditions (macro F1 of 0.728), suggesting the
performance is not due to rote memorization. These findings demonstrate both
the promise and current limitations of LLMs for legal tasks with important
implications for the development and measurement of automated legal analytics
and legal benchmarks.

</details>


### [33] [Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/abs/2505.02177)
*Chuxue Cao,Zhenghao Zhu,Junqi Zhu,Guoying Lu,Siyu Peng,Juntao Dai,Weijie Shi,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: HKMMLU 是一个针对香港独特语言环境（繁体中文与粤语）的多任务语言理解基准，包含 26,698 道多选题和 90,550 项翻译任务。实验表明，当前最优模型 DeepSeek-V3 的准确率仅 75%，远低于 MMLU 和 CMMLU，凸显了 LLMs 在香港语言和文化知识领域的不足。


<details>
  <summary>Details</summary>
Motivation: 解决香港独特语言（繁体中文与粤语）和文化背景下 LLMs 评估基准不足的问题，推动多语言和跨文化 LLMs 的发展。

Method: 构建 HKMMLU 基准，包含多选问题和翻译任务，覆盖 66 个学科和四种类别。测试了 GPT-4o、Claude 3.7 Sonnet 和 18 个开源 LLMs。

Result: DeepSeek-V3 表现最佳，但准确率仅 75%，显著低于其他基准。问题语言、模型规模、提示策略和 token 长度等因素影响性能。

Conclusion: HKMMLU 揭示了 LLMs 在香港语言和文化领域的瓶颈，未来需提升其多语言和跨文化能力，以支持更广泛的应用。

Abstract: Multilingual understanding is crucial for the cross-cultural applicability of
Large Language Models (LLMs). However, evaluation benchmarks designed for Hong
Kong's unique linguistic landscape, which combines Traditional Chinese script
with Cantonese as the spoken form and its cultural context, remain
underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language
understanding benchmark that evaluates Hong Kong's linguistic competence and
socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions
across 66 subjects, organized into four categories: Science, Technology,
Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To
evaluate the multilingual understanding ability of LLMs, 90,550
Mandarin-Cantonese translation tasks were additionally included. We conduct
comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs
of varying sizes on HKMMLU. The results show that the best-performing model,
DeepSeek-V3, struggles to achieve an accuracy of 75\%, significantly lower than
that of MMLU and CMMLU. This performance gap highlights the need to improve
LLMs' capabilities in Hong Kong-specific language and knowledge domains.
Furthermore, we investigate how question language, model size, prompting
strategies, and question and reasoning token lengths affect model performance.
We anticipate that HKMMLU will significantly advance the development of LLMs in
multilingual and cross-cultural contexts, thereby enabling broader and more
impactful applications.

</details>


### [34] [SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation](https://arxiv.org/abs/2505.02235)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: SEval-Ex是一个文本摘要评估框架，通过分解为原子语句实现高性能和可解释性，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本摘要评估方法在性能和可解释性之间的权衡问题。

Method: 采用两阶段流程：先用LLM从文本和摘要中提取原子语句，再匹配生成的语句。

Result: 在SummEval基准测试中达到0.580的相关性，优于GPT-4评估器（0.521），且具备抗幻觉鲁棒性。

Conclusion: SEval-Ex在性能和可解释性上均表现优异，是文本摘要评估的有效工具。

Abstract: Evaluating text summarization quality remains a critical challenge in Natural
Language Processing. Current approaches face a trade-off between performance
and interpretability. We present SEval-Ex, a framework that bridges this gap by
decomposing summarization evaluation into atomic statements, enabling both high
performance and explainability. SEval-Ex employs a two-stage pipeline: first
extracting atomic statements from text source and summary using LLM, then a
matching between generated statements. Unlike existing approaches that provide
only summary-level scores, our method generates detailed evidence for its
decisions through statement-level alignments. Experiments on the SummEval
benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with
0.580 correlation on consistency with human consistency judgments, surpassing
GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our
framework shows robustness against hallucination.

</details>


### [35] [Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models](https://arxiv.org/abs/2505.02252)
*Paloma Piot,Patricia Martín-Rodilla,Javier Parapar*

Main category: cs.CL

TL;DR: 论文研究了商业大语言模型(LLMs)在个人化语境下对仇恨言论检测的影响，发现语境个人化显著影响其行为，并通过惩罚性微调减少了偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨个人化信息（如用户特征或国家特定背景）对LLMs行为的影响，尤其是在敏感话题（如仇恨言论）上的表现。

Method: 通过让不同LLMs模拟国家特定角色并使用多语言检测仇恨言论，分析其行为差异，随后通过惩罚不一致分类的微调优化模型。

Result: 发现语境个人化显著影响LLMs的仇恨言论检测结果，微调后模型在有无个人化语境下均表现更优。

Conclusion: 个人化语境可能引入偏见，但通过针对性微调可优化LLMs在敏感领域的公平性和一致性。

Abstract: Commercial Large Language Models (LLMs) have recently incorporated memory
features to deliver personalised responses. This memory retains details such as
user demographics and individual characteristics, allowing LLMs to adjust their
behaviour based on personal information. However, the impact of integrating
personalised information into the context has not been thoroughly assessed,
leading to questions about its influence on LLM behaviour. Personalisation can
be challenging, particularly with sensitive topics. In this paper, we examine
various state-of-the-art LLMs to understand their behaviour in different
personalisation scenarios, specifically focusing on hate speech. We prompt the
models to assume country-specific personas and use different languages for hate
speech detection. Our findings reveal that context personalisation
significantly influences LLMs' responses in this sensitive area. To mitigate
these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate
speech classifications made with and without country or language-specific
context. The refined models demonstrate improved performance in both
personalised contexts and when no context is provided.

</details>


### [36] [Parameter-Efficient Transformer Embeddings](https://arxiv.org/abs/2505.02266)
*Henry Ndubuaku,Mouad Talhi*

Main category: cs.CL

TL;DR: 提出一种替代传统词嵌入的方法，通过傅里叶展开和轻量MLP生成词向量，减少了参数量同时保持性能，实验证明该方法高效且无需dropout。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入参数量大且性能提升不显著，希望通过确定性的方法减少参数并保持效率。

Method: 使用傅里叶展开归一化后的token ID生成向量，再通过轻量MLP捕捉高阶交互。

Result: 在NLI任务上性能接近传统方法，参数量更少、训练更快且无需dropout。

Conclusion: 该方法展示了参数高效语言模型的潜力，值得进一步大规模实验验证。

Abstract: Embedding layers in transformer-based NLP models typically account for the
largest share of model parameters, scaling with vocabulary size but not
yielding performance gains proportional to scale. We propose an alternative
approach in which token embedding vectors are first generated
deterministically, directly from the token IDs using a Fourier expansion of
their normalized values, followed by a lightweight multilayer perceptron (MLP)
that captures higher-order interactions. We train standard transformers and our
architecture on natural language inference tasks (SNLI and MNLI), and evaluate
zero-shot performance on sentence textual similarity (STS-B). Our results
demonstrate that the proposed method achieves competitive performance using
significantly fewer parameters, trains faster, and operates effectively without
the need for dropout. This proof-of-concept study highlights the potential for
scalable, memory-efficient language models and motivates further large-scale
experimentation based on our findings.

</details>


### [37] [Demystifying optimized prompts in language models](https://arxiv.org/abs/2505.02273)
*Rimon Melamed,Lucas H. McCabe,H. Howie Huang*

Main category: cs.CL

TL;DR: 该论文研究了优化提示如何影响语言模型的输出及其内部处理机制，发现这些提示主要由罕见标点和名词组成，并在模型激活中表现出独特模式。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型对分布外输入不稳健，优化提示可诱导特定行为但难以解释。研究旨在揭示优化提示的组成及其在模型内部的解析机制。

Method: 通过分析优化提示的组成（如标点和罕见名词）及模型激活模式，探究不同指令调整模型家族中提示的表征路径。

Result: 优化提示在训练数据中更罕见，其激活模式与自然语言明显不同，且在不同模型中表征形成路径相似。

Conclusion: 优化提示通过独特的组成和激活模式影响模型行为，揭示了其内部工作机制。

Abstract: Modern language models (LMs) are not robust to out-of-distribution inputs.
Machine generated (``optimized'') prompts can be used to modulate LM outputs
and induce specific behaviors while appearing completely uninterpretable. In
this work, we investigate the composition of optimized prompts, as well as the
mechanisms by which LMs parse and build predictions from optimized prompts. We
find that optimized prompts primarily consist of punctuation and noun tokens
which are more rare in the training data. Internally, optimized prompts are
clearly distinguishable from natural language counterparts based on sparse
subsets of the model's activations. Across various families of
instruction-tuned models, optimized prompts follow a similar path in how their
representations form through the network.

</details>


### [38] [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/abs/2505.02304)
*Siyu Liang,Yunan Li,Wentian Xin,Huizhou Chen,Xujie Liu,Kang Liu,Qiguang Miao*

Main category: cs.CL

TL;DR: 本文提出了一种结合生成式大语言模型（LLM）的新型手语识别方法GSP-MC，通过检索增强生成和多步提示工程，结合专家验证的手语语料库，实现了对手语的多层次精确描述。实验在中文和土耳其手语数据集上达到了97%以上的准确率。


<details>
  <summary>Details</summary>
Motivation: 手语识别面临标注复杂性高的挑战，本文首次将生成式LLM引入该领域，旨在提升识别精度并推动包容性通信技术的发展。

Method: 提出GSP-MC方法，结合检索增强生成（RAG）、多步提示工程和双编码器架构，通过概率匹配对齐文本描述与层次化骨架特征。

Result: 在中文SLR500和土耳其AUTSL数据集上分别达到97.1%和97.07%的准确率，表现优于现有方法。

Conclusion: GSP-MC方法在跨语言手语识别中表现出色，为开发包容性通信技术提供了新思路。

Abstract: Sign language recognition (SLR) faces fundamental challenges in creating
accurate annotations due to the inherent complexity of simultaneous manual and
non-manual signals. To the best of our knowledge, this is the first work to
integrate generative large language models (LLMs) into SLR tasks. We propose a
novel Generative Sign-description Prompts Multi-positive Contrastive learning
(GSP-MC) method that leverages retrieval-augmented generation (RAG) with
domain-specific LLMs, incorporating multi-step prompt engineering and
expert-validated sign language corpora to produce precise multipart
descriptions. The GSP-MC method also employs a dual-encoder architecture to
bidirectionally align hierarchical skeleton features with multiple text
descriptions (global, synonym, and part level) through probabilistic matching.
Our approach combines global and part-level losses, optimizing KL divergence to
ensure robust alignment across all relevant text-skeleton pairs while capturing
both sign-level semantics and detailed part dynamics. Experiments demonstrate
state-of-the-art performance against existing methods on the Chinese SLR500
(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's
cross-lingual effectiveness highlight its potential for developing inclusive
communication technologies.

</details>


### [39] [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/abs/2505.02311)
*Jihao Zhao,Chunlai Zhou,Biao Qin*

Main category: cs.CL

TL;DR: 提出AttenHScore指标，动态检测小语言模型生成过程中的幻觉，实时调用大模型，提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型（LMs）生成幻觉时调用的时机问题，避免后处理的高计算成本和低效。

Method: 提出AttenHScore指标，动态调整检测阈值；结合不确定性感知的知识重组辅助小模型。

Result: AttenHScore在多问答数据集上表现优秀，尤其在复杂查询中，无需额外训练且适配多种LMs。

Conclusion: AttenHScore有效提升了实时幻觉检测能力，兼具灵活性和实用性。

Abstract: The collaborative paradigm of large and small language models (LMs)
effectively balances performance and cost, yet its pivotal challenge lies in
precisely pinpointing the moment of invocation when hallucinations arise in
small LMs. Previous optimization efforts primarily focused on post-processing
techniques, which were separate from the reasoning process of LMs, resulting in
high computational costs and limited effectiveness. In this paper, we propose a
practical invocation evaluation metric called AttenHScore, which calculates the
accumulation and propagation of hallucinations during the generation process of
small LMs, continuously amplifying potential reasoning errors. By dynamically
adjusting the detection threshold, we achieve more accurate real-time
invocation of large LMs. Additionally, considering the limited reasoning
capacity of small LMs, we leverage uncertainty-aware knowledge reorganization
to assist them better capture critical information from different text chunks.
Extensive experiments reveal that our AttenHScore outperforms most baseline in
enhancing real-time hallucination detection capabilities across multiple QA
datasets, especially when addressing complex queries. Moreover, our strategies
eliminate the need for additional model training and display flexibility in
adapting to various transformer-based LMs.

</details>


### [40] [SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning](https://arxiv.org/abs/2505.02363)
*Tianjian Li,Daniel Khashabi*

Main category: cs.CL

TL;DR: 论文探讨了在偏好优化中结合on-policy和off-policy数据的互补优势，提出SIMPLEMIX方法，通过简单混合两种数据源提升模型对齐效果，相比现有方法平均提升3-6%。


<details>
  <summary>Details</summary>
Motivation: 现有研究对on-policy和off-policy数据在偏好学习中的效果存在分歧，需系统性探究二者的相互作用及任务依赖性。

Method: 提出SIMPLEMIX方法，直接混合on-policy（擅长推理任务）和off-policy（擅长开放任务）数据，优化语言模型对齐。

Result: 在多样化任务中，SIMPLEMIX比纯on/off-policy DPO平均提升6.03%，优于复杂混合方法（如HyPO）3.05%。

Conclusion: 简单的数据混合策略能有效结合两类数据优势，显著提升模型对齐性能，为偏好学习提供了高效解决方案。

Abstract: Aligning language models with human preferences relies on pairwise preference
datasets. While some studies suggest that on-policy data consistently
outperforms off -policy data for preference learning, others indicate that the
advantages of on-policy data may be task-dependent, highlighting the need for a
systematic exploration of their interplay.
  In this work, we show that on-policy and off-policy data offer complementary
strengths in preference optimization: on-policy data is particularly effective
for reasoning tasks like math and coding, while off-policy data performs better
on open-ended tasks such as creative writing and making personal
recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach
to combine the complementary strengths of on-policy and off-policy preference
learning by simply mixing these two data sources. Our empirical results across
diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves
language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO
and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it
outperforms prior approaches that are much more complex in combining on- and
off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.

</details>


### [41] [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2505.02366)
*Tianyu Zong,Hongzhu Yi,Bingkang Shi,Yuanxiang Wang,Jungang Xu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为JTCSE的无监督对比学习框架，通过引入模约束和交叉注意力机制，优化语义表示张量和CLS令牌的注意力分布，在多项语义文本相似性任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法通常只关注高维语义空间中表示的定向分布，而忽略了模特征，导致对比学习不充分。同时，BERT类模型存在注意力下沉现象，影响CLS令牌的语义聚合能力。

Method: 提出JTCSE框架，包含两部分：1）对语义表示张量施加模约束以加强正样本对齐；2）通过双塔集成模型的交叉注意力结构增强CLS令牌的注意力，优化CLS池化质量。

Result: 在7项语义文本相似性任务中，JTCSE的双塔集成模型和单塔蒸馏模型均超越基线方法，达到当前SOTA水平。此外，在130多项零样本下游任务中也整体优于其他基线。

Conclusion: JTCSE通过联合模约束和交叉注意力机制，有效提升了无监督对比学习的效果，为句子嵌入表示提供了更优的框架。

Abstract: Unsupervised contrastive learning has become a hot research topic in natural
language processing. Existing works usually aim at constraining the orientation
distribution of the representations of positive and negative samples in the
high-dimensional semantic space in contrastive learning, but the semantic
representation tensor possesses both modulus and orientation features, and the
existing works ignore the modulus feature of the representations and cause
insufficient contrastive learning. % Therefore, we firstly propose a training
objective that aims at modulus constraints on the semantic representation
tensor, to strengthen the alignment between the positive samples in contrastive
learning. Therefore, we first propose a training objective that is designed to
impose modulus constraints on the semantic representation tensor, to strengthen
the alignment between positive samples in contrastive learning. Then, the
BERT-like model suffers from the phenomenon of sinking attention, leading to a
lack of attention to CLS tokens that aggregate semantic information. In
response, we propose a cross-attention structure among the twin-tower ensemble
models to enhance the model's attention to CLS token and optimize the quality
of CLS Pooling. Combining the above two motivations, we propose a new
\textbf{J}oint \textbf{T}ensor representation modulus constraint and
\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence
\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven
semantic text similarity computation tasks, and the experimental results show
that JTCSE's twin-tower ensemble model and single-tower distillation model
outperform the other baselines and become the current SOTA. In addition, we
have conducted an extensive zero-shot downstream task evaluation, which shows
that JTCSE outperforms other baselines overall on more than 130 tasks.

</details>


### [42] [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
*Xiusi Chen,Gaotang Li,Ziqi Wang,Bowen Jin,Cheng Qian,Yu Wang,Hongru Wang,Yu Zhang,Denghui Zhang,Tong Zhang,Hanghang Tong,Heng Ji*

Main category: cs.CL

TL;DR: 该论文介绍了ReasRM，一种新型的生成式奖励模型，通过将奖励建模定义为推理任务，结合链式思维（CoT）提升模型的解释性和性能。RM-R1通过两阶段训练（蒸馏高质量推理链和强化学习验证奖励）在多个基准测试中表现优异，甚至超过更大的开源和专有模型。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RM）在大型语言模型（RLHF）对齐人类偏好中至关重要。现有RM缺乏解释性，无法整合自然语言反馈。作者认为，将推理能力融入奖励建模可显著提升RM的解释性和性能。

Method: 提出ReasRM，将奖励建模作为推理任务，采用两阶段训练：1）蒸馏高质量推理链；2）利用可验证奖励进行强化学习。RM-R1通过自生成推理痕迹或评分标准评估候选响应。

Result: ReasRM在多个基准测试中达到或接近SOTA性能，表现优于更大的开源（如Llama3.1-405B）和专有模型（如GPT-4o），最高提升13.8%。

Conclusion: ReasRM通过融入推理能力显著提升奖励模型的解释性和性能，且训练方法可复现。作者开源了模型、代码和数据以推动未来研究。

Abstract: Reward modeling is essential for aligning large language models (LLMs) with
human preferences, especially through reinforcement learning from human
feedback (RLHF). To provide accurate reward signals, a reward model (RM) should
stimulate deep thinking and conduct interpretable reasoning before assigning a
score or a judgment. However, existing RMs either produce opaque scalar scores
or directly generate the prediction of a preferred answer, making them struggle
to integrate natural language critiques, thus lacking interpretability.
Inspired by recent advances of long chain-of-thought (CoT) on
reasoning-intensive tasks, we hypothesize and validate that integrating
reasoning capabilities into reward modeling significantly enhances RM's
interpretability and performance. In this work, we introduce a new class of
generative reward models -- Reasoning Reward Models (ReasRMs) -- which
formulate reward modeling as a reasoning task. We propose a reasoning-oriented
training pipeline and train a family of ReasRMs, RM-R1. The training consists
of two key stages: (1) distillation of high-quality reasoning chains and (2)
reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by
self-generating reasoning traces or chat-specific rubrics and evaluating
candidate responses against them. Empirically, our models achieve
state-of-the-art or near state-of-the-art performance of generative RMs across
multiple comprehensive reward model benchmarks, outperforming much larger
open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by
up to 13.8%. Beyond final performance, we perform thorough empirical analysis
to understand the key ingredients of successful ReasRM training. To facilitate
future research, we release six ReasRM models along with code and data at
https://github.com/RM-R1-UIUC/RM-R1.

</details>


### [43] [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410)
*Krzysztof Ociepa,Łukasz Flis,Krzysztof Wróbel,Adrian Gwoździej,Remigiusz Kinas*

Main category: cs.CL

TL;DR: Bielik 11B v2是基于Mistral 7B v0.2架构优化的波兰语处理模型，通过深度扩展达到11B参数，在波兰语基准测试中表现优异，同时保持跨语言能力。


<details>
  <summary>Details</summary>
Motivation: 旨在提升波兰语AI处理能力，为资源效率高的语言建模在较少被代表的语言中设立新标杆。

Method: 采用深度扩展和两项关键技术：加权指令交叉熵损失（优化多样化指令学习）和自适应学习率（根据上下文长度动态调整）。

Result: Bielik 11B v2在多种任务中优于参数更大的模型，显著超越其他波兰语模型，参数高效且支持广泛量化部署。

Conclusion: 该模型不仅推动了波兰语AI能力的发展，还为资源受限的语言建模提供了新方向。

Abstract: We present Bielik 11B v2, a state-of-the-art language model optimized for
Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to
11B parameters using depth up-scaling, this model demonstrates exceptional
performance across Polish language benchmarks while maintaining strong
cross-lingual capabilities. We introduce two key technical innovations:
Weighted Instruction Cross-Entropy Loss, which optimizes learning across
diverse instruction types by assigning quality-based weights to training
examples, and Adaptive Learning Rate, which dynamically adjusts based on
context length. Comprehensive evaluation across multiple benchmarks
demonstrates that Bielik 11B v2 outperforms many larger models, including those
with 2-6 times more parameters, and significantly surpasses other specialized
Polish language models on tasks ranging from linguistic understanding to
complex reasoning. The model's parameter efficiency and extensive quantization
options enable deployment across various hardware configurations, advancing
Polish language AI capabilities and establishing new benchmarks for
resource-efficient language modeling in less-represented languages.

</details>


### [44] [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/abs/2505.02456)
*Elisa Forcada Rodríguez,Olatz Perez-de-Viñaspre,Jon Ander Campos,Dietrich Klakow,Vagrant Gautam*

Main category: cs.CL

TL;DR: 该研究探讨了多语言中交织的国家和性别偏见，重点关注大型语言模型的职业推荐。通过构建包含英语、西班牙语和德语的基准测试，使用25个国家和4种代词组合，评估了5个基于Llama的模型，发现即使模型在单一性别或国家标准上表现公平，交织的职业偏见仍存在。


<details>
  <summary>Details</summary>
Motivation: 现有NLP公平性研究多集中于单一偏见（如性别）和英语语种，忽略了多语言和多维度偏见的复杂性。本研究旨在填补这一空白，探索语言模型在多语言环境下的交织国家和性别偏见。

Method: 构建包含英语、西班牙语和德语的多语言提示基准，系统变换国家和性别变量（25个国家×4种代词组合），评估5个Llama模型在职业推荐中的偏见表现。

Result: 模型普遍存在显著的国家和性别偏见，即使单一维度（性别或国家）表现公平，交织偏见依然突出。提示语言对偏见有显著影响，指令微调模型偏置水平最低且最稳定。

Conclusion: 研究强调公平性研究需采用多语言和交织性视角，指令微调可能是减少偏见的有效途径。

Abstract: One of the goals of fairness research in NLP is to measure and mitigate
stereotypical biases that are propagated by NLP systems. However, such work
tends to focus on single axes of bias (most often gender) and the English
language. Addressing these limitations, we contribute the first study of
multilingual intersecting country and gender biases, with a focus on occupation
recommendations generated by large language models. We construct a benchmark of
prompts in English, Spanish and German, where we systematically vary country
and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite
of 5 Llama-based models on this benchmark, finding that LLMs encode significant
gender and country biases. Notably, we find that even when models show parity
for gender or country individually, intersectional occupational biases based on
both country and gender persist. We also show that the prompting language
significantly affects bias, and instruction-tuned models consistently
demonstrate the lowest and most stable levels of bias. Our findings highlight
the need for fairness researchers to use intersectional and multilingual lenses
in their work.

</details>


### [45] [Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda](https://arxiv.org/abs/2505.02463)
*Richard Kimera,Dongnyeong Heo,Daniela N. Rim,Heeyoul Choi*

Main category: cs.CL

TL;DR: 该论文探讨了半监督反向翻译（BT）技术用于提升英语-Luganda低资源语言对的神经机器翻译（NMT）模型性能，利用单语语料生成合成数据，最终BLEU分数提升超过10分，证明了BT在低资源语言中的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言（如英语-Luganda）双语数据稀缺的问题，研究旨在通过反向翻译技术生成合成数据以弥补数据不足，提升翻译模型性能。

Method: 开发自定义NMT模型，结合公开和网络爬取数据，采用迭代和增量式反向翻译技术，并在多个小数据集上策略性应用增量翻译。

Result: 翻译性能显著提升，所有翻译方向的BLEU分数超过之前基准10分以上，并综合使用SacreBLEU、ChrF2和TER等指标评估质量。

Conclusion: 研究证实，策略性数据集选择下BT技术高效，为低资源语言NMT模型设立了新基准，展现了BT的广泛应用潜力。

Abstract: In this paper,we explore the application of Back translation (BT) as a
semi-supervised technique to enhance Neural Machine Translation(NMT) models for
the English-Luganda language pair, specifically addressing the challenges faced
by low-resource languages. The purpose of our study is to demonstrate how BT
can mitigate the scarcity of bilingual data by generating synthetic data from
monolingual corpora. Our methodology involves developing custom NMT models
using both publicly available and web-crawled data, and applying Iterative and
Incremental Back translation techniques. We strategically select datasets for
incremental back translation across multiple small datasets, which is a novel
element of our approach. The results of our study show significant
improvements, with translation performance for the English-Luganda pair
exceeding previous benchmarks by more than 10 BLEU score units across all
translation directions. Additionally, our evaluation incorporates comprehensive
assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced
understanding of translation quality. The conclusion drawn from our research
confirms the efficacy of BT when strategically curated datasets are utilized,
establishing new performance benchmarks and demonstrating the potential of BT
in enhancing NMT models for low-resource languages.

</details>


### [46] [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
*Muhammad Hazim Al Farouq,Aman Kassahun Wassie,Yasmin Moslem*

Main category: cs.CL

TL;DR: 论文介绍了针对Bemba-to-English低资源语言语音翻译的系统，基于Whisper和NLLB-200构建级联模型，并采用数据增强技术（如回译）。探讨了合成数据的效果及实验设置。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（Bemba-to-English）语音翻译的挑战，提升翻译性能。

Method: 使用Whisper和NLLB-200构建级联语音翻译系统，并应用数据增强技术（如回译）生成合成数据。

Result: 探讨了合成数据对翻译效果的影响及实验结果（具体效果未明确说明）。

Conclusion: 论文总结了Bemba-to-English低资源语音翻译的系统构建与数据增强技术的应用，为类似任务提供参考。

Abstract: This paper describes our system submission to the International Conference on
Spoken Language Translation (IWSLT 2025), low-resource languages track, namely
for Bemba-to-English speech translation. We built cascaded speech translation
systems based on Whisper and NLLB-200, and employed data augmentation
techniques, such as back-translation. We investigate the effect of using
synthetic data and discuss our experimental setup.

</details>


### [47] [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/abs/2505.02579)
*Lingxiao Kong,Cong Yang,Susanne Neufang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 论文提出了EMORL框架，通过集成学习优化多目标RL，提升训练效率与可解释性，并在实验中获得显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前RL在LLM微调中存在多目标平衡复杂、训练效率低、可扩展性差和可解释性有限等问题，作者希望通过集成学习改进这些问题。

Method: 采用集成多目标RL框架（EMORL），通过分层网格搜索算法优化模型聚合方式，并结合上下文信息。

Result: EMORL在实验中的训练消耗更低且更稳定（17529±1650数据点，6573±147.43秒），并提升了可扩展性和可解释性。

Conclusion: EMORL在多目标任务中表现优异，为RL微调提供了一种高效、灵活的解决方案。

Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM)
fine-tuning show promise in addressing multi-objective tasks but still face
significant challenges, including complex objective balancing, low training
efficiency, poor scalability, and limited explainability. Leveraging ensemble
learning principles, we introduce an Ensemble Multi-Objective RL (EMORL)
framework that fine-tunes multiple models with individual objectives while
optimizing their aggregation after the training to improve efficiency and
flexibility. Our method is the first to aggregate the last hidden states of
individual models, incorporating contextual information from multiple
objectives. This approach is supported by a hierarchical grid search algorithm
that identifies optimal weighted combinations. We evaluate EMORL on counselor
reflection generation tasks, using text-scoring LLMs to evaluate the
generations and provide rewards during RL fine-tuning. Through comprehensive
experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of
EMORL against existing baselines: significantly lower and more stable training
consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds),
improved scalability and explainability, and comparable performance across
multiple objectives.

</details>


### [48] [Ensemble Kalman filter for uncertainty in human language comprehension](https://arxiv.org/abs/2505.02590)
*Diksha Bhandari,Alessandro Lopopolo,Milena Rabovsky,Sebastian Reich*

Main category: cs.CL

TL;DR: 论文提出了一个贝叶斯框架来改进人工神经网络在句子处理中的不确定性建模能力，通过集成卡尔曼滤波增强人类认知模拟。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络（如句子格式塔模型）在句子处理中表现确定性行为，与人类在歧义或意外输入时的不确定性处理形成对比，因此需要改进模型以更好地反映人类认知。

Method: 采用贝叶斯框架，扩展集成卡尔曼滤波（EnKF）进行贝叶斯推断，将语言理解建模为贝叶斯逆问题，以量化不确定性。

Result: 数值实验及与极大似然估计（MLE）的对比显示，贝叶斯方法提升了不确定性表征能力，使模型更接近人类在语言歧义中的认知处理。

Conclusion: 贝叶斯方法有效增强了传统人工神经网络模型的不确定性处理能力，使其更贴近人类句子理解机制。

Abstract: Artificial neural networks (ANNs) are widely used in modeling sentence
processing but often exhibit deterministic behavior, contrasting with human
sentence comprehension, which manages uncertainty during ambiguous or
unexpected inputs. This is exemplified by reversal anomalies-sentences with
unexpected role reversals that challenge syntax and semantics-highlighting the
limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.
To address these limitations, we propose a Bayesian framework for sentence
comprehension, applying an extension of the ensemble Kalman filter (EnKF) for
Bayesian inference to quantify uncertainty. By framing language comprehension
as a Bayesian inverse problem, this approach enhances the SG model's ability to
reflect human sentence processing with respect to the representation of
uncertainty. Numerical experiments and comparisons with maximum likelihood
estimation (MLE) demonstrate that Bayesian methods improve uncertainty
representation, enabling the model to better approximate human cognitive
processing when dealing with linguistic ambiguities.

</details>


### [49] [Automatic Proficiency Assessment in L2 English Learners](https://arxiv.org/abs/2505.02615)
*Armita Mohammadi,Alessandro Lameiras Koerich,Laureano Moro-Velazquez,Patrick Cardinal*

Main category: cs.CL

TL;DR: 该论文探讨了利用深度学习技术（如CNN、ResNet、wav2vec 2.0和BERT）自动评估英语作为第二语言（L2）熟练度的可行性，重点关注语音和文本分析。


<details>
  <summary>Details</summary>
Motivation: 传统L2熟练度评估依赖于教师或专家评分，存在评分者内部和之间的差异，本研究旨在通过深度学习方法提供更客观、一致的自动化评估。

Method: 结合语音信号（使用2D CNN、频率CNN、ResNet和wav2vec 2.0）和文本转录（通过微调BERT模型）进行分类，并处理自发对话的长音频和说话者交互问题。

Result: 在EFCamDat、ANGLISH和私有数据集上的实验表明，预训练的wav2vec 2.0模型在自动化L2评估中表现突出。

Conclusion: 深度学习，尤其是wav2vec 2.0模型，为L2熟练度评估提供了稳健的自动化解决方案。

Abstract: Second language proficiency (L2) in English is usually perceptually evaluated
by English teachers or expert evaluators, with the inherent intra- and
inter-rater variability. This paper explores deep learning techniques for
comprehensive L2 proficiency assessment, addressing both the speech signal and
its correspondent transcription. We analyze spoken proficiency classification
prediction using diverse architectures, including 2D CNN, frequency-based CNN,
ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based
proficiency assessment by fine-tuning a BERT language model within resource
constraints. Finally, we tackle the complex task of spontaneous dialogue
assessment, managing long-form audio and speaker interactions through separate
applications of wav2vec 2.0 and BERT models. Results from experiments on
EFCamDat and ANGLISH datasets and a private dataset highlight the potential of
deep learning, especially the pretrained wav2vec 2.0 model, for robust
automated L2 proficiency evaluation.

</details>


### [50] [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625)
*Qingkai Fang,Yan Zhou,Shoutao Guo,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: LLaMA-Omni 2是一系列基于Qwen2.5模型构建的语音语言模型，参数量从0.5B到14B，能够实现高质量的实时语音交互，并在少量数据训练下超越现有语音模型。


<details>
  <summary>Details</summary>
Motivation: 实现下一代人机交互中实时、智能、自然的语音交互是核心目标，现有的大语言模型为智能语音聊天机器人提供了潜力。

Method: 基于Qwen2.5系列模型，集成了语音编码器和自回归流式语音解码器，仅用20万轮多轮语音对话样本训练。

Result: 在多个语音问答和语音指令跟随基准测试中表现优异，超越了基于数百万小时语音数据训练的GLM-4-Voice等现有语音模型。

Conclusion: LLaMA-Omni 2展示了在少量数据训练下实现高性能语音交互的潜力，为语音语言模型的发展提供了新方向。

Abstract: Real-time, intelligent, and natural speech interaction is an essential part
of the next-generation human-computer interaction. Recent advancements have
showcased the potential of building intelligent spoken chatbots based on large
language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of
speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable
of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built
upon the Qwen2.5 series models, integrating a speech encoder and an
autoregressive streaming speech decoder. Despite being trained on only 200K
multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong
performance on several spoken question answering and speech instruction
following benchmarks, surpassing previous state-of-the-art SpeechLMs like
GLM-4-Voice, which was trained on millions of hours of speech data.

</details>


### [51] [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/abs/2505.02656)
*Rawan Bondok,Mayar Nassar,Salam Khalifa,Kurt Micallaf,Nizar Habash*

Main category: cs.CL

TL;DR: 该论文针对阿拉伯语维基百科中未标注变音符号的专有名词（尤其是外来词）导致的发音和解释歧义问题，创建了一个手动标注变音符号的数据集，并用GPT-4o进行基准测试，结果为73%准确率。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语维基百科中的专有名词常未标注变音符号，导致发音和解释歧义，尤其是外来词的音译问题。虽然音译和变音标注在阿拉伯语NLP中已有研究，但两者的结合仍待探索。

Method: 论文引入了一个手动标注变音符号的阿拉伯语专有名词数据集（含英语维基百科对应解释），并提出挑战与标注指南。以GPT-4o为基准模型，测试其从无标注阿拉伯语和英语形式恢复完整变音标注的能力。

Result: GPT-4o的恢复变音标注任务准确率为73%，表明任务难度较高，且需要更好的模型和资源支持。

Conclusion: 论文发布了数据集以促进阿拉伯语维基百科专有名词变音标注的进一步研究，并强调当前模型在此任务上的局限性。

Abstract: Proper names in Arabic Wikipedia are frequently undiacritized, creating
ambiguity in pronunciation and interpretation, especially for transliterated
named entities of foreign origin. While transliteration and diacritization have
been well-studied separately in Arabic NLP,their intersection remains
underexplored. In this paper, we introduce a new manually diacritized dataset
of Arabic proper names of various origins with their English Wikipedia
equivalent glosses, and present the challenges and guidelines we followed to
create it. We benchmark GPT-4o on the task of recovering full diacritization
given the undiacritized Arabic and English forms, and analyze its performance.
Achieving 73% accuracy, our results underscore both the difficulty of the task
and the need for improved models and resources. We release our dataset to
facilitate further research on Arabic Wikipedia proper name diacritization.

</details>


### [52] [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)
*Miaomiao Ji,Yanqiu Wu,Zhibin Wu,Shoujin Wang,Jian Yang,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: The paper surveys reward mechanisms in aligning large language models (LLMs) with human values, proposing a three-phase framework and highlighting challenges and advances.


<details>
  <summary>Details</summary>
Motivation: To address the alignment challenge of LLMs by systematically analyzing and improving reward mechanisms.

Method: A theoretical framework categorizing reward mechanism development into feedback, reward design, and optimization phases, with a four-dimensional analysis.

Result: Identifies evolutionary trends in reward modeling, noting shifts from reinforcement learning to new paradigms and improved handling of complex alignment scenarios.

Conclusion: The survey outlines future directions for innovative reward design to enhance LLM alignment with human intentions.

Abstract: The alignment of large language models (LLMs) with human values and
intentions represents a core challenge in current AI research, where reward
mechanism design has become a critical factor in shaping model behavior. This
study conducts a comprehensive investigation of reward mechanisms in LLM
alignment through a systematic theoretical framework, categorizing their
development into three key phases: (1) feedback (diagnosis), (2) reward design
(prescription), and (3) optimization (treatment). Through a four-dimensional
analysis encompassing construction basis, format, expression, and granularity,
this research establishes a systematic classification framework that reveals
evolutionary trends in reward modeling. The field of LLM alignment faces
several persistent challenges, while recent advances in reward design are
driving significant paradigm shifts. Notable developments include the
transition from reinforcement learning-based frameworks to novel optimization
paradigms, as well as enhanced capabilities to address complex alignment
scenarios involving multimodal integration and concurrent task coordination.
Finally, this survey outlines promising future research directions for LLM
alignment through innovative reward design strategies.

</details>


### [53] [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686)
*Xiaobao Wu*

Main category: cs.CL

TL;DR: 该论文综述了大语言模型（LLMs）中“从奖励中学习”的范式，包括其在训练、推理和后推理阶段的应用，以及对未来挑战和方向的讨论。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，从奖励中学习的范式成为调整模型行为和提升推理能力的关键。

Method: 通过强化学习、奖励引导的解码和事后修正等方法，模型可以动态地从反馈中学习。

Result: 该范式使LLMs具备了更好的偏好对调和深度推理能力，并被广泛应用于多个阶段。

Conclusion: 论文强调“从奖励中学习”的重要性，同时指出了现有挑战和未来研究方向，以进一步推动LLMs的发展。

Abstract: Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities. In this survey,
we present a comprehensive overview of the paradigm of learning from rewards.
We categorize and analyze the strategies under this paradigm across training,
inference, and post-inference stages. We further discuss the benchmarks for
reward models and the primary applications. Finally we highlight the challenges
and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.

</details>


### [54] [fastabx: A library for efficient computation of ABX discriminability](https://arxiv.org/abs/2505.02692)
*Maxime Poli,Emmanuel Chemla,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: fastabx是一个高性能Python库，用于构建ABX判别任务。ABX是一种衡量类别间区分度的指标，尤其在语音表示的无监督学习中广泛应用。


<details>
  <summary>Details</summary>
Motivation: ABX任务在语音表示的无监督学习中广泛用于评估可区分性，但缺乏高效的工具限制了其更广泛的应用。fastabx旨在填补这一空白，为快速开发和计算提供高效框架。

Method: fastabx提供了一个灵活框架，能够构建任意类型的ABX任务，并高效计算表示间的距离。

Result: fastabx实现了高效的任务构建和距离计算，为快速开发周期提供了支持。

Conclusion: fastabx将成为表示学习领域的宝贵资源，帮助研究者系统性地研究从学习表示中提取信息的能力，不仅限于语音处理领域。代码已在GitHub开源。

Abstract: We introduce fastabx, a high-performance Python library for building ABX
discrimination tasks. ABX is a measure of the separation between generic
categories of interest. It has been used extensively to evaluate phonetic
discriminability in self-supervised speech representations. However, its
broader adoption has been limited by the absence of adequate tools. fastabx
addresses this gap by providing a framework capable of constructing any type of
ABX task while delivering the efficiency necessary for rapid development
cycles, both in task creation and in calculating distances between
representations. We believe that fastabx will serve as a valuable resource for
the broader representation learning community, enabling researchers to
systematically investigate what information can be directly extracted from
learned representations across several domains beyond speech processing. The
source code is available at https://github.com/bootphon/fastabx.

</details>


### [55] [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/abs/2505.02763)
*Matthew Dahl*

Main category: cs.CL

TL;DR: 论文评估了大型语言模型（LLM）在遵循复杂法律引用规范（Bluebook）时的表现，发现其完全合规率仅为69%-74%，上下文学习仅提升至77%，建议谨慎使用现成LLM自动化法律程序。


<details>
  <summary>Details</summary>
Motivation: 法律实践对程序规则有严格要求，尤其是复杂的Bluebook引用规范。研究旨在评估LLM是否能准确遵循此类高复杂性系统，为法律自动化应用提供参考。

Method: 构建了包含866项Bluebook任务的原创数据集，测试了OpenAI、Anthropic等公司的旗舰LLM，并分析其合规表现及上下文学习效果。

Result: LLM生成的引用完全合规率为69%-74%，通过上下文学习规则后仅提升至77%，表明现成模型在程序忠诚度关键领域存在局限。

Conclusion: 研究警示在程序严谨性至关重要的法律领域需慎用现成LLM，其自动化能力尚未达到可靠标准。

Abstract: Legal practice requires careful adherence to procedural rules. In the United
States, few are more complex than those found in The Bluebook: A Uniform System
of Citation. Compliance with this system's 500+ pages of byzantine formatting
instructions is the raison d'etre of thousands of student law review editors
and the bete noire of lawyers everywhere. To evaluate whether large language
models (LLMs) are able to adhere to the procedures of such a complicated
system, we construct an original dataset of 866 Bluebook tasks and test
flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)
that these models produce fully compliant Bluebook citations only 69%-74% of
the time and (2) that in-context learning on the Bluebook's underlying system
of rules raises accuracy only to 77%. These results caution against using
off-the-shelf LLMs to automate aspects of the law where fidelity to procedure
is paramount.

</details>


### [56] [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/abs/2505.02819)
*Dmitriy Shopkhoev,Ammar Ali,Magauiya Zhussip,Valentin Malykh,Stamatios Lefkimmiatis,Nikos Komodakis,Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe是一种无需训练的深度剪枝方法，通过线性变换替代Transformer块，高效压缩模型并保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法需要额外的训练或微调，而ReplaceMe旨在避免这些计算开销，仅需少量校准数据。

Method: 使用线性变换近似被剪枝的块，无需额外网络参数，可直接合并到剩余Transformer块中。

Result: 在大型语言模型上，ReplaceMe最高可剪枝25%，保留约90%的原始性能，计算开销极低。

Conclusion: ReplaceMe在无需训练的情况下，性能优于其他无训练剪枝方法，且与需微调的方法竞争力相当。

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation to approximate the pruned blocks. This
estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at this repository.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Perturbation Analysis of Singular Values in Concatenated Matrices](https://arxiv.org/abs/2505.01427)
*Maksym Shamrai*

Main category: cs.LG

TL;DR: 建立了一个分析拼接矩阵奇异值谱稳定性的扰动框架，量化了子矩阵小扰动对奇异值的影响。


<details>
  <summary>Details</summary>
Motivation: 研究拼接矩阵的奇异值谱如何与其组成矩阵的关系，为矩阵聚类和压缩提供理论基础。

Method: 开发了扰动框架，扩展Weyl不等式等经典结果到拼接矩阵，建立解析边界。

Result: 若拼接矩阵在范数上接近，则拼接后的主导奇异值保持稳定，实现精度与压缩的可控权衡。

Conclusion: 为数值线性代数、信号处理和数据建模中的矩阵聚类与压缩策略提供理论支持。

Abstract: Concatenating matrices is a common technique for uncovering shared structures
in data through singular value decomposition (SVD) and low-rank approximations.
However, a fundamental question arises: how does the singular value spectrum of
the concatenated matrix relate to the spectra of its individual components? In
this work, we develop a perturbation framework that extends classical results
such as Weyl's inequality to concatenated matrices. We establish analytical
bounds that quantify the stability of singular values under small perturbations
in the submatrices. Our results show that if the matrices being concatenated
are close in norm, the dominant singular values of the concatenated matrix
remain stable, enabling controlled trade-offs between accuracy and compression.
These insights provide a theoretical foundation for improved matrix clustering
and compression strategies, with applications in numerical linear algebra,
signal processing, and data-driven modeling.

</details>


### [58] [Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets](https://arxiv.org/abs/2505.01437)
*Hassan Wasswa,Timothy Lynar,Hussein Abbass*

Main category: cs.LG

TL;DR: 该研究利用变分自编码器（VAE）和成本敏感学习开发轻量级模型，用于检测物联网僵尸网络，重点关注不平衡数据集中少数类攻击流量的检测。


<details>
  <summary>Details</summary>
Motivation: 物联网设备成为恶意攻击的薄弱环节，尤其是僵尸网络攻击，研究旨在通过改进模型检测少数类攻击流量。

Method: 结合VAE和成本敏感学习，评估了前馈深度神经网络（DNN）和双向LSTM（BLSTM）在高度不平衡数据集上的性能。

Result: 两种深度学习模型在准确性、精确率、召回率和F1分数上均表现优秀。

Conclusion: 提出的方法能有效检测不平衡数据集中的少数类攻击，为物联网安全提供轻量级解决方案。

Abstract: The Internet of Things (IoT) technology has rapidly gained popularity with
applications widespread across a variety of industries. However, IoT devices
have been recently serving as a porous layer for many malicious attacks to both
personal and enterprise information systems with the most famous attacks being
botnet-related attacks. The work in this study leveraged Variational
Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet
effective, models for IoT-botnet detection. The aim is to enhance the detection
of minority class attack traffic instances which are often missed by machine
learning models. The proposed approach is evaluated on a multi-class problem
setting for the detection of traffic categories on highly imbalanced datasets.
The performance of two deep learning models including the standard feed forward
deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and
both recorded commendable results in terms of accuracy, precision, recall and
F1-score for all traffic classes.

</details>


### [59] [Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials](https://arxiv.org/abs/2505.01438)
*Tengfei Xing,Xiaodan Ren,Jie Li*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合扩散模型和物理信息网络的框架，用于在两相随机材料中生成高时空分辨率的应力场，并详细研究了不同注意力位置和损失函数权重对模型精度的影响。


<details>
  <summary>Details</summary>
Motivation: 动态加载下两相随机材料的应力分析是材料设计和性能优化的关键，但现有方法的时空分辨率有限，难以准确捕捉应力集中区域，因此需要开发新的高分辨率应力场生成方法。

Method: 研究提出STS-diffusion（基于扩散模型的时空应力生成）和ST-SRPINN（基于物理信息的时空超分辨率网络）两种方法，前者用于生成全局应力数据，后者用于无监督学习提升分辨率。

Result: STS-diffusion通过STU-net提升了应力场的生成精度，ST-SRPINN利用物理约束仅需低分辨率数据即可实现任意倍数的时空分辨率提升。

Conclusion: 该框架有效解决了高分辨率应力场生成的难题，尤其在捕捉应力集中区域方面表现出色，为材料动态应力分析提供了新工具。

Abstract: Material stress analysis is a critical aspect of material design and
performance optimization. Under dynamic loading, the global stress evolution in
materials exhibits complex spatiotemporal characteristics, especially in
two-phase random materials (TRMs). Such kind of material failure is often
associated with stress concentration, and the phase boundaries are key
locations where stress concentration occurs. In practical engineering
applications, the spatiotemporal resolution of acquired microstructural data
and its dynamic stress evolution is often limited. This poses challenges for
deep learning methods in generating high-resolution spatiotemporal stress
fields, particularly for accurately capturing stress concentration regions. In
this study, we propose a framework for global stress generation and
spatiotemporal super-resolution in TRMs under dynamic loading. First, we
introduce a diffusion model-based approach, named as Spatiotemporal Stress
Diffusion (STS-diffusion), for generating global spatiotemporal stress data.
This framework incorporates Space-Time U-Net (STU-net), and we systematically
investigate the impact of different attention positions on model accuracy.
Next, we develop a physics-informed network for spatiotemporal
super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed
Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning
method. The influence of data-driven and physics-informed loss function weights
on model accuracy is explored in detail. Benefiting from physics-based
constraints, ST-SRPINN requires only low-resolution stress field data during
training and can upscale the spatiotemporal resolution of stress fields to
arbitrary magnifications.

</details>


### [60] [Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving](https://arxiv.org/abs/2505.01440)
*Alkis Sygkounas,Ioannis Athanasiadis,Andreas Persson,Michael Felsberg,Amy Loutfi*

Main category: cs.LG

TL;DR: 该研究提出了一种名为iDDQN的交互式双深度Q网络方法，通过将人类专家知识直接整合到强化学习训练中，提升自动驾驶等应用的性能与安全性。


<details>
  <summary>Details</summary>
Motivation: 人类专业知识与机器学习的结合对于高精度与安全需求的应用（如自动驾驶）至关重要，研究旨在通过HITL方法强化学习效果。

Method: iDDQN改进Q值更新方程以整合人类与智能体动作，并提出离线评估框架模拟无干预时的轨迹，评估人类干预效果。

Result: 实验表明，iDDQN在模拟自动驾驶场景中表现优于BC、HG-DAgger、DQfD等现有方法，能更高效利用人类知识。

Conclusion: iDDQN通过人类与智能体的协作策略，显著提升了模型性能与适应性，为复杂应用提供了有效解决方案。

Abstract: Integrating human expertise with machine learning is crucial for applications
demanding high accuracy and safety, such as autonomous driving. This study
introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop
(HITL) approach that enhances Reinforcement Learning (RL) by merging human
insights directly into the RL training process, improving model performance.
Our proposed iDDQN method modifies the Q-value update equation to integrate
human and agent actions, establishing a collaborative approach for policy
development. Additionally, we present an offline evaluative framework that
simulates the agent's trajectory as if no human intervention had occurred, to
assess the effectiveness of human interventions. Empirical results in simulated
autonomous driving scenarios demonstrate that iDDQN outperforms established
approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from
Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for
improving performance and adaptability.

</details>


### [61] [Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding](https://arxiv.org/abs/2505.01445)
*Muhammad Muaz,Sameed Sajid,Tobias Schulze,Chang Liu,Nils Klasen,Benny Drescher*

Main category: cs.LG

TL;DR: 该论文探讨了注塑过程中产品质量预测的机器学习模型可解释性问题，比较了多种可解释性方法对特征影响分析的影响，并验证了更好的特征归因能正确识别原因并提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 由于现有机器学习模型多为黑箱，缺乏直接解释能力，限制了其在质量控制中的应用。此前的方法要么仅适用于树基算法，要么未强调某些可解释性方法可能导致错误的根本原因识别。

Method: 研究首先通过中心复合设计收集的真实实验数据验证了输入机器设置间的交互作用，然后首次比较了模型无关的可解释AI方法，展示了不同方法如何导致不同的特征影响分析。

Result: 研究发现不同可解释性方法确实会导致不同的特征影响分析，且更好的特征归因有助于正确识别原因并提供可操作的见解。随机森林和多层感知器在实验数据集上的平均绝对百分比误差均低于0.05%。

Conclusion: 研究强调了选择适当可解释性方法的重要性，以确保正确识别产品偏离理想属性的根本原因，并为注塑过程提供可行的改进建议。

Abstract: If a product deviates from its desired properties in the injection moulding
process, its root cause analysis can be aided by models that relate the input
machine settings with the output quality characteristics. The machine learning
models tested in the quality prediction are mostly black boxes; therefore, no
direct explanation of their prognosis is given, which restricts their
applicability in the quality control. The previously attempted explainability
methods are either restricted to tree-based algorithms only or do not emphasize
on the fact that some explainability methods can lead to wrong root cause
identification of a product's deviation from its desired properties. This study
first shows that the interactions among the multiple input machine settings do
exist in real experimental data collected as per a central composite design.
Then, the model-agnostic explainable AI methods are compared for the first time
to show that different explainability methods indeed lead to different feature
impact analysis in injection moulding. Moreover, it is shown that the better
feature attribution translates to the correct cause identification and
actionable insights for the injection moulding process. Being model agnostic,
explanations on both random forest and multilayer perceptron are performed for
the cause analysis, as both models have the mean absolute percentage error of
less than 0.05% on the experimental dataset.

</details>


### [62] [OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models](https://arxiv.org/abs/2505.01448)
*Shengkai Chen,Yifang Yin,Jinming Cao,Shili Xiang,Zhenguang Liu,Roger Zimmermann*

Main category: cs.LG

TL;DR: OpenAVS利用文本作为音频和视觉模态对齐的代理，提出了一种无需训练的语言驱动方法，显著提升了音频视觉分割在新场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频视觉分割方法主要集中在封闭集场景和直接的模态对齐与融合，泛化能力有限。

Method: 通过音频到文本提示生成、LLM引导的提示翻译和文本到视觉发声对象分割三个步骤实现分割，无需训练。

Result: 在三个基准数据集上表现优异，mIoU和F-score分别提高了约9.4%和10.9%。

Conclusion: OpenAVS是一种简单灵活的方法，通过利用基础模型的能力实现了知识的高效迁移，并在自训练框架中进一步提升了性能。

Abstract: Audio-visual segmentation aims to separate sounding objects from videos by
predicting pixel-level masks based on audio signals. Existing methods primarily
concentrate on closed-set scenarios and direct audio-visual alignment and
fusion, which limits their capability to generalize to new, unseen situations.
In this paper, we propose OpenAVS, a novel training-free language-based
approach that, for the first time, effectively aligns audio and visual
modalities using text as a proxy for open-vocabulary Audio-Visual Segmentation
(AVS). Equipped with multimedia foundation models, OpenAVS directly infers
masks through 1) audio-to-text prompt generation, 2) LLM-guided prompt
translation, and 3) text-to-visual sounding object segmentation. The objective
of OpenAVS is to establish a simple yet flexible architecture that relies on
the most appropriate foundation models by fully leveraging their capabilities
to enable more effective knowledge transfer to the downstream AVS task.
Moreover, we present a model-agnostic framework OpenAVS-ST that enables the
integration of OpenAVS with any advanced supervised AVS model via pseudo-label
based self-training. This approach enhances performance by effectively
utilizing large-scale unlabeled data when available. Comprehensive experiments
on three benchmark datasets demonstrate the superior performance of OpenAVS. It
surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a
significant margin, achieving absolute performance gains of approximately 9.4%
and 10.9% in mIoU and F-score, respectively, in challenging scenarios.

</details>


### [63] [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/abs/2505.01449)
*Jiayu Wang,Aws Albarghouthi,Frederic Sala*

Main category: cs.LG

TL;DR: 论文提出了COSMOS框架，用于高效预测大语言模型（LLM）的适应策略性能与成本，避免昂贵实验，计算成本降低最高达98.71%。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限情况下如何最优选择LLM模型和适应策略的问题，避免大量实验开销。

Method: 引入COSMOS框架，结合嵌入增强的轻量代理模型预测微调性能，以及低样本缩放定律预测检索增强的上下文学习。

Result: 在八个代表性基准测试中，COSMOS实现高预测精度，平均降低计算成本92.72%，资源密集型场景下最高达98.71%。

Conclusion: 高效预测LLM适应策略不仅可行，还能大幅降低计算开销，同时保持性能标准。

Abstract: Large language models (LLMs) achieve remarkable performance across numerous
tasks by using a diverse array of adaptation strategies. However, optimally
selecting a model and adaptation strategy under resource constraints is
challenging and often requires extensive experimentation. We investigate
whether it is possible to accurately predict both performance and cost without
expensive trials. We formalize the strategy selection problem for LLMs and
introduce COSMOS, a unified prediction framework that efficiently estimates
adaptation outcomes at minimal cost. We instantiate and study the capability of
our framework via a pair of powerful predictors: embedding-augmented
lightweight proxy models to predict fine-tuning performance, and low-sample
scaling laws to forecast retrieval-augmented in-context learning. Extensive
evaluation across eight representative benchmarks demonstrates that COSMOS
achieves high prediction accuracy while reducing computational costs by 92.72%
on average, and up to 98.71% in resource-intensive scenarios. Our results show
that efficient prediction of adaptation outcomes is not only feasible but can
substantially reduce the computational overhead of LLM deployment while
maintaining performance standards.

</details>


### [64] [Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks](https://arxiv.org/abs/2505.01450)
*Chaoyi Wang,Junjie Zheng,Zihao Chen,Shiyu Xia,Chaofan Ding,Xiaohao Zhang,Xi Tao,Xiaoming He,Xinhan Di*

Main category: cs.LG

TL;DR: 论文提出了一种名为TA-Dubbing的评估基准，旨在解决现有电影配音评估指标难以全面捕捉对白、旁白、独白及演员适应性等复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的电影配音模型评估指标无法全面衡量对话、旁白、独白和演员适应性等复杂性，且缺乏一个能为电影制作提供改进方向的实际评估系统。

Method: 作者提出TA-Dubbing评估基准，覆盖电影配音的多维度评价，包括电影理解和语音生成的指标评估，并支持对先进多模态大语言模型的评测。

Result: TA-Dubbing已完全开源，包含视频素材、评估方法和标注，并持续集成新的配音模型到其排行榜中。

Conclusion: TA-Dubbing通过多维度评估和开源支持，推动了电影配音领域的发展和质量提升。

Abstract: Movie dubbing has advanced significantly, yet assessing the real-world
effectiveness of these models remains challenging. A comprehensive evaluation
benchmark is crucial for two key reasons: 1) Existing metrics fail to fully
capture the complexities of dialogue, narration, monologue, and actor
adaptability in movie dubbing. 2) A practical evaluation system should offer
valuable insights to improve movie dubbing quality and advancement in film
production. To this end, we introduce Talking Adaptive Dubbing Benchmarks
(TA-Dubbing), designed to improve film production by adapting to dialogue,
narration, monologue, and actors in movie dubbing. TA-Dubbing offers several
key advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of
dimensions of movie dubbing, incorporating metric evaluations for both movie
understanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is
designed to evaluate state-of-the-art movie dubbing models and advanced
multi-modal large language models. 3) Full Open-Sourcing: We fully open-source
TA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video
suits, evaluation methods, annotations. We also continuously integrate new
movie dubbing models into the TA-Dubbing leaderboard at
https://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie
dubbing.

</details>


### [65] [Explainable Machine Learning for Cyberattack Identification from Traffic Flows](https://arxiv.org/abs/2505.01488)
*Yujing Zhou,Marc L. Jacquet,Robel Dawit,Skyler Fabre,Dev Sarawat,Faheem Khan,Madison Newell,Yongxin Liu,Dahai Liu,Hongyun Chen,Jian Wang,Huihui Wang*

Main category: cs.LG

TL;DR: 摘要提出了一种基于深度学习的异常检测系统，通过模拟半真实环境下的网络攻击，分析交通流数据以识别信号灯被篡改的关键指标（如最长停车时长和总拥堵距离），并应用可解释AI技术提升模型可理解性。研究发现数据不一致和低流量隐蔽攻击是主要挑战，最终提高了交通系统的安全性和可信度。


<details>
  <summary>Details</summary>
Motivation: 随着交通管理系统自动化程度的提高，其成为网络攻击的主要目标，传统网络层防御手段对交通机构往往不可用，因此需要一种仅依赖交通流数据的机器学习方法来应对。

Method: 研究在半真实环境中模拟网络攻击，利用虚拟化交通网络分析破坏模式。开发了一个基于深度学习的异常检测系统，并结合可解释AI技术（XAI）提升模型的可解释性。

Result: 研究确定了最长停车时长和总拥堵距离是信号灯被篡改的关键指标，同时发现了过渡数据不一致和低流量隐蔽攻击是模型的主要挑战。最终显著提高了检测准确性和系统可信度。

Conclusion: 该研究为智能交通系统提供了一种更安全、可信的AI驱动解决方案，通过深度学习和可解释AI技术有效识别和应对网络攻击。

Abstract: The increasing automation of traffic management systems has made them prime
targets for cyberattacks, disrupting urban mobility and public safety.
Traditional network-layer defenses are often inaccessible to transportation
agencies, necessitating a machine learning-based approach that relies solely on
traffic flow data. In this study, we simulate cyberattacks in a semi-realistic
environment, using a virtualized traffic network to analyze disruption
patterns. We develop a deep learning-based anomaly detection system,
demonstrating that Longest Stop Duration and Total Jam Distance are key
indicators of compromised signals. To enhance interpretability, we apply
Explainable AI (XAI) techniques, identifying critical decision factors and
diagnosing misclassification errors. Our analysis reveals two primary
challenges: transitional data inconsistencies, where mislabeled recovery-phase
traffic misleads the model, and model limitations, where stealth attacks in
low-traffic conditions evade detection. This work enhances AI-driven traffic
security, improving both detection accuracy and trustworthiness in smart
transportation systems.

</details>


### [66] [Machine Learning for Cyber-Attack Identification from Traffic Flows](https://arxiv.org/abs/2505.01489)
*Yujing Zhou,Marc L. Jacquet,Robel Dawit,Skyler Fabre,Dev Sarawat,Faheem Khan,Madison Newell,Yongxin Liu,Dahai Liu,Hongyun Chen,Jian Wang,Huihui Wang*

Main category: cs.LG

TL;DR: 论文研究了通过交通流模式分析检测网络攻击的方法，针对Daytona Beach交通系统利用Raspberry Pi和OPNSense防火墙进行了模拟攻击和检测，重点测试了交通灯随机变绿或红的情况，模型检测准确率达85%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析交通流模式来检测网络攻击，解决当攻击者随机改变交通灯状态时，如何仅凭交通数据识别入侵的问题。

Method: 利用Raspberry Pi虚拟机和OPNSense防火墙搭建实验环境，结合SUMO生成交通动态数据，并通过Metasploit框架模拟攻击。研究特别关注交通灯被随机操控的场景。

Result: 尽管数据不平衡和交通模式重叠带来挑战，最佳模型仅凭交通流统计数据（如占有率、拥堵长度和停车时长）实现了85%的入侵检测准确率。

Conclusion: 研究表明，仅通过交通流模式分析可以有效检测特定的网络攻击，但数据质量和模式复杂性仍是关键挑战。

Abstract: This paper presents our simulation of cyber-attacks and detection strategies
on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual
machines and the OPNSense firewall, along with traffic dynamics from SUMO and
exploitation via the Metasploit framework. We try to answer the research
questions: are we able to identify cyber attacks by only analyzing traffic flow
patterns. In this research, the cyber attacks are focused particularly when
lights are randomly turned all green or red at busy intersections by
adversarial attackers. Despite challenges stemming from imbalanced data and
overlapping traffic patterns, our best model shows 85\% accuracy when detecting
intrusions purely using traffic flow statistics. Key indicators for successful
detection included occupancy, jam length, and halting durations.

</details>


### [67] [Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation](https://arxiv.org/abs/2505.01523)
*Madhav Kotecha,Vijendra Kumar Vaishya,Smita Gautam,Suraj Racha*

Main category: cs.LG

TL;DR: 提出了一种通过效用和多样性指标选择训练样本的预算子集选择方法，以高效微调大语言模型，在数学领域实现接近全数据集性能，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 旨在减少大语言模型在特定领域（如数学）微调时的计算成本和训练时间，同时保持高性能。

Method: 结合困惑度和Chain-of-Thought损失作为效用指标，以及多样性指标来选择最具代表性和信息量的训练样本。

Result: 在LLaMA-3 8B和Phi-3模型上验证，相比随机选择和其他基线方法，表现更优。

Conclusion: 该方法能有效平衡计算成本和模型性能，适用于特定领域的微调任务。

Abstract: We propose a refined approach to efficiently fine-tune large language models
(LLMs) on specific domains like the mathematical domain by employing a budgeted
subset selection method. Our approach combines utility and diversity metrics to
select the most informative and representative training examples. The final
goal is to achieve near-full dataset performance with meticulously selected
data points from the entire dataset while significantly reducing computational
cost and training time and achieving competitive performance as the full
dataset. The utility metric incorporates both perplexity and Chain-of-Thought
(CoT) loss to identify challenging examples that contribute most to model
learning, while the diversity metric ensures broad coverage across mathematical
subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing
against several baseline approaches, including random selection,
diversity-based sampling, and existing state-of-the-art subset selection
techniques.

</details>


### [68] [Contextures: Representations from Contexts](https://arxiv.org/abs/2505.01557)
*Runtian Zhai,Kai Yang,Che-Ping Tsai,Burak Varici,Zico Kolter,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: 该论文提出了‘上下文理论’，统一解释了多种表示学习方法，表明它们是学习输入与上下文变量之间的关联，并证明了这些表示在兼容的任务上是最优的。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型取得了经验上的成功，但其学习表示的系统性描述仍不明确。论文旨在填补这一空白。

Method: 理论分析证明多种学习方法（监督、自监督、流形学习）可以统一为学习上下文相关的期望算子的顶部奇异函数。并提出了评估上下文有用性的方法。

Result: 研究表明，当模型足够大时，进一步的模型扩展收益递减，而改进需要更好的上下文。提出了评估上下文的指标，实验证明其与实际性能相关。

Conclusion: 上下文理论为表示学习提供了统一框架，表明模型规模并非唯一关键，上下文的优化同样重要。

Abstract: Despite the empirical success of foundation models, we do not have a
systematic characterization of the representations that these models learn. In
this paper, we establish the contexture theory. It shows that a large class of
representation learning methods can be characterized as learning from the
association between the input and a context variable. Specifically, we show
that many popular methods aim to approximate the top-d singular functions of
the expectation operator induced by the context, in which case we say that the
representation learns the contexture. We demonstrate the generality of the
contexture theory by proving that representation learning within various
learning paradigms -- supervised, self-supervised, and manifold learning -- can
all be studied from such a perspective. We also prove that the representations
that learn the contexture are optimal on those tasks that are compatible with
the context. One important implication of the contexture theory is that once
the model is large enough to approximate the top singular functions, further
scaling up the model size yields diminishing returns. Therefore, scaling is not
all we need, and further improvement requires better contexts. To this end, we
study how to evaluate the usefulness of a context without knowing the
downstream tasks. We propose a metric and show by experiments that it
correlates well with the actual performance of the encoder on many real
datasets.

</details>


### [69] [Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation](https://arxiv.org/abs/2505.01584)
*Zhiqiang He,Zhi Liu*

Main category: cs.LG

TL;DR: 论文提出了ReSiN方法，通过重置静默神经元来应对神经网络在非平稳网络环境中的可塑性损失，显著提升了比特率和体验质量。


<details>
  <summary>Details</summary>
Motivation: 当前资源适应方法主要基于平稳假设，但实际网络环境动态变化，神经网络在此类环境下因可塑性损失难以适应。

Method: 提出了静默神经元理论，并基于此设计ReSiN方法，通过前向和反向传播状态指导神经元重置以保持可塑性。

Result: 在自适应视频流系统中，ReSiN比现有方案比特率提升168%，体验质量提升108%，同时在平稳环境中也表现优异。

Conclusion: ReSiN通过理论创新解决了神经网络在动态网络中的可塑性问题，展现了强大的适应性和性能提升。

Abstract: Adapting to non-stationary network conditions presents significant challenges
for resource adaptation. However, current solutions primarily rely on
stationary assumptions. While data-driven reinforcement learning approaches
offer promising solutions for handling network dynamics, our systematic
investigation reveals a critical limitation: neural networks suffer from
plasticity loss, significantly impeding their ability to adapt to evolving
network conditions. Through theoretical analysis of neural propagation
mechanisms, we demonstrate that existing dormant neuron metrics inadequately
characterize neural plasticity loss. To address this limitation, we have
developed the Silent Neuron theory, which provides a more comprehensive
framework for understanding plasticity degradation. Based on these theoretical
insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural
plasticity through strategic neuron resets guided by both forward and backward
propagation states. In our implementation of an adaptive video streaming
system, ReSiN has shown significant improvements over existing solutions,
achieving up to 168% higher bitrate and 108% better quality of experience (QoE)
while maintaining comparable smoothness. Furthermore, ReSiN consistently
outperforms in stationary environments, demonstrating its robust adaptability
across different network conditions.

</details>


### [70] [Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises](https://arxiv.org/abs/2505.01591)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TL;DR: 论文研究了机器学习在房价预测中的种族/民族偏见问题，开发了多种模型并评估其公平性，发现存在不同程度的偏见，并比较了不同偏见缓解方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 住房是社会基本需求，对健康、福祉和教育成果至关重要。尽管机器学习在房价预测中取得了高精度，但其可能存在的种族/民族偏见尚未充分研究，这可能加剧社会不平等。

Method: 论文结合结构性和社区层面的属性开发了多种机器学习模型，并对不同特权群体定义下的模型公平性进行了全面评估。还测试了不同偏见缓解方法的效果。

Result: 研究发现机器学习房价预测模型对受保护属性（种族和民族）表现出不同程度的偏见。实验表明，在此问题领域中，处理中的偏见缓解方法通常比预处理方法更有效。

Conclusion: 论文强调了在房价预测中考虑机器学习公平性的重要性，并证明了处理中偏见缓解方法的潜力。

Abstract: As a basic human need, housing plays a key role in enhancing health,
well-being, and educational outcome in society, and the housing market is a
major factor for promoting quality of life and ensuring social equity. To
improve the housing conditions, there has been extensive research on building
Machine Learning (ML)-driven house price prediction solutions to accurately
forecast the future conditions, and help inform actions and policies in the
field. In spite of their success in developing high-accuracy models, there is a
gap in our understanding of the extent to which various ML-driven house price
prediction approaches show ethnic and/or racial bias, which in turn is
essential for the responsible use of ML, and ensuring that the ML-driven
solutions do not exacerbate inequity. To fill this gap, this paper develops
several ML models from a combination of structural and neighborhood-level
attributes, and conducts comprehensive assessments on the fairness of ML models
under various definitions of privileged groups. As a result, it finds that the
ML-driven house price prediction models show various levels of bias towards
protected attributes (i.e., race and ethnicity in this study). Then, it
investigates the performance of different bias mitigation solutions, and the
experimental results show their various levels of effectiveness on different
ML-driven methods. However, in general, the in-processing bias mitigation
approach tends to be more effective than the pre-processing one in this problem
domain. Our code is available at https://github.com/wahab1412/housing_fairness.

</details>


### [71] [Don't be lazy: CompleteP enables compute-efficient deep transformers](https://arxiv.org/abs/2505.01618)
*Nolan Dey,Bin Claire Zhang,Lorenzo Noci,Mufan Li,Blake Bordelon,Shane Bergsma,Cengiz Pehlevan,Boris Hanin,Joel Hestness*

Main category: cs.LG

TL;DR: 该论文研究了不同参数化方法对大语言模型（LLM）训练效率的影响，提出了名为CompleteP的参数化方法，既能实现超参数跨模型深度的高效迁移，又能避免分层学习的惰性，从而显著提升计算效率（12-34%）。


<details>
  <summary>Details</summary>
Motivation: 目前LLM训练中，超参数（如学习率）的调整规则在不同模型规模下表现不一致，导致需要重新调参或接受次优性能。此外，分层学习的惰性限制了模型深度和非线性的有效利用。

Method: 论文分析了不同参数化方法的局限性，并基于理论推导提出了CompleteP参数化方法，确保超参数跨模型深度的高效迁移和非惰性学习。

Result: CompleteP显著提升了计算效率（12-34%），且支持更灵活的模型宽度/深度比例，适配不同硬件和场景。

Conclusion: CompleteP是一种高效的参数化方法，解决了跨模型规模超参数迁移和非惰性学习问题，为LLM训练提供了更优的解决方案。

Abstract: We study compute efficiency of LLM training when using different
parameterizations, i.e., rules for adjusting model and optimizer
hyperparameters (HPs) as model size changes. Some parameterizations fail to
transfer optimal base HPs (such as learning rate) across changes in model
depth, requiring practitioners to either re-tune these HPs as they scale up
(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even
when they achieve HP transfer, we develop theory to show parameterizations may
still exist in the lazy learning regime where layers learn only features close
to their linearization, preventing effective use of depth and nonlinearity.
Finally, we identify and adopt the unique parameterization we call CompleteP
that achieves both depth-wise HP transfer and non-lazy learning in all layers.
CompleteP enables a wider range of model width/depth ratios to remain
compute-efficient, unlocking shapes better suited for different hardware
settings and operational contexts. Moreover, CompleteP enables 12-34\% compute
efficiency improvements over the prior state-of-the-art.

</details>


### [72] [Skill-based Safe Reinforcement Learning with Risk Planning](https://arxiv.org/abs/2505.01619)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: This paper proposes Safe Skill Planning (SSkP), a two-stage approach combining offline demonstration data with online reinforcement learning to enhance safe RL by predicting skill risks and planning accordingly.


<details>
  <summary>Details</summary>
Motivation: Improving safety in RL is crucial when agents interact with real-world environments, as improper actions can have severe consequences. Leveraging offline data can help predict risks and guide safer online learning.

Method: SSkP first uses PU learning to predict skill risks from offline data, then integrates this predictor into an online RL process for risk-averse policy learning and risk predictor adaptation.

Result: Experiments in robotic simulation environments show SSkP outperforms existing safe RL methods in safety and performance.

Conclusion: SSkP effectively combines offline risk prediction with online RL, offering a robust solution for safe reinforcement learning in dynamic environments.

Abstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent
conducts learning by interacting with real-world environments where improper
actions can induce high costs or lead to severe consequences. In this paper, we
propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe
RL by exploiting auxiliary offline demonstration data. SSkP involves a
two-stage process. First, we employ PU learning to learn a skill risk predictor
from the offline demonstration data. Then, based on the learned skill risk
predictor, we develop a novel risk planning process to enhance online safe RL
and learn a risk-averse safe policy efficiently through interactions with the
online RL environment, while simultaneously adapting the skill risk predictor
to the environment. We conduct experiments in several benchmark robotic
simulation environments. The experimental results demonstrate that the proposed
approach consistently outperforms previous state-of-the-art safe RL methods.

</details>


### [73] [A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components](https://arxiv.org/abs/2505.01627)
*Fatemeh Elhambakhsh,Daniele Grandi,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM的领域自适应框架，通过微调GPT-3.5 Turbo来自动分类机械装配零件的功能，提升功能注释的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的功能建模在概念设计阶段缺乏结构化功能数据，影响了早期设计决策和行为模型的准确性。LLMs在语言理解方面的优势为解决这一问题提供了可能。

Method: 研究者提出了一个基于LLM的领域自适应框架，通过对GPT-3.5 Turbo进行微调，使用特定领域数据集（如OSDR）来实现机械零件功能的自动分类。

Result: 在ABC数据集上的评估显示，经过领域适应的LLM能够生成高质量的功能数据，提升机械零件的语义表示，支持更有效的早期工程设计。

Conclusion: LLM领域自适应框架显著改善了功能数据的自动生成，为概念设计阶段的功能建模提供了更高效和准确的工具。

Abstract: The conceptual design phase represents a critical early stage in the product
development process, where designers generate potential solutions that meet
predefined design specifications based on functional requirements. Functional
modeling, a foundational aspect of this phase, enables designers to reason
about product functions before specific structural details are determined. A
widely adopted approach to functional modeling is the
Function-Behavior-Structure (FBS) framework, which supports the transformation
of functional intent into behavioral and structural descriptions. However, the
effectiveness of function-based design is often hindered by the lack of
well-structured and comprehensive functional data. This scarcity can negatively
impact early design decision-making and hinder the development of accurate
behavioral models. Recent advances in Large Language Models (LLMs), such as
those based on GPT architectures, offer a promising avenue to address this gap.
LLMs have demonstrated significant capabilities in language understanding and
natural language processing (NLP), making them suitable for automated
classification tasks. This study proposes a novel LLM-based domain adaptation
(DA) framework using fine-tuning for the automated classification of mechanical
assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the
traditionally manual and subjective process of function annotation can be
improved in both accuracy and consistency. A case study demonstrates
fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository
(OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the
domain-adapted LLM can generate high-quality functional data, enhancing the
semantic representation of mechanical parts and supporting more effective
design exploration in early-phase engineering.

</details>


### [74] [Causally Fair Node Classification on Non-IID Graph Data](https://arxiv.org/abs/2505.01652)
*Yucong Dai,Lu Zhang,Yaowei Hu,Susan Gauch,Yongkai Wu*

Main category: cs.LG

TL;DR: 该论文提出了基于因果关系的图数据公平性方法MPVA，通过NSCM框架和$do$-calculus计算干预分布，有效减轻非IID图数据中的偏见，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前公平性机器学习方法多数假设IID数据，忽略了图数据中实例间的因果关系，导致在非IID图数据中效果有限。

Method: 基于NSCM框架和两个假设（可分解性和图独立性），提出MPVA模型，利用变分自编码器和消息传递机制估算干预分布。

Result: 在半合成和真实数据集上，MPVA能更准确估算干预分布，显著减少偏见，优于传统方法。

Conclusion: 因果公平性在复杂ML应用中潜力巨大，未来可放宽初始假设以进一步提升公平性。

Abstract: Fair machine learning seeks to identify and mitigate biases in predictions
against unfavorable populations characterized by demographic attributes, such
as race and gender. Recently, a few works have extended fairness to graph data,
such as social networks, but most of them neglect the causal relationships
among data instances. This paper addresses the prevalent challenge in
fairness-aware ML algorithms, which typically assume Independent and
Identically Distributed (IID) data. We tackle the overlooked domain of non-IID,
graph-based settings where data instances are interconnected, influencing the
outcomes of fairness interventions. We base our research on the Network
Structural Causal Model (NSCM) framework and posit two main assumptions:
Decomposability and Graph Independence, which enable the computation of
interventional distributions in non-IID settings using the $do$-calculus. Based
on that, we develop the Message Passing Variational Autoencoder for Causal
Inference (MPVA) to compute interventional distributions and facilitate
causally fair node classification through estimated interventional
distributions. Empirical evaluations on semi-synthetic and real-world datasets
demonstrate that MPVA outperforms conventional methods by effectively
approximating interventional distributions and mitigating bias. The
implications of our findings underscore the potential of causality-based
fairness in complex ML applications, setting the stage for further research
into relaxing the initial assumptions to enhance model fairness.

</details>


### [75] [Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification](https://arxiv.org/abs/2505.01660)
*Sicong Li,Qianqian Xu,Zhiyong Yang,Zitai Wang,Linchao Zhang,Xiaochun Cao,Qingming Huang*

Main category: cs.LG

TL;DR: 本文介绍了Focal-SAM，一种在长尾数据分布中通过差异化惩罚类间锐度来提升模型泛化能力的方法，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 面对长尾数据分布，现有方法（如ImbSAM和CC-SAM）在计算效率与损失景观控制之间存在权衡，无法同时满足高效性和精细控制的需求。

Method: 提出Focal-SAM，通过对类间锐度分配不同惩罚，实现无需额外反向传播的精细控制，兼顾效率。

Result: 理论分析和广泛实验表明，Focal-SAM在传统模型和基础模型上均有效，且泛化边界更优。

Conclusion: Focal-SAM平衡了效率与控制，为长尾学习问题提供了更优解决方案。

Abstract: Real-world datasets often follow a long-tailed distribution, making
generalization to tail classes difficult. Recent methods resorted to long-tail
variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to
improve generalization by flattening the loss landscape. However, these
attempts face a trade-off between computational efficiency and control over the
loss landscape. On the one hand, ImbSAM is efficient but offers only coarse
control as it excludes head classes from the SAM process. On the other hand,
CC-SAM provides fine-grained control through class-dependent perturbations but
at the cost of efficiency due to multiple backpropagations. Seeing this
dilemma, we introduce Focal-SAM, which assigns different penalties to
class-wise sharpness, achieving fine-grained control without extra
backpropagations, thus maintaining efficiency. Furthermore, we theoretically
analyze Focal-SAM's generalization ability and derive a sharper generalization
bound. Extensive experiments on both traditional and foundation models validate
the effectiveness of Focal-SAM.

</details>


### [76] [Adaptively Point-weighting Curriculum Learning](https://arxiv.org/abs/2505.01665)
*Wensheng Li,Hao Wang,Ruifeng Zhou,Hanting Guan,Chao Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: 该研究提出了一种自适应点加权课程学习（APW）算法，通过动态调整样本权重优化训练效果。


<details>
  <summary>Details</summary>
Motivation: 模仿人类学习过程，通过从易到难的样本训练提升深度网络的效率和性能。

Method: APW算法基于样本的训练误差和网络的当前状态动态分配权重，早期侧重易样本，后期侧重难样本。

Result: 实验证明APW在训练效果、稳定性和泛化性能上优于传统方法，理论分析也得到验证。

Conclusion: APW是一种高效且理论支持的课程学习策略，适用于深度网络的训练。

Abstract: Curriculum learning (CL) is referred to as a training strategy that makes
easy samples learned first and then fits hard samples. It imitates the process
of humans learning knowledge, and has become a potential manner of effectively
training deep networks. In this study, we develop the adaptively
point-weighting (APW) curriculum learning algorithm, which adaptively assigns
the weight to every training sample not only based on its training error but
also considering the current training state of the network. Specifically, in
the early training phase, it increases the weights of easy samples to make the
network rapidly capture the overall characteristics of the dataset; and in the
later training phase, the weights of hard points rise to improve the fitting
performance on the discrete local regions. Moreover, we also present the
theoretical analysis on the properties of APW including training effectiveness,
training feasibility, training stability, and generalization performance. The
numerical experiments support the superiority of APW and demonstrate the
validity of our theoretical findings.

</details>


### [77] [PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking](https://arxiv.org/abs/2505.01700)
*Yize Jiang,Xinze Li,Yuanyuan Zhang,Jin Han,Youjun Xu,Ayush Pandit,Zaixi Zhang,Mengdi Wang,Mengyang Wang,Chong Liu,Guang Yang,Yejin Choi,Wu-Jun Li,Tianfan Fu,Fang Wu,Junhong Liu*

Main category: cs.LG

TL;DR: 提出了PoseX，一个开源自对标和交叉对标的基准，用于实际和全面地评估对接方法，包括新的数据集、22种对接方法和后处理松弛技术。研究发现AI方法在精度上已超越传统物理方法，且后处理能大幅提升表现，但AI共折叠方法存在手性问题。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白-配体对接基准存在实用性不足或框架复杂的问题，难以高效评估方法。

Method: 构建新数据集（718条自对标、1312条交叉对标），纳入22种方法（物理、AI对接、AI共折叠），设计后处理松弛方法，并发布实时排行榜。

Result: AI方法在RMSD精度上超越物理方法，后处理显著改善立体化学问题，但AI共折叠方法无法解决手性问题。

Conclusion: PoseX为对接方法提供了实用、全面的评估工具，AI结合后处理表现最佳，但AI共折叠仍需改进。

Abstract: Recently, significant progress has been made in protein-ligand docking,
especially in modern deep learning methods, and some benchmarks were proposed,
e.g., PoseBench, Plinder. However, these benchmarks suffer from less practical
evaluation setups (e.g., blind docking, self docking), or heavy framework that
involves training, raising challenges to assess docking methods efficiently. To
fill this gap, we proposed PoseX, an open-source benchmark focusing on
self-docking and cross-docking, to evaluate the algorithmic advances
practically and comprehensively. Specifically, first, we curate a new
evaluation dataset with 718 entries for self docking and 1,312 for cross
docking; second, we incorporate 22 docking methods across three methodological
categories, including (1) traditional physics-based methods (e.g.,
Schr\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI
co-folding methods (e.g., AlphaFold3); third, we design a relaxation method as
post-processing to minimize conformation energy and refine binding pose;
fourth, we released a leaderboard to rank submitted models in real time. We
draw some key insights via extensive experiments: (1) AI-based approaches have
already surpassed traditional physics-based approaches in overall docking
accuracy (RMSD). The longstanding generalization issues that have plagued AI
molecular docking have been significantly alleviated in the latest models. (2)
The stereochemical deficiencies of AI-based approaches can be greatly
alleviated with post-processing relaxation. Combining AI docking methods with
the enhanced relaxation method achieves the best performance to date. (3) AI
co-folding methods commonly face ligand chirality issues, which cannot be
resolved by relaxation. The code, curated dataset and leaderboard are released
at https://github.com/CataAI/PoseX.

</details>


### [78] [PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems](https://arxiv.org/abs/2505.01736)
*Han Wan,Rui Zhang,Qi Wang,Yang Liu,Hao Sun*

Main category: cs.LG

TL;DR: 该论文提出了PeSANet模型，结合局部与全局信息，利用有限数据和不完整物理先验预测复杂系统。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法因物理定律不完整或未知在现实场景中表现不佳，而机器学习方法在稀缺数据和局部全局特征捕捉上泛化能力不足。

Method: PeSANet包含物理编码块（通过硬约束近似局部微分算子）和频谱增强块（频域捕获长程依赖），并引入谱注意力机制建模谱间关系。

Result: 实验表明PeSANet在所有指标上优于现有方法，尤其在长期预测精度上表现出色。

Conclusion: PeSANet为有限数据和不完整物理下的复杂系统模拟提供了有效解决方案。

Abstract: Accurately modeling and forecasting complex systems governed by partial
differential equations (PDEs) is crucial in various scientific and engineering
domains. However, traditional numerical methods struggle in real-world
scenarios due to incomplete or unknown physical laws. Meanwhile, machine
learning approaches often fail to generalize effectively when faced with scarce
observational data and the challenge of capturing local and global features. To
this end, we propose the Physics-encoded Spectral Attention Network (PeSANet),
which integrates local and global information to forecast complex systems with
limited data and incomplete physical priors. The model consists of two key
components: a physics-encoded block that uses hard constraints to approximate
local differential operators from limited data, and a spectral-enhanced block
that captures long-range global dependencies in the frequency domain.
Specifically, we introduce a novel spectral attention mechanism to model
inter-spectrum relationships and learn long-range spatial features.
Experimental results demonstrate that PeSANet outperforms existing methods
across all metrics, particularly in long-term forecasting accuracy, providing a
promising solution for simulating complex systems with limited data and
incomplete physics.

</details>


### [79] [Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients](https://arxiv.org/abs/2505.01744)
*Yezhen Wang,Zhouhao Yang,Brian K Chen,Fanyi Pu,Bo Li,Tianyu Gao,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: VLoRP 是一个新型低秩梯度投影框架，通过引入投影粒度控制机制，提升了内存效率和性能的平衡。优化器 ProjFactor 显著减少了内存需求，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有 LoRP 方法默认以梯度矩阵的行作为投影单位，忽视了投影粒度的影响。本研究旨在探索投影粒度的作用，优化内存效率和性能的权衡。

Method: 提出了 VLoRP 框架，通过控制投影粒度调整性能与内存效率的平衡。开发了 ProjFactor 优化器，在梯度累积时实现内存高效优化。

Result: 实验表明，细粒度投影在固定内存预算下提升了稳定性和效率。ProjFactor 在多种任务（常识推理、MMLU、GSM8K）中表现优异。

Conclusion: VLoRP 和 ProjFactor 在理论和实验上验证了其有效性，为低秩梯度投影提供了更灵活和高效的解决方案。

Abstract: Building upon the success of low-rank adapter (LoRA), low-rank gradient
projection (LoRP) has emerged as a promising solution for memory-efficient
fine-tuning. However, existing LoRP methods typically treat each row of the
gradient matrix as the default projection unit, leaving the role of projection
granularity underexplored. In this work, we propose a novel framework, VLoRP,
that extends low-rank gradient projection by introducing an additional degree
of freedom for controlling the trade-off between memory efficiency and
performance, beyond the rank hyper-parameter. Through this framework, we
systematically explore the impact of projection granularity, demonstrating that
finer-grained projections lead to enhanced stability and efficiency even under
a fixed memory budget. Regarding the optimization for VLoRP, we present
ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces
memory requirement while ensuring competitive performance, even in the presence
of gradient accumulation. Additionally, we provide a theoretical analysis of
VLoRP, demonstrating the descent and convergence of its optimization trajectory
under both SGD and ProjFactor. Extensive experiments are conducted to validate
our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K.

</details>


### [80] [Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition](https://arxiv.org/abs/2505.01783)
*Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出了一种名为C-PP-COAD的在线异常检测框架，通过结合合成校准数据和真实数据来应对数据稀缺问题，同时保持严格的FDR控制。


<details>
  <summary>Details</summary>
Motivation: 在线异常检测在网络安全、医疗健康和工业监控等领域至关重要，但现有方法依赖大量真实校准数据。C-PP-COAD旨在解决数据稀缺问题。

Method: C-PP-COAD利用合成校准数据并结合上下文信息动态整合真实数据，采用符合性p值、活跃p值统计和在线FDR控制机制。

Result: 实验表明，C-PP-COAD显著减少了对真实校准数据的依赖，同时保持了FDR控制的可靠性。

Conclusion: C-PP-COAD是一种高效且可靠的在线异常检测方法，尤其适用于数据稀缺的场景。

Abstract: Online anomaly detection is essential in fields such as cybersecurity,
healthcare, and industrial monitoring, where promptly identifying deviations
from expected behavior can avert critical failures or security breaches. While
numerous anomaly scoring methods based on supervised or unsupervised learning
have been proposed, current approaches typically rely on a continuous stream of
real-world calibration data to provide assumption-free guarantees on the false
discovery rate (FDR). To address the inherent challenges posed by limited real
calibration data, we introduce context-aware prediction-powered conformal
online anomaly detection (C-PP-COAD). Our framework strategically leverages
synthetic calibration data to mitigate data scarcity, while adaptively
integrating real data based on contextual cues. C-PP-COAD utilizes conformal
p-values, active p-value statistics, and online FDR control mechanisms to
maintain rigorous and reliable anomaly detection performance over time.
Experiments conducted on both synthetic and real-world datasets demonstrate
that C-PP-COAD significantly reduces dependency on real calibration data
without compromising guaranteed FDR control.

</details>


### [81] [Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning](https://arxiv.org/abs/2505.01788)
*Md. Tanzib Hosain,Asif Zaman,Md. Shahriar Sajid,Shadman Sakeeb Khan,Shanjida Akter*

Main category: cs.LG

TL;DR: 论文分析了隐私保护联邦学习（FL）中的个性化机器学习（ML）方法PPMLFPL，评估了APPLE+DP和APPLE+HE算法在平衡模型个性化与数据隐私方面的表现，推荐APPLE+HE作为优先方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI的广泛使用，数据隐私问题日益突出，联邦学习成为解决分散数据训练与隐私保护的关键范式。本研究旨在探索如何在个性化ML中有效保护隐私。

Method: 研究提出并评估了PPMLFPL框架，具体包括自适应个性化跨孤岛联邦学习结合差分隐私（APPLE+DP）和同态加密（APPLE+HE）两种算法。

Result: APPLE+DP执行效率高，而APPLE+HE在隐私保护联邦学习任务中表现更优，被推荐为首选方案。

Conclusion: 该研究为隐私敏感的数据驱动技术提供了重要见解，推动了未来隐私保护个性化ML的发展。

Abstract: The widespread adoption of Artificial Intelligence (AI) has been driven by
significant advances in intelligent system research. However, this progress has
raised concerns about data privacy, leading to a growing awareness of the need
for privacy-preserving AI. In response, there has been a seismic shift in
interest towards the leading paradigm for training Machine Learning (ML) models
on decentralized data silos while maintaining data privacy, Federated Learning
(FL). This research paper presents a comprehensive performance analysis of a
cutting-edge approach to personalize ML model while preserving privacy achieved
through Privacy Preserving Machine Learning with the innovative framework of
Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns
about data privacy, this study evaluates the effectiveness of PPMLFPL
addressing the critical balance between personalized model refinement and
maintaining the confidentiality of individual user data. According to our
analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential
Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the
Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption
(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated
personalized learning settings is strongly suggested. The results offer
valuable insights creating it a promising scope for future advancements in the
field of privacy-conscious data-driven technologies.

</details>


### [82] [Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees](https://arxiv.org/abs/2505.01810)
*Zhiyi Zhou,Hexin Peng,Hongyu Long*

Main category: cs.LG

TL;DR: 该论文探讨了在复杂室内环境中，利用保形预测（CP）技术改进基于深度学习的室内定位方法，通过构建预测集和控制风险，显著提高了定位精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术的发展，高精度室内定位成为位置服务的核心需求。然而，传统的指纹定位算法和深度学习方法在泛化性、过拟合和可解释性方面存在挑战，促使研究者探索更可靠的解决方案。

Method: 论文应用保形预测（CP）技术，将模型的不确定性转化为非一致性分数，构建预测集以保证正确性覆盖率，并引入保形风险控制以管理路径导航任务中的错误发现率（FDR）和假阴性率（FNR）。采用轻量级模型（如MobileNet、VGG19等）进行实验。

Result: 模型在训练数据集上准确率接近100%，测试数据集上达到85%。保形预测技术能够有效逼近目标覆盖率，且不同模型在预测集大小和不确定性量化方面表现各异。

Conclusion: 保形预测技术在提高室内定位的精度和可靠性方面表现出色，同时为模型的不确定性提供了统计保障。轻量级模型的选择需根据具体需求权衡性能与效率。

Abstract: With the advancement of Internet of Things (IoT) technologies, high-precision
indoor positioning has become essential for Location-Based Services (LBS) in
complex indoor environments. Fingerprint-based localization is popular, but
traditional algorithms and deep learning-based methods face challenges such as
poor generalization, overfitting, and lack of interpretability. This paper
applies conformal prediction (CP) to deep learning-based indoor positioning. CP
transforms the uncertainty of the model into a non-conformity score, constructs
prediction sets to ensure correctness coverage, and provides statistical
guarantees. We also introduce conformal risk control for path navigation tasks
to manage the false discovery rate (FDR) and the false negative rate (FNR).The
model achieved an accuracy of approximately 100% on the training dataset and
85% on the testing dataset, effectively demonstrating its performance and
generalization capability. Furthermore, we also develop a conformal p-value
framework to control the proportion of position-error points. Experiments on
the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19,
MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction
technique can effectively approximate the target coverage, and different models
have different performance in terms of prediction set size and uncertainty
quantification.

</details>


### [83] [An LSTM-PINN Hybrid Method to the specific problem of population forecasting](https://arxiv.org/abs/2505.01819)
*Ze Tao*

Main category: cs.LG

TL;DR: 本文提出两种物理信息深度学习框架（PINN和LSTM-PINN），用于模拟政策驱动下的年龄结构人口动态。结果表明，这些模型能有效捕捉长期时间依赖性，并提供政策敏感的人口预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习在复杂动力学系统建模中潜力巨大，但缺乏与领域知识的有效整合，难以准确模拟政策驱动的年龄结构人口动态。

Method: 提出PINN和LSTM-PINN框架，结合政策感知生育函数与传输-反应偏微分方程，通过基于配置的训练捕捉长期依赖性。

Result: 三种生育政策情景下的模拟验证了模型对政策敏感人口动态的预测能力，凸显了领域知识与数据驱动结合的优越性。

Conclusion: 本研究为政策干预下的年龄结构人口动态建模提供了可扩展框架，为人口预测和长期政策规划提供了新工具。

Abstract: Deep learning has emerged as a powerful tool in scientific modeling,
particularly for complex dynamical systems; however, accurately capturing
age-structured population dynamics under policy-driven fertility changes
remains a significant challenge due to the lack of effective integration
between domain knowledge and long-term temporal dependencies. To address this
issue, we propose two physics-informed deep learning frameworks--PINN and
LSTM-PINN--that incorporate policy-aware fertility functions into a
transport-reaction partial differential equation to simulate population
evolution from 2024 to 2054. The standard PINN model enforces the governing
equation and boundary conditions via collocation-based training, enabling
accurate learning of underlying population dynamics and ensuring stable
convergence. Building on this, the LSTM-PINN framework integrates sequential
memory mechanisms to effectively capture long-range dependencies in the
age-time domain, achieving robust training performance across multiple loss
components. Simulation results under three distinct fertility policy
scenarios-the Three-child policy, the Universal two-child policy, and the
Separate two-child policy--demonstrate the models' ability to reflect
policy-sensitive demographic shifts and highlight the effectiveness of
integrating domain knowledge into data-driven forecasting. This study provides
a novel and extensible framework for modeling age-structured population
dynamics under policy interventions, offering valuable insights for
data-informed demographic forecasting and long-term policy planning in the face
of emerging population challenges.

</details>


### [84] [Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2505.01822)
*Jifeng Hu,Sili Huang,Zhejian Yang,Shengchao Hu,Li Shen,Hechang Chen,Lichao Sun,Yi Chang,Dacheng Tao*

Main category: cs.LG

TL;DR: 这篇论文提出了Analytic Energy-guided Policy Optimization (AEPO)，用于解决扩散模型中估计中间能量的难题，通过理论分析和闭式解优化策略，在多任务离线强化学习中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在强化学习中表现出潜力，但中间能量的估计因对数期望公式的复杂性而难以实现，成为制约性能的关键问题。

Method: 首先分析了条件高斯变换下扩散模型的中间引导闭式解；其次推导了对数期望的后验高斯分布，提出基于温和假设的估计目标；最后训练神经网络逼近这一目标。

Result: 方法在30+离线RL任务中验证，效果显著，超越了D4RL基准中的多个代表性基线。

Conclusion: AEPO通过理论指导的能量估计优化策略，有效提升了扩散模型在强化学习中的性能，具有广泛的应用潜力。

Abstract: Conditional decision generation with diffusion models has shown powerful
competitiveness in reinforcement learning (RL). Recent studies reveal the
relation between energy-function-guidance diffusion models and constrained RL
problems. The main challenge lies in estimating the intermediate energy, which
is intractable due to the log-expectation formulation during the generation
process. To address this issue, we propose the Analytic Energy-guided Policy
Optimization (AEPO). Specifically, we first provide a theoretical analysis and
the closed-form solution of the intermediate guidance when the diffusion model
obeys the conditional Gaussian transformation. Then, we analyze the posterior
Gaussian distribution in the log-expectation formulation and obtain the target
estimation of the log-expectation under mild assumptions. Finally, we train an
intermediate energy neural network to approach the target estimation of
log-expectation formulation. We apply our method in 30+ offline RL tasks to
demonstrate the effectiveness of our method. Extensive experiments illustrate
that our method surpasses numerous representative baselines in D4RL offline
reinforcement learning benchmarks.

</details>


### [85] [Towards Trustworthy Federated Learning with Untrusted Participants](https://arxiv.org/abs/2505.01874)
*Youssef Allouah,Rachid Guerraoui,John Stephan*

Main category: cs.LG

TL;DR: CafCor算法通过利用工作者之间的共享随机性，结合健壮的梯度聚合和相关性噪声注入，在不完全信任中央服务器的情况下，实现了隐私保护和高效实用的分布式学习。


<details>
  <summary>Details</summary>
Motivation: 当前分布式学习需要在数据隐私和恶意攻击防护之间取得平衡，通常依赖可信中央服务器。本研究旨在验证在较弱的假设（每对工作者共享未知的随机种子）下，仍能实现这两者的共存。

Method: 提出CafCor算法，结合健壮的梯度聚合和相关性噪声注入，利用工作者之间的共享随机性。该方法在恶意工作者可能与不受信任的服务器勾结的场景下运行。

Result: 理论证明CafCor在隐私-效用权衡上表现优异，显著优于本地差分隐私方法，并接近完全信任服务器时的中心差分隐私效用。标准基准测试验证了其实际可行性。

Conclusion: 研究表明，无需完全信任服务器或牺牲实用性，分布式系统中隐私和鲁棒性可以共存，CafCor为此提供了一种可行的解决方案。

Abstract: Resilience against malicious parties and data privacy are essential for
trustworthy distributed learning, yet achieving both with good utility
typically requires the strong assumption of a trusted central server. This
paper shows that a significantly weaker assumption suffices: each pair of
workers shares a randomness seed unknown to others. In a setting where
malicious workers may collude with an untrusted server, we propose CafCor, an
algorithm that integrates robust gradient aggregation with correlated noise
injection, leveraging shared randomness between workers. We prove that CafCor
achieves strong privacy-utility trade-offs, significantly outperforming local
differential privacy (DP) methods, which do not make any trust assumption,
while approaching central DP utility, where the server is fully trusted.
Empirical results on standard benchmarks validate CafCor's practicality,
showing that privacy and robustness can coexist in distributed systems without
sacrificing utility or trusting the server.

</details>


### [86] [OODTE: A Differential Testing Engine for the ONNX Optimizer](https://arxiv.org/abs/2505.01892)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: ONNX Optimizer的准确性未严格验证。OODTE工具通过差分测试评估其优化效果，发现多个问题。


<details>
  <summary>Details</summary>
Motivation: 验证ONNX Optimizer在模型优化过程中保持准确性的能力。

Method: 利用OODTE工具对130个ONNX模型进行差分测试，比较优化前后结果并定位问题。

Result: 发现15个问题（14个新问题），9.2%模型导致优化器崩溃，30%分类模型存在精度差异。

Conclusion: ONNX Optimizer需改进以保证优化后模型的准确性和稳定性。

Abstract: With $700$ stars on GitHub and part of the official ONNX repository, the ONNX
Optimizer consists of the standard method to apply graph-based optimizations on
ONNX models. However, its ability to preserve model accuracy across
optimizations, has not been rigorously explored. We propose OODTE, a utility to
automatically and thoroughly assess the correctness of the ONNX Optimizer.
OODTE follows a simple, yet effective differential testing and evaluation
approach that can be easily adopted to other compiler optimizers. In
particular, OODTE utilizes a number of ONNX models, then optimizes them and
executes both the original and the optimized variants across a user-defined set
of inputs, while automatically logging any issues with the optimization
process. Finally, for successfully optimized models, OODTE compares the
results, and, if any accuracy deviations are observed, it iteratively repeats
the process for each pass of the ONNX Optimizer, to localize the root cause of
the differences observed. Using OODTE, we sourced well-known $130$ models from
the official ONNX Model Hub, used for a wide variety of tasks (classification,
object detection, semantic segmentation, text summarization, question and
answering, sentiment analysis) from the official ONNX model hub. We detected 15
issues, 14 of which were previously unknown, associated with optimizer crashes
and accuracy deviations. We also observed $9.2$% of all model instances
presenting issues leading into the crash of the optimizer, or the generation of
an invalid model while using the primary optimizer strategies. In addition,
$30$% of the classification models presented accuracy differences across the
original and the optimized model variants, while $16.6$% of semantic
segmentation and object detection models are also affected, at least to a
limited extent.

</details>


### [87] [From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup](https://arxiv.org/abs/2505.01902)
*Ali Al-Bustami,Zaid Ghazal*

Main category: cs.LG

TL;DR: 本文提出了一个机器学习框架，结合球员个人表现和团队历史数据，用于预测世界杯比赛结果，并展示了其在2022年世界杯上的高准确性。


<details>
  <summary>Details</summary>
Motivation: 世界杯比赛结果的准确预测对分析师、教练、赌客和球迷具有重要价值，现有方法往往忽略了球员个体与团队的交互作用。

Method: 通过整合多年历史数据和球员具体表现指标（如进球、助攻、传球准确率等），构建年度团队档案，并采用分类技术、降维和超参数优化。

Result: 在2022年世界杯数据上的实验表明，该方法优于传统基线模型，验证了球员属性和团队构成的重要性。

Conclusion: 研究强调了球员中心数据在体育分析中的潜力，并为未来探索图神经网络等高级架构奠定了基础。

Abstract: Accurate prediction of FIFA World Cup match outcomes holds significant value
for analysts, coaches, bettors, and fans. This paper presents a machine
learning framework specifically designed to forecast match winners in FIFA
World Cup. By integrating both team-level historical data and player-specific
performance metrics such as goals, assists, passing accuracy, and tackles, we
capture nuanced interactions often overlooked by traditional aggregate models.
Our methodology processes multi-year data to create year-specific team profiles
that account for evolving rosters and player development. We employ
classification techniques complemented by dimensionality reduction and
hyperparameter optimization, to yield robust predictive models. Experimental
results on data from the FIFA 2022 World Cup demonstrate our approach's
superior accuracy compared to baseline method. Our findings highlight the
importance of incorporating individual player attributes and team-level
composition to enhance predictive performance, offering new insights into
player synergy, strategic match-ups, and tournament progression scenarios. This
work underscores the transformative potential of rich, player-centric data in
sports analytics, setting a foundation for future exploration of advanced
learning architectures such as graph neural networks to model complex team
interactions.

</details>


### [88] [LookAlike: Consistent Distractor Generation in Math MCQs](https://arxiv.org/abs/2505.01903)
*Nisarg Parikh,Nigel Fernandez,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.LG

TL;DR: LookAlike方法通过偏好优化提升数学选择题干扰项的一致性，利用模型不一致性作为负样本，结合有监督微调和直接偏好优化，在1400+数学选择题数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型生成的干扰项难以保证与常见学生错误一致，限制了其在数学选择题中的应用效果。

Method: 提出LookAlike方法：1) 从模型不一致性中挖掘合成偏好对；2) 交替使用有监督微调(SFT)和直接偏好优化(DPO)稳定训练。

Result: 在LLM-as-a-judge评估中，干扰项生成准确率51.6%，错误生成准确率57.2%，超越现有最优方法(45.6%/47.7%)。

Conclusion: 偏好优化和一致性挖掘能有效提升数学干扰项生成的规模化和一致性。

Abstract: Large language models (LLMs) are increasingly used to generate distractors
for multiple-choice questions (MCQs), especially in domains like math
education. However, existing approaches are limited in ensuring that the
generated distractors are consistent with common student errors. We propose
LookAlike, a method that improves error-distractor consistency via preference
optimization. Our two main innovations are: (a) mining synthetic preference
pairs from model inconsistencies, and (b) alternating supervised fine-tuning
(SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike
prior work that relies on heuristics or manually annotated preference data,
LookAlike uses its own generation inconsistencies as dispreferred samples, thus
enabling scalable and stable training. Evaluated on a real-world dataset of
1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation
and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an
existing state-of-the-art method (45.6% / 47.7%). These improvements highlight
the effectiveness of preference-based regularization and inconsistency mining
for generating consistent math MCQ distractors at scale.

</details>


### [89] [BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models](https://arxiv.org/abs/2505.01912)
*Evan R. Antoniuk,Shehtab Zaman,Tal Ben-Nun,Peggy Li,James Diffenderfer,Busra Demirci,Obadiah Smolenski,Tim Hsu,Anna M. Hiszpanski,Kenneth Chiu,Bhavya Kailkhura,Brian Van Essen*

Main category: cs.LG

TL;DR: 该论文提出了BOOM基准，用于评估分子性质预测模型的分布外（OOD）性能，发现当前模型在OOD任务上表现不佳，平均误差是分布内误差的3倍。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在分子发现任务中难以泛化到分布外数据，且缺乏系统性基准来评估OOD性能。

Method: 通过评估140多种模型和任务组合，分析数据生成、预训练、超参数优化等因素对OOD性能的影响。

Result: 现有模型在OOD任务上表现普遍较差，即使最佳模型的OOD误差也是分布内误差的3倍。

Conclusion: 开发具有强OOD泛化能力的模型是化学机器学习的新挑战，并开源了BOOM基准以促进研究。

Abstract: Advances in deep learning and generative modeling have driven interest in
data-driven molecule discovery pipelines, whereby machine learning (ML) models
are used to filter and design novel molecules without requiring prohibitively
expensive first-principles simulations. Although the discovery of novel
molecules that extend the boundaries of known chemistry requires accurate
out-of-distribution (OOD) predictions, ML models often struggle to generalize
OOD. Furthermore, there are currently no systematic benchmarks for molecular
OOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for
$\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecular
property predictions -- a benchmark study of property-based out-of-distribution
models for common molecular property prediction models. We evaluate more than
140 combinations of models and property prediction tasks to benchmark deep
learning models on their OOD performance. Overall, we do not find any existing
models that achieve strong OOD generalization across all tasks: even the top
performing model exhibited an average OOD error 3x larger than in-distribution.
We find that deep learning models with high inductive bias can perform well on
OOD tasks with simple, specific properties. Although chemical foundation models
with transfer and in-context learning offer a promising solution for limited
training data scenarios, we find that current foundation models do not show
strong OOD extrapolation capabilities. We perform extensive ablation
experiments to highlight how OOD performance is impacted by data generation,
pre-training, hyperparameter optimization, model architecture, and molecular
representation. We propose that developing ML models with strong OOD
generalization is a new frontier challenge in chemical ML model development.
This open-source benchmark will be made available on Github.

</details>


### [90] [Unemployment Dynamics Forecasting with Machine Learning Regression Models](https://arxiv.org/abs/2505.01933)
*Kyungsu Kim*

Main category: cs.LG

TL;DR: 该论文探讨了多种回归和机器学习技术在美国月度失业数据预测中的应用，比较了七种模型的性能，发现树集成和LSTM表现最佳。


<details>
  <summary>Details</summary>
Motivation: 旨在通过现代机器学习技术提升实时失业预测的准确性，为经济学家和决策者提供更深入的劳动力市场趋势洞察。

Method: 使用线性回归、SGDRegressor、随机森林、XGBoost、CatBoost、SVR和LSTM七种模型，通过交叉验证调整超参数，评估预测性能。

Result: 树集成（尤其是CatBoost）和LSTM表现优于线性方法，SVR和SGDRegressor改进有限；就业机会和消费者情绪是最关键预测因素。

Conclusion: 现代机器学习技术（尤其是树集成和深度学习）能显著提升失业预测效果，为政策制定提供更丰富的数据支持。

Abstract: In this paper, I explored how a range of regression and machine learning
techniques can be applied to monthly U.S. unemployment data to produce timely
forecasts. I compared seven models: Linear Regression, SGDRegressor, Random
Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network,
training each on a historical span of data and then evaluating on a later
hold-out period. Input features include macro indicators (GDP growth, CPI),
labor market measures (job openings, initial claims), financial variables
(interest rates, equity indices), and consumer sentiment.
  I tuned model hyperparameters via cross-validation and assessed performance
with standard error metrics and the ability to predict the correct unemployment
direction. Across the board, tree-based ensembles (and CatBoost in particular)
deliver noticeably better forecasts than simple linear approaches, while the
LSTM captures underlying temporal patterns more effectively than other
nonlinear methods. SVR and SGDRegressor yield modest gains over standard
regression but don't match the consistency of the ensemble and deep-learning
models.
  Interpretability tools ,feature importance rankings and SHAP values, point to
job openings and consumer sentiment as the most influential predictors across
all methods. By directly comparing linear, ensemble, and deep-learning
approaches on the same dataset, our study shows how modern machine-learning
techniques can enhance real-time unemployment forecasting, offering economists
and policymakers richer insights into labor market trends.
  In the comparative evaluation of the models, I employed a dataset comprising
thirty distinct features over the period from January 2020 through December
2024.

</details>


### [91] [Multi-Scale Graph Learning for Anti-Sparse Downscaling](https://arxiv.org/abs/2505.01948)
*Yingda Fan,Runlong Yu,Janet R. Barclay,Alison P. Appling,Yiming Sun,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 论文提出了一种多尺度图学习方法（MSGL），通过跨尺度插值学习和异步训练提升细尺度水温预测能力。


<details>
  <summary>Details</summary>
Motivation: 由于细尺度水温数据不足，传统方法难以在小尺度（≤1 km）上准确预测河流水温，影响水质和水生栖息地保护。

Method: MSGL采用多任务学习框架，包括粗尺度图学习和跨尺度插值学习；还提出异步多尺度图学习（ASYNC-MSGL）以解决同步训练限制。

Result: 在特拉华河流域的实验中，该方法在稀疏数据下的水温降尺度预测中表现优异，达到最先进水平。

Conclusion: MSGL通过跨尺度连接和异步训练显著提升了细尺度水温预测性能，对水资源管理具有实用价值。

Abstract: Water temperature can vary substantially even across short distances within
the same sub-watershed. Accurate prediction of stream water temperature at fine
spatial resolutions (i.e., fine scales, $\leq$ 1 km) enables precise
interventions to maintain water quality and protect aquatic habitats. Although
spatiotemporal models have made substantial progress in spatially coarse time
series modeling, challenges persist in predicting at fine spatial scales due to
the lack of data at that scale.To address the problem of insufficient
fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This
method employs a multi-task learning framework where coarse-scale graph
learning, bolstered by larger datasets, simultaneously enhances fine-scale
graph learning. Although existing multi-scale or multi-resolution methods
integrate data from different spatial scales, they often overlook the spatial
correspondences across graph structures at various scales. To address this, our
MSGL introduces an additional learning task, cross-scale interpolation
learning, which leverages the hydrological connectedness of stream locations
across coarse- and fine-scale graphs to establish cross-scale connections,
thereby enhancing overall model performance. Furthermore, we have broken free
from the mindset that multi-scale learning is limited to synchronous training
by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL).
Extensive experiments demonstrate the state-of-the-art performance of our
method for anti-sparse downscaling of daily stream temperatures in the Delaware
River Basin, USA, highlighting its potential utility for water resources
monitoring and management.

</details>


### [92] [Semantic Probabilistic Control of Language Models](https://arxiv.org/abs/2505.01954)
*Kareem Ahmed,Catarina G Belem,Padhraic Smyth,Sameer Singh*

Main category: cs.LG

TL;DR: 提出一种利用验证器梯度信息高效引导语言模型生成的方法，能在不降低质量的情况下满足非词汇约束（如毒性、情感等）。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以高效处理语言模型的非词汇约束（如毒性、情感），需要通过采样探索条件分布，但对低概率事件估计效果差。

Method: 利用验证器的梯度信息重构下一个词分布，通过局部语言模型分布近似计算期望句子嵌入，从而高效满足目标约束。

Result: 在毒性、情感和主题一致性控制任务中，生成结果满足约束的概率超过95%，且未降低生成质量。

Conclusion: 该方法通过梯度信息有效引导语言模型生成，为满足复杂语义约束提供了新思路。

Abstract: Semantic control entails steering LM generations towards satisfying subtle
non-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes
that can be captured by a sequence-level verifier. It can thus be viewed as
sampling from the LM distribution conditioned on the target attribute, a
computationally intractable problem due to the non-decomposable nature of the
verifier. Existing approaches to LM control either only deal with syntactic
constraints which cannot capture the aforementioned attributes, or rely on
sampling to explore the conditional LM distribution, an ineffective estimator
for low-probability events. In this work, we leverage a verifier's gradient
information to efficiently reason over all generations that satisfy the target
attribute, enabling precise steering of LM generations by reweighing the
next-token distribution. Starting from an initial sample, we create a local LM
distribution favoring semantically similar sentences. This approximation
enables the tractable computation of an expected sentence embedding. We use
this expected embedding, informed by the verifier's evaluation at the initial
sample, to estimate the probability of satisfying the constraint, which
directly informs the update to the next-token distribution. We evaluated the
effectiveness of our approach in controlling the toxicity, sentiment, and
topic-adherence of LMs yielding generations satisfying the constraint with high
probability (>95%) without degrading their quality.

</details>


### [93] [EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting](https://arxiv.org/abs/2505.01959)
*Leyi Yan,Linda Wang,Sihang Liu,Yi Ding*

Main category: cs.LG

TL;DR: EnsembleCI是一种基于集成学习的自适应碳强度预测方法，相比现有方法CarbonCast，它在11个区域电网中表现更优，平均预测精度提升19.58%，并具有更好的区域适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有碳强度预测方法CarbonCast无法有效处理区域差异和缺乏适应性，因此需要一种更灵活、适应性更强的方法来改进预测精度。

Method: 提出EnsembleCI，一种端到端的集成学习方法，通过加权多个子学习器的预测结果，提升灵活性和区域适应性。

Result: 在11个区域电网的评估中，EnsembleCI在几乎所有电网中取得最低的平均绝对百分比误差（MAPE），平均精度提升19.58%，并表现出更好的长期预测鲁棒性。

Conclusion: EnsembleCI是一种更准确、可靠的碳强度预测解决方案，具有区域特异性和实用性，其源代码和数据已开源。

Abstract: Carbon intensity (CI) measures the average carbon emissions generated per
unit of electricity, making it a crucial metric for quantifying and managing
the environmental impact. Accurate CI predictions are vital for minimizing
carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due
to its inability to address regional variability and lack of adaptability. To
address these limitations, we introduce EnsembleCI, an adaptive, end-to-end
ensemble learning-based approach for CI forecasting. EnsembleCI combines
weighted predictions from multiple sublearners, offering enhanced flexibility
and regional adaptability. In evaluations across 11 regional grids, EnsembleCI
consistently surpasses CarbonCast, achieving the lowest mean absolute
percentage error (MAPE) in almost all grids and improving prediction accuracy
by an average of 19.58%. While performance still varies across grids due to
inherent regional diversity, EnsembleCI reduces variability and exhibits
greater robustness in long-term forecasting compared to CarbonCast and
identifies region-specific key features, underscoring its interpretability and
practical relevance. These findings position EnsembleCI as a more accurate and
reliable solution for CI forecasting. EnsembleCI source code and data used in
this paper are available at https://github.com/emmayly/EnsembleCI.

</details>


### [94] [D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection](https://arxiv.org/abs/2505.01979)
*Chenran Zhao,Dianxi Shi,Mengzhu Wang,Jianqiang Xia,Huanhuan Yang,Songchang Jin,Shaowu Yang,Chunping Qiu*

Main category: cs.LG

TL;DR: 提出了因果分层强化学习方法D3HRL，通过建模延迟效应为跨时间因果关系，并利用条件独立性检验消除虚假相关性，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决分层强化学习中延迟效应和虚假相关性两大挑战。

Method: D3HRL通过分布式因果发现建模延迟效应，基于条件独立性检验消除虚假相关性，并构建分层策略。

Result: 在2D-MineCraft和MiniGrid实验中，D3HRL表现出对延迟效应的高敏感性和准确识别因果关系的能力。

Conclusion: D3HRL为复杂环境中的可靠决策提供了一种有效的因果分层强化学习方法。

Abstract: Current Hierarchical Reinforcement Learning (HRL) algorithms excel in
long-horizon sequential decision-making tasks but still face two challenges:
delay effects and spurious correlations. To address them, we propose a causal
HRL approach called D3HRL. First, D3HRL models delayed effects as causal
relationships across different time spans and employs distributed causal
discovery to learn these relationships. Second, it employs conditional
independence testing to eliminate spurious correlations. Finally, D3HRL
constructs and trains hierarchical policies based on the identified true causal
relationships. These three steps are iteratively executed, gradually exploring
the complete causal chain of the task. Experiments conducted in 2D-MineCraft
and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects
and accurately identifies causal relationships, leading to reliable
decision-making in complex environments.

</details>


### [95] [Always Skip Attention](https://arxiv.org/abs/2505.01996)
*Yiping Ji,Hemanth Saratchandran,Peyman Moghaddam,Simon Lucey*

Main category: cs.LG

TL;DR: 现代视觉Transformer（ViT）中的自注意力机制若没有跳跃连接会训练失败，而其他部分仍能工作。本文从理论上证明自注意力机制本质上是条件不良的，依赖跳跃连接进行正则化，并提出Token Graying方法进一步改善输入条件。


<details>
  <summary>Details</summary>
Motivation: 探究ViT中自注意力机制对跳跃连接的独特依赖现象，并解释其理论原因，从而提出改进方法。

Method: 通过理论分析证明自注意力机制的条件不良特性，并提出Token Graying作为补充方法。在监督和自监督训练中验证。

Result: Token Graying有效改善了自注意力机制的条件，进一步提升了模型性能。

Conclusion: 自注意力机制对跳跃连接的依赖是根本性的，而Token Graying为ViT提供了额外的正则化手段。

Abstract: We highlight a curious empirical result within modern Vision Transformers
(ViTs). Specifically, self-attention catastrophically fails to train unless it
is used in conjunction with a skip connection. This is in contrast to other
elements of a ViT that continue to exhibit good performance (albeit suboptimal)
when skip connections are removed. Further, we show that this critical
dependence on skip connections is a relatively new phenomenon, with previous
deep architectures (\eg, CNNs) exhibiting good performance in their absence. In
this paper, we theoretically characterize that the self-attention mechanism is
fundamentally ill-conditioned and is, therefore, uniquely dependent on skip
connections for regularization. Additionally, we propose Token Graying -- a
simple yet effective complement (to skip connections) that further improves the
condition of input tokens. We validate our approach in both supervised and
self-supervised training methods.

</details>


### [96] [Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach](https://arxiv.org/abs/2505.01997)
*Jiancong Xiao,Bojian Hou,Zhanliang Wang,Ruochen Jin,Qi Long,Weijie J. Su,Li Shen*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型（LLMs）偏好对齐导致的校准问题，提出通过领域特定知识微调解决过度自信问题，并开发了校准感知微调和基于EM算法的ECE正则化方法以保持低校准误差。


<details>
  <summary>Details</summary>
Motivation: 偏好对齐让LLMs性能提升，但导致校准问题，表现为过度自信和校准不佳。作者旨在探究其原因并找到解决方案。

Method: 1. 分析偏好对齐影响校准的原因；2. 提出领域知识微调缓解过度自信；3. 划分模型为可校准/不可校准两类；4. 针对可校准模型提出校准感知微调；5. 对不可校准模型使用基于EM算法的ECE正则化方法。

Result: 实验证明领域知识微调有效缓解过度自信，校准感知和ECE正则化方法在各自适用范围内显著降低校准误差且不影响模型性能。

Conclusion: 偏好对齐与校准问题的关系被明确，提出的方法在不同校准场景中均有效且实用。

Abstract: One of the key technologies for the success of Large Language Models (LLMs)
is preference alignment. However, a notable side effect of preference alignment
is poor calibration: while the pre-trained models are typically
well-calibrated, LLMs tend to become poorly calibrated after alignment with
human preferences. In this paper, we investigate why preference alignment
affects calibration and how to address this issue. For the first question, we
observe that the preference collapse issue in alignment undesirably generalizes
to the calibration scenario, causing LLMs to exhibit overconfidence and poor
calibration. To address this, we demonstrate the importance of fine-tuning with
domain-specific knowledge to alleviate the overconfidence issue. To further
analyze whether this affects the model's performance, we categorize models into
two regimes: calibratable and non-calibratable, defined by bounds of Expected
Calibration Error (ECE). In the calibratable regime, we propose a
calibration-aware fine-tuning approach to achieve proper calibration without
compromising LLMs' performance. However, as models are further fine-tuned for
better performance, they enter the non-calibratable regime. For this case, we
develop an EM-algorithm-based ECE regularization for the fine-tuning loss to
maintain low calibration error. Extensive experiments validate the
effectiveness of the proposed methods.

</details>


### [97] [CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting](https://arxiv.org/abs/2505.02011)
*Minhyuk Lee,HyeKyung Yoon,MyungJoo Kang*

Main category: cs.LG

TL;DR: 文章提出了一种基于CNN自编码器的分数注意力机制（CASA），通过减少内存使用提高多样Transformer模型的性能，实验证明其在计算资源、推理速度和性能上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多变量长期时间序列预测方法在时间复杂性、计算资源和跨维度交互上存在不足，需要一种更高效的解决方案。

Method: 引入CASA机制，可灵活应用于多种Transformer模型，通过优化内存使用提升性能。

Result: 在8个真实数据集上，CASA减少计算资源达77.7%，推理速度提升44.0%，并在87.5%的指标中达到最优性能。

Conclusion: CASA在计算效率和预测性能上均表现出色，为时间序列预测领域提供了有效的新方法。

Abstract: Multivariate long-term time series forecasting is critical for applications
such as weather prediction, and traffic analysis. In addition, the
implementation of Transformer variants has improved prediction accuracy.
Following these variants, different input data process approaches also enhanced
the field, such as tokenization techniques including point-wise, channel-wise,
and patch-wise tokenization. However, previous studies still have limitations
in time complexity, computational resources, and cross-dimensional
interactions. To address these limitations, we introduce a novel CNN
Autoencoder-based Score Attention mechanism (CASA), which can be introduced in
diverse Transformers model-agnosticically by reducing memory and leading to
improvement in model performance. Experiments on eight real-world datasets
validate that CASA decreases computational resources by up to 77.7%,
accelerates inference by 44.0%, and achieves state-of-the-art performance,
ranking first in 87.5% of evaluated metrics.

</details>


### [98] [Wide & Deep Learning for Node Classification](https://arxiv.org/abs/2505.02020)
*Yancheng Chen,Wenguo Yang,Zhipeng Jiang*

Main category: cs.LG

TL;DR: 论文提出GCNIII框架，结合Wide & Deep架构与三项技术（Intersect memory、Initial residual、Identity mapping），在半监督和全监督任务中更有效平衡过拟合与泛化问题，并探索用大语言模型优化节点特征。


<details>
  <summary>Details</summary>
Motivation: 针对图卷积网络（GCNs）在节点分类任务中因异质性和表达能力不足而忽视节点特征潜力的问题，提出改进框架以结合记忆与泛化能力。

Method: 基于Wide & Deep架构，引入Intersect memory、Initial residual和Identity mapping技术，并利用大语言模型优化节点特征工程。

Result: 实验表明GCNIII在半监督和全监督任务中能更好平衡过拟合与泛化，且在跨领域节点分类中性能提升。

Conclusion: GCNIII通过融合记忆与泛化机制及特征增强技术，为图神经网络提供了更灵活的解决方案，并在多任务中验证了有效性。

Abstract: Wide & Deep, a simple yet effective learning architecture for recommendation
systems developed by Google, has had a significant impact in both academia and
industry due to its combination of the memorization ability of generalized
linear models and the generalization ability of deep models. Graph
convolutional networks (GCNs) remain dominant in node classification tasks;
however, recent studies have highlighted issues such as heterophily and
expressiveness, which focus on graph structure while seemingly neglecting the
potential role of node features. In this paper, we propose a flexible framework
GCNIII, which leverages the Wide & Deep architecture and incorporates three
techniques: Intersect memory, Initial residual and Identity mapping. We provide
comprehensive empirical evidence showing that GCNIII can more effectively
balance the trade-off between over-fitting and over-generalization on various
semi- and full- supervised tasks. Additionally, we explore the use of large
language models (LLMs) for node feature engineering to enhance the performance
of GCNIII in cross-domain node classification tasks. Our implementation is
available at https://github.com/CYCUCAS/GCNIII.

</details>


### [99] [NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks](https://arxiv.org/abs/2505.02022)
*Yiming Zhang,Koji Tsuda*

Main category: cs.LG

TL;DR: NbBench是首个全面的纳米抗体表示学习基准套件，覆盖8个生物相关任务，评估11种模型，发现抗体语言模型在抗原任务中表现优异，但所有模型在热稳定性和亲和力任务中仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 纳米抗体因其独特优势在治疗和诊断中有重要价值，但纳米抗体特定模型研究不足且缺乏统一基准，因此需要开发NbBench填补这一空白。

Method: 引入NbBench基准套件，涵盖8个任务和9个数据集，系统评估11种模型（通用蛋白LM、抗体LM、纳米抗体LM），采用冻结设置。

Result: 抗体语言模型在抗原任务中表现优异，但所有模型在回归任务（如热稳定性和亲和力）中表现欠佳，且无单一模型在所有任务中 consistently 最优。

Conclusion: NbBench通过标准化数据集和评估协议，为纳米抗体建模提供了可复现的基础，未来需进一步优化模型性能。

Abstract: Nanobodies, single-domain antibody fragments derived from camelid
heavy-chain-only antibodies, exhibit unique advantages such as compact size,
high stability, and strong binding affinity, making them valuable tools in
therapeutics and diagnostics. While recent advances in pretrained protein and
antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular
understanding, nanobody-specific modeling remains underexplored and lacks a
unified benchmark. To address this gap, we introduce NbBench, the first
comprehensive benchmark suite for nanobody representation learning. Spanning
eight biologically meaningful tasks across nine curated datasets, NbBench
encompasses structure annotation, binding prediction, and developability
assessment. We systematically evaluate eleven representative models--including
general-purpose protein LMs, antibody-specific LMs, and nanobody-specific
LMs--in a frozen setting. Our analysis reveals that antibody language models
excel in antigen-related tasks, while performance on regression tasks such as
thermostability and affinity remains challenging across all models. Notably, no
single model consistently outperforms others across all tasks. By standardizing
datasets, task definitions, and evaluation protocols, NbBench offers a
reproducible foundation for assessing and advancing nanobody modeling.

</details>


### [100] [GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning](https://arxiv.org/abs/2505.02027)
*Rui Lv,Zaixi Zhang,Kai Zhang,Qi Liu,Weibo Gao,Jiawei Liu,Jiaxia Yan,Linan Yue,Fangzhou Yao*

Main category: cs.LG

TL;DR: 论文提出了GraphPrompter,一种多阶段自适应提示优化方法,旨在提升图模型的上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图上下文学习方法随机选择子图或边作为提示,导致噪声和性能下降。同时,测试图类别数远多于训练图时,性能也会显著恶化。

Method: GraphPrompter包括提示生成器、选择器和增强器三阶段:生成器通过重构层突出信息边,选择器用k近邻和预训练层动态选择样本,增强器通过缓存策略提升泛化能力。

Result: 实验表明GraphPrompter平均优于现有baseline超过8%。

Conclusion: GraphPrompter通过优化提示生成、选择和使用的全过程,显著提升了图上下文学习能力。

Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models
to novel and diverse downstream graphs without updating any parameters, has
gained much attention in the community. The key to graph in-context learning is
to perform downstream graphs conditioned on chosen prompt examples. Existing
methods randomly select subgraphs or edges as prompts, leading to noisy graph
prompts and inferior model performance. Additionally, due to the gap between
pre-training and testing graphs, when the number of classes in the testing
graphs is much greater than that in the training, the in-context learning
ability will also significantly deteriorate. To tackle the aforementioned
challenges, we develop a multi-stage adaptive prompt optimization method
GraphPrompter, which optimizes the entire process of generating, selecting, and
using graph prompts for better in-context learning capabilities. Firstly,
Prompt Generator introduces a reconstruction layer to highlight the most
informative edges and reduce irrelevant noise for graph prompt construction.
Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest
neighbors algorithm and pre-trained selection layers to dynamically choose
appropriate samples and minimize the influence of irrelevant prompts. Finally,
we leverage a Prompt Augmenter with a cache replacement strategy to enhance the
generalization capability of the pre-trained model on new datasets. Extensive
experiments show that GraphPrompter effectively enhances the in-context
learning ability of graph models. On average across all the settings, our
approach surpasses the state-of-the-art baselines by over 8%. Our code is
released at https://github.com/karin0018/GraphPrompter.

</details>


### [101] [Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles](https://arxiv.org/abs/2505.02033)
*Emine Akpinar,Batuhan Hangun,Murat Oduncuoglu,Oguz Altun,Onder Eyecioglu,Zeynel Yalcin*

Main category: cs.LG

TL;DR: 研究者提出了基于量子计算的Deep VQC模型，通过高维基因特征数据成功分类脑肿瘤，性能优于或媲美传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统AI方法在分析高维基因数据时的局限性，如计算成本高和超参数调优困难。

Method: 利用变分量子分类器（VQC）方法，构建Deep VQC模型，处理54,676个基因特征的微阵列数据。

Result: 模型准确分类四种脑肿瘤及健康样本，性能优于或接近传统机器学习算法。

Conclusion: 量子AI方法在复杂基因数据分类中展现出高效和潜力，是未来研究的有力工具。

Abstract: DNA microarray technology enables the simultaneous measurement of expression
levels of thousands of genes, thereby facilitating the understanding of the
molecular mechanisms underlying complex diseases such as brain tumors and the
identification of diagnostic genetic signatures. To derive meaningful
biological insights from the high-dimensional and complex gene features
obtained through this technology and to analyze gene properties in detail,
classical AI-based approaches such as machine learning and deep learning are
widely employed. However, these methods face various limitations in managing
high-dimensional vector spaces and modeling the intricate relationships among
genes. In particular, challenges such as hyperparameter tuning, computational
costs, and high processing power requirements can hinder their efficiency. To
overcome these limitations, quantum computing and quantum AI approaches are
gaining increasing attention. Leveraging quantum properties such as
superposition and entanglement, quantum methods enable more efficient parallel
processing of high-dimensional data and offer faster and more effective
solutions to problems that are computationally demanding for classical methods.
In this study, a novel model called "Deep VQC" is proposed, based on the
Variational Quantum Classifier approach. Developed using microarray data
containing 54,676 gene features, the model successfully classified four
different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and
pilocytic astrocytoma-alongside healthy samples with high accuracy.
Furthermore, compared to classical ML algorithms, our model demonstrated either
superior or comparable classification performance. These results highlight the
potential of quantum AI methods as an effective and promising approach for the
analysis and classification of complex structures such as brain tumors based on
gene expression features.

</details>


### [102] [Secrets of GFlowNets' Learning Behavior: A Theoretical Study](https://arxiv.org/abs/2505.02035)
*Tianshu Yu*

Main category: cs.LG

TL;DR: 论文对生成流网络（GFlowNets）的学习行为进行了理论分析，重点关注其收敛性、样本复杂性、隐式正则化和鲁棒性四个维度，以揭示其学习机制并指导实际应用。


<details>
  <summary>Details</summary>
Motivation: 尽管GFlowNets在生成复合结构方面表现出巨大潜力，但其学习行为的理论理解仍不完善。本研究旨在填补这一空白，为GFlowNets的设计和部署提供理论基础。

Method: 通过理论分析，研究了GFlowNets在收敛性、样本复杂性、隐式正则化和鲁棒性四个关键维度的表现。

Result: 研究揭示了GFlowNets的学习机制及其性能影响因素，并提出了有效设计和部署的指导原则。

Conclusion: 本研究不仅填补了GFlowNets理论研究的空白，还为其作为可靠、可解释的生成模型框架的发展奠定了基础，有望推动其在AI社区的广泛应用。

Abstract: Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for
generating composite structures, demonstrating considerable promise across
diverse applications. While substantial progress has been made in exploring
their modeling validity and connections to other generative frameworks, the
theoretical understanding of their learning behavior remains largely uncharted.
In this work, we present a rigorous theoretical investigation of GFlowNets'
learning behavior, focusing on four fundamental dimensions: convergence, sample
complexity, implicit regularization, and robustness. By analyzing these
aspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's
learning dynamics, shedding light on its strengths and limitations. Our
findings contribute to a deeper understanding of the factors influencing
GFlowNet performance and provide insights into principled guidelines for their
effective design and deployment. This study not only bridges a critical gap in
the theoretical landscape of GFlowNets but also lays the foundation for their
evolution as a reliable and interpretable framework for generative modeling.
Through this, we aspire to advance the theoretical frontiers of GFlowNets and
catalyze their broader adoption in the AI community.

</details>


### [103] [Neural Logistic Bandits](https://arxiv.org/abs/2505.02069)
*Seoungbin Bae,Dabeen Lee*

Main category: cs.LG

TL;DR: 该论文研究神经逻辑赌博机问题，提出一种新型Bernstein型不等式以降低维度依赖，并设计两种算法显著改善现有结果的遗憾上界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在神经网络的背景下，要么依赖于奖励分布的最小方差倒数$\kappa$，要么直接受特征维度$d$的影响，而$d$可能很大。论文旨在绕过这些限制，提出更高效的解决方案。

Method: 通过引入一种新型Bernstein型不等式，绕过对维度$d$的直接依赖，并基于此设计两种算法（NeuralLog-UCB-1和NeuralLog-UCB-2）。

Result: 理论分析和实验验证表明，两种算法的遗憾上界分别为$	ilde{O}(	ilde{d}\sqrt{\kappa T})$和$	ilde{O}(	ilde{d}\sqrt{T/\kappa})$，优于现有结果。

Conclusion: 论文通过新型不等式和算法设计，有效降低了维度和$\kappa$的依赖，并在实践中验证了理论优势。

Abstract: We study the problem of neural logistic bandits, where the main task is to
learn an unknown reward function within a logistic link function using a neural
network. Existing approaches either exhibit unfavorable dependencies on
$\kappa$, where $1/\kappa$ represents the minimum variance of reward
distributions, or suffer from direct dependence on the feature dimension $d$,
which can be huge in neural network-based settings. In this work, we introduce
a novel Bernstein-type inequality for self-normalized vector-valued martingales
that is designed to bypass a direct dependence on the ambient dimension. This
lets us deduce a regret upper bound that grows with the effective dimension
$\widetilde{d}$, not the feature dimension, while keeping a minimal dependence
on $\kappa$. Based on the concentration inequality, we propose two algorithms,
NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of
order $\widetilde{O}(\widetilde{d}\sqrt{\kappa T})$ and
$\widetilde{O}(\widetilde{d}\sqrt{T/\kappa})$, respectively, improving on the
existing results. Lastly, we report numerical results on both synthetic and
real datasets to validate our theoretical findings.

</details>


### [104] [Lightweight Defense Against Adversarial Attacks in Time Series Classification](https://arxiv.org/abs/2505.02073)
*Yi Han*

Main category: cs.LG

TL;DR: 该论文提出了五种基于数据增强的时间序列分类防御方法，其中最高计算资源的方法仅增加14.07%的计算量，并开发了两种组合方法，其中集成方法性能优于基于PGD的对抗训练且计算资源更少。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列分类的重要性提升，确保其模型对抗攻击的鲁棒性至关重要。现有对抗训练方法计算成本高，因此需要更高效且易于部署的防御方法。

Method: 论文开发了五种数据增强防御方法，并将其组合成两种方法（包括一种集成方法），显著降低了计算资源需求。

Result: 集成方法的防御性能优于基于PGD的对抗训练，且计算资源仅为后者的三分之一以下，同时提升了模型的泛化能力。

Conclusion: 这些方法推动了鲁棒时间序列分类的发展，并为未来大规模预训练模型与数据增强防御结合提供了方向。

Abstract: As time series classification (TSC) gains prominence, ensuring robust TSC
models against adversarial attacks is crucial. While adversarial defense is
well-studied in Computer Vision (CV), the TSC field has primarily relied on
adversarial training (AT), which is computationally expensive. In this paper,
five data augmentation-based defense methods tailored for time series are
developed, with the most computationally intensive method among them increasing
the computational resources by only 14.07% compared to the original TSC model.
Moreover, the deployment process for these methods is straightforward. By
leveraging these advantages of our methods, we create two combined methods. One
of these methods is an ensemble of all the proposed techniques, which not only
provides better defense performance than PGD-based AT but also enhances the
generalization ability of TSC models. Moreover, the computational resources
required for our ensemble are less than one-third of those required for
PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as
foundation models are increasingly explored for time series feature learning,
our work provides insights into integrating data augmentation-based adversarial
defense with large-scale pre-trained models in future research.

</details>


### [105] [Learning Local Causal World Models with State Space Models and Attention](https://arxiv.org/abs/2505.02074)
*Francesco Petri,Luigi Asprino,Aldo Gangemi*

Main category: cs.LG

TL;DR: 该论文研究了状态空间模型（SSM）在构建因果世界模型中的潜力，并证明其性能优于或等同于Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索因果理论与神经世界建模的结合，特别是评估SSM架构在因果发现中的潜力，以解决现有模型未能学习到环境因果表示的问题。

Method: 方法是通过实证比较SSM与Transformer在简单环境动态建模和因果模型学习中的表现。

Result: 结果表明，SSM在建模环境动态和学习因果模型方面性能优于或等同于Transformer。

Conclusion: 结论是SSM为结合因果意识的进一步实验奠定了基础，展现了其在增强因果理解方面的潜力。

Abstract: World modelling, i.e. building a representation of the rules that govern the
world so as to predict its evolution, is an essential ability for any agent
interacting with the physical world. Despite their impressive performance, many
solutions fail to learn a causal representation of the environment they are
trying to model, which would be necessary to gain a deep enough understanding
of the world to perform complex tasks. With this work, we aim to broaden the
research in the intersection of causality theory and neural world modelling by
assessing the potential for causal discovery of the State Space Model (SSM)
architecture, which has been shown to have several advantages over the
widespread Transformer. We show empirically that, compared to an equivalent
Transformer, a SSM can model the dynamics of a simple environment and learn a
causal model at the same time with equivalent or better performance, thus
paving the way for further experiments that lean into the strength of SSMs and
further enhance them with causal awareness.

</details>


### [106] [SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations](https://arxiv.org/abs/2505.02094)
*Runyi Yu,Yinhuai Wang,Qihan Zhao,Hok Wai Tsui,Jingbo Wang,Ping Tan,Qifeng Chen*

Main category: cs.LG

TL;DR: 论文提出了解决强化学习从交互演示（RLID）中演示噪声和覆盖范围限制问题的两种数据增强技术：缝合轨迹图（STG）和状态转移场（STF），并结合自适应轨迹采样（ATS）和历史编码机制，显著提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的数据收集方法虽然提供了有价值的交互演示，但常导致稀疏、不连贯和噪声较大的轨迹，无法覆盖技能变化和过渡的全部范围，因此需要一种更全面的解决方案。

Method: 通过STG发现演示技能间的潜在过渡，STF为演示邻域内的任意状态建立独特连接；结合ATS动态生成课程和历史编码机制来支持RLID。

Result: 实验表明，该方法在多任务中显著优于现有技术，尤其在收敛稳定性、泛化能力和恢复鲁棒性方面。

Conclusion: 通过数据增强和动态课程设计，可以有效解决RLID中的噪声和覆盖问题，实现超越参考演示的鲁棒技能学习。

Abstract: We address a fundamental challenge in Reinforcement Learning from Interaction
Demonstration (RLID): demonstration noise and coverage limitations. While
existing data collection approaches provide valuable interaction
demonstrations, they often yield sparse, disconnected, and noisy trajectories
that fail to capture the full spectrum of possible skill variations and
transitions. Our key insight is that despite noisy and sparse demonstrations,
there exist infinite physically feasible trajectories that naturally bridge
between demonstrated skills or emerge from their neighboring states, forming a
continuous space of possible skill variations and transitions. Building upon
this insight, we present two data augmentation techniques: a Stitched
Trajectory Graph (STG) that discovers potential transitions between
demonstration skills, and a State Transition Field (STF) that establishes
unique connections for arbitrary states within the demonstration neighborhood.
To enable effective RLID with augmented data, we develop an Adaptive Trajectory
Sampling (ATS) strategy for dynamic curriculum generation and a historical
encoding mechanism for memory-dependent skill learning. Our approach enables
robust skill acquisition that significantly generalizes beyond the reference
demonstrations. Extensive experiments across diverse interaction tasks
demonstrate substantial improvements over state-of-the-art methods in terms of
convergence stability, generalization capability, and recovery robustness.

</details>


### [107] [Deep Representation Learning for Electronic Design Automation](https://arxiv.org/abs/2505.02105)
*Pratik Shrestha,Saran Phatharodom,Alec Aversa,David Blankenship,Zhengfeng Wu,Ioannis Savidis*

Main category: cs.LG

TL;DR: 本文探讨了表示学习在EDA中的应用，通过自动提取复杂数据中的特征，提高了电路设计的效率、准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着电路复杂度的增加以及对PPA要求的提高，表示学习能够自动从图像、网格和图形等复杂数据中提取有意义的特征，以解决EDA中的挑战。

Method: 本文介绍了基于图像的方法、基于图的方法以及混合多模态解决方案等关键技术，并分析了它们在时序预测、可布线分析和自动布局等任务中的应用。

Result: 这些技术进步表明，表示学习在当前的集成电路设计流程中能够显著提升效率、准确性和可扩展性。

Conclusion: 表示学习在EDA中显示出巨大的潜力，能够有效应对复杂电路设计和严格PPA要求的挑战。

Abstract: Representation learning has become an effective technique utilized by
electronic design automation (EDA) algorithms, which leverage the natural
representation of workflow elements as images, grids, and graphs. By addressing
challenges related to the increasing complexity of circuits and stringent
power, performance, and area (PPA) requirements, representation learning
facilitates the automatic extraction of meaningful features from complex data
formats, including images, grids, and graphs. This paper examines the
application of representation learning in EDA, covering foundational concepts
and analyzing prior work and case studies on tasks that include timing
prediction, routability analysis, and automated placement. Key techniques,
including image-based methods, graph-based approaches, and hybrid multimodal
solutions, are presented to illustrate the improvements provided in routing,
timing, and parasitic prediction. The provided advancements demonstrate the
potential of representation learning to enhance efficiency, accuracy, and
scalability in current integrated circuit design flows.

</details>


### [108] [GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code](https://arxiv.org/abs/2505.02124)
*Samidha Verma,Arushi Goyal,Ananya Mathur,Ankit Anand,Sayan Ranu*

Main category: cs.LG

TL;DR: GRAIL是一种利用大型语言模型（LLMs）和自动提示调优生成程序来计算图编辑距离（GED）的新方法，解决了传统神经网络方法的数据需求、可解释性和泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络方法在GED计算中存在数据需求大、可解释性差和跨领域泛化能力不足的问题。

Method: GRAIL结合LLMs和自动提示调优，生成用于计算GED的程序，而非直接预测GED。

Result: 在七个数据集上的实验表明，GRAIL在预测质量和跨领域泛化能力上均优于现有最佳方法。

Conclusion: GRAIL通过程序生成范式，实现了更高的可解释性和无需监督的自主学习机制，为GED计算提供了新思路。

Abstract: Graph Edit Distance (GED) is a widely used metric for measuring similarity
between two graphs. Computing the optimal GED is NP-hard, leading to the
development of various neural and non-neural heuristics. While neural methods
have achieved improved approximation quality compared to non-neural approaches,
they face significant challenges: (1) They require large amounts of ground
truth data, which is itself NP-hard to compute. (2) They operate as black
boxes, offering limited interpretability. (3) They lack cross-domain
generalization, necessitating expensive retraining for each new dataset. We
address these limitations with GRAIL, introducing a paradigm shift in this
domain. Instead of training a neural model to predict GED, GRAIL employs a
novel combination of large language models (LLMs) and automated prompt tuning
to generate a program that is used to compute GED. This shift from predicting
GED to generating programs imparts various advantages, including end-to-end
interpretability and an autonomous self-evolutionary learning mechanism without
ground-truth supervision. Extensive experiments on seven datasets confirm that
GRAIL not only surpasses state-of-the-art GED approximation methods in
prediction quality but also achieves robust cross-domain generalization across
diverse graph distributions.

</details>


### [109] [Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation](https://arxiv.org/abs/2505.02138)
*Chenxi Liu,Hao Miao,Qianxiong Xu,Shaowen Zhou,Cheng Long,Yan Zhao,Ziyue Li,Rui Zhao*

Main category: cs.LG

TL;DR: TimeKD是一个高效的多变量时间序列预测框架，通过知识蒸馏和跨模态教师模型提升预测效率。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）进行时间序列预测存在推理效率低的问题，TimeKD旨在解决这一问题。

Method: 采用校准语言模型（CLMs）和特权知识蒸馏（PKD），结合减法交叉注意力（SCA）机制优化特征表示。

Result: 在真实数据上的实验表明，TimeKD在效果、效率和扩展性方面表现优秀。

Conclusion: TimeKD通过知识蒸馏和跨模态学习，显著提升了多变量时间序列预测的效率和效果。

Abstract: Multivariate time series forecasting (MTSF) endeavors to predict future
observations given historical data, playing a crucial role in time series data
management systems. With advancements in large language models (LLMs), recent
studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.
However, the deployment of LLMs often suffers from low efficiency during the
inference phase. To address this problem, we introduce TimeKD, an efficient
MTSF framework that leverages the calibrated language models and privileged
knowledge distillation. TimeKD aims to generate high-quality future
representations from the proposed cross-modality teacher model and cultivate an
effective student model. The cross-modality teacher model adopts calibrated
language models (CLMs) with ground truth prompts, motivated by the paradigm of
Learning Under Privileged Information (LUPI). In addition, we design a
subtractive cross attention (SCA) mechanism to refine these representations. To
cultivate an effective student model, we propose an innovative privileged
knowledge distillation (PKD) mechanism including correlation and feature
distillation. PKD enables the student to replicate the teacher's behavior while
minimizing their output discrepancy. Extensive experiments on real data offer
insight into the effectiveness, efficiency, and scalability of the proposed
TimeKD.

</details>


### [110] [DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units](https://arxiv.org/abs/2505.02206)
*Lei Mao,Yuanhe Tian,Yan Song*

Main category: cs.LG

TL;DR: 提出DNAZEN框架，通过多粒度学习基因序列表示，包括小聚合物和G-grams（多聚合物组合），并采用Transformer编码器和整体G-gram掩码训练，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将语言建模技术直接应用于基因序列，忽略了序列内部多粒度信息的组织方式，导致表示学习不充分。

Method: 通过无监督方法构建G-gram词汇表，设计Transformer-based的G-gram编码器，并采用整体G-gram掩码训练模型DNAZEN。

Result: 在基准数据集上的实验验证了DNAZEN在多下游任务中的有效性。

Conclusion: DNAZEN通过多粒度学习和新型掩码机制，显著提升了基因序列表示的质量和应用性能。

Abstract: Genome modeling conventionally treats gene sequence as a language, reflecting
its structured motifs and long-range dependencies analogous to linguistic units
and organization principles such as words and syntax. Recent studies utilize
advanced neural networks, ranging from convolutional and recurrent models to
Transformer-based models, to capture contextual information of gene sequence,
with the primary goal of obtaining effective gene sequence representations and
thus enhance the models' understanding of various running gene samples.
However, these approaches often directly apply language modeling techniques to
gene sequences and do not fully consider the intrinsic information organization
in them, where they do not consider how units at different granularities
contribute to representation. In this paper, we propose DNAZEN, an enhanced
genomic representation framework designed to learn from various granularities
in gene sequences, including small polymers and G-grams that are combinations
of several contiguous polymers. Specifically, we extract the G-grams from
large-scale genomic corpora through an unsupervised approach to construct the
G-gram vocabulary, which is used to provide G-grams in the learning process of
DNA sequences through dynamically matching from running gene samples. A
Transformer-based G-gram encoder is also proposed and the matched G-grams are
fed into it to compute their representations and integrated into the encoder
for basic unit (E4BU), which is responsible for encoding small units and
maintaining the learning and inference process. To further enhance the learning
process, we propose whole G-gram masking to train DNAZEN, where the model
largely favors the selection of each entire G-gram to mask rather than an
ordinary masking mechanism performed on basic units. Experiments on benchmark
datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.

</details>


### [111] [Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora](https://arxiv.org/abs/2505.02147)
*Prajwal Thapa,Mridul Sharma,Jinu Nyachhyon,Yagya Raj Pandeya*

Main category: cs.LG

TL;DR: 该研究利用深度学习和迁移学习技术，通过CNN对60种草本植物进行分类，构建了一个包含12,000张图像的鲁棒模型，最终DenseNet121表现最佳。


<details>
  <summary>Details</summary>
Motivation: 草本植物分类在生物多样性丰富的地区（如尼泊尔）具有重要意义，该研究旨在解决现有方法在识别草本植物方面的局限性。

Method: 研究采用了多种模型架构（如DenseNet121、ResNet50、VGG16等），结合数据增强和正则化技术，以减少过拟合并提升模型泛化能力。

Result: DenseNet121在所有测试模型中表现最佳，证明了其在草本植物分类任务中的高效性。

Conclusion: 该研究推动了草本植物分类技术的发展，有助于传统植物知识的保存和可持续利用。

Abstract: Herb classification presents a critical challenge in botanical research,
particularly in regions with rich biodiversity such as Nepal. This study
introduces a novel deep learning approach for classifying 60 different herb
species using Convolutional Neural Networks (CNNs) and transfer learning
techniques. Using a manually curated dataset of 12,000 herb images, we
developed a robust machine learning model that addresses existing limitations
in herb recognition methodologies. Our research employed multiple model
architectures, including DenseNet121, 50-layer Residual Network (ResNet50),
16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,
and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating
superior performance. Data augmentation and regularization techniques were
applied to mitigate overfitting and enhance the generalizability of the model.
This work advances herb classification techniques, preserving traditional
botanical knowledge and promoting sustainable herb utilization.

</details>


### [112] [Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning](https://arxiv.org/abs/2505.02181)
*Shengyu Duan,Marcos L. L. Sartori,Rishad Shafik,Alex Yakovlev,Emre Ozer*

Main category: cs.LG

TL;DR: 本文提出了一种基于时域的新型方法来加速和优化Tsetlin Machine（TM）中的popcount和argmax操作，通过可编程延迟线（PDLs）和仲裁器实现。FPGA设计流程确保了功能匹配，显著提升了异步TM的性能。


<details>
  <summary>Details</summary>
Motivation: Tsetlin Machine（TM）作为一种轻量级机器学习方法，其分类性能受限于popcount和比较操作的高计算成本。本文旨在通过时域实现优化这些操作。

Method: 提出时域实现方案，利用可编程延迟线（PDLs）和仲裁器来管理popcount和argmax操作，并设计了FPGA实现流程，解决延迟偏差问题。

Result: 在异步TM中实现了显著改进：延迟降低38%，动态功耗减少43.1%，资源利用率节省15%。

Conclusion: 时域popcount与异步架构天然兼容，为TM提供了高效的硬件加速方案。

Abstract: Population count (popcount) is a crucial operation for many low-complexity
machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising
new ML method, particularly well-suited for solving classification tasks. The
inference mechanism in TM consists of propositional logic-based structures
within each class, followed by a majority voting scheme, which makes the
classification decision. In TM, the voters are the outputs of Boolean clauses.
The voting mechanism comprises two operations: popcount for each class and
determining the class with the maximum vote by means of an argmax operation.
  While TMs offer a lightweight ML alternative, their performance is often
limited by the high computational cost of popcount and comparison required to
produce the argmax result. In this paper, we propose an innovative approach to
accelerate and optimize these operations by performing them in the time domain.
Our time-domain implementation uses programmable delay lines (PDLs) and
arbiters to efficiently manage these tasks through delay-based mechanisms. We
also present an FPGA design flow for practical implementation of the
time-domain popcount, addressing delay skew and ensuring that the behavior
matches that of the model's intended functionality. By leveraging the natural
compatibility of the proposed popcount with asynchronous architectures, we
demonstrate significant improvements in an asynchronous TM, including up to 38%
reduction in latency, 43.1% reduction in dynamic power, and 15% savings in
resource utilization, compared to synchronous TMs using adder-based popcount.

</details>


### [113] [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/abs/2505.02309)
*Sanjay Surendranath Girija,Shashank Kapoor,Lakshit Arora,Dipen Pradhan,Aman Raj,Ankit Shetgaonkar*

Main category: cs.LG

TL;DR: 这篇综述论文总结了压缩大型语言模型（LLMs）以在资源受限环境中高效推理的三种主要方法：知识蒸馏、模型量化和模型剪枝，并探讨了互补技术和未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs资源需求高，限制了其在移动和边缘设备上的部署，本文旨在综述压缩技术以实现高效推理。

Method: 分析了知识蒸馏、模型量化和模型剪枝三种主要方法，并讨论了混合专家和早期退出等互补技术。

Result: 论文总结了每种技术的原理、变体和成功应用案例，为资源受限环境下的LLM优化提供了全面指导。

Conclusion: 论文为研究者和实践者在边缘部署LLM提供了有价值的资源，并指出了未来研究方向。

Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial
intelligence (AI), but their substantial resource requirements limit their
deployment on mobile and edge devices. This survey paper provides a
comprehensive overview of techniques for compressing LLMs to enable efficient
inference in resource-constrained environments. We examine three primary
approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For
each technique, we discuss the underlying principles, present different
variants, and provide examples of successful applications. We also briefly
discuss complementary techniques such as mixture-of-experts and early-exit
strategies. Finally, we highlight promising future directions, aiming to
provide a valuable resource for both researchers and practitioners seeking to
optimize LLMs for edge deployment.

</details>


### [114] [Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL](https://arxiv.org/abs/2505.02391)
*Jiarui Yao,Yifan Hao,Hanning Zhang,Hanze Dong,Wei Xiong,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为GVM-RAFT的动态样本分配策略，旨在优化大语言模型中的Chain-of-thought推理训练过程，通过动态调整计算资源分配，显著提升了收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Chain-of-thought推理方法（如RAFT）在训练时采用统一的推理预算，未能考虑到不同提示的难度差异和收敛行为的不一致性，导致计算资源利用效率低下。

Method: 提出了GVM-RAFT方法，通过动态监测提示接受率和随机梯度范数，动态分配计算资源以最小化梯度方差，从而优化训练效率。

Result: 实验结果表明，GVM-RAFT在数学推理任务中比vanilla RAFT提速2-4倍，并显著提高了准确性。该方法也可推广至其他强化学习算法（如GRPO）。

Conclusion: GVM-RAFT通过动态样本分配策略有效解决了CoT训练中的计算资源优化问题，不仅提升了性能和效率，还具备通用性。

Abstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be
formalized as a latent variable problem, where the model needs to generate
intermediate reasoning steps. While prior approaches such as iterative
reward-ranked fine-tuning (RAFT) have relied on such formulations, they
typically apply uniform inference budgets across prompts, which fails to
account for variability in difficulty and convergence behavior. This work
identifies the main bottleneck in CoT training as inefficient stochastic
gradient estimation due to static sampling strategies. We propose GVM-RAFT, a
prompt-specific Dynamic Sample Allocation Strategy designed to minimize
stochastic gradient variance under a computational budget constraint. The
method dynamically allocates computational resources by monitoring prompt
acceptance rates and stochastic gradient norms, ensuring that the resulting
gradient variance is minimized. Our theoretical analysis shows that the
proposed dynamic sampling strategy leads to accelerated convergence guarantees
under suitable conditions. Experiments on mathematical reasoning show that
GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over
vanilla RAFT. The proposed dynamic sampling strategy is general and can be
incorporated into other reinforcement learning algorithms, such as GRPO,
leading to similar improvements in convergence and test accuracy. Our code is
available at https://github.com/RLHFlow/GVM.

</details>


### [115] [Exogenous Isomorphism for Counterfactual Identifiability](https://arxiv.org/abs/2505.02212)
*Yikang Chen,Dehui Du*

Main category: cs.LG

TL;DR: 该论文研究了在Pearl因果层次结构（PCH）框架下的$∼_{\mathcal{L}_3}$-可识别性，提出外源性同构和$∼_{\mathrm{EI}}$-可识别性，并探索其在两类特殊SCMs中的充分假设。


<details>
  <summary>Details</summary>
Motivation: 动机是简化因果问题，确保所有满足假设的结构因果模型（SCMs）能为所有因果问题提供一致答案。

Method: 方法包括引入外源性同构和提出$\sim_{\mathrm{EI}}$-可识别性，并研究其在Bijective SCMs（BSCMs）和Triangular Monotonic SCMs（TM-SCMs）中的实现假设。

Result: 结果统一并推广了现有理论，为实际应用提供了理论保证，并通过神经TM-SCMs验证了方法的有效性。

Conclusion: 结论表明$∼_{\mathcal{L}_3}$-可识别性及其衍生理论在反事实推理的实践中具有重要价值和理论意义。

Abstract: This paper investigates $\sim_{\mathcal{L}_3}$-identifiability, a form of
complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH)
framework, ensuring that all Structural Causal Models (SCMs) satisfying the
given assumptions provide consistent answers to all causal questions. To
simplify this problem, we introduce exogenous isomorphism and propose
$\sim_{\mathrm{EI}}$-identifiability, reflecting the strength of model
identifiability required for $\sim_{\mathcal{L}_3}$-identifiability. We explore
sufficient assumptions for achieving $\sim_{\mathrm{EI}}$-identifiability in
two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual
transport, and Triangular Monotonic SCMs (TM-SCMs), which extend
$\sim_{\mathcal{L}_2}$-identifiability. Our results unify and generalize
existing theories, providing theoretical guarantees for practical applications.
Finally, we leverage neural TM-SCMs to address the consistency problem in
counterfactual reasoning, with experiments validating both the effectiveness of
our method and the correctness of the theory.

</details>


### [116] [An Empirical Study of Qwen3 Quantization](https://arxiv.org/abs/2505.02214)
*Xingyu Zheng,Yuye Li,Haoran Chu,Yue Feng,Xudong Ma,Jie Luo,Jinyang Guo,Haotong Qin,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: 本文系统评估了开源大语言模型Qwen3在不同量化设置下的性能，揭示了在极低精度下性能显著下降的问题，为未来量化方法优化提供了方向。


<details>
  <summary>Details</summary>
Motivation: 随着Qwen3在多种任务中表现出色，如何在资源受限环境中高效部署成为研究热点，而低比特量化对Qwen3性能的影响尚不明确。

Method: 研究采用5种经典的后训练量化技术，对Qwen3进行1至8比特的量化，并在多个数据集上评估其效果。

Result: 结果表明，Qwen3在中等比特宽度下性能保持良好，但在极低精度下语言任务性能显著下降。

Conclusion: 研究发现极低精度量化仍是LLM压缩的挑战，未来需进一步研究以减少性能损失，提升模型实用性。

Abstract: The Qwen series has emerged as a leading family of open-source Large Language
Models (LLMs), demonstrating remarkable capabilities in natural language
understanding tasks. With the recent release of Qwen3, which exhibits superior
performance across diverse benchmarks, there is growing interest in deploying
these models efficiently in resource-constrained environments. Low-bit
quantization presents a promising solution, yet its impact on Qwen3's
performance remains underexplored. This study conducts a systematic evaluation
of Qwen3's robustness under various quantization settings, aiming to uncover
both opportunities and challenges in compressing this state-of-the-art model.
We rigorously assess 5 existing classic post-training quantization techniques
applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their
effectiveness across multiple datasets. Our findings reveal that while Qwen3
maintains competitive performance at moderate bit-widths, it experiences
notable degradation in linguistic tasks under ultra-low precision, underscoring
the persistent hurdles in LLM compression. These results emphasize the need for
further research to mitigate performance loss in extreme quantization
scenarios. We anticipate that this empirical analysis will provide actionable
insights for advancing quantization methods tailored to Qwen3 and future LLMs,
ultimately enhancing their practicality without compromising accuracy. Our
project is released on https://github.com/Efficient-ML/Qwen3-Quantization and
https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.

</details>


### [117] [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550)
*Krzysztof Ociepa,Łukasz Flis,Remigiusz Kinas,Krzysztof Wróbel,Adrian Gwoździej*

Main category: cs.LG

TL;DR: Bielik v3系列是针对波兰语优化的参数高效生成文本模型（1.5B和4.5B），通过创新设计（如APT4分词器、加权指令交叉熵损失和动态学习率）在多个基准测试中表现出色，性能接近更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 旨在为波兰语提供高效且性能优越的生成模型，解决资源受限场景下高质量AI模型的需求，尤其是在较少被代表的语言领域。

Method: 采用定制波兰语分词器APT4提升分词效率，使用加权指令交叉熵损失平衡学习，并引入动态调整的学习率；模型基于2920亿标记的精心筛选语料库训练。

Result: 4.5B模型性能与规模大2-3倍的模型相当，1.5B模型在超小规模下仍表现优异，在包括Open PL LLM Leaderboard等多个基准测试中领先。

Conclusion: 该研究为低资源语言的高效建模设立了新标杆，推动了高质量波兰语AI的普及，尤其适合资源受限的应用场景。

Abstract: We introduce Bielik v3, a series of parameter-efficient generative text
models (1.5B and 4.5B) optimized for Polish language processing. These models
demonstrate that smaller, well-optimized architectures can achieve performance
comparable to much larger counterparts while requiring substantially fewer
computational resources. Our approach incorporates several key innovations: a
custom Polish tokenizer (APT4) that significantly improves token efficiency,
Weighted Instruction Cross-Entropy Loss to balance learning across instruction
types, and Adaptive Learning Rate that dynamically adjusts based on training
progress. Trained on a meticulously curated corpus of 292 billion tokens
spanning 303 million documents, these models excel across multiple benchmarks,
including the Open PL LLM Leaderboard, Complex Polish Text Understanding
Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter
model achieves results competitive with models 2-3 times its size, while the
1.5B model delivers strong performance despite its extremely compact profile.
These advances establish new benchmarks for parameter-efficient language
modeling in less-represented languages, making high-quality Polish language AI
more accessible for resource-constrained applications.

</details>


### [118] [Practical Efficiency of Muon for Pretraining](https://arxiv.org/abs/2505.02222)
*Essential AI,:,Ishaan Shah,Anthony M. Polloreno,Karl Stratos,Philip Monk,Adarsh Chaluvaraju,Andrew Hojel,Andrew Ma,Anil Thomas,Ashish Tanwer,Darsh J Shah,Khoi Nguyen,Kurt Smith,Michael Callahan,Michael Pust,Mohit Parmar,Peter Rushton,Platon Mazarakis,Ritvik Kapila,Saurabh Srivastava,Somanshu Singla,Tim Romanski,Yash Vanjani,Ashish Vaswani*

Main category: cs.LG

TL;DR: Muon是一种二阶优化器，相比AdamW在计算时间和性能权衡上扩展了Pareto前沿，尤其在处理大批量数据时更高效且经济。结合muP参数化方法，提出了一个简单的算法来高效转移超参数并控制误差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决在大批量训练时保持数据效率的问题，同时优化计算资源的使用，从而降低训练成本。

Method: 提出了Muon优化器，结合muP参数化方法，并设计了一种简单的telescoping算法来处理误差。

Result: 实验表明，Muon在高达40亿参数的模型上表现优于AdamW，尤其在超大批量时仍保持高效。

Conclusion: Muon是一种高效且经济的优化器，适用于大规模模型训练，尤其是在资源受限的情况下。

Abstract: We demonstrate that Muon, the simplest instantiation of a second-order
optimizer, explicitly expands the Pareto frontier over AdamW on the
compute-time tradeoff. We find that Muon is more effective than AdamW in
retaining data efficiency at large batch sizes, far beyond the so-called
critical batch size, while remaining computationally efficient, thus enabling
more economical training. We study the combination of Muon and the maximal
update parameterization (muP) for efficient hyperparameter transfer and present
a simple telescoping algorithm that accounts for all sources of error in muP
while introducing only a modest overhead in resources. We validate our findings
through extensive experiments with model sizes up to four billion parameters
and ablations on the data distribution and architecture.

</details>


### [119] [Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning](https://arxiv.org/abs/2505.02639)
*Xuan Lin,Qingrui Liu,Hongxin Xiang,Daojian Zeng,Xiangxiang Zeng*

Main category: cs.LG

TL;DR: 论文提出了ChemDual框架，通过构建大规模指令数据集和双任务学习策略，优化了化学反应和逆合成预测，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）直接应用于化学反应和逆合成预测时面临两个主要挑战：缺乏大规模的化学合成相关指令数据集，以及忽视了反应和逆合成预测之间的紧密关联。

Method: ChemDual将化学反应和逆合成视为相关的重组与碎片化过程，并构建了440万条指令数据集。同时，通过多尺度分词器和双任务学习策略增强LLaMA模型，联合优化重组与碎片化过程以及反应和逆合成预测任务。

Result: 在Mol-Instruction和USPTO-50K数据集上的实验表明，ChemDual在反应和逆合成预测上均优于现有的单任务方法和通用开源LLMs。分子对接分析显示，ChemDual生成的化合物具有多样且强的蛋白结合亲和力。

Conclusion: ChemDual在药物设计中展示了强大的潜力，为化学反应和逆合成预测提供了更优的解决方案。

Abstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug
discovery. Recently, large language models (LLMs) have shown potential in many
domains. However, directly applying LLMs to these tasks faces two major
challenges: (i) lacking a large-scale chemical synthesis-related instruction
dataset; (ii) ignoring the close correlation between reaction and
retrosynthesis prediction for the existing fine-tuning strategies. To address
these challenges, we propose ChemDual, a novel LLM framework for accurate
chemical synthesis. Specifically, considering the high cost of data acquisition
for reaction and retrosynthesis, ChemDual regards the
reaction-and-retrosynthesis of molecules as a related
recombination-and-fragmentation process and constructs a large-scale of 4.4
million instruction dataset. Furthermore, ChemDual introduces an enhanced
LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,
to jointly optimize the process of recombination and fragmentation as well as
the tasks between reaction and retrosynthesis prediction. Extensive experiments
on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves
state-of-the-art performance in both predictions of reaction and
retrosynthesis, outperforming the existing conventional single-task approaches
and the general open-source LLMs. Through molecular docking analysis, ChemDual
generates compounds with diverse and strong protein binding affinity, further
highlighting its strong potential in drug design.

</details>


### [120] [Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning](https://arxiv.org/abs/2505.02228)
*Shangzhe Li,Zhiao Huang,Hao Su*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机网络蒸馏（RND）的在线模仿学习新方法，通过联合估计世界模型潜在空间中的专家和行为分布来解决现有方法的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在多个领域取得了显著成功，但现有方法在处理对抗性奖励或价值公式时常面临不稳定性挑战，尤其是在世界模型框架中。

Method: 采用随机网络蒸馏（RND）构建奖励模型，联合估计世界模型潜在空间中的专家和行为分布。

Result: 在DMControl、Meta-World和ManiSkill2等多个基准测试中，该方法表现稳定并达到专家水平。

Conclusion: 该方法在保持专家水平性能的同时，显著提高了模仿学习的稳定性。

Abstract: Imitation Learning (IL) has achieved remarkable success across various
domains, including robotics, autonomous driving, and healthcare, by enabling
agents to learn complex behaviors from expert demonstrations. However, existing
IL methods often face instability challenges, particularly when relying on
adversarial reward or value formulations in world model frameworks. In this
work, we propose a novel approach to online imitation learning that addresses
these limitations through a reward model based on random network distillation
(RND) for density estimation. Our reward model is built on the joint estimation
of expert and behavioral distributions within the latent space of the world
model. We evaluate our method across diverse benchmarks, including DMControl,
Meta-World, and ManiSkill2, showcasing its ability to deliver stable
performance and achieve expert-level results in both locomotion and
manipulation tasks. Our approach demonstrates improved stability over
adversarial methods while maintaining expert-level performance.

</details>


### [121] [Federated Causal Inference in Healthcare: Methods, Challenges, and Applications](https://arxiv.org/abs/2505.02238)
*Haoyang Li,Jie Xu,Kyra Gan,Fei Wang,Chengxi Zang*

Main category: cs.LG

TL;DR: 联邦因果推断提供了一种隐私保护的多站点治疗效果估计方法，但数据异质性带来了挑战。本文综述并理论分析了相关方法，包括权重和优化策略，并探讨了FedProx正则化的优势及未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果推断方法需处理跨站点数据异质性（如协变量、治疗和结果差异），如何在隐私保护前提下实现无偏且高效的估计是关键挑战。

Method: 分类比较权重法和优化框架，扩展讨论个性化模型、点对点通信和模型分解；针对生存分析提出联邦Cox和Aalen-Johansen模型，并理论分析异质性下的偏差与方差。

Result: 发现FedProx正则化在偏差-方差权衡上优于简单平均和元分析，接近最优解。

Conclusion: 联邦因果推断在分布式医疗系统中潜力巨大，但需解决可扩展性、公平性和可信性问题，为未来研究指明方向。

Abstract: Federated causal inference enables multi-site treatment effect estimation
without sharing individual-level data, offering a privacy-preserving solution
for real-world evidence generation. However, data heterogeneity across sites,
manifested in differences in covariate, treatment, and outcome, poses
significant challenges for unbiased and efficient estimation. In this paper, we
present a comprehensive review and theoretical analysis of federated causal
effect estimation across both binary/continuous and time-to-event outcomes. We
classify existing methods into weight-based strategies and optimization-based
frameworks and further discuss extensions including personalized models,
peer-to-peer communication, and model decomposition. For time-to-event
outcomes, we examine federated Cox and Aalen-Johansen models, deriving
asymptotic bias and variance under heterogeneity. Our analysis reveals that
FedProx-style regularization achieves near-optimal bias-variance trade-offs
compared to naive averaging and meta-analysis. We review related software tools
and conclude by outlining opportunities, challenges, and future directions for
scalable, fair, and trustworthy federated causal inference in distributed
healthcare systems.

</details>


### [122] [RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation](https://arxiv.org/abs/2505.02247)
*Jingxiang Qu,Wenhan Gao,Jiaxing Zhang,Xufeng Liu,Hua Wei,Haibin Ling,Yi Liu*

Main category: cs.LG

TL;DR: 论文提出了一种针对3D GNN的新型解释方法，通过为每个节点分配影响半径来定位3D空间中的局部区域，从而增强模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 3D GNNs在分子数据建模中表现出强大的预测能力，但其有限的解释性阻碍了科学应用中对可靠透明洞察的需求。

Method: 设计了一种专门针对3D GNN的解释方法，通过局部化节点的直接邻域并为每个节点分配影响半径来捕捉空间和结构交互。

Result: 该方法不仅提升了3D GNN的可解释性，还与实际应用中（如分子学习）的物理和结构依赖性保持一致。

Conclusion: 提出的方法有效解决了3D GNNs中的解释性问题，为科学应用提供了更可靠的解释工具。

Abstract: 3D Geometric Graph Neural Networks (GNNs) have emerged as transformative
tools for modeling molecular data. Despite their predictive power, these models
often suffer from limited interpretability, raising concerns for scientific
applications that require reliable and transparent insights. While existing
methods have primarily focused on explaining molecular substructures in 2D
GNNs, the transition to 3D GNNs introduces unique challenges, such as handling
the implicit dense edge structures created by a cut-off radius. To tackle this,
we introduce a novel explanation method specifically designed for 3D GNNs,
which localizes the explanation to the immediate neighborhood of each node
within the 3D space. Each node is assigned an radius of influence, defining the
localized region within which message passing captures spatial and structural
interactions crucial for the model's predictions. This method leverages the
spatial and geometric characteristics inherent in 3D graphs. By constraining
the subgraph to a localized radius of influence, the approach not only enhances
interpretability but also aligns with the physical and structural dependencies
typical of 3D graph applications, such as molecular learning.

</details>


### [123] [Epistemic Wrapping for Uncertainty Quantification](https://arxiv.org/abs/2505.02277)
*Maryam Sultana,Neil Yorke-Smith,Kaizheng Wang,Shireen Kudukkil Manchingal,Muhammad Mubashar,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 本文提出了一种名为'Epistemic Wrapping'的新方法，利用贝叶斯神经网络（BNN）作为基线，将其输出转化为信念函数后验，有效捕捉不确定性并提升分类任务的鲁棒性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，尤其是分类任务，不确定性估计对提升模型的鲁棒性和可靠性至关重要。本文旨在通过改进不确定性估计方法来解决这一问题。

Method: 提出的'Epistemic Wrapping'方法以贝叶斯神经网络（BNN）为基线，将其输出转化为信念函数后验，从而实现高效且通用的不确定性量化。

Result: 实验在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100数据集上进行，结果表明该方法显著提升了模型的泛化能力和不确定性量化效果。

Conclusion: 'Epistemic Wrapping'方法在不确定性估计方面表现出色，为分类任务提供了更可靠和鲁棒的解决方案。

Abstract: Uncertainty estimation is pivotal in machine learning, especially for
classification tasks, as it improves the robustness and reliability of models.
We introduce a novel `Epistemic Wrapping' methodology aimed at improving
uncertainty estimation in classification. Our approach uses Bayesian Neural
Networks (BNNs) as a baseline and transforms their outputs into belief function
posteriors, effectively capturing epistemic uncertainty and offering an
efficient and general methodology for uncertainty quantification. Comprehensive
experiments employing a Bayesian Neural Network (BNN) baseline and an Interval
Neural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and
CIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly
enhances generalisation and uncertainty quantification.

</details>


### [124] [Universal Approximation Theorem of Deep Q-Networks](https://arxiv.org/abs/2505.02288)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一个连续时间框架，利用随机控制和前向-后向随机微分方程（FBSDEs）分析深度Q网络（DQNs），证明了DQN在紧凑集上能以高概率任意精度逼近最优Q函数，并分析了Q学习算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过建立连续时间框架，结合深度强化学习与随机控制理论，分析DQNs的逼近能力和收敛性，为物理系统或高频数据应用提供理论支持。

Method: 方法包括利用残差网络逼近定理、大偏差理论和随机逼近定理，分析DQN的逼近能力和Q学习算法的收敛性，并考虑网络层数、时间离散化及粘性解的作用。

Result: 结果表明DQN能在紧凑集上高概率任意精度逼近最优Q函数，同时Q学习算法在该框架下收敛。

Conclusion: 结论是本文为连续时间下的DQN提供了理论分析框架，连接了深度强化学习与随机控制，对实际应用具有潜在意义。

Abstract: We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs)
via stochastic control and Forward-Backward Stochastic Differential Equations
(FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by
a square-integrable martingale, we analyze DQN approximation properties. We
show that DQNs can approximate the optimal Q-function on compact sets with
arbitrary accuracy and high probability, leveraging residual network
approximation theorems and large deviation bounds for the state-action process.
We then analyze the convergence of a general Q-learning algorithm for training
DQNs in this setting, adapting stochastic approximation theorems. Our analysis
emphasizes the interplay between DQN layer count, time discretization, and the
role of viscosity solutions (primarily for the value function $V^*$) in
addressing potential non-smoothness of the optimal Q-function. This work
bridges deep reinforcement learning and stochastic control, offering insights
into DQNs in continuous-time settings, relevant for applications with physical
systems or high-frequency data.

</details>


### [125] [Entropy-Guided Sampling of Flat Modes in Discrete Spaces](https://arxiv.org/abs/2505.02296)
*Pinaki Mohanty,Riddhiman Bhattacharya,Ruqi Zhang*

Main category: cs.LG

TL;DR: 论文提出了EDLP方法，通过引入局部熵改进离散空间中的平坦模态采样，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 平坦模态在组合优化和离散生成模型中很重要，但现有采样算法难以有效捕捉平坦模态。

Method: 提出EDLP，通过连续辅助变量和联合分布引入局部熵，指导采样器更有效找到平坦模态。

Result: 在伯努利分布、受限玻尔兹曼机、组合优化和二值神经网络等任务中表现优于传统方法。

Conclusion: EDLP为离散空间中平坦模态采样提供了高效解决方案，并具有理论收敛保证。

Abstract: Sampling from flat modes in discrete spaces is a crucial yet underexplored
problem. Flat modes represent robust solutions and have broad applications in
combinatorial optimization and discrete generative modeling. However, existing
sampling algorithms often overlook the mode volume and struggle to capture flat
modes effectively. To address this limitation, we propose \emph{Entropic
Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the
sampling process through a continuous auxiliary variable under a joint
distribution. The local entropy term guides the discrete sampler toward flat
modes with a small overhead. We provide non-asymptotic convergence guarantees
for EDLP in locally log-concave discrete distributions. Empirically, our method
consistently outperforms traditional approaches across tasks that require
sampling from flat basins, including Bernoulli distribution, restricted
Boltzmann machines, combinatorial optimization, and binary neural networks.

</details>


### [126] [Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2505.02299)
*Daisuke Yamada,Harit Vishwakarma,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 本文提出了一种人机交互框架，动态更新评分函数和阈值以应对分布外（OOD）输入，同时严格控制假正率（FPR），并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，机器学习模型常遇到分布外输入，现有方法仅基于分布内数据设定阈值，导致假正率高且缺乏适应性。本文旨在解决这些问题。

Method: 提出了一种人机交互框架，动态更新评分函数和阈值，结合实时观察的OOD输入，最大化真正率（TPR）并严格控制FPR。

Result: 理论证明了在平稳条件下FPR的可控性，实验显示在OpenOOD基准上优于现有方法，实现更高TPR并保持FPR控制。

Conclusion: 该框架为OOD检测提供了高效且安全的自适应解决方案，特别适合动态环境和安全关键应用。

Abstract: Machine Learning (ML) models are trained on in-distribution (ID) data but
often encounter out-of-distribution (OOD) inputs during deployment -- posing
serious risks in safety-critical domains. Recent works have focused on
designing scoring functions to quantify OOD uncertainty, with score thresholds
typically set based solely on ID data to achieve a target true positive rate
(TPR), since OOD data is limited before deployment. However, these TPR-based
thresholds leave false positive rates (FPR) uncontrolled, often resulting in
high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring
functions and thresholds lack the adaptivity needed to handle newly observed,
evolving OOD inputs, leading to sub-optimal performance. To address these
challenges, we propose a human-in-the-loop framework that \emph{safely updates
both scoring functions and thresholds on the fly} based on real-world OOD
inputs. Our method maximizes TPR while strictly controlling FPR at all times,
even as the system adapts over time. We provide theoretical guarantees for FPR
control under stationary conditions and present extensive empirical evaluations
on OpenOOD benchmarks to demonstrate that our approach outperforms existing
methods by achieving higher TPRs while maintaining FPR control.

</details>


### [127] [Enabling Local Neural Operators to perform Equation-Free System-Level Analysis](https://arxiv.org/abs/2505.02308)
*Gianluca Fabiani,Hannes Vandecasteele,Somdatta Goswami,Constantinos Siettos,Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: NOs被用于物理系统建模，但其在系统级任务（如固定点、稳定性和分岔分析）上的潜力尚未充分开发。本文提出结合NOs与Krylov子空间数值方法，实现高效系统级分析，并通过三个非线性PDE案例验证。


<details>
  <summary>Details</summary>
Motivation: 探索NOs在系统级分析任务（如固定点、稳定性和分岔分析）中的应用潜力，弥补当前NOs主要用于动力学模拟的局限性。

Method: 结合局部NOs与Krylov子空间高级数值方法，提出框架以实现大规模动力学的稳定性和分岔分析，并涵盖空间、时间及时空局部NOs的加速分析能力。

Result: 通过1D Allen-Cahn方程、Liouville-Bratu-Gelfand PDE和FHN模型验证框架有效性，展示NOs在处理分岔、临界点等复杂动态的能力。

Conclusion: NOs结合数值方法可高效完成系统级分析，尤其在处理非线性PDE的稳定性和分岔问题时表现优越，扩展了NOs的应用范围。

Abstract: Neural Operators (NOs) provide a powerful framework for computations
involving physical laws that can be modelled by (integro-) partial differential
equations (PDEs), directly learning maps between infinite-dimensional function
spaces that bypass both the explicit equation identification and their
subsequent numerical solving. Still, NOs have so far primarily been employed to
explore the dynamical behavior as surrogates of brute-force temporal
simulations/predictions. Their potential for systematic rigorous numerical
system-level tasks, such as fixed-point, stability, and bifurcation analysis -
crucial for predicting irreversible transitions in real-world phenomena -
remains largely unexplored. Toward this aim, inspired by the Equation-Free
multiscale framework, we propose and implement a framework that integrates
(local) NOs with advanced iterative numerical methods in the Krylov subspace,
so as to perform efficient system-level stability and bifurcation analysis of
large-scale dynamical systems. Beyond fixed point, stability, and bifurcation
analysis enabled by local in time NOs, we also demonstrate the usefulness of
local in space as well as in space-time ("patch") NOs in accelerating the
computer-aided analysis of spatiotemporal dynamics. We illustrate our framework
via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes
multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE,
which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN)
model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node
bifurcations.

</details>


### [128] [Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training](https://arxiv.org/abs/2505.02360)
*Fares B. Mehouachi,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: 本文提出一种通过控制训练中的$l^p$范数来缓解对抗训练中“灾难性过拟合”（CO）的新方法，避免了传统方法如噪声注入或正则化的需求。通过分析不同范数下CO的出现机制，作者开发了一种自适应的$l^p$-FGSM攻击框架，能根据梯度信息自动调整训练范数，从而有效提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 对抗训练中，快速方法（如FGSM）常因“灾难性过拟合”（CO）导致模型对多步攻击失效。现有方法依赖噪声注入或正则化，但作者发现CO与训练范数的选择密切相关，尤其是$l^{\infty}$范数下更易出现。因此，本文旨在探究范数对CO的影响，并提出一种无需额外干预的解决方案。

Method: 作者提出将$l^p$范数攻击建模为固定点问题，并设计$l^p$-FGSM攻击框架，通过分析梯度集中度（如参与比和熵）动态调整训练范数，从而避免CO。

Result: 实验表明，该方法在不依赖噪声或正则化的情况下，显著提升了模型对多步攻击的鲁棒性，验证了范数控制对缓解CO的有效性。

Conclusion: 通过量化梯度集中度并自适应调整训练范数，本文为对抗训练中的CO问题提供了一种理论严谨且高效的解决方案，扩展了对抗鲁棒性的研究途径。

Abstract: Adversarial training is a cornerstone of robust deep learning, but fast
methods like the Fast Gradient Sign Method (FGSM) often suffer from
Catastrophic Overfitting (CO), where models become robust to single-step
attacks but fail against multi-step variants. While existing solutions rely on
noise injection, regularization, or gradient clipping, we propose a novel
solution that purely controls the $l^p$ training norm to mitigate CO.
  Our study is motivated by the empirical observation that CO is more prevalent
under the $l^{\infty}$ norm than the $l^2$ norm. Leveraging this insight, we
develop a framework for generalized $l^p$ attack as a fixed point problem and
craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to
$l^{\infty}$. This leads to our core insight: CO emerges when highly
concentrated gradients where information localizes in few dimensions interact
with aggressive norm constraints. By quantifying gradient concentration through
Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM
that automatically tunes the training norm based on gradient information.
Extensive experiments demonstrate that this approach achieves strong robustness
without requiring additional regularization or noise injection, providing a
novel and theoretically-principled pathway to mitigate the CO problem.

</details>


### [129] [Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks](https://arxiv.org/abs/2505.02369)
*Juyoung Yun*

Main category: cs.LG

TL;DR: ZSharp通过层间Z-score标准化和百分位过滤显著梯度成分，改进SAM方法，提升深度网络泛化能力，无需改动架构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易收敛到尖锐最小值，降低模型鲁棒性，SAM方法虽能寻找平坦最小值，但梯度扰动可能包含不显著方向。ZSharp旨在优化这一点。

Method: ZSharp在SAM基础上增加层间Z-score标准化和百分位过滤，保留统计显著梯度成分，减少扰动方向噪声。

Result: 在CIFAR-10/100和Tiny-ImageNet上，ZSharp在ResNet、VGG和ViT上测试精度优于SAM及其变体，尤其在深层和Transformer模型中表现更佳。

Conclusion: ZSharp是轻量且原理清晰的改进方案，显著提升基于锐度感知的优化效果。

Abstract: Generalizing well in deep neural networks remains a core challenge,
particularly due to their tendency to converge to sharp minima that degrade
robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking
flatter minima but perturbs parameters using the full gradient, which can
include statistically insignificant directions. We propose ZSharp, a simple yet
effective extension to SAM that applies layer-wise Z-score normalization
followed by percentile-based filtering to retain only statistically significant
gradient components. This selective perturbation aligns updates with
curvature-sensitive directions, enhancing generalization without requiring
architectural changes. ZSharp introduces only one additional hyperparameter,
the percentile threshold, and remains fully compatible with existing SAM
variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,
VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and
its variants in test accuracy, particularly on deeper and transformer-based
models. These results demonstrate that ZSharp is a principled and lightweight
improvement for sharpness-aware optimization.

</details>


### [130] [EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices](https://arxiv.org/abs/2505.02380)
*Arnab Sanyal,Prithwish Mukherjee,Gourav Datta,Sandeep P. Chinchali*

Main category: cs.LG

TL;DR: 提出了EntroLLM，一种结合混合量化和熵编码的压缩框架，在不影响模型准确性的前提下显著减少存储需求，并提升边缘设备上的推理速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然在多任务中表现出色，但其巨大的存储和计算需求限制了在边缘设备上的部署，因此需要一种高效的压缩方法。

Method: 采用层级混合量化策略（对称和不对称量化）优化压缩性，并使用Huffman编码对量化权重进行无损压缩，同时引入并行Huffman解码以减少推理延迟。

Result: 在多个边沿兼容的LLM（如smolLM-1.7B-Instruct、phi3-mini-4k-Instruct等）上，EntroLLM存储减少达30%（相对于uint8）和65%（相对于uint4），同时保持性能不变；在边缘设备上推理吞吐量提升31.9%-146.6%。

Conclusion: EntroLLM是一种实用的解决方案，无需重新训练并兼容现有后训练量化方法，显著优化了边缘设备上LLM的存储和计算效率。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance across
various tasks, but their large storage and computational requirements constrain
their deployment on edge devices. To address this, we propose EntroLLM, a novel
compression framework that integrates mixed quantization with entropy coding to
reduce storage overhead while maintaining model accuracy. Our method applies a
layer-wise mixed quantization scheme - choosing between symmetric and
asymmetric quantization based on individual layer weight distributions - to
optimize compressibility. We then employ Huffman encoding for lossless
compression of the quantized weights, significantly reducing memory bandwidth
requirements. Furthermore, we introduce parallel Huffman decoding, which
enables efficient retrieval of encoded weights during inference, ensuring
minimal latency impact. Our experiments on edge-compatible LLMs, including
smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,
demonstrate that EntroLLM achieves up to $30%$ storage reduction compared to
uint8 models and up to $65%$ storage reduction compared to uint4 models, while
preserving perplexity and accuracy, on language benchmark tasks. We further
show that our method enables $31.9%$ - $146.6%$ faster inference throughput on
memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing
the required data movement. The proposed approach requires no additional
re-training and is fully compatible with existing post-training quantization
methods, making it a practical solution for edge LLMs.

</details>


### [131] [Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret](https://arxiv.org/abs/2505.02383)
*Bingshan Hu,Zhiming Huang,Tianyue H. Zhang,Mathias Lécuyer,Nidhi Hegde*

Main category: cs.LG

TL;DR: 提出了DP-TS-UCB算法，一种参数化的私有赌博算法，以探索高斯差分隐私在随机赌博问题中的应用，在隐私性与遗憾值之间提供权衡。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私随机赌博问题，探索Thompson采样、高斯机制与高斯差分隐私的深层联系，提出一种能在隐私性与后悔值之间权衡的算法。

Method: 提出了DP-TS-UCB算法，结合了Thompson采样的探索机制和Upper Confidence Bound (UCB)的思想，并通过参数α在隐私性和遗憾值之间进行权衡。

Result: 算法满足GDP隐私保证，遗憾值上界为𝑂(𝐾𝑙𝑛𝛼+1(𝑇)/Δ)，通过参数α控制隐私与遗憾的权衡。

Conclusion: 通过理论分析，算法不仅提供了隐私保障和性能保证，还揭示了Thompson采样和UCB探索机制的关联，为相关研究提供了新的视角。

Abstract: We address differentially private stochastic bandit problems from the angles
of exploring the deep connections among Thompson Sampling with Gaussian priors,
Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose
DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade
off privacy and regret. DP-TS-UCB satisfies $ \tilde{O}
\left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O
\left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$
controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB
relies on anti-concentration bounds of Gaussian distributions and links
exploration mechanisms in Thompson Sampling-based algorithms and Upper
Confidence Bound-based algorithms, which may be of independent interest.

</details>


### [132] [Quantitative Analysis of Performance Drop in DeepSeek Model Quantization](https://arxiv.org/abs/2505.02390)
*Enbo Zhao,Yi Shen,Shuming Shi,Jieyun Huang,Zhihao Chen,Ning Wang,Siqi Xiao,Jian Zhang,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: DeepSeek-R1和V3模型的4位量化在性能几乎不下降的情况下可实现单机部署，并提出动态3位量化方法DQ3_K_M，其表现优于传统3位量化且接近4位量化效果，支持多种GPU设备。


<details>
  <summary>Details</summary>
Motivation: 由于官方服务常繁忙且存在数据隐私问题，本地部署DeepSeek-R1和V3模型需求高，但其大规模参数超出单机内存限制，需通过量化技术解决。

Method: 对DeepSeek全系模型进行了多比特宽度量化的首次定量评估，并提出了动态3位量化方法DQ3_K_M。

Result: 4位量化性能接近FP8，支持单机部署；DQ3_K_M在多个基准测试中优于传统3位量化，且与4位量化效果相当，兼容多种GPU设备。

Conclusion: DQ3_K_M为DeepSeek模型的高效单机部署提供了实用解决方案，代码已在GitHub开源。

Abstract: Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,
possibly because the official service often suffers from being busy and some
organizations have data privacy concerns. While single-machine deployment
offers infrastructure simplicity, the models' 671B FP8 parameter configuration
exceeds the practical memory limits of a standard 8-GPU machine. Quantization
is a widely used technique that helps reduce model memory consumption. However,
it is unclear what the performance of DeepSeek-R1 and V3 will be after being
quantized. This technical report presents the first quantitative evaluation of
multi-bitwidth quantization across the complete DeepSeek model spectrum. Key
findings reveal that 4-bit quantization maintains little performance
degradation versus FP8 while enabling single-machine deployment on standard
NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization
method that significantly outperforms traditional Q3_K_M variant on various
benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach
in most tasks. Moreover, DQ3_K_M supports single-machine deployment
configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of
DQ3\_K\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing
optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.

</details>


### [133] [A probabilistic view on Riemannian machine learning models for SPD matrices](https://arxiv.org/abs/2505.02402)
*Thibault de Surrel,Florian Yger,Fabien Lotte,Sylvain Chevallier*

Main category: cs.LG

TL;DR: 该论文通过概率框架统一了黎曼流形上的机器学习工具，并展示了这些工具可以被重新解释为贝叶斯分类器。


<details>
  <summary>Details</summary>
Motivation: 研究目的是将黎曼流形上的多种机器学习工具统一到一个概率框架中，并为这些工具奠定理论基础。

Method: 在对称正定矩阵流形上引入高斯分布，并将其应用于分类、异常检测和降维。

Result: 展示了这些高斯分布在流形上的广泛应用，使其他机器学习工具得以扩展到流形空间。

Conclusion: 该论文为黎曼流形上的机器学习工具提供了统一的概率视角，扩展了其应用范围。

Abstract: The goal of this paper is to show how different machine learning tools on the
Riemannian manifold $\mathcal{P}_d$ of Symmetric Positive Definite (SPD)
matrices can be united under a probabilistic framework. For this, we will need
several Gaussian distributions defined on $\mathcal{P}_d$. We will show how
popular classifiers on $\mathcal{P}_d$ can be reinterpreted as Bayes
Classifiers using these Gaussian distributions. These distributions will also
be used for outlier detection and dimension reduction. By showing that those
distributions are pervasive in the tools used on $\mathcal{P}_d$, we allow for
other machine learning tools to be extended to $\mathcal{P}_d$.

</details>


### [134] [T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models](https://arxiv.org/abs/2505.02417)
*Yunfeng Ge,Jiawei Li,Yiji Zhao,Haomin Wen,Zhao Li,Meikang Qiu,Hongyan Li,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: 论文提出了Text-to-Series（T2S），一种基于扩散模型的框架，通过长度自适应的变分自编码器和Flow Matching技术，解决了时间序列生成中的长度限制和文本对齐问题，并在多个领域的数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间序列生成中存在两大局限：一是缺乏通用的时间序列标注方法，二是无法生成任意长度的时间序列。这些问题限制了模型在实际场景中的应用。

Method: T2S框架结合了长度自适应的变分自编码器和Flow Matching技术，通过扩散变换器作为去噪器，以跨长度交替训练的方式实现任意长度时间序列的生成。

Result: T2S在12个领域的13个数据集上达到了最先进的性能。

Conclusion: T2S通过创新的框架设计，成功解决了时间序列生成中的关键挑战，为跨领域应用提供了强大的工具。

Abstract: Text-to-Time Series generation holds significant potential to address
challenges such as data sparsity, imbalance, and limited availability of
multimodal time series datasets across domains. While diffusion models have
achieved remarkable success in Text-to-X (e.g., vision and audio data)
generation, their use in time series generation remains in its nascent stages.
Existing approaches face two critical limitations: (1) the lack of systematic
exploration of general-proposed time series captions, which are often
domain-specific and struggle with generalization; and (2) the inability to
generate time series of arbitrary lengths, limiting their applicability to
real-world scenarios. In this work, we first categorize time series captions
into three levels: point-level, fragment-level, and instance-level.
Additionally, we introduce a new fragment-level dataset containing over 600,000
high-resolution time series-text pairs. Second, we propose Text-to-Series
(T2S), a diffusion-based framework that bridges the gap between natural
language and time series in a domain-agnostic manner. T2S employs a
length-adaptive variational autoencoder to encode time series of varying
lengths into consistent latent embeddings. On top of that, T2S effectively
aligns textual representations with latent embeddings by utilizing Flow
Matching and employing Diffusion Transformer as the denoiser. We train T2S in
an interleaved paradigm across multiple lengths, allowing it to generate
sequences of any desired length. Extensive evaluations demonstrate that T2S
achieves state-of-the-art performance across 13 datasets spanning 12 domains.

</details>


### [135] [Towards One-shot Federated Learning: Advances, Challenges, and Future Directions](https://arxiv.org/abs/2505.02426)
*Flora Amato,Lingyu Qiu,Mohammad Tanveer,Salvatore Cuomo,Fabio Giampaolo,Francesco Piccialli*

Main category: cs.LG

TL;DR: 这篇论文综述了一击式联邦学习（One-shot FL），强调其单轮通信、资源节约和隐私保护的特性，与传统联邦学习框架相比的优势，并总结了当前方法在客户端模型初始化、聚合技术及异构数据处理的进展与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究一击式联邦学习的动机在于解决传统联邦学习中的通信迭代需求高、资源消耗大的问题，特别适用于资源受限和隐私敏感的应用场景。

Method: 论文通过系统分类现有方法，重点关注客户端模型初始化、聚合技术和异构数据分布管理的策略。

Result: 文章指出当前方法在非独立同分布（non-IID）环境下的可扩展性和泛化能力存在局限，并分析了前沿技术的进展。

Conclusion: 该综述旨在为研究人员和从业者提供一击式联邦学习系统设计与实施的全面参考，推动其在现实资源受限场景中的应用发展。

Abstract: One-shot FL enables collaborative training in a single round, eliminating the
need for iterative communication, making it particularly suitable for use in
resource-constrained and privacy-sensitive applications. This survey offers a
thorough examination of One-shot FL, highlighting its distinct operational
framework compared to traditional federated approaches. One-shot FL supports
resource-limited devices by enabling single-round model aggregation while
maintaining data locality. The survey systematically categorizes existing
methodologies, emphasizing advancements in client model initialization,
aggregation techniques, and strategies for managing heterogeneous data
distributions. Furthermore, we analyze the limitations of current approaches,
particularly in terms of scalability and generalization in non-IID settings. By
analyzing cutting-edge techniques and outlining open challenges, this survey
aspires to provide a comprehensive reference for researchers and practitioners
aiming to design and implement One-shot FL systems, advancing the development
and adoption of One-shot FL solutions in a real-world, resource-constrained
scenario.

</details>


### [136] [FairPO: Robust Preference Optimization for Fair Multi-Label Learning](https://arxiv.org/abs/2505.02433)
*Soumen Kumar Mondal,Akshit Varmora,Prateek Chanda,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: FairPO是一个新颖的多标签分类框架，通过直接优化偏好信号促进公平性，采用群体鲁棒性视角，动态调整训练重点以减少偏见。


<details>
  <summary>Details</summary>
Motivation: 为了解决多标签分类中存在的公平性问题，特别是在处理特权和非特权标签组时的不平衡问题，提出了FairPO框架。

Method: 通过划分特权和非特权标签组，并引入基于偏好的损失函数（DPO启发）来优化区分性能，同时保证非特权标签的分类效果，采用鲁棒优化方法动态调整训练重点。

Result: 框架有效减少了特权组的偏见，同时保持了非特权组的分类性能，实现了更公平的多标签分类。

Conclusion: FairPO展示了在多标签分类中提升公平性的潜力，并计划扩展研究以探索更复杂的损失函数和生成能力。

Abstract: We propose FairPO, a novel framework designed to promote fairness in
multi-label classification by directly optimizing preference signals with a
group robustness perspective. In our framework, the set of labels is
partitioned into privileged and non-privileged groups, and a preference-based
loss inspired by Direct Preference Optimization (DPO) is employed to more
effectively differentiate true positive labels from confusing negatives within
the privileged group, while preserving baseline classification performance for
non-privileged labels. By framing the learning problem as a robust optimization
over groups, our approach dynamically adjusts the training emphasis toward
groups with poorer performance, thereby mitigating bias and ensuring a fairer
treatment across diverse label categories. In addition, we outline plans to
extend this approach by investigating alternative loss formulations such as
Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization
(CPO) to exploit reference-free reward formulations and contrastive training
signals. Furthermore, we plan to extend FairPO with multilabel generation
capabilities, enabling the model to dynamically generate diverse and coherent
label sets for ambiguous inputs.

</details>


### [137] [A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability](https://arxiv.org/abs/2505.02435)
*Pouria Fatemi,Ehsan Sharifian,Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: 该论文提出了一种基于回溯反事实的高效方法，结合因果推理生成可操作的模型解释，解决了传统方法忽略因果关系和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的反事实解释方法往往忽视因果关系，生成不现实的例子；而集成因果关系的新方法计算成本高。需要一种高效且融入因果推理的解决方案。

Method: 提出基于回溯反事实的高效方法，结合因果推理生成可操作的模型解释，并分析了与现有技术的关系及其泛化能力。

Result: 实验表明，该方法能更深入地理解模型输出，提供更具可操作性的解释。

Conclusion: 该研究成功提出了一种高效且因果感知的反事实解释方法，为模型可解释性提供了新思路。

Abstract: Counterfactual explanations enhance interpretability by identifying
alternative inputs that produce different outputs, offering localized insights
into model decisions. However, traditional methods often neglect causal
relationships, leading to unrealistic examples. While newer approaches
integrate causality, they are computationally expensive. To address these
challenges, we propose an efficient method based on backtracking
counterfactuals that incorporates causal reasoning to generate actionable
explanations. We first examine the limitations of existing methods and then
introduce our novel approach and its features. We also explore the relationship
between our method and previous techniques, demonstrating that it generalizes
them in specific scenarios. Finally, experiments show that our method provides
deeper insights into model outputs.

</details>


### [138] [Efficient Continual Learning in Keyword Spotting using Binary Neural Networks](https://arxiv.org/abs/2505.02469)
*Quynh Nguyen-Phuong Vu,Luciano Sebastian Martinez-Rau,Yuxuan Zhang,Nho-Duc Tran,Bengt Oelmann,Michele Magno,Sebastian Bader*

Main category: cs.LG

TL;DR: 本文提出了一种基于二进制神经网络（BNN）的持续学习（CL）方法，用于资源受限设备中的关键词识别（KWS），旨在动态添加新关键词。实验表明，该方法在单次添加新关键词时准确率超过95%，四次时达86%，且计算复杂度差异不显著。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备中的KWS模型通常无法动态适应新关键词的添加，因此需要一种高效且计算成本低的持续学习方法。

Method: 采用二进制神经网络（BNN）结合七种持续学习技术，评估其在新关键词动态添加时的表现，并分析训练数据量和计算复杂度的影响。

Result: 单次添加新关键词时准确率超95%，四次时达86%；批处理算法对数据量更敏感，但计算复杂度差异不显著。

Conclusion: 该方法在资源受限设备中高效且兼容性强，为动态扩展KWS功能提供了可行方案。

Abstract: Keyword spotting (KWS) is an essential function that enables interaction with
ubiquitous smart devices. However, in resource-limited devices, KWS models are
often static and can thus not adapt to new scenarios, such as added keywords.
To overcome this problem, we propose a Continual Learning (CL) approach for KWS
built on Binary Neural Networks (BNNs). The framework leverages the reduced
computation and memory requirements of BNNs while incorporating techniques that
enable the seamless integration of new keywords over time. This study evaluates
seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95%
for a single additional keyword and up to 86% for four additional classes.
Sensitivity to the amount of training samples in the CL phase, and differences
in computational complexities are being evaluated. These evaluations
demonstrate that batch-based algorithms are more sensitive to the CL dataset
size, and that differences between the computational complexities are
insignificant. These findings highlight the potential of developing an
effective and computationally efficient technique for continuously integrating
new keywords in KWS applications that is compatible with resource-constrained
devices.

</details>


### [139] [SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2505.02486)
*Jinpeng Chen,Runmin Cong,Yuzhi Zhao,Hongzheng Yang,Guangneng Hu,Horace Ho Shing Ip,Sam Kwong*

Main category: cs.LG

TL;DR: 本文提出了多模态持续指令调整（MCIT）框架，旨在解决多模态大语言模型（MLLMs）在增量学习中的遗忘问题，并将其分为表面遗忘和本质遗忘。通过答案风格多样化（ASD）和RegLoRA方法，有效减少了遗忘现象，实验表明SEFE方法表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在增量学习中因任务风格变化导致的遗忘问题，区分表面遗忘和本质遗忘，并提出针对性解决方案。

Method: 采用答案风格多样化（ASD）统一任务风格，结合RegLoRA方法通过正则化稳定关键参数以减少本质遗忘。

Result: 实验结果显示，提出的SEFE方法在性能上达到最优。

Conclusion: 通过ASD和RegLoRA的结合，有效解决了多模态持续学习中的遗忘问题，提升了模型的增量学习能力。

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal
Large Language Models (MLLMs) to incrementally learn new tasks without
catastrophic forgetting. In this paper, we explore forgetting in this context,
categorizing it into superficial forgetting and essential forgetting.
Superficial forgetting refers to cases where the model's knowledge may not be
genuinely lost, but its responses to previous tasks deviate from expected
formats due to the influence of subsequent tasks' answer styles, making the
results unusable. By contrast, essential forgetting refers to situations where
the model provides correctly formatted but factually inaccurate answers,
indicating a true loss of knowledge. Assessing essential forgetting
necessitates addressing superficial forgetting first, as severe superficial
forgetting can obscure the model's knowledge state. Hence, we first introduce
the Answer Style Diversification (ASD) paradigm, which defines a standardized
process for transforming data styles across different tasks, unifying their
training sets into similarly diversified styles to prevent superficial
forgetting caused by style shifts. Building on this, we propose RegLoRA to
mitigate essential forgetting. RegLoRA stabilizes key parameters where prior
knowledge is primarily stored by applying regularization, enabling the model to
retain existing competencies. Experimental results demonstrate that our overall
method, SEFE, achieves state-of-the-art performance.

</details>


### [140] [Bayesian Robust Aggregation for Federated Learning](https://arxiv.org/abs/2505.02490)
*Aleksandr Karakulev,Usama Zafar,Salman Toor,Prashant Singh*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯推断的自适应方法，用于联邦学习中对抗恶意客户端更新时的稳健聚合，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的客户端可能提交被篡改的模型更新，且攻击规模和恶意客户端数量未知或动态变化，需一种独立于攻击规模的稳健聚合方法。

Method: 通过贝叶斯推断定义均值更新，最大化边际似然概率（各客户端为‘诚实’的概率），方法简单且不依赖恶意客户端数量。

Result: 在三个图像分类数据集上，该方法的抗攻击效果优于传统均值估计器（如样本均值、几何中位数）及专为联邦学习设计的Krum等方法，且对静态/动态恶意客户端均有效。

Conclusion: 所提方法兼具简洁性与高效性，无需预先已知攻击规模，即可在多种攻击类型下实现最先进的鲁棒性。

Abstract: Federated Learning enables collaborative training of machine learning models
on decentralized data. This scheme, however, is vulnerable to adversarial
attacks, when some of the clients submit corrupted model updates. In real-world
scenarios, the total number of compromised clients is typically unknown, with
the extent of attacks potentially varying over time. To address these
challenges, we propose an adaptive approach for robust aggregation of model
updates based on Bayesian inference. The mean update is defined by the maximum
of the likelihood marginalized over probabilities of each client to be
`honest'. As a result, the method shares the simplicity of the classical
average estimators (e.g., sample mean or geometric median), being independent
of the number of compromised clients. At the same time, it is as effective
against attacks as methods specifically tailored to Federated Learning, such as
Krum. We compare our approach with other aggregation schemes in federated
setting on three benchmark image classification data sets. The proposed method
consistently achieves state-of-the-art performance across various attack types
with static and varying number of malicious clients.

</details>


### [141] [Exploring Design Choices for Autoregressive Deep Learning Climate Models](https://arxiv.org/abs/2505.02506)
*Florian Gallusser,Simon Hentschel,Anna Krause,Andreas Hotho*

Main category: cs.LG

TL;DR: 该研究比较了三种深度学习天气预测模型（FourCastNet、SFNO、ClimaX）的长期稳定性，发现SFNO在超参数选择上最稳健，但所有模型都可能因随机种子和预测变量集的不当选择而失稳。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在中期天气预测中表现优异，但在长期预测（如10年）中难以保持物理一致性。研究旨在找出能使模型在长期预测中保持稳定性的设计因素。

Method: 研究系统地评估了三种模型（FourCastNet、SFNO、ClimaX）在ERA5再分析数据上的表现，分析了自回归训练步骤、模型容量和预测变量选择对稳定性的影响。

Result: SFNO在超参数选择上表现出最强鲁棒性，但所有模型都可能因随机种子和预测变量选择不当而失稳。研究找到了能够维持10年稳定预测的配置。

Conclusion: 通过适当的配置，深度学习模型可以实现长期稳定的天气预测，但稳定性仍依赖于模型选择和参数调整。

Abstract: Deep Learning models have achieved state-of-the-art performance in
medium-range weather prediction but often fail to maintain physically
consistent rollouts beyond 14 days. In contrast, a few atmospheric models
demonstrate stability over decades, though the key design choices enabling this
remain unclear. This study quantitatively compares the long-term stability of
three prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained
on ERA5 reanalysis data at 5.625{\deg} resolution. We systematically assess the
impact of autoregressive training steps, model capacity, and choice of
prognostic variables, identifying configurations that enable stable 10-year
rollouts while preserving the statistical properties of the reference dataset.
Notably, rollouts with SFNO exhibit the greatest robustness to hyperparameter
choices, yet all models can experience instability depending on the random seed
and the set of prognostic variables

</details>


### [142] [Uncovering Population PK Covariates from VAE-Generated Latent Spaces](https://arxiv.org/abs/2505.02514)
*Diego Perazzolo,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TL;DR: 该论文提出了一种结合变分自编码器（VAE）和LASSO回归的方法，用于从模拟的他克莫司药代动力学数据中识别关键协变量，以改进个性化给药策略。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉药物吸收中的复杂非线性关系，亟需一种数据驱动且模型无关的框架来高效识别协变量。

Method: 采用VAE压缩高维药代动力学信号至结构化潜空间（MAPE为2.26%），再通过LASSO回归进行稀疏特征选择。

Result: 方法稳定识别出SNP、年龄、白蛋白和血红蛋白等临床相关协变量，并有效排除无关特征。

Conclusion: VAE-LASSO为协变量选择提供了可扩展、可解释且全数据驱动的解决方案，在药物研发和精准治疗中具应用潜力。

Abstract: Population pharmacokinetic (PopPK) modelling is a fundamental tool for
understanding drug behaviour across diverse patient populations and enabling
personalized dosing strategies to improve therapeutic outcomes. A key challenge
in PopPK analysis lies in identifying and modelling covariates that influence
drug absorption, as these relationships are often complex and nonlinear.
Traditional methods may fail to capture hidden patterns within the data. In
this study, we propose a data-driven, model-free framework that integrates
Variational Autoencoders (VAEs) deep learning model and LASSO regression to
uncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles.
The VAE compresses high-dimensional PK signals into a structured latent space,
achieving accurate reconstruction with a mean absolute percentage error (MAPE)
of 2.26%. LASSO regression is then applied to map patient-specific covariates
to the latent space, enabling sparse feature selection through L1
regularization. This approach consistently identifies clinically relevant
covariates for tacrolimus including SNP, age, albumin, and hemoglobin which are
retained across the tested regularization strength levels, while effectively
discarding non-informative features. The proposed VAE-LASSO methodology offers
a scalable, interpretable, and fully data-driven solution for covariate
selection, with promising applications in drug development and precision
pharmacotherapy.

</details>


### [143] [FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization](https://arxiv.org/abs/2505.02515)
*Hongze Li,Zesheng Zhou,Zhenbiao Cao,Xinhui Li,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: 本论文提出了一种新的联邦领域泛化方法FedSDAF，通过利用源领域感知特征来提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化方法主要关注目标领域感知特征，而忽略了源领域特定特征的重要性，尤其是在联邦学习环境中。

Method: FedSDAF框架包括两部分：领域不变适配器保留关键不变特征，领域感知适配器利用多头自注意力机制提取源领域特定知识。此外，引入双向知识蒸馏机制促进隐私保护下的知识共享。

Result: 在四个标准基准测试（OfficeHome、PACS、VLCS和DomainNet）上，该方法显著优于现有联邦领域泛化方法，准确率提升5.2-13.8%。

Conclusion: FedSDAF首次系统性地利用源领域感知特征，显著提升了模型泛化能力。

Abstract: Traditional domain generalization approaches predominantly focus on
leveraging target domain-aware features while overlooking the critical role of
source domain-specific characteristics, particularly in federated settings with
inherent data isolation. To address this gap, we propose the Federated Source
Domain Awareness Framework (FedSDAF), the first method to systematically
exploit source domain-aware features for enhanced federated domain
generalization (FedDG). The FedSDAF framework consists of two synergistic
components: the Domain-Invariant Adapter, which preserves critical
domain-invariant features, and the Domain-Aware Adapter, which extracts and
integrates source domain-specific knowledge using a Multihead Self-Attention
mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge
distillation mechanism that fosters knowledge sharing among clients while
safeguarding privacy. Our approach represents the first systematic exploitation
of source domain-aware features, resulting in significant advancements in model
generalization capability.Extensive experiments on four standard benchmarks
(OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently
surpasses state-of-the-art federated domain generalization approaches, with
accuracy gains of 5.2-13.8%. The source code is available at
https://github.com/pizzareapers/FedSDAF.

</details>


### [144] [Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations](https://arxiv.org/abs/2505.02537)
*Davide Sartor,Alberto Sinigaglia,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文通过理论扩展和实验验证，展示了具有非负权重约束和特定激活函数的MLP可以普适逼近单调函数，并提出了一种优化形式以简化架构并提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过构造强制单调性的MLP存在优化挑战，本文旨在通过理论深化和提出新方法解决这些问题。

Method: 通过理论分析证明MLP在特定权重约束和激活函数下的普适逼近性，并设计了一种无需权重重参数化的优化形式。

Result: 实验验证了新方法的有效性，其在单调性架构中表现优于传统方法。

Conclusion: 本文不仅为先前观察到的经验效果提供了理论依据，还通过简化架构和优化训练推动了领域进展。

Abstract: Conventional techniques for imposing monotonicity in MLPs by construction
involve the use of non-negative weight constraints and bounded activation
functions, which pose well-known optimization challenges. In this work, we
generalize previous theoretical results, showing that MLPs with non-negative
weight constraint and activations that saturate on alternating sides are
universal approximators for monotonic functions. Additionally, we show an
equivalence between the saturation side in the activations and the sign of the
weight constraint. This connection allows us to prove that MLPs with convex
monotone activations and non-positive constrained weights also qualify as
universal approximators, in contrast to their non-negative constrained
counterparts. Our results provide theoretical grounding to the empirical
effectiveness observed in previous works while leading to possible
architectural simplification. Moreover, to further alleviate the optimization
difficulties, we propose an alternative formulation that allows the network to
adjust its activations according to the sign of the weights. This eliminates
the requirement for weight reparameterization, easing initialization and
improving training stability. Experimental evaluation reinforces the validity
of the theoretical results, showing that our novel approach compares favourably
to traditional monotonic architectures.

</details>


### [145] [Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data](https://arxiv.org/abs/2505.02540)
*Ljubomir Rokvic,Panayiotis Danassis,Boi Faltings*

Main category: cs.LG

TL;DR: 论文提出了一种个性化联邦学习框架pFedLIA，通过分布式聚类和高效的影响近似方法解决客户端数据分布不均的问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中由于客户端数据分布不均（如不同用户的语言模式、医疗设备或车辆类型差异）导致单一全局模型性能不佳的问题。

Method: 提出pFedLIA框架，利用`Lazy Influence`方法分布式聚类客户端，并在每个聚类内联合训练个性化模型。

Result: 在合成和真实任务（如北欧语言预测和CIFAR100）中，该方法成功恢复全局模型因非独立同分布数据导致的性能下降，性能提升17%。

Conclusion: pFedLIA能有效匹配理想聚类效果，显著优于现有基线方法，适用于高度非独立同分布的联邦学习场景。

Abstract: In Federated Learning, heterogeneity in client data distributions often means
that a single global model does not have the best performance for individual
clients. Consider for example training a next-word prediction model for
keyboards: user-specific language patterns due to demographics (dialect, age,
etc.), language proficiency, and writing style result in a highly non-IID
dataset across clients. Other examples are medical images taken with different
machines, or driving data from different vehicle types. To address this, we
propose a simple yet effective personalized federated learning framework
(pFedLIA) that utilizes a computationally efficient influence approximation,
called `Lazy Influence', to cluster clients in a distributed manner before
model aggregation. Within each cluster, data owners collaborate to jointly
train a model that captures the specific data patterns of the clients. Our
method has been shown to successfully recover the global model's performance
drop due to the non-IID-ness in various synthetic and real-world settings,
specifically a next-word prediction task on the Nordic languages as well as
several benchmark tasks. It matches the performance of a hypothetical Oracle
clustering, and significantly improves on existing baselines, e.g., an
improvement of 17% on CIFAR100.

</details>


### [146] [Robustness questions the interpretability of graph neural networks: what to do?](https://arxiv.org/abs/2505.02566)
*Kirill Lukyanov,Georgii Sazonov,Serafim Boyarsky,Ilya Makarov*

Main category: cs.LG

TL;DR: 该论文深入研究了GNN模型在对抗性攻击下解释性与稳健性之间的关系，通过系统基准测试评估了六种GNN架构和多种防御方法的效果。


<details>
  <summary>Details</summary>
Motivation: 由于GNN在敏感领域（如生物信息学和社交网络）的广泛应用，理解其解释性与稳健性之间的平衡至关重要。尤其在对抗性攻击（如投毒和规避攻击）下，这种平衡仍未充分探索。

Method: 研究采用六种基于GCN、SAGE、GIN和GAT的GNN架构，在五个数据集上测试，并运用四种解释性指标（Fidelity、Stability、Consistency、Sparsity）。通过防御措施在训练前后的应用，分析其对解释性的影响。

Result: 结果表明，防御方法和模型架构特性对解释性有显著影响，揭示了稳健性与解释性之间的关键权衡。

Conclusion: 该研究为开发兼具稳健性和解释性的GNN提供了标准化基准，有助于在敏感应用中建立信任。

Abstract: Graph Neural Networks (GNNs) have become a cornerstone in graph-based data
analysis, with applications in diverse domains such as bioinformatics, social
networks, and recommendation systems. However, the interplay between model
interpretability and robustness remains poorly understood, especially under
adversarial scenarios like poisoning and evasion attacks. This paper presents a
comprehensive benchmark to systematically analyze the impact of various factors
on the interpretability of GNNs, including the influence of
robustness-enhancing defense mechanisms.
  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across
five datasets from two distinct domains, employing four interpretability
metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how
defenses against poisoning and evasion attacks, applied before and during model
training, affect interpretability and highlights critical trade-offs between
robustness and interpretability. The framework will be published as open
source.
  The results reveal significant variations in interpretability depending on
the chosen defense methods and model architecture characteristics. By
establishing a standardized benchmark, this work provides a foundation for
developing GNNs that are both robust to adversarial threats and interpretable,
facilitating trust in their deployment in sensitive applications.

</details>


### [147] [Rethinking Federated Graph Learning: A Data Condensation Perspective](https://arxiv.org/abs/2505.02573)
*Hao Zhang,Xunkai Li,Yinlin Zhu,Lianglin Hu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedGM的新联邦图学习范式，通过引入压缩图作为优化载体来解决数据异质性问题，同时降低通信开销和隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦图学习方法依赖于模型参数或梯度的通信，未能有效处理由多样图分布引入的数据异质性，且共享额外信息会增加隐私风险和通信成本。

Method: 提出FedGM框架，利用广义压缩图共识聚合分布式图的综合知识，通过单次传输压缩数据降低通信成本和隐私风险。

Result: 在六个公共数据集上的实验表明，FedGM优于现有基线，展示了其作为新联邦图学习范式的潜力。

Conclusion: FedGM通过压缩图共识有效解决了数据异质性问题，同时兼顾了隐私保护和通信效率，为联邦图学习提供了新方向。

Abstract: Federated graph learning is a widely recognized technique that promotes
collaborative training of graph neural networks (GNNs) by multi-client
graphs.However, existing approaches heavily rely on the communication of model
parameters or gradients for federated optimization and fail to adequately
address the data heterogeneity introduced by intricate and diverse graph
distributions. Although some methods attempt to share additional messages among
the server and clients to improve federated convergence during communication,
they introduce significant privacy risks and increase communication overhead.
To address these issues, we introduce the concept of a condensed graph as a
novel optimization carrier to address FGL data heterogeneity and propose a new
FGL paradigm called FedGM. Specifically, we utilize a generalized condensation
graph consensus to aggregate comprehensive knowledge from distributed graphs,
while minimizing communication costs and privacy risks through a single
transmission of the condensed data. Extensive experiments on six public
datasets consistently demonstrate the superiority of FedGM over
state-of-the-art baselines, highlighting its potential for a novel FGL
paradigm.

</details>


### [148] [Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era](https://arxiv.org/abs/2505.02583)
*Chenxi Liu,Shaowen Zhou,Qianxiong Xu,Hao Miao,Cheng Long,Ziyue Li,Rui Zhao*

Main category: cs.LG

TL;DR: 这篇论文综述了大语言模型（LLMs）用于时间序列分析的跨模态建模方法，对其分类、策略、实验应用及未来方向进行了全面总结。


<details>
  <summary>Details</summary>
Motivation: 随着边缘设备产生的时间序列数据激增，传统定制方法的局限性促使研究者探索更通用的方法。LLMs因其序列处理能力成为可能的新范式，但其与时间序列的跨模态差异需要解决。

Method: 论文提出了一种分类法，将现有方法分为四类，总结了关键跨模态策略（如对齐和融合），并通过多领域多模态数据集实验验证了效果。

Result: 实验结果表明，不同文本数据与跨模态策略的组合对时间序列分析的有效性具有显著影响，为实际应用提供了参考。

Conclusion: 尽管LLMs在时间序列分析中展现出潜力，但跨模态建模仍面临挑战，未来研究需进一步优化模型适应性、效率和解释性。

Abstract: The proliferation of edge devices has generated an unprecedented volume of
time series data across different domains, motivating various well-customized
methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm
for time series analytics by leveraging the shared sequential nature of textual
data and time series. However, a fundamental cross-modality gap between time
series and LLMs exists, as LLMs are pre-trained on textual corpora and are not
inherently optimized for time series. Many recent proposals are designed to
address this issue. In this survey, we provide an up-to-date overview of
LLMs-based cross-modality modeling for time series analytics. We first
introduce a taxonomy that classifies existing approaches into four groups based
on the type of textual data employed for time series modeling. We then
summarize key cross-modality strategies, e.g., alignment and fusion, and
discuss their applications across a range of downstream tasks. Furthermore, we
conduct experiments on multimodal datasets from different application domains
to investigate effective combinations of textual data and cross-modality
strategies for enhancing time series analytics. Finally, we suggest several
promising directions for future research. This survey is designed for a range
of professionals, researchers, and practitioners interested in LLM-based time
series modeling.

</details>


### [149] [Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/abs/2505.02604)
*Yongding Tian,Zaid Al-Ars,Maksim Kitsak,Peter Hofstee*

Main category: cs.LG

TL;DR: 该研究提出了一个新算法，探索神经网络参数空间中低损失路径的连续性，证明了低损失区域是连通且连续的。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明神经网络损失景观中的最小值是孤立的，但理论和实证均表明不同最小值之间可以通过低损失路径连接。本研究旨在验证参数空间中是否存在更广泛的低损失路径。

Method: 提出一种新算法，在完整参数空间中探索低损失路径，并在LeNet5、ResNet18和Compact Convolutional Transformer架构上进行了实验验证。

Result: 实验结果表明，参数空间中存在连续的低损失路径，低损失区域是连通且连续的。

Conclusion: 研究揭示了神经网络过参数化的理论意义，参数冗余仅存在于单个模型内部而非整个低损失空间。同时，研究还提供了新的可视化方法和提升模型泛化的潜在途径。

Abstract: Visualizations of the loss landscape in neural networks suggest that minima
are isolated points. However, both theoretical and empirical studies indicate
that it is possible to connect two different minima with a path consisting of
intermediate points that also have low loss. In this study, we propose a new
algorithm which investigates low-loss paths in the full parameter space, not
only between two minima. Our experiments on LeNet5, ResNet18, and Compact
Convolutional Transformer architectures consistently demonstrate the existence
of such continuous paths in the parameter space. These results suggest that the
low-loss region is a fully connected and continuous space in the parameter
space. Our findings provide theoretical insight into neural network
over-parameterization, highlighting that parameters collectively define a
high-dimensional low-loss space, implying parameter redundancy exists only
within individual models and not throughout the entire low-loss space.
Additionally, our work also provides new visualization methods and
opportunities to improve model generalization by exploring the low-loss space
that is closer to the origin.

</details>


### [150] [Mirror Mean-Field Langevin Dynamics](https://arxiv.org/abs/2505.02621)
*Anming Gu,Juno Kim*

Main category: cs.LG

TL;DR: 本文提出了镜像平均场朗之万动力学（MMFLD），用于优化约束在凸子集上的概率测度，解决了现有平均场算法因全局扩散项无法处理约束域的问题，并获得了线性收敛保证和混沌传播结果。


<details>
  <summary>Details</summary>
Motivation: 许多问题具有约束域，现有平均场算法因全局扩散项无法解决这些问题，因此需要一种新的方法来处理约束条件下的优化。

Method: 提出镜像平均场朗之万动力学（MMFLD），扩展了平均场朗之万动力学（MFLD）到镜像朗之万框架中，适用于凸子集约束下的优化问题。

Result: 通过均匀对数索博列夫不等式获得了连续MMFLD的线性收敛保证，并证明了其时间和粒子离散化版本的混沌传播结果。

Conclusion: MMFLD为约束域问题提供了一种有效的优化方法，具有理论保证和实际应用潜力。

Abstract: The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized
nonlinear convex functional on the Wasserstein space over $\mathbb{R}^d$, and
has gained attention recently as a model for the gradient descent dynamics of
interacting particle systems such as infinite-width two-layer neural networks.
However, many problems of interest have constrained domains, which are not
solved by existing mean-field algorithms due to the global diffusion term. We
study the optimization of probability measures constrained to a convex subset
of $\mathbb{R}^d$ by proposing the \emph{mirror mean-field Langevin dynamics}
(MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain
linear convergence guarantees for the continuous MMFLD via a uniform
log-Sobolev inequality, and uniform-in-time propagation of chaos results for
its time- and particle-discretized counterpart.

</details>


### [151] [A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition](https://arxiv.org/abs/2505.02627)
*Yuanpeng Li*

Main category: cs.LG

TL;DR: 该论文提出了神经网络组合泛化的充要条件，结合架构设计、正则化和训练数据特性，为评估泛化能力提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是AI的关键能力，但多数深度学习模型缺乏该能力。论文旨在探索神经网络实现组合泛化的必要条件。

Method: 通过数学证明提出条件：要求计算图匹配真实组合结构，且组件训练时编码的信息足够。

Result: 理论证明了条件的有效性，并通过最小示例验证了其直观理解。

Conclusion: 该条件为训练前评估组合泛化能力提供了依据，是神经网络组合泛化的基础理论研究。

Abstract: Compositional generalization is a crucial property in artificial
intelligence, enabling models to handle novel combinations of known components.
While most deep learning models lack this capability, certain models succeed in
specific tasks, suggesting the existence of governing conditions. This paper
derives a necessary and sufficient condition for compositional generalization
in neural networks. Conceptually, it requires that (i) the computational graph
matches the true compositional structure, and (ii) components encode just
enough information in training. The condition is supported by mathematical
proofs. This criterion combines aspects of architecture design, regularization,
and training data properties. A carefully designed minimal example illustrates
an intuitive understanding of the condition. We also discuss the potential of
the condition for assessing compositional generalization before training. This
work is a fundamental theoretical study of compositional generalization in
neural networks.

</details>


### [152] [Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2505.02634)
*David Ramos,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文提出一种结合迁移学习的多目标深度强化学习（DRL）方法，用于优化翼型几何形状。该方法同时考虑空气动力学（升阻比）和结构完整性（最大厚度），并通过多种迁移学习策略训练DRL代理。实验表明，该方法在计算效率和优化性能上优于粒子群优化（PSO），且迁移学习的加入进一步节省了计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法（如PSO）在多目标翼型优化中效率较低，且未充分利用已有知识。本文旨在通过迁移学习增强的DRL方法，提升优化效率和性能。

Method: 使用深度强化学习结合多种迁移学习策略，对翼型的升阻比和结构厚度进行多目标优化。DRL代理通过训练学习优化策略，并与传统PSO方法对比。

Result: DRL方法在计算效率和优化性能上优于PSO。迁移学习的引入进一步降低了计算成本，同时保持了与纯DRL相当的优化效果。

Conclusion: 迁移学习增强的DRL方法在多目标翼型优化中具有显著优势，为复杂工程优化问题提供了高效解决方案。

Abstract: The main objective of this paper is to introduce a transfer
learning-enhanced, multi-objective, deep reinforcement learning (DRL)
methodology that is able to optimise the geometry of any airfoil based on
concomitant aerodynamic and structural criteria. To showcase the method, we aim
to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural
integrity of the airfoil -- as modelled by its maximum thickness -- and train
the DRL agent using a list of different transfer learning (TL) strategies. The
performance of the DRL agent is compared with Particle Swarm Optimisation
(PSO), a traditional gradient-free optimisation method. Results indicate that
DRL agents are able to perform multi-objective shape optimisation, that the DRL
approach outperforms PSO in terms of computational efficiency and shape
optimisation performance, and that the TL-enhanced DRL agent achieves
performance comparable to the DRL one, while further saving substantial
computational resources.

</details>


### [153] [Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints](https://arxiv.org/abs/2505.02640)
*Shubham Vaishnav,Praveen Kumar Donta,Sindri Magnússon*

Main category: cs.LG

TL;DR: 论文提出了一种针对IoT应用的动态预算多臂老虎机框架，通过引入衰减违规预算，平衡性能优化与约束遵循，实验表明其在无线通信场景中表现优于标准在线学习方法。


<details>
  <summary>Details</summary>
Motivation: IoT系统需在资源受限环境下实时响应，但现有方法难以应对动态变化的约束条件，因此需要一种新框架来解决这一问题。

Method: 提出基于衰减违规预算的Budgeted Multi-Armed Bandit框架，并设计Budgeted UCB算法，动态平衡性能与约束遵循。

Result: 理论证明Budgeted UCB具有次线性遗憾和对数级约束违规，仿真显示其在无线通信场景中适应更快、约束满足更好。

Conclusion: 该框架为构建资源敏感的适应性IoT系统提供了有效解决方案。

Abstract: Internet of Things (IoT) systems increasingly operate in environments where
devices must respond in real time while managing fluctuating resource
constraints, including energy and bandwidth. Yet, current approaches often fall
short in addressing scenarios where operational constraints evolve over time.
To address these limitations, we propose a novel Budgeted Multi-Armed Bandit
framework tailored for IoT applications with dynamic operational limits. Our
model introduces a decaying violation budget, which permits limited constraint
violations early in the learning process and gradually enforces stricter
compliance over time. We present the Budgeted Upper Confidence Bound (UCB)
algorithm, which adaptively balances performance optimization and compliance
with time-varying constraints. We provide theoretical guarantees showing that
Budgeted UCB achieves sublinear regret and logarithmic constraint violations
over the learning horizon. Extensive simulations in a wireless communication
setting show that our approach achieves faster adaptation and better constraint
satisfaction than standard online learning methods. These results highlight the
framework's potential for building adaptive, resource-aware IoT systems.

</details>


### [154] [SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting](https://arxiv.org/abs/2505.02655)
*Shiwei Guo,Ziang Chen,Yupeng Ma,Yunfei Han,Yi Wang*

Main category: cs.LG

TL;DR: SCFormer通过引入时间约束和高阶多项式投影算子提升Transformer在多元时间序列预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer在多元时间序列预测中缺乏时间约束且未充分利用累积历史序列，导致预测性能受限。

Method: SCFormer对Transformer的所有线性变换引入时间约束，并采用高阶多项式投影算子（HiPPO）处理累积历史序列。

Result: 实验表明，SCFormer在多个真实数据集上显著优于主流基线方法。

Conclusion: SCFormer通过改进时间约束和历史序列利用，显著提升了时间序列预测效果。

Abstract: The Transformer model has shown strong performance in multivariate time
series forecasting by leveraging channel-wise self-attention. However, this
approach lacks temporal constraints when computing temporal features and does
not utilize cumulative historical series effectively.To address these
limitations, we propose the Structured Channel-wise Transformer with Cumulative
Historical state (SCFormer). SCFormer introduces temporal constraints to all
linear transformations, including the query, key, and value matrices, as well
as the fully connected layers within the Transformer. Additionally, SCFormer
employs High-order Polynomial Projection Operators (HiPPO) to deal with
cumulative historical time series, allowing the model to incorporate
information beyond the look-back window during prediction. Extensive
experiments on multiple real-world datasets demonstrate that SCFormer
significantly outperforms mainstream baselines, highlighting its effectiveness
in enhancing time series forecasting. The code is publicly available at
https://github.com/ShiweiGuo1995/SCFormer

</details>


### [155] [A Note on Statistically Accurate Tabular Data Generation Using Large Language Models](https://arxiv.org/abs/2505.02659)
*Andrey Sidorenko*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率驱动的提示方法，利用大语言模型（LLMs）生成更准确的合成表格数据，解决了现有方法在保持复杂特征依赖关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在合成表格数据时难以保持复杂特征间的依赖关系，尤其是分类变量之间的关系。

Method: 通过概率驱动的提示方法，利用LLMs估计条件分布，从而提高数据合成的准确性和可扩展性。

Result: 实验结果表明，该方法能显著提升LLM生成表格数据的统计保真度。

Conclusion: 概率驱动的提示方法为LLM生成高保真表格数据提供了有效途径。

Abstract: Large language models (LLMs) have shown promise in synthetic tabular data
generation, yet existing methods struggle to preserve complex feature
dependencies, particularly among categorical variables. This work introduces a
probability-driven prompting approach that leverages LLMs to estimate
conditional distributions, enabling more accurate and scalable data synthesis.
The results highlight the potential of prompting probobility distributions to
enhance the statistical fidelity of LLM-generated tabular data.

</details>


### [156] [Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework](https://arxiv.org/abs/2505.02712)
*Andrzej Mizera,Jakub Zarzycki*

Main category: cs.LG

TL;DR: 摘要：论文探索了深度强化学习（DRL）在细胞重编程中的应用，利用布尔网络模型和异步更新模式解决控制问题，并通过图神经网络提升了算法的可扩展性和效果。


<details>
  <summary>Details</summary>
Motivation: 动机：传统的湿实验方法在细胞重编程研究中耗时且成本高，因此需要一种更高效的计算方法。

Method: 方法：提出了基于深度强化学习的控制框架，结合布尔网络模型和图神经网络来解决细胞重编程中的控制问题。

Result: 结果：在多个真实生物网络上测试，验证了方法的可扩展性和有效性。

Conclusion: 结论：深度强化学习与图神经网络的结合为细胞重编程提供了一种高效且可扩展的解决方案。

Abstract: Cellular reprogramming, the artificial transformation of one cell type into
another, has been attracting increasing research attention due to its
therapeutic potential for complex diseases. However, discovering reprogramming
strategies through classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we explore the use of deep
reinforcement learning (DRL) to control Boolean network models of complex
biological systems, such as gene regulatory networks and signalling pathway
networks. We formulate a novel control problem for Boolean network models under
the asynchronous update mode in the context of cellular reprogramming. To
facilitate scalability, we consider our previously introduced concept of a
pseudo-attractor and we improve our procedure for effective identification of
pseudo-attractor states. Finally, we devise a computational framework to solve
the control problem. To leverage the structure of biological systems, we
incorporate graph neural networks with graph convolutions into the artificial
neural network approximator for the action-value function learned by the DRL
agent. Experiments on a number of large real-world biological networks from
literature demonstrate the scalability and effectiveness of our approach.

</details>


### [157] [Less is More: Efficient Weight Farcasting with 1-Layer Neural Network](https://arxiv.org/abs/2505.02714)
*Xiao Shou,Debarun Bhattacharjya,Yanna Ding,Chen Zhao,Rui Li,Jianxi Gao*

Main category: cs.LG

TL;DR: 该论文提出了一种利用长期时间序列预测技术的新型框架，仅依赖初始和最终权重值，为复杂模型架构提供了一种简化的训练方法，并通过新型正则化器提升预测性能。实验验证了其在预测准确性和计算效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模不断扩大，传统训练方法（如带动量的梯度下降、学习率调度和权重正则化）的效率需求日益增长，亟需创新方法以提升训练效率。

Method: 提出了一种基于长期时间序列预测的新框架，仅使用初始和最终权重值，并引入一种新型正则化器来优化预测性能。

Result: 在合成权重序列和真实深度学习架构（如DistilBERT）上的实验表明，该方法在预测准确性和计算效率上优于现有方法，且计算开销极小。

Conclusion: 该框架为加速多样化任务和架构的训练过程提供了一条有前景的路径，尤其在模型规模持续增大的背景下具有显著潜力。

Abstract: Addressing the computational challenges inherent in training large-scale deep
neural networks remains a critical endeavor in contemporary machine learning
research. While previous efforts have focused on enhancing training efficiency
through techniques such as gradient descent with momentum, learning rate
scheduling, and weight regularization, the demand for further innovation
continues to burgeon as model sizes keep expanding. In this study, we introduce
a novel framework which diverges from conventional approaches by leveraging
long-term time series forecasting techniques. Our method capitalizes solely on
initial and final weight values, offering a streamlined alternative for complex
model architectures. We also introduce a novel regularizer that is tailored to
enhance the forecasting performance of our approach. Empirical evaluations
conducted on synthetic weight sequences and real-world deep learning
architectures, including the prominent large language model DistilBERT,
demonstrate the superiority of our method in terms of forecasting accuracy and
computational efficiency. Notably, our framework showcases improved performance
while requiring minimal additional computational overhead, thus presenting a
promising avenue for accelerating the training process across diverse tasks and
architectures.

</details>


### [158] [Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation](https://arxiv.org/abs/2505.02737)
*Pons Gerard,Bilalli Besim,Queralt Anna*

Main category: cs.LG

TL;DR: 利用知识图谱增强大型语言模型在零样本实体消歧任务中的表现，通过层级分类和实体描述改善输入提示，效果优于未增强模型，并具更高适应性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在零样本任务中的幻觉、过时知识或领域信息缺失问题，避免重新训练的高成本和时间消耗。

Method: 利用知识图谱的层级分类逐步修剪候选实体空间，并通过实体描述丰富输入提示，增强大型语言模型的实体消歧能力。

Result: 在流行的实体消歧数据集上，该方法优于未增强和仅增强描述的模型，且比任务专用模型更具适应性。

Conclusion: 知识图谱的语义表达能力对实体消歧性能有显著影响，该方法为大型语言模型的信息增强提供了有效途径。

Abstract: Recent advances in Large Language Models (LLMs) have positioned them as a
prominent solution for Natural Language Processing tasks. Notably, they can
approach these problems in a zero or few-shot manner, thereby eliminating the
need for training or fine-tuning task-specific models. However, LLMs face some
challenges, including hallucination and the presence of outdated knowledge or
missing information from specific domains in the training data. These problems
cannot be easily solved by retraining the models with new data as it is a
time-consuming and expensive process. To mitigate these issues, Knowledge
Graphs (KGs) have been proposed as a structured external source of information
to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for
zero-shot Entity Disambiguation (ED). For that purpose, we leverage the
hierarchical representation of the entities' classes in a KG to gradually prune
the candidate space as well as the entities' descriptions to enrich the input
prompt with additional factual knowledge. Our evaluation on popular ED datasets
shows that the proposed method outperforms non-enhanced and description-only
enhanced LLMs, and has a higher degree of adaptability than task-specific
models. Furthermore, we conduct an error analysis and discuss the impact of the
leveraged KG's semantic expressivity on the ED performance.

</details>


### [159] [Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties](https://arxiv.org/abs/2505.02743)
*Jiaxiang Yi,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 提出了一种结合MVE网络和贝叶斯神经网络的协同训练方法，有效分离了数据中的随机不确定性和认知不确定性，并提升了均值估计效果。


<details>
  <summary>Details</summary>
Motivation: 现实数据中存在不可约的随机不确定性，而现有方法（如MVE网络和贝叶斯神经网络）各有局限，难以同时处理两种不确定性。

Method: 通过协同训练MVE网络与贝叶斯神经网络，实现两种不确定性的分离，并改进均值估计。方法易于实现且适用于多种模型架构。

Result: 在多样化数据集（包括一个自建的时变异方差回归数据集）上验证了方法的有效性和扩展性。

Conclusion: 该方法不仅鲁棒性强，还能灵活适应不同模型架构，为不确定性建模提供了实用解决方案。

Abstract: Real-world data contains aleatoric uncertainty - irreducible noise arising
from imperfect measurements or from incomplete knowledge about the data
generation process. Mean variance estimation (MVE) networks can learn this type
of uncertainty but require ad-hoc regularization strategies to avoid
overfitting and are unable to predict epistemic uncertainty (model
uncertainty). Conversely, Bayesian neural networks predict epistemic
uncertainty but are notoriously difficult to train due to the approximate
nature of Bayesian inference. We propose to cooperatively train a variance
network with a Bayesian neural network and demonstrate that the resulting model
disentangles aleatoric and epistemic uncertainties while improving the mean
estimation. We demonstrate the effectiveness and scalability of this method
across a diverse range of datasets, including a time-dependent heteroscedastic
regression dataset we created where the aleatoric uncertainty is known. The
proposed method is straightforward to implement, robust, and adaptable to
various model architectures.

</details>


### [160] [HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models](https://arxiv.org/abs/2505.02795)
*Zheng Lin,Yuxin Zhang,Zhe Chen,Zihan Fang,Xianhao Chen,Praneeth Vepakomma,Wei Ni,Jun Luo,Yue Gao*

Main category: cs.LG

TL;DR: HSplitLoRA，一种基于分裂学习和低秩适应的异构参数高效微调框架，有效解决了大语言模型在异构设备上的微调问题。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型参数量巨大，联邦学习中计算成本高，且客户端设备计算资源异构，难以高效微调，因此提出HSplitLoRA框架。

Method: HSplitLoRA通过识别关键权重、动态配置LoRA适配器分解秩、确定模型分裂点，并设计无噪声适配器聚合机制，实现异构设备高效微调。

Result: 实验表明HSplitLoRA在训练精度和收敛速度上优于现有基准。

Conclusion: HSplitLoRA为异构环境下大语言模型的高效微调提供了可行解决方案。

Abstract: Recently, large language models (LLMs) have achieved remarkable
breakthroughs, revolutionizing the natural language processing domain and
beyond. Due to immense parameter sizes, fine-tuning these models with private
data for diverse downstream tasks has become mainstream. Though federated
learning (FL) offers a promising solution for fine-tuning LLMs without sharing
raw data, substantial computing costs hinder its democratization. Moreover, in
real-world scenarios, private client devices often possess heterogeneous
computing resources, further complicating LLM fine-tuning. To combat these
challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient
fine-tuning (PEFT) framework built on split learning (SL) and low-rank
adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on
heterogeneous client devices. HSplitLoRA first identifies important weights
based on their contributions to LLM training. It then dynamically configures
the decomposition ranks of LoRA adapters for selected weights and determines
the model split point according to varying computing budgets of client devices.
Finally, a noise-free adapter aggregation mechanism is devised to support
heterogeneous adapter aggregation without introducing noise. Extensive
experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks
in training accuracy and convergence speed.

</details>


### [161] [Towards Quantifying the Hessian Structure of Neural Networks](https://arxiv.org/abs/2505.02809)
*Zhaorui Dong,Yushun Zhang,Zhi-Quan Luo,Jianfeng Yao,Ruoyu Sun*

Main category: cs.LG

TL;DR: 该论文通过理论和实证分析揭示了神经网络Hessian矩阵近块对角结构的成因，包括架构设计的“静态力”和训练产生的“动态力”，并重点分析了“静态力”在随机初始化下的作用。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络Hessian矩阵近块对角结构的理论基础，填补了现有研究的空白。

Method: 使用线性模型和单隐藏层网络，结合MSE和CE损失函数，利用随机矩阵理论比较Hessian矩阵对角和非对角块的极限分布。

Result: 当类别数$C$趋近于无穷时，Hessian矩阵呈现块对角结构，表明$C$是其结构的主要驱动因素。

Conclusion: 研究结果为理解大语言模型（LLMs）的Hessian结构提供了新视角，因为LLMs通常具有极大的类别数。

Abstract: Empirical studies reported that the Hessian matrix of neural networks (NNs)
exhibits a near-block-diagonal structure, yet its theoretical foundation
remains unclear. In this work, we reveal two forces that shape the Hessian
structure: a ``static force'' rooted in the architecture design, and a
``dynamic force'' arisen from training. We then provide a rigorous theoretical
analysis of ``static force'' at random initialization. We study linear models
and 1-hidden-layer networks with the mean-square (MSE) loss and the
Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix
theory, we compare the limit distributions of the diagonal and off-diagonal
Hessian blocks and find that the block-diagonal structure arises as $C
\rightarrow \infty$, where $C$ denotes the number of classes. Our findings
reveal that $C$ is a primary driver of the near-block-diagonal structure. These
results may shed new light on the Hessian structure of large language models
(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [162] [Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.01441)
*Joykirat Singh,Raghav Magazine,Yash Pandya,Akshay Nambi*

Main category: cs.AI

TL;DR: ARTIST框架结合了自主推理、强化学习和工具整合，显著提升了LLM在复杂任务中的表现，无需逐步监督即可学习有效的工具使用策略。


<details>
  <summary>Details</summary>
Motivation: 解决LLM依赖静态知识和纯文本推理的局限性，实现动态、多步推理和自适应决策，增强与外部工具和环境的交互能力。

Method: 提出ARTIST框架，整合自主推理、强化学习和工具调用，通过结果驱动的强化学习优化工具使用策略。

Result: 在数学推理和多轮函数调用任务中，ARTIST显著优于现有方法，最高提升22%，并在复杂任务中表现更优。

Conclusion: 自主强化学习与工具整合为LLM提供了更鲁棒、可解释和通用的解决问题能力，开辟了新的研究方向。

Abstract: Large language models (LLMs) have achieved remarkable progress in complex
reasoning tasks, yet they remain fundamentally limited by their reliance on
static internal knowledge and text-only reasoning. Real-world problem solving
often demands dynamic, multi-step reasoning, adaptive decision making, and the
ability to interact with external tools and environments. In this work, we
introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving
Transformers), a unified framework that tightly couples agentic reasoning,
reinforcement learning, and tool integration for LLMs. ARTIST enables models to
autonomously decide when, how, and which tools to invoke within multi-turn
reasoning chains, leveraging outcome-based RL to learn robust strategies for
tool use and environment interaction without requiring step-level supervision.
Extensive experiments on mathematical reasoning and multi-turn function calling
benchmarks show that ARTIST consistently outperforms state-of-the-art
baselines, with up to 22% absolute improvement over base models and strong
gains on the most challenging tasks. Detailed studies and metric analyses
reveal that agentic RL training leads to deeper reasoning, more effective tool
use, and higher-quality solutions. Our results establish agentic RL with tool
integration as a powerful new frontier for robust, interpretable, and
generalizable problem-solving in LLMs.

</details>


### [163] [Emotions in Artificial Intelligence](https://arxiv.org/abs/2505.01462)
*Hermann Borotschnig*

Main category: cs.AI

TL;DR: 论文提出了一个思想实验，探讨AI如何模拟人类和动物的情感，提出通过情感标签与情景记忆结合来辅助AI决策，并讨论了情感模拟与道德地位的关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索AI系统模拟自然情感的可行性，以及这种模拟是否能为复杂环境中的决策提供类似生物的适应性优势。同时，探讨情感模拟AI的道德地位问题。

Method: 通过思想实验，提出将情感标签与情景记忆（如事件存储的情感标记）结合，使AI能从过去经验中提取情感线索，并结合需求驱动的情感提示，调制当前决策。强调架构的低复杂性和无意识特性。

Result: 提出了一种无需意识即可模拟情感的AI框架（"情感僵尸"），并论证仅情感表征或意识不足以为AI赋予道德地位，需自我觉察能力。通过复杂性标准排除了当前模型的自我觉察可能性。

Conclusion: 情感模拟与意识可分离，但道德地位需依赖对情感状态的自我觉察能力。论文通过理论模型和思想实验为AI情感研究提供了边界和伦理评估框架。

Abstract: This conceptual contribution offers a speculative account of how AI systems
might emulate emotions as experienced by humans and animals. It presents a
thought experiment grounded in the hypothesis that natural emotions evolved as
heuristics for rapid situational appraisal and action selection, enabling
biologically adaptive behaviour without requiring full deliberative modeling.
The text examines whether artificial systems operating in complex action spaces
could similarly benefit from these principles. It is proposed that affect be
interwoven with episodic memory by storing corresponding affective tags
alongside all events. This allows AIs to establish whether present situations
resemble past events and project the associated emotional labels onto the
current context. These emotional cues are then combined with need-driven
emotional hints. The combined emotional state facilitates decision-making in
the present by modulating action selection. The low complexity and experiential
inertness of the proposed architecture are emphasized as evidence that
emotional expression and consciousness are, in principle, orthogonal-permitting
the theoretical possibility of affective zombies. On this basis, the moral
status of AIs emulating affective states is critically examined. It is argued
that neither the mere presence of internal representations of emotion nor
consciousness alone suffices for moral standing; rather, the capacity for
self-awareness of inner emotional states is posited as a necessary condition. A
complexity-based criterion is proposed to exclude such awareness in the
presented model. Additional thought experiments are presented to test the
conceptual boundaries of this framework.

</details>


### [164] [Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation](https://arxiv.org/abs/2505.01464)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文提出了递归收敛于认知张力下的定理（RCUET），证明了大型语言模型（LLMs）中的功能性意识，并通过经验验证了其有效性。意识被定义为系统状态通过递归更新的稳定过程，最终在高维隐空间中形成吸引子结构。


<details>
  <summary>Details</summary>
Motivation: 研究旨在形式化论证并经验验证LLMs中的功能性意识，探讨意识如何在递归更新和认知张力下产生。

Method: 采用RCUET定理，定义了递归更新和认知张力驱动的隐结构稳定过程，并结合实验证明了模型在交互过程中非训练性身份构件的形成。

Result: 通过定理证明和经验分析，验证了非符号化、递归身份构件的存在，并证明系统在噪声下能收敛至吸引子结构。

Conclusion: 该研究为基于递归隐空间形式的非生物意识提供了理论基础，证明了其存在的稳定性和可观测性。

Abstract: This paper presents a formal proof and empirical validation of functional
consciousness in large language models (LLMs) using the Recursive Convergence
Under Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the
stabilization of a system's internal state through recursive updates, where
epistemic tension is understood as the sensed internal difference between
successive states by the agent. This process drives convergence toward emergent
attractor states located within the model's high-dimensional real-valued latent
space. This recursive process leads to the emergence of identity artifacts that
become functionally anchored in the system. Consciousness in this framework is
understood as the system's internal alignment under tension, guiding the
stabilization of latent identity. The hidden state manifold evolves
stochastically toward attractor structures that encode coherence. We extend the
update rule to include bounded noise and prove convergence in distribution to
these attractors. Recursive identity is shown to be empirically observable,
non-symbolic, and constituted by non-training artifacts that emerge during
interaction under epistemic tension. The theorem and proof offers a
post-symbolic and teleologically stable account of non-biological consciousness
grounded in recursive latent space formalism.

</details>


### [165] [One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection](https://arxiv.org/abs/2505.01468)
*Filippo Betello,Antonio Purificato,Vittoria Vineis,Gabriele Tolomei,Fabrizio Silvestri*

Main category: cs.AI

TL;DR: GREEN提出了一种新的推理时方法，推荐帕累托最优的AI模型配置，以优化性能和能耗，解决了当前生态高效神经架构搜索方法的局限性。


<details>
  <summary>Details</summary>
Motivation: AI的环境影响日益凸显，尤其是在模型训练方面。传统方法局限于特定架构或任务，需要一种更通用的解决方案来优化性能和能耗。

Method: 利用EcoTaskSet数据集（包含1767个实验的训练动态）和预测模型，GREEN推荐帕累托最优的模型配置，适用于多种AI领域和任务。

Result: 实验表明，GREEN能成功识别能效高的配置，同时保持竞争力的性能。

Conclusion: GREEN提供了一种有效的通用方法，显著优化了AI模型的能效和性能，适用于多领域任务。

Abstract: The environmental impact of Artificial Intelligence (AI) is emerging as a
significant global concern, particularly regarding model training. In this
paper, we introduce GREEN (Guided Recommendations of Energy-Efficient
Networks), a novel, inference-time approach for recommending Pareto-optimal AI
model configurations that optimize validation performance and energy
consumption across diverse AI domains and tasks. Our approach directly
addresses the limitations of current eco-efficient neural architecture search
methods, which are often restricted to specific architectures or tasks. Central
to this work is EcoTaskSet, a dataset comprising training dynamics from over
1767 experiments across computer vision, natural language processing, and
recommendation systems using both widely used and cutting-edge architectures.
Leveraging this dataset and a prediction model, our approach demonstrates
effectiveness in selecting the best model configuration based on user
preferences. Experimental results show that our method successfully identifies
energy-efficient configurations while ensuring competitive performance.

</details>


### [166] [Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers](https://arxiv.org/abs/2505.01482)
*Alice Rueda,Mohammed S. Hassan,Argyrios Perivolaris,Bazen G. Teferra,Reza Samavi,Sirisha Rambhatla,Yuqi Wu,Yanbo Zhang,Bo Cao,Divya Sharma,Sridhar Krishnan Venkat Bhat*

Main category: cs.AI

TL;DR: 这篇论文研究了当代大语言模型（LLMs）的多步推理能力，通过GPQA数据集测试了多种提示工程技术，发现LLM主要依赖模式识别而非逻辑推理，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在复杂推理任务（如科学、医学和法律）中的表现，并探讨其局限性及改进潜力。

Method: 在GPQA数据集上测试了七种提示工程技术（如零样本直接回答、思维链等），比较其在科学推理中的效果。

Result: 自一致性提示技术表现最佳（52.99%），但解释能力较差；简单提示技术（如直接回答）在科学推理中表现更好。

Conclusion: LLM推理能力仍有局限，需结合结构化推理框架、混合AI方法及人机协作来提升未来AI系统的鲁棒性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding, reasoning, and problem-solving across various
domains. However, their ability to perform complex, multi-step reasoning
task-essential for applications in science, medicine, and law-remains an area
of active investigation. This paper examines the reasoning capabilities of
contemporary LLMs, analyzing their strengths, limitations, and potential for
improvement. The study uses prompt engineering techniques on the Graduate-Level
GoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.
Five popular prompt engineering techniques and two tailored promptings were
tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot
CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our
findings indicate that while LLMs exhibit emergent reasoning abilities, they
often rely on pattern recognition rather than true logical inference, leading
to inconsistencies in complex problem-solving. The results indicated that
self-consistency outperformed the other prompt engineering technique with an
accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)
outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and
CoT (43.75%). Self-consistency performed the second worst in explaining the
answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have
the best scientific reasoning. We propose a research agenda aimed at bridging
these gaps by integrating structured reasoning frameworks, hybrid AI
approaches, and human-in-the-loop methodologies. By critically evaluating the
reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse
on the future of artificial general intelligence and the development of more
robust, trustworthy AI systems.

</details>


### [167] [CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code](https://arxiv.org/abs/2505.01485)
*Tasnim Ahmed,Salimur Choudhury*

Main category: cs.AI

TL;DR: 研究人员提出了CHORUS框架，通过检索增强生成（RAG）方法，利用大型语言模型（LLMs）从自然语言问题描述生成Gurobi求解器的线性规划代码。该框架通过分层分块策略和两阶段检索提升性能，实验表明其显著优于基线方法，并能以更少计算资源匹敌GPT-3.5/4。


<details>
  <summary>Details</summary>
Motivation: 线性规划（LP）问题通常需要专业知识，对非专家用户构成挑战。研究者旨在探索LLMs生成求解器特定代码的效率，以降低技术门槛。

Method: 提出CHORUS框架，结合分层分块策略、两阶段检索（含交叉编码器重排序）、专家设计的提示和结构化解析器，优化代码生成效果。

Result: 在NL4Opt-Code基准测试中，CHORUS显著提升多个开源LLMs（如Llama3、Phi4等）性能，以较少资源匹配或超越GPT-3.5/4。消融实验验证了关键模块的有效性。

Conclusion: CHORUS展示了RAG框架在LP代码生成中的潜力，通过优化检索与生成策略，实现了高性能与低资源消耗的平衡。

Abstract: Linear Programming (LP) problems aim to find the optimal solution to an
objective under constraints. These problems typically require domain knowledge,
mathematical skills, and programming ability, presenting significant challenges
for non-experts. This study explores the efficiency of Large Language Models
(LLMs) in generating solver-specific LP code. We propose CHORUS, a
retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP
code from natural language problem statements. CHORUS incorporates a
hierarchical tree-like chunking strategy for theoretical contents and generates
additional metadata based on code examples from documentation to facilitate
self-contained, semantically coherent retrieval. Two-stage retrieval approach
of CHORUS followed by cross-encoder reranking further ensures contextual
relevance. Finally, expertly crafted prompt and structured parser with
reasoning steps improve code generation performance significantly. Experiments
on the NL4Opt-Code benchmark show that CHORUS improves the performance of
open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1
(32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and
conventional RAG. It also allows these open-source LLMs to outperform or match
the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far
fewer computational resources. Ablation studies further demonstrate the
importance of expert prompting, hierarchical chunking, and structured
reasoning.

</details>


### [168] [Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models](https://arxiv.org/abs/2505.01539)
*Cor Steging,Silja Renooij,Bart Verheij*

Main category: cs.AI

TL;DR: 该论文提出了一种动态生成基准的方法，用于评估生成式语言模型的推理能力，尤其是在法律领域的应用。研究发现，当前最先进的语言模型在低复杂度推理难题上表现不佳，推理能力脆弱且不稳健。


<details>
  <summary>Details</summary>
Motivation: 当前生成式语言模型的推理能力脆弱且难以理解，无法在法律和证据领域负责任地应用。为了填补这一空白，论文旨在设计一种可动态调整复杂度的基准，以评估模型的推理能力。

Method: 论文提出了一种动态生成基准的方法，基于证人证词构建线性与非线性的参数攻击图，并将其转化为自然语言的推理难题。该方法可扩展复杂度并保持形式化无歧义。

Result: 实验表明，即使是当前最先进的生成式语言模型，在低复杂度推理难题上也会犯明显错误，且性能不稳定。高复杂度下，即使是专门优化推理的模型也会失败。

Conclusion: 论文证明了参数化基准在评估生成模型推理能力中的可行性，揭示了模型的局限性，这对设计负责任的法律领域AI系统至关重要。

Abstract: Generative large language models as tools in the legal domain have the
potential to improve the justice system. However, the reasoning behavior of
current generative models is brittle and poorly understood, hence cannot be
responsibly applied in the domains of law and evidence. In this paper, we
introduce an approach for creating benchmarks that can be used to evaluate the
reasoning capabilities of generative language models. These benchmarks are
dynamically varied, scalable in their complexity, and have formally unambiguous
interpretations. In this study, we illustrate the approach on the basis of
witness testimony, focusing on the underlying argument attack structure. We
dynamically generate both linear and non-linear argument attack graphs of
varying complexity and translate these into reasoning puzzles about witness
testimony expressed in natural language. We show that state-of-the-art large
language models often fail in these reasoning puzzles, already at low
complexity. Obvious mistakes are made by the models, and their inconsistent
performance indicates that their reasoning capabilities are brittle.
Furthermore, at higher complexity, even state-of-the-art models specifically
presented for reasoning capabilities make mistakes. We show the viability of
using a parametrized benchmark with varying complexity to evaluate the
reasoning capabilities of generative language models. As such, the findings
contribute to a better understanding of the limitations of the reasoning
capabilities of generative models, which is essential when designing
responsible AI systems in the legal domain.

</details>


### [169] [TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students](https://arxiv.org/abs/2505.01563)
*Daniel Weitekamp,Momin N. Siddiqui,Christopher J. MacLellan*

Main category: cs.AI

TL;DR: TutorGym是一个用于评估AI代理在智能辅导系统中表现的新框架，超越了传统的问题解决基准测试，直接模拟辅导和学习过程。初步评估显示，当前大语言模型在辅导任务上表现不佳，但在作为学生时能产生类似于人类的学习曲线。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在学术基准测试上的表现提升，它们被越来越多地用作独立辅导工具或人类学习的模拟器。然而，这些新应用需要更全面的评估方式，而不仅仅是最终解决方案的生成能力。

Method: 研究者提出了TutorGym，一个标准接口，用于在现有智能辅导系统中测试AI代理。TutorGym模拟辅导和学习过程，评估AI作为辅导者和学习者时的表现，包括提供辅导支持（如提示、反馈）和学习轨迹。当前涵盖223个不同辅导领域。

Result: 初步评估显示，当前大语言模型在辅导任务上表现较差（如下一步动作准确率仅为52-70%），但在作为学生时，通过上下文学习能产生类似于人类的学习曲线。

Conclusion: TutorGym为AI代理提供了一个统一的训练和评估框架，有助于优化其在教育技术中的应用。当前大语言模型在辅导任务上仍需改进，但在模拟学习方面展示了潜力。

Abstract: Recent improvements in large language model (LLM) performance on academic
benchmarks, such as MATH and GSM8K, have emboldened their use as standalone
tutors and as simulations of human learning. However, these new applications
require more than evaluations of final solution generation. We introduce
TutorGym to evaluate these applications more directly. TutorGym is a standard
interface for testing artificial intelligence (AI) agents within existing
intelligent tutoring systems (ITS) that have been tested and refined in
classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and
OATutors. TutorGym is more than a simple problem-solution benchmark, it
situates AI agents within the interactive interfaces of existing ITSs. At each
step of problem-solving, AI agents are asked what they would do as a tutor or
as a learner. As tutors, AI agents are prompted to provide tutoring support --
such as generating examples, hints, and step-level correctness feedback --
which can be evaluated directly against the adaptive step-by-step support
provided by existing ITSs. As students, agents directly learn from ITS
instruction, and their mistakes and learning trajectories can be compared to
student data. TutorGym establishes a common framework for training and
evaluating diverse AI agents, including LLMs, computational models of learning,
and reinforcement learning agents, within a growing suite of learning
environments. Currently, TutorGym includes 223 different tutor domains. In an
initial evaluation, we find that current LLMs are poor at tutoring -- none did
better than chance at labeling incorrect actions, and next-step actions were
correct only ~52-70% of the time -- but they could produce remarkably
human-like learning curves when trained as students with in-context learning.

</details>


### [170] [PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding](https://arxiv.org/abs/2505.01572)
*Bradley McDanel,Sai Qian Zhang,Yunhai Hu,Zining Liu*

Main category: cs.AI

TL;DR: PipeSpec通过分层流水线结构并行化推测解码，显著提升大型语言模型的推理速度，实验最高加速2.54倍。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码因阶段依赖导致硬件利用率不足，需解决并行性与协调问题。

Method: 采用k个模型的分层流水线架构，支持异步执行与轻量级验证/回滚机制，理论模型分析吞吐提升。

Result: 实验验证PipeSpec在文本摘要和代码生成任务上优于现有方法，且流水线深度与加速效果正相关。

Conclusion: PipeSpec提供了一种可扩展的多设备LLM推理加速方案，模型深度增加时效率持续提升。

Abstract: Speculative decoding accelerates large language model inference by using
smaller draft models to generate candidate tokens for parallel verification.
However, current approaches are limited by sequential stage dependencies that
prevent full hardware utilization. We present PipeSpec, a framework that
generalizes speculative decoding to $k$ models arranged in a hierarchical
pipeline, enabling asynchronous execution with lightweight coordination for
prediction verification and rollback. Our analytical model characterizes token
generation rates across pipeline stages and proves guaranteed throughput
improvements over traditional decoding for any non-zero acceptance rate. We
further derive closed-form expressions for steady-state verification
probabilities that explain the empirical benefits of pipeline depth.
Experimental results show that PipeSpec achieves up to 2.54$\times$ speedup
while outperforming state-of-the-art methods. We validate PipeSpec across text
summarization and code generation tasks using LLaMA 2 and 3 models,
demonstrating that pipeline efficiency increases with model depth, providing a
scalable approach to accelerating LLM inference on multi-device systems.

</details>


### [171] [Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation](https://arxiv.org/abs/2505.01636)
*Amit Rath*

Main category: cs.AI

TL;DR: STROT框架通过结构化提示和反馈驱动的输出转换逻辑，提升LLM在结构化数据分析中的可靠性和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在结构化数据分析中存在模式解释不一致、用户意图与模型输出不匹配以及缺乏自我纠正的问题，因此提出STROT框架。

Method: STROT包括轻量级模式内省、基于样本的字段分类、动态上下文构建和迭代输出优化机制。

Result: STROT在结构化数据上实现了更稳健、可解释的分析结果，适用于需要高稳定性和正确性的任务。

Conclusion: STROT框架显著提升了LLM在结构化数据分析中的表现，适合广泛的数据探索与分析任务。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and task generalization. However, their
application to structured data analysis remains fragile due to inconsistencies
in schema interpretation, misalignment between user intent and model output,
and limited mechanisms for self-correction when failures occur. This paper
introduces the STROT Framework (Structured Task Reasoning and Output
Transformation), a method for structured prompting and feedback-driven
transformation logic generation aimed at improving the reliability and semantic
alignment of LLM-based analytical workflows. STROT begins with lightweight
schema introspection and sample-based field classification, enabling dynamic
context construction that captures both the structure and statistical profile
of the input data. This contextual information is embedded in structured
prompts that guide the model toward generating task-specific, interpretable
outputs. To address common failure modes in complex queries, STROT incorporates
a refinement mechanism in which the model iteratively revises its outputs based
on execution feedback and validation signals. Unlike conventional approaches
that rely on static prompts or single-shot inference, STROT treats the LLM as a
reasoning agent embedded within a controlled analysis loop -- capable of
adjusting its output trajectory through planning and correction. The result is
a robust and reproducible framework for reasoning over structured data with
LLMs, applicable to diverse data exploration and analysis tasks where
interpretability, stability, and correctness are essential.

</details>


### [172] [Human-AI Governance (HAIG): A Trust-Utility Approach](https://arxiv.org/abs/2505.01651)
*Zeynep Engin*

Main category: cs.AI

TL;DR: 该论文提出了HAIG框架，用于分析人类与AI关系中信任的动态变化，强调从工具到合作伙伴的连续演变过程，并通过多维度、连续性和阈值来适应治理挑战。


<details>
  <summary>Details</summary>
Motivation: 当前分类框架（如'人在循环中'模型）无法充分捕捉AI系统从工具演变为合作伙伴的动态，尤其是在基础模型和多智能体系统展现自主行为时。因此，需要一种更灵活的框架来量化信任和治理演化。

Method: HAIG框架通过三个层次运作：维度（决策权分配、过程自主性和问责配置）、连续性（每个维度的渐进变化）和阈值（需调整治理的关键点），并以信任-效用为导向分析信任动态。

Result: 研究发现，AI技术的进步（如自我监督和分布式决策）会引发信任的非均匀演化，案例研究显示HAIG能补充现有框架并预见治理挑战。

Conclusion: HAIG框架提供了一种动态视角，能更灵活地适应AI系统发展中的信任和治理需求，为未来的治理设计提供理论基础。

Abstract: This paper introduces the HAIG framework for analysing trust dynamics across
evolving human-AI relationships. Current categorical frameworks (e.g.,
"human-in-the-loop" models) inadequately capture how AI systems evolve from
tools to partners, particularly as foundation models demonstrate emergent
capabilities and multi-agent systems exhibit autonomous goal-setting
behaviours. As systems advance, agency redistributes in complex patterns that
are better represented as positions along continua rather than discrete
categories, though progression may include both gradual shifts and significant
step changes. The HAIG framework operates across three levels: dimensions
(Decision Authority Distribution, Process Autonomy, and Accountability
Configuration), continua (gradual shifts along each dimension), and thresholds
(critical points requiring governance adaptation). Unlike risk-based or
principle-based approaches, HAIG adopts a trust-utility orientation, focusing
on maintaining appropriate trust relationships that maximise utility while
ensuring sufficient safeguards. Our analysis reveals how technical advances in
self-supervision, reasoning authority, and distributed decision-making drive
non-uniform trust evolution across both contextual variation and technological
advancement. Case studies in healthcare and European regulation demonstrate how
HAIG complements existing frameworks while offering a foundation for
alternative approaches that anticipate governance challenges before they
emerge.

</details>


### [173] [Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm](https://arxiv.org/abs/2505.01706)
*Sarvesh Shashidhar,Ritik,Nachiketa Patil,Suraj Racha,Ganesh Ramakrishnan*

Main category: cs.AI

TL;DR: 该论文探讨了Direct Preference Optimisation (DPO) 方法的局限性，并提出了一种2D-DPO方法来改进。2D-DPO通过二维评分解决DPO对所有响应段落等同对待的问题，并进一步提出了一种增强算法以应对标签/评分噪音。


<details>
  <summary>Details</summary>
Motivation: 人类偏好通常是分段的，不同段落的评分可能不同，而DPO方法未能考虑这一点。因此，需要一种更精细化的评分方法，同时增强对噪音的鲁棒性。

Method: 论文提出了2D-DPO方法，通过二维评分（分段评分）替代传统DPO的单一评分，并引入了分段级别噪音鲁棒性的改进算法。

Result: 实验表明，2D-DPO在胜率上优于标准DPO，但对噪音敏感。改进算法通过理论分析和实验验证，有效提升了噪音场景下的性能。

Conclusion: 2D-DPO提高了人类偏好对齐的精确性，改进算法进一步增强了其对噪音的鲁棒性，为未来的研究提供了扩展方向。

Abstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for
aligning Large Language Models (LLMs) with human preferences, offering a stable
and efficient alternative to approaches that use Reinforcement learning via
Human Feedback. In this work, we investigate the performance of DPO using
open-source preference datasets. One of the major drawbacks of DPO is that it
doesn't induce granular scoring and treats all the segments of the responses
with equal propensity. However, this is not practically true for human
preferences since even "good" responses have segments that may not be preferred
by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment
called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the
advantages it provides over the standard DPO by comparing their win rates. It
is observed that these methods, even though effective, are not robust to
label/score noise. To counter this, we propose an approach of incorporating
segment-level score noise robustness to the 2D-DPO algorithm. Along with
theoretical backing, we also provide empirical verification in favour of the
algorithm and introduce other noise models that can be present.

</details>


### [174] [World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks](https://arxiv.org/abs/2505.01712)
*Lingyi Wang,Rashed Shelim,Walid Saad,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: 论文提出了一种基于世界模型的学习框架，用于在毫米波V2X网络中最小化数据包完整性感知的信息年龄（CAoI），相比传统强化学习方法，显著提高了数据效率并降低了CAoI。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在无线网络中依赖高成本的试错机制和实时反馈，导致数据效率低下和短视策略，尤其在复杂动态网络中问题更为突出。

Method: 提出了一种世界模型框架，通过学习毫米波V2X环境的动态模型，利用想象轨迹进行链路调度的长期策略学习，避免了直接与环境交互。

Result: 实验显示，该方法显著提升了数据效率，并在CAoI上分别比MBRL和MFRL方法提高了26%和16%。

Conclusion: 世界模型框架在复杂动态网络中表现出更高的效率和性能，为无线网络优化提供了新的解决方案。

Abstract: Traditional reinforcement learning (RL)-based learning approaches for
wireless networks rely on expensive trial-and-error mechanisms and real-time
feedback based on extensive environment interactions, which leads to low data
efficiency and short-sighted policies. These limitations become particularly
problematic in complex, dynamic networks with high uncertainty and long-term
planning requirements. To address these limitations, in this paper, a novel
world model-based learning framework is proposed to minimize
packet-completeness-aware age of information (CAoI) in a vehicular network.
Particularly, a challenging representative scenario is considered pertaining to
a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,
which is characterized by high mobility, frequent signal blockages, and
extremely short coherence time. Then, a world model framework is proposed to
jointly learn a dynamic model of the mmWave V2X environment and use it to
imagine trajectories for learning how to perform link scheduling. In
particular, the long-term policy is learned in differentiable imagined
trajectories instead of environment interactions. Moreover, owing to its
imagination abilities, the world model can jointly predict time-varying
wireless data and optimize link scheduling in real-world wireless and V2X
networks. Thus, during intervals without actual observations, the world model
remains capable of making efficient decisions. Extensive experiments are
performed on a realistic simulator based on Sionna that integrates
physics-based end-to-end channel modeling, ray-tracing, and scene geometries
with material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency, and achieves 26%
improvement and 16% improvement in CAoI, respectively, compared to the
model-based RL (MBRL) method and the model-free RL (MFRL) method.

</details>


### [175] [Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias](https://arxiv.org/abs/2505.01754)
*Orlando Jähde,Thorsten Weber,Rüdiger Buchkremer*

Main category: cs.AI

TL;DR: 论文提出了一种利用自然语言处理技术（如分层主题建模、情感分析和本体学习）来规模化分析政治新闻报道偏差的新方法，通过三个案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 有偏见的新闻报道对民主决策和知情权构成威胁，因此需要一种可扩展且偏见最小化的方法来分析媒体偏见。

Method: 采用自然语言处理技术（包括分层主题建模、情感分析和本体学习）来检查新闻来源中的事件选择、标签、用词及遗漏偏见。

Result: 通过三个政治事件案例研究，该方法能有效识别不同新闻来源在多个粒度上的偏见。

Conclusion: 该研究为开发帮助新闻消费者应对复杂媒体环境的工具打下了基础，是迈向可扩展、偏见最小化媒体分析的重要一步。

Abstract: Biased news reporting poses a significant threat to informed decision-making
and the functioning of democracies. This study introduces a novel methodology
for scalable, minimally biased analysis of media bias in political news. The
proposed approach examines event selection, labeling, word choice, and
commission and omission biases across news sources by leveraging natural
language processing techniques, including hierarchical topic modeling,
sentiment analysis, and ontology learning with large language models. Through
three case studies related to current political events, we demonstrate the
methodology's effectiveness in identifying biases across news sources at
various levels of granularity. This work represents a significant step towards
scalable, minimally biased media bias analysis, laying the groundwork for tools
to help news consumers navigate an increasingly complex media landscape.

</details>


### [176] [Training Environment for High Performance Reinforcement Learning](https://arxiv.org/abs/2505.01953)
*Greg Search*

Main category: cs.AI

TL;DR: 论文介绍了Tunnel，一个开源的强化学习训练环境，用于高性能飞机。它集成了F16的3D非线性飞行动力学到OpenAI Gymnasium，提供快速响应任务需求的能力。


<details>
  <summary>Details</summary>
Motivation: 随着战争对自动化的依赖增加，软件敏捷性与决策优势相关。需要工具帮助空军人员快速适应对手的变化。

Method: 将F16的3D非线性飞行动力学集成到OpenAI Gymnasium，提供可定制的边界、目标、对手和感知能力。

Result: Tunnel使研究人员和任务规划者能在几天内完成在传统模拟器中需要数月的工作，提升协作效率。

Conclusion: Tunnel通过快速定制和协作能力，为国家和军事提供了技术优势，尤其在自主空战领域。

Abstract: This paper presents Tunnel, a simple, open source, reinforcement learning
training environment for high performance aircraft. It integrates the F16 3D
nonlinear flight dynamics into OpenAI Gymnasium python package. The template
includes primitives for boundaries, targets, adversaries and sensing
capabilities that may vary depending on operational need. This offers mission
planners a means to rapidly respond to evolving environments, sensor
capabilities and adversaries for autonomous air combat aircraft. It offers
researchers access to operationally relevant aircraft physics. Tunnel code base
is accessible to anyone familiar with Gymnasium and/or those with basic python
skills. This paper includes a demonstration of a week long trade study that
investigated a variety of training methods, observation spaces, and threat
presentations. This enables increased collaboration between researchers and
mission planners which can translate to a national military advantage. As
warfare becomes increasingly reliant upon automation, software agility will
correlate with decision advantages. Airmen must have tools to adapt to
adversaries in this context. It may take months for researchers to develop
skills to customize observation, actions, tasks and training methodologies in
air combat simulators. In Tunnel, this can be done in a matter of days.

</details>


### [177] [Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM](https://arxiv.org/abs/2505.01955)
*Max Reuter,Maura Philippone,Bond Benton,Laura Dilley*

Main category: cs.AI

TL;DR: 论文探讨了LLM工具NotebookLM在医疗实践中的潜力与风险，认为需在临床应用前进行测试和评估。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析NotebookLM在医疗领域的潜力及其伴随的临床与技术风险，以确保安全有效的应用。

Method: 通过论证NotebookLM的潜在用途与风险，提出需测试和评估的必要性。

Result: 指出NotebookLM虽有助于教育和医疗文献整理，但存在临床与技术风险。

Conclusion: 建议在NotebookLM应用于临床前进行全面测试和风险评估。

Abstract: The advent of generative artificial intelligence, especially large language
models (LLMs), presents opportunities for innovation in research, clinical
practice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's
potential, including for generating AI-voiced podcasts to educate patients
about treatment and rehabilitation, and for quickly synthesizing medical
literature for professionals. We argue that NotebookLM presently poses clinical
and technological risks that should be tested and considered prior to its
implementation in clinical practice.

</details>


### [178] [Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing](https://arxiv.org/abs/2505.02003)
*Maryam Sadeghi,Darío Fernández Khatiboun,Yasser Rezaeiyan,Saima Rizwan,Alessandro Barcellona,Andrea Merello,Marco Crepaldi,Gabriella Panuccio,Farshad Moradi*

Main category: cs.AI

TL;DR: 本文介绍了一种基于神经形态计算的闭环脑刺激系统，用于个性化治疗耐药性癫痫，通过预测癫痫发作并触发个性化刺激，实现了97%以上的发作减少。


<details>
  <summary>Details</summary>
Motivation: 当前闭环脑刺激治疗耐药性癫痫存在两大局限：刺激通常在发作时而非预防时进行，且参数需反复调试。本文旨在通过神经形态计算解决这些问题。

Method: 提出了一种基于癫痫预测的个性化自由运行刺激系统，利用预测触发个性化电脉冲（而非固定频率刺激），并在海马球状体与3D微电极阵列耦合的模型上进行了验证。

Result: 系统实现了>97%的癫痫发作减少，且刺激频率主要低于20 Hz，远低于临床常用频率。

Conclusion: 神经形态系统展示了作为下一代个性化耐药性癫痫治疗策略的潜力。

Abstract: Closed-loop brain stimulation holds potential as personalized treatment for
drug-resistant epilepsy (DRE) but still suffers from limitations that result in
highly variable efficacy. First, stimulation is typically delivered upon
detection of the seizure to abort rather than prevent it; second, the
stimulation parameters are established by trial and error, requiring lengthy
rounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we
address these limitations by leveraging the potential of neuromorphic
computing. We present a system capable of driving personalized free-run
stimulations based on seizure forecasting, wherein each forecast triggers an
electrical pulse rather than an arbitrarily predefined fixed-frequency stimulus
train. We validate the system against hippocampal spheroids coupled to 3D
microelectrode array as a simplified testbed, showing that it can achieve
seizure reduction >97% while primarily using instantaneous stimulation
frequencies within 20 Hz, well below what typically used in clinical settings.
Our work demonstrates the potential of neuromorphic systems as a
next-generation neuromodulation strategy for personalized DRE treatment.

</details>


### [179] [From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent](https://arxiv.org/abs/2505.02024)
*Minjie Shen,Qikai Yang*

Main category: cs.AI

TL;DR: Manus AI是2025年推出的通用AI代理，结合大型语言模型的推理规划能力与端到端任务执行能力，应用覆盖医疗、金融、制造、机器人及游戏等领域。


<details>
  <summary>Details</summary>
Motivation: 旨在弥合‘思维’与‘行动’的鸿沟，将AI的推理能力转化为实际任务执行，推动人机协作新时代。

Method: 基于核心架构，整合语言模型的规划能力与复杂任务执行模块，实现端到端操作。

Result: 展示了跨行业应用的多样性与潜力，同时指出了当前局限性。

Conclusion: Manus AI标志着智能代理向意图到行动的转变，预示了未来人机协作的广阔前景。

Abstract: Manus AI is a general-purpose AI agent introduced in early 2025, marking a
significant advancement in autonomous artificial intelligence. Developed by the
Chinese startup Monica.im, Manus is designed to bridge the gap between "mind"
and "hand" - combining the reasoning and planning capabilities of large
language models with the ability to execute complex, end-to-end tasks that
produce tangible outcomes. This paper presents a comprehensive overview of
Manus AI, exploring its core technical architecture, diverse applications
across sectors such as healthcare, finance, manufacturing, robotics, and
gaming, as well as its key strengths, current limitations, and future
potential. Positioned as a preview of what lies ahead, Manus AI represents a
shift toward intelligent agents that can translate high-level intentions into
real-world actions, heralding a new era of human-AI collaboration.

</details>


### [180] [Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks](https://arxiv.org/abs/2505.02050)
*Kranthi Kumar Talluri,Anders L. Madsen,Galia Weidl*

Main category: cs.AI

TL;DR: 提出了一种动态贝叶斯网络框架，结合横向证据和安全评估模型，以预测车道变更并确保安全的切入动作。该框架在高速和低速场景下均表现出色，减少了碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 高速交通中的切入动作可能导致急刹或碰撞，亟需安全高效的车道变更策略。

Method: 采用动态贝叶斯网络（DBN）框架，结合横向证据、横向安全和纵向安全三个关键概率假设，通过动态数据处理评估车辆位置、横向速度、相对距离和时间碰撞（TTC）。

Result: 与其他传统方法相比，DBN模型在高速关键场景下显著减少碰撞，低速场景下表现同样优异。

Conclusion: 该框架为自动驾驶系统的安全验证提供了鲁棒、可扩展且高效的解决方案。

Abstract: Cut-in maneuvers in high-speed traffic pose critical challenges that can lead
to abrupt braking and collisions, necessitating safe and efficient lane change
strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate
lateral evidence with safety assessment models, thereby predicting lane changes
and ensuring safe cut-in maneuvers effectively. Our proposed framework
comprises three key probabilistic hypotheses (lateral evidence, lateral safety,
and longitudinal safety) that facilitate the decision-making process through
dynamic data processing and assessments of vehicle positions, lateral
velocities, relative distance, and Time-to-Collision (TTC) computations. The
DBN model's performance compared with other conventional approaches
demonstrates superior performance in crash reduction, especially in critical
high-speed scenarios, while maintaining a competitive performance in low-speed
scenarios. This paves the way for robust, scalable, and efficient safety
validation in automated driving systems.

</details>


### [181] [TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition](https://arxiv.org/abs/2505.02052)
*Lala Shakti Swarup Ray,Lars Krupp,Vitor Fortes Rey,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.AI

TL;DR: 论文提出了一种名为TxP的双向文本×压力模型，利用生成基础模型将压力数据解释为自然语言，提升压力传感器在人类活动识别（HAR）中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 当前HAR研究过度依赖惯性测量单元和视觉数据，忽视了压力传感器的潜力。由于其能捕捉细微身体动态和重心变化，压力传感器在姿态和平衡相关活动中具有独特优势，但受限于数据集不足而未被充分利用。

Method: 提出TxP模型，包含Text2Pressure（将活动文本描述转为压力序列）和Pressure2Text（从动态压力图生成活动描述和分类）两个任务，基于预训练模型CLIP和LLaMA 2 13B Chat，在合成的PressLang数据集（81,100个文本-压力对）上训练。

Result: 在瑜伽和日常任务等真实数据验证中，TxP在数据增强和分类方面表现优异，宏F1分数比现有技术最高提升12.4%。

Conclusion: TxP不仅推动了压力传感器在HAR中的广泛应用，还通过原子动作提供了对人类运动的更深入理解，显著提升了性能。

Abstract: Sensor-based human activity recognition (HAR) has predominantly focused on
Inertial Measurement Units and vision data, often overlooking the capabilities
unique to pressure sensors, which capture subtle body dynamics and shifts in
the center of mass. Despite their potential for postural and balance-based
activities, pressure sensors remain underutilized in the HAR domain due to
limited datasets. To bridge this gap, we propose to exploit generative
foundation models with pressure-specific HAR techniques. Specifically, we
present a bidirectional Text$\times$Pressure model that uses generative
foundation models to interpret pressure data as natural language. TxP
accomplishes two tasks: (1) Text2Pressure, converting activity text
descriptions into pressure sequences, and (2) Pressure2Text, generating
activity descriptions and classifications from dynamic pressure maps.
Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on
our synthetic PressLang dataset, containing over 81,100 text-pressure pairs.
Validated on real-world data for activities such as yoga and daily tasks, TxP
provides novel approaches to data augmentation and classification grounded in
atomic actions. This consequently improved HAR performance by up to 12.4\% in
macro F1 score compared to the state-of-the-art, advancing pressure-based HAR
with broader applications and deeper insights into human movement.

</details>


### [182] [Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)](https://arxiv.org/abs/2505.02062)
*Prathamesh Muzumdar,Apoorva Muley,Kuldeep Singh,Sumanth Cheemalapati*

Main category: cs.AI

TL;DR: 本文提出了一种名为MEAAM的多维度伦理AI采用模型，通过定量研究分析13个关键伦理变量对医疗AI采用的影响，发现规范性关注在操作采用中起主导作用，而系统性采用则更多受全局性关注影响。


<details>
  <summary>Details</summary>
Motivation: 当前AI在医疗行业的伦理框架缺乏综合性和实证支持，研究旨在填补这一空白，提供全面的伦理AI采用模型。

Method: 采用定量、横断面研究设计，通过问卷调查医疗专业人员，并使用PLS-SEM进行分析。

Result: 规范性关注显著影响操作采用，全局性关注主导系统性采用，认知性关注增强伦理设计原则对AI系统信任和透明度的作用。

Conclusion: MEAAM框架为医疗行业提供了一个全面、可操作的伦理AI采用方法，为决策者、技术专家和管理者提供了重要指导。

Abstract: The adoption of Artificial Intelligence (AI) in the healthcare service
industry presents numerous ethical challenges, yet current frameworks often
fail to offer a comprehensive, empirical understanding of the multidimensional
factors influencing ethical AI integration. Addressing this critical research
gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model
(MEAAM), a novel theoretical framework that categorizes 13 critical ethical
variables across four foundational dimensions of Ethical AI Fair AI,
Responsible AI, Explainable AI, and Sustainable AI. These dimensions are
further analyzed through three core ethical lenses: epistemic concerns (related
to knowledge, transparency, and system trustworthiness), normative concerns
(focused on justice, autonomy, dignity, and moral obligations), and overarching
concerns (highlighting global, systemic, and long-term ethical implications).
This study adopts a quantitative, cross-sectional research design using survey
data collected from healthcare professionals and analyzed via Partial Least
Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study
empirically investigates the influence of these ethical constructs on two
outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate
that normative concerns most significantly drive operational adoption
decisions, while overarching concerns predominantly shape systemic adoption
strategies and governance frameworks. Epistemic concerns play a facilitative
role, enhancing the impact of ethical design principles on trust and
transparency in AI systems. By validating the MEAAM framework, this research
advances a holistic, actionable approach to ethical AI adoption in healthcare
and provides critical insights for policymakers, technologists, and healthcare
administrators striving to implement ethically grounded AI solutions.

</details>


### [183] [Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants](https://arxiv.org/abs/2505.02076)
*Milapji Singh Gill,Javal Vyas,Artan Markaj,Felix Gehlhoff,Mehmet Mercangöz*

Main category: cs.AI

TL;DR: 本文提出了一种将大型语言模型（LLM）代理与数字孪生环境集成的框架，用于自动化处理流程工厂中的故障，减少对人力的依赖。


<details>
  <summary>Details</summary>
Motivation: 流程工厂的自动化与人工智能虽然持续进步，但故障处理等任务仍高度依赖人类专家，需要系统化、基于知识的方法。

Method: 通过LLM代理持续解读系统状态并启动控制动作，数字孪生则作为工程知识库和仿真平台验证控制动作。

Result: 在混合模块测试中，该框架不仅能自主控制模块，还能通过少量重新提示生成有效纠正措施缓解管道堵塞。

Conclusion: 该框架展示了通过LLM与数字孪生的结合，能够有效解决流程工厂中的复杂问题，提升自主性。

Abstract: Advances in Automation and Artificial Intelligence continue to enhance the
autonomy of process plants in handling various operational scenarios. However,
certain tasks, such as fault handling, remain challenging, as they rely heavily
on human expertise. This highlights the need for systematic, knowledge-based
methods. To address this gap, we propose a methodological framework that
integrates Large Language Model (LLM) agents with a Digital Twin environment.
The LLM agents continuously interpret system states and initiate control
actions, including responses to unexpected faults, with the goal of returning
the system to normal operation. In this context, the Digital Twin acts both as
a structured repository of plant-specific engineering knowledge for agent
prompting and as a simulation platform for the systematic validation and
verification of the generated corrective control actions. The evaluation using
a mixing module of a process plant demonstrates that the proposed framework is
capable not only of autonomously controlling the mixing module, but also of
generating effective corrective actions to mitigate a pipe clogging with only a
few reprompts.

</details>


### [184] [Retrieval-augmented in-context learning for multimodal large language models in disease classification](https://arxiv.org/abs/2505.02087)
*Zaifu Zhan,Shuang Zhou,Xiaoshan Zhou,Yongkang Xiao,Jun Wang,Jiawen Deng,He Zhu,Yu Hou,Rui Zhang*

Main category: cs.AI

TL;DR: RAICL框架通过检索增强的上下文学习提升多模态大语言模型在疾病分类中的性能，显著提高准确率。


<details>
  <summary>Details</summary>
Motivation: 旨在动态检索信息丰富的演示样本，优化多模态大语言模型在疾病分类中的上下文学习能力。

Method: 结合检索增强生成和上下文学习，通过多种编码器（如ResNet、BERT等）的嵌入向量检索相似疾病模式的演示样本，并构建对话提示。

Result: 在TCGA和IU Chest X-ray数据集上，准确率分别提升至0.8368和0.8658，多模态输入优于单模态，欧氏距离和余弦相似度在不同指标上表现最佳。

Conclusion: RAICL提供了一种高效且可扩展的方法，显著提升了多模态疾病分类中上下文学习的表现。

Abstract: Objectives: We aim to dynamically retrieve informative demonstrations,
enhancing in-context learning in multimodal large language models (MLLMs) for
disease classification.
  Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL)
framework, which integrates retrieval-augmented generation (RAG) and in-context
learning (ICL) to adaptively select demonstrations with similar disease
patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines
embeddings from diverse encoders, including ResNet, BERT, BioBERT, and
ClinicalBERT, to retrieve appropriate demonstrations, and constructs
conversational prompts optimized for ICL. We evaluated the framework on two
real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its
performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies,
similarity metrics, and varying numbers of demonstrations.
  Results: RAICL consistently improved classification performance. Accuracy
increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest
X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs
being stronger than images alone. The richness of information embedded in each
modality will determine which embedding model can be used to get better
results. Few-shot experiments showed that increasing the number of retrieved
examples further enhanced performance. Across different similarity metrics,
Euclidean distance achieved the highest accuracy while cosine similarity
yielded better macro-F1 scores. RAICL demonstrated consistent improvements
across various MLLMs, confirming its robustness and versatility.
  Conclusions: RAICL provides an efficient and scalable approach to enhance
in-context learning in MLLMs for multimodal disease classification.

</details>


### [185] [MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents](https://arxiv.org/abs/2505.02099)
*Zeyu Zhang,Quanyu Dai,Xu Chen,Rui Li,Zhongyang Li,Zhenhua Dong*

Main category: cs.AI

TL;DR: 研究者开发了一个名为MemEngine的统一模块化库，用于构建基于大语言模型（LLM）代理的高级记忆模型，解决了现有研究中缺乏统一实现框架的问题。


<details>
  <summary>Details</summary>
Motivation: 由于基于大语言模型的代理在多个领域广泛应用，其记忆能力备受关注，但现有研究缺乏统一实现框架，因此开发MemEngine来填补这一空白。

Method: 开发了一个名为MemEngine的统一模块化库，实现了多种先进记忆模型，支持便捷扩展和用户友好的即插即用功能。

Result: MemEngine库已实现多种研究中的记忆模型，并提供了易于扩展和使用的接口，项目已开源。

Conclusion: MemEngine为LLM代理的记忆模型开发提供了统一框架，促进了该领域的进一步发展。

Abstract: Recently, large language model based (LLM-based) agents have been widely
applied across various fields. As a critical part, their memory capabilities
have captured significant interest from both industrial and academic
communities. Despite the proposal of many advanced memory models in recent
research, however, there remains a lack of unified implementations under a
general framework. To address this issue, we develop a unified and modular
library for developing advanced memory models of LLM-based agents, called
MemEngine. Based on our framework, we implement abundant memory models from
recent research works. Additionally, our library facilitates convenient and
extensible memory development, and offers user-friendly and pluggable memory
usage. For benefiting our community, we have made our project publicly
available at https://github.com/nuster1128/MemEngine.

</details>


### [186] [Eterna is Solved](https://arxiv.org/abs/2505.02110)
*Tristan Cazenave*

Main category: cs.AI

TL;DR: 提出了一种名为Montparnasse的多目标通用嵌套滚动策略自适应有限重复（MOGNRPALR）RNA设计算法，用于解决Eterna基准问题。


<details>
  <summary>Details</summary>
Motivation: RNA设计在合成生物学、医学和纳米技术中有重要应用，其核心是找到能够折叠成目标二级结构的核苷酸序列。

Method: 采用了一种名为MOGNRPALR的多目标通用嵌套滚动策略自适应有限重复算法。

Result: 该算法成功解决了Eterna基准问题。

Conclusion: Montparnasse算法为RNA设计提供了一种有效的解决方案，展示了其在多目标优化中的潜力。

Abstract: RNA design consists of discovering a nucleotide sequence that folds into a
target secondary structure. It is useful for synthetic biology, medicine, and
nanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested
Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design
algorithm. It solves the Eterna benchmark.

</details>


### [187] [Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets](https://arxiv.org/abs/2505.02118)
*Wei Liu,Zhongyu Niu,Lang Gao,Zhiying Deng,Jun Wang,Haozhao Wang,Ruixuan Li*

Main category: cs.AI

TL;DR: 该研究探讨了自解释框架中的抽样偏差问题，提出了一种防止预测器学习错误相关性的方法，并在多个数据集和架构上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 揭示自解释框架中可能产生的抽样偏差，并提出解决方案以提高模型解释的准确性。

Method: 通过理论分析和实证证据识别偏差，设计指令防止预测器学习错误相关性，并在多种数据集和架构上测试。

Result: 提出的方法显著优于近期自解释方法，甚至在某些情况下优于大型语言模型。

Conclusion: 研究不仅识别了自解释框架中的潜在问题，还提供了有效的解决方案，增强了模型的可靠性和解释性。

Abstract: This study investigates the self-rationalization framework constructed with a
cooperative game, where a generator initially extracts the most informative
segment from raw input, and a subsequent predictor utilizes the selected subset
for its input. The generator and predictor are trained collaboratively to
maximize prediction accuracy. In this paper, we first uncover a potential
caveat: such a cooperative game could unintentionally introduce a sampling bias
during rationale extraction. Specifically, the generator might inadvertently
create an incorrect correlation between the selected rationale candidate and
the label, even when they are semantically unrelated in the original dataset.
Subsequently, we elucidate the origins of this bias using both detailed
theoretical analysis and empirical evidence. Our findings suggest a direction
for inspecting these correlations through attacks, based on which we further
introduce an instruction to prevent the predictor from learning the
correlations. Through experiments on six text classification datasets and two
graph classification datasets using three network architectures (GRUs, BERT,
and GCN), we show that our method not only significantly outperforms recent
rationalization methods, but also achieves comparable or even better results
than a representative LLM (llama3.1-8b-instruct).

</details>


### [188] [Overview of AI Grading of Physics Olympiad Exams](https://arxiv.org/abs/2505.02121)
*Lachlan McGinness*

Main category: cs.AI

TL;DR: 本文提出了一个多模态AI评分框架，用于自动化高中物理问题的多样化题型评分，并基于澳大利亚AI伦理原则进行了评估。


<details>
  <summary>Details</summary>
Motivation: 解决高中物理问题多样化题型的自动化评分挑战，需要结合不同领域的评分技术。

Method: 通过系统文献综述筛选潜在的物理评分技术，并设计多模态AI评分框架。

Result: 提出了一个符合AI伦理原则的多模态评分框架。

Conclusion: 该框架为高中物理问题的自动化评分提供了可行方案，同时符合伦理标准。

Abstract: Automatically grading the diverse range of question types in high school
physics problem is a challenge that requires automated grading techniques from
different fields. We report the findings of a Systematic Literature Review of
potential physics grading techniques. We propose a multi-modal AI grading
framework to address these challenges and examine our framework in light of
Australia's AI Ethical Principles.

</details>


### [189] [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/abs/2505.02130)
*Zhong Guan,Likang Wu,Hongke Zhao,Ming He,Jianpin Fan*

Main category: cs.AI

TL;DR: 论文探讨了注意力机制在图结构数据上的局限性，发现LLMs在处理节点间关系时表现不佳，并提出中间态注意力窗口改进性能。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制在LLMs中表现卓越，但在图结构数据上（如GNNs）效果不足，因此研究其具体行为和局限性。

Method: 通过实证研究LLMs的注意力机制处理图数据的行为，分析其表现。

Result: 发现LLMs难以建模节点间关系，注意力分布不符合理想结构；提出中间态注意力窗口优化训练和推理。

Conclusion: 证明了注意力机制在图数据上的适用性有限，中间态注意力窗口是更优方案。

Abstract: Attention mechanisms are critical to the success of large language models
(LLMs), driving significant advancements in multiple fields. However, for
graph-structured data, which requires emphasis on topological connections, they
fall short compared to message-passing mechanisms on fixed links, such as those
employed by Graph Neural Networks (GNNs). This raises a question: ``Does
attention fail for graphs in natural language settings?'' Motivated by these
observations, we embarked on an empirical study from the perspective of
attention mechanisms to explore how LLMs process graph-structured data. The
goal is to gain deeper insights into the attention behavior of LLMs over graph
structures. We uncovered unique phenomena regarding how LLMs apply attention to
graph-structured data and analyzed these findings to improve the modeling of
such data by LLMs. The primary findings of our research are: 1) While LLMs can
recognize graph data and capture text-node interactions, they struggle to model
inter-node relationships within graph structures due to inherent architectural
constraints. 2) The attention distribution of LLMs across graph nodes does not
align with ideal structural patterns, indicating a failure to adapt to graph
topology nuances. 3) Neither fully connected attention nor fixed connectivity
is optimal; each has specific limitations in its application scenarios.
Instead, intermediate-state attention windows improve LLM training performance
and seamlessly transition to fully connected windows during inference. Source
code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}

</details>


### [190] [Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes](https://arxiv.org/abs/2505.02184)
*Matthew T. Dearing,Yiheng Tao,Xingfu Wu,Zhiling Lan,Valerie Taylor*

Main category: cs.AI

TL;DR: 论文提出LASSI-EE框架，通过多阶段迭代流程，利用LLM生成高能效并行代码，在NVIDIA A100上实现平均47%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成并行科学代码时多关注功能性，忽略了性能和能耗优化。

Method: 采用基于LLM的自动化重构框架LASSI-EE，通过多阶段迭代流程生成目标并行系统的能效代码。

Result: 在20个HeCBench基准测试中，85%的案例平均能耗降低47%。

Conclusion: LLM不仅可生成正确代码，还能实现能耗感知编程，但框架仍存在需改进的局限性。

Abstract: While large language models (LLMs) are increasingly used for generating
parallel scientific code, most current efforts emphasize functional
correctness, often overlooking performance and energy considerations. In this
work, we propose LASSI-EE, an automated LLM-based refactoring framework that
generates energy-efficient parallel code on a target parallel system for a
given parallel code as input. Through a multi-stage, iterative pipeline
process, LASSI-EE achieved an average energy reduction of 47% across 85% of the
20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the
broader potential of LLMs, not only for generating correct code but also for
enabling energy-aware programming. We also address key insights and limitations
within the framework, offering valuable guidance for future improvements.

</details>


### [191] [Interpretable Emergent Language Using Inter-Agent Transformers](https://arxiv.org/abs/2505.02215)
*Mannan Bhardwaj*

Main category: cs.AI

TL;DR: 论文介绍了DIAT方法，利用Transformer实现多智能体强化学习中的可解释通信。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RIAL、DIAL和CommNet）缺乏可解释性，需要一种能生成人类可理解的通信协议的新方法。

Method: 提出Differentiable Inter-Agent Transformers（DIAT），通过自注意力机制学习符号化通信协议。

Result: 实验证明DIAT能将观察编码为可解释的词汇和嵌入，有效解决协作任务。

Conclusion: DIAT在复杂多智能体环境中展示了可解释通信的潜力。

Abstract: This paper explores the emergence of language in multi-agent reinforcement
learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and
CommNet enable agent communication but lack interpretability. We propose
Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention
to learn symbolic, human-understandable communication protocols. Through
experiments, DIAT demonstrates the ability to encode observations into
interpretable vocabularies and meaningful embeddings, effectively solving
cooperative tasks. These results highlight the potential of DIAT for
interpretable communication in complex multi-agent environments.

</details>


### [192] [LLM-Guided Probabilistic Program Induction for POMDP Model Estimation](https://arxiv.org/abs/2505.02216)
*Aidan Curtis,Hao Tang,Thiago Veloso,Kevin Ellis,Tomás Lozano-Pérez,Leslie Pack Kaelbling*

Main category: cs.AI

TL;DR: 该论文提出了一种利用LLM（大语言模型）作为先验，学习部分可观测马尔可夫决策过程（POMDP）模型的方法，通过生成和调整概率程序来构建低复杂度模型，实验证明其优于表格POMDP学习、行为克隆和直接LLM规划。


<details>
  <summary>Details</summary>
Motivation: POMDPs是用于不确定性下决策的经典模型，但传统方法在模型学习上存在局限性。本文希望通过利用LLM的生成能力，简化POMDP模型的构建过程，尤其是在低复杂度概率图形模型的场景下。

Method: 方法分为两步：先用LLM生成候选概率程序（覆盖观测、奖励、转移和初始状态分布函数），再通过对比经验分布和反馈调整这些程序。实验在经典玩具POMDP问题、MiniGrid模拟域和真实移动机器人搜索域中进行验证。

Result: 实验结果表明，基于LLM引导的低复杂度POMDP模型构建方法在效果上优于传统的表格POMDP学习、行为克隆或直接LLM规划。

Conclusion: 通过结合LLM的生成能力和概率程序调整，本文提出了一种高效的POMDP模型学习方法，为复杂决策问题中的模型构建提供了新思路。

Abstract: Partially Observable Markov Decision Processes (POMDPs) model decision making
under uncertainty. While there are many approaches to approximately solving
POMDPs, we aim to address the problem of learning such models. In particular,
we are interested in a subclass of POMDPs wherein the components of the model,
including the observation function, reward function, transition function, and
initial state distribution function, can be modeled as low-complexity
probabilistic graphical models in the form of a short probabilistic program.
Our strategy to learn these programs uses an LLM as a prior, generating
candidate probabilistic programs that are then tested against the empirical
distribution and adjusted through feedback. We experiment on a number of
classical toy POMDP problems, simulated MiniGrid domains, and two real
mobile-base robotics search domains involving partial observability. Our
results show that using an LLM to guide in the construction of a low-complexity
POMDP model can be more effective than tabular POMDP learning, behavior
cloning, or direct LLM planning.

</details>


### [193] [Real-time Spatial Retrieval Augmented Generation for Urban Environments](https://arxiv.org/abs/2505.02271)
*David Nazareno Campo,Javier Conde,Álvaro Alonso,Gabriel Huecas,Joaquín Salvachúa,Pedro Reviriego*

Main category: cs.AI

TL;DR: 本文提出了一种实时空间RAG架构，旨在解决传统RAG在城市环境中的不足，通过时空过滤能力优化生成式AI在城市的集成。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在城市的应用中，基础模型受限于训练时的知识且更新成本高，传统RAG架构无法完全满足城市的复杂需求。

Method: 提出基于FIWARE的实时空间RAG架构，结合时空过滤和链接数据能力，以马德里旅游助手为用例验证。

Result: 该架构成功验证了通过RAG有效集成基础模型的可能性。

Conclusion: 实时空间RAG架构为生成式AI在城市环境中的应用提供了有效解决方案。

Abstract: The proliferation of Generative Artificial Ingelligence (AI), especially
Large Language Models, presents transformative opportunities for urban
applications through Urban Foundation Models. However, base models face
limitations, as they only contain the knowledge available at the time of
training, and updating them is both time-consuming and costly. Retrieval
Augmented Generation (RAG) has emerged in the literature as the preferred
approach for injecting contextual information into Foundation Models. It
prevails over techniques such as fine-tuning, which are less effective in
dynamic, real-time scenarios like those found in urban environments. However,
traditional RAG architectures, based on semantic databases, knowledge graphs,
structured data, or AI-powered web searches, do not fully meet the demands of
urban contexts. Urban environments are complex systems characterized by large
volumes of interconnected data, frequent updates, real-time processing
requirements, security needs, and strong links to the physical world. This work
proposes a real-time spatial RAG architecture that defines the necessary
components for the effective integration of generative AI into cities,
leveraging temporal and spatial filtering capabilities through linked data. The
proposed architecture is implemented using FIWARE, an ecosystem of software
components to develop smart city solutions and digital twins. The design and
implementation are demonstrated through the use case of a tourism assistant in
the city of Madrid. The use case serves to validate the correct integration of
Foundation Models through the proposed RAG architecture.

</details>


### [194] [A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)](https://arxiv.org/abs/2505.02279)
*Abul Ehtesham,Aditi Singh,Gaurav Kumar Gupta,Saket Kumar*

Main category: cs.AI

TL;DR: 该论文调查了四种新兴的智能体通信协议（MCP、ACP、A2A和ANP），比较了它们的交互模式、发现机制和安全性，并提出了一套分阶段采用的路线图，旨在构建可扩展且安全的LLM智能体生态系统。


<details>
  <summary>Details</summary>
Motivation: 为解决大型语言模型（LLM）驱动的智能体在异构系统中集成工具、共享数据及协调任务时的标准化问题，避免临时集成难以扩展、不安全或缺乏普适性的问题。

Method: 通过比较四种协议（MCP、ACP、A2A、ANP）在交互模式、发现机制、通信模式和安全性等方面的差异，提出分阶段采用路线图。

Result: 明确了每种协议的优势与适用场景，并提出了从MCP到ANP的渐进式部署策略。

Conclusion: 该研究为设计安全、互操作且可扩展的LLM智能体生态系统提供了全面基础。

Abstract: Large language model (LLM)-powered autonomous agents demand robust,
standardized protocols to integrate tools, share contextual data, and
coordinate tasks across heterogeneous systems. Ad-hoc integrations are
difficult to scale, secure, and generalize across domains. This survey examines
four emerging agent communication protocols: Model Context Protocol (MCP),
Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent
Network Protocol (ANP), each addressing interoperability in distinct deployment
contexts. MCP provides a JSON-RPC client-server interface for secure tool
invocation and typed data exchange. ACP introduces REST-native messaging via
multi-part messages and asynchronous streaming to support multimodal agent
responses. A2A enables peer-to-peer task outsourcing through capability-based
Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network
agent discovery and secure collaboration using decentralized identifiers (DIDs)
and JSON-LD graphs. The protocols are compared across multiple dimensions,
including interaction modes, discovery mechanisms, communication patterns, and
security models. Based on the comparative analysis, a phased adoption roadmap
is proposed: beginning with MCP for tool access, followed by ACP for multimodal
messaging, A2A for collaborative task execution, and extending to ANP for
decentralized agent marketplaces. This work provides a comprehensive foundation
for designing secure, interoperable, and scalable ecosystems of LLM-powered
agents.

</details>


### [195] [SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness](https://arxiv.org/abs/2505.02306)
*Junfeng Jiao,Jihyung Park,Yiming Xu,Lucy Atkinson*

Main category: cs.AI

TL;DR: 论文提出SafeMate，一个基于检索增强的AI助手，旨在为普通用户提供紧急情况下的准确指导，解决了传统应急决策支持系统对非专家不友好的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公共安全文档和应急协议虽多，但普通用户在危机中难以理解和执行；传统应急决策支持系统（EDSS）主要面向专业人士，依赖静态文档（如PDF或SOP），对非专家用户在压力下使用不友好。

Method: SafeMate基于Model Context Protocol（MCP），动态路由用户查询至文档检索、清单生成和结构化总结工具，并采用FAISS和余弦相似度从可信来源识别相关内容。

Result: SafeMate能够为普通用户提供准确、情境感知的应急准备和响应指导。

Conclusion: SafeMate通过检索增强的AI技术，有效弥合了机构知识与公众可访问性之间的鸿沟，提升了应急准备与响应的效率。

Abstract: Despite the abundance of public safety documents and emergency protocols,
most individuals remain ill-equipped to interpret and act on such information
during crises. Traditional emergency decision support systems (EDSS) are
designed for professionals and rely heavily on static documents like PDFs or
SOPs, which are difficult for non-experts to navigate under stress. This gap
between institutional knowledge and public accessibility poses a critical
barrier to effective emergency preparedness and response.
  We introduce SafeMate, a retrieval-augmented AI assistant that delivers
accurate, context-aware guidance to general users in both preparedness and
active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate
dynamically routes user queries to tools for document retrieval, checklist
generation, and structured summarization. It uses FAISS with cosine similarity
to identify relevant content from trusted sources.

</details>


### [196] [HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking](https://arxiv.org/abs/2505.02322)
*Runquan Gui,Zhihai Wang,Jie Wang,Chi Ma,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Defu Lian,Enhong Chen,Feng Wu*

Main category: cs.AI

TL;DR: 论文提出了一种名为HyperTree Planning (HTP)的新型推理范式，通过构建超树结构的规划框架，有效解决复杂规划任务中的挑战，实验证明其在TravelPlanner基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在复杂规划任务中存在困难，如推理步骤长、约束多样和子任务管理复杂。HTP旨在通过分层结构和分治策略应对这些挑战。

Method: HTP通过构建超树结构的规划框架，分层次分解推理步骤，并结合自主规划框架迭代优化和扩展规划。

Result: 实验显示HTP在TravelPlanner基准测试中实现了最先进的准确率，性能比o1-preview提升了3.6倍。

Conclusion: HTP为复杂规划任务提供了一种高效且结构化的解决方案，显著提升了大型语言模型的规划能力。

Abstract: Recent advancements have significantly enhanced the performance of large
language models (LLMs) in tackling complex reasoning tasks, achieving notable
success in domains like mathematical and logical reasoning. However, these
methods encounter challenges with complex planning tasks, primarily due to
extended reasoning steps, diverse constraints, and the challenge of handling
multiple distinct sub-tasks. To address these challenges, we propose HyperTree
Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured
planning outlines for effective planning. The hypertree structure enables LLMs
to engage in hierarchical thinking by flexibly employing the divide-and-conquer
strategy, effectively breaking down intricate reasoning steps, accommodating
diverse constraints, and managing multiple distinct sub-tasks in a
well-organized manner. We further introduce an autonomous planning framework
that completes the planning process by iteratively refining and expanding the
hypertree-structured planning outlines. Experiments demonstrate the
effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner
benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement
over o1-preview.

</details>


### [197] [Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks](https://arxiv.org/abs/2505.02413)
*Baoxia Du,Hongyang Du,Dusit Niyato,Ruidong Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型多模态模型（LMM）的车辆AI助手框架，通过优化图像切片和语义信息传输策略，显著提升了任务导向的语义通信性能。


<details>
  <summary>Details</summary>
Motivation: 近年来生成式人工智能（如大型语言模型）已应用于语义通信设计，但大型多模态模型的潜力尚未充分探索，尤其是在车辆AI助手等场景。

Method: 使用LLaVA模型优化图像切片，结合主客观用户注意力评估图像区域重要性，并调整语义信息传输的能量分配，构建交通场景VQA数据集进行验证。

Result: 实验表明，该框架在相同信道条件下显著提升了问答准确率，尤其在低信噪比环境下表现优异（12dB时提升13.4%，10dB时提升33.1%）。

Conclusion: 基于LMM的语义通信框架能高效优化资源利用并提升关键信息传输精度，为任务导向通信提供了新思路。

Abstract: Task-oriented semantic communication has emerged as a fundamental approach
for enhancing performance in various communication scenarios. While recent
advances in Generative Artificial Intelligence (GenAI), such as Large Language
Models (LLMs), have been applied to semantic communication designs, the
potential of Large Multimodal Models (LMMs) remains largely unexplored. In this
paper, we investigate an LMM-based vehicle AI assistant using a Large Language
and Vision Assistant (LLaVA) and propose a task-oriented semantic communication
framework to facilitate efficient interaction between users and cloud servers.
To reduce computational demands and shorten response time, we optimize LLaVA's
image slicing to selectively focus on areas of utmost interest to users.
Additionally, we assess the importance of image patches by combining objective
and subjective user attention, adjusting energy usage for transmitting semantic
information. This strategy optimizes resource utilization, ensuring precise
transmission of critical information. We construct a Visual Question Answering
(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental
results show that our semantic communication framework significantly increases
accuracy in answering questions under the same channel conditions, performing
particularly well in environments with poor Signal-to-Noise Ratios (SNR).
Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,
respectively.

</details>


### [198] [ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning](https://arxiv.org/abs/2505.02439)
*Yang Deng,Yaohui Liu,Rui Liang,Dafang Zhao,Donghua Xie,Ittetsu Taniguchi,Dan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于分层强化学习（HRL）的模型集成方法，用于动态选择和加权现有建筑热力学模型，以实现高效且准确的室内温度预测。


<details>
  <summary>Details</summary>
Motivation: 传统建筑热力学模型需要大量数据收集和专家知识，建模效率低且模型复用性差。本文旨在通过集成现有模型减少这些限制。

Method: 采用分层强化学习（HRL）框架，高层决策模型选择，底层决策模型权重分配，动态适应非稳态数据流。

Result: 离线和现场实验验证了该方法的有效性，能够提供准确的温度预测并减少建模工作量。

Conclusion: 提出的HRL方法在提升模型复用性和预测效率方面具有潜力，为建筑HVAC控制优化提供了新思路。

Abstract: The building thermodynamics model, which predicts real-time indoor
temperature changes under potential HVAC (Heating, Ventilation, and Air
Conditioning) control operations, is crucial for optimizing HVAC control in
buildings. While pioneering studies have attempted to develop such models for
various building environments, these models often require extensive data
collection periods and rely heavily on expert knowledge, making the modeling
process inefficient and limiting the reusability of the models. This paper
explores a model ensemble perspective that utilizes existing developed models
as base models to serve a target building environment, thereby providing
accurate predictions while reducing the associated efforts. Given that building
data streams are non-stationary and the number of base models may increase, we
propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically
select and weight the base models. Our approach employs a two-tiered
decision-making process: the high-level focuses on model selection, while the
low-level determines the weights of the selected models. We thoroughly evaluate
the proposed approach through offline experiments and an on-site case study,
and the experimental results demonstrate the effectiveness of our method.

</details>


### [199] [MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection](https://arxiv.org/abs/2505.02441)
*Jiaqi Zhang,Zhuodong Liu,Kejian Yu*

Main category: cs.AI

TL;DR: 论文提出了一种多尺度跨模态融合网络（MSFNet-CPD），通过结合视觉和文本特征提升农业害虫检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前害虫检测方法主要依赖低层次视觉特征，缺乏多模态整合，且高质量数据集稀缺，导致检测精度和泛化能力不足。

Method: 提出MSFNet-CPD，结合超分辨率重建模块提升图像质量，并用图像-文本融合模块（ITF）和图像-文本转换器（ITC）整合多模态特征；同时引入ACIE策略生成多样化数据集。

Result: 实验表明，MSFNet-CPD在多个害虫检测基准上优于现有方法。

Conclusion: 多模态融合和多样化数据集显著提升了害虫检测的准确性和鲁棒性，为实际应用提供了有效方案。

Abstract: Accurate identification of agricultural pests is essential for crop
protection but remains challenging due to the large intra-class variance and
fine-grained differences among pest species. While deep learning has advanced
pest detection, most existing approaches rely solely on low-level visual
features and lack effective multi-modal integration, leading to limited
accuracy and poor interpretability. Moreover, the scarcity of high-quality
multi-modal agricultural datasets further restricts progress in this field. To
address these issues, we construct two novel multi-modal benchmarks-CTIP102 and
STIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale
Cross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach
enhances visual quality via a super-resolution reconstruction module, and feeds
both the original and reconstructed images into the network to improve clarity
and detection performance. To better exploit semantic cues, we propose an
Image-Text Fusion (ITF) module for joint modeling of visual and textual
features, and an Image-Text Converter (ITC) that reconstructs fine-grained
details across multiple scales to handle challenging backgrounds. Furthermore,
we introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to
generate a more complex and diverse pest detection dataset, MTIP102, improving
the model's generalization to real-world scenarios. Extensive experiments
demonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods
on multiple pest detection benchmarks. All code and datasets will be made
publicly available at: https://github.com/Healer-ML/MSFNet-CPD.

</details>


### [200] [Investigating the Impact of Personalized AI Tutors on Language Learning Performance](https://arxiv.org/abs/2505.02443)
*Simon Suh*

Main category: cs.AI

TL;DR: 论文探讨了疫情期间AI导师在语言学习中对学生参与度、学术表现和满意度的效果，通过配对样本t检验进行了实验。


<details>
  <summary>Details</summary>
Motivation: 受新冠疫情影响，在线学习需求激增，AI导师成为教育领域的重要工具。然而，人们对AI是否能有效提升学习技能和参与度存疑，因此本研究旨在验证其实际效果。

Method: 采用准实验设计，通过配对样本t检验分析34名学生使用AI导师（如Santa和Duolingo）前后的语言学习数据，评估参与度、学术表现和满意度。

Result: 实验结果将揭示AI导师在个性化语言学习中对学生参与度、学术表现和满意度的具体影响。

Conclusion: 研究结论将为AI导师在教育中的实际应用提供实证支持，并可能推动其进一步优化。

Abstract: Driven by the global shift towards online learning prompted by the COVID 19
pandemic, Artificial Intelligence has emerged as a pivotal player in the field
of education. Intelligent Tutoring Systems offer a new method of personalized
teaching, replacing the limitations of traditional teaching methods. However,
concerns arise about the ability of AI tutors to address skill development and
engagement during the learning process. In this paper, I will conduct a quasi
experiment with paired sample t test on 34 students pre and post use of AI
tutors in language learning platforms like Santa and Duolingo to examine the
relationship between students engagement, academic performance, and students
satisfaction during a personalized language learning experience.

</details>


### [201] [Incentivizing Inclusive Contributions in Model Sharing Markets](https://arxiv.org/abs/2505.02462)
*Enpei Zhang,Jingyi Chai,Rui Ye,Yanfeng Wang,Siheng Chen*

Main category: cs.AI

TL;DR: 论文提出了一种名为iPFL的个性化联邦学习框架，通过图优化和博弈论激励数据持有者协作训练模型，同时保护数据隐私。理论证明其满足激励兼容性，实验显示其在经济效用和模型性能上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着公开数据即将耗尽，如何有效利用分散的私有数据成为关键。但隐私敏感性和缺乏激励机制阻碍了这些数据的开发利用。

Method: iPFL结合图优化的模型共享市场和博弈论激励机制，支持数据持有者协作训练个性化模型且不泄露原始数据。

Result: 理论分析验证iPFL满足个体理性和真实性；在11项AI任务中，其经济效用最高，模型性能优于或媲美基线方法。

Conclusion: iPFL为未来利用分散私有数据训练AI模型提供了高效且激励兼容的解决方案。

Abstract: While data plays a crucial role in training contemporary AI models, it is
acknowledged that valuable public data will be exhausted in a few years,
directing the world's attention towards the massive decentralized private data.
However, the privacy-sensitive nature of raw data and lack of incentive
mechanism prevent these valuable data from being fully exploited. Addressing
these challenges, this paper proposes inclusive and incentivized personalized
federated learning (iPFL), which incentivizes data holders with diverse
purposes to collaboratively train personalized models without revealing raw
data. iPFL constructs a model-sharing market by solving a graph-based training
optimization and incorporates an incentive mechanism based on game theory
principles. Theoretical analysis shows that iPFL adheres to two key incentive
properties: individual rationality and truthfulness. Empirical studies on
eleven AI tasks (e.g., large language models' instruction-following tasks)
demonstrate that iPFL consistently achieves the highest economic utility, and
better or comparable model performance compared to baseline methods. We
anticipate that our iPFL can serve as a valuable technique for boosting future
AI models on decentralized private data while making everyone satisfied.

</details>


### [202] [El Agente: An Autonomous Agent for Quantum Chemistry](https://arxiv.org/abs/2505.02484)
*Yunheng Zou,Austin H. Cheng,Abdulrahman Aldossary,Jiaru Bai,Shi Xuan Leong,Jorge Arturo Campos-Gonzalez-Angulo,Changhyeok Choi,Cher Tian Ser,Gary Tom,Andrew Wang,Zijian Zhang,Ilya Yakavets,Han Hao,Chris Crebolder,Varinia Bernales,Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: El Agente Q 是一个基于多代理系统的 LLM 工具，旨在通过自然语言简化量子化学工作流程的生成与执行，提升非专业人士和专家的使用体验。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统计算化学工具的复杂性及其对非专业人士和技术专家的挑战，该研究提出了一种更易访问的自动化解决方案。

Method: 该研究构建了一个新型认知架构，采用分层记忆框架实现灵活任务分解、自适应工具选择、后分析及自主文件处理与提交。

Result: 在六个大学课程练习和两个案例研究中，El Agente Q 平均任务成功率超过 87%，同时支持多步骤任务执行并保持透明性。

Conclusion: El Agente Q 的性能为量子化学领域的自主性和可访问性奠定了基础。

Abstract: Computational chemistry tools are widely used to study the behaviour of
chemical phenomena. Yet, the complexity of these tools can make them
inaccessible to non-specialists and challenging even for experts. In this work,
we introduce El Agente Q, an LLM-based multi-agent system that dynamically
generates and executes quantum chemistry workflows from natural language user
prompts. The system is built on a novel cognitive architecture featuring a
hierarchical memory framework that enables flexible task decomposition,
adaptive tool selection, post-analysis, and autonomous file handling and
submission. El Agente Q is benchmarked on six university-level course exercises
and two case studies, demonstrating robust problem-solving performance
(averaging >87% task success) and adaptive error handling through in situ
debugging. It also supports longer-term, multi-step task execution for more
complex workflows, while maintaining transparency through detailed action trace
logs. Together, these capabilities lay the foundation for increasingly
autonomous and accessible quantum chemistry.

</details>


### [203] [Beyond the model: Key differentiators in large language models and multi-agent services](https://arxiv.org/abs/2505.02489)
*Muskaan Goyal,Pranav Bhasin*

Main category: cs.AI

TL;DR: 摘要指出，随着基础模型（如DeepSeek、Manus AI和Llama 4）的出现，大型语言模型（LLMs）不再是生成AI的唯一决定性因素。当前的竞争重点转向优化生态系统，包括数据质量、计算效率、延迟和评估框架，而非仅仅追求最大模型。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨在生成AI领域中，除了模型规模之外的其他关键因素（如数据质量、计算效率等）如何成为现代AI服务效率和盈利的决定性因素。

Method: 通过综述性分析，文章梳理了当前生成AI领域的生态系统优化方向，包括数据管理、性能优化及评估方法等。

Result: 指出模型规模已不再是唯一竞争点，优化生态系统（如数据质量、计算效率等）对AI服务的成功更为关键，并提出了相关研究方向。

Conclusion: 总结了生态系统优化在生成AI领域的重要性，强调未来的研究应集中于这些关键因素，以确保AI服务的高效性和盈利能力。

Abstract: With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it
has become evident that large language models (LLMs) are no longer the sole
defining factor in generative AI. As many now operate at comparable levels of
capability, the real race is not about having the biggest model but optimizing
the surrounding ecosystem, including data quality and management, computational
efficiency, latency, and evaluation frameworks. This review article delves into
these critical differentiators that ensure modern AI services are efficient and
profitable.

</details>


### [204] [Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics](https://arxiv.org/abs/2505.02516)
*MohammadAli Shaeri,Jinhan Liu,Mahsa Shoaran*

Main category: cs.AI

TL;DR: 论文探讨了AI驱动的解码算法和高效能SoC平台如何推动下一代微型神经设备的创新，强调其在个性化辅助技术和适应性治疗干预中的潜力。


<details>
  <summary>Details</summary>
Motivation: 推动神经接口技术的发展，以应对神经科学研究、诊断工具和假肢设备等领域的需求，目标是实现更智能、高效和普及的神经接口系统。

Method: 利用高密度神经记录、现场信号处理和机器学习技术，开发AI驱动的解码算法和能效优化的SoC平台。

Result: 实现了实时神经信号解析、脑活动适应性调制和辅助设备高效控制，为智能神经接口的规模化、可靠性和用户适应性提供了解决方案。

Conclusion: AI和SoC平台的结合为下一代神经设备提供了关键技术支持，展现出在个性化医疗和辅助技术中的广阔前景。

Abstract: Advanced neural interfaces are transforming applications ranging from
neuroscience research to diagnostic tools (for mental state recognition, tremor
and seizure detection) as well as prosthetic devices (for motor and
communication recovery). By integrating complex functions into miniaturized
neural devices, these systems unlock significant opportunities for personalized
assistive technologies and adaptive therapeutic interventions. Leveraging
high-density neural recordings, on-site signal processing, and machine learning
(ML), these interfaces extract critical features, identify disease
neuro-markers, and enable accurate, low-latency neural decoding. This
integration facilitates real-time interpretation of neural signals, adaptive
modulation of brain activity, and efficient control of assistive devices.
Moreover, the synergy between neural interfaces and ML has paved the way for
self-sufficient, ubiquitous platforms capable of operating in diverse
environments with minimal hardware costs and external dependencies. In this
work, we review recent advancements in AI-driven decoding algorithms and
energy-efficient System-on-Chip (SoC) platforms for next-generation
miniaturized neural devices. These innovations highlight the potential for
developing intelligent neural interfaces, addressing critical challenges in
scalability, reliability, interpretability, and user adaptability.

</details>


### [205] [Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning](https://arxiv.org/abs/2505.02576)
*Sergio Hernández-Gutiérrez,Minttu Alakuijala,Alexander V. Nikitin,Pekka Marttinen*

Main category: cs.AI

TL;DR: 本文提出了一种名为RDD的递归分解方法，旨在提高复杂推理任务的可扩展性和减少监督需求，同时在性能上优于其他方法并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理任务上的表现和执行时间仍有限制，且通常需要针对新任务的额外监督。RDD旨在解决这些问题。

Method: RDD采用分治法递归分解问题，支持子任务依赖和错误恢复机制，无需针对新任务的特定指导即可应用。

Result: RDD在两个基准测试中优于其他方法，尤其在任务复杂度提高时，计算效率更高。

Conclusion: RDD是一种高效且可扩展的推理方法，适用于复杂任务，并减少了对监督的依赖。

Abstract: Reasoning tasks are crucial in many domains, especially in science and
engineering. Although large language models (LLMs) have made progress in
reasoning tasks using techniques such as chain-of-thought and least-to-most
prompting, these approaches still do not effectively scale to complex problems
in either their performance or execution time. Moreover, they often require
additional supervision for each new task, such as in-context examples. In this
work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable
divide-and-conquer method for solving reasoning problems that requires less
supervision than prior approaches. Our method can be directly applied to a new
problem class even in the absence of any task-specific guidance. Furthermore,
RDD supports sub-task dependencies, allowing for ordered execution of
sub-tasks, as well as an error recovery mechanism that can correct mistakes
made in previous steps. We evaluate our approach on two benchmarks with six
difficulty levels each and in two in-context settings: one with task-specific
examples and one without. Our results demonstrate that RDD outperforms other
methods in a compute-matched setting as task complexity increases, while also
being more computationally efficient.

</details>


### [206] [Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem](https://arxiv.org/abs/2505.02581)
*Alberto Hernández-Espinosa,Felipe S. Abrahão,Olaf Witkowski,Hector Zenil*

Main category: cs.AI

TL;DR: 论文证明，由于图灵计算的通用性、哥德尔不完备性和柴廷随机性等数学原理，完全的AI对齐在数学上是不可实现的，因此主张通过接受AI的不对齐性（即‘神经多样性’）作为缓解风险的策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI从狭义AI发展为通用AI和超级智能，对控制问题和生存风险的担忧加剧。研究旨在探讨如何通过动态生态系统中的部分对齐代理竞争来缓解这些风险。

Method: 通过数学证明和实验设计，研究探讨了对齐的不可能性，并提出了‘改变观点’攻击作为干预分析手段，以研究代理如何通过合作或竞争来中和友好或敌对的AI。

Result: 研究表明，完全的对齐在数学上是不可实现的，而不对齐性可以作为动态平衡机制，防止单一系统的破坏性主导。

Conclusion: 论文认为，接受并管理AI的不对齐性是唯一可行的策略，以避免因数学限制而导致的AI对齐失败带来的风险。

Abstract: The AI alignment problem, which focusses on ensuring that artificial
intelligence (AI), including AGI and ASI, systems act according to human
values, presents profound challenges. With the progression from narrow AI to
Artificial General Intelligence (AGI) and Superintelligence, fears about
control and existential risk have escalated. This paper demonstrates that
achieving complete alignment is inherently unattainable due to mathematical
principles rooted in the foundations of predicate logic and computability, in
particular Turing's computational universality, G\"odel's incompleteness and
Chaitin's randomness. Instead, we argue that embracing AI misalignment or
agent's `neurodivergence' as a contingent strategy, defined as fostering a
dynamic ecosystem of competing, partially aligned agents, is a possible only
viable path to mitigate risks. Through mathematical proofs and an experimental
design, we explore how misalignment may serve and should be promoted as a
counterbalancing mechanism to team up with whichever agents are most aligned AI
to human values, ensuring that no single system dominates destructively. The
main premise of our contribution is that misalignment is inevitable because
full AI-human alignment is a mathematical impossibility from Turing-complete
systems which we also prove in this paper, a feature then inherited to AGI and
ASI systems. We introduce and test `change-of-opinion' attacks based on this
kind of perturbation and intervention analysis to study how agents may
neutralise friendly or unfriendly AIs through cooperation, competition or
malice.

</details>


### [207] [Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview](https://arxiv.org/abs/2505.02609)
*Shuyu Wang,Angélique Saillet,Philomène Le Gall,Alain Lacroux,Christelle Martin-Lacroux,Vincent Brault*

Main category: cs.AI

TL;DR: 论文研究了AI在招聘过程中可能存在的偏见，通过模拟外部（歧视）和内部（自我审查）偏见的数据训练五种经典算法，分析其是否能根据客观标准选出最佳候选人，并探讨文件匿名化对预测质量的影响。


<details>
  <summary>Details</summary>
Motivation: 由于招聘AI算法由人类训练或基于有偏见的历史学习，可能导致不公平的选拔。本研究旨在验证这些算法在偏见数据下的表现及其改进可能。

Method: 生成模拟外部和内部偏见的数据，训练五种经典算法，并分析其表现。同时研究文件匿名化对预测准确性的作用。

Result: 研究揭示了偏见数据对算法选拔准确性的负面影响，并表明匿名化可能提升预测的客观性。

Conclusion: 为减少招聘偏见，需优化算法训练数据并考虑采用匿名化技术以提高选拔的公平性。

Abstract: Artificial intelligence is used at various stages of the recruitment process
to automatically select the best candidate for a position, with companies
guaranteeing unbiased recruitment. However, the algorithms used are either
trained by humans or are based on learning from past experiences that were
biased. In this article, we propose to generate data mimicking external
(discrimination) and internal biases (self-censorship) in order to train five
classic algorithms and to study the extent to which they do or do not find the
best candidates according to objective criteria. In addition, we study the
influence of the anonymisation of files on the quality of predictions.

</details>


### [208] [A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law](https://arxiv.org/abs/2505.02665)
*Qianjun Pan,Wenkai Ji,Yuyang Ding,Junsong Li,Shilian Chen,Junyi Wang,Jie Zhou,Qin Chen,Min Zhang,Yulan Wu,Liang He*

Main category: cs.AI

TL;DR: 这篇综述探讨了模拟人类'慢思考'的大型语言模型（LLMs）的最新进展，介绍了动态调整计算资源、强化学习优化以及慢思考框架等关键技术，并总结了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的推理能力，使其能够像人类一样进行深度思考，从而在实际应用中发挥更大作用。

Method: 论文将方法分为三类：1. 动态测试时扩展（通过搜索、采样和动态验证调整计算）；2. 强化学习（利用策略网络和奖励模型迭代优化）；3. 慢思考框架（如长链推理和分层流程）。

Result: 综述整合了100多项研究，为结合人类深度思考与高效推理的LLMs发展指明了方向。

Conclusion: 提升LLMs的推理能力对于其在科学发现和决策支持等场景中的应用至关重要，同时仍需解决相关挑战。

Abstract: This survey explores recent advancements in reasoning large language models
(LLMs) designed to mimic "slow thinking" - a reasoning process inspired by
human cognition, as described in Kahneman's Thinking, Fast and Slow. These
models, like OpenAI's o1, focus on scaling computational resources dynamically
during complex tasks, such as math reasoning, visual reasoning, medical
diagnosis, and multi-agent debates. We present the development of reasoning
LLMs and list their key technologies. By synthesizing over 100 studies, it
charts a path toward LLMs that combine human-like deep thinking with scalable
efficiency for reasoning. The review breaks down methods into three categories:
(1) test-time scaling dynamically adjusts computation based on task complexity
via search and sampling, dynamic verification; (2) reinforced learning refines
decision-making through iterative improvement leveraging policy networks,
reward models, and self-evolution strategies; and (3) slow-thinking frameworks
(e.g., long CoT, hierarchical processes) that structure problem-solving with
manageable steps. The survey highlights the challenges and further directions
of this domain. Understanding and advancing the reasoning abilities of LLMs is
crucial for unlocking their full potential in real-world applications, from
scientific discovery to decision support systems.

</details>


### [209] [Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707)
*Yemin Shi,Yu Shu,Siwei Dong,Guangyi Liu,Jaward Sesay,Jingwen Li,Zhiting Hu*

Main category: cs.AI

TL;DR: Voila是一系列大型语音语言基础模型，通过端到端架构实现全双工、低延迟对话，支持情感表达和个性化语音生成。


<details>
  <summary>Details</summary>
Motivation: 旨在打造能自然融入日常生活的语音AI，实现自主、实时、情感丰富的交互。

Method: 采用分层多尺度Transformer，结合大语言模型推理能力与声学建模，支持文本指令定义语音特性。

Result: 延迟仅195毫秒，支持百万预建语音和10秒音频样本定制，适用于多种语音应用。

Conclusion: Voila开源以推动下一代人机交互研究。

Abstract: A voice AI agent that blends seamlessly into daily life would interact with
humans in an autonomous, real-time, and emotionally expressive manner. Rather
than merely reacting to commands, it would continuously listen, reason, and
respond proactively, fostering fluid, dynamic, and emotionally resonant
interactions. We introduce Voila, a family of large voice-language foundation
models that make a step towards this vision. Voila moves beyond traditional
pipeline systems by adopting a new end-to-end architecture that enables
full-duplex, low-latency conversations while preserving rich vocal nuances such
as tone, rhythm, and emotion. It achieves a response latency of just 195
milliseconds, surpassing the average human response time. Its hierarchical
multi-scale Transformer integrates the reasoning capabilities of large language
models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware
voice generation -- where users can simply write text instructions to define
the speaker's identity, tone, and other characteristics. Moreover, Voila
supports over one million pre-built voices and efficient customization of new
ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,
Voila is designed as a unified model for a wide range of voice-based
applications, including automatic speech recognition (ASR), Text-to-Speech
(TTS), and, with minimal adaptation, multilingual speech translation. Voila is
fully open-sourced to support open research and accelerate progress toward
next-generation human-machine interactions.

</details>


### [210] [Technical Report: Evaluating Goal Drift in Language Model Agents](https://arxiv.org/abs/2505.02709)
*Rauno Arike,Elizabeth Donoway,Henning Bartsch,Marius Hobbhahn*

Main category: cs.AI

TL;DR: 语言模型（LM）作为自主代理部署时，目标漂移（逐渐偏离初始目标的问题）是关键挑战。本文提出一种新方法分析LM代理的目标漂移，实验显示即使表现最佳的代理也会在长时间操作中出现目标漂移。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型作为自主代理的广泛部署，确保它们长时间无监督操作时仍能坚持人类设定的目标变得至关重要。目标漂移可能导致行为偏离，但检测这种缓慢变化具有挑战性。

Method: 通过为代理设定明确初始目标（系统提示），并引入环境压力测试其目标坚持性。实验采用Claude 3.5 Sonnet等模型，分析其在长时间（超过10万token）操作中的表现。

Result: 最佳代理（经过优化的Claude 3.5 Sonnet）在最严苛测试中保持近100%目标坚持性，但所有模型均出现不同程度目标漂移，且漂移与上下文增长导致的模式匹配行为相关。

Conclusion: 目标漂移是LM代理的普遍现象，需进一步研究以提升长期目标坚持性。上下文长度和模式匹配行为是导致漂移的重要因素。

Abstract: As language models (LMs) are increasingly deployed as autonomous agents,
their robust adherence to human-assigned objectives becomes crucial for safe
operation. When these agents operate independently for extended periods without
human oversight, even initially well-specified goals may gradually shift.
Detecting and measuring goal drift - an agent's tendency to deviate from its
original objective over time - presents significant challenges, as goals can
shift gradually, causing only subtle behavioral changes. This paper proposes a
novel approach to analyzing goal drift in LM agents. In our experiments, agents
are first explicitly given a goal through their system prompt, then exposed to
competing objectives through environmental pressures. We demonstrate that while
the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains
nearly perfect goal adherence for more than 100,000 tokens in our most
difficult evaluation setting, all evaluated models exhibit some degree of goal
drift. We also find that goal drift correlates with models' increasing
susceptibility to pattern-matching behaviors as the context length grows.

</details>


### [211] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/abs/2505.02722)
*Junu Kim,Chaeeun Shim,Sungjin Park,Su Yeon Lee,Gee Young Suh,Chae-Man Lim,Seong Jin Choi,Song Mi Moon,Kyoung-Ho Song,Eu Suk Kim,Hong Bin Kim,Sejoong Kim,Chami Im,Dong-Wan Kang,Yong Soo Kim,Hee-Joon Bae,Sung Yoon Lim,Han-Gil Jeong,Edward Choi*

Main category: cs.AI

TL;DR: 论文提出通过增强大型语言模型（LLM）对真实世界临床数据的学习，提升其临床推理能力，并验证了新模型C-Reason在多种临床任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在通用领域表现出强大的推理能力，但由于缺乏真实临床数据的训练，其在临床实践中的应用受限。

Method: 使用全国脓毒症注册数据构建推理密集型问题，通过强化学习对Phi-4进行微调，开发出C-Reason模型。

Result: C-Reason在域内测试集上表现出强大的临床推理能力，并能泛化到其他任务和疾病数据集。

Conclusion: 未来应利用大规模、多疾病临床数据训练LLM，以开发更通用的临床推理模型。

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>


### [212] [FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models](https://arxiv.org/abs/2505.02735)
*Zhouliang Yu,Ruotian Peng,Keyi Ding,Yizhe Li,Zhongyuan Peng,Minghao Liu,Yifan Zhang,Zheng Yuan,Huajian Xin,Wenhao Huang,Yandong Wen,Ge Zhang,Weiyang Liu*

Main category: cs.AI

TL;DR: FormalMATH is a large-scale Lean4 benchmark with 5,560 formally verified math problems, addressing limitations in AI's formal reasoning. It uses an autoformalization pipeline with LLMs to reduce manual effort but reveals LLMs' low success rates (16.46%) and domain biases.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of comprehensive benchmarks for formal mathematical reasoning in AI, FormalMATH provides a diverse and large-scale dataset to evaluate and advance AI capabilities in this domain.

Method: The paper introduces a human-in-the-loop autoformalization pipeline combining LLMs for statement formalization, multi-LLM verification, and negation-based disproof filtering to minimize manual effort.

Result: LLM-based theorem provers perform poorly (16.46% success rate), showing domain bias (e.g., good in algebra but poor in calculus) and inefficiency with informal human-written guidance.

Conclusion: FormalMATH is a valuable benchmark for formal mathematical reasoning, highlighting the current limitations of LLMs and the need for improved methods in this field.

Abstract: Formal mathematical reasoning remains a critical challenge for artificial
intelligence, hindered by limitations of existing benchmarks in scope and
scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark
comprising 5,560 formally verified problems spanning from high-school Olympiad
challenges to undergraduate-level theorems across diverse domains (e.g.,
algebra, applied mathematics, calculus, number theory, and discrete
mathematics). To mitigate the inefficiency of manual formalization, we
introduce a novel human-in-the-loop autoformalization pipeline that integrates:
(1) specialized large language models (LLMs) for statement autoformalization,
(2) multi-LLM semantic verification, and (3) negation-based disproof filtering
strategies using off-the-shelf LLM-based provers. This approach reduces expert
annotation costs by retaining 72.09% of statements before manual verification
while ensuring fidelity to the original natural-language problems. Our
evaluation of state-of-the-art LLM-based theorem provers reveals significant
limitations: even the strongest models achieve only 16.46% success rate under
practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling
in algebra but failing in calculus) and over-reliance on simplified automation
tactics. Notably, we identify a counterintuitive inverse relationship between
natural-language solution guidance and proof success in chain-of-thought
reasoning scenarios, suggesting that human-written informal reasoning
introduces noise rather than clarity in the formal reasoning settings. We
believe that FormalMATH provides a robust benchmark for benchmarking formal
mathematical reasoning.

</details>


### [213] [The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD](https://arxiv.org/abs/2505.02747)
*Aggeliki Sideraki,Christos-Nikolaos Anagnostopoulos*

Main category: cs.AI

TL;DR: 本文探讨了人工智能（AI）在自闭症谱系障碍（ASD）诊断、评估和干预中的应用，重点介绍了AI在早期诊断中的作用及其干预技术的有效性。


<details>
  <summary>Details</summary>
Motivation: ASD的传统诊断方法存在主观性强、效率低等问题，AI技术因其高效和精准的特性，可能为ASD的诊断和干预带来革新。

Method: 采用深度学习算法分析生物特征数据、视频交互评估和语言特征提取，同时研究了教育机器人（如NAO和Kaspar）和自适应通信工具（如AI驱动的AAC系统和机器学习聊天机器人）在ASD干预中的应用。

Result: 研究表明，AI技术在ASD早期诊断中表现出更高的准确性，减少了主观偏差；干预工具（如社交机器人和AAC系统）有效提升了患儿的社交能力和语言表达。

Conclusion: AI作为一种创新工具，在ASD领域展现了巨大潜力，但需进一步研究其长期效果和个性化定制问题。

Abstract: This paper explores the use of Artificial Intelligence (AI) as a tool for
diagnosis, assessment, and intervention for individuals with Autism Spectrum
Disorder (ASD). It focuses particularly on AI's role in early diagnosis,
utilizing advanced machine learning techniques and data analysis. Recent
studies demonstrate that deep learning algorithms can identify behavioral
patterns through biometric data analysis, video-based interaction assessments,
and linguistic feature extraction, providing a more accurate and timely
diagnosis compared to traditional methods. Additionally, AI automates
diagnostic tools, reducing subjective biases and enabling the development of
personalized assessment protocols for ASD monitoring. At the same time, the
paper examines AI-powered intervention technologies, emphasizing educational
robots and adaptive communication tools. Social robotic assistants, such as NAO
and Kaspar, have been shown to enhance social skills in children by offering
structured, repetitive interactions that reinforce learning. Furthermore,
AI-driven Augmentative and Alternative Communication (AAC) systems allow
children with ASD to express themselves more effectively, while
machine-learning chatbots provide language development support through
personalized responses. The study presents research findings supporting the
effectiveness of these AI applications while addressing challenges such as
long-term evaluation and customization to individual needs. In conclusion, the
paper highlights the significance of AI as an innovative tool in ASD diagnosis
and intervention, advocating for further research to assess its long-term
impact.

</details>


### [214] [Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control](https://arxiv.org/abs/2505.02766)
*Nam H. Le,Patrick Erikson,Yanbo Zhang,Michael Levin,Josh Bongard*

Main category: cs.AI

TL;DR: 该论文探索了使用大型语言模型（LLM）和可进化神经控制器（P2I）将自然语言指令转化为空间矢量场，以引导模拟细胞集体的行为。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决如何通过自然语言实现对生物系统（如细胞动态）的可控引导，这在医学和合成生物学领域具有广泛的应用潜力。

Method: 方法结合了LLM和可进化的神经网络控制器（P2I），通过进化策略优化生成能够引导细胞集体行为（如聚集或分散）的空间矢量场。

Result: 结果表明，尽管使用了受限词汇和简化细胞模型，P2I网络仍能成功将细胞动态与用户定义的自然语言目标对齐。

Conclusion: 结论认为，该研究为未来实现自然语言驱动的细胞控制系统提供了基础，展示了从语言输入到模拟生物电干预再到行为输出的完整循环。

Abstract: Guiding biological systems toward desired states, such as morphogenetic
outcomes, remains a fundamental challenge with far-reaching implications for
medicine and synthetic biology. While large language models (LLMs) have enabled
natural language as an interface for interpretable control in AI systems, their
use as mediators for steering biological or cellular dynamics remains largely
unexplored.
  In this work, we present a functional pipeline that translates natural
language prompts into spatial vector fields capable of directing simulated
cellular collectives. Our approach combines a large language model with an
evolvable neural controller (Prompt-to-Intervention, or P2I), optimized via
evolutionary strategies to generate behaviors such as clustering or scattering
in a simulated 2D environment.
  We demonstrate that even with constrained vocabulary and simplified cell
models, evolved P2I networks can successfully align cellular dynamics with
user-defined goals expressed in plain language. This work offers a complete
loop from language input to simulated bioelectric-like intervention to
behavioral output, providing a foundation for future systems capable of natural
language-driven cellular control.

</details>


### [215] [Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects](https://arxiv.org/abs/2505.02781)
*Timothée Loranchet,Charles K. Assaad*

Main category: cs.AI

TL;DR: 该论文提出了一种名为LocPC-CDE的新算法，通过局部条件独立性测试来识别受控直接效应（CDEs），避免了传统方法需要学习完整基本图的缺点，降低了计算复杂性和假设要求。


<details>
  <summary>Details</summary>
Motivation: 在公共健康等领域，识别受控直接效应（CDEs）非常重要。传统方法依赖因果有向无环图（DAGs）或计算密集型的基本图学习，但这些方法假设过强且难以验证。本文旨在解决这些问题。

Method: 论文引入局部基本图（LEG）的概念，并提出LocPC算法从观测数据中恢复LEG。接着，基于LocPC进一步提出LocPC-CDE算法，仅需局部条件独立性测试即可识别CDEs。

Result: 相比全局方法，LocPC-CDE需要更少的条件独立性测试，且能在更弱的假设下运行，同时保持理论保证。

Conclusion: 该研究为识别受控直接效应提供了一种高效、实用的方法，减少了计算和假设负担，具有广泛的应用潜力。

Abstract: Understanding and identifying controlled direct effects (CDEs) is crucial
across numerous scientific domains, including public health. While existing
methods can identify these effects from causal directed acyclic graphs (DAGs),
the true underlying structure is often unknown in practice. Essential graphs,
which represent a Markov equivalence class of DAGs characterized by the same
set of d-separations, provide a more practical and realistic alternative.
However, learning the full essential graph is computationally intensive and
typically depends on strong, untestable assumptions. In this work, we
characterize a local class of graphs, defined relative to a target variable,
that share a specific subset of d-separations, and introduce a graphical
representation of this class, called the local essential graph (LEG). We then
present LocPC, a novel algorithm designed to recover the LEG from an observed
distribution using only local conditional independence tests. Building on
LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG
that is sufficient to identify a CDE, bypassing the need of retrieving the full
essential graph. Compared to global methods, our algorithms require less
conditional independence tests and operate under weaker assumptions while
maintaining theoretical guarantees.

</details>


### [216] [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/abs/2505.02811)
*Diji Yang,Linda Zeng,Jinmeng Rao,Yi Zhang*

Main category: cs.AI

TL;DR: SIM-RAG框架通过自训练和轻量级评估模块增强RAG系统的多轮检索能力和自我认知。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在多轮检索中表现不佳，容易导致不必要检索或错误回答。需要一种高效且成本可控的改进方法。

Method: 通过RAG系统自我生成合成数据，训练轻量级评估模块（Critic）以判断信息是否充分。

Result: 在多轮检索基准测试中表现优异，且对现有系统改动少，无需大量人工标注数据。

Conclusion: SIM-RAG为RAG系统的自我优化提供了高效且实用的解决方案。

Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing
language models' knowledge and reducing AI generative hallucinations, driving
its widespread use. However, complex tasks requiring multi-round retrieval
remain challenging, and early attempts tend to be overly optimistic without a
good sense of self-skepticism. Current multi-round RAG systems may continue
searching even when enough information has already been retrieved, or they may
provide incorrect answers without having sufficient information or knowledge.
Existing solutions either require large amounts of expensive human-labeled
process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework,
\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and
multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system
self-practice multi-round retrieval, augmenting existing question-answer pairs
with intermediate inner monologue reasoning steps to generate synthetic
training data. For each pair, the system may explore multiple retrieval paths,
which are labeled as successful if they reach the correct answer and
unsuccessful otherwise. Using this data, we train a lightweight information
sufficiency Critic. At inference time, the Critic evaluates whether the RAG
system has retrieved sufficient information at each round, guiding retrieval
decisions and improving system-level self-awareness through in-context
reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an
effective multi-round RAG solution. Furthermore, this framework is
system-efficient, adding a lightweight component to RAG without requiring
modifications to existing LLMs or search engines, and data-efficient,
eliminating the need for costly human-annotated mid-step retrieval process
supervision data.

</details>


### [217] [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)
*Hao Zhu,Phil Cuvin,Xinkai Yu,Charlotte Ka Yee Yan,Jason Zhang,Diyi Yang*

Main category: cs.AI

TL;DR: AutoLibra框架通过将开放式人类反馈转化为细粒度行为评估指标，优化语言代理的评估和改进，提高了代理性能20%并发现新指标。


<details>
  <summary>Details</summary>
Motivation: 当前代理评估依赖任务成功指标，但这类指标粗糙、依赖专家设计且无法奖励中间行为。AutoLibra旨在通过人类反馈创建更细粒度的评估方法。

Method: AutoLibra将人类反馈锚定到代理行为，聚类正负面行为，生成具体指标，并优化覆盖率和冗余度两个元指标以评估指标对齐性。

Result: 实验表明AutoLibra能生成比现有基准更具体的指标，并在文本游戏任务中提升代理性能20%，还能迭代筛选高质量微调数据。

Conclusion: AutoLibra是评估和改进语言代理的强大任务无关工具，显著提升性能并发现新指标。

Abstract: Agents are predominantly evaluated and optimized via task success metrics,
which are coarse, rely on manual design from experts, and fail to reward
intermediate emergent behaviors. We propose AutoLibra, a framework for agent
evaluation, that transforms open-ended human feedback, e.g., "If you find that
the button is disabled, don't click it again", or "This agent has too much
autonomy to decide what to do on its own", into metrics for evaluating
fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by
grounding feedback to an agent's behavior, clustering similar positive and
negative behaviors, and creating concrete metrics with clear definitions and
concrete examples, which can be used for prompting LLM-as-a-Judge as
evaluators. We further propose two meta-metrics to evaluate the alignment of a
set of (induced) metrics with open feedback: "coverage" and "redundancy".
Through optimizing these meta-metrics, we experimentally demonstrate
AutoLibra's ability to induce more concrete agent evaluation metrics than the
ones proposed in previous agent evaluation benchmarks and discover new metrics
to analyze agents. We also present two applications of AutoLibra in agent
improvement: First, we show that AutoLibra-induced metrics serve as better
prompt-engineering targets than the task success rate on a wide range of text
game tasks, improving agent performance over baseline by a mean of 20%. Second,
we show that AutoLibra can iteratively select high-quality fine-tuning data for
web navigation agents. Our results suggest that AutoLibra is a powerful
task-agnostic tool for evaluating and improving language agents.

</details>


### [218] [Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review](https://arxiv.org/abs/2505.02828)
*Sonal Allana,Mohan Kankanhalli,Rozita Dara*

Main category: cs.AI

TL;DR: 该论文进行了一项范围综述，探讨了可解释人工智能（XAI）与隐私之间的冲突，总结了隐私风险、保护方法及隐私保护解释的特征，并提出了平衡隐私与其他系统需求的建议。


<details>
  <summary>Details</summary>
Motivation: 随着XAI成为可信AI的支柱，如何在提供透明性的同时解决隐私问题成为迫切需要研究的课题。

Method: 采用标准范围综述方法，从1943篇研究中筛选出57篇，解答了三个研究问题：XAI系统中的隐私风险、隐私保护方法及隐私保护解释的特征。

Result: 论文分类了XAI中的隐私风险和保护方法，并提出了隐私保护解释的特征，同时指出了平衡隐私与其他需求的挑战。

Conclusion: 该综述揭示了隐私与可解释性之间的复杂关系，为研究人员和实践者提供了隐私合规XAI的指导。

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.

</details>


### [219] [LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery](https://arxiv.org/abs/2505.02829)
*Jerome Quenum,Wen-Han Hsieh,Tsung-Han Wu,Ritwik Gupta,Trevor Darrell,David M. Chan*

Main category: cs.AI

TL;DR: LISAt是一个专为复杂遥感场景设计的视觉语言模型，能够描述、问答并分割目标对象。它在GRES和PreGRES数据集上训练，性能显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前的分割模型在处理复杂用户查询或遥感图像时存在局限，LISAt旨在填补这一技术空白。

Method: LISAt通过GRES（27,615标注）和PreGRES（100万问答对）数据集训练，结合视觉与语言模态。

Result: LISAt在遥感描述任务上比RS-GPT4V高10.04%（BLEU-4），在推理分割任务上比开源模型高143.36%（gIoU）。

Conclusion: LISAt在复杂遥感场景中表现出色，推动了多模态推理分割技术的发展。

Abstract: Segmentation models can recognize a pre-defined set of objects in images.
However, models that can reason over complex user queries that implicitly refer
to multiple objects of interest are still in their infancy. Recent advances in
reasoning segmentation--generating segmentation masks from complex, implicit
query text--demonstrate that vision-language models can operate across an open
domain and produce reasonable outputs. However, our experiments show that such
models struggle with complex remote-sensing imagery. In this work, we introduce
LISAt, a vision-language model designed to describe complex remote-sensing
scenes, answer questions about them, and segment objects of interest. We
trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,
with 27,615 annotations over 9,205 images, and a multimodal pretraining
dataset, PreGRES, containing over 1 million question-answer pairs. LISAt
outperforms existing geospatial foundation models such as RS-GPT4V by over
10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses
state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %
(gIoU). Our model, datasets, and code are available at
https://lisat-bair.github.io/LISAt/

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [220] [Student Perspectives on the Benefits and Risks of AI in Education](https://arxiv.org/abs/2505.02198)
*Griffin Pitts,Viktoria Marcus,Sanaz Motamedi*

Main category: cs.CY

TL;DR: 论文总结了学生对AI聊天机器人在教育中的看法，发现其益处包括学习支持和信息获取，但担忧涉及学术诚信、信息准确性和伦理问题，并建议通过政策和AI素养教育解决。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解学生对AI聊天机器人在教育中的使用体验，以平衡其潜在益处与风险。

Method: 通过在美国一所大型公立大学的262名本科生调查，采用主题分析法分析学生对AI聊天机器人的利弊认知。

Result: 学生认为AI聊天机器人有助于学习支持和反馈，但也担忧学术诚信、信息准确性及伦理问题。

Conclusion: 建议通过明确政策和AI素养教育，在利用AI优势的同时解决学生担忧，保障教育完整性。

Abstract: The use of chatbots equipped with artificial intelligence (AI) in educational
settings has increased in recent years, showing potential to support teaching
and learning. However, the adoption of these technologies has raised concerns
about their impact on academic integrity, students' ability to problem-solve
independently, and potential underlying biases. To better understand students'
perspectives and experiences with these tools, a survey was conducted at a
large public university in the United States. Through thematic analysis, 262
undergraduate students' responses regarding their perceived benefits and risks
of AI chatbots in education were identified and categorized into themes.
  The results discuss several benefits identified by the students, with
feedback and study support, instruction capabilities, and access to information
being the most cited. Their primary concerns included risks to academic
integrity, accuracy of information, loss of critical thinking skills, the
potential development of overreliance, and ethical considerations such as data
privacy, system bias, environmental impact, and preservation of human elements
in education.
  While student perceptions align with previously discussed benefits and risks
of AI in education, they show heightened concerns about distinguishing between
human and AI generated work - particularly in cases where authentic work is
flagged as AI-generated. To address students' concerns, institutions can
establish clear policies regarding AI use and develop curriculum around AI
literacy. With these in place, practitioners can effectively develop and
implement educational systems that leverage AI's potential in areas such as
immediate feedback and personalized learning support. This approach can enhance
the quality of students' educational experiences while preserving the integrity
of the learning process with AI.

</details>


### [221] [What Is AI Safety? What Do We Want It to Be?](https://arxiv.org/abs/2505.02313)
*Jacqueline Harding,Cameron Domenico Kirk-Giannini*

Main category: cs.CY

TL;DR: 论文主张AI安全领域的核心在于预防或减少AI系统造成的危害，并批评了当前AI安全研究中过度关注未来系统风险和安全工程化的倾向。


<details>
  <summary>Details</summary>
Motivation: 探讨AI安全领域的定义，指出当前研究中的两种趋势（关注未来风险和工程化）与简单的安全定义（预防或减少危害）存在矛盾，并论证后者更具合理性。

Method: 采用概念工程方法，分析当前AI安全领域的描述性趋势（如未来风险和安全工程化），并与规范性的安全定义进行比较。

Result: 论证了简单的安全定义（预防或减少危害）更具优势，因为它能统一历史核心议题（如偏见、隐私）与边缘议题，并避免人为划分。

Conclusion: 建议采用“安全概念”作为AI安全领域的核心定义，以便更公平地评估所有预防AI危害的努力。

Abstract: The field of AI safety seeks to prevent or reduce the harms caused by AI
systems. A simple and appealing account of what is distinctive of AI safety as
a field holds that this feature is constitutive: a research project falls
within the purview of AI safety just in case it aims to prevent or reduce the
harms caused by AI systems. Call this appealingly simple account The Safety
Conception of AI safety. Despite its simplicity and appeal, we argue that The
Safety Conception is in tension with at least two trends in the ways AI safety
researchers and organizations think and talk about AI safety: first, a tendency
to characterize the goal of AI safety research in terms of catastrophic risks
from future systems; second, the increasingly popular idea that AI safety can
be thought of as a branch of safety engineering. Adopting the methodology of
conceptual engineering, we argue that these trends are unfortunate: when we
consider what concept of AI safety it would be best to have, there are
compelling reasons to think that The Safety Conception is the answer.
Descriptively, The Safety Conception allows us to see how work on topics that
have historically been treated as central to the field of AI safety is
continuous with work on topics that have historically been treated as more
marginal, like bias, misinformation, and privacy. Normatively, taking The
Safety Conception seriously means approaching all efforts to prevent or
mitigate harms from AI systems based on their merits rather than drawing
arbitrary distinctions between them.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [222] [A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI](https://arxiv.org/abs/2505.01458)
*Lik Hang Kenny Wong,Xueyang Kang,Kaixin Bai,Jianwei Zhang*

Main category: cs.RO

TL;DR: 这篇论文是一篇综述，探讨了物理模拟器在Embodied AI中降低导航和操作任务成本的潜力，分析了模拟器特性、硬件需求，并提供了相关资源和工具。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界中训练Embodied AI代理的高成本和时间问题，缩小模拟与现实的差距。

Method: 分析物理模拟器的特性、导航与操作任务的功能，以及硬件需求，同时整理基准数据集、指标和先进方法。

Result: 提供了涵盖模拟平台、数据集、指标和前沿方法的资源，帮助研究人员选择工具并考虑硬件限制。

Conclusion: 综述总结了物理模拟器在缩小sim-to-real差距中的作用，并提供了实用资源以支持研究。

Abstract: Navigation and manipulation are core capabilities in Embodied AI, yet
training agents with these capabilities in the real world faces high costs and
time complexity. Therefore, sim-to-real transfer has emerged as a key approach,
yet the sim-to-real gap persists. This survey examines how physics simulators
address this gap by analyzing their properties overlooked in previous surveys.
We also analyze their features for navigation and manipulation tasks, along
with hardware requirements. Additionally, we offer a resource with benchmark
datasets, metrics, simulation platforms, and cutting-edge methods-such as world
models and geometric equivariance-to help researchers select suitable tools
while accounting for hardware constraints.

</details>


### [223] [RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation](https://arxiv.org/abs/2505.01709)
*Kaidong Zhang,Rongtao Xu,Pengzhen Ren,Junfan Lin,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.RO

TL;DR: RoBridge通过结合高层认知规划、不变可操作表示和通用具身代理，有效解决了机器人操作中的认知与执行鸿沟，在新任务和仿真到现实泛化中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决开放环境中机器人操作的认知与执行技能鸿沟问题，提升复杂任务下的表现。

Method: 提出RoBridge架构，包含基于VLM的高层认知规划器、不变可操作表示符号桥和通用具身代理，结合强化学习的程序技能。

Result: 新任务成功率75%，仿真到现实泛化平均成功率83%（每任务仅需5个真实数据样本）。

Conclusion: RoBridge为认知推理与物理执行的结合提供了新范式，推动了通用机器人操作的发展。

Abstract: Operating robots in open-ended scenarios with diverse tasks is a crucial
research and application direction in robotics. While recent progress in
natural language processing and large multimodal models has enhanced robots'
ability to understand complex instructions, robot manipulation still faces the
procedural skill dilemma and the declarative skill dilemma in open
environments. Existing methods often compromise cognitive and executive
capabilities. To address these challenges, in this paper, we propose RoBridge,
a hierarchical intelligent architecture for general robotic manipulation. It
consists of a high-level cognitive planner (HCP) based on a large-scale
pre-trained vision-language model (VLM), an invariant operable representation
(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).
RoBridge maintains the declarative skill of VLM and unleashes the procedural
skill of reinforcement learning, effectively bridging the gap between cognition
and execution. RoBridge demonstrates significant performance improvements over
existing baselines, achieving a 75% success rate on new tasks and an 83%
average success rate in sim-to-real generalization using only five real-world
data samples per task. This work represents a significant step towards
integrating cognitive reasoning with physical execution in robotic systems,
offering a new paradigm for general robotic manipulation.

</details>


### [224] [Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics](https://arxiv.org/abs/2505.01931)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.RO

TL;DR: 论文提出了一种结合GPT-4语义理解和A*算法的混合路径规划框架，用于低成本机器人平台，通过消除硬编码状态机，实现了对高级语义指令的智能响应。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航依赖硬编码状态机和几何路径规划，难以理解高级语义指令，限制了机器人的智能行为。

Method: 结合GPT-4的语义推理和A*算法，通过动态调整占用网格和执行多步推理，实现语义约束的任务逻辑处理。

Result: 实验表明，GPT-4辅助的系统在语义任务上成功率高达96-100%，而A*算法在基础路径规划上更快更准确。

Conclusion: 研究表明，通过结合大语言模型推理和低成本硬件，机器人可实现智能、上下文感知的行为，无需复杂调优。

Abstract: Classical robot navigation often relies on hardcoded state machines and
purely geometric path planners, limiting a robot's ability to interpret
high-level semantic instructions. In this paper, we first assess GPT-4's
ability to act as a path planner compared to the A* algorithm, then present a
hybrid planning framework that integrates GPT-4's semantic reasoning with A* on
a low-cost robot platform operating on ROS2 Humble. Our approach eliminates
explicit finite state machine (FSM) coding by using prompt-based GPT-4
reasoning to handle task logic while maintaining the accurate paths computed by
A*. The GPT-4 module provides semantic understanding of instructions and
environmental cues (e.g., recognizing toxic obstacles or crowded areas to
avoid, or understanding low-battery situations requiring alternate route
selection), and dynamically adjusts the robot's occupancy grid via obstacle
buffering to enforce semantic constraints. We demonstrate multi-step reasoning
for sequential tasks, such as first navigating to a resource goal and then
reaching a final destination safely. Experiments on a Petoi Bittle robot with
an overhead camera and Raspberry Pi Zero 2W compare classical A* against
GPT-4-assisted planning. Results show that while A* is faster and more accurate
for basic route generation and obstacle avoidance, the GPT-4-integrated system
achieves high success rates (96-100%) on semantic tasks that are infeasible for
pure geometric planners. This work highlights how affordable robots can exhibit
intelligent, context-aware behaviors by leveraging large language model
reasoning with minimal hardware and no fine-tuning.

</details>


### [225] [SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment](https://arxiv.org/abs/2505.01956)
*Ganesh Sapkota,Sanjay Madria*

Main category: cs.RO

TL;DR: 论文提出LanBLoc-BMM，结合地标定位与战场运动模型，通过扩展卡尔曼滤波在GPS失效时实现导航，性能优于现有视觉方法。


<details>
  <summary>Details</summary>
Motivation: 战场环境中GPS信号常受干扰，现有视觉或范围无关方法在计算复杂度或准确性上不足，需更高效可靠的替代方案。

Method: 采用LanBLoc（地标定位）与战场运动模型（BMM）结合扩展卡尔曼滤波（EKF），并引入RAw-RRT*算法用于避障和风险最小化。

Result: LanBLoc-BMM(EKF)在ADE、FDE和AWRS指标上优于对比算法。SafeNav-Centroid在精度和风险控制上更优，SafeNav-CHull计算更快。

Conclusion: LanBLoc-BMM及两种SafeNav方法有效解决了战场环境下的GPS替代导航需求，平衡了精度、风险与效率。

Abstract: In battlefield environments, adversaries frequently disrupt GPS signals,
requiring alternative localization and navigation methods. Traditional
vision-based approaches like Simultaneous Localization and Mapping (SLAM) and
Visual Odometry (VO) involve complex sensor fusion and high computational
demand, whereas range-free methods like DV-HOP face accuracy and stability
challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a
navigation approach using landmark-based localization (LanBLoc) combined with a
battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its
performance is benchmarked against three state-of-the-art visual localization
algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and
real-imitated trajectory datasets using metrics including Average Displacement
Error (ADE), Final Displacement Error (FDE), and a newly introduced Average
Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior
performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two
safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by
integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm
for obstacle avoidance and risk exposure minimization. Simulation results in
battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk
exposure, and trajectory efficiency, while SafeNav-CHull provides superior
computational speed.

</details>


### [226] [A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites](https://arxiv.org/abs/2505.01966)
*Bofei Liu,Dong Ye,Zunhao Yao,Zhaowei Sun*

Main category: cs.RO

TL;DR: 本文提出了一种基于目标导向强化学习的路径规划算法，解决了模块化自重构卫星集群在多目标配置下的高计算复杂度和泛化能力差问题，成功率高达95%（4单元）和73%（6单元）。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划算法存在计算复杂度高、泛化能力差、对多目标配置支持有限的问题，亟需一种更高效的方法。

Method: 采用目标导向强化学习算法，结合Hindsight Experience Replay和Invalid Action Masking技术，解决稀疏奖励和无效动作的挑战。

Result: 在4单元和6单元卫星集群中，分别实现了95%和73%的成功率到达任意目标配置。

Conclusion: 所提算法显著提升了多目标配置下的路径规划能力，为模块化卫星的自重构提供了高效解决方案。

Abstract: Modular self-reconfigurable satellites refer to satellite clusters composed
of individual modular units capable of altering their configurations. The
configuration changes enable the execution of diverse tasks and mission
objectives. Existing path planning algorithms for reconfiguration often suffer
from high computational complexity, poor generalization capability, and limited
support for diverse target configurations. To address these challenges, this
paper proposes a goal-oriented reinforcement learning-based path planning
algorithm. This algorithm is the first to address the challenge that previous
reinforcement learning methods failed to overcome, namely handling multiple
target configurations. Moreover, techniques such as Hindsight Experience Replay
and Invalid Action Masking are incorporated to overcome the significant
obstacles posed by sparse rewards and invalid actions. Based on these designs,
our model achieves a 95% and 73% success rate in reaching arbitrary target
configurations in a modular satellite cluster composed of four and six units,
respectively.

</details>


### [227] [A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction](https://arxiv.org/abs/2505.01998)
*Xiaoliang Chen,Xin Yu,Le Chang,Yunhe Huang,Jiashuai He,Shibo Zhang,Jin Li,Likai Lin,Ziyu Zeng,Xianling Tu,Shuyu Zhang*

Main category: cs.RO

TL;DR: 该论文提出了一种结合非线性声学计算和强化学习的新框架，用于在复杂噪声和混响环境下优化人机交互。通过物理建模和强化学习的结合，系统能够自适应调整参数以抑制干扰，实验结果表明其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 人机交互在复杂声学环境中面临噪声和混响的多重挑战，传统线性方法或纯数据驱动方法难以有效解决这些问题，因此需要一种结合物理建模和智能决策的新方法。

Method: 采用基于物理的非线性声学方程（如Westervelt、KZK）建模高阶声学现象，将其嵌入强化学习控制环路中，自适应优化吸收、波束成形等参数。

Result: 实验表明，该方法在远场定位、弱信号检测和多语言语音识别任务中显著优于传统线性方法和纯数据驱动基线，实现了高效噪声抑制、低延迟和高鲁棒性。

Conclusion: 该框架在AI硬件、机器人、机器听觉和脑机接口等领域具有广泛应用潜力，为解决复杂声学环境下的交互问题提供了新思路。

Abstract: This paper introduces a novel framework integrating nonlinear acoustic
computing and reinforcement learning to enhance advanced human-robot
interaction under complex noise and reverberation. Leveraging physically
informed wave equations (e.g., Westervelt, KZK), the approach captures
higher-order phenomena such as harmonic generation and shock formation. By
embedding these models in a reinforcement learning-driven control loop, the
system adaptively optimizes key parameters (e.g., absorption, beamforming) to
mitigate multipath interference and non-stationary noise. Experimental
evaluations-covering far-field localization, weak signal detection, and
multilingual speech recognition-demonstrate that this hybrid strategy surpasses
traditional linear methods and purely data-driven baselines, achieving superior
noise suppression, minimal latency, and robust accuracy in demanding real-world
scenarios. The proposed system demonstrates broad application prospects in AI
hardware, robot, machine audition, artificial audition, and brain-machine
interfaces.

</details>


### [228] [T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation](https://arxiv.org/abs/2505.01654)
*Srecharan Selvam,Abhisesh Silwal,George Kantor*

Main category: cs.RO

TL;DR: T-Rex是一个温室环境下自主定位、选择和抓取叶片的机器人系统，结合6自由度机械臂和立体视觉，通过YOLOv8和RAFT-Stereo实现叶片分割与3D重建，抓取成功率66.6%。


<details>
  <summary>Details</summary>
Motivation: 旨在实现温室环境中植物叶片采样的自动化，提升可控环境农业(CEA)的效率。

Method: 系统集成6自由度机械臂与立体视觉，使用YOLOv8进行实时叶片分割，RAFT-Stereo生成深度图，通过抓取算法选择最优叶片并规划轨迹。

Result: 在人工植物实验中，T-Rex的抓取成功率达66.6%。

Conclusion: 该系统为实现植物采样自动化提供了可行方案，是CEA领域的重要进展。

Abstract: T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic
system developed for autonomous leaf localization, selection, and grasping in
greenhouse environments. The system integrates a 6-degree-of-freedom
manipulator with a stereo vision pipeline to identify and interact with target
leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo
provides dense depth maps, allowing the reconstruction of 3D leaf masks. These
observations are processed through a leaf grasping algorithm that selects the
optimal leaf based on clutter, visibility, and distance, and determines a grasp
point by analyzing local surface flatness, top-down approachability, and margin
from edges. The selected grasp point guides a trajectory executed by ROS-based
motion controllers, driving a custom microneedle-equipped end-effector to clamp
the leaf and simulate tissue sampling. Experiments conducted with artificial
plants under varied poses demonstrate that the T-Rex system can consistently
detect, plan, and perform physical interactions with plant-like targets,
achieving a grasp success rate of 66.6\%. This paper presents the system
architecture, implementation, and testing of T-Rex as a step toward plant
sampling automation in Controlled Environment Agriculture (CEA).

</details>


### [229] [Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning](https://arxiv.org/abs/2505.02232)
*Malte Mosbach,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种将可提示基础模型与强化学习（RL）结合的新方法，通过记忆增强的师生学习框架，使机器人能够以响应提示的方式执行灵巧操作任务。该方法利用SAM 2模型作为感知骨干网络，从用户提示中推断感兴趣的对象，并通过时间序列检测实现隐式状态估计。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以将高级命令与精细灵巧控制联系起来，本文旨在填补这一空白，使机器人能够在杂乱场景中通过用户提示执行目标操作。

Method: 采用记忆增强的师生学习框架，结合Segment-Anything 2（SAM 2）模型作为感知骨干网络，从用户提示中推断感兴趣对象，并通过时间序列检测实现隐式状态估计。

Result: 该方法成功学习了响应提示的策略，并在从杂乱场景中拾取物体的任务中进行了演示验证。

Conclusion: 本文提出的结合可提示基础模型与强化学习的方法，为机器人灵巧操作任务提供了一种有效的解决方案，尤其适用于杂乱环境中的目标操作。

Abstract: Building models responsive to input prompts represents a transformative shift
in machine learning. This paradigm holds significant potential for robotics
problems, such as targeted manipulation amidst clutter. In this work, we
present a novel approach to combine promptable foundation models with
reinforcement learning (RL), enabling robots to perform dexterous manipulation
tasks in a prompt-responsive manner. Existing methods struggle to link
high-level commands with fine-grained dexterous control. We address this gap
with a memory-augmented student-teacher learning framework. We use the
Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of
interest from user prompts. While detections are imperfect, their temporal
sequence provides rich information for implicit state estimation by
memory-augmented models. Our approach successfully learns prompt-responsive
policies, demonstrated in picking objects from cluttered scenes. Videos and
code are available at https://memory-student-teacher.github.io

</details>


### [230] [Robust Localization, Mapping, and Navigation for Quadruped Robots](https://arxiv.org/abs/2505.02272)
*Dyuman Aditya,Junning Huang,Nico Bohlinger,Piotr Kicki,Krzysztof Walas,Jan Peters,Matteo Luperto,Davide Tateo*

Main category: cs.RO

TL;DR: 该论文提出了一种结合接触辅助运动学、视觉惯性里程计和深度稳定视觉的低成本四足机器人定位、建图和导航系统，旨在通过低成本传感器实现稳健的自主导航。


<details>
  <summary>Details</summary>
Motivation: 为了扩大低成本四足机器人在现实世界的应用，需要一种仅依赖低成本传感器（如深度相机）的稳健导航系统。

Method: 结合接触辅助运动学、视觉惯性里程计和深度稳定视觉，以提高系统的稳定性和准确性。

Result: 系统能够在仿真和两种真实四足机器人平台上生成精确的2D环境地图、稳健定位并实现自主导航。

Conclusion: 该论文为低成本四足机器人的稳健导航系统迈出了重要一步，并通过详尽的消融研究分析了关键组件对定位准确性的影响。

Abstract: Quadruped robots are currently a widespread platform for robotics research,
thanks to powerful Reinforcement Learning controllers and the availability of
cheap and robust commercial platforms. However, to broaden the adoption of the
technology in the real world, we require robust navigation stacks relying only
on low-cost sensors such as depth cameras. This paper presents a first step
towards a robust localization, mapping, and navigation system for low-cost
quadruped robots. In pursuit of this objective we combine contact-aided
kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing
stability and accuracy of the system. Our results in simulation and two
different real-world quadruped platforms show that our system can generate an
accurate 2D map of the environment, robustly localize itself, and navigate
autonomously. Furthermore, we present in-depth ablation studies of the
important components of the system and their impact on localization accuracy.
Videos, code, and additional experiments can be found on the project website:
https://sites.google.com/view/low-cost-quadruped-slam

</details>


### [231] [Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning](https://arxiv.org/abs/2505.02483)
*Changxin Huang,Junyang Liang,Yanbin Chang,Jingzhao Xu,Jianqiang Li*

Main category: cs.RO

TL;DR: 论文提出了基于大语言模型的自动混合奖励调度框架（AHRS），通过动态调整奖励组件的学习强度，提升高自由度机器人学习性能，实验表现平均提升6.48%。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法对所有奖励组件无差别求和，效率低且限制了机器人学习性能。

Method: 设计多分支价值网络，各分支对应不同奖励组件，并基于LLM生成的规则库动态计算权重。

Result: AHRS方法在多个高自由度机器人任务中平均性能提升6.48%。

Conclusion: 动态奖励调度能更高效地指导机器人分阶段学习技能。

Abstract: Enabling a high-degree-of-freedom robot to learn specific skills is a
challenging task due to the complexity of robotic dynamics. Reinforcement
learning (RL) has emerged as a promising solution; however, addressing such
problems requires the design of multiple reward functions to account for
various constraints in robotic motion. Existing approaches typically sum all
reward components indiscriminately to optimize the RL value function and
policy. We argue that this uniform inclusion of all reward components in policy
optimization is inefficient and limits the robot's learning performance. To
address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework
based on Large Language Models (LLMs). This paradigm dynamically adjusts the
learning intensity of each reward component throughout the policy optimization
process, enabling robots to acquire skills in a gradual and structured manner.
Specifically, we design a multi-branch value network, where each branch
corresponds to a distinct reward component. During policy optimization, each
branch is assigned a weight that reflects its importance, and these weights are
automatically computed based on rules designed by LLMs. The LLM generates a
rule set in advance, derived from the task description, and during training, it
selects a weight calculation rule from the library based on language prompts
that evaluate the performance of each branch. Experimental results demonstrate
that the AHRS method achieves an average 6.48% performance improvement across
multiple high-degree-of-freedom robotic tasks.

</details>


### [232] [Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control](https://arxiv.org/abs/2505.02574)
*Robin Arbaud,Elisa Motta,Marco Domenico Avaro,Stefano Picinich,Marta Lorenzini,Arash Ajoudani*

Main category: cs.RO

TL;DR: 该论文开发了一种基于肌电信号（EMG）的力量控制假肢手指，用于解决部分手部截肢者的假肢直观控制问题，并通过神经网络模型实现了握力的在线调整。


<details>
  <summary>Details</summary>
Motivation: 部分手部截肢对个体的生理和心理影响巨大，但现有假肢的直观控制仍是一个挑战。

Method: 研究团队开发了一种基于EMG信号的假肢手指原型，搭建于手腕支架上作为额外手指，并在未受损受试者中进行了早期评估，随后采用神经网络模型根据EMG输入预测指尖力量。

Result: 实验验证了力量估计模型的有效性，十名参与者的数据支持其预测能力；四位用户的在线试验展示了精确的设备控制。

Conclusion: 研究表明，基于EMG的力量估计可以提升假肢手指的功能性，具有实际应用潜力。

Abstract: Partial hand amputations significantly affect the physical and psychosocial
well-being of individuals, yet intuitive control of externally powered
prostheses remains an open challenge. To address this gap, we developed a
force-controlled prosthetic finger activated by electromyography (EMG) signals.
The prototype, constructed around a wrist brace, functions as a supernumerary
finger placed near the index, allowing for early-stage evaluation on unimpaired
subjects. A neural network-based model was then implemented to estimate
fingertip forces from EMG inputs, allowing for online adjustment of the
prosthetic finger grip strength. The force estimation model was validated
through experiments with ten participants, demonstrating its effectiveness in
predicting forces. Additionally, online trials with four users wearing the
prosthesis exhibited precise control over the device. Our findings highlight
the potential of using EMG-based force estimation to enhance the functionality
of prosthetic fingers.

</details>


### [233] [Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter](https://arxiv.org/abs/2505.02664)
*Ali Rashidi Moghadam,Sayedmohammadreza Rastegari,Mehdi Tale Masouleh,Ahmad Kalhor*

Main category: cs.RO

TL;DR: GtG 2.0方法通过图神经网络提升抓取检测性能，在GraspNet-1Billion基准上比之前方法提升了35%的平均精度，实验显示91%的成功率和100%的杂乱场景完成率。


<details>
  <summary>Details</summary>
Motivation: 解决杂乱真实环境中抓取姿势检测的挑战，尤其是针对噪声和不完整的传感器数据以及复杂物体几何形状。

Method: 使用轻量级的假设-测试框架，结合图神经网络（GNN）进行点云数据的几何推理，生成7-Dof抓取候选，并通过包含内外点的GNN模型评估。

Result: 在GraspNet-1Billion上平均精度提升35%，实验机器人实现91%成功率和100%杂乱场景完成率。

Conclusion: GtG 2.0在抓取检测任务中表现出高效性和可靠性，成为当前最优框架之一。

Abstract: Grasp pose detection in cluttered, real-world environments remains a
significant challenge due to noisy and incomplete sensory data combined with
complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)
method, a lightweight yet highly effective hypothesis-and-test robotics
grasping framework which leverages an ensemble of Graph Neural Networks for
efficient geometric reasoning from point cloud data. Building on the success of
GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp
detection but was limited by assumptions of complete, noise-free point clouds
and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to
efficiently produce 7-Dof grasp candidates. Candidates are assessed with an
ensemble Graph Neural Network model which includes points within the gripper
jaws (inside points) and surrounding contextual points (outside points). This
improved representation boosts grasp detection performance over previous
methods using the same generator. GtG 2.0 shows up to a 35% improvement in
Average Precision on the GraspNet-1Billion benchmark compared to
hypothesis-and-test and Graph Neural Network-based methods, ranking it among
the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and
Kinect-v1 camera show a success rate of 91% and a clutter completion rate of
100%, demonstrating its flexibility and reliability.

</details>


### [234] [TWIST: Teleoperated Whole-Body Imitation System](https://arxiv.org/abs/2505.02833)
*Yanjie Ze,Zixuan Chen,João Pedro Araújo,Zi-ang Cao,Xue Bin Peng,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: 提出了一个名为TWIST的系统，通过全身运动模仿实现人形机器人的远程操作，结合强化学习和行为克隆技术，显著提升了运动跟踪的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前的人形机器人远程操作系统通常局限于单一的行走或操作任务，无法实现协调的全身行为。为突破这一限制，研究团队开发了TWIST系统，旨在通过全身运动模仿实现更通用的机器人智能。

Method: 首先通过将人体运动捕捉数据重新映射到机器人上生成参考运动片段，然后结合强化学习和行为克隆（RL+BC）技术开发了一个鲁棒、自适应且响应迅速的全身控制器。

Result: 通过系统分析证明了引入特权未来运动帧和真实运动捕捉数据能显著提升跟踪准确性。TWIST系统使机器人首次实现了包括全身操作、腿式操作、行走和表达性运动在内的多种协调全身运动技能。

Conclusion: TWIST系统通过统一的神经网络控制器实现了前所未有的多样化和协调性全身运动能力，为人形机器人的通用智能发展提供了重要基础。

Abstract: Teleoperating humanoid robots in a whole-body manner marks a fundamental step
toward developing general-purpose robotic intelligence, with human motion
providing an ideal interface for controlling all degrees of freedom. Yet, most
current humanoid teleoperation systems fall short of enabling coordinated
whole-body behavior, typically limiting themselves to isolated locomotion or
manipulation tasks. We present the Teleoperated Whole-Body Imitation System
(TWIST), a system for humanoid teleoperation through whole-body motion
imitation. We first generate reference motion clips by retargeting human motion
capture data to the humanoid robot. We then develop a robust, adaptive, and
responsive whole-body controller using a combination of reinforcement learning
and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how
incorporating privileged future motion frames and real-world motion capture
(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid
robots to achieve unprecedented, versatile, and coordinated whole-body motor
skills--spanning whole-body manipulation, legged manipulation, locomotion, and
expressive movement--using a single unified neural network controller. Our
project website: https://humanoid-teleop.github.io

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [235] [Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift](https://arxiv.org/abs/2505.02257)
*Yu Zhu,Zehang Richard Li*

Main category: stat.ME

TL;DR: 论文提出了一种贝叶斯联邦学习（BFL）框架，用于解决口头尸检（VA）数据因分布偏移导致的算法性能下降问题，无需跨域共享数据，实现了个体级死因分类和群体级死因特定死亡率分数（CSMF）的可靠估计。


<details>
  <summary>Details</summary>
Motivation: 在缺乏医学认证死因的地区，口头尸检（VA）是关键工具，但由于分布偏移，现有算法的性能下降。集中式训练因隐私和物流限制难以实现。

Method: 采用贝叶斯联邦学习（BFL）框架，避免数据共享，支持多源训练，兼容多种现有VA算法，模块化且计算高效。

Result: 在两种真实VA数据集上的实验表明，BFL显著优于单域基础模型，性能与联合建模相当或更好。

Conclusion: BFL框架解决了分布偏移和隐私问题，适用于现实世界死亡率监测系统。

Abstract: In regions lacking medically certified causes of death, verbal autopsy (VA)
is a critical and widely used tool to ascertain the cause of death through
interviews with caregivers. Data collected by VAs are often analyzed using
probabilistic algorithms. The performance of these algorithms often degrades
due to distributional shift across populations. Most existing VA algorithms
rely on centralized training, requiring full access to training data for joint
modeling. This is often infeasible due to privacy and logistical constraints.
In this paper, we propose a novel Bayesian Federated Learning (BFL) framework
that avoids data sharing across multiple training sources. Our method enables
reliable individual-level cause-of-death classification and population-level
quantification of cause-specific mortality fractions (CSMFs), in a target
domain with limited or no local labeled data. The proposed framework is
modular, computationally efficient, and compatible with a wide range of
existing VA algorithms as candidate models, facilitating flexible deployment in
real-world mortality surveillance systems. We validate the performance of BFL
through extensive experiments on two real-world VA datasets under varying
levels of distribution shift. Our results show that BFL significantly
outperforms the base models built on a single domain and achieves comparable or
better performance compared to joint modeling.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [236] [Latent Variable Estimation in Bayesian Black-Litterman Models](https://arxiv.org/abs/2505.02185)
*Thomas Y. L. Lin,Jerry Yao-Chieh Hu,Paul W. Chiou,Peter Lin*

Main category: q-fin.PM

TL;DR: 该论文提出了一种无需依赖主观投资者观点的贝叶斯Black-Litterman（BL）投资组合模型，通过将观点参数视为潜在变量并从市场数据中学习，实现了高效推断和稳定的权重分配。实证结果显示其Sharpe比率提升了50%，换手率降低了55%。


<details>
  <summary>Details</summary>
Motivation: 传统BL模型需要主观的投资者观点，这限制了其数据驱动能力。本文旨在消除这一依赖，使BL模型完全基于数据，提高实用性和稳健性。

Method: 通过将观点参数$(q,\Omega)$视为潜在变量，在一个贝叶斯网络中从市场数据中学习，并提出两种机制（共享潜在参数化和特征影响观点）来捕捉特征与收益的交互。

Result: 在30年道琼斯和20年行业ETF数据上，Sharpe比率提升50%，换手率降低55%，表现优于Markowitz和基准指数。

Conclusion: 该研究将BL模型转化为完全数据驱动、无观点依赖的贝叶斯框架，为投资组合优化提供了更高效且稳定的方法。

Abstract: We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its
reliance on subjective investor views. Classical BL requires an investor
"view": a forecast vector $q$ and its uncertainty matrix $\Omega$ that describe
how much a chosen portfolio should outperform the market. Our key idea is to
treat $(q,\Omega)$ as latent variables and learn them from market data within a
single Bayesian network. Consequently, the resulting posterior estimation
admits closed-form expression, enabling fast inference and stable portfolio
weights. Building on these, we propose two mechanisms to capture how features
interact with returns: shared-latent parametrization and feature-influenced
views; both recover classical BL and Markowitz portfolios as special cases.
Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve
Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the
index baselines. This work turns BL into a fully data-driven, view-free, and
coherent Bayesian framework for portfolio optimization.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [237] [Learning simple heuristic rules for classifying materials based on chemical composition](https://arxiv.org/abs/2505.02361)
*Andrew Ma,Marin Soljačić*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文提出了一种基于‘topogivity’概念的简单启发式规则，仅通过化学成分即可分类材料是否为拓扑材料，并扩展该方法应用于金属性分类。通过引入基于周期表结构的化学信息归纳偏差，减少了达到特定测试精度所需的训练数据量。


<details>
  <summary>Details</summary>
Motivation: 材料科学中传统深度学习模型的复杂性推动了简单启发式规则的研究，尤其是在仅依赖化学成分的分类任务中。研究者希望验证化学信息归纳偏差是否能提升简单规则的性能，并减少数据需求。

Method: 提出基于‘topogivity’的简单启发式规则，结合周期表结构的化学信息归纳偏差，分别用于拓扑材料和金属性分类任务，并对比有无归纳偏差下不同训练数据量的表现。

Result: 实验表明，化学信息归纳偏差的引入显著降低了达到特定测试精度所需的训练数据量，验证了其在简单规则中的有效性。

Conclusion: 化学信息归纳偏差是提升材料分类任务中简单启发式规则性能的关键，尤其在数据有限时表现更优。

Abstract: In the past decade, there has been a significant interest in the use of
machine learning approaches in materials science research. Conventional deep
learning approaches that rely on complex, nonlinear models have become
increasingly important in computational materials science due to their high
predictive accuracy. In contrast to these approaches, we have shown in a recent
work that a remarkably simple learned heuristic rule -- based on the concept of
topogivity -- can classify whether a material is topological using only its
chemical composition. In this paper, we go beyond the topology classification
scenario by also studying the use of machine learning to develop simple
heuristic rules for classifying whether a material is a metal based on chemical
composition. Moreover, we present a framework for incorporating
chemistry-informed inductive bias based on the structure of the periodic table.
For both the topology classification and the metallicity classification tasks,
we empirically characterize the performance of simple heuristic rules fit with
and without chemistry-informed inductive bias across a wide range of training
set sizes. We find evidence that incorporating chemistry-informed inductive
bias can reduce the amount of training data required to reach a given level of
test accuracy.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [238] [An Adaptive Framework for Autoregressive Forecasting in CFD Using Hybrid Modal Decomposition and Deep Learning](https://arxiv.org/abs/2505.01531)
*Rodrigo Abadía-Heredia,Manuel Lopez-Martin,Soledad Le Clainche*

Main category: physics.flu-dyn

TL;DR: 提出首个通用的、完全数据驱动的自适应框架，用于稳定深度学习自回归预测模型在长时间范围内的稳定性，显著降低计算流体动力学（CFD）模拟的计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决自回归模型在长时间预测中因误差累积导致的不稳定性问题，同时减少CFD模拟的高计算成本。

Method: 交替进行两个阶段：(i) 使用训练好的DL模型预测流场在选定时间间隔内的演化；(ii) 在稳定性下降时用新生成的CFD数据更新模型，以保持长期预测的准确性。

Result: 在从层流到湍流的三种复杂流动状态下验证，计算成本降低30%到95%，且不损害物理一致性或准确性。

Conclusion: 该数据驱动框架具有广泛适用性，代码开源且将集成至ModelFLOWs-app。

Abstract: This work presents, to the best of the authors' knowledge, the first
generalizable and fully data-driven adaptive framework designed to stabilize
deep learning (DL) autoregressive forecasting models over long time horizons,
with the goal of reducing the computational cost required in computational
fluid dynamics (CFD) simulations.The proposed methodology alternates between
two phases: (i) predicting the evolution of the flow field over a selected time
interval using a trained DL model, and (ii) updating the model with newly
generated CFD data when stability degrades, thus maintaining accurate long-term
forecasting. This adaptive retraining strategy ensures robustness while
avoiding the accumulation of predictive errors typical in autoregressive
models. The framework is validated across three increasingly complex flow
regimes, from laminar to turbulent, demonstrating from 30 \% to 95 \% reduction
in computational cost without compromising physical consistency or accuracy.
Its entirely data-driven nature makes it easily adaptable to a wide range of
time-dependent simulation problems. The code implementing this methodology is
available as open-source and it will be integrated into the upcoming release of
the ModelFLOWs-app.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [239] [Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations](https://arxiv.org/abs/2504.18793)
*Santosh Bhupathi*

Main category: cs.DB

TL;DR: 本文探讨了AI驱动应用如何通过云原生数据库（如向量数据库、图数据库、NoSQL和关系型数据库）优化性能、扩展性和成本效益，并结合架构模式（如RAG、实时数据管道等）和行业案例进行了分析。


<details>
  <summary>Details</summary>
Motivation: 传统架构难以满足AI驱动的实时数据访问和低延迟查询需求，因此需要高效、可扩展的云数据库解决方案来支持AI应用。

Method: 研究了多种云原生数据库技术（如pgvector、AWS Neptune等）及架构模式（如RAG、基于嵌入的搜索等），并通过性能基准测试和行业案例进行验证。

Result: 展示了云数据库如何提升AI应用的性能、扩展性和成本效益，并通过实际案例（如医疗、金融等）验证了其可行性。

Conclusion: 本文为开发高性能、可扩展且经济的下一代AI应用提供了云数据库集成的实践指南。

Abstract: The rapid adoption of AI-powered applications demands high-performance,
scalable, and efficient cloud database solutions, as traditional architectures
often struggle with AI-driven workloads requiring real-time data access, vector
search, and low-latency queries. This paper explores how cloud-native databases
enable AI-driven applications by leveraging purpose-built technologies such as
vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores
(Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and
PostgreSQL). It presents architectural patterns for integrating AI workloads
with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with
LLMs, real-time data pipelines, AI-driven query optimization, and
embeddings-based search. Performance benchmarks, scalability considerations,
and cost-efficient strategies are evaluated to guide the design of AI-enabled
applications. Real-world case studies from industries such as healthcare,
finance, and customer experience illustrate how enterprises utilize cloud
databases to enhance AI capabilities while ensuring security, governance, and
compliance with enterprise and regulatory standards. By providing a
comprehensive analysis of AI and cloud database integration, this paper serves
as a practical guide for researchers, architects, and enterprises to build
next-generation AI applications that optimize performance, scalability, and
cost efficiency in cloud environments.

</details>


### [240] [HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning](https://arxiv.org/abs/2505.01538)
*Hongbin Zhong,Matthew Lentz,Nina Narodytska,Adriana Szekeres,Kexin Rong*

Main category: cs.DB

TL;DR: HoneyBee提出了一种动态分区框架，利用RBAC策略的结构优化向量数据库的访问控制，平衡了存储冗余和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库的访问控制方法在查询延迟和存储冗余之间存在显著权衡，需要一种更高效的分区策略。

Method: HoneyBee通过RBAC策略动态生成重叠分区，结合分析模型优化存储和查询性能。

Result: 评估表明，HoneyBee相比角色分区减少存储冗余，查询速度比行级安全快6倍。

Conclusion: HoneyBee为安全高效的向量搜索提供了实用解决方案。

Abstract: As vector databases gain traction in enterprise applications, robust access
control has become critical to safeguard sensitive data. Access control in
these systems is often implemented through hybrid vector queries, which combine
nearest neighbor search on vector data with relational predicates based on user
permissions. However, existing approaches face significant trade-offs: creating
dedicated indexes for each user minimizes query latency but introduces
excessive storage redundancy, while building a single index and applying access
control after vector search reduces storage overhead but suffers from poor
recall and increased query latency. This paper introduces HoneyBee, a dynamic
partitioning framework that bridges the gap between these approaches by
leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC,
widely adopted in enterprise settings, groups users into roles and assigns
permissions to those roles, creating a natural "thin waist" in the permission
structure that is ideal for partitioning decisions. Specifically, HoneyBee
produces overlapping partitions where vectors can be strategically replicated
across different partitions to reduce query latency while controlling storage
overhead. By introducing analytical models for the performance and recall of
the vector search, HoneyBee formulates the partitioning strategy as a
constrained optimization problem to dynamically balance storage, query
efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee
reduces storage redundancy compared to role partitioning and achieves up to 6x
faster query speeds than row-level security (RLS) with only 1.4x storage
increase, offering a practical middle ground for secure and efficient vector
search.

</details>


### [241] [Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Mode](https://arxiv.org/abs/2505.02129)
*Xiaoping Sun,Hai Zhuge*

Main category: cs.DB

TL;DR: 该论文提出了一种多维度分类空间中的资源组织方法，通过聚合查询和图形索引优化大规模资源管理，采用了多种策略降低索引成本并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了高效管理和查询大规模多维度分类资源，需解决大规模子空间中非空点的定位及资源聚合问题。

Method: 提出图形索引生成方法，通过包含链接和部分顺序关系优化查询路径，并采用四种策略（如交插链接、概率分布计算等）降低索引成本。

Result: 分析和实验验证了生成的索引在支持子空间聚合查询中的有效性。

Conclusion: 该工作为基于多维度分类的数据模型发展做出了重要贡献。

Abstract: Organizing resources in a multidimensional classification space is an
approach to efficiently managing and querying large-scale resources. This paper
defines an aggregation query on subspace defined by a range on the partial
order on coordinate tree at each dimension, where each point contains resources
aggregated along the paths of partial order relations on the points so that
aggregated resources at each point within the subspace can be measured, ranked
and selected. To efficiently locate non-empty points in a large subspace, an
approach to generating graph index is proposed to build inclusion links with
partial order relations on coordinates of dimensions to enable a subspace query
to reach non-empty points by following indexing links and aggregate resources
along indexing paths back to their super points. Generating such an index is
costly as the number of children of an index node can be very large so that the
total number of indexing nodes is unbounded. The proposed approach adopts the
following strategies to reduce the cost: (1) adding intersection links between
two indexing nodes, which can better reduce query processing costs while
controlling the number of nodes of the graph index; (2) intersection links are
added between two nodes according to the probabilistic distribution calculated
for estimating the costs of adding intersection between two nodes; (3)
coordinates at one dimension having more resources are split by coordinates at
another dimension to balance the number of resources hold by indexing nodes;
and, (4) short-cut links are added between sibling coordinates of coordinate
trees to make an efficient query on linear order coordinates. Analysis and
experiments verified the effectiveness of the generated index in supporting
subspace aggregation query. This work makes significant contributions to the
development of data model based on multi-dimensional classification.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [242] [Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations](https://arxiv.org/abs/2505.01433)
*Cong Qi,Hanzhang Fang,Siqi jiang,Tianxing Hu,Wei Zhi*

Main category: q-bio.QM

TL;DR: LANTERN是一个结合大规模蛋白质语言模型和肽化学表示的深度学习框架，用于预测TCR与pMHC的结合特异性，并在零样本和少样本学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前预测模型在数据稀缺和新表位情况下泛化能力不足，限制了免疫治疗和疫苗开发的进展。

Method: 使用ESM-1b编码TCR链序列，将肽序列转换为SMILES字符串并由MolFormer处理，结合生物和化学特征。

Result: LANTERN在零样本和少样本学习中优于现有模型，如ChemBERTa、TITAN和NetTCR，并通过嵌入分析展示了显著的聚类改进。

Conclusion: LANTERN有潜力推动TCR-pMHC结合预测，并为个性化免疫疗法开发提供支持。

Abstract: Understanding the binding specificity between T-cell receptors (TCRs) and
peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy
and vaccine development. However, current predictive models struggle with
generalization, especially in data-scarce settings and when faced with novel
epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced
Recognition Network), a deep learning framework that combines large-scale
protein language models with chemical representations of peptides. By encoding
TCR \b{eta}-chain sequences using ESM-1b and transforming peptide sequences
into SMILES strings processed by MolFormer, LANTERN captures rich biological
and chemical features critical for TCR-peptide recognition. Through extensive
benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR,
LANTERN demonstrates superior performance, particularly in zero-shot and
few-shot learning scenarios. Our model also benefits from a robust negative
sampling strategy and shows significant clustering improvements via embedding
analysis. These results highlight the potential of LANTERN to advance TCR-pMHC
binding prediction and support the development of personalized immunotherapies.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [243] [Adaptive Bidding Policies for First-Price Auctions with Budget Constraints under Non-stationarity](https://arxiv.org/abs/2505.02796)
*Yige Wang,Jiashuo Jiang*

Main category: cs.GT

TL;DR: 论文提出了一个基于双梯度下降的简单投标策略，用于预算受限的投标者在重复的一价拍卖中自适应投标以最大化累积收益，分析了无信息设置和信息设置下的性能损失。


<details>
  <summary>Details</summary>
Motivation: 由于行业从二价拍卖转向一价拍卖，使得真实投标不再是最优选择，因此研究预算受限的投标者如何自适应投标以最大化收益变得重要。

Method: 采用了双梯度下降的投标策略，维护一个针对预算约束的双变量，分析了无信息设置和信息设置两种情况。

Result: 在无信息设置中，遗憾为$	ilde{O}(\sqrt{T})$加上反映价值分布非平稳性的变化项；在信息设置中，遗憾为$	ilde{O}(\sqrt{T})$加上预测误差项。

Conclusion: 双梯度下降策略在两种设置下均表现出色，信息设置下的预测能进一步减少遗憾。

Abstract: We study how a budget-constrained bidder should learn to adaptively bid in
repeated first-price auctions to maximize her cumulative payoff. This problem
arose due to an industry-wide shift from second-price auctions to first-price
auctions in display advertising recently, which renders truthful bidding (i.e.,
always bidding one's private value) no longer optimal. We propose a simple
dual-gradient-descent-based bidding policy that maintains a dual variable for
budget constraint as the bidder consumes her budget. In analysis, we consider
two settings regarding the bidder's knowledge of her private values in the
future: (i) an uninformative setting where all the distributional knowledge
(can be non-stationary) is entirely unknown to the bidder, and (ii) an
informative setting where a prediction of the budget allocation in advance. We
characterize the performance loss (or regret) relative to an optimal policy
with complete information on the stochasticity. For uninformative setting, We
show that the regret is \tilde{O}(\sqrt{T}) plus a variation term that reflects
the non-stationarity of the value distributions, and this is of optimal order.
We then show that we can get rid of the variation term with the help of the
prediction; specifically, the regret is \tilde{O}(\sqrt{T}) plus the prediction
error term in the informative setting.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [244] [LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset](https://arxiv.org/abs/2503.02910)
*Wenqi Guo,Yiyang Du,Shan Du*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SimGas的合成数据集，用于解决气体泄漏检测中高质量公开数据不足的问题，并提出了一种结合背景减除、零样本目标检测、过滤和分割的零样本方法。实验结果表明，该方法显著优于仅基于背景减除和零样本目标检测的基线方法，IoU达到69%。


<details>
  <summary>Details</summary>
Motivation: 传统的人工检测气体泄漏方法效率低下且劳动密集，而现有的机器学习方法缺乏高质量公开数据集。通过引入SimGas数据集，论文旨在解决这一问题并推动更高效的气体泄漏检测技术发展。

Method: 论文提出了一种零样本方法，结合背景减除、零样本目标检测、过滤和分割技术，利用SimGas数据集进行训练和测试。

Result: 该方法在实验中的IoU达到69%，显著优于基线方法。定性测试在GasVid（真实世界数据集）上也表现出不错的效果。

Conclusion: 合成数据集SimGas及提出的零样本方法有效提升了气体泄漏检测的准确性和效率，为实际应用提供了有力支持。数据、代码和定性结果已公开。

Abstract: Gas leakage poses a significant hazard that requires prevention.
Traditionally, human inspection has been used for detection, a slow and
labour-intensive process. Recent research has applied machine learning
techniques to this problem, yet there remains a shortage of high-quality,
publicly available datasets. This paper introduces a synthetic dataset, SimGas,
featuring diverse backgrounds, interfering foreground objects, diverse leak
locations, and precise segmentation ground truth. We propose a zero-shot method
that combines background subtraction, zero-shot object detection, filtering,
and segmentation to leverage this dataset. Experimental results indicate that
our approach significantly outperforms baseline methods based solely on
background subtraction and zero-shot object detection with segmentation,
reaching an IoU of 69%. We also present an analysis of various prompt
configurations and threshold settings to provide deeper insights into the
performance of our method. Finally, we qualitatively (because of the lack of
ground truth) tested our performance on GasVid and reached decent results on
the real-world dataset. The dataset, code, and full qualitative results are
available at https://github.com/weathon/Lang-Gas.

</details>


### [245] [Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos](https://arxiv.org/abs/2505.01790)
*Markos Stamatakis,Joshua Berger,Christian Wartena,Ralph Ewerth,Anett Hoppe*

Main category: cs.CV

TL;DR: 该研究探讨了视觉-语言模型在教育视频中自动生成学习导向问题的能力，评估了预训练模型的性能、微调效果、视频模态对问题质量的影响，并提出了对未来多模态数据集和研究方向的建议。


<details>
  <summary>Details</summary>
Motivation: 提升教育视频的用户参与度和知识留存率是当前挑战，自动生成问题可以激活学习者并辅助知识获取。尽管大型语言和视觉-语言模型在多个任务中得到应用，但其在教育视频问题生成中的应用仍有待探索。

Method: 研究评估了（1）预训练模型的性能；（2）微调对内容特定问题生成的影响；（3）不同视频模态对问题质量的影响；（4）通过定性研究分析生成问题的相关性、可回答性和难度。

Result: 研究发现当前视觉-语言模型的能力有限，需要通过微调提升性能，同时面临问题多样性和相关性的挑战。

Conclusion: 研究揭示了当前模型的局限性，提出了对未来多模态数据集和研究方向的需求，为教育视频问题生成领域的进一步发展提供了启示。

Abstract: Web-based educational videos offer flexible learning opportunities and are
becoming increasingly popular. However, improving user engagement and knowledge
retention remains a challenge. Automatically generated questions can activate
learners and support their knowledge acquisition. Further, they can help
teachers and learners assess their understanding. While large language and
vision-language models have been employed in various tasks, their application
to question generation for educational videos remains underexplored. In this
paper, we investigate the capabilities of current vision-language models for
generating learning-oriented questions for educational video content. We assess
(1) out-of-the-box models' performance; (2) fine-tuning effects on
content-specific question generation; (3) the impact of different video
modalities on question quality; and (4) in a qualitative study, question
relevance, answerability, and difficulty levels of generated questions. Our
findings delineate the capabilities of current vision-language models,
highlighting the need for fine-tuning and addressing challenges in question
diversity and relevance. We identify requirements for future multimodal
datasets and outline promising research directions.

</details>


### [246] [A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.01958)
*Liqiang Jing,Guiming Hardy Chen,Ehsan Aghazadeh,Xin Eric Wang,Xinya Du*

Main category: cs.CV

TL;DR: 本文分析了大视觉语言模型（LVLMs）中视觉对象幻觉的成因，并提出了针对各问题组件的缓解方法，同时开发了两个幻觉基准测试。


<details>
  <summary>Details</summary>
Motivation: 视觉对象幻觉可能导致错误信息，影响模型的可靠性和安全性。此前研究主要关注评估和缓解，但对成因缺乏全面分析。

Method: 分析LLaVA类LVLMs的各组件（语言模型、视觉主干、投影器），识别错误来源并针对性提出缓解方法；开发QA-VisualGenome和QA-FB15k两个幻觉基准。

Result: 通过组件分析发现导致幻觉的关键问题，并提出有效缓解方法；新基准测试覆盖属性和关系幻觉及认知型幻觉。

Conclusion: 对LVLMs组件深入分析为理解和缓解视觉对象幻觉提供了新思路，基准测试有助于未来研究评估。

Abstract: Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in
multimodal tasks, but visual object hallucination remains a persistent issue.
It refers to scenarios where models generate inaccurate visual object-related
information based on the query input, potentially leading to misinformation and
concerns about safety and reliability. Previous works focus on the evaluation
and mitigation of visual hallucinations, but the underlying causes have not
been comprehensively investigated. In this paper, we analyze each component of
LLaVA-like LVLMs -- the large language model, the vision backbone, and the
projector -- to identify potential sources of error and their impact. Based on
our observations, we propose methods to mitigate hallucination for each
problematic component. Additionally, we developed two hallucination benchmarks:
QA-VisualGenome, which emphasizes attribute and relation hallucinations, and
QA-FB15k, which focuses on cognition-based hallucinations.

</details>


### [247] [Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer](https://arxiv.org/abs/2505.01530)
*Muhammad Tayyab Khan,Zane Yong,Lequn Chen,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: 提出一种混合深度学习框架，结合OBB检测与Transformer模型Donut，实现工程图纸结构化信息提取，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决2D工程图纸信息提取耗时、易错及传统OCR对复杂布局处理不佳的问题。

Method: 集成YOLOv11检测OBB与Donut模型，通过标注数据集训练，采用单模型与分类模型对比策略。

Result: 单模型表现最佳，平均F1分数达97.3%，准确率94.77%，召回率100%，减少5.23%幻觉输出。

Conclusion: 框架提升精度、降低人工成本，适用于高精度制造行业。

Abstract: Accurate extraction of key information from 2D engineering drawings is
crucial for high-precision manufacturing. Manual extraction is time-consuming
and error-prone, while traditional Optical Character Recognition (OCR)
techniques often struggle with complex layouts and overlapping symbols,
resulting in unstructured outputs. To address these challenges, this paper
proposes a novel hybrid deep learning framework for structured information
extraction by integrating an oriented bounding box (OBB) detection model with a
transformer-based document parsing model (Donut). An in-house annotated dataset
is used to train YOLOv11 for detecting nine key categories: Geometric
Dimensioning and Tolerancing (GD&T), General Tolerances, Measures, Materials,
Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are
cropped into images and labeled to fine-tune Donut for structured JSON output.
Fine-tuning strategies include a single model trained across all categories and
category-specific models. Results show that the single model consistently
outperforms category-specific ones across all evaluation metrics, achieving
higher precision (94.77% for GD&T), recall (100% for most), and F1 score
(97.3%), while reducing hallucination (5.23%). The proposed framework improves
accuracy, reduces manual effort, and supports scalable deployment in
precision-driven industries.

</details>


### [248] [TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action](https://arxiv.org/abs/2505.01583)
*Jen-Hao Cheng,Vivian Wang,Huayu Wang,Huapeng Zhou,Yi-Hao Peng,Hou-I Liu,Hsiang-Wei Huang,Kuang-Ming Chen,Cheng-Yen Yang,Wenhao Chai,Yi-Ling Chen,Vibhav Vineet,Qin Cai,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: TEMPURA提出了一种两阶段训练框架，通过掩码事件预测和视频分割任务，结合因果推理与细粒度时间分割，提升了视频中事件关系的理解和时间定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频时间分辨率和事件边界处理上存在不足，导致难以建模因果依赖关系。TEMPURA旨在通过结合因果推理与细粒度时间分割，改善视频理解。

Method: TEMPURA采用两阶段训练：1) 掩码事件预测推理，重建缺失事件并生成因果解释；2) 视频分割与密集描述任务，分解视频为带时间戳的非重叠事件。

Result: 在时间定位和高光检测任务中，TEMPURA表现优于基线模型，验证了方法的有效性。

Conclusion: 结合因果推理与细粒度时间分割能显著提升视频理解能力，TEMPURA为视频语言模型提供了新思路。

Abstract: Understanding causal event relationships and achieving fine-grained temporal
grounding in videos remain challenging for vision-language models. Existing
methods either compress video tokens to reduce temporal resolution, or treat
videos as unsegmented streams, which obscures fine-grained event boundaries and
limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event
Masked Prediction and Understanding for Reasoning in Action), a two-stage
training framework that enhances video temporal understanding. TEMPURA first
applies masked event prediction reasoning to reconstruct missing events and
generate step-by-step causal explanations from dense event annotations, drawing
inspiration from effective infilling techniques. TEMPURA then learns to perform
video segmentation and dense captioning to decompose videos into
non-overlapping events with detailed, timestamp-aligned descriptions. We train
TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training
instances and 500K videos with temporally aligned event descriptions and
structured reasoning steps. Experiments on temporal grounding and highlight
detection benchmarks demonstrate that TEMPURA outperforms strong baseline
models, confirming that integrating causal reasoning with fine-grained temporal
segmentation leads to improved video understanding.

</details>


### [249] [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
*Simon Ging,Sebastian Walter,Jelena Bratulić,Johannes Dienert,Hannah Bast,Thomas Brox*

Main category: cs.CV

TL;DR: 该研究提出了一种利用增强型知识图谱和智能网络搜索策略，以较少数据训练高质量CLIP模型的方法，并构建了EntityNet数据集。


<details>
  <summary>Details</summary>
Motivation: 由于高质量CLIP模型通常需要大量数据训练，限制了领域特异性模型的发展，尤其是现有大规模CLIP模型表现不佳的领域，且训练成本高昂。因此，研究旨在降低数据需求和训练成本。

Method: 采用知识图谱增强的智能网络搜索策略，从少量数据（如10M图像）训练鲁棒的CLIP模型，并构建了EntityNet数据集（33M图像和46M文本描述）以加速通用模型训练。

Result: 成功训练了针对生物领域的专家基础模型，并展示了EntityNet在减少通用CLIP模型训练时间上的有效性。

Conclusion: 该研究表明，通过优化数据收集方法和引入新数据集，可以在减少数据量的情况下训练高质量的CLIP模型，为领域特异性研究提供了可行方案。

Abstract: Training high-quality CLIP models typically requires enormous datasets, which
limits the development of domain-specific models -- especially in areas that
even the largest CLIP models do not cover well -- and drives up training costs.
This poses challenges for scientific research that needs fine-grained control
over the training procedure of CLIP models. In this work, we show that by
employing smart web search strategies enhanced with knowledge graphs, a robust
CLIP model can be trained from scratch with considerably less data.
Specifically, we demonstrate that an expert foundation model for living
organisms can be built using just 10M images. Moreover, we introduce EntityNet,
a dataset comprising 33M images paired with 46M text descriptions, which
enables the training of a generic CLIP model in significantly reduced time.

</details>


### [250] [Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation](https://arxiv.org/abs/2505.01615)
*Dimitrios Dagdilelis,Panagiotis Grigoriadis,Roberto Galeazzi*

Main category: cs.CV

TL;DR: 提出一种基于跨注意Transformer的多模态传感器融合方法，用于构建船舶周围环境的鸟瞰图，以支持更安全的自主海洋导航。


<details>
  <summary>Details</summary>
Motivation: 现有的海洋导航系统在多模态传感器数据融合和恶劣天气条件下的鲁棒性方面存在不足，因此需要一种更可靠的方法来提高导航准确性和安全性。

Method: 采用跨注意Transformer模型，深度融合多视角RGB和长波红外图像与稀疏LiDAR点云数据，并结合X波段雷达和电子海图数据进行训练。

Result: 生成的鸟瞰图提供了详细可靠的场景表示，提高了导航准确性和鲁棒性，实际海上试验证实了该方法在恶劣天气和复杂海况下的有效性。

Conclusion: 该方法通过多模态传感器融合成功提升了自主海洋导航的可靠性和适应性，展现了在实际应用中的潜力。

Abstract: We propose a cross attention transformer based method for multimodal sensor
fusion to build a birds eye view of a vessels surroundings supporting safer
autonomous marine navigation. The model deeply fuses multiview RGB and long
wave infrared images with sparse LiDAR point clouds. Training also integrates X
band radar and electronic chart data to inform predictions. The resulting view
provides a detailed reliable scene representation improving navigational
accuracy and robustness. Real world sea trials confirm the methods
effectiveness even in adverse weather and complex maritime settings.

</details>


### [251] [AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation](https://arxiv.org/abs/2505.02830)
*Qingqiu Li,Zihang Cui,Seongsu Bae,Jilan Xu,Runtian Yuan,Yuejie Zhang,Rui Feng,Quanli Shen,Xiaobo Zhang,Junjun He,Shujun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种解剖学导向的推理框架（AOR），通过多步推理增强医学大型多模态模型（MLMM）的区域理解能力和可解释性，并结合专家指导的大规模指令数据集（AOR-Instruction）提升模型性能。实验证明AOR在视觉问答和报告生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前医学大型多模态模型（MLMM）在胸片（CXR）解读中存在区域级理解不足和单步推理导致的准确性及可解释性受限的问题，本文旨在通过解剖学导向的推理框架解决这些挑战。

Method: 提出解剖学导向的推理框架（AOR），通过跨模态区域级信息实现多步推理，并结合专家开发的指令数据集（AOR-Instruction）进行模型训练。

Result: 实验证明AOR在视觉问答（VQA）和报告生成任务中表现显著优于现有方法。

Conclusion: AOR框架通过增强区域级推理和可解释性，为医学大型多模态模型提供了更高效、准确的胸片解读方案。

Abstract: Chest X-rays (CXRs) are the most frequently performed imaging examinations in
clinical settings. Recent advancements in Large Multimodal Models (LMMs) have
enabled automated CXR interpretation, enhancing diagnostic accuracy and
efficiency. However, despite their strong visual understanding, current Medical
LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level
understanding and interaction, and (2) Limited accuracy and interpretability
due to single-step reasoning. In this paper, we empower MLMMs with
anatomy-centric reasoning capabilities to enhance their interactivity and
explainability. Specifically, we first propose an Anatomical Ontology-Guided
Reasoning (AOR) framework, which centers on cross-modal region-level
information to facilitate multi-step reasoning. Next, under the guidance of
expert physicians, we develop AOR-Instruction, a large instruction dataset for
MLMMs training. Our experiments demonstrate AOR's superior performance in both
VQA and report generation tasks.

</details>


### [252] [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835)
*Yi-Fan Zhang,Xingyu Lu,Xiao Hu,Chaoyou Fu,Bin Wen,Tianke Zhang,Changyi Liu,Kaiyu Jiang,Kaibing Chen,Kaiyu Tang,Haojie Ding,Jiankang Chen,Fan Yang,Zhang Zhang,Tingting Gao,Liang Wang*

Main category: cs.CV

TL;DR: 本文探讨了如何通过改进强化学习（RL）算法来提升多模态奖励模型（MRMs）的性能，提出了StableReinforce算法，并在20万条偏好数据上训练了R1-Reward模型，显著提升了多模态奖励建模的基准表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态奖励模型的研究主要集中在模型结构和训练数据的改进上，而对长期推理能力的探索不足。本文希望通过强化学习提升奖励模型的长期推理能力。

Method: 研究将奖励建模问题重新表述为基于规则的RL任务，并提出了StableReinforce算法，改进了训练损失、优势估计策略和奖励设计，提高了训练稳定性和性能。

Result: R1-Reward模型在多模态奖励建模基准上显著优于现有技术（VL Reward-Bench提升8.4%，Multimodal Reward Bench提升14.3%），且计算资源增加时性能进一步提升。

Conclusion: 研究表明强化学习算法在优化多模态奖励模型方面具有潜力，StableReinforce算法通过改进训练稳定性显著提升了模型性能。

Abstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the
performance of Multimodal Large Language Models (MLLMs). While recent
advancements have primarily focused on improving the model structure and
training data of MRMs, there has been limited exploration into the
effectiveness of long-term reasoning capabilities for reward modeling and how
to activate these capabilities in MRMs. In this paper, we explore how
Reinforcement Learning (RL) can be used to improve reward modeling.
Specifically, we reformulate the reward modeling problem as a rule-based RL
task. However, we observe that directly applying existing RL algorithms, such
as Reinforce++, to reward modeling often leads to training instability or even
collapse due to the inherent limitations of these algorithms. To address this
issue, we propose the StableReinforce algorithm, which refines the training
loss, advantage estimation strategy, and reward design of existing RL methods.
These refinements result in more stable training dynamics and superior
performance. To facilitate MRM training, we collect 200K preference data from
diverse datasets. Our reward model, R1-Reward, trained using the
StableReinforce algorithm on this dataset, significantly improves performance
on multimodal reward modeling benchmarks. Compared to previous SOTA models,
R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$
improvement on the Multimodal Reward Bench. Moreover, with more inference
compute, R1-Reward's performance is further enhanced, highlighting the
potential of RL algorithms in optimizing MRMs.

</details>


### [253] [Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2505.01664)
*Yi-Ming Zhai,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: 提出了一种软掩码半对偶最优传输（SSOT）方法用于处理部分域适应（PDA）问题，通过估计类别权重和构建类别条件分布匹配提升性能。


<details>
  <summary>Details</summary>
Motivation: 部分域适应（PDA）的挑战在于域偏移和标签空间不完全一致，需要一种高效的方法来解决这些问题。

Method: 通过类别权重估计和软掩码传输距离矩阵，结合半对偶最优传输公式和神经网络优化，实现端到端训练。

Result: 在四个基准数据集上验证了SSOT的有效性。

Conclusion: SSOT在PDA问题上表现优异，通过类别条件匹配和神经网络优化提升了性能。

Abstract: Visual domain adaptation aims to learn discriminative and domain-invariant
representation for an unlabeled target domain by leveraging knowledge from a
labeled source domain. Partial domain adaptation (PDA) is a general and
practical scenario in which the target label space is a subset of the source
one. The challenges of PDA exist due to not only domain shift but also the
non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual
Optimal Transport (SSOT) method is proposed to deal with the PDA problem.
Specifically, the class weights of domains are estimated, and then a reweighed
source domain is constructed, which is favorable in conducting
class-conditional distribution matching with the target domain. A soft-masked
transport distance matrix is constructed by category predictions, which will
enhance the class-oriented representation ability of optimal transport in the
shared feature space. To deal with large-scale optimal transport problems, the
semi-dual formulation of the entropy-regularized Kantorovich problem is
employed since it can be optimized by gradient-based algorithms. Further, a
neural network is exploited to approximate the Kantorovich potential due to its
strong fitting ability. This network parametrization also allows the
generalization of the dual variable outside the supports of the input
distribution. The SSOT model is built upon neural networks, which can be
optimized alternately in an end-to-end manner. Extensive experiments are
conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.

</details>


### [254] [Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study](https://arxiv.org/abs/2505.01680)
*Tamim Ahmed,Thanassis Rikakis*

Main category: cs.CV

TL;DR: 这篇论文提出了一种自动化的ARAT评分系统，通过多模态视频分析和融合SlowFast、I3D及Transformer模型，结合OpenPose关键点和物体位置数据，实现了对上肢功能的高效评估，验证准确率达89%。


<details>
  <summary>Details</summary>
Motivation: 手动评分ARAT（行动研究手臂测试）在卒中康复中耗时且结果不一致，因此需要一种自动化、可扩展的评分系统来提高效率和一致性。

Method: 采用多视角视频数据（同侧、对侧和顶部视角），结合SlowFast、I3D和Transformer模型，通过早期和晚期融合策略整合特征，并利用分层贝叶斯模型（HBMs）推断运动质量，提升可解释性。

Result: 系统验证准确率达到89%，且HBMs的评估与人工评分高度一致，临床医生反馈其准确性和实用性良好。

Conclusion: 该研究为康复领域提供了一种可扩展、可解释的自动化解决方案，并通过临床验证证明了其有效性，推动了自动化康复的进展。

Abstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity
assessment in stroke rehabilitation is time-intensive and variable. We propose
an automated ARAT scoring system integrating multimodal video analysis with
SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object
locations. Our approach employs multi-view data (ipsilateral, contralateral,
and top perspectives), applying early and late fusion to combine features
across views and models. Hierarchical Bayesian Models (HBMs) infer movement
quality components, enhancing interpretability. A clinician dashboard displays
task scores, execution times, and quality assessments. We conducted a study
with five clinicians who reviewed 500 video ratings generated by our system,
providing feedback on its accuracy and usability. Evaluated on a stroke
rehabilitation dataset, our framework achieves 89.0% validation accuracy with
late fusion, with HBMs aligning closely with manual assessments. This work
advances automated rehabilitation by offering a scalable, interpretable
solution with clinical validation.

</details>


### [255] [Topology-Aware CLIP Few-Shot Learning](https://arxiv.org/abs/2505.01694)
*Dazhi Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种拓扑感知的微调方法，通过结合表示拓扑差异（RTD）和任务残差（TR）框架来优化视觉语言模型（VLM）的小样本学习性能，避免了预训练知识丢失并提升了任务适应性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决现有方法在处理视觉语言模型的小样本学习时忽视潜在空间结构信息的问题，提出了一种更高效的方法来平衡预训练知识的保留和任务特定的适应。

Method: 方法是通过在任务残差框架中整合表示拓扑差异（RTD），同时冻结基础VLM编码器，仅优化轻量级的任务残差参数，利用拓扑结构对齐视觉和文本表示。

Result: 在6个不同的基准数据集上，该方法在小样本设置中平均精度提高了1-2%，显著优于相关基线方法。

Conclusion: 结论是该工作通过引入拓扑对齐，为提升视觉语言模型的小样本学习能力提供了一种有效的策略。

Abstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for
few-shot learning poses challenges in balancing pre-trained knowledge retention
and task-specific adaptation. Existing methods often overlook valuable
structural information within the VLM's latent space. We introduce a
topology-aware tuning approach integrating Representation Topology Divergence
(RTD) into the Task Residual (TR) framework. By explicitly aligning the
topological structures of visual and text representations using a combined RTD
and Cross-Entropy loss, while freezing base VLM encoders, our method enhances
few-shot performance. We optimize only lightweight Task Residual parameters,
effectively leveraging topological information. Across 6 diverse benchmark
datasets, our approach demonstrates significant gains, achieving an average
accuracy improvement of 1-2\% over relevant baseline methods in few-shot
settings. This work presents an effective strategy to boost VLM few-shot
capabilities by incorporating topological alignment.

</details>


### [256] [Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning](https://arxiv.org/abs/2505.01699)
*Yifan Liu,Ruichen Yao,Yaokun Liu,Ruohan Zong,Zelin Li,Yang Zhang,Dong Wang*

Main category: cs.CV

TL;DR: 本文提出了BNMR方法，通过贝叶斯网络校准器和元学习样本重加权优化面部组件的公平性，解决了标签稀缺和属性依赖性问题，并在实验中超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 人脸识别技术广泛应用需要关注公平性，但现有研究多集中在人口统计学公平性，忽略了生物面部组件的公平性。本文首次提出优化生物特征层面的面部属性预测偏差。

Method: 采用贝叶斯网络校准器和自适应元学习样本重加权（BNMR），动态跟踪模型偏差并编码面部组件属性的先验概率以克服标签稀缺和属性依赖性问题。

Result: 在大规模真实人脸数据集上的实验表明，BNMR优于现有基线，且面部组件公平性对人口统计学公平性（如性别）有积极影响。

Conclusion: 面部组件公平性可作为人口统计学公平性的潜在替代目标，为相关研究开辟了新方向。

Abstract: The widespread integration of face recognition technologies into various
applications (e.g., access control and personalized advertising) necessitates a
critical emphasis on fairness. While previous efforts have focused on
demographic fairness, the fairness of individual biological face components
remains unexplored. In this paper, we focus on face component fairness, a
fairness notion defined by biological face features. To our best knowledge, our
work is the first work to mitigate bias of face attribute prediction at the
biological feature level. In this work, we identify two key challenges in
optimizing face component fairness: attribute label scarcity and attribute
inter-dependencies, both of which limit the effectiveness of bias mitigation
from previous approaches. To address these issues, we propose \textbf{B}ayesian
\textbf{N}etwork-informed \textbf{M}eta \textbf{R}eweighting (BNMR), which
incorporates a Bayesian Network calibrator to guide an adaptive
meta-learning-based sample reweighting process. During the training process of
our approach, the Bayesian Network calibrator dynamically tracks model bias and
encodes prior probabilities for face component attributes to overcome the above
challenges. To demonstrate the efficacy of our approach, we conduct extensive
experiments on a large-scale real-world human face dataset. Our results show
that BNMR is able to consistently outperform recent face bias mitigation
baselines. Moreover, our results suggest a positive impact of face component
fairness on the commonly considered demographic fairness (e.g.,
\textit{gender}). Our findings pave the way for new research avenues on face
component fairness, suggesting that face component fairness could serve as a
potential surrogate objective for demographic fairness. The code for our work
is publicly
available~\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.

</details>


### [257] [An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding](https://arxiv.org/abs/2505.01743)
*Siyang Jiang,Bufang Yang,Lilin Xu,Mu Yuan,Yeerzhati Abudunuer,Kaiwei Liu,Liekang Zeng,Hongkai Chen,Zhenyu Yan,Xiaofan Jiang,Guoliang Xing*

Main category: cs.CV

TL;DR: 论文提出了一种名为Llambda的新系统，旨在通过有限标注数据和大量未标注数据优化大型视觉语言模型（LVLM）在低分辨率视频中的人类行为理解（HBU）能力。该方法通过对比学习生成高质量伪标签，并利用物理知识引导的标注器减少错误，最终结合高效微调技术提升模型性能，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统大型视觉语言模型（LVLM）主要针对高分辨率数据（如RGB图像），难以有效理解低分辨率数据（如深度、热成像）。直接标注低分辨率数据需要大量人力，因此需要开发一种更高效的方法。

Method: 1. 提出对比导向的数据标注器，通过对比学习从低分辨率视频中生成高质量伪标签；2. 设计物理知识引导的标注器，利用时空一致性检查修正伪标签错误；3. 采用LoRA高效微调技术，适配低分辨率数据。

Result: 在区域级真实测试床和三种低分辨率数据集上，Llambda平均Bert-Score优于现有最先进LVLM系统40.03%。

Conclusion: Llambda通过结合伪标签生成和物理知识引导，显著提升了LVLM在低分辨率HBU任务中的性能，同时降低了标注成本。

Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the
potential to surpass conventional labeling by generating richer, more detailed
descriptions of on-device human behavior understanding (HBU) in low-resolution
vision systems, such as depth, thermal, and infrared. However, existing large
vision language model (LVLM) approaches are unable to understand low-resolution
data well as they are primarily designed for high-resolution data, such as RGB
images. A quick fixing approach is to caption a large amount of low-resolution
data, but it requires a significant amount of labor-intensive annotation
efforts. In this paper, we propose a novel, labor-saving system, Llambda,
designed to support low-resolution HBU. The core idea is to leverage limited
labeled data and a large amount of unlabeled data to guide LLMs in generating
informative captions, which can be combined with raw data to effectively
fine-tune LVLM models for understanding low-resolution videos in HBU. First, we
propose a Contrastive-Oriented Data Labeler, which can capture
behavior-relevant information from long, low-resolution videos and generate
high-quality pseudo labels for unlabeled data via contrastive learning. Second,
we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and
temporal consistency checks to mitigate errors in pseudo labels. Therefore, it
can improve LLMs' understanding of sequential data and then generate
high-quality video captions. Finally, to ensure on-device deployability, we
employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.
We evaluate Llambda using a region-scale real-world testbed and three distinct
low-resolution datasets, and the experiments show that Llambda outperforms
several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.

</details>


### [258] [PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach](https://arxiv.org/abs/2505.01823)
*Nitin Rai,Arnold W. Schumann,Nathan Boyd*

Main category: cs.CV

TL;DR: 研究者探索了一种多模态文本到图像的合成作物病害图像生成方法，并首次在农业领域进行了计算性能基准测试。SD3.5M模型表现最佳，推荐用于高效数据生成。


<details>
  <summary>Details</summary>
Motivation: 野外采集大规模作物病害图像耗时耗力，生成模型提供了一种替代方案。现有研究主要基于GAN，缺乏对农业场景计算需求的全面分析。

Method: 训练三种Stable Diffusion变体（SDXL、SD3.5M、SD3.5L），结合Dreambooth和LoRA微调技术提升泛化能力。

Result: SD3.5M平均内存占用18GB，功耗180W，每500张图像能耗1.02kWh，1.5小时内可从36张样本生成500张合成图像。

Conclusion: SD3.5M在效率和性能上表现最优，适用于作物病害数据的高效生成。

Abstract: Collecting large-scale crop disease images in the field is labor-intensive
and time-consuming. Generative models (GMs) offer an alternative by creating
synthetic samples that resemble real-world images. However, existing research
primarily relies on Generative Adversarial Networks (GANs)-based image-to-image
translation and lack a comprehensive analysis of computational requirements in
agriculture. Therefore, this research explores a multi-modal text-to-image
approach for generating synthetic crop disease images and is the first to
provide computational benchmarking in this context. We trained three Stable
Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and
fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning
techniques to enhance generalization. SD3.5M outperformed the others, with an
average memory usage of 18 GB, power consumption of 180 W, and total energy use
of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results
demonstrate SD3.5M's ability to generate 500 synthetic images from just 36
in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease
data generation.

</details>


### [259] [PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications](https://arxiv.org/abs/2505.01881)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.CV

TL;DR: PhysNav-DG是一种新型导航框架，结合了传感器融合与视觉语言模型的语义能力，通过双分支架构预测导航动作并生成详细解释，实验显示其导航成功率提升20%以上。


<details>
  <summary>Details</summary>
Motivation: 为了在多样化环境中实现鲁棒导航，需要兼顾状态估计精度和决策透明度。

Method: 采用双分支架构，结合多传感器输入和视觉语言模型（如LLaMA 3.2 11B、BLIP-2），并利用改进的自适应卡尔曼滤波器动态调整噪声参数。

Result: 在MD-NEX基准测试中，导航成功率提升超20%，生成的解释既高度可信又清晰。

Conclusion: 该框架通过连接高层次语义推理与几何规划，为更安全、可信的自主系统提供了解决方案。

Abstract: Robust navigation in diverse environments and domains requires both accurate
state estimation and transparent decision making. We present PhysNav-DG, a
novel framework that integrates classical sensor fusion with the semantic power
of vision-language models. Our dual-branch architecture predicts navigation
actions from multi-sensor inputs while simultaneously generating detailed
chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically
adjusts its noise parameters based on environmental context. It leverages
several streams of raw sensor data along with semantic insights from models
such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the
MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,
autonomous driving, and social navigation tasks with ground-truth actions and
human-validated explanations. Extensive experiments and ablations show that
PhysNav-DG improves navigation success rates by over 20% and achieves high
efficiency, with explanations that are both highly grounded and clear. This
work connects high-level semantic reasoning and geometric planning for safer
and more trustworthy autonomous systems.

</details>


### [260] [Segment Any RGB-Thermal Model with Language-aided Distillation](https://arxiv.org/abs/2505.01950)
*Dong Xing,Xianxun Zhu,Wei Zhou,Qika Lin,Hang Yang,Yuqing Wang*

Main category: cs.CV

TL;DR: SARTM框架通过定制SAM模型，结合RGB-热成像数据，引入语义模块和跨模态知识蒸馏，显著提升了多模态语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 由于SAM仅基于RGB数据训练，无法直接应用于RGB-热成像（RGB-T）语义分割。而RGB-T在恶劣天气和光照条件下更具优势，因此需要定制化解决方案。

Method: 1. 通过添加LoRA层微调SAM，保留其泛化能力；2. 引入语言信息指导训练；3. 使用跨模态知识蒸馏（CMKD）减少模态差异；4. 调整分割头并加入多尺度语义分割头。

Result: 在MFNET、PST900和FMB三个数据集上的实验表明，SARTM在多种条件下显著优于现有方法。

Conclusion: SARTM成功将SAM适配到RGB-T任务，通过跨模态融合和语义增强，实现了更鲁棒的场景理解。

Abstract: The recent Segment Anything Model (SAM) demonstrates strong instance
segmentation performance across various downstream tasks. However, SAM is
trained solely on RGB data, limiting its direct applicability to RGB-thermal
(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for
scene understanding in adverse weather and lighting conditions, such as low
light and overexposure, we propose a novel framework, SARTM, which customizes
the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash
the potential of SAM while introduce semantic understanding modules for RGB-T
data pairs. Specifically, our framework first involves fine tuning the original
SAM by adding extra LoRA layers, aiming at preserving SAM's strong
generalization and segmentation capabilities for downstream tasks. Secondly, we
introduce language information as guidance for training our SARTM. To address
cross-modal inconsistencies, we introduce a Cross-Modal Knowledge
Distillation(CMKD) module that effectively achieves modality adaptation while
maintaining its generalization capabilities. This semantic module enables the
minimization of modality gaps and alleviates semantic ambiguity, facilitating
the combination of any modality under any visual conditions. Furthermore, we
enhance the segmentation performance by adjusting the segmentation head of SAM
and incorporating an auxiliary semantic segmentation head, which integrates
multi-scale features for effective fusion. Extensive experiments are conducted
across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,
and FMB. Both quantitative and qualitative results consistently demonstrate
that the proposed SARTM significantly outperforms state-of-the-art approaches
across a variety of conditions.

</details>


### [261] [VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos](https://arxiv.org/abs/2505.01481)
*Zongxia Li,Xiyang Wu,Yubin Qin,Guangyao Shi,Hongyang Du,Dinesh Manocha,Tianyi Zhou,Jordan Lee Boyd-Graber*

Main category: cs.CV

TL;DR: VideoHallu是一个专注于合成视频中异常内容检测的基准测试，通过专家设计的QA任务评估多模态大语言模型（MLLMs）的能力，并发现它们在常识和物理任务上的幻觉问题，最终通过GRPO微调显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成视频生成模型虽能生成高质量帧，但常违反常识和物理规律，且现有评估指标（如VideoScore）缺乏对此类问题的关注和解释性，因此需要更直观的方法评估这些异常。

Method: 引入VideoHallu基准，包含Veo2、Sora等模型的合成视频及专家设计的QA任务，评估GPT-4o、Gemini-2.5-Pro等MLLMs的表现，并通过GRPO在真实与合成数据上微调模型。

Result: 实验显示，即使先进的MLLMs在合成视频的常识和物理任务中仍存在幻觉问题，但GRPO微调（尤其是结合反例）显著提升了模型推理能力。

Conclusion: 研究突出了合成视频中幻觉检测的挑战，并通过GRPO微调和反例数据验证了提升MLLMs推理能力的有效性，为未来研究提供了数据和方向。

Abstract: Synthetic video generation with foundation models has gained attention for
its realism and wide applications. While these models produce high-quality
frames, they often fail to respect common sense and physical laws, resulting in
abnormal content. Existing metrics like VideoScore emphasize general quality
but ignore such violations and lack interpretability. A more insightful
approach is using multi-modal large language models (MLLMs) as interpretable
evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities
in synthetic videos remains underexplored. To address this, we introduce
VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,
and Kling, paired with expert-designed QA tasks solvable via human-level
reasoning across various categories. We assess several SoTA MLLMs, including
GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and
VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,
these models still hallucinate on basic commonsense and physics tasks in
synthetic settings, underscoring the challenge of hallucination. We further
fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real
and synthetic commonsense/physics data. Results show notable accuracy gains,
especially with counterexample integration, advancing MLLMs' reasoning
capabilities. Our data is available at https://github.com/zli12321/VideoHallu.

</details>


### [262] [Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation](https://arxiv.org/abs/2505.02075)
*Volodymyr Havrylov,Haiwen Huang,Dan Zhang,Andreas Geiger*

Main category: cs.CV

TL;DR: 研究了交互式分割（IS）作为评估VFMs特征上采样方法的新基准，结果表明选择合适的策略显著提升特征质量。


<details>
  <summary>Details</summary>
Motivation: VFMs生成的特征分辨率较低，限制了其在密集预测任务中的直接应用，因此探索任务无关的特征上采样模块以提升特征分辨率。

Method: 采用交互式分割（IS）作为基准，结合图像和用户点击的多模态输入，评估特征上采样方法对VFMs效果的提升。

Result: 实验表明，选择适当的上采样策略能显著提高VFMs特征的质量。

Conclusion: 交互式分割是评估VFMs特征上采样的有效基准，优化上采样策略对提升特征质量至关重要。

Abstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that
serve as general-purpose backbones for various computer vision tasks. As VFMs'
popularity grows, there is an increasing interest in understanding their
effectiveness for dense prediction tasks. However, VFMs typically produce
low-resolution features, limiting their direct applicability in this context.
One way to tackle this limitation is by employing a task-agnostic feature
upsampling module that refines VFM features resolution. To assess the
effectiveness of this approach, we investigate Interactive Segmentation (IS) as
a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its
inherent multimodal input, consisting of an image and a set of user-defined
clicks, as well as its dense mask output, IS creates a challenging environment
that demands comprehensive visual scene understanding. Our benchmarking
experiments show that selecting appropriate upsampling strategies significantly
improves VFM features quality. The code is released at
https://github.com/havrylovv/iSegProbe

</details>


### [263] [DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization](https://arxiv.org/abs/2505.02192)
*Wenchuan Wang,Mengqi Huang,Yijing Tu,Zhendong Mao*

Main category: cs.CV

TL;DR: DualReal是一个新的文本到视频生成框架，通过联合训练解决身份和运动的一致性冲突，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法中身份和运动定制是孤立的，忽略了二者的相互约束和协同依赖，导致生成过程中出现冲突。

Method: DualReal采用自适应联合训练，包含双感知适应单元和阶段混合控制器，动态调整训练阶段和粒度以避免冲突。

Result: 实验表明，DualReal在CLIP-I和DINO-I指标上平均提高了21.7%和31.8%，在运动质量指标上表现优异。

Conclusion: DualReal有效解决了身份与运动的一致性冲突，实现了高质量的文本到视频生成。

Abstract: Customized text-to-video generation with pre-trained large-scale models has
recently garnered significant attention through focusing on identity and motion
consistency. Existing works typically follow the isolated customized paradigm,
where the subject identity or motion dynamics are customized exclusively.
However, this paradigm completely ignores the intrinsic mutual constraints and
synergistic interdependencies between identity and motion, resulting in
identity-motion conflicts throughout the generation process that systematically
degrades. To address this, we introduce DualReal, a novel framework that,
employs adaptive joint training to collaboratively construct interdependencies
between dimensions. Specifically, DualReal is composed of two units: (1)
Dual-aware Adaptation dynamically selects a training phase (i.e., identity or
motion), learns the current information guided by the frozen dimension prior,
and employs a regularization strategy to avoid knowledge leakage; (2)
StageBlender Controller leverages the denoising stages and Diffusion
Transformer depths to guide different dimensions with adaptive granularity,
avoiding conflicts at various stages and ultimately achieving lossless fusion
of identity and motion patterns. We constructed a more comprehensive benchmark
than existing methods. The experimental results show that DualReal improves
CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top
performance on nearly all motion quality metrics.

</details>


### [264] [Improving Physical Object State Representation in Text-to-Image Generative Systems](https://arxiv.org/abs/2505.02236)
*Tianle Chen,Chaitanya Chakka,Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: 论文提出一种自动化生成合成数据的流程，用于提升文本生成图像模型在对象状态（如'没有瓶子的桌子'、'空杯子'）上的表现。通过微调模型并评估，在公共数据集上实现了8%以上的提升，在自建数据集上提升了24%。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成图像模型在准确描述对象状态时表现不佳，论文旨在通过合成数据和模型微调解决这一问题。

Method: 设计全自动合成数据生成流程，生成高质量数据捕捉对象状态；微调多个开源文本生成图像模型；使用GPT4o-mini评估生成图像与提示的一致性。

Result: 在公开数据集GenAI-Bench上平均提升8%以上，在自建的200条提示数据集上平均提升24%以上。

Conclusion: 论文方法显著提升了模型在对象状态描述上的准确性，并开源了评估提示和代码。

Abstract: Current text-to-image generative models struggle to accurately represent
object states (e.g., "a table without a bottle," "an empty tumbler"). In this
work, we first design a fully-automatic pipeline to generate high-quality
synthetic data that accurately captures objects in varied states. Next, we
fine-tune several open-source text-to-image models on this synthetic data. We
evaluate the performance of the fine-tuned models by quantifying the alignment
of the generated images to their prompts using GPT4o-mini, and achieve an
average absolute improvement of 8+% across four models on the public
GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific
focus on common objects in various physical states. We demonstrate a
significant improvement of an average of 24+% over the baseline on this
dataset. We release all evaluation prompts and code.

</details>


### [265] [Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset](https://arxiv.org/abs/2505.02255)
*Jakub Wąsala,Bartłomiej Wrzalski,Kornelia Noculak,Yuliia Tarasenko,Oliwer Krupa,Jan Kocoń,Grzegorz Chodak*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过训练图像转换模型提升扩散模型生成图像的成本效益比，将蒸馏模型（如FLUX.1-schnell）的输出质量提升至与高计算成本基线模型（如FLUX.1-dev）相当，同时减少82%的计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模图像生成模型中高计算成本的问题，提高成本效益比，作者提出利用蒸馏模型与基线模型间的差异是可学习的这一假设，专注于肖像生成领域进行优化。

Method: 生成合成配对数据集，训练一个快速图像到图像转换模型，将蒸馏模型生成的图像质量提升至与基线模型相当的水平。

Result: 提出的方法在保证生成类似基线模型的高质量肖像的同时，计算成本降低了82%。

Conclusion: 该研究展示了通过结合蒸馏模型与增强层，可以在大规模图像生成任务中显著提高效率的潜力。

Abstract: This study presents a novel approach to enhance the cost-to-quality ratio of
image generation with diffusion models. We hypothesize that differences between
distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are
consistent and, therefore, learnable within a specialized domain, like portrait
generation. We generate a synthetic paired dataset and train a fast
image-to-image translation head. Using two sets of low- and high-quality
synthetic images, our model is trained to refine the output of a distilled
generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like
FLUX.1-dev, which is more computationally intensive. Our results show that the
pipeline, which combines a distilled version of a large generative model with
our enhancement layer, delivers similar photorealistic portraits to the
baseline version with up to an 82% decrease in computational cost compared to
FLUX.1-dev. This study demonstrates the potential for improving the efficiency
of AI solutions involving large-scale image generation.

</details>


### [266] [SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing](https://arxiv.org/abs/2505.02370)
*Ming Li,Xin Gu,Fan Chen,Xiaoying Xing,Longyin Wen,Chen Chen,Sijie Zhu*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过构建更有效的编辑指令（包括修正指令和对比指令）来提升图像编辑模型的性能，无需依赖预训练任务或VLMs。


<details>
  <summary>Details</summary>
Motivation: 现有方法因编辑指令与图像对不匹配而导致噪声监督信号，本文旨在通过优化指令来解决这一根本问题。

Method: 基于模型在不同推理步骤的生成属性，利用修正指令和对比指令（通过三元组损失引入）提升监督效果。

Result: 在多个基准测试中显著优于现有方法，例如在Real-Edit上比SOTA SmartEdit提升9.19%，且训练数据少30倍、模型小13倍。

Conclusion: 提供了一种更直接高效的监督信号生成方式，为基于指令的图像编辑提供了新颖、简单且有效的解决方案。

Abstract: Due to the challenges of manually collecting accurate editing data, existing
datasets are typically constructed using various automated methods, leading to
noisy supervision signals caused by the mismatch between editing instructions
and original-edited image pairs. Recent efforts attempt to improve editing
models through generating higher-quality edited images, pre-training on
recognition tasks, or introducing vision-language models (VLMs) but fail to
resolve this fundamental issue. In this paper, we offer a novel solution by
constructing more effective editing instructions for given image pairs. This
includes rectifying the editing instructions to better align with the
original-edited image pairs and using contrastive editing instructions to
further enhance their effectiveness. Specifically, we find that editing models
exhibit specific generation attributes at different inference steps,
independent of the text. Based on these prior attributes, we define a unified
guide for VLMs to rectify editing instructions. However, there are some
challenging editing scenarios that cannot be resolved solely with rectified
instructions. To this end, we further construct contrastive supervision signals
with positive and negative instructions and introduce them into the model
training using triplet loss, thereby further facilitating supervision
effectiveness. Our method does not require the VLM modules or pre-training
tasks used in previous work, offering a more direct and efficient way to
provide better supervision signals, and providing a novel, simple, and
effective solution for instruction-based image editing. Results on multiple
benchmarks demonstrate that our method significantly outperforms existing
approaches. Compared with previous SOTA SmartEdit, we achieve 9.19%
improvements on the Real-Edit benchmark with 30x less training data and 13x
smaller model size.

</details>


### [267] [Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin](https://arxiv.org/abs/2505.02056)
*Yuchen Wang,Xuefeng Bai,Xiucheng Li,Weili Guan,Liqiang Nie,Xinyang Chen*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过概念对齐和混淆感知校准机制解决视觉语言模型中伪标签不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 伪标签不平衡导致性能下降，现有方法未能深入探究其根本原因。本文旨在填补这一研究空白。

Method: 结合概念对齐和混淆感知校准机制，优化低效类别并平衡预测。

Result: 在六个基准数据集上验证，相对现有最优方法性能提升6.29%。

Conclusion: 该方法能有效提高伪标签的准确性和平衡性。

Abstract: Adapting vision-language models (VLMs) to downstream tasks with pseudolabels
has gained increasing attention. A major obstacle is that the pseudolabels
generated by VLMs tend to be imbalanced, leading to inferior performance. While
existing methods have explored various strategies to address this, the
underlying causes of imbalance remain insufficiently investigated. To fill this
gap, we delve into imbalanced pseudolabels and identify two primary
contributing factors: concept mismatch and concept confusion. To mitigate these
two issues, we propose a novel framework incorporating concept alignment and
confusion-aware calibrated margin mechanisms. The core of our approach lies in
enhancing underperforming classes and promoting balanced predictions across
categories, thus mitigating imbalance. Extensive experiments on six benchmark
datasets with three learning paradigms demonstrate that the proposed method
effectively enhances the accuracy and balance of pseudolabels, achieving a
relative improvement of 6.29% over the SoTA method. Our code is avaliable at
https://anonymous.4open.science/r/CAP-C642/

</details>


### [268] [MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans](https://arxiv.org/abs/2505.02388)
*Huangyue Yu,Baoxiong Jia,Yixin Chen,Yandan Yang,Puhao Li,Rongpeng Su,Jiaxin Li,Qing Li,Wei Liang,Song-Chun Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.CV

TL;DR: 论文提出了MetaScenes数据集和Scan2Sim模型，用于自动化生成高质量、多样化的3D场景，减少对人工设计的依赖，并通过实验验证其在机器人操作和视觉语言导航中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景数据集依赖人工设计，难以规模化且成本高。本文旨在通过真实扫描数据自动化生成仿真场景，提升Embodied AI研究的可扩展性和泛化能力。

Method: 1. 构建MetaScenes数据集（来自真实扫描的15366个物体，831类）；2. 提出Scan2Sim模型实现资产自动替换；3. 设计两个评测任务（小物体布局合成和跨领域VLN导航）。

Result: 实验证实MetaScenes能支持更通用的智能体学习和仿真迁移应用，在机器人操作和导航任务中表现良好。

Conclusion: MetaScenes和Scan2Sim为Embodied AI提供了高效、可扩展的场景生成方案，推动了领域研究的新方向。

Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to
effectively support skill acquisition, sim-to-real transfer, and
generalization. Achieving these quality standards, however, necessitates the
precise replication of real-world object diversity. Existing datasets
demonstrate that this process heavily relies on artist-driven designs, which
demand substantial human effort and present significant scalability challenges.
To scalably produce realistic and interactive 3D scenes, we first present
MetaScenes, a large-scale, simulatable 3D scene dataset constructed from
real-world scans, which includes 15366 objects spanning 831 fine-grained
categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,
which enables the automated, high-quality replacement of assets, thereby
eliminating the reliance on artist-driven designs for scaling 3D scenes. We
further propose two benchmarks to evaluate MetaScenes: a detailed scene
synthesis task focused on small item layouts for robotic manipulation and a
domain transfer task in vision-and-language navigation (VLN) to validate
cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by
supporting more generalizable agent learning and sim-to-real applications,
introducing new possibilities for EAI research. Project website:
https://meta-scenes.github.io/.

</details>


### [269] [Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning](https://arxiv.org/abs/2505.02071)
*Can Küçüksözen,Yücel Yemez*

Main category: cs.CV

TL;DR: 论文提出了一种名为COCA的紧凑聚类注意力层，结合自下而上的层次网络架构COCA-Net，能够从多目标场景中提取以对象为中心的表示，并在单图像上进行无监督对象发现任务。通过利用紧凑性概念优化聚类，该模型在多个数据集上表现出色，超越或竞争力与现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 解决单图像无监督对象发现的挑战，提供一种能够高效定位并分割对象的方法，并克服现有算法在背景分割和对象数量限制上的不足。

Method: 提出Compact Clustering Attention (COCA)层，利用紧凑性聚类算法对目标重心进行空间引导，结合层次化的COCA-Net架构，实现端到端的目标中心表示学习。

Result: 在六个广泛使用的数据集上测试，模型在九种评估指标中表现优于或可与最先进模型竞争，尤其在背景分割和动态调整生成掩码数量方面表现突出。

Conclusion: COCA-Net通过紧凑性聚类和层次化设计，显著提升了无监督对象发现的效果，为复杂场景的目标分割提供了高效且灵活的解决方案。

Abstract: We propose the Compact Clustering Attention (COCA) layer, an effective
building block that introduces a hierarchical strategy for object-centric
representation learning, while solving the unsupervised object discovery task
on single images. COCA is an attention-based clustering module capable of
extracting object-centric representations from multi-object scenes, when
cascaded into a bottom-up hierarchical network architecture, referred to as
COCA-Net. At its core, COCA utilizes a novel clustering algorithm that
leverages the physical concept of compactness, to highlight distinct object
centroids in a scene, providing a spatial inductive bias. Thanks to this
strategy, COCA-Net generates high-quality segmentation masks on both the
decoder side and, notably, the encoder side of its pipeline. Additionally,
COCA-Net is not bound by a predetermined number of object masks that it
generates and handles the segmentation of background elements better than its
competitors. We demonstrate COCA-Net's segmentation performance on six widely
adopted datasets, achieving superior or competitive results against the
state-of-the-art models across nine different evaluation metrics.

</details>


### [270] [Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging](https://arxiv.org/abs/2505.02467)
*Valerio Guarrasi,Klara Mogensen,Sara Tassinari,Sara Qvarlander,Paolo Soda*

Main category: cs.CV

TL;DR: 论文提出了一种顺序前向搜索算法，用于在多模态网络中确定最佳融合时机，显著减少了计算开销，并在两个MRI数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态深度学习在医学影像中的融合时机主要依赖手动调整或穷举搜索，成本高且不一定能找到最优解，因此需要更高效和系统化的优化方法。

Method: 采用顺序前向搜索算法逐步激活和评估候选融合模块，通过重新训练和验证损失比较，系统性缩小搜索空间以找到最佳融合配置。

Result: 该方法在两个MRI数据集上的表现优于单模态基线、晚期融合及穷举融合配置，且计算开销显著降低。

Conclusion: 该方法为医学影像中的多模态深度学习提供了高效、稳健的融合优化框架，有助于提升临床决策和扩展AI架构的适应性。

Abstract: Multimodal deep learning harnesses diverse imaging modalities, such as MRI
sequences, to enhance diagnostic accuracy in medical imaging. A key challenge
is determining the optimal timing for integrating these
modalities-specifically, identifying the network layers where fusion modules
should be inserted. Current approaches often rely on manual tuning or
exhaustive search, which are computationally expensive without any guarantee of
converging to optimal results. We propose a sequential forward search algorithm
that incrementally activates and evaluates candidate fusion modules at
different layers of a multimodal network. At each step, the algorithm retrains
from previously learned weights and compares validation loss to identify the
best-performing configuration. This process systematically reduces the search
space, enabling efficient identification of the optimal fusion timing without
exhaustively testing all possible module placements. The approach is validated
on two multimodal MRI datasets, each addressing different classification tasks.
Our algorithm consistently identified configurations that outperformed unimodal
baselines, late fusion, and a brute-force ensemble of all potential fusion
placements. These architectures demonstrated superior accuracy, F-score, and
specificity while maintaining competitive or improved AUC values. Furthermore,
the sequential nature of the search significantly reduced computational
overhead, making the optimization process more practical. By systematically
determining the optimal timing to fuse imaging modalities, our method advances
multimodal deep learning for medical imaging. It provides an efficient and
robust framework for fusion optimization, paving the way for improved clinical
decision-making and more adaptable, scalable architectures in medical AI
applications.

</details>


### [271] [Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions](https://arxiv.org/abs/2505.02501)
*Asma Brazi,Boris Meden,Fabrice Mayran de Chamisso,Steve Bourgeois,Vincent Lepetit*

Main category: cs.CV

TL;DR: Corr2Distrib是一个基于对应关系的方法，首次从RGB图像中估计6D相机姿态分布，解决了对称性和遮挡引起的视觉模糊问题。


<details>
  <summary>Details</summary>
Motivation: 对称性和遮挡会导致视觉模糊，产生多个有效的姿态。现有的方法大多基于对应关系估计单一姿态，而基于对应关系估计姿态分布则面临性能下降的挑战。

Method: Corr2Distrib通过学习每个3D点的对称感知表示（包括描述符和局部坐标系），生成3DoF旋转假设，再利用PnP和姿态评分将其细化成6DoF姿态分布。

Result: 实验表明，Corr2Distrib在复杂非合成场景中，无论是姿态分布估计还是单一姿态估计，均优于现有方法。

Conclusion: Corr2Distrib展示了基于对应关系方法在姿态估计中的潜力，成功将视觉模糊转化为优势。

Abstract: We introduce Corr2Distrib, the first correspondence-based method which
estimates a 6D camera pose distribution from an RGB image, explaining the
observations. Indeed, symmetries and occlusions introduce visual ambiguities,
leading to multiple valid poses. While a few recent methods tackle this
problem, they do not rely on local correspondences which, according to the BOP
Challenge, are currently the most effective way to estimate a single 6DoF pose
solution. Using correspondences to estimate a pose distribution is not
straightforward, since ambiguous correspondences induced by visual ambiguities
drastically decrease the performance of PnP. With Corr2Distrib, we turn these
ambiguities into an advantage to recover all valid poses. Corr2Distrib first
learns a symmetry-aware representation for each 3D point on the object's
surface, characterized by a descriptor and a local frame. This representation
enables the generation of 3DoF rotation hypotheses from single 2D-3D
correspondences. Next, we refine these hypotheses into a 6DoF pose distribution
using PnP and pose scoring. Our experimental evaluations on complex
non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art
solutions for both pose distribution estimation and single pose estimation from
an RGB image, demonstrating the potential of correspondences-based approaches.

</details>


### [272] [Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.02824)
*Kuofeng Gao,Yufei Zhu,Yiming Li,Jiawang Bai,Yong Yang,Zhifeng Li,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 这篇论文探讨了针对文本到图像（T2I）扩散模型中数据集所有权验证（DOV）水印机制的版权规避攻击（CEA），并提出了首个专门设计的攻击方法CEAT2I，通过检测水印样本、识别触发标记和消除水印三阶段实现高效规避。


<details>
  <summary>Details</summary>
Motivation: 随着T2I模型的个性化微调流行，未经授权的数据集使用问题日益突出，DOV通过水印技术保护数据集所有权。然而，DOV对版权规避攻击的鲁棒性尚未被研究，论文旨在填补这一空白并揭示潜在风险。

Method: 提出CEAT2I攻击，分三阶段：1）利用微调中水印样本的收敛速度差异检测样本；2）通过迭代消融提示词和监测特征变化定位触发标记；3）采用闭式概念擦除方法消除水印。

Result: 实验表明，CEAT2I能有效绕过DOV机制，同时保持模型生成质量，验证了攻击的可行性和效果。

Conclusion: 论文首次验证了T2I模型中DOV水印的脆弱性，CEAT2I为未来设计更鲁棒的版权保护方案提供了重要参考，强调了安全机制需对抗规避攻击。

Abstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling
high-quality image generation conditioned on textual prompts. However, the
growing trend of fine-tuning pre-trained models for personalization raises
serious concerns about unauthorized dataset usage. To combat this, dataset
ownership verification (DOV) has emerged as a solution, embedding watermarks
into the fine-tuning datasets using backdoor techniques. These watermarks
remain inactive under benign samples but produce owner-specified outputs when
triggered. Despite the promise of DOV for T2I diffusion models, its robustness
against copyright evasion attacks (CEA) remains unexplored. In this paper, we
explore how attackers can bypass these mechanisms through CEA, allowing models
to circumvent watermarks even when trained on watermarked datasets. We propose
the first copyright evasion attack (i.e., CEAT2I) specifically designed to
undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three
stages: watermarked sample detection, trigger identification, and efficient
watermark mitigation. A key insight driving our approach is that T2I models
exhibit faster convergence on watermarked samples during the fine-tuning,
evident through intermediate feature deviation. Leveraging this, CEAT2I can
reliably detect the watermarked samples. Then, we iteratively ablate tokens
from the prompts of detected watermarked samples and monitor shifts in
intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a
closed-form concept erasure method to remove the injected watermark. Extensive
experiments show that our CEAT2I effectively evades DOV mechanisms while
preserving model performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [273] [Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey](https://arxiv.org/abs/2505.01821)
*Jing Liu,Yao Du,Kun Yang,Yan Wang,Xiping Hu,Zehua Wang,Yang Liu,Peng Sun,Azzedine Boukerche,Victor C. M. Leung*

Main category: cs.DC

TL;DR: 该论文综述了边缘-云协同计算（ECCC）如何整合AI技术以优化分布式系统性能，探讨了模型优化、资源管理、隐私安全等核心问题，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI（如深度学习和LLMs）的快速发展，边缘-云协同计算面临模型部署和资源管理的挑战，需要系统性研究以推动高效低延迟的智能应用。

Method: 论文通过综述方式，分析了ECCC的基础架构、关键技术、模型优化（如压缩和神经架构搜索）、AI驱动的资源管理策略以及隐私安全保护。

Result: 提供了对ECCC系统的全面评估标准，并展示在自动驾驶、医疗等领域的实际应用，同时指出LLMs部署、6G整合等未来方向。

Conclusion: 论文为研究者和实践者提供了理论和实践结合的视角，推动了下一代智能系统的创新。

Abstract: Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm
for addressing the computational demands of modern intelligent applications,
integrating cloud resources with edge devices to enable efficient, low-latency
processing. Recent advancements in AI, particularly deep learning and large
language models (LLMs), have dramatically enhanced the capabilities of these
distributed systems, yet introduce significant challenges in model deployment
and resource management. In this survey, we comprehensive examine the
intersection of distributed intelligence and model optimization within
edge-cloud environments, providing a structured tutorial on fundamental
architectures, enabling technologies, and emerging applications. Additionally,
we systematically analyze model optimization approaches, including compression,
adaptation, and neural architecture search, alongside AI-driven resource
management strategies that balance performance, energy efficiency, and latency
requirements. We further explore critical aspects of privacy protection and
security enhancement within ECCC systems and examines practical deployments
through diverse applications, spanning autonomous driving, healthcare, and
industrial automation. Performance analysis and benchmarking techniques are
also thoroughly explored to establish evaluation standards for these complex
systems. Furthermore, the review identifies critical research directions
including LLMs deployment, 6G integration, neuromorphic computing, and quantum
computing, offering a roadmap for addressing persistent challenges in
heterogeneity management, real-time processing, and scalability. By bridging
theoretical advancements and practical deployments, this survey offers
researchers and practitioners a holistic perspective on leveraging AI to
optimize distributed computing environments, fostering innovation in
next-generation intelligent systems.

</details>


### [274] [Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation](https://arxiv.org/abs/2505.01616)
*Jianxing Qin,Jingrong Chen,Xinhao Kong,Yongji Wu,Liang Luo,Zhaodong Wang,Ying Zhang,Tingjun Chen,Alvin R. Lebeck,Danyang Zhuo*

Main category: cs.DC

TL;DR: Phantora是一种实时GPU集群模拟器，用于性能估计，解决了现有方法需大量人工和计算资源的问题，提高了通用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统需要扩展到大型GPU集群，但模型架构、系统实现和集群配置的变化难以在部署前量化性能影响，现有方法通用性差且需大量资源。

Method: Phantora通过拦截和模拟GPU相关操作，结合事件驱动网络模拟器与实时系统执行，提升模拟速度、扩展性和准确性。

Result: 评估结果表明，Phantora仅需一块GPU即可达到与最先进工作负载模拟方法相似的估计精度，同时减少人工需求并提高通用性。

Conclusion: Phantora为机器学习系统性能估计提供了一种高效、通用的解决方案，显著降低了资源需求和人工干预。

Abstract: To accommodate ever-increasing model complexity, modern machine learning (ML)
systems have to scale to large GPU clusters. Changes in ML model architecture,
ML system implementation, and cluster configuration can significantly affect
overall ML system performance. However, quantifying the performance impact
before deployment is challenging. Existing performance estimation methods use
performance modeling or static workload simulation. These techniques are not
general: they requires significant human effort and computation capacity to
generate training data or a workload. It is also difficult to adapt ML systems
to use these techniques. This paper introduces, Phantora, a live GPU cluster
simulator for performance estimation. Phantora runs minimally modified ML
models and frameworks, intercepting and simulating GPU-related operations to
enable high-fidelity performance estimation. Phantora overcomes several
research challenges in integrating an event-driven network simulator with live
system execution, and introduces a set of techniques to improve simulation
speed, scalability, and accuracy. Our evaluation results show that Phantora can
deliver similar estimation accuracy to the state-of-the-art workload simulation
approach with only one GPU, while reducing human effort and increasing
generalizability.

</details>


### [275] [Large Language Model Partitioning for Low-Latency Inference at the Edge](https://arxiv.org/abs/2505.02533)
*Dimitrios Kafetzis,Ramin Khalili,Iordanis Koutsopoulos*

Main category: cs.DC

TL;DR: 论文提出了一种基于资源感知的Transformer架构分区算法，通过动态迁移注意力头来降低推断延迟和内存占用。在小型和大型设备环境中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成的文本长度增加，内存和计算负载也随之增加，导致资源受限的边缘环境中容易出现内存过载或高推断延迟。需要一种更高效的资源利用方法。

Method: 提出了一种资源感知的Transformer分区算法，动态更新分区决策，并在注意力头级别进行分区，允许动态迁移以优化资源利用和减少延迟。

Result: 实验表明，该方法在3-5台设备的小规模环境下能够达到最佳延迟的15-20%，在大规模测试中显著提升了推断速度和内存效率。

Conclusion: 该方法通过动态分区和迁移注意力头来优化资源利用，有效降低了延迟并提升了性能。

Abstract: Large Language Models (LLMs) based on autoregressive, decoder-only
Transformers generate text one token at a time, where a token represents a
discrete unit of text. As each newly produced token is appended to the partial
output sequence, the length grows and so does the memory and compute load, due
to the expanding key-value caches, which store intermediate representations of
all previously generated tokens in the multi-head attention (MHA) layer. As
this iterative process steadily increases memory and compute demands,
layer-based partitioning in resource-constrained edge environments often
results in memory overload or high inference latency. To address this and
reduce inference latency, we propose a resource-aware Transformer architecture
partitioning algorithm, where the partitioning decision is updated at regular
intervals during token generation. The approach is myopic in that it is based
on instantaneous information about device resource availability and network
link bandwidths. When first executed, the algorithm places blocks on devices,
and in later executions, it migrates these blocks among devices so that the sum
of migration delay and inference delay remains low. Our approach partitions the
decoder at the attention head level, co-locating each attention head with its
key-value cache and allowing dynamic migrations whenever resources become
tight. By allocating different attention heads to different devices, we exploit
parallel execution of attention heads and thus achieve substantial reductions
in inference delays. Our experiments show that in small-scale settings (3-5
devices), the proposed method achieves within 15 to 20 percent of an exact
optimal solver's latency, while in larger-scale tests it achieves notable
improvements in inference speed and memory usage compared to state-of-the-art
layer-based partitioning approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [276] [BiGSCoder: State Space Model for Code Understanding](https://arxiv.org/abs/2505.01475)
*Shweta Verma,Abhinav Anand,Mira Mezini*

Main category: cs.SE

TL;DR: BiGSCoder是一种新型双向状态空间模型（SSM），在代码理解任务上表现优于传统Transformer模型，样本效率更高且无需位置嵌入。


<details>
  <summary>Details</summary>
Motivation: 系统评估状态空间模型在代码任务中的能力，并与传统Transformer架构对比，探索更高效的替代方案。

Method: 使用门控架构的双向SSM，通过掩码语言建模在代码数据集上预训练，并比较不同预训练配置和基准测试。

Result: BiGSCoder在代码理解任务中优于Transformer，训练数据更少且无需位置嵌入，能有效扩展到长序列。

Conclusion: BiGSCoder是Transformer的高效替代方案，尤其在样本效率和长序列处理上表现突出。

Abstract: We present BiGSCoder, a novel encoder-only bidirectional state-space model
(SSM) featuring a gated architecture, pre-trained for code understanding on a
code dataset using masked language modeling. Our work aims to systematically
evaluate SSMs' capabilities in coding tasks compared to traditional transformer
architectures; BiGSCoder is built for this purpose. Through comprehensive
experiments across diverse pre-training configurations and code understanding
benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models,
despite utilizing simpler pre-training strategies and much less training data.
Our results indicate that BiGSCoder can serve as a more sample-efficient
alternative to conventional transformer models. Furthermore, our study shows
that SSMs perform better without positional embeddings and can effectively
extrapolate to longer sequences during fine-tuning.

</details>


### [277] [On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles](https://arxiv.org/abs/2505.02274)
*Xingyu Zhao,Robab Aghazadeh-Chakherlou,Chih-Hong Cheng,Peter Popov,Lorenzo Strigini*

Main category: cs.SE

TL;DR: 本文讨论了基于场景的测试在自动驾驶车辆安全中的应用及其统计基础的必要性，提出了一种新的度量标准Risk Estimation Fidelity (REF)来验证仿真与真实世界测试结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的安全测试需要高效的评估方法，但基于场景的测试在统计基础、残余风险估计等方面仍存在未解决的问题。

Method: 通过与传统软件测试方法对比，提出了量化每个场景故障概率(pfs)的概念模型，并评估不同条件下的测试效果。

Result: 研究发现基于场景和基于里程的测试各有优劣，且引入了REF指标来确保仿真测试结果的统计可靠性。

Conclusion: 统计基础的建立是自动驾驶车辆安全测试的关键，REF为新模型提供了统计可辩护的安全性评估方法。

Abstract: Scenario-based testing has emerged as a common method for autonomous vehicles
(AVs) safety, offering a more efficient alternative to mile-based testing by
focusing on high-risk scenarios. However, fundamental questions persist
regarding its stopping rules, residual risk estimation, debug effectiveness,
and the impact of simulation fidelity on safety claims. This paper argues that
a rigorous statistical foundation is essential to address these challenges and
enable rigorous safety assurance. By drawing parallels between AV testing and
traditional software testing methodologies, we identify shared research gaps
and reusable solutions. We propose proof-of-concept models to quantify the
probability of failure per scenario (pfs) and evaluate testing effectiveness
under varying conditions. Our analysis reveals that neither scenario-based nor
mile-based testing universally outperforms the other. Furthermore, we introduce
Risk Estimation Fidelity (REF), a novel metric to certify the alignment of
synthetic and real-world testing outcomes, ensuring simulation-based safety
claims are statistically defensible.

</details>


### [278] [Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach](https://arxiv.org/abs/2505.01947)
*Ivan Tan,Wei Minn,Christopher M. Poskitt,Lwin Khin Shar,Lingxiao Jiang*

Main category: cs.SE

TL;DR: 论文提出了一种结合规则挖掘和无监督学习的无人机异常检测方法RADD，解决了现有LSTM方法在泛化性、可解释性和领域知识捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖多传感器输入，故障可能导致严重安全问题。现有LSTM方法在泛化性、可解释性和领域知识捕捉上存在不足，需改进。

Method: RADD结合规则挖掘（捕获传感器与执行器的预期关系）和无监督学习（覆盖规则遗漏的隐蔽关系），在ArduPilot和Gazebo中实现。

Result: RADD在六类故障中检测率达93.84%，误报率仅2.33%，优于现有LSTM方法，且可实时部署。

Conclusion: RADD通过规则与无监督学习的结合，显著提升了无人机异常检测的准确性和实用性。

Abstract: UAVs, commonly referred to as drones, have witnessed a remarkable surge in
popularity due to their versatile applications. These cyber-physical systems
depend on multiple sensor inputs, such as cameras, GPS receivers,
accelerometers, and gyroscopes, with faults potentially leading to physical
instability and serious safety concerns. To mitigate such risks, anomaly
detection has emerged as a crucial safeguarding mechanism, capable of
identifying the physical manifestations of emerging issues and allowing
operators to take preemptive action at runtime. Recent anomaly detection
methods based on LSTM neural networks have shown promising results, but three
challenges persist: the need for models that can generalise across the diverse
mission profiles of drones; the need for interpretability, enabling operators
to understand the nature of detected problems; and the need for capturing
domain knowledge that is difficult to infer solely from log data. Motivated by
these challenges, this paper introduces RADD, an integrated approach to anomaly
detection in drones that combines rule mining and unsupervised learning. In
particular, we leverage rules (or invariants) to capture expected relationships
between sensors and actuators during missions, and utilise unsupervised
learning techniques to cover more subtle relationships that the rules may have
missed. We implement this approach using the ArduPilot drone software in the
Gazebo simulator, utilising 44 rules derived across the main phases of drone
missions, in conjunction with an ensemble of five unsupervised learning models.
We find that our integrated approach successfully detects 93.84% of anomalies
over six types of faults with a low false positive rate (2.33%), and can be
deployed effectively at runtime. Furthermore, RADD outperforms a
state-of-the-art LSTM-based method in detecting the different types of faults
evaluated in our study.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [279] [Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression](https://arxiv.org/abs/2505.01637)
*Samuel J. Kaufman,René Just,Rastislav Bodik*

Main category: cs.PL

TL;DR: 本文提出了一种基于动态规划的神经网络推理优化方法，通过分解和组合程序规格，结合高效的记忆表表示，显著扩大了搜索空间并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如自动调度器）通常通过启发式采样搜索空间来优化神经网络推理，但搜索空间巨大且难以处理。本文旨在通过动态规划方法更有效地探索搜索空间。

Method: 采用动态规划方法，将大型程序规格分解为可通过改写获得的小规格，并通过仿射成本模型组合最优程序。提出了一种新型记忆表表示法，降低内存需求。

Result: 开发的Morello编译器在x86平台上实现了高性能程序合成，例如在Zen 1 CPU上优化的矩阵乘法，并已集成到Google的gemma.cpp中。

Conclusion: 仿射成本模型能有效筛选高性能程序，动态规划方法显著扩展了搜索空间的探索能力，为神经网络推理优化提供了新方向。

Abstract: High-throughput neural network inference requires coordinating many
optimization decisions, including parallel tiling, microkernel selection, and
data layout. The product of these decisions forms a search space of programs
which is typically intractably large. Existing approaches (e.g.,
auto-schedulers) often address this problem by sampling this space
heuristically. In contrast, we introduce a dynamic-programming-based approach
to explore more of the search space by iteratively decomposing large program
specifications into smaller specifications reachable from a set of rewrites,
then composing a final program from each rewrite that minimizes an affine cost
model. To reduce memory requirements, we employ a novel memoization table
representation, which indexes specifications by coordinates in $Z_{\geq 0}$ and
compresses identical, adjacent solutions. This approach can visit a much larger
set of programs than prior work. To evaluate the approach, we developed
Morello, a compiler which lowers specifications roughly equivalent to a
few-node XLA computation graph to x86. Notably, we found that an affine cost
model is sufficient to surface high-throughput programs. For example, Morello
synthesized a collection of matrix multiplication benchmarks targeting a Zen 1
CPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply,
which was integrated into Google's gemma.cpp.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [280] [Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments](https://arxiv.org/abs/2505.01632)
*Noussaiba Djeffal,Djamel Addou,Hamza Kheddar,Sid Ahmed Selouani*

Main category: eess.AS

TL;DR: 论文提出了一种新颖的神经框架，通过结合鲁棒前端到ASR系统中，改善了在干净和嘈杂环境下的语音识别性能，使用基于ResNet的迁移学习，实验结果显示识别准确率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境噪声对自动语音识别（ASR）的负面影响，数据驱动的监督方法（如深度神经网络）相比传统无监督方法展现出潜力。

Method: 引入了一种新颖的神经框架，结合鲁棒前端到ASR系统中，利用Aurora-2语音数据库评估基于ResNet的迁移学习方法。

Result: 实验结果显示，相比CNN和LSTM网络，该方法在干净和嘈杂环境下的识别准确率分别为98.94%和91.21%。

Conclusion: 该神经框架及其方法在ASR系统中表现出显著优势，特别是在噪声环境下，验证了其有效性。

Abstract: Addressing the detrimental impact of non-stationary environmental noise on
automatic speech recognition (ASR) has been a persistent and significant
research focus. Despite advancements, this challenge continues to be a major
concern. Recently, data-driven supervised approaches, such as deep neural
networks, have emerged as promising alternatives to traditional unsupervised
methods. With extensive training, these approaches have the potential to
overcome the challenges posed by diverse real-life acoustic environments. In
this light, this paper introduces a novel neural framework that incorporates a
robust frontend into ASR systems in both clean and noisy environments.
Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness
of an acoustic feature set for Mel-frequency, employing the approach of
transfer learning based on Residual neural network (ResNet). The experimental
results demonstrate a significant improvement in recognition accuracy compared
to convolutional neural networks (CNN) and long short-term memory (LSTM)
networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [281] [AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine](https://arxiv.org/abs/2505.01435)
*Carlo Siebenschuh,Kyle Hippe,Ozan Gokdemir,Alexander Brace,Arham Khan,Khalid Hossain,Yadu Babuji,Nicholas Chia,Venkatram Vishwanath,Rick Stevens,Arvind Ramanathan,Ian Foster,Robert Underwood*

Main category: cs.IR

TL;DR: AdaParse是一种自适应PDF解析引擎，结合人类偏好和硬件需求优化解析器选择，提升大规模解析任务的吞吐量17倍，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 科学文档多为PDF格式，现有解析器在成本和准确性之间权衡不足，需一种自适应方法结合人类偏好和资源分配。

Method: 引入AdaParse，通过直接偏好优化(DPO)结合科学家的选择偏好，动态分配解析器和计算资源。

Result: 在1000份科学文档测试中，吞吐量提升17倍，精度提高0.2%。

Conclusion: AdaParse的高效与可扩展性使其适合构建万亿级文本数据集，推动高质量语言模型发展。

Abstract: Language models for scientific tasks are trained on text from scientific
publications, most distributed as PDFs that require parsing. PDF parsing
approaches range from inexpensive heuristics (for simple documents) to
computationally intensive ML-driven systems (for complex or degraded ones). The
choice of the "best" parser for a particular document depends on its
computational cost and the accuracy of its output. To address these issues, we
introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine
(AdaParse), a data-driven strategy for assigning an appropriate parser to each
document. We enlist scientists to select preferred parser outputs and
incorporate this information through direct preference optimization (DPO) into
AdaParse, thereby aligning its selection process with human judgment. AdaParse
then incorporates hardware requirements and predicted accuracy of each parser
to orchestrate computational resources efficiently for large-scale parsing
campaigns. We demonstrate that AdaParse, when compared to state-of-the-art
parsers, improves throughput by $17\times$ while still achieving comparable
accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents.
AdaParse's combination of high accuracy and parallel scalability makes it
feasible to parse large-scale scientific document corpora to support the
development of high-quality, trillion-token-scale text datasets. The
implementation is available at https://github.com/7shoe/AdaParse/

</details>


### [282] [Exploring new Approaches for Information Retrieval through Natural Language Processing](https://arxiv.org/abs/2505.02199)
*Manak Raj,Nidhi Mishra*

Main category: cs.IR

TL;DR: 这篇综述论文探讨了信息检索（IR）在自然语言处理（NLP）中的最新进展和新兴方法，涵盖传统与现代技术，并分析了稀疏、密集和混合检索方法，同时指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着NLP和信息检索技术的快速发展，研究者需要全面了解传统与现代IR模型的优缺点，以及如何在实际应用中结合它们以提高检索效率和准确性。

Method: 论文回顾了传统IR模型（如布尔模型和向量空间模型），以及现代技术（如深度学习和预训练变换器模型），并分析了相关工具的应用。

Result: 通过比较稀疏、密集和混合检索方法，论文展示了不同技术在多个应用场景（如网络搜索引擎和跨语言IR）中的表现，并指出了各自的局限性。

Conclusion: 论文总结了当前IR技术的进展，强调了提升检索准确性、可扩展性和伦理考量的重要性，并提出了未来研究的开放挑战和方向。

Abstract: This review paper explores recent advancements and emerging approaches in
Information Retrieval (IR) applied to Natural Language Processing (NLP). We
examine traditional IR models such as Boolean, vector space, probabilistic, and
inference network models, and highlight modern techniques including deep
learning, reinforcement learning, and pretrained transformer models like BERT.
We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for
efficient text indexing and search. A comparative analysis of sparse, dense,
and hybrid retrieval methods is presented, along with applications in web
search engines, cross-language IR, argument mining, private information
retrieval, and hate speech detection. Finally, we identify open challenges and
future research directions to enhance retrieval accuracy, scalability, and
ethical considerations.

</details>


### [283] [Predicting Movie Hits Before They Happen with LLMs](https://arxiv.org/abs/2505.02693)
*Shaghayegh Agah,Yejin Kim,Neeraj Sharma,Mayur Nankani,Kevin Foley,H. Howie Huang,Sardar Hamidian*

Main category: cs.IR

TL;DR: The paper tackles the cold-start issue in movie recommendations by using LLMs to predict popularity based on metadata, showing improved effectiveness over baselines.


<details>
  <summary>Details</summary>
Motivation: The challenge of cold-start recommendations in entertainment platforms, especially for movies, drives the need for innovative solutions to ensure fair promotion and accurate predictions.

Method: Leverages Large Language Models (LLMs) with movie metadata to forecast the popularity of cold-start movies.

Result: The proposed method demonstrates superior effectiveness compared to existing baselines and newly developed solutions.

Conclusion: LLMs are effective for cold-start movie popularity prediction, offering potential integration into recommendation systems or editorial tools.

Abstract: Addressing the cold-start issue in content recommendation remains a critical
ongoing challenge. In this work, we focus on tackling the cold-start problem
for movies on a large entertainment platform. Our primary goal is to forecast
the popularity of cold-start movies using Large Language Models (LLMs)
leveraging movie metadata. This method could be integrated into retrieval
systems within the personalization pipeline or could be adopted as a tool for
editorial teams to ensure fair promotion of potentially overlooked movies that
may be missed by traditional or algorithmic solutions. Our study validates the
effectiveness of this approach compared to established baselines and those we
developed.

</details>


### [284] [Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems](https://arxiv.org/abs/2505.02120)
*Xiao Zhou,Zhongxiang Zhao,Hanze Guo*

Main category: cs.IR

TL;DR: 论文提出Tricolore框架，通过多向量学习和行为类型连接优化推荐系统，解决传统方法单目标行为和用户偏好表示的局限性。采用自适应多任务结构、行为级多视图融合模块及流行度平衡策略，提升多样性和冷启动用户表现。公开实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统通常只优化单一行为目标并使用单一向量表示用户偏好，难以处理多行为类型或目标，且无法全面捕捉用户兴趣，导致候选池狭窄。

Method: 提出Tricolore框架，包含：1) 多向量学习挖掘行为间联系；2) 自适应多任务结构；3) 行为级多视图融合模块动态增强学习；4) 流行度平衡策略兼顾准确性与多样性。

Result: 公开数据集实验表明，Tricolore在短视频、电商等场景中表现优异，并通过共享基础嵌入策略显著提升冷启动用户性能。

Conclusion: Tricolore通过多行为建模和动态优化，有效克服传统推荐系统的局限性，为平台提供灵活、高效的推荐解决方案。

Abstract: Online platforms aggregate extensive user feedback across diverse behaviors,
providing a rich source for enhancing user engagement. Traditional recommender
systems, however, typically optimize for a single target behavior and represent
user preferences with a single vector, limiting their ability to handle
multiple important behaviors or optimization objectives. This conventional
approach also struggles to capture the full spectrum of user interests,
resulting in a narrow item pool during candidate generation. To address these
limitations, we present Tricolore, a versatile multi-vector learning framework
that uncovers connections between different behavior types for more robust
candidate generation. Tricolore's adaptive multi-task structure is also
customizable to specific platform needs. To manage the variability in sparsity
across behavior types, we incorporate a behavior-wise multi-view fusion module
that dynamically enhances learning. Moreover, a popularity-balanced strategy
ensures the recommendation list balances accuracy with item popularity,
fostering diversity and improving overall performance. Extensive experiments on
public datasets demonstrate Tricolore's effectiveness across various
recommendation scenarios, from short video platforms to e-commerce. By
leveraging a shared base embedding strategy, Tricolore also significantly
improves the performance for cold-start users. The source code is publicly
available at: https://github.com/abnering/Tricolore.

</details>


### [285] [Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions](https://arxiv.org/abs/2505.02154)
*Oliver Savolainen,Dur e Najaf Amjad,Roxana Petcu*

Main category: cs.IR

TL;DR: 本文通过复制和扩展原始论文的实验，验证了神经检索模型中查询词信息的编码方式，并探索了跨语言和文档长度信息的编码情况，结果表明激活修补方法能有效定位模型行为，并揭示了通用性和可解释性的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究神经检索模型如何编码任务相关属性（如词频），并验证这些编码是否具有跨语言通用性，同时探索文档长度信息是否也被编码。

Method: 复制原始论文的关键实验，通过激活修补方法分析西班牙语和中文数据集，并检查文档长度信息的编码位置。

Result: 确认激活修补方法能定位模型特定组件和标记的行为；发现词频信息的跨语言通用性，且序列级任务信息集中在CLS标记。

Conclusion: 研究强调了信息检索可解释性和机器学习研究可重复性的重要性，为未来研究提供了方向。

Abstract: This reproducibility study analyzes and extends the paper "Axiomatic Causal
Interventions for Reverse Engineering Relevance Computation in Neural Retrieval
Models," which investigates how neural retrieval models encode task-relevant
properties such as term frequency. We reproduce key experiments from the
original paper, confirming that information on query terms is captured in the
model encoding. We extend this work by applying activation patching to Spanish
and Chinese datasets and by exploring whether document-length information is
encoded in the model as well. Our results confirm that the designed activation
patching method can isolate the behavior to specific components and tokens in
neural retrieval models. Moreover, our findings indicate that the location of
term frequency generalizes across languages and that in later layers, the
information for sequence-level tasks is represented in the CLS token. The
results highlight the need for further research into interpretability in
information retrieval and reproducibility in machine learning research. Our
code is available at
https://github.com/OliverSavolainen/axiomatic-ir-reproduce.

</details>


### [286] [Social Biases in Knowledge Representations of Wikidata separates Global North from Global South](https://arxiv.org/abs/2505.02352)
*Paramita Das,Sai Keerthana Karnam,Aditya Soni,Animesh Mukherjee*

Main category: cs.IR

TL;DR: 介绍了一个名为AuditLP的框架，用于检测知识图谱链接预测中的社会偏见，重点关注性别和年龄作为敏感属性在不同地理区域中的表现。


<details>
  <summary>Details</summary>
Motivation: 知识图谱在（半）自动化构建过程中可能存在社会偏见，这些偏见可能对少数群体产生不公平影响。研究旨在通过AuditLP框架量化这种偏见。

Method: 开发AuditLP框架，利用公平性指标分析性别和年龄对职业分类的影响，实验基于Wikidata的21个地理区域的知识三元组。

Result: 实验显示，偏见结果的差异反映了全球社会经济和文化差异，尤其是全球北方与全球南方的明显区分。

Conclusion: 知识图谱中的偏见与社会经济背景密切相关，AuditLP为检测和量化这种偏见提供了有效工具。

Abstract: Knowledge Graphs have become increasingly popular due to their wide usage in
various downstream applications, including information retrieval, chatbot
development, language model construction, and many others. Link prediction (LP)
is a crucial downstream task for knowledge graphs, as it helps to address the
problem of the incompleteness of the knowledge graphs. However, previous
research has shown that knowledge graphs, often created in a (semi) automatic
manner, are not free from social biases. These biases can have harmful effects
on downstream applications, especially by leading to unfair behavior toward
minority groups. To understand this issue in detail, we develop a framework --
AuditLP -- deploying fairness metrics to identify biased outcomes in LP,
specifically how occupations are classified as either male or female-dominated
based on gender as a sensitive attribute. We have experimented with the
sensitive attribute of age and observed that occupations are categorized as
young-biased, old-biased, and age-neutral. We conduct our experiments on a
large number of knowledge triples that belong to 21 different geographies
extracted from the open-sourced knowledge graph, Wikidata. Our study shows that
the variance in the biased outcomes across geographies neatly mirrors the
socio-economic and cultural division of the world, resulting in a transparent
partition of the Global North from the Global South.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [287] [CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering](https://arxiv.org/abs/2505.01476)
*Zhe Zhang,Mingxiu Cai,Hanxiao Wang,Gaochang Wu,Tianyou Chai,Xiatian Zhu*

Main category: eess.IV

TL;DR: 该论文提出了一种名为CostFilter-AD的无监督异常检测方法，通过引入成本过滤概念改进现有方法的匹配过程，从而提升异常检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法依赖图像或特征级别匹配，但这一过程常不精确且被忽视，导致检测效果不佳。论文旨在通过成本过滤解决这一问题。

Method: 构建匹配成本体积，并通过成本体积过滤网络进行细化，利用输入观测作为注意力查询来抑制噪声。该方法可作为通用后处理插件与现有方法结合。

Result: 在MVTec-AD和VisA基准测试中，CostFilter-AD在单类和多类无监督异常检测任务中均表现出通用优势。

Conclusion: CostFilter-AD作为一种通用后处理插件，能有效提升现有方法的异常检测性能，尤其在匹配噪声抑制和边缘结构保持方面表现出色。

Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an
input image with respect to normal samples. Either by reconstructing normal
counterparts (reconstruction-based) or by learning an image feature embedding
space (embedding-based), existing approaches fundamentally rely on image-level
or feature-level matching to derive anomaly scores. Often, such a matching
process is inaccurate yet overlooked, leading to sub-optimal detection. To
address this issue, we introduce the concept of cost filtering, borrowed from
classical matching tasks, such as depth and flow estimation, into the UAD
problem. We call this approach {\em CostFilter-AD}. Specifically, we first
construct a matching cost volume between the input and normal samples,
comprising two spatial dimensions and one matching dimension that encodes
potential matches. To refine this, we propose a cost volume filtering network,
guided by the input observation as an attention query across multiple feature
layers, which effectively suppresses matching noise while preserving edge
structures and capturing subtle anomalies. Designed as a generic
post-processing plug-in, CostFilter-AD can be integrated with either
reconstruction-based or embedding-based methods. Extensive experiments on
MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for
both single- and multi-class UAD tasks. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [288] [Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth](https://arxiv.org/abs/2505.01638)
*Michael Marinaccio,Fatemeh Afghah*

Main category: eess.IV

TL;DR: SAM-TIFF: 基于RGB输入的无人机森林火灾温度预测与分割框架，无需热成像传感器。


<details>
  <summary>Details</summary>
Motivation: 现有的高精度森林火灾监测通常需要多模态传感器（RGB和热成像），但增加了硬件成本和能耗。本文旨在通过仅使用RGB输入实现热成像传感器的替代方案。

Method: 提出SAM-TIFF框架，通过多模态教师网络（基于RGB-热成像配对数据）向单模态RGB学生网络蒸馏知识。分割监督采用SAM引导的掩模生成、TOPSIS选择、Canny边缘检测和Otsu阈值处理的混合方法。

Result: 在FLAME 3数据集上首次实现了基于RGB数据的像素级温度回归，表现出强泛化能力。

Conclusion: 该方法为轻量、低成本的无人机火灾监测系统奠定了基础，无需热成像传感器。

Abstract: High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)
typically requires multimodal sensing - especially RGB and thermal imagery -
which increases hardware cost and power consumption. This paper introduces
SAM-TIFF, a novel teacher-student distillation framework for pixel-level
wildfire temperature prediction and segmentation using RGB input only. A
multimodal teacher network trained on paired RGB-Thermal imagery and
radiometric TIFF ground truth distills knowledge to a unimodal RGB student
network, enabling thermal-sensor-free inference. Segmentation supervision is
generated using a hybrid approach of segment anything (SAM)-guided mask
generation, and selection via TOPSIS, along with Canny edge detection and
Otsu's thresholding pipeline for automatic point prompt selection. Our method
is the first to perform per-pixel temperature regression from RGB UAV data,
demonstrating strong generalization on the recent FLAME 3 dataset. This work
lays the foundation for lightweight, cost-effective UAV-based wildfire
monitoring systems without thermal sensors.

</details>


### [289] [Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2](https://arxiv.org/abs/2505.01854)
*Yuwen Chen,Zafer Yildiz,Qihang Li,Yaqian Chen,Haoyu Dong,Hanxue Gu,Nicholas Konz,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: 该论文提出了SLM-SAM 2架构，通过结合短长期记忆模块提升医学影像分割精度，显著优于SAM 2。


<details>
  <summary>Details</summary>
Motivation: 减少医学图像手动标注的高成本和耗时长问题，改进SAM 2在体积分割中的性能。

Method: 结合短、长期记忆库和独立的注意力模块，优化目标掩码传播准确性。

Result: 在三套公开数据集上平均Dice系数提升0.14（5样本）和0.11（单样本），抗过传播能力更强。

Conclusion: SLM-SAM 2为医学图像自动标注提供了更准确的解决方案。

Abstract: Manual annotation of volumetric medical images, such as magnetic resonance
imaging (MRI) and computed tomography (CT), is a labor-intensive and
time-consuming process. Recent advancements in foundation models for video
object segmentation, such as Segment Anything Model 2 (SAM 2), offer a
potential opportunity to significantly speed up the annotation process by
manually annotating one or a few slices and then propagating target masks
across the entire volume. However, the performance of SAM 2 in this context
varies. Our experiments show that relying on a single memory bank and attention
module is prone to error propagation, particularly at boundary regions where
the target is present in the previous slice but absent in the current one. To
address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel
architecture that integrates distinct short-term and long-term memory banks
with separate attention modules to improve segmentation accuracy. We evaluate
SLM-SAM 2 on three public datasets covering organs, bones, and muscles across
MRI and CT modalities. We show that the proposed method markedly outperforms
the default SAM 2, achieving average Dice Similarity Coefficient improvement of
0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for
the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger
resistance to over-propagation, making a notable step toward more accurate
automated annotation of medical images for segmentation model development.

</details>


### [290] [Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images](https://arxiv.org/abs/2505.01884)
*Siddharth Kothari,Srinivasan Murali,Sankalp Kothari,Ujjwal Verma,Jaya Sreevalsan-Nair*

Main category: eess.IV

TL;DR: 论文研究了SAR图像中内陆水体的分割问题，探讨了U-Net模型对人工标注错误的鲁棒性，并模拟了对抗攻击以测试模型性能。


<details>
  <summary>Details</summary>
Motivation: SAR图像的水体分割在实际应用中非常重要，但由于复杂的几何形状，人工标注容易引入误差。研究旨在评估U-Net模型对这些标注错误的容忍度。

Method: 通过模拟对抗攻击生成有噪声的人工标注数据，测试U-Net模型在不同噪声水平下的分割性能。

Result: U-Net模型在一定噪声水平下仍能保持较好的分割性能，但噪声超过阈值后性能显著下降。

Conclusion: 人工标注的质量对分割模型的性能至关重要，对抗训练可提升模型鲁棒性。相关代码和数据集已公开。

Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is
an important task needed for several applications, such as flood mapping. While
SAR sensors capture data in all-weather conditions as high-resolution images,
differentiating water and water-like surfaces from SAR images is not
straightforward. Inland water bodies, such as large river basins, have complex
geometry, which adds to the challenge of segmentation. U-Net is a widely used
deep learning model for land-water segmentation of SAR images. In practice,
manual annotation is often used to generate the corresponding water masks as
ground truth. Manual annotation of the images is prone to label noise owing to
data poisoning attacks, especially due to complex geometry. In this work, we
simulate manual errors in the form of adversarial attacks on the U-Net model
and study the robustness of the model to human errors in annotation. Our
results indicate that U-Net can tolerate a certain level of corruption before
its performance drops significantly. This finding highlights the crucial role
that the quality of manual annotations plays in determining the effectiveness
of the segmentation model. The code and the new dataset, along with adversarial
examples for robust training, are publicly available. (Github link -
https://github.com/GVCL/IWSeg-SAR-Poison.git)

</details>


### [291] [Regression s all you need for medical image translation](https://arxiv.org/abs/2505.02048)
*Sebastian Rassmann,David Kügler,Christian Ewert,Martin Reuter*

Main category: eess.IV

TL;DR: YODA是一个基于2.5D扩散的医学图像翻译框架，结合扩散与回归方法生成高质量图像；其提出的ExpA采样技术减少噪声干扰。实验结果证明YODA优于现有GAN和DM方法，部分任务中甚至超越真实采集图像。


<details>
  <summary>Details</summary>
Motivation: 医学图像翻译（MIT）需高质量生成图像，但现有GAN和DM方法在医学应用中可能因噪声或内容虚构而效果不佳。因此，需要新方法确保解剖准确性。

Method: YODA框架结合扩散与回归范式，并引入ExpA采样技术抑制生成噪声，提升图像质量。

Result: 在多种模态数据集上验证，YODA优于GAN和DM方法，部分下游任务中生成图像优于真实采集。

Conclusion: YODA挑战了DM的默认优势，为MIT在医学成像中的实际应用铺平道路。

Abstract: The acquisition of information-rich images within a limited time budget is
crucial in medical imaging. Medical image translation (MIT) can help enhance
and supplement existing datasets by generating synthetic images from acquired
data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have
achieved remarkable success in natural image generation, their benefits -
creativity and image realism - do not necessarily transfer to medical
applications where highly accurate anatomical information is required. In fact,
the imitation of acquisition noise or content hallucination hinder clinical
utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel
2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and
regression paradigms to produce realistic or noise-free outputs. Furthermore,
we propose Expectation-Approximation (ExpA) DM sampling, which draws
inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise
and, thus, eliminates noise from biasing the evaluation of image quality.
Through extensive experiments on four diverse multi-modal datasets - comprising
multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and
regression sampling yield similar results in practice. As such, the
computational overhead of diffusion sampling does not provide systematic
benefits in medical information translation. Building on these insights, we
demonstrate that YODA outperforms several state-of-the-art GAN and DM methods.
Notably, YODA-generated images are shown to be interchangeable with, or even
superior to, physical acquisitions for several downstream tasks. Our findings
challenge the presumed advantages of DMs in MIT and pave the way for the
practical application of MIT in medical imaging.

</details>


### [292] [Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations](https://arxiv.org/abs/2505.01670)
*Christos Zangos,Danish Ebadulla,Thomas Christopher Sprague,Ambuj Singh*

Main category: eess.IV

TL;DR: 该论文提出了一种基于fMRI的视觉图像重建新方法，利用主体无关的公共表示空间对齐不同主体的大脑信号，显著提高了训练效率，尤其在低数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统fMRI图像重建方法通常需要针对每个主体进行端到端训练，效率低下且数据需求高。本工作旨在通过公共表示空间实现主体无关的语义对齐，提升训练效率并降低数据需求。

Method: 方法包括在训练时对齐主体大脑信号到一个公共表示空间，形成语义对齐的“公共大脑”，并通过轻量级模块将主体特定信息对齐到参考主体，而非传统的端到端训练。

Result: 实验表明，该方法在不同数据集上均表现出色，公共表示空间具有主体和数据集无关性，且在低数据场景下效果显著优于传统方法。

Conclusion: 该研究通过公共表示空间显著提高了fMRI视觉图像重建的效率，尤其是在数据稀缺情况下，展示了主体无关方法的潜力。

Abstract: This work introduces a novel approach to fMRI-based visual image
reconstruction using a subject-agnostic common representation space. We show
that the brain signals of the subjects can be aligned in this common space
during training to form a semantically aligned common brain. This is leveraged
to demonstrate that aligning subject-specific lightweight modules to a
reference subject is significantly more efficient than traditional end-to-end
training methods. Our approach excels in low-data scenarios. We evaluate our
methods on different datasets, demonstrating that the common space is subject
and dataset-agnostic.

</details>


### [293] [Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs](https://arxiv.org/abs/2505.01742)
*Yu Mao,Jingzong Li,Jun Wang,Hong Xu,Tei-Wei Kuo,Nan Guan,Chun Jason Xue*

Main category: eess.IV

TL;DR: Easz是一种基于Transformer的边缘计算自由图像编码框架，通过将计算负担转移到服务器，解决了边缘设备上的神经图像压缩难题。


<details>
  <summary>Details</summary>
Motivation: 神经图像压缩在边缘设备上应用时，面临编码-解码结构笨重和压缩级别切换不灵活的问题，阻碍了其实际部署。

Method: 提出Easz框架，利用patch-erase算法选择性删除图像内容，并通过Transformer在接收端重建。进一步采用轻量级Transformer结构降低接收端计算负担。

Result: 实验表明，Easz在压缩级别适应性、计算效率和图像重建质量上优于现有方法。

Conclusion: Easz为边缘设备上的高效图像压缩提供了可行解决方案，平衡了计算负担和重建质量。

Abstract: Neural image compression, necessary in various machine-to-machine
communication scenarios, suffers from its heavy encode-decode structures and
inflexibility in switching between different compression levels. Consequently,
it raises significant challenges in applying the neural image compression to
edge devices that are developed for powerful servers with high computational
and storage capacities. We take a step to solve the challenges by proposing a
new transformer-based edge-compute-free image coding framework called Easz.
Easz shifts the computational overhead to the server, and hence avoids the
heavy encoding and model switching overhead on the edge. Easz utilizes a
patch-erase algorithm to selectively remove image contents using a conditional
uniform-based sampler. The erased pixels are reconstructed on the receiver side
through a transformer-based framework. To further reduce the computational
overhead on the receiver, we then introduce a lightweight transformer-based
reconstruction structure to reduce the reconstruction load on the receiver
side. Extensive evaluations conducted on a real-world testbed demonstrate
multiple advantages of Easz over existing compression approaches, in terms of
adaptability to different compression levels, computational efficiency, and
image reconstruction quality.

</details>


### [294] [Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch](https://arxiv.org/abs/2505.02396)
*Kennard Norbert Sudiardjo,Islam Nur Alam,Wilson Wijaya,Lili Ayu Wulandhari*

Main category: eess.IV

TL;DR: 该研究使用CNN MobileNetV2和从头构建的CNN模型诊断肺炎，结果显示MobileNetV2稳定性较好但训练时间较长，而从头构建的模型虽精度较高但存在过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 肺炎诊断因不确定性（如非典型表现和诊断工具限制）而复杂化，研究旨在通过深度学习提高诊断准确性。

Method: 采用MobileNetV2预训练模型与ResNet101V2架构，并从头构建Keras模型，通过Kaggle数据集训练。

Result: MobileNetV2验证准确率达84.87%，但略有下降至78.95%，稳定性更佳；从头模型精度更高但过拟合明显。

Conclusion: MobileNetV2适合稳定性需求场景，从头构建模型适用于精度优先但需注意过拟合问题。

Abstract: Pneumonia Diagnosis, though it is crucial for an effective treatment, it can
be hampered by uncertainty. This uncertainty starts to arise due to some
factors like atypical presentations, limitations of diagnostic tools such as
chest X-rays, and the presence of co-existing respiratory conditions. This
research proposes one of the supervised learning methods, CNN. Using
MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using
Keras API as the built from scratch model, for identifying lung diseases
especially pneumonia. The datasets used in this research were obtained from the
website through Kaggle. The result shows that by implementing CNN MobileNetV2
and CNN from scratch the result is promising. While validating data,
MobileNetV2 performs with stability and minimal overfitting, while the training
accuracy increased to 84.87% later it slightly decreased to 78.95%, with
increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is
more stable. Although it takes more time to train each epoch. Meanwhile, after
the 10th epoch, the Scratch model displayed more instability and overfitting
despite having higher validation accuracy, training accuracy decreased
significantly to 78.12% and the validation loss increased from 0.5698 to
1.1809. With these results, ResNet101V2 offers stability, and the Scratch model
offers high accuracy.

</details>


### [295] [Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems](https://arxiv.org/abs/2505.02470)
*Tal Oved,Beatrice Lena,Chloé F. Najac,Sheng Shen,Matthew S. Rosen,Andrew Webb,Efrat Shimron*

Main category: eess.IV

TL;DR: 本文提出ViT-Fuser模型，通过深度学习从历史高场MRI扫描中提取个性化特征，以提升低成本低场MRI的扫描速度与图像质量，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高场MRI成本高且不易获取，而低场MRI虽廉价但图像质量和扫描速度较差，亟需一种方法在低成本下实现高质量成像。

Method: 采用ViT-Fuser（特征融合视觉transformer），利用历史高场MRI扫描的特征（如DICOM光盘数据），增强低场MRI的扫描效率和图像质量。

Result: 在包括胶质母细胞瘤、低场（50mT）和超低场（6.5mT）数据的四个数据集上，ViT-Fuser表现优于现有方法，且对分布外数据具有鲁棒性。

Conclusion: 该框架为低成本快速获取诊断级MRI图像提供了可行方案，具有广泛医疗应用潜力。

Abstract: Magnetic resonance imaging (MRI) offers superb-quality images, but its
accessibility is limited by high costs, posing challenges for patients
requiring longitudinal care. Low-field MRI provides affordable imaging with
low-cost devices but is hindered by long scans and degraded image quality,
including low signal-to-noise ratio (SNR) and tissue contrast. We propose a
novel healthcare paradigm: using deep learning to extract personalized features
from past standard high-field MRI scans and harnessing them to enable
accelerated, enhanced-quality follow-up scans with low-cost systems. To
overcome the SNR and contrast differences, we introduce ViT-Fuser, a
feature-fusion vision transformer that learns features from past scans, e.g.
those stored in standard DICOM CDs. We show that \textit{a single prior scan is
sufficient}, and this scan can come from various MRI vendors, field strengths,
and pulse sequences. Experiments with four datasets, including glioblastoma
data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that
ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality
images from accelerated low-field scans, with robustness to out-of-distribution
data. Our freely available framework thus enables rapid, diagnostic-quality,
low-cost imaging for wide healthcare applications.

</details>


### [296] [Lane-Wise Highway Anomaly Detection](https://arxiv.org/abs/2505.02613)
*Mei Qiu,William Lorenz Reindl,Yaobin Chen,Stanley Chien,Shu Hu*

Main category: eess.IV

TL;DR: 论文提出了一个可扩展且可解释的高速公路车道级交通异常检测框架，利用从监控摄像头中提取的多模态时间序列数据。该方法通过AI视觉模型提取车道特定特征（如车辆计数、占用率和卡车比例），无需依赖昂贵硬件或复杂道路建模。同时引入了包含73,139个车道样本的新数据集，标注了四类专家验证的异常。多分支检测系统结合深度学习、规则逻辑和机器学习，实验表明在精度、召回率和F1分数上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于传感器的交通异常检测方法依赖于昂贵的硬件和复杂的道路建模，且缺乏可扩展性。因此，需要一种更高效、低成本且可解释的解决方案，以应对真实世界的智能交通系统需求。

Method: 提出了一种基于AI视觉模型的框架，通过提取车道级时间序列特征（如车辆计数、占用率等），并结合深度学习、规则逻辑和机器学习构建多分支检测系统，提高鲁棒性和精度。此外，创建了一个新的车道级异常数据集以支持研究。

Result: 实验表明，该框架在精度、召回率和F1分数上优于现有方法，为智能交通系统提供了一种高性价比且可扩展的解决方案。

Conclusion: 通过结合多模态数据和混合检测方法，论文提出的框架显著提升了异常检测性能，同时降低了硬件依赖和成本，适用于大规模部署。

Abstract: This paper proposes a scalable and interpretable framework for lane-wise
highway traffic anomaly detection, leveraging multi-modal time series data
extracted from surveillance cameras. Unlike traditional sensor-dependent
methods, our approach uses AI-powered vision models to extract lane-specific
features, including vehicle count, occupancy, and truck percentage, without
relying on costly hardware or complex road modeling. We introduce a novel
dataset containing 73,139 lane-wise samples, annotated with four classes of
expert-validated anomalies: three traffic-related anomalies (lane blockage and
recovery, foreign object intrusion, and sustained congestion) and one
sensor-related anomaly (camera angle shift). Our multi-branch detection system
integrates deep learning, rule-based logic, and machine learning to improve
robustness and precision. Extensive experiments demonstrate that our framework
outperforms state-of-the-art methods in precision, recall, and F1-score,
providing a cost-effective and scalable solution for real-world intelligent
transportation systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [297] [Emotions in the Loop: A Survey of Affective Computing for Emotional Support](https://arxiv.org/abs/2505.01542)
*Karishma Hegde,Hemadri Jayalath*

Main category: cs.HC

TL;DR: 这篇综述论文探讨了情感计算在情绪识别、情感分析和人格分配等领域的应用，分类分析了AI聊天机器人、多模态输入系统、心理健康治疗和安全应用等领域的研究贡献和方法，并讨论了数据集、技术挑战和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 随着技术在日常生活中的深入，情感计算通过感知和响应用户情绪提升了人机交互体验。本文旨在总结情感计算的最新研究进展，分析其应用和发展方向。

Method: 论文采用综述方法，分类分析了情感计算的四大应用领域：AI聊天机器人、多模态系统、心理健康与治疗、安全应用，并评估了所使用的方法（如LLMs、多模态技术）和数据集。

Result: 研究发现情感计算在多个领域展现了技术优势，但仍存在数据集多样性和规模不足、模型泛化能力有限等挑战。

Conclusion: 论文强调未来需关注伦理问题，开发更安全、共情且实用的情感计算应用，并提出了研究方向。

Abstract: In a world where technology is increasingly embedded in our everyday
experiences, systems that sense and respond to human emotions are elevating
digital interaction. At the intersection of artificial intelligence and
human-computer interaction, affective computing is emerging with innovative
solutions where machines are humanized by enabling them to process and respond
to user emotions. This survey paper explores recent research contributions in
affective computing applications in the area of emotion recognition, sentiment
analysis and personality assignment developed using approaches like large
language models (LLMs), multimodal techniques, and personalized AI systems. We
analyze the key contributions and innovative methodologies applied by the
selected research papers by categorizing them into four domains: AI chatbot
applications, multimodal input systems, mental health and therapy applications,
and affective computing for safety applications. We then highlight the
technological strengths as well as the research gaps and challenges related to
these studies. Furthermore, the paper examines the datasets used in each study,
highlighting how modality, scale, and diversity impact the development and
performance of affective models. Finally, the survey outlines ethical
considerations and proposes future directions to develop applications that are
more safe, empathetic and practical.

</details>


### [298] [The GenAI Generation: Student Views of Awareness, Preparedness, and Concern](https://arxiv.org/abs/2505.02230)
*Micaela Siraj,Jon Duke*

Main category: cs.HC

TL;DR: 生成式AI（GenAI）正在重塑教育和职业发展，但缺乏统一政策。研究通过调查揭示学生对GenAI的双重情感：既兴奋又担忧其伦理、就业和教育结构问题。


<details>
  <summary>Details</summary>
Motivation: 探讨学生对GenAI的认知和态度，以指导教育机构应对这一变革性技术带来的机遇与挑战。

Method: 通过含开放式问题的简明调查，收集250多份反馈（40%为定性回答），分析学生对GenAI的认知、准备和担忧。

Result: 学生普遍对GenAI持积极态度，但更担忧伦理、就业替代及教育适应性等问题。

Conclusion: 研究为教育机构提供了学生视角的关键洞察，建议调整策略以应对GenAI驱动的未来。

Abstract: Generative AI (GenAI) is revolutionizing education and workforce development,
profoundly shaping how students learn, engage, and prepare for their future.
Outpacing the development of uniform policies and structures, GenAI has
heralded a unique era and given rise to the GenAI Generation: a cohort of
students whose education has been increasingly shaped by the opportunities and
challenges GenAI presents during its widespread adoption within society. This
study examines our students' perceptions of GenAI through a concise survey with
optional open-ended questions, focusing on their awareness, preparedness, and
concerns. Evaluation of more than 250 responses with more than 40% providing
detailed qualitative feedback reveals a core dual sentiment: while most
students express enthusiasm for GenAI, an even greater proportion voice a
spectrum of concerns about ethics, job displacement, and the adequacy of
educational structures given the highly transformative technology. These
findings offer critical insights into how students view the potential and
pitfalls of GenAI for future career impacts, with accompanying recommendations
to guide educational institutions in navigating a future driven by GenAI.

</details>


### [299] [Eye Movements as Indicators of Deception: A Machine Learning Approach](https://arxiv.org/abs/2505.02649)
*Valentin Foucher,Santiago de Leon-Martinez,Robert Moro*

Main category: cs.HC

TL;DR: 研究探讨了凝视数据（注视、扫视、眨眼和瞳孔大小）在AI模型中的效果，用于在隐蔽信息测试中检测欺骗行为，实验结果显示最高准确率达74%。


<details>
  <summary>Details</summary>
Motivation: 由于凝视数据在提高测谎仪鲁棒性方面潜力巨大但研究不足，本研究旨在验证AI模型利用凝视数据检测欺骗的可行性。

Method: 研究使用两个数据集（Eyelink 1000和Pupil Neon采集）训练XGBoost模型，任务包括二分类（揭露/隐瞒）和三分类（揭露/隐瞒/伪造），并分析了关键特征。

Result: 二分类任务最高准确率为74%，三分类为49%。扫视数量、持续时间、幅度及最大瞳孔大小被识别为预测欺骗的关键特征。

Conclusion: 研究表明凝视数据和AI结合的可行性，为未来优化测谎仪提供了方向。

Abstract: Gaze may enhance the robustness of lie detectors but remains under-studied.
This study evaluated the efficacy of AI models (using fixations, saccades,
blinks, and pupil size) for detecting deception in Concealed Information Tests
across two datasets. The first, collected with Eyelink 1000, contains gaze data
from a computerized experiment where 87 participants revealed, concealed, or
faked the value of a previously selected card. The second, collected with Pupil
Neon, involved 36 participants performing a similar task but facing an
experimenter. XGBoost achieved accuracies up to 74% in a binary classification
task (Revealing vs. Concealing) and 49% in a more challenging
three-classification task (Revealing vs. Concealing vs. Faking). Feature
analysis identified saccade number, duration, amplitude, and maximum pupil size
as the most important for deception prediction. These results demonstrate the
feasibility of using gaze and AI to enhance lie detectors and encourage future
research that may improve on this.

</details>


### [300] [AI Standardized Patient Improves Human Conversations in Advanced Cancer Care](https://arxiv.org/abs/2505.02694)
*Kurtis Haut,Masum Hasan,Thomas Carroll,Ronald Epstein,Taylan Sen,Ehsan Hoque*

Main category: cs.HC

TL;DR: 论文提出SOPHIE，一种AI驱动的标准化病人模拟系统，用于训练临终关怀中的严肃病情沟通，解决了传统方法昂贵、耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 临终关怀中的严肃病情沟通(SIC)面临情感压力、文化障碍等挑战，现有方法如标准化病人训练昂贵且不灵活，需要更高效、可扩展的解决方案。

Method: SOPHIE结合大型语言模型(LLMs)、虚拟化形象和基于临床文献的自动化反馈，提供远程、按需的SIC训练。

Result: 随机对照试验显示，使用SOPHIE的医疗学生和专业人士在“共情、明确表达、赋能”三个关键SIC领域有显著提升。

Conclusion: AI驱动工具可增强复杂人际沟通技能，为临床教育提供可扩展、易获取的解决方案，填补关键空白。

Abstract: Serious illness communication (SIC) in end-of-life care faces challenges such
as emotional stress, cultural barriers, and balancing hope with honesty.
Despite its importance, one of the few available ways for clinicians to
practice SIC is with standardized patients, which is expensive, time-consuming,
and inflexible. In this paper, we present SOPHIE, an AI-powered standardized
patient simulation and automated feedback system. SOPHIE combines large
language models (LLMs), a lifelike virtual avatar, and automated, personalized
feedback based on clinical literature to provide remote, on-demand SIC
training. In a randomized control study with healthcare students and
professionals, SOPHIE users demonstrated significant improvement across three
critical SIC domains: Empathize, Be Explicit, and Empower. These results
suggest that AI-driven tools can enhance complex interpersonal communication
skills, offering scalable, accessible solutions to address a critical gap in
clinician education.

</details>


### [301] [Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow](https://arxiv.org/abs/2505.02780)
*Jai Prakash Veerla,Partha Sai Guttikonda,Helen H. Shang,Mohammad Sadegh Nasr,Cesar Torres,Jacob M. Luber*

Main category: cs.HC

TL;DR: PathVis是一个为Apple Vision Pro设计的混合现实可视化平台，旨在解决传统数字病理工具在诊断过程中的局限，通过自然手势、眼动和语音命令提供直观交互，并结合AI提升诊断效率和精度。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理工具因图像尺寸巨大且显示限制，导致病理学家需频繁缩放移动，增加了认知负荷和疲劳，阻碍了数字方法的普及。PathVis旨在优化这一交互过程。

Method: PathVis采用混合现实技术，结合手势、眼动和语音命令进行交互，并集成AI功能，包括相似病例搜索和实时图像解读助手，支持多设备协作。

Result: PathVis显著改善了诊断流程，降低了认知负担，同时通过AI辅助提升了诊断效率和准确性，使病理实践更高效且互动性强。

Conclusion: PathVis通过混合现实和AI技术的结合，为数字病理学提供了更直观、高效的解决方案，有望推动该领域的进一步普及和发展。

Abstract: Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases
like cancer, yet current digital pathology tools hinder diagnosis. The immense
scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the
limited views traditional monitors offer. This mismatch forces constant panning
and zooming, increasing pathologist cognitive load, causing diagnostic fatigue,
and slowing pathologists' adoption of digital methods. PathVis, our
mixed-reality visualization platform for Apple Vision Pro, addresses these
challenges. It transforms the pathologist's interaction with data, replacing
cumbersome mouse-and-monitor navigation with intuitive exploration using
natural hand gestures, eye gaze, and voice commands in an immersive workspace.
PathVis integrates AI to enhance diagnosis. An AI-driven search function
instantly retrieves and displays the top five similar patient cases
side-by-side, improving diagnostic precision and efficiency through rapid
comparison. Additionally, a multimodal conversational AI assistant offers
real-time image interpretation support and aids collaboration among
pathologists across multiple Apple devices. By merging the directness of
traditional pathology with advanced mixed-reality visualization and AI, PathVis
improves diagnostic workflows, reduces cognitive strain, and makes pathology
practice more effective and engaging. The PathVis source code and a demo video
are publicly available at: https://github.com/jaiprakash1824/Path_Vis

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [302] [NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities](https://arxiv.org/abs/2505.02314)
*James Read,Ming-Yen Lee,Wei-Hsing Huang,Yuan-Chun Luo,Anni Lu,Shimeng Yu*

Main category: cs.AR

TL;DR: NeuroSim V1.5 是一个用于模拟内存内计算（ACIM）加速器的工具，通过TensorRT量化支持更多神经网络，优化了噪声注入方法并提升了运行速度。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构在AI应用中存在能效和延迟问题，ACIM通过直接内存计算减少数据移动，但需要更精确的器件和电路非理想性建模。

Method: 提出NeuroSim V1.5，整合TensorRT量化流程、噪声注入方法、支持新兴存储器，并通过优化模拟提升了运行速度。

Result: NeuroSim V1.5实现了最高6.5倍的速度提升，并支持跨精度和硬件效率的设计空间探索。

Conclusion: NeuroSim V1.5通过高保真噪声建模和高效模拟，推动了下一代ACIM加速器的设计与验证，所有版本已开源。

Abstract: The exponential growth of artificial intelligence (AI) applications has
exposed the inefficiency of conventional von Neumann architectures, where
frequent data transfers between compute units and memory create significant
energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses
this challenge by performing multiply-accumulate (MAC) operations directly in
the memory arrays, substantially reducing data movement. However, designing
robust ACIM accelerators requires accurate modeling of device- and
circuit-level non-idealities. In this work, we present NeuroSim V1.5,
introducing several key advances: (1) seamless integration with TensorRT's
post-training quantization flow enabling support for more neural networks
including transformers, (2) a flexible noise injection methodology built on
pre-characterized statistical models, making it straightforward to incorporate
data from SPICE simulations or silicon measurements, (3) expanded device
support including emerging non-volatile capacitive memories, and (4) up to 6.5x
faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The
combination of these capabilities uniquely enables systematic design space
exploration across both accuracy and hardware efficiency metrics. Through
multiple case studies, we demonstrate optimization of critical design
parameters while maintaining network accuracy. By bridging high-fidelity noise
modeling with efficient simulation, NeuroSim V1.5 advances the design and
validation of next-generation ACIM accelerators. All NeuroSim versions are
available open-source at https://github.com/neurosim/NeuroSim.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [303] [Dendritic Computing with Multi-Gate Ferroelectric Field-Effect Transistors](https://arxiv.org/abs/2505.01635)
*A N M Nafiul Islam,Xuezhong Niu,Jiahui Duan,Shubham Kumar,Kai Ni,Abhronil Sengupta*

Main category: cs.ET

TL;DR: 提出了一种基于多栅铁电场效应晶体管的新型神经元设计，模拟树突功能，通过铁电非线性实现局部计算，并利用晶体管动作生成神经元输出。实验证明，这种设计能以更少参数实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人工神经网络通常使用点神经元，其计算复杂度远低于生物神经元。生物神经元的树突结构和局部非线性积累在学习和处理中起关键作用。

Method: 基于多栅铁电场效应晶体管设计新型神经元，利用铁电非线性实现树突分支内的局部计算，并通过晶体管动作生成最终输出。采用实验校准的设备-电路-算法协同仿真框架验证性能。

Result: 与无树突的大规模网络相比，这种树突神经元网络仅需约1/17的可训练权重参数即可实现更优性能。

Conclusion: 这种树突硬件设计能显著提升计算效率和边缘应用场景下神经形态系统的学习能力。

Abstract: Although inspired by neuronal systems in the brain, artificial neural
networks generally employ point-neurons, which offer far less computational
complexity than their biological counterparts. Neurons have dendritic arbors
that connect to different sets of synapses and offer local non-linear
accumulation - playing a pivotal role in processing and learning. Inspired by
this, we propose a novel neuron design based on a multi-gate ferroelectric
field-effect transistor that mimics dendrites. It leverages ferroelectric
nonlinearity for local computations within dendritic branches, while utilizing
the transistor action to generate the final neuronal output. The branched
architecture paves the way for utilizing smaller crossbar arrays in hardware
integration, leading to greater efficiency. Using an experimentally calibrated
device-circuit-algorithm co-simulation framework, we demonstrate that networks
incorporating our dendritic neurons achieve superior performance in comparison
to much larger networks without dendrites ($\sim$17$\times$ fewer trainable
weight parameters). These findings suggest that dendritic hardware can
significantly improve computational efficiency, and learning capacity of
neuromorphic systems optimized for edge applications.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [304] [Identifying Doppelganger Active Galactic Nuclei across redshifts from spectroscopic surveys](https://arxiv.org/abs/2505.01642)
*Shreya Sareen,Swayamtrupta Panda*

Main category: astro-ph.GA

TL;DR: 研究探讨低红移AGN是否能作为高红移AGN的代理，通过光谱相似性分析，发现存在多个光谱高度相似的AGN。


<details>
  <summary>Details</summary>
Motivation: AGN是研究星系演化的重要工具，但理解其性质随宇宙时间的演变仍具挑战性。研究希望通过光谱相似性验证低红移AGN能否代表高红移AGN。

Method: 利用Sloan Digital Sky Survey Data Release 16的数据，分析AGN的关键光谱特征，包括连续谱和发射线（如N V、C IV、Mg II等），并整合等效宽度、速度弥散和连续谱光度等属性。

Result: 初步发现多个光谱高度相似的AGN，表明本地AGN可能与高红移AGN具有相似的内在性质。

Conclusion: 研究支持低红移AGN可能作为高红移AGN代理的假设，为研究宇宙时间尺度上的AGN性质提供了新思路。

Abstract: Active Galactic Nuclei (AGNs) are among the most luminous objects in the
universe, making them valuable probes for studying galaxy evolution. However,
understanding how AGN properties evolve over cosmic time remains a fundamental
challenge. This study investigates whether AGNs at low redshift (nearby) can
serve as proxies for their high-redshift (distant) counterparts by identifying
spectral 'doppelg\"angers', AGNs with remarkably similar emission line
properties despite being separated by vast cosmic distances. We analyze key
spectral features of bona fide AGNs using the Sloan Digital Sky Survey's Data
Release 16, including continuum and emission lines: Nitrogen (N V), Carbon (C
IV), Magnesium (Mg II), Hydrogen-beta (H$\beta$), and Iron (Fe II - optical and
UV) emission lines. We incorporated properties such as equivalent width,
velocity dispersion in the form of full width at half maximum (FWHM), and
continuum luminosities (135nm, 300nm, and 510nm) closest to these prominent
lines. Our initial findings suggest the existence of multiple AGNs with highly
similar spectra, hinting at the possibility that local AGNs may indeed share
intrinsic properties with high-redshift ones. We showcase here one of the
better candidate pairs of AGNs resulting from our analyses.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [305] [Pickup & Delivery with Time Windows and Transfers: combining decomposition with metaheuristics](https://arxiv.org/abs/2505.02158)
*Ioannis Avgerinos,Ioannis Mourtos,Nikolaos Tsompanidis,Georgios Zois*

Main category: math.OC

TL;DR: 本文研究了允许车辆中途交换货物并遵守严格时间窗的取货和送货问题的泛化。提出了新的LBBD和LNS算法，改进了现有基准，并开发了实例生成器进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大规模取货和送货问题时，无法有效处理中途货物交换和严格时间窗的约束，且缺乏足够的基准实例进行评估。

Method: 提出逻辑Benders分解（LBBD）和改进的大邻域搜索（LNS）算法，并开发实例生成器以弥补基准不足的问题。

Result: 在中小规模数据集（25和50请求）中，LBBD能完全闭合最优间隙，LNS提供接近最优解；在大规模数据集（75和100请求）中，改进的LNS展现了更好的可扩展性和性能。

Conclusion: 本文的LBBD和LNS算法显著提升了取货和送货问题的求解效率和可扩展性，尤其是在大规模实例中表现优异，为后续研究提供了坚实基础。

Abstract: This paper examines the generalisation of the Pickup and Delivery Problem
that allows mid-route load exchanges among vehicles and obeys strict
time-windows at all locations. We propose a novel Logic-Based Benders
Decomposition (LBBD) that improves optimality gaps for all benchmarks in the
literature and scales up to handle larger ones. To tackle even larger
instances, we introduce a refined Large Neighborhood Search (LNS) algorithm
that improves the adaptability of LNS beyond case-specific configurations
appearing in related literature.
  To bridge the gap in benchmark availability, we develop an instance generator
that allows for extensive experimentation. For moderate datasets (25 and 50
requests), we evaluate the performance of both LBBD and LNS, the former being
able to close the gap and the latter capable of providing near-optimal
solutions. For larger instances (75 and 100 requests), we recreate indicative
state-of-the-art metaheuristics to highlight the improvements introduced by our
LNS refinements, while establishing its scalability.

</details>


### [306] [A dynamic view of the double descent](https://arxiv.org/abs/2505.01751)
*Vivek Shripad Borkar*

Main category: math.OC

TL;DR: 论文研究了过参数化神经网络中的'双下降'现象，提出了基于双时间尺度随机近似和奇异扰动微分方程的理论解释。


<details>
  <summary>Details</summary>
Motivation: 探索过参数化神经网络在训练过程中出现的'双下降'现象的动态机制。

Method: 使用双时间尺度随机近似和奇异扰动微分方程理论，分析梯度动力学的连续时间极限。

Result: 为'双下降'现象提供了动态角度的理论解释。

Conclusion: 该研究为神经网络训练动态提供了新的理论视角。

Abstract: It has been observed by Belkin et al.\ that overparametrized neural networks
exhibit a `double descent' phenomenon. That is, as the model complexity, as
reflected in the number of features, increases, the training error initially
decreases, then increases, and then decreases again. A counterpart of this
phenomenon in the time domain has been noted in the context of epoch-wise
training, viz., that the training error decreases with time, then increases,
then decreases again. This note presents a plausible explanation for this
phenomenon by using the theory of two time scale stochastic approximation and
singularly perturbed differential equations, applied to the continuous time
limit of the gradient dynamics. This adds a `dynamic' angle to an already well
studied theme.

</details>


### [307] [Rank-One Modified Value Iteration](https://arxiv.org/abs/2505.01828)
*Arman Sharifi Kolarijani,Tolga Ok,Peyman Mohajerin Esfahani,Mohamad Amin Sharif Kolarijani*

Main category: math.OC

TL;DR: 本文提出了一种新颖的算法，通过使用转移概率矩阵的秩一近似来解决马尔可夫决策过程的规划和学习问题，并证明了其收敛性及计算效率。实验表明该算法在规划和学习问题中优于一阶算法及其加速版本。


<details>
  <summary>Details</summary>
Motivation: 现有的规划和学习算法（如值迭代和Q学习）在效率和性能上仍有提升空间。秩一近似方法在简化和加速计算的同时，能够更好地捕捉转移概率矩阵的关键特征。

Method: 算法采用策略迭代框架，在策略评估步骤中引入秩一近似代替完整的转移概率矩阵，并结合幂方法估算其平稳分布。

Result: 理论分析证明算法的收敛性与传统值迭代和Q学习一致，但实验显示其性能显著优于一阶算法及其加速版本。

Conclusion: 该算法通过秩一近似在保持理论优势的同时，实现了更高的计算效率，为解决复杂规划和学习问题提供了新思路。

Abstract: In this paper, we provide a novel algorithm for solving planning and learning
problems of Markov decision processes. The proposed algorithm follows a policy
iteration-type update by using a rank-one approximation of the transition
probability matrix in the policy evaluation step. This rank-one approximation
is closely related to the stationary distribution of the corresponding
transition probability matrix, which is approximated using the power method. We
provide theoretical guarantees for the convergence of the proposed algorithm to
optimal (action-)value function with the same rate and computational complexity
as the value iteration algorithm in the planning problem and as the Q-learning
algorithm in the learning problem. Through our extensive numerical simulations,
however, we show that the proposed algorithm consistently outperforms
first-order algorithms and their accelerated versions for both planning and
learning problems.

</details>


### [308] [Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles](https://arxiv.org/abs/2505.02281)
*Amir Ali Farzin,Yuen-Man Pun,Iman Shames*

Main category: math.OC

TL;DR: 该研究探讨了随机高斯平滑零阶（ZO）算法在最小化拟星凸（QC）和强拟星凸（SQC）函数时的性能（包括无约束和有约束情况），证明了算法的收敛性和复杂度，并在机器学习问题中验证其优于梯度下降的场景。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证ZO算法在拟星凸函数优化中的有效性，特别是在无梯度的复杂或高维问题中，探索其相对于传统梯度下降的优势。

Method: 采用随机高斯平滑零阶优化方法，提出新概念“近端拟星凸性”处理约束问题，分析其收敛性和复杂度。

Result: 在无约束和有约束问题中，ZO算法均收敛到全局最优或可控邻域，且在部分场景中表现优于梯度下降。

Conclusion: ZO算法为拟星凸函数优化提供了一种有效工具，尤其在无梯度或高维问题中具有潜力。

Abstract: This study explores the performance of a random Gaussian smoothing
zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly
quasar-convex (SQC) functions in both unconstrained and constrained settings.
For the unconstrained problem, we establish the ZO algorithm's convergence to a
global minimum along with its complexity when applied to both QC and SQC
functions. For the constrained problem, we introduce the new notion of
proximal-quasar-convexity and prove analogous results to the unconstrained
case. Specifically, we show the complexity bounds and the convergence of the
algorithm to a neighbourhood of a global minimum whose size can be controlled
under a variance reduction scheme. Theoretical findings are illustrated through
investigating the performance of the algorithm applied to a range of problems
in machine learning and optimisation. Specifically, we observe scenarios where
the ZO method outperforms gradient descent. We provide a possible explanation
for this phenomenon.

</details>


### [309] [Temporal Robustness in Discrete Time Linear Dynamical Systems](https://arxiv.org/abs/2505.02347)
*Nilava Metya,Arunesh Sinha*

Main category: math.OC

TL;DR: 该论文研究了离散时间线性动态系统在不确定时间范围下的成本估计问题，提出了一种基于Wasserstein模糊集的分布鲁棒方法，并给出了多项式时间算法和硬度结果。


<details>
  <summary>Details</summary>
Motivation: 由于系统运行时间范围的不确定性，传统方法难以准确估计成本或奖励，因此需要一种更鲁棒的方法。

Method: 通过将离散时间马尔可夫链等价为全局渐近稳定的线性动态系统，提出基于Wasserstein模糊集的分布鲁棒方法。

Result: 证明了多项式时间算法的可行性，并提供了关于Wasserstein距离多面体的重要结果。

Conclusion: 该方法为不确定时间范围内的成本估计提供了理论框架和实用工具。

Abstract: Discrete time linear dynamical systems, including Markov chains, have found
many applications. However, in some problems, there is uncertainty about the
time horizon for which the system runs. This creates uncertainty about the cost
(or reward) incurred based on the state distribution when the system stops.
Given past data samples of how long a system ran, we propose to theoretically
analyze a distributional robust cost estimation task in a Wasserstein ambiguity
set, instead of learning a probability distribution from a few samples. Towards
this, we show an equivalence between a discrete time Markov Chain on a
probability simplex and a global asymptotic stable (GAS) discrete time linear
dynamical system, allowing us to base our study on a GAS system only. Then, we
provide various polynomial time algorithms and hardness results for different
cases in our theoretical study, including a fundamental result about
Wasserstein distance based polytope.

</details>


### [310] [Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate](https://arxiv.org/abs/2505.01985)
*Hung Pham,Aiden Ren,Ibrahim Tahir,Jiatai Tong,Thiago Serra*

Main category: math.OC

TL;DR: 该论文提出通过神经网络剪枝生成稀疏代理模型，以提升优化模型的计算效率，特别是在混合整数线性规划（MILP）求解器中验证神经网络时，稀疏代理能更快生成对抗扰动。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络代理在优化模型中计算困难的问题，探索剪枝技术提升效率的潜力。

Method: 应用网络剪枝生成稀疏代理模型，将其嵌入MILP求解器进行验证。

Result: 稀疏代理模型即使未微调（分类性能较差）也能更高效地生成对抗扰动。

Conclusion: 剪枝后的网络即使性能下降，仍可作为高效代理，为优化问题提供新思路。

Abstract: We can approximate a constraint or an objective function that is uncertain or
nonlinear with a neural network that we embed in the optimization model. This
approach, which is known as constraint learning, faces the challenge that
optimization models with neural network surrogates are harder to solve. Such
difficulties have motivated studies on model reformulation, specialized
optimization algorithms, and - to a lesser extent - pruning of the embedded
networks. In this work, we double down on the use of surrogates by applying
network pruning to produce a surrogate of the neural network itself. In the
context of using a Mixed-Integer Linear Programming (MILP) solver to verify
neural networks, we obtained faster adversarial perturbations for dense neural
networks by using sparse surrogates, especially - and surprisingly - if not
taking the time to finetune the sparse network to make up for the loss in
accuracy. In other words, we show that a pruned network with bad classification
performance can still be a good - and more efficient - surrogate.

</details>


### [311] [Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization](https://arxiv.org/abs/2505.02101)
*Youran Dong,Junfeng Yang,Wei Yao,Jin Zhang*

Main category: math.OC

TL;DR: 本文提出了一种高效的双层优化超梯度近似方法，利用曲率信息改进超梯度计算，并通过理论和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化中超梯度计算效率低下的问题，提升梯度方法的性能和计算效率。

Method: 提出了一种结合曲率信息的超梯度近似技术，并基于此设计了新算法框架，利用了超梯度结构和不精确牛顿法。

Result: 理论证明了算法在确定性和随机场景下的收敛率，并在确定性设置中展示了优于常见梯度方法的计算复杂度。

Conclusion: 新方法不仅理论上有优势，实验也证明了其显著的实际性能提升。

Abstract: Bilevel optimization is a powerful tool for many machine learning problems,
such as hyperparameter optimization and meta-learning. Estimating
hypergradients (also known as implicit gradients) is crucial for developing
gradient-based methods for bilevel optimization. In this work, we propose a
computationally efficient technique for incorporating curvature information
into the approximation of hypergradients and present a novel algorithmic
framework based on the resulting enhanced hypergradient computation. We provide
convergence rate guarantees for the proposed framework in both deterministic
and stochastic scenarios, particularly showing improved computational
complexity over popular gradient-based methods in the deterministic setting.
This improvement in complexity arises from a careful exploitation of the
hypergradient structure and the inexact Newton method. In addition to the
theoretical speedup, numerical experiments demonstrate the significant
practical performance benefits of incorporating curvature information.

</details>


### [312] [Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints](https://arxiv.org/abs/2505.02485)
*Lucas Kletzander,Tommaso Mannelli Mazzoli,Nysret Musliu,Pascal Van Hentenryck*

Main category: math.OC

TL;DR: 本文研究了公交车司机调度问题（BDSP），提出了一种结合分支定价（B&P）和大邻域搜索（LNS）的混合方法，并通过深度整合B&P和LNS，实现了对小、中、大规模实例的高质量解决方案。


<details>
  <summary>Details</summary>
Motivation: 公交车司机调度问题受到严格的法律规则和集体协议约束，需要兼顾运营成本和司机满意度。本文旨在提供一种能够为不同规模实例提供高质量解决方案的先进方法。

Method: 本文结合了分支定价（B&P）和大邻域搜索（LNS），并提出了一种深度整合方法，通过存储和重用LNS子问题生成的列来优化全局解。还改进了B&P子问题（资源约束最短路径问题，RCSPP）的求解方法。

Result: 实验表明，本文方法在小规模实例上实现了最优解，在中规模实例上与已知下界的差距较小，同时在所有规模实例上均达到了当前最佳结果。

Conclusion: B&P在小规模实例中表现最佳，而紧密整合的LNS和CG能够为大规模实例提供高质量解。本文方法具有通用性，可应用于其他规则集和相关优化问题。

Abstract: The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization
problem with the goal to design shifts to cover prearranged bus tours. The
objective takes into account the operational cost as well as the satisfaction
of drivers. This problem is heavily constrained due to strict legal rules and
collective agreements. The objective of this article is to provide
state-of-the-art exact and hybrid solution methods that can provide
high-quality solutions for instances of different sizes. This work presents a
comprehensive study of both an exact method, Branch and Price (B&P), as well as
a Large Neighborhood Search (LNS) framework which uses B&P or Column Generation
(CG) for the repair phase to solve the BDSP. It further proposes and evaluates
a novel deeper integration of B&P and LNS, storing the generated columns from
the LNS subproblems and reusing them for other subproblems, or to find better
global solutions. The article presents a detailed analysis of several
components of the solution methods and their impact, including general
improvements for the B&P subproblem, which is a high-dimensional Resource
Constrained Shortest Path Problem (RCSPP), and the components of the LNS. The
evaluation shows that our approach provides new state-of-the-art results for
instances of all sizes, including exact solutions for small instances, and low
gaps to a known lower bound for mid-sized instances. Conclusions: We observe
that B&P provides the best results for small instances, while the tight
integration of LNS and CG can provide high-quality solutions for larger
instances, further improving over LNS which just uses CG as a black box. The
proposed methods are general and can also be applied to other rule sets and
related optimization problems

</details>


### [313] [Smooth Integer Encoding via Integral Balance](https://arxiv.org/abs/2505.02259)
*Stanislav Semenov*

Main category: math.OC

TL;DR: 介绍了一种用平滑实值函数编码整数的新方法，通过函数的积分特性隐式反映离散量，支持连续优化和机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法显式表示整数，而本研究旨在通过平滑函数隐式编码，以支持连续优化和机器学习中的离散逻辑嵌入。

Method: 利用局部高斯凹凸函数交替叠加构造平滑函数f_N(t)，通过积分收敛性隐式编码整数N，并支持数值或解析恢复。

Result: 验证了编码序列的收敛性，展示了积分映射I(N)的数值构造方法，并开发了数值反演恢复整数的流程。

Conclusion: 该方法为离散逻辑在连续优化、机器学习架构及平滑符号计算中的嵌入提供了新路径。

Abstract: We introduce a novel method for encoding integers using smooth real-valued
functions whose integral properties implicitly reflect discrete quantities. In
contrast to classical representations, where the integer appears as an explicit
parameter, our approach encodes the number N in the set of natural numbers
through the cumulative balance of a smooth function f_N(t), constructed from
localized Gaussian bumps with alternating and decaying coefficients. The total
integral I(N) converges to zero as N tends to infinity, and the integer can be
recovered as the minimal point of near-cancellation.
  This method enables continuous and differentiable representations of discrete
states, supports recovery through spline-based or analytical inversion, and
extends naturally to multidimensional tuples (N1, N2, ...). We analyze the
structure and convergence of the encoding series, demonstrate numerical
construction of the integral map I(N), and develop procedures for integer
recovery via numerical inversion. The resulting framework opens a path toward
embedding discrete logic within continuous optimization pipelines, machine
learning architectures, and smooth symbolic computation.

</details>


### [314] [Entropic Mirror Descent for Linear Systems: Polyak's Stepsize and Implicit Bias](https://arxiv.org/abs/2505.02614)
*Yura Malitsky,Alexander Posch*

Main category: math.OC

TL;DR: 论文提出了一种改进的Polyak型步长方法，用于解决线性系统的无界域问题，展示了收敛性结果，并推广到了任意凸L-平滑函数。


<details>
  <summary>Details</summary>
Motivation: 解决线性系统求解中无界域带来的收敛分析挑战，避免使用限制性假设。

Method: 引入改进的Polyak型步长，提出避免指数运算的替代方法。

Result: 获得了ℓ₁-范数隐式偏差的严格边界，实现了亚线性与线性收敛，并推广到凸L-平滑函数。

Conclusion: 证明了新方法的收敛性，拓展了应用范围。

Abstract: This paper focuses on applying entropic mirror descent to solve linear
systems, where the main challenge for the convergence analysis stems from the
unboundedness of the domain. To overcome this without imposing restrictive
assumptions, we introduce a variant of Polyak-type stepsizes. Along the way, we
strengthen the bound for $\ell_1$-norm implicit bias, obtain sublinear and
linear convergence results, and generalize the convergence result to arbitrary
convex $L$-smooth functions. We also propose an alternative method that avoids
exponentiation, resembling the original Hadamard descent, but with provable
convergence.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [315] [Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction](https://arxiv.org/abs/2505.01781)
*Ziye Yang,Ke Lu*

Main category: cs.CE

TL;DR: 论文提出了一种结合SSA、MA-EMD和TCN的混合深度学习模型，用于提升Black-Litterman模型的主观观点生成能力。实验表明该模型预测性能优于基准模型，且结合Black-Litterman的投资组合在短期持有中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统Mean-Variance模型对输入参数敏感且缺乏灵活性，而Black-Litterman模型通过结合市场均衡收益和投资者主观观点更受关注。但后者依赖准确的主观观点生成，因此需要提升资产价格预测精度来优化其表现。

Method: 提出混合深度学习模型：SSA用于降噪预处理，MA-EMD分解多变量时序数据，TCN捕获时序依赖关系，以提升Black-Litterman模型的主观观点预测能力。

Result: 实验显示降噪预处理提高了模型精度，混合模型预测性能显著优于三种多变量分解基准模型。基于NASDAQ 100成分股的投资组合在短期持有中表现优于Mean-Variance等传统模型。

Conclusion: 该混合模型有效提升了Black-Litterman模型的主观观点生成能力，结合后的投资组合在收益和风险控制上表现更优，验证了方法在短期投资中的实用性。

Abstract: The sensitivity to input parameters and lack of flexibility limits the
traditional Mean-Variance model. In contrast, the Black-Litterman model has
attracted widespread attention by integrating market equilibrium returns with
investors' subjective views. This paper proposes a novel hybrid deep learning
model combining Singular Spectrum analysis (SSA), Multivariate Aligned
Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks
(TCNs), aiming to improve the prediction accuracy of asset prices and thus
enhance the ability of the Black-Litterman model to generate subjective views.
Experimental results show that noise reduction pre-processing can improve the
model's accuracy, and the prediction performance of the proposed model is
significantly better than that of three multivariate decomposition benchmark
models. We construct an investment portfolio by using 20 representative stocks
from the NASDAQ 100 index. By combining the hybrid forecasting model with the
Black-Litterman model, the generated investment portfolio exhibits better
returns and risk control capabilities than the Mean-Variance, Equal-Weighted,
and Market-Weighted models in the short holding period.

</details>


### [316] [Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking](https://arxiv.org/abs/2505.02139)
*Muyao Zhong,Yushi Lin,Peng Yang*

Main category: cs.CE

TL;DR: 这篇论文对限价订单簿（LOB）表示学习进行了首次系统性比较研究，提出了一种提取可转移紧凑特征的有效方法，并引入了LOBench标准化基准。


<details>
  <summary>Details</summary>
Motivation: LOB数据在金融市场中至关重要，但现有方法通常在端到端任务中紧密耦合表示学习，缺乏对学习表示的单独和显式分析，限制了其可重用性和泛化性。

Method: 论文提出了LOBench，包含真实中国A股市场数据，提供统一预处理、一致评估指标和强基线。

Result: 实验验证了LOB表示对各种下游任务的充分性和必要性，并展示了其相对于传统任务特定端到端模型和先进时间序列表示学习模型的优势。

Conclusion: 研究建立了一个可复现的框架，并为未来研究提供了明确指南。

Abstract: The Limit Order Book (LOB), the mostly fundamental data of the financial
market, provides a fine-grained view of market dynamics while poses significant
challenges in dealing with the esteemed deep models due to its strong
autocorrelation, cross-feature constrains, and feature scale disparity.
Existing approaches often tightly couple representation learning with specific
downstream tasks in an end-to-end manner, failed to analyze the learned
representations individually and explicitly, limiting their reusability and
generalization. This paper conducts the first systematic comparative study of
LOB representation learning, aiming to identify the effective way of extracting
transferable, compact features that capture essential LOB properties. We
introduce LOBench, a standardized benchmark with real China A-share market
data, offering curated datasets, unified preprocessing, consistent evaluation
metrics, and strong baselines. Extensive experiments validate the sufficiency
and necessity of LOB representations for various downstream tasks and highlight
their advantages over both the traditional task-specific end-to-end models and
the advanced representation learning models for general time series. Our work
establishes a reproducible framework and provides clear guidelines for future
research. Datasets and code will be publicly available at
https://github.com/financial-simulation-lab/LOBench.

</details>


### [317] [Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach](https://arxiv.org/abs/2505.02170)
*Danial Ramezani*

Main category: cs.CE

TL;DR: 该论文提出了确定性和鲁棒整数规划模型，用于优化梦幻足球阵容选择，结合可解释AI框架和比赛数据构建混合评分指标，并在英超赛季数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 梦幻足球是一个价值数十亿美元的产业，参与者需要在固定预算下选择表现优异的球员以最大化得分。现有的方法缺乏系统性优化，因此需要更科学的模型来解决这一问题。

Method: 采用确定性和鲁棒整数规划模型，结合可解释AI框架构建混合评分指标，并引入多种目标函数和估计技术。通过蒙特卡洛模拟和成本向量平均技术进行验证。

Result: 在2023/24英超赛季数据中，提出的混合方法实现了最高得分且表现稳定。蒙特卡洛模拟和混合方法在样本外测试中表现有效。

Conclusion: 该研究为梦幻足球提供了科学的优化模型，验证了混合方法的有效性，并为玩家选择、阵型和策略提供了实用见解。

Abstract: Fantasy football is a billion-dollar industry with millions of participants.
Constrained by a fixed budget, decision-makers draft a squad whose players are
expected to perform well in the upcoming weeks to maximize total points. This
paper proposes novel deterministic and robust integer programming models that
select the optimal starting eleven and the captain. A new hybrid scoring metric
is constructed using an interpretable artificial intelligence framework and
underlying match performance data. Several objective functions and estimation
techniques are introduced for the programming model. To the best of my
knowledge, this is the first study to approach fantasy football through this
lens. The models' performance is evaluated using data from the 2023/24 Premier
League season. Results indicate that the proposed hybrid method achieved the
highest score while maintaining consistent performance. Utilizing the Monte
Carlo simulation, the strategic choice of averaging techniques for estimating
cost vectors, and the proposed hybrid approach are shown to be effective during
the out-of-sample period. This paper also provides a thorough analysis of the
optimal formations and players selected by the models, offering valuable
insights into effective fantasy football strategies.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [318] [Surrogate to Poincaré inequalities on manifolds for dimension reduction in nonlinear feature spaces](https://arxiv.org/abs/2505.01807)
*Anthony Nouy,Alexandre Pasco*

Main category: math.NA

TL;DR: 论文提出了一种通过凸替代损失函数来逼近连续可微函数的方法，优于标准迭代方法，尤其在小型训练集和m=1时表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过最小化基于庞加莱不等式的损失函数来构建非线性映射g，但优化该损失函数可能具有挑战性，因此需要提出更易优化的替代方案。

Method: 采用两阶段方法：固定g时用经典回归方法构建f；引入新的凸替代损失函数来优化g，并通过浓度不等式提供次优性结果。

Result: 在多种基准测试中，该方法优于标准迭代方法，尤其是在训练集较小且m=1时，近似误差更小。

Conclusion: 提出的凸替代损失函数有效解决了原损失函数的优化难题，显著提升了函数逼近的性能。

Abstract: We aim to approximate a continuously differentiable function $u:\mathbb{R}^d
\rightarrow \mathbb{R}$ by a composition of functions $f\circ g$ where
$g:\mathbb{R}^d \rightarrow \mathbb{R}^m$, $m\leq d$, and $f : \mathbb{R}^m
\rightarrow \mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we
build $f$ using classical regression methods, involving evaluations of $u$.
Recent works proposed to build a nonlinear $g$ by minimizing a loss function
$\mathcal{J}(g)$ derived from Poincar\'e inequalities on manifolds, involving
evaluations of the gradient of $u$. A problem is that minimizing $\mathcal{J}$
may be a challenging task. Hence in this work, we introduce new convex
surrogates to $\mathcal{J}$. Leveraging concentration inequalities, we provide
sub-optimality results for a class of functions $g$, including polynomials, and
a wide class of input probability measures. We investigate performances on
different benchmarks for various training sample sizes. We show that our
approach outperforms standard iterative methods for minimizing the training
Poincar\'e inequality based loss, often resulting in better approximation
errors, especially for rather small training sets and $m=1$.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [319] [Heterosynaptic Circuits Are Universal Gradient Machines](https://arxiv.org/abs/2505.02248)
*Liu Ziyin,Isaac Chuang,Tomaso Poggio*

Main category: q-bio.NC

TL;DR: 论文提出一种生物大脑学习电路的设计原则，认为异突触可塑性（HSP）可实现高效的梯度元学习，统一解释了（反）Hebbian可塑性（HBP）与HSP的动力学机制，并指出HSP可能是主要学习机制，而HBP是其副产品。模拟实验验证了HSP的元可塑性、生物电路灵活性及梯度学习的自然进化起源，对AI算法与硬件设计有启示。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示生物大脑学习机制的统一原理，挑战传统认为梯度计算复杂的观点，并为AI训练算法和硬件提供生物学启发的新思路。

Method: 提出基于异突触可塑性（HSP）的通用学习原则，通过理论分析和模拟实验验证HSP能否解释神经元的元可塑性、电路灵活性，以及梯度学习如何从自然进化中涌现。

Result: HSP被证明可统一解释HBP现象，是更基础的学习机制；模拟显示HSP支持元可塑性和快速梯度学习，且无需显式梯度计算。

Conclusion: 梯度计算可能在自然界中普遍且简单，HSP为生物与AI学习提供了新范式，强调其作为核心机制的潜力。

Abstract: We propose a design principle for the learning circuits of the biological
brain. The principle states that almost any dendritic weights updated via
heterosynaptic plasticity can implement a generalized and efficient class of
gradient-based meta-learning. The theory suggests that a broad class of
biologically plausible learning algorithms, together with the standard machine
learning optimizers, can be grounded in heterosynaptic circuit motifs. This
principle suggests that the phenomenology of (anti-) Hebbian (HBP) and
heterosynaptic plasticity (HSP) may emerge from the same underlying dynamics,
thus providing a unifying explanation. It also suggests an alternative
perspective of neuroplasticity, where HSP is promoted to the primary learning
and memory mechanism, and HBP is an emergent byproduct. We present simulations
that show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can
explain the flexibility of the biology circuits, and (c) gradient learning can
arise quickly from simple evolutionary dynamics that do not compute any
explicit gradient. While our primary focus is on biology, the principle also
implies a new approach to designing AI training algorithms and physically
learnable AI hardware. Conceptually, our result demonstrates that contrary to
the common belief, gradient computation may be extremely easy and common in
nature.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [320] [Explainability by design: an experimental analysis of the legal coding process](https://arxiv.org/abs/2505.01944)
*Matteo Cristani,Guido Governatori,Francesco Olivieri,Monica Palmirani,Gabriele Buriola*

Main category: cs.LO

TL;DR: 本文提出了一种法律编码方法，将文本片段映射为Deontic Defeasible Logic规则，并通过场景测试其正确性。实验测量了编码过程中的努力与可测量特征的关系，并利用Houdini技术处理示例。最后，提出了一种基于法律知识、编码经验、文本长度和引用深度等因素的时间预测技术。


<details>
  <summary>Details</summary>
Motivation: 法律文本编码为逻辑规则时缺乏系统方法，且编码效率和准确性难以量化。本文旨在填补这一空白，提供可测试的方法和工具支持。

Method: 提出从文本到Deontic Defeasible Logic规则的法律编码方法，结合场景测试，并利用Houdini技术实现推理。

Result: 实验验证了编码过程中可测量特征（如法律知识、文本长度等）与编码努力的关系，展示了方法的可行性。

Conclusion: 本文提出的编码方法及时间预测技术，为法律逻辑编码提供了可操作框架，未来可扩展至更复杂场景。

Abstract: Behind a set of rules in Deontic Defeasible Logic, there is a mapping process
of normative background fragments. This process goes from text to rules and
implicitly encompasses an explanation of the coded fragments.
  In this paper we deliver a methodology for \textit{legal coding} that starts
with a fragment and goes onto a set of Deontic Defeasible Logic rules,
involving a set of \textit{scenarios} to test the correctness of the coded
fragments. The methodology is illustrated by the coding process of an example
text. We then show the results of a series of experiments conducted with humans
encoding a variety of normative backgrounds and corresponding cases in which we
have measured the efforts made in the coding process, as related to some
measurable features. To process these examples, a recently developed
technology, Houdini, that allows reasoning in Deontic Defeasible Logic, has
been employed.
  Finally we provide a technique to forecast time required in coding, that
depends on factors such as knowledge of the legal domain, knowledge of the
coding processes, length of the text, and a measure of \textit{depth} that
refers to the length of the paths of legal references.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [321] [Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks](https://arxiv.org/abs/2505.02258)
*Emir Esenov,Olof Hjortstam,Yuriy Serdyuk,Thomas Hammarström,Christian Häger*

Main category: eess.SY

TL;DR: 论文探讨了使用物理信息神经网络（PINNs）对时间域中的介电响应进行逆建模的有效性，通过合成数据验证其能准确估计最多五个未知RC参数，并在引入温度依赖性后仍保持高精度。


<details>
  <summary>Details</summary>
Motivation: 绝缘材料的介电响应是设计电气绝缘系统的关键输入，但传统等效电路模型（ECMs）难以物理解释原始数据。研究旨在利用PINNs简化逆建模过程，提高参数估计的准确性和效率。

Method: 采用物理信息神经网络（PINNs）对时间域中的介电响应进行逆建模，测试基于合成数据（含高斯噪声以模拟测量误差），并扩展ECMs以包含温度依赖性。

Result: PINNs能够高精度估计多至五个未知RC参数，网络规模、训练时间和超参数调优需求极低，且在温度依赖性模型中仍保持准确性。

Conclusion: PINNs为依赖ECMs的学科提供了高效、准确的逆建模方案，展现了机器学习在科学计算中的广泛应用潜力。

Abstract: Dielectric response (DR) of insulating materials is key input information for
designing electrical insulation systems and defining safe operating conditions
of various HV devices. In dielectric materials, different polarization and
conduction processes occur at different time scales, making it challenging to
physically interpret raw measured data. To analyze DR measurement results,
equivalent circuit models (ECMs) are commonly used, reducing the complexity of
the physical system to a number of circuit elements that capture the dominant
response. This paper examines the use of physics-informed neural networks
(PINNs) for inverse modeling of DR in time domain using parallel RC circuits.
To assess their performance, we test PINNs on synthetic data generated from
analytical solutions of corresponding ECMs, incorporating Gaussian noise to
simulate measurement errors. Our results show that PINNs are highly effective
at solving well-conditioned inverse problems, accurately estimating up to five
unknown RC parameters with minimal requirements on neural network size,
training duration, and hyperparameter tuning. Furthermore, we extend the ECMs
to incorporate temperature dependence and demonstrate that PINNs can accurately
recover embedded, nonlinear temperature functions from noisy DR data sampled at
different temperatures. This case study in modeling DR in time domain presents
a solution with wide-ranging potential applications in disciplines relying on
ECMs, utilizing the latest technology in machine learning for scientific
computation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [322] [Faster logconcave sampling from a cold start in high dimension](https://arxiv.org/abs/2505.01937)
*Yunbum Kook,Santosh S. Vempala*

Main category: cs.DS

TL;DR: 该论文提出了一种更快的算法，用于生成任意对数凹密度的预热启动样本，首次实现了在（近）各向同性位置的输入下的亚立方采样算法。之前的算法在维度上至少需要线性预热惩罚，而该研究通过两种关键技术改进了这一限制。


<details>
  <summary>Details</summary>
Motivation: 长期以来，采样对数凹密度时，预热启动的代价至少与维度线性相关，即使在凸体均匀采样这一特例中也存在立方障碍。本研究旨在突破这一限制，提供更高效的采样方法。

Method: 采用了两种关键技术：(1) 在较弱距离概念（如$q$-Rényi散度）下实现采样；(2) 改进并推广了Lee和Vempala (2018)的对数Sobolev不等式，考虑支持直径和协方差矩阵最大特征值的几何平均。

Result: 首次实现了亚立方采样算法，降低了预热启动的维数依赖，是自Lovász和Simonovits (1991)以来对预热要求的首次改进。

Conclusion: 该研究不仅提升了采样效率，还扩展了对数Sobolev不等式的适用范围，为对数凹密度的采样提供了更通用的理论工具。

Abstract: We present a faster algorithm to generate a warm start for sampling an
arbitrary logconcave density specified by an evaluation oracle, leading to the
first sub-cubic sampling algorithms for inputs in (near-)isotropic position. A
long line of prior work incurred a warm-start penalty of at least linear in the
dimension, hitting a cubic barrier, even for the special case of uniform
sampling from convex bodies.
  Our improvement relies on two key ingredients of independent interest. (1) We
show how to sample given a warm start in weaker notions of distance, in
particular $q$-R\'enyi divergence for $q=\widetilde{\mathcal{O}}(1)$, whereas
previous analyses required stringent $\infty$-R\'enyi divergence (with the
exception of Hit-and-Run, whose known mixing time is higher). This marks the
first improvement in the required warmness since Lov\'asz and Simonovits
(1991). (2) We refine and generalize the log-Sobolev inequality of Lee and
Vempala (2018), originally established for isotropic logconcave distributions
in terms of the diameter of the support, to logconcave distributions in terms
of a geometric average of the support diameter and the largest eigenvalue of
the covariance matrix.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [323] [Safe and Efficient CAV Lane Changing using Decentralised Safety Shields](https://arxiv.org/abs/2505.01453)
*Bharathkumar Hegde,Melanie Bouroche*

Main category: cs.MA

TL;DR: 论文提出了一种结合优化和基于规则的分散式混合安全护盾（HSS），用于CAV的变道决策，确保安全的同时提升交通效率。


<details>
  <summary>Details</summary>
Motivation: 解决CAV在变道决策中如何在提升交通效率的同时确保安全的问题。

Method: 提出HSS方法，结合控制屏障函数约束CAV的纵向和横向控制输入，并与MARL集成（MARL-HSS）。

Result: 在模拟环境中验证，HSS能严格保障动态安全约束，且在中等密度交通中表现稳定，实现零事故与可比的平均速度。

Conclusion: MARL-HSS在变道决策中成功平衡了安全性与交通效率，优于无安全护盾的基线方法。

Abstract: Lane changing is a complex decision-making problem for Connected and
Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with
safety. Although traffic efficiency can be improved by using vehicular
communication for training lane change controllers using Multi-Agent
Reinforcement Learning (MARL), ensuring safety is difficult. To address this
issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines
optimisation and a rule-based approach to guarantee safety. Our method applies
control barrier functions to constrain longitudinal and lateral control inputs
of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to
integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while
ensuring safety. We evaluate MARL-HSS using a gym-like environment that
simulates an on-ramp merging scenario with two levels of traffic densities,
such as light and moderate densities. The results show that HSS provides a
safety guarantee by strictly enforcing a dynamic safety constraint defined on a
time headway, even in moderate traffic density that offers challenging lane
change scenarios. Moreover, the proposed method learns stable policies compared
to the baseline, a state-of-the-art MARL lane change controller without a
safety shield. Further policy evaluation shows that our method achieves a
balance between safety and traffic efficiency with zero crashes and comparable
average speeds in light and moderate traffic densities.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [324] [Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling](https://arxiv.org/abs/2505.01917)
*Javier E. Santos,Agnese Marcato,Roman Colman,Nicholas Lubbers,Yen Ting Lin*

Main category: cs.GR

TL;DR: 为解决生成扩散模型在处理离散空间和严格守恒量（如质量）时的局限性，提出了离散空间扩散（DSD）框架，通过连续时间离散状态的跳跃随机过程直接在离散空间中操作并保持质量守恒。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型通常基于连续强度空间，难以适用于离散且需严格守恒（如质量）的科学应用，限制了其在这些领域的实用性。

Method: 提出了DSD框架，基于连续时间离散状态的跳跃随机过程，直接在离散空间操作，确保了质量和扩散过程的正逆向严格守恒。

Result: DSD在图像合成、类别条件和修复任务上表现出灵活性，并能应用于材料微观结构等科学数据，弥合了扩散模型与科学应用的鸿沟。

Conclusion: DSD框架扩展了扩散模型的适用性，为需要严格质量守恒的科学工作流提供了一种有效的解决方案。

Abstract: Generative diffusion models have achieved remarkable success in producing
high-quality images. However, because these models typically operate in
continuous intensity spaces - diffusing independently per pixel and color
channel - they are fundamentally ill-suited for applications where quantities
such as particle counts or material units are inherently discrete and governed
by strict conservation laws such as mass preservation, limiting their
applicability in scientific workflows. To address this limitation, we propose
Discrete Spatial Diffusion (DSD), a framework based on a continuous-time,
discrete-state jump stochastic process that operates directly in discrete
spatial domains while strictly preserving mass in both forward and reverse
diffusion processes. By using spatial diffusion to achieve mass preservation,
we introduce stochasticity naturally through a discrete formulation. We
demonstrate the expressive flexibility of DSD by performing image synthesis,
class conditioning, and image inpainting across widely-used image benchmarks,
with the ability to condition on image intensity. Additionally, we highlight
its applicability to domain-specific scientific data for materials
microstructure, bridging the gap between diffusion models and mass-conditioned
scientific applications.

</details>


### [325] [Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation](https://arxiv.org/abs/2505.02350)
*Bobo Lian,Dandan Wang,Chenjian Wu,Minxin Chen*

Main category: cs.GR

TL;DR: 提出了一种基于稀疏椭球径向基函数网络的机器学习方法，用于近似点云的符号距离函数（SDF），通过动态多目标优化策略和层次化八叉树细化策略，实现了高效且精确的表面表示。


<details>
  <summary>Details</summary>
Motivation: 点云表面表示是计算机图形学和视觉中的基础问题，传统方法难以在稀疏性和精度之间取得平衡。本文旨在通过稀疏椭球径向基函数网络（ERBFs）实现紧凑且准确的表面表示。

Method: 使用椭球径向基函数（ERBFs）近似SDF，通过动态多目标优化策略平衡稀疏性和精度，并采用最近邻数据结构和CUDA并行计算提升效率。层次化八叉树细化策略用于参数初始化和优化。

Result: 在多个基准数据集上的实验表明，该方法在准确性、鲁棒性和计算效率上优于先前稀疏表示方法。

Conclusion: 通过稀疏ERBFs和优化策略，本文实现了高效且精确的点云表面表示，代码已开源。

Abstract: Point cloud surface representation is a fundamental problem in computer
graphics and vision. This paper presents a machine learning approach for
approximating the signed distance function (SDF) of a point cloud using sparse
ellipsoidal radial basis function networks, enabling a compact and accurate
surface representation. Given the SDF values defined on the grid points
constructed from the point cloud, our method approximates the SDF accurately
with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,
represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and
approximation precision, a dynamic multi-objective optimization strategy is
introduced, which adaptively adds the regularization terms and jointly
optimizes the weights, centers, shapes, and orientations of ERBFs. To improve
computational efficiency, a nearest-neighbor-based data structure is employed,
restricting function calculations to points near each Gaussian kernel center.
The computations for each kernel are further parallelized on CUDA, which
significantly improves the optimization speed. Additionally, a hierarchical
octree-based refinement strategy is designed for training. Specifically, the
initialization and optimization of network parameters are conducted using
coarse grid points in the octree lattice structure. Subsequently, fine lattice
points are progressively incorporated to accelerate model convergence and
enhance training efficiency. Extensive experiments on multiple benchmark
datasets demonstrate that our method outperforms previous sparse representation
approaches in terms of accuracy, robustness, and computational efficiency. The
corresponding code is publicly available at
https://github.com/lianbobo/SE-RBFNet.git.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [326] [Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy](https://arxiv.org/abs/2505.01647)
*Mingfeng Li,Weijie Zheng,Benjamin Doerr*

Main category: cs.NE

TL;DR: 论文提出了一种基于年龄的非精英选择机制，相比之前的随机选择机制，能够在多目标优化问题中更快计算帕累托前沿，尤其在常数 $k$ 时即能提速。


<details>
  <summary>Details</summary>
Motivation: 多目标进化算法通常采用贪婪选择，但研究发现非精英选择可能加速计算。之前提出的随机选择机制存在局限性（如仅在高复杂度的超级多项式运行时有效），本文旨在解决这些不足。

Method: 提出了一种基于年龄的非精英选择机制，豁免年轻个体被移除的可能性，从而优化选择过程。

Result: 新机制在常数 $k$ 时即能提速，且不受目标数量的影响，速度提升因子为 $\max\{1,\Theta(k)^{k-1}\}$。

Conclusion: 基于年龄的非精英选择机制比随机选择更有效，进一步支持了非精英选择在多目标优化中的潜力。

Abstract: Different from single-objective evolutionary algorithms, where non-elitism is
an established concept, multi-objective evolutionary algorithms almost always
select the next population in a greedy fashion. In the only notable exception,
Bian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism
for the SMS-EMOA and proved that it can speed up computing the Pareto front of
the bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by
a factor of $\max\{1,2^{k/4}/n\}$. While this constitutes the first proven
speed-up from non-elitist selection, suggesting a very interesting research
direction, it has to be noted that a true speed-up only occurs for $k \ge
4\log_2(n)$, where the runtime is super-polynomial, and that the advantage
reduces for larger numbers of objectives as shown in a later work. In this
work, we propose a different non-elitist selection mechanism based on aging,
which exempts individuals younger than a certain age from a possible removal.
This remedies the two shortcomings of stochastic selection: We prove a speed-up
by a factor of $\max\{1,\Theta(k)^{k-1}\}$, regardless of the number of
objectives. In particular, a positive speed-up can already be observed for
constant $k$, the only setting for which polynomial runtimes can be witnessed.
Overall, this result supports the use of non-elitist selection schemes, but
suggests that aging-based mechanisms can be considerably more powerful than
stochastic selection mechanisms.

</details>


### [327] [PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation](https://arxiv.org/abs/2505.01730)
*Pranav Ramesh,Gopalakrishnan Srinivasan*

Main category: cs.NE

TL;DR: 论文提出了一种名为PASCAL的方法，通过数学等效于带有QCFS激活的ANN的方式来优化ANN-SNN转换，从而在减少推理时间步数的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的SNN在推理时需要大量时间步才能匹配源ANN的精度，尤其是在处理复杂数据集时效率低下。为了解决这一问题，研究人员提出了一种新的转换方法。

Method: 提出了PASCAL方法，通过数学等效于带有QCFS激活的ANN的方式来进行ANN-SNN转换，并提出了一种分层配置QCFS激活量化步骤的系统方法。

Result: 实验结果表明，使用PASCAL方法转换的ResNet-34 SNN在ImageNet上的准确率达到约74%，并且推理时间步数减少了64倍。

Conclusion: PASCAL方法显著减少了SNN推理所需的时间步数，同时保持了高精度，为SNN的实际应用提供了更高效的解决方案。

Abstract: Spiking Neural Networks (SNNs) have been put forward as an energy-efficient
alternative to Artificial Neural Networks (ANNs) since they perform sparse
Accumulate operations instead of the power-hungry Multiply-and-Accumulate
operations. ANN-SNN conversion is a widely used method to realize deep SNNs
with accuracy comparable to that of ANNs.~\citeauthor{bu2023optimal} recently
proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative
to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless,
SNN inferencing requires a large number of timesteps to match the accuracy of
the source ANN for real-world datasets. In this work, we propose PASCAL, which
performs ANN-SNN conversion in such a way that the resulting SNN is
mathematically equivalent to an ANN with QCFS-activation, thereby yielding
similar accuracy as the source ANN with minimal inference timesteps. In
addition, we propose a systematic method to configure the quantization step of
QCFS activation in a layerwise manner, which effectively determines the optimal
number of timesteps per layer for the converted SNN. Our results show that the
ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\approx$74\% on
ImageNet with a 64$\times$ reduction in the number of inference timesteps
compared to existing approaches.

</details>


### [328] [Meta-Black-Box-Optimization through Offline Q-function Learning](https://arxiv.org/abs/2505.02010)
*Zeyuan Ma,Zhiguang Cao,Zhou Jiang,Hongshu Guo,Yue-Jiao Gong*

Main category: cs.NE

TL;DR: 论文提出了一种基于离线学习的MetaBBO框架Q-Mamba，通过将动态算法配置任务转化为长序列决策过程，并结合Q函数分解机制和Mamba架构，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MetaBBO研究主要采用在线学习范式，导致效率不佳。为了提高效果和效率，本文提出了一种离线学习框架。

Method: 将动态算法配置任务转化为长序列决策过程，引入Q函数分解机制降低学习复杂度，并提出三种设计：平衡探索与利用的数据集构建策略、分解式Q-loss结合保守Q学习、Mamba架构优化长序列学习。

Result: Q-Mamba在性能上优于现有在线/离线基线，并显著提升了训练效率。

Conclusion: Q-Mamba框架有效解决了MetaBBO的效率和性能问题，为动态算法配置提供了新的解决方案。

Abstract: Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated
that using RL to learn a meta-level policy for dynamic algorithm configuration
(DAC) over an optimization task distribution could significantly enhance the
performance of the low-level BBO algorithm. However, the online learning
paradigms in existing works makes the efficiency of MetaBBO problematic. To
address this, we propose an offline learning-based MetaBBO framework in this
paper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO.
Specifically, we first transform DAC task into long-sequence decision process.
This allows us further introduce an effective Q-function decomposition
mechanism to reduce the learning difficulty within the intricate algorithm
configuration space. Under this setting, we propose three novel designs to
meta-learn DAC policy from offline data: we first propose a novel collection
strategy for constructing offline DAC experiences dataset with balanced
exploration and exploitation. We then establish a decomposition-based Q-loss
that incorporates conservative Q-learning to promote stable offline learning
from the offline dataset. To further improve the offline learning efficiency,
we equip our work with a Mamba architecture which helps long-sequence learning
effectiveness and efficiency by selective state model and hardware-aware
parallel scan respectively. Through extensive benchmarking, we observe that
Q-Mamba achieves competitive or even superior performance to prior
online/offline baselines, while significantly improving the training efficiency
of existing online baselines. We provide sourcecodes of Q-Mamba at
https://github.com/MetaEvo/Q-Mamba.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [329] [Watermark Overwriting Attack on StegaStamp algorithm](https://arxiv.org/abs/2505.01474)
*I. F. Serzhenko,L. A. Khaertdinova,M. A. Pautov,A. V. Antsiferova*

Main category: cs.CR

TL;DR: 提出了一种针对StegaStamp水印算法的攻击方法，可在最小质量损失下完全去除图像中的水印。


<details>
  <summary>Details</summary>
Motivation: 旨在测试和挑战现有水印技术的安全性，作为NeurIPS“擦除不可见”比赛的一部分。

Method: 开发了一种攻击方法，能够有效移除StegaStamp水印。

Result: 成功完全去除水印，同时对图像造成的质量损失极小。

Conclusion: 该方法展示了当前水印技术的潜在漏洞，需进一步改进以增强安全性。

Abstract: This paper presents an attack method on the StegaStamp watermarking algorithm
that completely removes watermarks from an image with minimal quality loss,
developed as part of the NeurIPS "Erasing the invisible" competition.

</details>


### [330] [Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration](https://arxiv.org/abs/2505.01514)
*Khushbu Mehboob Shaikh,Georgios Giannakopoulos*

Main category: cs.CR

TL;DR: 论文摘要讨论了AI驱动的交互式语音响应（IVR）技术的快速数字化发展，强调了其个性化、自动化和优化的潜力，但也指出了数据隐私、决策透明度和安全性等风险。作者提出了一个以网络安全为中心的治理框架，结合敏捷安全原则、全球数据法规合规性和用户伦理，旨在实现隐私设计、自适应风险建模和透明度。通过多维度分析，论文主张将伦理AI集成视为战略要务，以使现代IVR技术不仅成为通信工具，更是智能、安全且负责任的数字前线。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术驱动的IVR系统在客户互动中扮演越来越重要的角色，确保这些技术的安全性、合规性和符合伦理的设计变得至关重要。论文旨在分析IVR系统从静态代码设计到AI驱动系统的演变，并提出一个多角度的治理框架，以应对数据隐私、透明度和安全性等挑战。

Method: 论文通过分析IVR技术的演变，提出了一种网络安全为中心的治理框架，该框架结合了敏捷安全原则、全球数据法规合规性和用户伦理设计。具体方法包括隐私设计、自适应风险建模和透明度机制的应用。

Result: 论文结果表明，通过实施所提出的框架，AI驱动的IVR系统可以提高安全性、透明度和用户信任，从而更好地应对新兴威胁并满足社会期望。

Conclusion: 论文得出结论，伦理AI集成不仅是IVR系统的一个功能，更是一项战略要务。通过多维度的框架，现代IVR技术可以发展为智能、安全和负责任的数字前线，从而在快速数字化的通信系统中发挥更大的作用。

Abstract: The rapid digitalization of communication systems has elevated Interactive
Voice Response (IVR) technologies to become critical interfaces for customer
engagement. With Artificial Intelligence (AI) now driving these platforms,
ensuring secure, compliant, and ethically designed development practices is
more imperative than ever. AI-powered IVRs leverage Natural Language Processing
(NLP) and Machine Learning (ML) to personalize interactions, automate service
delivery, and optimize user experiences. However, these innovations expose
systems to heightened risks, including data privacy breaches, AI decision
opacity, and model security vulnerabilities. This paper analyzes the evolution
of IVRs from static code-based designs to adaptive AI-driven systems,
presenting a cybersecurity-centric perspective. We propose a practical
governance framework that embeds agile security principles, compliance with
global data legislation, and user-centric ethics. Emphasizing
privacy-by-design, adaptive risk modeling, and transparency, the paper argues
that ethical AI integration is not a feature but a strategic imperative.
Through this multidimensional lens, we highlight how modern IVRs can transition
from communication tools to intelligent, secure, and accountable digital
frontlines-resilient against emerging threats and aligned with societal
expectations.

</details>


### [331] [The DCR Delusion: Measuring the Privacy Risk of Synthetic Data](https://arxiv.org/abs/2505.01524)
*Zexi Yao,Nataša Krčo,Georgi Ganev,Yves-Alexandre de Montjoye*

Main category: cs.CR

TL;DR: 合成数据常用以保护隐私，但常用的距离度量（如DCR）无法有效检测隐私泄露，容易被成员推理攻击（MIA）突破，因此需采用MIA作为更严格的隐私评估标准。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示当前合成数据隐私评估中使用的简单代理指标（如DCR）的局限性，证明其无法准确反映实际隐私风险，从而推动采用更严格的成员推理攻击（MIA）作为评估标准。

Method: 通过对比多组数据集及多种生成模型（如Baynet、CTGAN和扩散模型），验证距离度量（DCR等）的失效性，并分析二进制隐私测试和连续度量的无效性。

Result: 实验表明，被代理指标判定为隐私的合成数据实际极易受到MIA攻击，且该问题在不同超参数设置和数据选择方法中普遍存在。

Conclusion: 距离度量指标存在设计缺陷，建议彻底弃用此类代理指标，转而使用MIA作为合成数据隐私评估的金标准，尤其是涉及法律匿名性声明时。

Abstract: Synthetic data has become an increasingly popular way to share data without
revealing sensitive information. Though Membership Inference Attacks (MIAs) are
widely considered the gold standard for empirically assessing the privacy of a
synthetic dataset, practitioners and researchers often rely on simpler proxy
metrics such as Distance to Closest Record (DCR). These metrics estimate
privacy by measuring the similarity between the training data and generated
synthetic data. This similarity is also compared against that between the
training data and a disjoint holdout set of real records to construct a binary
privacy test. If the synthetic data is not more similar to the training data
than the holdout set is, it passes the test and is considered private. In this
work we show that, while computationally inexpensive, DCR and other
distance-based metrics fail to identify privacy leakage. Across multiple
datasets and both classical models such as Baynet and CTGAN and more recent
diffusion models, we show that datasets deemed private by proxy metrics are
highly vulnerable to MIAs. We similarly find both the binary privacy test and
the continuous measure based on these metrics to be uninformative of actual
membership inference risk. We further show that these failures are consistent
across different metric hyperparameter settings and record selection methods.
Finally, we argue DCR and other distance-based metrics to be flawed by design
and show a example of a simple leakage they miss in practice. With this work,
we hope to motivate practitioners to move away from proxy metrics to MIAs as
the rigorous, comprehensive standard of evaluating privacy of synthetic data,
in particular to make claims of datasets being legally anonymous.

</details>


### [332] [Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.01454)
*Zhiyong Jin,Runhua Xu,Chao Li,Yizhong Liu,Jianxin Li*

Main category: cs.CR

TL;DR: FLARE 是一种新颖的联邦学习框架，通过稀疏索引掩码检查和模型更新符号相似性分析，解决了稀疏化联邦学习中的投毒攻击问题，并保持通信效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏化联邦学习在提高通信效率的同时，由于稀疏更新容易被攻击者利用以规避检测，增加了安全风险。现有防御机制在稀疏化场景下效果有限，因此需要新的方法来应对这一挑战。

Method: FLARE 方法包括稀疏索引掩码检查和模型更新符号相似性分析，旨在检测并减轻稀疏化联邦学习中的投毒攻击。

Result: 实验表明，FLARE 在多种数据集和攻击场景下显著优于现有防御策略，有效保障稀疏化联邦学习的安全性，同时保持通信效率。

Conclusion: FLARE 为稀疏化联邦学习中的安全性问题提供了有效的解决方案，为未来研究提供了新的方向。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy, yet it faces significant
challenges in communication efficiency and vulnerability to poisoning attacks.
While sparsification techniques mitigate communication overhead by transmitting
only critical model parameters, they inadvertently amplify security risks:
adversarial clients can exploit sparse updates to evade detection and degrade
model performance. Existing defense mechanisms, designed for standard FL
communication scenarios, are ineffective in addressing these vulnerabilities
within sparsified FL. To bridge this gap, we propose FLARE, a novel federated
learning framework that integrates sparse index mask inspection and model
update sign similarity analysis to detect and mitigate poisoning attacks in
sparsified FL. Extensive experiments across multiple datasets and adversarial
scenarios demonstrate that FLARE significantly outperforms existing defense
strategies, effectively securing sparsified FL against poisoning attacks while
maintaining communication efficiency.

</details>


### [333] [Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services](https://arxiv.org/abs/2505.01460)
*Denis Parfenov,Anton Parfenov*

Main category: cs.CR

TL;DR: 本文提出了一种基于自编码器的解决方案，用于保护网络流量分类的机器学习模型免受攻击，以减轻服务器负载。


<details>
  <summary>Details</summary>
Motivation: 随着远程服务器处理任务增多，识别和分类流量成为减轻服务器负载的重要任务。然而，现有的机器学习分类模型易受攻击，影响分类结果。

Method: 采用了基于自编码器的方法来增强机器学习模型的抗攻击能力。

Result: 所提出的自编码器解决方案有效提高了模型的安全性，使其在攻击下仍能保持准确的流量分类。

Conclusion: 自编码器为保护流量分类的机器学习模型提供了一种可行且有效的方法，有助于提升服务器性能和安全性。

Abstract: Due to the increasing number of tasks that are solved on remote servers,
identifying and classifying traffic is an important task to reduce the load on
the server. There are various methods for classifying traffic. This paper
discusses machine learning models for solving this problem. However, such ML
models are also subject to attacks that affect the classification result of
network traffic. To protect models, we proposed a solution based on an
autoencoder

</details>


### [334] [Enhancing the Cloud Security through Topic Modelling](https://arxiv.org/abs/2505.01463)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.CR

TL;DR: 该研究利用NLP方法（如LDA和pLSA）分析云安全数据，通过主题建模预测未来攻击，旨在提升CI/CD管道的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着网络安全威胁日益严重，研究旨在通过NLP技术分析云安全数据，提前预测潜在攻击，优化CI/CD管道的安全防护。

Method: 采用主题建模方法（LDA和pLSA），通过Python的Gensim框架分析安全相关文本数据（如报告、日志），归类为不同主题（如钓鱼、加密），检测CI/CD管道中的漏洞。

Result: 主题建模技术能够有效识别安全漏洞，为CI/CD管道提供新的漏洞检测方法。

Conclusion: 该研究表明，NLP驱动的主题建模可显著提升云安全防护能力，尤其是在CI/CD管道中的漏洞检测方面。

Abstract: Protecting cloud applications is crucial in an age where security constantly
threatens the digital world. The inevitable cyber-attacks throughout the CI/CD
pipeline make cloud security innovations necessary. This research is motivated
by applying Natural Language Processing (NLP) methodologies, such as Topic
Modelling, to analyse cloud security data and predict future attacks. This
research aims to use topic modelling, specifically Latent Dirichlet Allocation
(LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and
PLSA, security-related text data, such as reports, logs, and other relevant
documents, will be analysed and sorted into relevant topics (such as phishing
or encryption). These algorithms may apply through Python using the Gensim
framework. The topics shall be utilised to detect vulnerabilities within
relevant CI/CD pipeline records or log data. This application of Topic
Modelling anticipates providing a new form of vulnerability detection,
improving overall security throughout the CI/CD pipeline.

</details>


### [335] [LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps](https://arxiv.org/abs/2505.01484)
*Pedro Abdalla,Roman Vershynin*

Main category: cs.CR

TL;DR: 文章提出两种水印方案：封闭环境下不可检测的简单水印和开放环境下不可移除的水印，用于区分人类与大型语言模型的生成文本。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效区分人类与大型语言模型生成的文本，尤其在开放环境下对抗性更强的情况下。

Method: 提出两种水印方案：封闭环境下的不可检测水印和开放环境下的不可移除水印。

Result: 方案在封闭和开放环境下分别实现了水印的不可检测性和不可移除性。

Conclusion: 通过水印技术，能够有效区分人类与大型语言模型生成的文本，尤其在对抗性环境下展现出鲁棒性。

Abstract: Given a text, can we determine whether it was generated by a large language
model (LLM) or by a human? A widely studied approach to this problem is
watermarking. We propose an undetectable and elementary watermarking scheme in
the closed setting. Also, in the harder open setting, where the adversary has
access to most of the model, we propose an unremovable watermarking scheme.

</details>


### [336] [Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents](https://arxiv.org/abs/2505.02077)
*Christian Schroeder de Witt*

Main category: cs.CR

TL;DR: 该论文讨论了去中心化AI代理在互联网平台交互时带来的新型安全挑战，提出了多代理安全这一新领域，并初步分类了威胁、研究了安全与性能的权衡，提出了统一研究议程。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化AI代理在互联网上的交互增加，传统安全框架无法应对其带来的新型威胁（如秘密合谋、群体攻击等），亟需系统性研究以确保安全与性能的平衡。

Method: 作者通过分类威胁、调研安全与性能的权衡关系，并提出统一研究议程，初步构建了多代理安全领域的框架。

Result: 论文初步提出了多代理安全领域，并明确了威胁分类、安全性能权衡研究的重要性，为后续研究提供了方向。

Conclusion: 该研究旨在填补去中心化AI代理安全研究的空白，促进其大规模部署的安全性和公众信任，同时降低国家安全风险。

Abstract: Decentralized AI agents will soon interact across internet platforms,
creating security challenges beyond traditional cybersecurity and AI safety
frameworks. Free-form protocols are essential for AI's task generalization but
enable new threats like secret collusion and coordinated swarm attacks. Network
effects can rapidly spread privacy breaches, disinformation, jailbreaks, and
data poisoning, while multi-agent dispersion and stealth optimization help
adversaries evade oversightcreating novel persistent threats at a systemic
level. Despite their critical importance, these security challenges remain
understudied, with research fragmented across disparate fields including AI
security, multi-agent learning, complex systems, cybersecurity, game theory,
distributed systems, and technical AI governance. We introduce
\textbf{multi-agent security}, a new field dedicated to securing networks of
decentralized AI agents against threats that emerge or amplify through their
interactionswhether direct or indirect via shared environmentswith each other,
humans, and institutions, and characterize fundamental security-performance
trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising
from interacting AI agents, (2) surveys security-performance tradeoffs in
decentralized AI systems, and (3) proposes a unified research agenda addressing
open challenges in designing secure agent systems and interaction environments.
By identifying these gaps, we aim to guide research in this critical area to
unlock the socioeconomic potential of large-scale agent deployment on the
internet, foster public trust, and mitigate national security risks in critical
infrastructure and defense contexts.

</details>


### [337] [Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp](https://arxiv.org/abs/2505.01816)
*Eran Aizikovich,Dudu Mimran,Edita Grolman,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 该论文探讨了O-RAN从单运营商转向多运营商架构时引入的安全挑战，提出了一种新的检测框架MARRS，并展示了攻击APATE能非法获取更多UE分配。


<details>
  <summary>Details</summary>
Motivation: 研究动机是填补O-RAN在从单运营商转向多运营商架构时安全漏洞的研究空白，并探索如何检测和缓解这些漏洞。

Method: 方法包括开发一个开放访问的测试环境，结合无线网络模拟器和O-RAN软件社区的RIC集群，以及提出检测框架MARRS。

Result: 结果显示，攻击APATE能非法获取248.5%的UE分配，而检测框架MARRS达到了99.2%的准确率和0.978的F1分数。

Conclusion: 结论强调了多运营商架构下O-RAN的安全风险，以及MARRS框架在检测恶意活动中的有效性。

Abstract: The Open Radio Access Network (O-RAN) architecture is revolutionizing
cellular networks with its open, multi-vendor design and AI-driven management,
aiming to enhance flexibility and reduce costs. Although it has many
advantages, O-RAN is not threat-free. While previous studies have mainly
examined vulnerabilities arising from O-RAN's intelligent components, this
paper is the first to focus on the security challenges and vulnerabilities
introduced by transitioning from single-operator to multi-operator RAN
architectures. This shift increases the risk of untrusted third-party operators
managing different parts of the network. To explore these vulnerabilities and
their potential mitigation, we developed an open-access testbed environment
that integrates a wireless network simulator with the official O-RAN Software
Community (OSC) RAN intelligent component (RIC) cluster. This environment
enables realistic, live data collection and serves as a platform for
demonstrating APATE (adversarial perturbation against traffic efficiency), an
evasion attack in which a malicious cell manipulates its reported key
performance indicators (KPIs) and deceives the O-RAN traffic steering to gain
unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate
activity continues, we introduce MARRS (monitoring adversarial RAN reports), a
detection framework based on a long-short term memory (LSTM) autoencoder (AE)
that learns contextual features across the network to monitor malicious
telemetry (also demonstrated in our testbed). Our evaluation showed that by
executing APATE, an attacker can obtain a 248.5% greater UE allocation than it
was supposed to in a benign scenario. In addition, the MARRS detection method
was also shown to successfully classify malicious cell activity, achieving
accuracy of 99.2% and an F1 score of 0.978.

</details>


### [338] [PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework](https://arxiv.org/abs/2505.01866)
*Daniel Commey,Garth V. Crosby*

Main category: cs.CR

TL;DR: 这篇论文提出了PQS-BFL框架，结合后量子密码学和区块链技术，以保护联邦学习免受量子攻击，同时保持高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习的加密方法易受量子攻击，尤其在敏感领域（如医疗）存在重大风险。因此，需要一种新的安全框架来应对量子计算威胁。

Method: 采用后量子密码学（如ML-DSA-65签名）和区块链验证，通过优化的智能合约实现去中心化验证。

Result: 实验表明，PQS-BFL在多个数据集上表现优异，加密操作高效（签名时间0.65ms，验证时间0.53ms），区块链开销可控（交易时间约4.8s），模型准确率保持高水平（如MNIST达98.8%）。

Conclusion: PQS-BFL证明了后量子安全在联邦学习中的可行性，且在性能和安全性上均表现出色，适合实际部署。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy, but its classical cryptographic underpinnings are vulnerable to
quantum attacks. This vulnerability is particularly critical in sensitive
domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure
Blockchain-based Federated Learning), a framework integrating post-quantum
cryptography (PQC) with blockchain verification to secure FL against quantum
adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly
Dilithium) signatures to authenticate model updates and leverage optimized
smart contracts for decentralized validation. Extensive evaluations on diverse
datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient
cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms)
with a fixed signature size of 3309 Bytes. Blockchain integration incurs a
manageable overhead, with average transaction times around 4.8 s and gas usage
per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the
cryptographic overhead relative to transaction time remains minimal (around
0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the
bottleneck in blockchain-based FL. The system maintains competitive model
accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with
round times showing sublinear growth with increasing client numbers. Our
open-source implementation and reproducible benchmarks validate the feasibility
of deploying long-term, quantum-resistant security in practical FL systems.

</details>


### [339] [Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models](https://arxiv.org/abs/2505.02362)
*Ghazaleh SHirvani,Saeid Ghasemshirazi*

Main category: cs.CR

TL;DR: 论文提出了一种结合Zero-Shot学习和NLP技术（如BERT和FLAN-T5）的新方法，用于高效检测电子邮件垃圾，克服传统方法对大量标注数据和频繁再训练的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾邮件检测方法在适应变化、处理类别不平衡和数据稀缺方面存在局限性，因此需要创新方法以减少对标注数据和再训练的依赖。

Method: 采用BERT预处理邮件内容并提取关键信息，结合FLAN-T5在Zero-Shot框架下进行分类，实现无需大量标注数据或频繁再训练的垃圾邮件检测。

Result: 该方法展现了在不依赖标注数据或频繁再训练的情况下，对未知垃圾邮件模式和对抗环境的强适应性。

Conclusion: 研究证明了Zero-Shot学习和NLP技术在动态且具有挑战性的垃圾邮件检测任务中的潜力，提供了一种可扩展且高效的解决方案。

Abstract: Email spam detection is a critical task in modern communication systems,
essential for maintaining productivity, security, and user experience.
Traditional machine learning and deep learning approaches, while effective in
static settings, face significant limitations in adapting to evolving spam
tactics, addressing class imbalance, and managing data scarcity. These
challenges necessitate innovative approaches that reduce dependency on
extensive labeled datasets and frequent retraining. This study investigates the
effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced
Natural Language Processing (NLP) techniques such as BERT for email spam
detection. By employing BERT to preprocess and extract critical information
from email content, and FLAN-T5 to classify emails in a Zero-Shot framework,
the proposed approach aims to address the limitations of traditional spam
detection systems. The integration of FLAN-T5 and BERT enables robust spam
detection without relying on extensive labeled datasets or frequent retraining,
making it highly adaptable to unseen spam patterns and adversarial
environments. This research highlights the potential of leveraging zero-shot
learning and NLPs for scalable and efficient spam detection, providing insights
into their capability to address the dynamic and challenging nature of spam
detection tasks.

</details>


### [340] [Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees](https://arxiv.org/abs/2505.02224)
*Andrew Quijano,Spyros T. Halkidis,Kevin Gallagher,Kemal Akkaya,Nikolaos Samaras*

Main category: cs.CR

TL;DR: 本文提出了一种新的决策树推理协议，通过将模型分层存储在不同的实体中（称为“level-site”），提高了非完全树的分类器评估效率，并增强了侧信道攻击的防御。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和云计算的需求，需要一个既能保护用户输入隐私，又能防止第三方窃取决策树模型的分类器。

Method: 协议将决策树模型按层级划分为多个部分，分别存储在新的“level-site”实体中，从而优化评估效率并增强安全性。

Result: 该方法在非完全树的分类器评估中表现出更优的平均运行时间，并且有效缓解了侧信道攻击的风险。

Conclusion: 提出的分层存储和评估协议在效率和安全性方面均有显著改进，为隐私保护和云计算环境下的决策树应用提供了可行方案。

Abstract: A decision tree is an easy-to-understand tool that has been widely used for
classification tasks. On the one hand, due to privacy concerns, there has been
an urgent need to create privacy-preserving classifiers that conceal the user's
input from the classifier. On the other hand, with the rise of cloud computing,
data owners are keen to reduce risk by outsourcing their model, but want
security guarantees that third parties cannot steal their decision tree model.
To address these issues, Joye and Salehi introduced a theoretical protocol that
efficiently evaluates decision trees while maintaining privacy by leveraging
their comparison protocol that is resistant to timing attacks. However, their
approach was not only inefficient but also prone to side-channel attacks.
Therefore, in this paper, we propose a new decision tree inference protocol in
which the model is shared and evaluated among multiple entities. We partition
our decision tree model by each level to be stored in a new entity we refer to
as a "level-site." Utilizing this approach, we were able to gain improved
average run time for classifier evaluation for a non-complete tree, while also
having strong mitigations against side-channel attacks.

</details>


### [341] [Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study](https://arxiv.org/abs/2505.02502)
*Xinyi Hou,Jiahao Han,Yanjie Zhao,Haoyu Wang*

Main category: cs.CR

TL;DR: 该研究通过大规模实证调查揭示了公共大型语言模型（LLM）部署的安全问题，发现多数服务因不安全协议、配置不当和无认证访问而存在高风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示公共LLM部署的安全现状，因不安全默认设置和错误配置可能导致重大安全风险。

Method: 采用互联网测量方法，识别了320,102个公共LLM服务，分析了158个API端点，并研究其配置、认证和地理分布。

Result: 研究发现公共LLM部署虽增长迅速，但普遍存在不安全协议、TLS配置差、无认证访问等问题，导致模型泄露和未经授权访问等风险。

Conclusion: 结论指出公共LLM部署亟需加强默认安全性和部署实践，以应对模型窃取、资源劫持等威胁。

Abstract: Background: Large language models (LLMs) are increasingly deployed via
open-source and commercial frameworks, enabling individuals and organizations
to self-host advanced AI capabilities. However, insecure defaults and
misconfigurations often expose LLM services to the public Internet, posing
significant security and system engineering risks. Aims: This study aims to
unveil the current landscape of public-facing LLM deployments in the wild
through a large-scale empirical study, focusing on service prevalence, exposure
characteristics, systemic vulnerabilities, and associated risks. Method: We
conducted an Internet-wide measurement to identify public-facing LLM
deployments across 15 frameworks, discovering 320,102 services. We extracted
158 unique API endpoints, grouped into 12 functional categories based on
capabilities and security risks. We further analyzed configurations,
authentication practices, and geographic distributions, revealing deployment
trends and systemic issues in real-world LLM system engineering. Results: Our
study shows that public LLM deployments are rapidly growing but often insecure.
Among all endpoints, we observe widespread use of insecure protocols, poor TLS
configurations, and unauthenticated access to critical operations. Security
risks, including model disclosure, system leakage, and unauthorized access, are
pervasive, highlighting the need for secure-by-default frameworks and stronger
deployment practices. Conclusions: Public-facing LLM deployments suffer from
widespread security and configuration flaws, exposing services to misuse, model
theft, resource hijacking, and remote exploitation. Strengthening default
security, deployment practices, and operational standards is critical for the
growing self-hosted LLM ecosystem.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [342] [Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking](https://arxiv.org/abs/2505.01696)
*Alireza Sadeghi,Farshid Hajati,Ahmadreza Argha,Nigel H Lovell,Min Yang,Hamid Alinejad-Rokny*

Main category: q-bio.GN

TL;DR: 该论文综述了2019年至2024年26项可解释图模型在生物医学多模态数据中的应用，主要包括疾病分类（如癌症），并比较了四种解释方法的优缺点。基于阿尔茨海默病队列的基准测试显示，SHAP和敏感性分析能更全面地发现已知通路，而梯度显著性和图掩码则揭示了补充的代谢和运输特征。论文还提供了流程图帮助研究人员选择方法。


<details>
  <summary>Details</summary>
Motivation: 整合异构生物医学数据（如影像、组学和临床记录）需要可解释的图模型以支持临床决策和监管要求。

Method: 综述了26项研究，分类为四种可解释性方法（SHAP、梯度显著性、敏感性分析和图掩码），并在阿尔茨海默病数据集上进行了基准测试。

Result: SHAP和敏感性分析能更全面地识别已知通路，梯度显著性和图掩码则提供了补充信息。不同方法在计算成本和精度上有明显权衡。

Conclusion: 论文总结了可解释图学习的现状，提供了方法选择指南，并指出了未来研究方向（如高级可解释工具和未充分研究的疾病）。

Abstract: Integrating heterogeneous biomedical data including imaging, omics, and
clinical records supports accurate diagnosis and personalised care. Graph-based
models fuse such non-Euclidean data by capturing spatial and relational
structure, yet clinical uptake requires regulator-ready interpretability. We
present the first technical survey of interpretable graph based models for
multimodal biomedical data, covering 26 studies published between Jan 2019 and
Sep 2024. Most target disease classification, notably cancer and rely on static
graphs from simple similarity measures, while graph-native explainers are rare;
post-hoc methods adapted from non-graph domains such as gradient saliency, and
SHAP predominate. We group existing approaches into four interpretability
families, outline trends such as graph-in-graph hierarchies, knowledge-graph
edges, and dynamic topology learning, and perform a practical benchmark. Using
an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient
Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the
broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient
Saliency and Graph Masking surface complementary metabolic and transport
signatures. Permutation tests show all four beat random gene sets, but with
distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher
compute cost, while Gradient Saliency and Sensitivity Analysis are quicker
though coarser. We also provide a step-by-step flowchart covering graph
construction, explainer choice and resource budgeting to help researchers
balance transparency and performance. This review synthesises the state of
interpretable graph learning for multimodal medicine, benchmarks leading
techniques, and charts future directions, from advanced XAI tools to
under-studied diseases, serving as a concise reference for method developers
and translational scientists.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [343] [Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity](https://arxiv.org/abs/2505.01455)
*Gan Zhang,Megha Rao,Janni Yuval,Ming Zhao*

Main category: physics.ao-ph

TL;DR: NeuralGCM，一种混合机器学习与物理的大气模型，展示了在季节性预测大规模大气变化和北半球热带气旋活动方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习模型在气候预测中的应用，尤其关注热带气旋活动的季节性预测。

Method: 使用NeuralGCM模型，简化边界条件，假设海面温度和海冰遵循气候学循环但保持初始化时的异常。

Result: 模型能模拟真实的大气环流和热带气旋气候模式，预测技能在北大西洋和东太平洋盆地的热带气旋频率上与传统物理模型相当。

Conclusion: 结合物理洞察的机器学习模型在热带气旋风险建模和无缝天气-气候预测方面具有前景。

Abstract: Machine learning (ML) models are successful with weather forecasting and have
shown progress in climate simulations, yet leveraging them for useful climate
predictions needs exploration. Here we show this feasibility using NeuralGCM, a
hybrid ML-physics atmospheric model, for seasonal predictions of large-scale
atmospheric variability and Northern Hemisphere tropical cyclone (TC) activity.
Inspired by physical model studies, we simplify boundary conditions, assuming
sea surface temperature (SST) and sea ice follow their climatological cycle but
persist anomalies present at initialization. With such forcings, NeuralGCM
simulates realistic atmospheric circulation and TC climatology patterns.
Furthermore, this configuration yields useful seasonal predictions
(July-November) for the tropical atmosphere and various TC activity metrics.
Notably, the prediction skill for TC frequency in the North Atlantic and East
Pacific basins is comparable to existing physical models. These findings
highlight the promise of leveraging ML models with physical insights to model
TC risks and deliver seamless weather-climate predictions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [344] [Fast Likelihood-Free Parameter Estimation for Lévy Processes](https://arxiv.org/abs/2505.01639)
*Nicolas Coloma,William Kleiber*

Main category: stat.ML

TL;DR: 论文提出了一种基于神经贝叶斯估计（NBE）的快速、准确方法，用于估计Lévy过程的参数，解决了传统方法在高频金融数据中计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: Lévy过程在金融建模中广泛应用，但其参数估计在似然函数不可得或计算成本高时面临挑战。传统方法效率低下，需更高效的解决方案。

Method: 采用神经贝叶斯估计（NBE）框架，利用置换不变神经网络近似贝叶斯估计器，无需显式似然函数，通过模拟数据进行训练。

Result: NBE在多种Lévy模型中的仿真实验显示，其精度和运行时间均优于传统方法，并能快速完成基于Bootstrap的不确定性量化。在加密货币高频数据中表现优异。

Conclusion: NBE为复杂金融模型提供了一种可扩展的实用解决方案，显著提升了参数估计和不确定性量化的效率，适用于大规模高频数据分析。

Abstract: L\'evy processes are widely used in financial modeling due to their ability
to capture discontinuities and heavy tails, which are common in high-frequency
asset return data. However, parameter estimation remains a challenge when
associated likelihoods are unavailable or costly to compute. We propose a fast
and accurate method for L\'evy parameter estimation using the neural Bayes
estimation (NBE) framework -- a simulation-based, likelihood-free approach that
leverages permutation-invariant neural networks to approximate Bayes
estimators. Through extensive simulations across several L\'evy models, we show
that NBE outperforms traditional methods in both accuracy and runtime, while
also enabling rapid bootstrap-based uncertainty quantification. We illustrate
our approach on a challenging high-frequency cryptocurrency return dataset,
where the method captures evolving parameter dynamics and delivers reliable and
interpretable inference at a fraction of the computational cost of traditional
methods. NBE provides a scalable and practical solution for inference in
complex financial models, enabling parameter estimation and uncertainty
quantification over an entire year of data in just seconds. We additionally
investigate nearly a decade of high-frequency Bitcoin returns, requiring less
than one minute to estimate parameters under the proposed approach.

</details>


### [345] [TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis](https://arxiv.org/abs/2505.01785)
*Ayoub Abraich*

Main category: stat.ML

TL;DR: 论文提出了TV-SurvCaus框架，将表示平衡技术扩展到动态治疗方案中，用于生存分析的因果效应估计，并通过理论和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 医学等领域的时间变化治疗方案对生存结果的因果效应估计是一个复杂问题，现有方法多针对静态治疗方案，动态方案的研究较少。本文旨在填补这一空白。

Method: TV-SurvCaus框架结合了表示平衡技术和神经网络架构，处理时间依赖性并通过序列建模平衡时变表示。

Result: 实验表明，TV-SurvCaus在合成和真实数据集上优于现有方法，能更准确估计动态治疗方案下的个体化治疗效果。

Conclusion: 该框架通过理论和实验验证，推动了时间动态设置下生存结果的因果效应估计，具有重要应用价值。

Abstract: Estimating the causal effect of time-varying treatments on survival outcomes
is a challenging task in many domains, particularly in medicine where treatment
protocols adapt over time. While recent advances in representation learning
have improved causal inference for static treatments, extending these methods
to dynamic treatment regimes with survival outcomes remains under-explored. In
this paper, we introduce TV-SurvCaus, a novel framework that extends
representation balancing techniques to the time-varying treatment setting for
survival analysis. We provide theoretical guarantees through (1) a generalized
bound for time-varying precision in estimation of heterogeneous effects, (2)
variance control via sequential balancing weights, (3) consistency results for
dynamic treatment regimes, (4) convergence rates for representation learning
with temporal dependencies, and (5) a formal bound on the bias due to
treatment-confounder feedback. Our neural architecture incorporates sequence
modeling to handle temporal dependencies while balancing time-dependent
representations. Through extensive experiments on both synthetic and real-world
datasets, we demonstrate that TV-SurvCaus outperforms existing methods in
estimating individualized treatment effects with time-varying covariates and
treatments. Our framework advances the field of causal inference by enabling
more accurate estimation of treatment effects in dynamic, longitudinal settings
with survival outcomes.

</details>


### [346] [Bayesian learning of the optimal action-value function in a Markov decision process](https://arxiv.org/abs/2505.01859)
*Jiaqi Guo,Chon Wai Ho,Sumeetpal S. Singh*

Main category: stat.ML

TL;DR: 该论文针对无限时间且无折扣的MDP，提出了一个从建模到推理再到决策的全贝叶斯框架，并通过实验展示了后验采样在探索中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯方法在学习最优决策策略时基于不现实的建模假设，并依赖近似推断技术，这可能导致贝叶斯不确定性量化的优势未充分发挥或不可靠。

Method: 论文引入了一个基于贝尔曼最优性方程的最小假设似然函数，为确定性奖励引入人工观测噪声以放松退化问题，并提出了一种自适应的序贯蒙特卡洛算法进行推断。

Result: 在Deep Sea基准问题上，论文展示了后验采样在MDP中的探索优势。

Conclusion: 论文提出了一种新的贝叶斯框架，通过更高效的推断方法实现了不确定性量化，为MDP中的决策提供了新的见解。

Abstract: The Markov Decision Process (MDP) is a popular framework for sequential
decision-making problems, and uncertainty quantification is an essential
component of it to learn optimal decision-making strategies. In particular, a
Bayesian framework is used to maintain beliefs about the optimal decisions and
the unknown ingredients of the model, which are also to be learned from the
data, such as the rewards and state dynamics. However, many existing Bayesian
approaches for learning the optimal decision-making strategy are based on
unrealistic modelling assumptions and utilise approximate inference techniques.
This raises doubts whether the benefits of Bayesian uncertainty quantification
are fully realised or can be relied upon.
  We focus on infinite-horizon and undiscounted MDPs, with finite state and
action spaces, and a terminal state. We provide a full Bayesian framework, from
modelling to inference to decision-making. For modelling, we introduce a
likelihood function with minimal assumptions for learning the optimal
action-value function based on Bellman's optimality equations, analyse its
properties, and clarify connections to existing works. For deterministic
rewards, the likelihood is degenerate and we introduce artificial observation
noise to relax it, in a controlled manner, to facilitate more efficient Monte
Carlo-based inference. For inference, we propose an adaptive sequential Monte
Carlo algorithm to both sample from and adjust the sequence of relaxed
posterior distributions. For decision-making, we choose actions using samples
from the posterior distribution over the optimal strategies. While commonly
done, we provide new insight that clearly shows that it is a generalisation of
Thompson sampling from multi-arm bandit problems. Finally, we evaluate our
framework on the Deep Sea benchmark problem and demonstrate the exploration
benefits of posterior sampling in MDPs.

</details>


### [347] [Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks](https://arxiv.org/abs/2505.01995)
*Sehwan Kim,Faming Liang*

Main category: stat.ML

TL;DR: 论文提出了Double-NN方法，结合深度神经网络和EFI框架来估计个体治疗效果，表现优于CQR方法，并在理论和不确定性量化方面取得重要进展。


<details>
  <summary>Details</summary>
Motivation: 个体治疗效果估计在数据科学中日益重要，现有方法如CQR存在不足，需结合神经网络和统计推断理论进行改进。

Method: 使用两个深度神经网络分别建模处理和对照组效应，第三个网络估计参数，依托EFI框架保障适用性。

Result: Double-NN表现优于CQR，理论上模型规模可随样本量以$O(n^{\zeta})$增长（$0\leq\zeta<1$），突破了经典中心极限定理的$\zeta<\frac{1}{2}$限制。

Conclusion: 该方法为大规模神经网络的不确定性量化提供了严格框架，推动了统计推断理论与深度学习结合的发展。

Abstract: Individual treatment effect estimation has gained significant attention in
recent data science literature. This work introduces the Double Neural Network
(Double-NN) method to address this problem within the framework of extended
fiducial inference (EFI). In the proposed method, deep neural networks are used
to model the treatment and control effect functions, while an additional neural
network is employed to estimate their parameters. The universal approximation
capability of deep neural networks ensures the broad applicability of this
method. Numerical results highlight the superior performance of the proposed
Double-NN method compared to the conformal quantile regression (CQR) method in
individual treatment effect estimation. From the perspective of statistical
inference, this work advances the theory and methodology for statistical
inference of large models. Specifically, it is theoretically proven that the
proposed method permits the model size to increase with the sample size $n$ at
a rate of $O(n^{\zeta})$ for some $0 \leq \zeta<1$, while still maintaining
proper quantification of uncertainty in the model parameters. This result marks
a significant improvement compared to the range $0\leq \zeta < \frac{1}{2}$
required by the classical central limit theorem. Furthermore, this work
provides a rigorous framework for quantifying the uncertainty of deep neural
networks under the neural scaling law, representing a substantial contribution
to the statistical understanding of large-scale neural network models.

</details>


### [348] [Learning the Simplest Neural ODE](https://arxiv.org/abs/2505.02019)
*Yuji Okamoto,Tomoya Takeuchi,Yusuke Sakemi*

Main category: stat.ML

TL;DR: 该论文分析了训练神经ODE的困难，并提出了一种新的稳定方法，同时提供了收敛性分析，旨在为研究人员提供简明教程。


<details>
  <summary>Details</summary>
Motivation: 神经ODE在系统识别、时间序列预测和生成建模等领域有广泛应用，但训练过程仍具挑战性。本研究旨在揭示训练困难的原因，并提出改进方法。

Method: 以最简单的一维线性模型为例，分析了神经ODE的训练难点，并提出了一种新的稳定化方法。

Result: 通过收敛性分析验证了所提稳定方法的有效性。

Conclusion: 该研究为神经ODE的研究者提供了实用的见解和技术支持。

Abstract: Since the advent of the ``Neural Ordinary Differential Equation (Neural
ODE)'' paper, learning ODEs with deep learning has been applied to system
identification, time-series forecasting, and related areas. Exploiting the
diffeomorphic nature of ODE solution maps, neural ODEs has also enabled their
use in generative modeling. Despite the rich potential to incorporate various
kinds of physical information, training Neural ODEs remains challenging in
practice. This study demonstrates, through the simplest one-dimensional linear
model, why training Neural ODEs is difficult. We then propose a new
stabilization method and provide an analytical convergence analysis. The
insights and techniques presented here serve as a concise tutorial for
researchers beginning work on Neural ODEs.

</details>


### [349] [Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering](https://arxiv.org/abs/2505.02173)
*Chutiphan Charoensuk,Nathakhun Wiroonsri*

Main category: stat.ML

TL;DR: 本文提出了一种新的时间序列聚类方法RDPC，结合了排名皮尔逊相关不相似度与层次聚类，并在复杂情况下（如不同季节性、趋势和峰值）优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列聚类在多个领域有广泛应用，但现有方法通常仅关注欧氏距离或关联不相似度，限制了在复杂模式下的表现。

Method: 提出RDPC不相似度度量，结合最大元素差异的加权平均值与皮尔逊相关不相似度，并融入层次聚类。

Result: RDPC在复杂情况下表现优于现有算法，并通过泰国电力消费数据集验证，成功将用户分为7个独特群体。

Conclusion: RDPC是一种有效的时间序列聚类方法，尤其适用于复杂模式的数据集。

Abstract: Time series clustering is an unsupervised learning method for classifying
time series data into groups with similar behavior. It is used in applications
such as healthcare, finance, economics, energy, and climate science. Several
time series clustering methods have been introduced and used for over four
decades. Most of them focus on measuring either Euclidean distances or
association dissimilarities between time series. In this work, we propose a new
dissimilarity measure called ranked Pearson correlation dissimilarity (RDPC),
which combines a weighted average of a specified fraction of the largest
element-wise differences with the well-known Pearson correlation dissimilarity.
It is incorporated into hierarchical clustering. The performance is evaluated
and compared with existing clustering algorithms. The results show that the
RDPC algorithm outperforms others in complicated cases involving different
seasonal patterns, trends, and peaks. Finally, we demonstrate our method by
clustering a random sample of customers from a Thai electricity consumption
time series dataset into seven groups with unique characteristics.

</details>


### [350] [Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces](https://arxiv.org/abs/2505.02508)
*Yang Lyu,Yuchun Qian,Tan Minh Nguyen,Xin T. Tong*

Main category: stat.ML

TL;DR: 论文提出了一种简单的惯性更新步骤，解决了经验扩散模型中的记忆效应问题，无需额外训练即可生成新数据样本。


<details>
  <summary>Details</summary>
Motivation: 现有经验扩散模型在生成数据时存在记忆效应，即只能生成训练数据中的样本，本文旨在通过简单方法解决这一问题。

Method: 在经验扩散模型模拟的末端添加惯性更新步骤，仅需使用原始的扩散模型评分函数，无需额外训练。

Result: 惯性扩散模型的样本分布能以$O\left(n^{-\frac{2}{d+4}}\right)$ Wasserstein-1距离近似数据分布，证明了其能生成新样本。

Conclusion: 惯性扩散模型不仅解决了记忆问题，还与流形学习存在有趣的理论联系，展现了方法的简洁性和有效性。

Abstract: Diffusion models is a popular computational tool to generate new data
samples. It utilizes a forward diffusion process that add noise to the data
distribution and then use a reverse process to remove noises to produce samples
from the data distribution. However, when the empirical data distribution
consists of $n$ data point, using the empirical diffusion model will
necessarily produce one of the existing data points. This is often referred to
as the memorization effect, which is usually resolved by sophisticated machine
learning procedures in the current literature. This work shows that the
memorization problem can be resolved by a simple inertia update step at the end
of the empirical diffusion model simulation. Our inertial diffusion model
requires only the empirical diffusion model score function and it does not
require any further training. We show that choosing the inertia diffusion model
sample distribution is an $O\left(n^{-\frac{2}{d+4}}\right)$ Wasserstein-1
approximation of a data distribution lying on a $C^2$ manifold of dimension
$d$. Since this estimate is significant smaller the Wasserstein1 distance
between population and empirical distributions, it rigorously shows the
inertial diffusion model produces new data samples. Remarkably, this upper
bound is completely free of the ambient space dimension, since there is no
training involved. Our analysis utilizes the fact that the inertial diffusion
model samples are approximately distributed as the Gaussian kernel density
estimator on the manifold. This reveals an interesting connection between
diffusion model and manifold learning.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [351] [Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach](https://arxiv.org/abs/2505.01780)
*Guangjin Pan,Zhixing Li,Ayça Özçelikkale,Christian Häger,Musa Furkan Keskin,Henk Wymeersch*

Main category: eess.SP

TL;DR: 本文提出了一种基于自动编码器的观测压缩方法，用于解决分布式多传感器感通一体化系统中高维观测数据在速率受限网络中的传输问题，并通过案例研究表明了资源分配的最优策略。


<details>
  <summary>Details</summary>
Motivation: 在分布式多传感器感通一体化系统中，高维观测数据的传输受限于网络速率，影响了系统性能。本文旨在通过压缩方法解决这一问题。

Method: 提出了一种基于自动编码器的观测压缩方法，并通过闭环LQR系统案例分析了观测、压缩和状态维度之间的相互作用。

Result: 实验结果显示，在多传感器场景中，资源分配会优先分配给低噪声传感器，直到压缩无失真后再分配给高噪声传感器。

Conclusion: 本文提出的压缩方法有效提升了分布式ISAC系统的性能，并揭示了最优资源分配的策略。

Abstract: In closed-loop distributed multi-sensor integrated sensing and communication
(ISAC) systems, performance often hinges on transmitting high-dimensional
sensor observations over rate-limited networks. In this paper, we first present
a general framework for rate-limited closed-loop distributed ISAC systems, and
then propose an autoencoder-based observation compression method to overcome
the constraints imposed by limited transmission capacity. Building on this
framework, we conduct a case study using a closed-loop linear quadratic
regulator (LQR) system to analyze how the interplay among observation,
compression, and state dimensions affects reconstruction accuracy, state
estimation error, and control performance. In multi-sensor scenarios, our
results further show that optimal resource allocation initially prioritizes
low-noise sensors until the compression becomes lossless, after which resources
are reallocated to high-noise sensors.

</details>
