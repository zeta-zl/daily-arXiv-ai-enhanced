<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.NI](#cs.NI) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 33]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SC](#cs.SC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CY](#cs.CY) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 5]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Key words: 大语言模型、文学评价、审美偏好、主成分分析、聚类分析

TL;DR: 该研究将大语言模型（LLMs）作为“主观文学评论家”，探索其在文学评估中的审美偏好和评价模式，发现不同模型具有独特的评价特征。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在揭示LLMs在文学评价中的表现，观察其是否具有类似人类批评学派的个体化评价特征，而非中立基准工具。

Method: 将十篇日本科幻短篇小说翻译成英文，由六种先进LLM在七次独立会话中评估，采用主成分分析和聚类技术分析数据。

Result: 发现评价一致性差异显著（α值范围1.00至0.35），识别出五种评价模式，不同故事间评价方差差异达4.5倍，TF-IDF分析显示各模型评价词汇独特。

Conclusion: LLMs可能具有类似人类批评学派的个体化评价特征，而非中性基准工具，其隐含价值体系受RLHF影响。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [2] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Key words: 多模态大语言模型、地图视觉问答、基准数据集、等值区域地图、地图设计、地理知识

TL;DR: MapIQ是一个用于评估多模态大语言模型（MLLMs）在地图视觉问答（Map-VQA）中性能的新基准数据集，涵盖多种地图类型和主题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有Map-VQA研究主要集中在等值区域地图上，覆盖的专题类别和视觉分析任务有限，需要更全面的评估基准。

Method: 提出MapIQ数据集，包含14,706个问答对，覆盖3种地图类型和6个主题，并评估多种MLLMs在6种视觉分析任务中的表现。

Result: 对比模型表现与人类基线，并通过地图设计变化实验分析模型的鲁棒性、敏感性和对地理知识的依赖。

Conclusion: MapIQ为Map-VQA性能改进提供了潜在路径，揭示了MLLMs的局限性。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [3] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Key words: 跨语言情感分析，少样本学习，增量学习，波斯语，预训练模型

TL;DR: 研究通过少样本学习和增量学习方法在波斯语中进行跨语言情感分析，利用XLM-RoBERTa等预训练模型，在有限数据下实现高准确率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 目标是开发一种模型，能够在数据有限的波斯语中利用高资源语言的知识进行情感分析。

Method: 使用XLM-RoBERTa、mDeBERTa和DistilBERT三种预训练模型，通过少样本和增量学习对波斯语数据进行微调。

Result: mDeBERTa和XLM-RoBERTa在波斯语情感分析中达到96%的准确率。

Conclusion: 结合少样本学习、增量学习和多语言预训练模型在跨语言情感分析中效果显著。

Abstract: This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [4] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Key words: 多模态学习, 单模态特征, 配对模态特征, PgM框架

TL;DR: 本文提出了PgM框架，通过将多模态表示分为单模态和配对模态特征，实现了更全面的特征学习和灵活的下游任务适配。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 多模态学习中，单模态和配对模态特征的区分未得到充分研究，PgM旨在填补这一空白。

Method: PgM框架包括模态分割器、单模态学习器、配对模态学习器和单配对模态解码器，分别处理不同特征。

Result: 实验证明PgM在多种多模态任务中表现优异，且具有向现有模型的迁移能力。

Conclusion: PgM通过特征划分和学习优化，显著提升了多模态学习的性能。

Abstract: Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [5] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Key words: 多模态、表格问答、可解释性、模块化设计、审计

TL;DR: ExpliCIT-QA是一个多模态表格问答系统，通过模块化设计提供可解释的答案，并在透明度和可审计性方面取得改进。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决现有表格问答系统中缺乏可解释性的问题，特别适用于敏感领域如金融和医疗。

Method: 采用多模态表格理解、语言推理、自动代码生成、代码执行和自然语言解释的模块化设计。

Result: 在TableVQA-Bench基准测试中表现优于现有基线，提升了可解释性和透明度。

Conclusion: ExpliCIT-QA填补了端到端表格问答系统的可解释性空白，适用于需要审计的敏感领域。

Abstract: We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [6] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Key words: 数据科学, Python笔记本, LLM, 信息流图, 执行依赖

TL;DR: 论文提出了一种通过有限语法分析辅助LLM完整理解Python笔记本的方法CRABS，用于生成信息流图和执行依赖图，解决了LLM在理解笔记本时的幻觉和长上下文挑战。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 评估、重用和适应数据科学和机器学习Python笔记本需要理解其信息流和操作，但重新执行笔记本往往不切实际。

Method: 采用CRABS策略，结合浅层语法分析和AST分析，通过LLM解决剩余歧义，生成信息流和依赖图。

Result: 在50个代表性Kaggle笔记本上，CRABS平均F1得分为98%（信息流识别）和99%（依赖识别）。

Conclusion: CRABS通过结合语法分析和LLM，有效解决了笔记本理解问题，为笔记本分析提供了实用解决方案。

Abstract: Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [7] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Key words: 主观性检测，多语言任务，情感特征，Transformer，类不平衡

TL;DR: AI Wizards团队在CLEF 2025 CheckThat! Lab Task 1（新闻文章主观性检测）中表现突出，通过融合情感特征提升模型性能，尤其在多语言和零样本设置下取得了优异成绩。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨如何通过结合情感评分与句子表示来增强基于Transformer的分类器性能，解决多语言环境下的主观性检测问题。

Method: 使用mDeBERTaV3-base、ModernBERT-base和Llama3.2-1B等模型，并集成情感特征；通过决策阈值校准解决类别不平衡问题。

Result: 情感特征显著提升了模型性能，在希腊语任务中取得了第一名（Macro F1 = 0.51）。

Conclusion: 结合情感特征的策略在多语言主观性检测任务中有效，且零样本泛化能力优秀。

Abstract: This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [8] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)
*Dante Campregher,Yanxu Chen,Sander Hoffman,Maria Heuss*

Key words: 可重复性研究, 大型语言模型, 注意力头, 事实与反事实信息, 领域特异性

TL;DR: 本文通过再现性研究探讨了大型语言模型如何管理竞争性事实与反事实信息，重点关注注意力头的作用，并验证了先前研究的发现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于澄清注意力头在模型如何处理矛盾信息中的作用，验证近期相关研究的可重复性。

Method: 研究方法包括再现和整合Ortu等人、Yu等人及McDougall等人的研究，分析注意力头强度与事实输出比例的关系，评估其抑制机制的竞争性假设，并探讨注意力模式的领域特异性。

Result: 结果表明，促进事实输出的注意力头通过通用复制抑制而非选择性反事实抑制实现效果，且其行为具有领域依赖性，更大模型表现出更专一和类别敏感的模式。

Conclusion: 结论指出注意力头的抑制机制是通用而非选择性的，且其行为受模型规模和领域影响。

Abstract: This paper presents a reproducibility study examining how Large Language
Models (LLMs) manage competing factual and counterfactual information, focusing
on the role of attention heads in this process. We attempt to reproduce and
reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and
Pavlick and McDougall et al. that investigate the competition between
model-learned facts and contradictory context information through Mechanistic
Interpretability tools. Our study specifically examines the relationship
between attention head strength and factual output ratios, evaluates competing
hypotheses about attention heads' suppression mechanisms, and investigates the
domain specificity of these attention patterns. Our findings suggest that
attention heads promoting factual output do so via general copy suppression
rather than selective counterfactual suppression, as strengthening them can
also inhibit correct facts. Additionally, we show that attention head behavior
is domain-dependent, with larger models exhibiting more specialized and
category-sensitive patterns.

</details>


### [9] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)
*Yash Ingle,Pruthwik Mishra*

Key words: 语言识别、印度语言、数据集、基线模型、深度学习

TL;DR: 论文介绍了一个包含英语和22种印度官方语言的数据集，并提出基于机器学习和深度学习的基线模型，用于语言识别任务。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 语言识别是NLP中的基础任务，尤其在印度多语言环境中，由于语言间的相似性和代码混合等问题，任务更具挑战性。

Method: 创建了一个包含23万句的数据集，并使用机器学习和深度学习的最优方法开发了基线模型。

Result: 基线模型在语言识别任务中表现优异，与当前最优模型相当。

Conclusion: 该研究为印度多语言环境中的语言识别提供了新的数据集和有效的基线模型。

Abstract: The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

</details>


### [10] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
*Mohammad Samragh,Arnav Kundu,David Harrison,Kumari Nishu,Devang Naik,Minsik Cho,Mehrdad Farajtabar*

Key words: 自回归语言模型, 多标记预测, 推理加速, LoRA, 辅助损失

TL;DR: 提出了一种利用自回归语言模型固有知识的新框架，通过同时预测多个后续标记，显著提高了生成速度，且在质量上无损失。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 自回归语言模型的顺序生成性质限制了推理速度和并行性，尤其是在生成后期。本文旨在解决这一限制。

Method: 结合掩码输入、门控LoRA、轻量级采样器模块、辅助训练损失和推测生成策略，实现多标记预测。

Result: 在代码和数学任务中速度提升近5倍，聊天和知识任务提升2.5倍，且质量保持不变。

Conclusion: 新框架有效提升了自回归模型的生成效率，证明了多标记预测的潜力。

Abstract: Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

</details>


### [11] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)
*Junhong Ye,Xu Yuan,Xinying Qiu*

Key words: PII识别, 跨领域迁移, 数据融合, 小样本学习

TL;DR: 研究了跨领域模型迁移、多领域数据融合和样本高效学习对PII识别的效果，结果显示法律领域数据对传记文本迁移效果好，医学领域则较差。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨自动文本匿名化中PII识别的有效性。

Method: 使用医疗、法律和传记领域的标注语料库，评估模型在域内性能、跨域迁移性、融合和小样本学习四个维度上的表现。

Result: 法律领域数据对传记文本迁移效果好，医学领域迁移效果差，融合效果因领域而异，低专业化领域仅需10%训练数据即可实现高质量识别。

Conclusion: 跨领域迁移和样本高效学习在PII识别中具有潜力，但效果因领域而异。

Abstract: Accurate recognition of personally identifiable information (PII) is central
to automated text anonymization. This paper investigates the effectiveness of
cross-domain model transfer, multi-domain data fusion, and sample-efficient
learning for PII recognition. Using annotated corpora from healthcare (I2B2),
legal (TAB), and biography (Wikipedia), we evaluate models across four
dimensions: in-domain performance, cross-domain transferability, fusion, and
few-shot learning. Results show legal-domain data transfers well to
biographical texts, while medical domains resist incoming transfer. Fusion
benefits are domain-specific, and high-quality recognition is achievable with
only 10% of training data in low-specialization domains.

</details>


### [12] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)
*Xiangyu Yang,Xinying Qiu*

Key words: 语法纠错, 语法可接受性, 双向框架, 知识迁移, 多语言

TL;DR: 提出COLA-GEC框架，通过双向知识迁移提升语法纠错（GEC）和语法可接受性判断（COLA）任务，在多语言基准上取得最优结果。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: GEC和COLA任务共享语法知识但独立发展，通过双向框架实现互补提升。

Method: 利用GEC数据集增强COLA模型，并通过动态损失函数将COLA信号整合到GEC训练中。

Result: 在多语言基准上实现最优性能，但标点纠错仍是挑战。

Conclusion: COLA-GEC框架有效提升任务性能，未来需优化语法建模。

Abstract: Grammatical Error Correction (GEC) and grammatical acceptability judgment
(COLA) are core tasks in natural language processing, sharing foundational
grammatical knowledge yet typically evolving independently. This paper
introduces COLA-GEC, a novel bidirectional framework that enhances both tasks
through mutual knowledge transfer. First, we augment grammatical acceptability
models using GEC datasets, significantly improving their performance across
multiple languages. Second, we integrate grammatical acceptability signals into
GEC model training via a dynamic loss function, effectively guiding corrections
toward grammatically acceptable outputs. Our approach achieves state-of-the-art
results on several multilingual benchmarks. Comprehensive error analysis
highlights remaining challenges, particularly in punctuation error correction,
providing insights for future improvements in grammatical modeling.

</details>


### [13] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)
*Tianyou Huang,Xinglu Chen,Jingshen Zhang,Xinying Qiu,Ruiying Niu*

Key words: DualReward, 强化学习, 干扰项生成, 自适应缩放

TL;DR: DualReward是一个新的强化学习框架，用于在完形填空中自动生成干扰项。它通过双奖励结构和自适应缩放机制，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的干扰项生成方法主要依赖于监督学习或静态生成模型，缺乏对多样性和动态调整的关注。

Method: 采用双奖励结构和自适应缩放机制，动态调整奖励信号强度。

Result: 在多个数据集上表现优于基线方法，特别是在多样跨域数据上提升明显。

Conclusion: DualReward提供了一个灵活框架，能平衡从可靠人类示例中学习并探索高质量干扰项。

Abstract: This paper introduces DualReward, a novel reinforcement learning framework
for automatic distractor generation in cloze tests. Unlike conventional
approaches that rely primarily on supervised learning or static generative
models, our method employs a dual reward structure with adaptive scaling that
differentiates between human-created gold standard distractors and
model-generated candidates. The framework dynamically adjusts reward signal
intensity based on model performance and confidence. We evaluate our approach
on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets,
demonstrating consistent improvements over state-of-the-art baselines.
Experimental results show that our adaptive reward scaling mechanism provides
modest but consistent benefits on homogeneous datasets (CLOTH-F) and more
substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data
(MCQ), suggesting its particular effectiveness for handling varied question
types and domains. Our work offers a flexible framework that effectively
balances learning from reliable human examples while exploring novel,
high-quality distractors for automated test generation.

</details>


### [14] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)
*Jiachen Zhao,Jing Huang,Zhengxuan Wu,David Bau,Weiyan Shi*

Key words: LLMs, 安全机制, 有害性方向, 拒绝方向, Latent Guard

TL;DR: 该论文揭示了大型语言模型（LLMs）内部对有害性的理解与其拒绝行为的关系，并提出了一种新的安全机制——潜在守卫（Latent Guard），用于检测不安全输入并减少过度拒绝。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探究LLMs是否真正理解有害性，而不仅仅是表面上的拒绝行为，并分析其内部安全机制。

Method: 发现并分析了LLMs内部的“有害性方向”与“拒绝方向”，并利用这一发现开发了Latent Guard作为安全应用。

Result: LLMs对有害性的内部理解比拒绝行为更鲁棒，Latent Guard的性能与专门训练的安全模型相当或更好。

Conclusion: LLMs对有害性的理解独立且鲁棒，为AI安全研究提供了新视角。

Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand
harmfulness beyond just refusing? Prior work has shown that LLMs' refusal
behaviors can be mediated by a one-dimensional subspace, i.e., a refusal
direction. In this work, we identify a new dimension to analyze safety
mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a
separate concept from refusal. There exists a harmfulness direction that is
distinct from the refusal direction. As causal evidence, steering along the
harmfulness direction can lead LLMs to interpret harmless instructions as
harmful, but steering along the refusal direction tends to elicit refusal
responses directly without reversing the model's judgment on harmfulness.
Furthermore, using our identified harmfulness concept, we find that certain
jailbreak methods work by reducing the refusal signals without reversing the
model's internal belief of harmfulness. We also find that adversarially
finetuning models to accept harmful instructions has minimal impact on the
model's internal belief of harmfulness. These insights lead to a practical
safety application: The model's latent harmfulness representation can serve as
an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing
over-refusals that is robust to finetuning attacks. For instance, our Latent
Guard achieves performance comparable to or better than Llama Guard 3 8B, a
dedicated finetuned safeguard model, across different jailbreak methods. Our
findings suggest that LLMs' internal understanding of harmfulness is more
robust than their refusal decision to diverse input instructions, offering a
new perspective to study AI safety

</details>


### [15] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)
*Bo Zeng,Chenyang Lyu,Sinuo Liu,Mingyan Zeng,Minghao Wu,Xuanfan Ni,Tianqi Shi,Yu Zhao,Yefeng Liu,Chenyu Zhu,Ruizhe Li,Jiahui Geng,Qing Li,Yu Tong,Longyue Wang,Weihua Luo,Kaifu Zhang*

Key words: 指令跟随、多语言评测、大语言模型、本地化、性能差距

TL;DR: 该论文提出了一个多语言指令跟随评测基准Marco-Bench-MIF，覆盖30种语言，通过混合翻译与验证的流程解决语言和文化差异问题，评测了20+ LLMs并发现高低资源语言间的性能差距、模型规模对性能的影响以及机器翻译数据的局限性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有指令跟随评测数据集多为英语或机器翻译版本，限制了在多语言环境中的适用性。

Method: 使用混合翻译与验证的流程，扩展IFEval到30种语言的本地化版本Marco-Bench-MIF。

Result: 发现高低资源语言间存在25-35%的性能差距，模型规模影响性能45-60%，机器翻译数据低估了本地化数据的准确性（7-22%）。

Conclusion: 多语言指令跟随存在关键词一致性保留和组合约束遵循的挑战，Marco-Bench-MIF为未来研究提供了基准。

Abstract: Instruction-following capability has become a major ability to be evaluated
for Large Language Models (LLMs). However, existing datasets, such as IFEval,
are either predominantly monolingual and centered on English or simply machine
translated to other languages, limiting their applicability in multilingual
contexts. In this paper, we present an carefully-curated extension of IFEval to
a localized multilingual version named Marco-Bench-MIF, covering 30 languages
with varying levels of localization. Our benchmark addresses linguistic
constraints (e.g., modifying capitalization requirements for Chinese) and
cultural references (e.g., substituting region-specific company names in
prompts) via a hybrid pipeline combining translation with verification. Through
comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1)
25-35% accuracy gap between high/low-resource languages, (2) model scales
largely impact performance by 45-60% yet persists script-specific challenges,
and (3) machine-translated data underestimates accuracy by7-22% versus
localized data. Our analysis identifies challenges in multilingual instruction
following, including keyword consistency preservation and compositional
constraint adherence across languages. Our Marco-Bench-MIF is available at
https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [16] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Key words: 几何问题求解, 深度学习, 多模态大语言模型, 评估指标, 挑战与方向

TL;DR: 本文综述了深度学习在几何问题求解中的应用，包括任务总结、方法回顾、评估指标分析及未来方向讨论。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 几何问题求解是数学推理的重要领域，涉及教育和人工智能能力评估等关键领域，近年来深度学习技术的发展引发了研究热潮。

Method: 论文通过综合总结几何问题求解的相关任务、深度学习方法的回顾、评估指标和方法的详细分析，以及当前挑战和未来方向的讨论，提供了全面的参考。

Result: 论文提供了一个全面的深度学习在几何问题求解中的实用参考，并创建了GitHub上持续更新的论文列表。

Conclusion: 本文旨在促进几何问题求解领域的进一步发展，为深度学习在该领域的应用提供全面参考。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [17] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Key words: 多语言图表理解, 视觉语言模型, 问答基准, 低资源语言, 非拉丁文字

TL;DR: PolyChartQA是一个多语言图表问答基准，涵盖22,606图表和26,151问答对，支持10种语言，旨在解决图表理解基准的英语中心问题。该方法通过分离图表数据与渲染代码，实现了多语言图表的灵活生成。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有图表理解基准主要基于英语，限制了其在全球范围内的可访问性和适用性。因此，需要开发一个多语言的图表问答基准，以促进更具包容性的视觉语言模型。

Method: 使用解耦的流水线方法，将图表数据与渲染代码分离，通过翻译数据和复用代码生成多语言图表。利用先进的LLM翻译技术并实施严格的质量控制，以确保语言的准确性和语义一致性。

Result: 实验显示，英语与其他语言（尤其是使用非拉丁文字的低资源语言）之间存在显著的性能差距。

Conclusion: PolyChartQA为推进全球包容性视觉语言模型奠定了基础。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [18] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Key words: 分词,BPE,GPU并行化,批处理推理

TL;DR: BlockBPE是一种基于GPU并行化的BPE分词实现，优化了高吞吐量批处理推理，相比现有方法提升了效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有CPU分词方法在GPU批处理推理中表现不佳，阻碍了大模型管道的效率。

Method: 提出BlockBPE，通过并行化GPU实现BPE，省略正则预分词步骤，优化线程块内的合并操作。

Result: 在批处理任务中，BlockBPE的吞吐量比tiktoken高2倍，比HuggingFace Tokenizers高2.5倍。

Conclusion: BlockBPE显著提升了分词的效率和吞吐量，适用于GPU批处理推理。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [19] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)
*Yi Zhao,Zuchao Li,Hai Zhao,Baoyuan Qi,Guoming Liu*

Key words: 任务无关提示压缩, 动态注意力感知, 信息熵, 注意力机制, LLMs

TL;DR: 论文提出了一种动态注意力感知的任务无关提示压缩方法（DAC），通过整合熵和注意力信息，动态感知压缩过程中的熵变化，以实现细粒度的提示压缩。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有方法主要依赖信息熵作为压缩指标，但忽略了算法层面的注意力关键令牌以及压缩过程中的熵变化问题。

Method: 提出动态注意力感知方法（DAC），整合熵和注意力信息，动态感知压缩过程中的熵变化。

Result: 在多领域的广泛实验中，DAC在多样化任务和LLMs中表现优异，显示出其强大效果。

Conclusion: DAC通过动态整合熵和注意力信息，实现了更高效的提示压缩，为任务无关压缩提供了新思路。

Abstract: Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

</details>


### [20] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)
*Yi Zhao,Zuchao Li,Hai Zhao*

Key words: LLMs, 注意力矩阵, KV缓存, 计算优化, IAM框架

TL;DR: 论文提出IAM框架，通过利用不同规模LLMs之间注意力矩阵的高相似性，实现加速注意力计算和减少KV缓存使用，实验效果显著。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在长上下文场景下资源消耗巨大，现有方法主要关注模型内部稀疏性，忽略了外部信息的优化潜力。

Method: 通过测量注意力矩阵相似性，选择映射层并提出IAM框架，实现小规模与大规模LLMs之间的注意力映射。

Result: IAM在预填充阶段加速15%，减少22.1%的KV缓存使用，且性能损失可忽略。

Conclusion: IAM作为一种通用方法，与现有优化技术正交，可显著提升LLMs效率。

Abstract: LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

</details>


### [21] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)
*Artem Alekseev,Mikhail Chaichuk,Miron Butko,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Key words: 知识图问答,多跳推理,时间问题,查询生成,CoT推理

TL;DR: 本文提出了一种多阶段的基于查询的知识图问答（KGQA）框架，用于解决多跳推理和时间问题，通过生成可执行查询而非直接答案，提升性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决大型语言模型在多跳推理和时间问答中的局限性，提供模块化替代方案。

Method: 采用多阶段查询框架，结合新颖的实体链接和谓词匹配方法，利用CoT推理。

Result: 在多跳和时间问答数据集上验证了框架的有效性，展示了小型语言模型的潜力。

Conclusion: 基于查询的多阶段KGQA框架为多跳和时间问答提供了一种有效解决方案。

Abstract: Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [22] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)
*Xinyu Wang,Vahid Partovi Nia,Peng Lu,Jerry Huang,Xiao-Wen Chang,Boxing Chen,Yufei Cui*

Key words: 大型语言模型（LLM）、PoT量化、后训练算法、低精度、加速推理

TL;DR: 提出了一种新型的PoT（2的幂次）量化框架，用于大型语言模型（LLM）权重量化，显著提升低精度格式下的准确性和推理速度。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 由于LLM部署需要大量计算资源，现有的PoT量化方法在GPU上效果不佳，需要一种更高效的解决方案。

Method: 采用两步后训练算法：先初始化量化尺度，再通过最小校准集优化尺度，同时设计更高效的解量化方法。

Result: 在2-和3-bit低精度下超越现有整数量化方法，GPU上的解量化速度提升显著（V100加速3.67倍，RTX 4090加速1.63倍）。

Conclusion: 提出的PoT框架在低精度与推理速度上均优于现有方法，为LLM高效部署提供了新方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various natural language processing (NLP) tasks. However, their deployment is
challenging due to the substantial computational resources required.
Power-of-two (PoT) quantization is a general tool to counteract this
difficulty. Albeit previous works on PoT quantization can be efficiently
dequantized on CPUs using fixed-point addition, it showed less effectiveness on
GPUs. The reason is entanglement of the sign bit and sequential bit
manipulations needed for dequantization. We propose a novel POT quantization
framework for LLM weights that (i) outperforms state-of-the-art accuracy in
extremely low-precision number formats, and (ii) enables faster inference
through more efficient dequantization. To maintain the accuracy of the
quantized model, we introduce a two-step post-training algorithm: (i)
initialize the quantization scales with a robust starting point, and (ii)
refine these scales using a minimal calibration set. The performance of our PoT
post-training algorithm surpasses the current state-of-the-art in integer
quantization, particularly at low precisions such as 2- and 3-bit formats. Our
PoT quantization accelerates the dequantization step required for the floating
point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and
$1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [23] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)
*Ziyu Ge,Gabriel Chua,Leanne Tan,Roy Ka-Wei Lee*

Key words: 毒性保留翻译、低资源语言、Singlish、小样本提示工程、文化敏感治理

TL;DR: 本文提出一种两阶段框架，用于低资源语言对的毒性保留翻译，通过小样本提示工程和模型优化，展示其在混合语言Singlish上的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在线交流中低资源语言和文化方言的翻译常忽略本地俚语和毒性内容，缺乏有效工具，需解决毒性保留翻译的挑战。

Method: 通过人工验证的小样本提示工程和模型-提示对优化，结合语义相似度评估。

Result: 定量人工评估证实该框架在翻译质量和毒性保留上的有效性。

Conclusion: 该框架提升翻译质量的同时支持多文化LLMs的文化敏感治理和低资源场景的基准测试，强调社会语言细微差异的重要性。

Abstract: As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

</details>


### [24] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)
*Yuhong Zhang,Jialu Li,Shilai Yang,Yuchen Xu,Gert Cauwenberghs,Tzyy-Ping Jung*

Key words: 阅读理解, 大型语言模型, 图结构表征, 眼动追踪, 人机协同学习

TL;DR: 论文通过图结构文本表征比较了人类与LLMs在阅读理解中的语言理解，发现LLMs在图拓扑结构层面具有高度一致性，为人类与AI的协同学习提供了新见解。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨人类与大型语言模型（LLMs）在阅读理解中的语言理解差异，及其在功能性任务中的应用潜力。

Method: 使用LLM基于的AI代理将文本中的词分组为节点和边，构建图结构文本表征，并结合眼动数据比较重要节点和边的注视分布。

Result: LLMs在图拓扑结构的语言理解上表现出高度一致性，验证了其与人类阅读理解的相关性。

Conclusion: 研究深化了对人类与LLMs语言理解的认识，为未来人机协同学习策略提供了理论基础。

Abstract: Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

</details>


### [25] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)
*Yuki Sakamoto,Takahisa Uchida,Hiroshi Ishiguro*

Key words: 大语言模型、价值相似性、信任、人际亲密度、社会科学

TL;DR: 研究探讨了价值相似性对LLM代理之间关系构建的影响，通过实验验证了价值相似性与信任和人际亲密度之间的正相关关系。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨人类社会中价值相似性在由LLM代理组成的人工社会中是否同样适用。

Method: 通过两个实验，首先评估LLM中价值观的可控性，然后生成具有特定价值观的代理对并分析其相互评价。

Result: 价值相似性较高的代理对表现出更强的相互信任和人际亲密度。

Conclusion: LLM代理模拟可作为社会科学理论的有效测试平台，有助于解释价值观如何影响关系构建。

Abstract: Large language models (LLMs) have emerged as powerful tools for simulating
complex social phenomena using human-like agents with specific traits. In human
societies, value similarity is important for building trust and close
relationships; however, it remains unexplored whether this principle holds true
in artificial societies comprising LLM agents. Therefore, this study
investigates the influence of value similarity on relationship-building among
LLM agents through two experiments. First, in a preliminary experiment, we
evaluated the controllability of values in LLMs to identify the most effective
model and prompt design for controlling the values. Subsequently, in the main
experiment, we generated pairs of LLM agents imbued with specific values and
analyzed their mutual evaluations of trust and interpersonal closeness
following a dialogue. The experiments were conducted in English and Japanese to
investigate language dependence. The results confirmed that pairs of agents
with higher value similarity exhibited greater mutual trust and interpersonal
closeness. Our findings demonstrate that the LLM agent simulation serves as a
valid testbed for social science theories, contributes to elucidating the
mechanisms by which values influence relationship building, and provides a
foundation for inspiring new theories and insights into the social sciences.

</details>


### [26] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)
*Lukas Ellinger,Miriam Anschütz,Georg Groh*

Key words: Large Language Models, homonyms, definition quality, simplification, educational NLP

TL;DR: 论文研究了大型语言模型（LLM）在不同目标群体中对多义词定义的简化对质量的影响，发现简化会显著降低定义的完整性，易导致误解。通过微调模型可以改善响应质量。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究目的是探索定义简化对不同目标群体（如儿童或语言学习者）的影响，尤其是在多义词定义中避免信息丢失和误导。

Method: 使用DeepSeek v3等多种LLM，通过LLM-as-Judge和人工标注评估简化对多义词定义质量的影响，并对Llama 3.1 8B进行微调优化。

Result: 简化显著降低了定义的完整性，增加了误解风险；微调后模型在多义词响应质量上有显著提升。

Conclusion: 教育类NLP应用需要在简洁性和完整性之间找到平衡，以确保为所有学习者提供可靠的上下文感知定义。

Abstract: Large Language Models (LLMs) can provide accurate word definitions and
explanations for any context. However, the scope of the definition changes for
different target groups, like children or language learners. This is especially
relevant for homonyms, words with multiple meanings, where oversimplification
might risk information loss by omitting key senses, potentially misleading
users who trust LLM outputs. We investigate how simplification impacts homonym
definition quality across three target groups: Normal, Simple, and ELI5. Using
two novel evaluation datasets spanning multiple languages, we test DeepSeek v3,
Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge
and human annotations. Our results show that simplification drastically
degrades definition completeness by neglecting polysemy, increasing the risk of
misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization
substantially improves homonym response quality across all prompt types. These
findings highlight the need to balance simplicity and completeness in
educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [27] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)
*Josip Jukić*

Key words: 语言模型、表征平滑性、参数效率、主动学习、弱监督

TL;DR: 该论文研究神经网络语言模型中的数据与参数效率问题，提出基于表征平滑性的优化技术和弱监督方法，显著提升了模型性能和效率。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决语言模型中的数据与参数效率问题，提升模型的鲁棒性和泛化能力，减少对标注数据的依赖。

Method: 1. 分析表征平滑性，提出基于雅可比和海森矩阵的正则化策略；2. 结合主动学习与参数高效微调；3. 通过上下文学习增强弱监督技术。

Result: 在多个NLP任务中，方法显著优于传统技术，提升了性能、稳定性和效率。

Conclusion: 所提方法有效减少标注需求，提升模型在低资源环境中的表现。

Abstract: This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

</details>


### [28] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)
*Anca Dinu,Andra-Maria Florescu,Alina Resceanu*

Key words: 语言创造力,大型语言模型,原创性,隐喻使用,词形变化

TL;DR: 论文介绍了一种通用语言创造力测试，用于评估人类和大型语言模型（LLM）在生成新词和短语方面的能力，结果显示LLM在多数任务中表现优于人类。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在比较人类和LLM在语言创造力方面的差异，探究两者在原创性、精细度和灵活性上的表现。

Method: 设计了基于词形变化和隐喻使用的任务，对24名人类和24个LLM进行测试，并使用OCSAI工具自动评估回答。

Result: LLM在大多数任务中优于人类，且在原创性、精细度和灵活性上表现更佳；人类更倾向于扩展创造力，而LLM偏向固定创造力。

Conclusion: LLM在语言创造力任务中整体表现优于人类，但两者在创造力类型上存在差异。

Abstract: The following paper introduces a general linguistic creativity test for
humans and Large Language Models (LLMs). The test consists of various tasks
aimed at assessing their ability to generate new original words and phrases
based on word formation processes (derivation and compounding) and on
metaphorical language use. We administered the test to 24 humans and to an
equal number of LLMs, and we automatically evaluated their answers using OCSAI
tool for three criteria: Originality, Elaboration, and Flexibility. The results
show that LLMs not only outperformed humans in all the assessed criteria, but
did better in six out of the eight test tasks. We then computed the uniqueness
of the individual answers, which showed some minor differences between humans
and LLMs. Finally, we performed a short manual analysis of the dataset, which
revealed that humans are more inclined towards E(extending)-creativity, while
LLMs favor F(ixed)-creativity.

</details>


### [29] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)
*Anthony G Cohn,Robert E Blackwell*

Key words: 大型语言模型,方向推理,基准测试,LLMs,方向判断

TL;DR: 研究了28种大型语言模型（LLM）在方向推理能力上的表现，通过模板生成的基准测试其方向判断准确性，发现即使是新型的大型推理模型也无法完全正确回答所有方向问题。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨LLMs在方向推理任务中的表现和能力，尤其是对基本方向（如东、西、南、北）的理解和判断。

Method: 使用模板生成的基准测试，测试LLMs在不同场景下（如运动方式、人称视角等）的方向推理能力。

Result: 即使是较新的大型推理模型也无法在所有测试问题上可靠地给出正确的方向判断。

Conclusion: LLMs在方向推理任务中表现有限，需进一步优化。

Abstract: We investigate the abilities of 28 Large language Models (LLMs) to reason
about cardinal directions (CDs) using a benchmark generated from a set of
templates, extensively testing an LLM's ability to determine the correct CD
given a particular scenario. The templates allow for a number of degrees of
variation such as means of locomotion of the agent involved, and whether set in
the first, second or third person. Even the newer Large Reasoning Models are
unable to reliably determine the correct CD for all questions. This paper
summarises and extends earlier work presented at COSIT-24.

</details>


### [30] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)
*Jeremi K. Ochab,Mateusz Matias,Tymoteusz Boba,Tomasz Walkowiak*

Key words: AI检测, 风格度量, 梯度提升机, spaCy, 文本预处理

TL;DR: 论文提出了一种基于模块化风格度量管道的二元AI检测方法，使用spaCy模型进行文本预处理和特征提取，并结合轻量级梯度提升机作为分类器。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 旨在通过非神经网络的、计算成本低但可解释的方法，高效检测机器生成的文本。

Method: 采用spaCy模型进行文本预处理（如分词、命名实体识别等）并提取大量特征，使用轻量级梯度提升机作为分类器，并在大规模机器生成文本训练集上进行训练。

Result: 通过优化参数，提升了分类器的性能，充分利用了训练集的数据。

Conclusion: 该方法在二元AI检测任务中表现出色，同时保持了计算效率和可解释性。

Abstract: This submission to the binary AI detection task is based on a modular
stylometric pipeline, where: public spaCy models are used for text
preprocessing (including tokenisation, named entity recognition, dependency
parsing, part-of-speech tagging, and morphology annotation) and extracting
several thousand features (frequencies of n-grams of the above linguistic
annotations); light-gradient boosting machines are used as the classifier. We
collect a large corpus of more than 500 000 machine-generated texts for the
classifier's training. We explore several parameter options to increase the
classifier's capacity and take advantage of that training set. Our approach
follows the non-neural, computationally inexpensive but explainable approach
found effective previously.

</details>


### [31] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)
*Giuliano Martinelli,Tommaso Bonomo,Pere-Lluís Huguet Cabot,Roberto Navigli*

Key words: 共指消解,长文本,基准测试,BOOKCOREF

TL;DR: 论文提出了一个新方法，自动生成高质量的长文本共指消解标注，并创建了首个书籍规模的共指消解基准BOOKCOREF，实验证明其提升现有系统性能，同时揭示当前模型在长文本上的不足。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 现有的共指消解基准在长文本评估上不足，无法充分评估系统在书籍规模下的表现。

Method: 开发了一种自动标注流水线，生成了书籍规模的共指消解基准BOOKCOREF。

Result: 基准显著提升现有系统性能（+20 CoNLL-F1），同时揭示了模型在长文本上的局限性。

Conclusion: BOOKCOREF填补了长文本共指消解基准的空白，促进了相关研究。

Abstract: Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [32] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)
*Tosin Adewumi,Foteini Simistira Liwicki,Marcus Liwicki,Viktor Gardelli,Lama Alkhaled,Hamam Mokayed*

Key words: 数学学习, LLM, 游戏化, 形成性反馈

TL;DR: 研究表明，结合苏格拉底法、思维链推理、简化游戏化和形成性反馈的MEGA方法在数学学习中效果优于传统逐步法（CoT）。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 部分学生在数学学习中遇到困难，导致他们避开数学相关学科，而传统教学方法效果有限。

Method: 研究比较了MEGA方法与传统的逐步法（CoT），使用大学生参与者，基于GSM8K和MATH数据集进行测试。

Result: 学生在MEGA方法中的学习体验更好，尤其是在难度较高的MATH数据集中，MEGA的表现显著优于CoT。

Conclusion: MEGA方法在数学学习，尤其是高难度问题中，效果显著优于传统方法。

Abstract: This paper presents an intervention study on the effects of the combined
methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)
simplified gamification and (4) formative feedback on university students'
Maths learning driven by large language models (LLMs). We call our approach
Mathematics Explanations through Games by AI LLMs (MEGA). Some students
struggle with Maths and as a result avoid Math-related discipline or subjects
despite the importance of Maths across many fields, including signal
processing. Oftentimes, students' Maths difficulties stem from suboptimal
pedagogy. We compared the MEGA method to the traditional step-by-step (CoT)
method to ascertain which is better by using a within-group design after
randomly assigning questions for the participants, who are university students.
Samples (n=60) were randomly drawn from each of the two test sets of the Grade
School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)
datasets, based on the error margin of 11%, the confidence level of 90%, and a
manageable number of samples for the student evaluators. These samples were
used to evaluate two capable LLMs at length (Generative Pretrained Transformer
4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for
capability. The results showed that students agree in more instances that the
MEGA method is experienced as better for learning for both datasets. It is even
much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH
dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [33] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)
*Payal Bhattad,Sai Manoj Pudukotai Dinakarrao,Anju Gupta*

Key words: 文本数据增强, 大语言模型, 语义一致性, 迭代增强, 主题建模

TL;DR: 论文提出了一种评估大语言模型（LLM）文本增强的框架，包括可扩展性分析和迭代增强方法，验证了其在实际NLP任务中的有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 解决文本数据增强中语义保留不足的问题，尤其在低资源NLP任务中。

Method: 提出两个评估组件：可扩展性分析和迭代增强与摘要细化（IASR）。

Result: GPT-3.5 Turbo在语义保真度、多样性和效率上表现最佳，应用于实际任务时显著提升主题粒度。

Conclusion: 验证了提出的框架在LLM增强中的实用性和结构化评估的重要性。

Abstract: Text data augmentation is a widely used strategy for mitigating data sparsity
in natural language processing (NLP), particularly in low-resource settings
where limited samples hinder effective semantic modeling. While augmentation
can improve input diversity and downstream interpretability, existing
techniques often lack mechanisms to ensure semantic preservation during
large-scale or iterative generation, leading to redundancy and instability.
This work introduces a principled evaluation framework for large language model
(LLM) based text augmentation, comprising two components: (1) Scalability
Analysis, which measures semantic consistency as augmentation volume increases,
and (2) Iterative Augmentation with Summarization Refinement (IASR), which
evaluates semantic drift across recursive paraphrasing cycles. Empirical
evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the
best balance of semantic fidelity, diversity, and generation efficiency.
Applied to a real-world topic modeling task using BERTopic with GPT-enhanced
few-shot labeling, the proposed approach results in a 400% increase in topic
granularity and complete elimination of topic overlaps. These findings
validated the utility of the proposed frameworks for structured evaluation of
LLM-based augmentation in practical NLP pipelines.

</details>


### [34] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)
*Pavel Šindelář,Ondřej Bojar*

Key words: ELOQUENT, Sensemaking, 语言模型, 评估任务

TL;DR: ELOQUENT是一个共享任务集，旨在为生成语言模型制定易于测试的高层次评估标准。Sensemaking是其中之一，通过教师、学生和评估者的三步流程评估模型的理解能力。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 为生成语言模型开发易于测试的高层次评估标准，以衡量其理解能力。

Method: 通过三个步骤评估模型：(1)教师系统生成问题，(2)学生系统回答问题，(3)评估系统评分，严格依赖给定输入材料。

Result: 2025年版本中，参与者表现不一：教师任务需改进评估策略；学生任务中模型表现尚可但受限；评估任务中LLM评分存在误判。

Conclusion: 任务设计有效，但需优化评估策略和模型限制文本依赖的能力。

Abstract: ELOQUENT is a set of shared tasks that aims to create easily testable
high-level criteria for evaluating generative language models. Sensemaking is
one such shared task.
  In Sensemaking, we try to assess how well generative models ``make sense out
of a given text'' in three steps inspired by exams in a classroom setting: (1)
Teacher systems should prepare a set of questions, (2) Student systems should
answer these questions, and (3) Evaluator systems should score these answers,
all adhering rather strictly to a given set of input materials.
  We report on the 2025 edition of Sensemaking, where we had 7 sources of test
materials (fact-checking analyses of statements, textbooks, transcribed
recordings of a lecture, and educational videos) spanning English, German,
Ukrainian, and Czech languages.
  This year, 4 teams participated, providing us with 2 Teacher submissions, 2
Student submissions, and 2 Evaluator submissions. We added baselines for
Teacher and Student using commercial large language model systems. We devised a
fully automatic evaluation procedure, which we compare to a minimalistic manual
evaluation.
  We were able to make some interesting observations. For the first task, the
creation of questions, better evaluation strategies will still have to be
devised because it is difficult to discern the quality of the various candidate
question sets. In the second task, question answering, the LLMs examined
overall perform acceptably, but restricting their answers to the given input
texts remains problematic. In the third task, evaluation of question answers,
our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm
erroneously rate both garbled question-answer pairs and answers to mixed-up
questions as acceptable.

</details>


### [35] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)
*Michael Carl,Takanori Mizowaki,Aishvarya Ray,Masaru Yamada,Devi Sri Bandaru,Xinyue Ren*

Key words: 行为翻译,认知过程,情感状态,击键分析,眼动追踪

TL;DR: 本文提出了一个行为翻译风格空间（BTSS），用于描述可能的翻译行为模式，并通过多层嵌入结构分析眼动和手指运动数据，揭示背后的认知和情感过程。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究动机在于通过观察翻译行为（如眼动和手指运动）来探索其背后的高阶认知和情感状态，从而更好地理解翻译过程。

Method: 方法包括分析击键和注视数据，并将行为模式组织为多层次嵌入的BTSS，作为计算翻译代理的基础。

Result: 研究结果表明，BTSS能够模拟人类翻译生产过程中的情感、自动化行为和认知的动态变化。

Conclusion: 结论是BTSS为理解翻译行为背后的认知和情感机制提供了一个有效框架，并为开发计算翻译代理奠定了基础。

Abstract: The paper introduces a Behavioural Translation Style Space (BTSS) that
describes possible behavioural translation patterns. The suggested BTSS is
organized as a hierarchical structure that entails various embedded processing
layers. We posit that observable translation behaviour - i.e., eye and finger
movements - is fundamental when executing the physical act of translation but
it is caused and shaped by higher-order cognitive processes and affective
translation states. We analyse records of keystrokes and gaze data as
indicators of the hidden mental processing structure and organize the
behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the
basis for a computational translation agent to simulate the temporal dynamics
of affect, automatized behaviour and cognition during human translation
production.

</details>


### [36] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)
*Reuben Smit,Retief Louw,Herman Kamper*

Key words: ASR-free, few-shot learning, child speech, SSL models, low-resource settings

TL;DR: 探索了一种在低资源环境中无需ASR的孤立单词阅读评估方法，通过少量样本将儿童语音与成人参考模板进行比较，但实验显示SSL模型在处理儿童语音时效果显著下降。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 在低资源环境中，开发一种无需自动语音识别（ASR）的孤立单词阅读评估方法，以解决儿童语音处理的挑战。

Method: 使用少量样本的儿童语音与成人参考模板进行比较，利用自监督学习（SSL）模型的中间层进行编码，并研究了离散化SSL特征和模板的质心平均等设计选项。

Result: 理想化实验中，成人语音表现良好，但儿童语音输入效果显著下降，即使使用儿童模板。

Conclusion: 尽管SSL表示在低资源语音任务中表现成功，但在少量样本分类系统中处理儿童数据时存在局限性。

Abstract: We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

</details>


### [37] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)
*Shilin Zhou,Zhenghua Li*

Key words: Automatic Speech Recognition, Keyword Recognition, Multi-grained Fusion, Large Language Models

TL;DR: 该论文提出了一种新颖的多粒度融合方法，结合token级和phrase级融合的优势，提升了ASR模型在关键词识别中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 端到端ASR模型在通用语音转录中表现优异，但在识别上下文相关的关键词（如专有名词或用户特定实体）时仍有不足。

Method: 采用多粒度融合策略，结合Large Language Models（LLMs）的上下文知识，通过late-fusion平衡token级精确度和phrase级理解。

Result: 在中文和英文数据集上取得了state-of-the-art的关键词识别性能，同时保持了非关键词文本的高准确率。

Conclusion: token级和phrase级组件在多粒度框架中互补，共同提升了性能。

Abstract: While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

</details>


### [38] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)
*Yikang Liu,Wanyang Zhang,Yiming Wang,Jialong Tang,Pei Zhang,Baosong Yang,Fei Huang,Rui Wang,Hai Hu*

Key words: translationese, T-index, language models, machine translation, quality estimation

TL;DR: 论文提出了一种名为T-index的量化翻译语言特征测量方法，通过对比性微调语言模型计算，验证了其跨域通用性和人工判断有效性。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前缺乏定量测量翻译语言特征（translationese）的方法，作者希望通过T-index填补这一空白，并验证其适用性。

Method: 利用对比性微调的两个语言模型（LMs）计算T-index，并在合成数据及真实翻译数据集上进行评估。

Result: T-index表现出色，仅需少量数据（1-5k对）即能有效捕捉真实翻译中的特征，且与人工标注显著相关（Pearson's r=0.568）。此外，T-index与BLEU等现有MT质量评估指标相关性低，表明其可作为补充指标。

Conclusion: T-index是一种高效、稳健的翻译语言特征量化工具，适用于跨域场景并能补充现有MT质量评估体系。

Abstract: In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

</details>


### [39] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)
*Johann Frei,Nils Feldhus,Lisa Raithel,Roland Roller,Alexander Meyer,Frank Kramer*

Key words: HL7 FHIR, LLM代理, 临床笔记, 结构化数据, 互操作性

TL;DR: 论文提出了一种名为Infherno的端到端框架，利用LLM代理、代码执行和医学术语数据库工具，将自由格式的临床笔记转换为结构化的FHIR资源，优于现有模块化系统和LLM方法。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前基于规则的系统或LLM方法在临床笔记转换为FHIR资源时存在通用性不足和结构不符合的问题。

Method: 采用端到端框架，结合LLM代理、代码执行和医学术语数据库工具，设计了Infherno系统。

Result: Infherno能较好地符合FHIR文档模式，且在从非结构化文本预测FHIR资源方面表现优于人类基线。

Conclusion: 该框架支持临床数据集成和跨机构互操作性，具有实际应用价值。

Abstract: For clinical data integration and healthcare services, the HL7 FHIR standard
has established itself as a desirable format for interoperability between
complex health data. Previous attempts at automating the translation from
free-form clinical notes into structured FHIR resources rely on modular,
rule-based systems or LLMs with instruction tuning and constrained decoding.
Since they frequently suffer from limited generalizability and structural
inconformity, we propose an end-to-end framework powered by LLM agents, code
execution, and healthcare terminology database tools to address these issues.
Our solution, called Infherno, is designed to adhere to the FHIR document
schema and competes well with a human baseline in predicting FHIR resources
from unstructured text. The implementation features a front end for custom and
synthetic data and both local and proprietary models, supporting clinical data
integration processes and interoperability across institutions.

</details>


### [40] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)
*Feng Xiao,Jicong Fan*

Key words: 文本异常检测、基准测试、预训练语言模型、嵌入质量、低秩特性

TL;DR: 该论文通过全面实证研究，提出了一个文本异常检测的基准，并通过多种预训练语言模型嵌入和多领域数据集进行评估。研究发现嵌入质量对异常检测效果至关重要，且深度学习方法在LLM嵌入下与传统浅层算法相比无优势。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 当前文本异常检测方法缺乏标准化和全面的评估基准，限制了方法的比较和创新。本研究旨在填补这一空白，推动异常检测技术的发展。

Method: 研究利用了早期语言模型（如GloVe、BERT）、多种LLM（如LLaMa-2、Mistral）、多领域文本数据集（如新闻、社交媒体），并结合全面评估指标（AUROC、AUPRC）。

Result: 研究发现嵌入质量直接影响异常检测效果，且深度学习方法在LLM嵌入下与传统方法（如KNN、孤立森林）表现相当。此外，跨模型性能矩阵呈现低秩特性，为快速模型评估和选择提供了高效策略。

Conclusion: 该研究为文本异常检测提供了标准化基准和工具包，推动了未来稳健且可扩展的异常检测系统的研究。

Abstract: Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

</details>


### [41] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Key words: 大型语言模型（LLMs）,硬件描述语言（VHDL）,Chain-of-Descriptions（CoDes）,代码生成,总结

TL;DR: 本文研究了大型语言模型（LLMs）在硬件描述语言（VHDL）中的表现，发现其性能不足，并提出了一种名为Chain-of-Descriptions（CoDes）的新方法，显著提升了LLMs在VHDL代码生成和总结任务中的表现。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 尽管LLMs在一般代码相关任务中广泛应用，但在硬件描述语言（VHDL）领域的评估和改进研究较少。本研究旨在填补这一空白。

Method: 提出CoDes方法，通过生成一系列中间描述步骤来增强LLMs的输入提示，从而提高VHDL代码生成和总结的性能。

Result: 实验表明，CoDes方法在多个指标上显著优于标准提示策略。

Conclusion: CoDes不仅提升了LLMs在VHDL任务中的表现，还为未来研究提供了框架。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [42] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Key words: 性别偏见, 阿尔茨海默病, 语音感知, 声学分析

TL;DR: 研究发现，在阿尔茨海默病（AD）语音感知中存在性别偏见，男性语音更易被识别为AD，尤其在汉语中。声学分析显示，男性语音的shimmer值与AD感知显著相关，而语音部分与AD识别呈负相关。语言对AD感知无显著影响，但性别偏见问题需在AD检测模型开发中关注。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探讨性别偏见在AD语音感知中的影响，以改进AD检测模型的公平性和准确性。

Method: 通过16名中国听众的感知实验，评估中希两种语言的AD语音，并进行声学分析。

Result: 男性语音更易被识别为AD，其中shimmer值与之显著相关；语言无显著影响。

Conclusion: 性别偏见在AD语音感知中起关键作用，需在模型开发中加以解决。

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [43] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Key words: 大型语言模型；多代理辩论框架；模糊性检测；性能提升

TL;DR: 摘要介绍了一个多代理辩论框架，旨在提升大型语言模型（LLMs）的模糊请求检测与解决能力。实验表明，该框架显著提升了模型的性能，特别是在处理复杂模糊请求时。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: LLMs在理解和生成人类语言方面表现出色，但在处理用户请求模糊性方面面临挑战。本文旨在通过多代理辩论框架解决这一问题。

Method: 引入了一个三模型架构（Llama3-8B、Gemma2-9B、Mistral-7B）的辩论框架，并使用包含多样模糊性的数据集进行评估。

Result: 辩论框架显著提升了Llama3-8B和Mistral-7B的性能，其中Mistral-7B带领的辩论达到76.7%的成功率，并有效处理复杂模糊请求。

Conclusion: 结构化辩论是增强LLM能力的有效方法，为开发更鲁棒和自适应语言理解系统提供了重要见解。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


### [44] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)
*Meysam Alizadeh,Fabrizio Gilardi,Zeynab Samei,Mohsen Mosleh*

Key words: 大语言模型、社交媒体、demographics、偏见、滥用风险

TL;DR: 评估具有网页浏览功能的大语言模型（LLMs）是否能通过用户名推断社交媒体用户的 demographics，结果显示其具备一定准确性，但也存在偏见和滥用风险。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索 LLMs 在实时获取社交媒体数据方面的能力，填补现有研究空白。

Method: 使用合成数据集（48 个 Twitter 账户）和调查数据集（1,384 名国际参与者），分析 LLMs 解析社交媒体内容的能力。

Result: LLMs 能预测用户 demographics，但会因账户活动少引入性别和政治偏见。

Conclusion: 此技术对计算社会科学有价值，但需防范滥用，建议限制公共访问并保留研究用途。

Abstract: Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

</details>


### [45] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)
*Yucheng Sun,Alessandro Stolfo,Mrinmaya Sachan*

Key words: 语言模型,算术错误,内部激活,错误检测,自我纠正

TL;DR: 研究发现语言模型内部激活可用于检测算术错误，通过简单探测器可实现高精度错误预测，并可推广到复杂任务中，实现模型自我纠正。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 探索语言模型内部激活是否能够用于检测算术错误，以提高模型自我纠正能力。

Method: 使用3位加法作为控制环境，训练轻量级错误探测器，并扩展到结构化链式推理任务中。

Result: 探测器在简单算术任务中准确率超过90%，并能推广到复杂任务中，提升任务准确性。

Conclusion: 算术错误可从内部激活中预测，简单探测器是实现轻量级模型自我纠正的有效途径。

Abstract: We investigate whether internal activations in language models can be used to
detect arithmetic errors. Starting with a controlled setting of 3-digit
addition, we show that simple probes can accurately decode both the model's
predicted output and the correct answer from hidden states, regardless of
whether the model's output is correct. Building on this, we train lightweight
error detectors that predict model correctness with over 90% accuracy. We then
extend our analysis to structured chain-of-thought traces on addition-only
GSM8K problems and find that probes trained on simple arithmetic generalize
well to this more complex setting, revealing consistent internal
representations. Finally, we demonstrate that these probes can guide selective
re-prompting of erroneous reasoning steps, improving task accuracy with minimal
disruption to correct outputs. Our findings suggest that arithmetic errors can
be anticipated from internal activations alone, and that simple probes offer a
viable path toward lightweight model self-correction.

</details>


### [46] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)
*Chandana Cheerla*

Key words: 企业数据, RAG框架, 混合检索, 元数据过滤, 语义分块

TL;DR: 本文提出了一种改进的RAG框架，解决了传统RAG在处理结构化数据时的局限性，通过混合检索策略和元数据过滤显著提升了检索性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 企业在决策中越来越依赖私有数据，但现有LLMs和RAG框架在处理异构数据时存在不足。

Method: 结合密集嵌入和BM25的混合检索策略，利用SpaCy NER进行元数据过滤，通过语义分块和表格结构保留优化数据处理。

Result: 在多个指标上显著提升，如Precision@5提高15%，Recall@5提高13%，同时在忠实度、完整性和相关性方面得分更高。

Conclusion: 该框架能有效提升企业任务的响应质量，未来将扩展至多模态数据和基于代理的检索。

Abstract: Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [47] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)
*Yik Siu Chan,Zheng-Xin Yong,Stephen H. Bach*

Key words: 链式思维（CoT）、对齐风险、线性探测、实时监测、语言模型

TL;DR: 研究了通过链式思维（CoT）预测语言模型输出的对齐风险，发现基于CoT激活的线性探测比文本方法更有效，且能早期干预。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 开放权重推理语言模型生成的链式思维可能包含有害内容，研究如何预测最终响应的对齐风险。

Method: 评估了人类、大语言模型和文本分类器对CoT文本或激活的监测方法，特别关注线性探测的性能。

Result: 基于CoT激活的线性探测在预测安全/不安全响应上显著优于文本方法，且能在生成早期就准确预测。

Conclusion: 轻量级探测可实现实时安全监控和早期干预，适用于不同模型和基准。

Abstract: Open-weights reasoning language models generate long chains-of-thought (CoTs)
before producing a final response, which improves performance but introduces
additional alignment risks, with harmful content often appearing in both the
CoTs and the final outputs. In this work, we investigate if we can use CoTs to
predict final response misalignment. We evaluate a range of monitoring
approaches, including humans, highly-capable large language models, and text
classifiers, using either CoT text or activations. First, we find that a simple
linear probe trained on CoT activations can significantly outperform all
text-based methods in predicting whether a final response will be safe or
unsafe. CoT texts are often unfaithful and can mislead humans and classifiers,
while model latents (i.e., CoT activations) offer a more reliable predictive
signal. Second, the probe makes accurate predictions before reasoning
completes, achieving strong performance even when applied to early CoT
segments. These findings generalize across model sizes, families, and safety
benchmarks, suggesting that lightweight probes could enable real-time safety
monitoring and early intervention during generation.

</details>


### [48] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)
*Suman Adhya,Debarshi Kumar Sanyal*

Key words: 主题建模,超球面空间,后验塌缩,球形切片Wasserstein距离

TL;DR: 提出了一种名为S2WTM的新方法，通过球形切片Wasserstein距离解决VAE-NTM中的后验塌缩问题，提高了主题建模的性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在解决VAE-NTM中因后验塌缩导致的潜在表示失效问题，同时保持超球面结构的建模优势。

Method: S2WTM采用单位超球面上的先验分布，并用球形切片Wasserstein距离对齐后验分布与先验。

Result: 实验表明，S2WTM在生成更连贯和多样主题的同时，提升了下游任务的性能。

Conclusion: S2WTM有效解决了后验塌缩问题，并优于现有主题模型。

Abstract: Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

</details>


### [49] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)
*David Mizrahi,Anders Boesen Lindbo Larsen,Jesse Allardice,Suzie Petryk,Yuri Gorokhov,Jeffrey Li,Alex Fang,Josh Gardner,Tom Gunter,Afshin Dehghan*

Key words: 数据选择，预训练，基准目标，模型规模，BETR

TL;DR: 论文提出了一种名为BETR的显式优化方法，通过将预训练文档与目标基准相似度匹配来选择数据，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CL

Motivation: 研究旨在探索通过对数据选择目标进行显式优化是否能更有效地提升模型性能。

Method: 提出了BETR方法，通过嵌入共享空间计算相似度，并训练轻量级分类器来预测全量数据的评分。

Result: BETR在10个任务中的9个均表现优异，计算效率提升了2.1倍，且适用于不同规模的模型。

Conclusion: 直接匹配预训练数据与目标任务能有效塑造模型能力，且选择策略需随模型规模调整。

Abstract: Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Key words: 图神经网络, 材料成形, RUGNN, 时间动态建模, 空间长程依赖

TL;DR: 该论文提出了一种名为RUGNN的图神经网络替代模型，用于预测材料成形过程中的变形场，解决了传统AI模型在捕捉3D空间关系和排列不变性方面的局限性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统AI替代模型在捕捉复杂3D空间关系和排列不变性方面表现有限，因此作者开发了基于图神经网络的RUGNN模型，以更准确地预测材料成形过程中的变形。

Method: RUGNN模型结合了门控循环单元（GRUs）建模时间动态，以及基于U-Net的图降采样/上采样机制处理空间长程依赖。此外，提出了新颖的“节点到表面”接触表示方法，提高了计算效率。

Result: RUGNN模型在冷成形和热成形案例中验证了其准确性，预测结果与地面真实有限元模拟高度匹配，优于其他基线GNN架构。

Conclusion: RUGNN是一种可靠的方法，能够支持板材成形设计，通过提供准确的制造性预测。

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [51] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Key words: 脊柱手术、住院时间、机器学习、LSTM、注意力机制、可解释AI

TL;DR: 论文提出了一种名为SurgeryLSTM的模型，通过双向LSTM和注意力机制预测脊柱手术后住院时间，显著提升了预测准确性和可解释性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在开发一种能够利用时序数据预测脊柱手术后住院时间的机器学习模型，并强调模型的可解释性对临床决策的重要性。

Method: 比较了传统模型（线性回归、随机森林、SVM、XGBoost）与提出的SurgeryLSTM（带注意力的双向LSTM），使用围手术期结构化电子健康记录数据进行评估。

Result: SurgeryLSTM在预测准确率（R2=0.86）上最优，注意力机制帮助识别关键时序特征。主要预测因子包括骨病、慢性肾病和腰椎融合术。

Conclusion: SurgeryLSTM为脊柱手术住院时间预测提供了高精度且可解释的解决方案，适合集成到临床决策支持系统中。

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [52] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Key words: 不确定性量化,虚拟传感,蒙特卡洛丢弃,分形预测,神经算子

TL;DR: CMCO框架结合蒙特卡洛丢弃和分形预测，为神经算子提供高效的不确定性量化，适用于多种高难度领域的实时虚拟传感。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度学习中不确定性量化（UQ）在实时虚拟传感中的挑战，尤其是在高数据稀疏、噪声或非共位的领域。

Method: 结合蒙特卡洛丢弃和分形预测，构建Conformalized Monte Carlo Operator（CMCO）框架，无需重新训练、集成或定制损失函数即可生成空间解析的不确定性估计。

Result: 在湍流、弹塑性变形和全球宇宙辐射剂量估计三个应用中，CMCO实现了近乎标称的经验覆盖，即使在强空间梯度和代理传感设置下也表现出色。

Conclusion: CMCO为神经算子提供了通用的、即插即用的UQ解决方案，为实时可信推理的科学机器学习奠定了基础。

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [53] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Key words: Einstein Fields, 神经张量场, 数值相对论, 自动微分

TL;DR: 介绍了Einstein Fields，一种用于压缩四维数值相对论模拟的神经表示，通过自动微分生成物理量，展现出在建模连续时空、存储效率和易用性方面的潜力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决数值相对论模拟中计算密集的问题，提供一种紧凑且高效的神经表示方法。

Method: 使用Neural Tensor Fields建模相对论的核心张量场（度量），通过自动微分离线物理量。

Result: 在多个相对论测试案例中表现出色，支持连续时空建模、网格无关性和高效存储。

Conclusion: Einstein Fields为数值相对论提供了更可扩展和表达丰富的方法，并开源了JAX库。

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [54] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Key words: 合成数据,表格数据,隐私保护,分类法,基准框架

TL;DR: 本文综述了合成表格数据生成的最新进展，重点介绍了保持特征关系、统计保真和隐私保护的方法，并提出了基于实际目标的分类法和基准框架。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着隐私法规日益严格，真实数据获取受限，合成数据生成成为关键解决方案，尤其在表格数据领域。

Method: 提出了基于生成目标的新分类法，强调了条件生成和风险敏感建模等方法。

Result: 通过结合理论创新与实际需求，为未来研究和隐私关键环境中的应用提供了指导和基准。

Conclusion: 本文为合成表格数据的研究和实践提供了全面指导，强调实用性与隐私保护的平衡。

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [55] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Key words: 事件时间序列、张量表示、稀疏自动编码器、X射线天文学、异常检测

TL;DR: 提出了一种新的基于张量和稀疏自动编码器的事件时间序列表示方法，用于处理不规则事件数据，并在X射线天文学数据中验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 事件时间序列的不规则性和复杂性使得传统方法难以提取有意义的模式，因此需要新的表示方法和分析框架。

Method: 采用二维和三维张量表示事件时间序列，并结合稀疏自动编码器学习物理意义的潜在表示。

Result: 该方法成功捕捉了事件的时间与频谱特征，并支持多种下游任务，如异常检测和语义聚类。

Conclusion: 该框架为跨科学和工业领域的复杂事件时间序列分析提供了灵活、可扩展的解决方案。

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [56] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Key words: 深度生成模型、工业设计、轮胎架构、扩散模型、条件生成

TL;DR: 该研究比较了五种代表性深度生成模型在工业轮胎架构生成任务中的表现，重点关注无条件生成、组件条件生成和尺寸约束生成三种场景，并提出了一种无需额外训练的类别修复方法。扩散模型整体表现最佳，但具体模型在不同场景下各有优势。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨哪种深度生成模型最适合复杂的工业制造设计任务，尤其是轮胎架构的生成。

Method: 研究了五种模型（VAE、GAN、MMVAE、DDPM、MDM），并提出了类别修复方法以处理条件生成场景。使用几何感知指标评估空间一致性、组件交互、结构连通性和感知保真度。

Result: 扩散模型整体表现最佳；掩码训练的VAE在组件条件任务中优于MMVAE+；MDM在分布内表现优异，而DDPM在分布外尺寸约束任务中泛化能力更强。

Conclusion: 不同模型在不同工业场景中各有优势，扩散模型整体表现最佳，适合实际应用。

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [57] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Key words: grokking, 神经网络, 脱落稳健性, 嵌入相似性, 稀疏性

TL;DR: 本文介绍了用于预测神经网络“顿悟”（grokking）行为的多种实用指标，包括脱落稳健性、嵌入相似性和稀疏性等。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探索神经网络在训练后期测试精度突增的现象（即“顿悟”），并提供可预测该行为的指标。

Method: 通过脱落稳健性曲线（DRC）、测试精度在随机脱落下的方差、非活跃神经元比例和嵌入分布等指标分析“顿悟”过程。

Result: 脱落稳健性和测试精度方差在“顿悟”期间表现出特征性变化，神经元活跃度和嵌入分布也与该行为相关。

Conclusion: 这些指标不仅能预测“顿悟”行为，还揭示了其起源和背后的机制。

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [58] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Key words: 

TL;DR: 论文提出了一种结合零知识证明（ZKP）的联邦学习隐私保护评估协议，避免通过性能指标泄露敏感信息。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决联邦学习在评估阶段可能通过共享性能指标泄露敏感信息的问题。

Method: 采用零知识证明（ZKP），客户端生成简洁证明以断言其本地损失低于预定义阈值，避免暴露原始损失值。

Result: 在MNIST和HAR数据集上验证了方法的可行性，评估了计算开销、通信成本和可验证性。

Conclusion: 提出的协议能够有效保护隐私并实现可验证的联邦学习评估，计算和通信开销可控。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [59] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Key words: 单细胞技术,空间转录组,基于代理的建模,深度学习,图ODE网络,注意力机制

TL;DR: 介绍了STAGED方法，结合基于代理的建模与深度学习，模拟细胞间的通讯及其对细胞内基因调控网络的影响，提升对复杂细胞动态的适应性表示。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统基于代理的建模依赖手工规则，缺乏数据驱动的方法，无法有效学习细胞间复杂的相互作用。

Method: STAGED整合基于代理的建模与深度学习，采用图ODE网络和注意力机制动态学习基因交互强度。

Result: 模型能够捕捉细胞间和细胞内相互作用，更准确地表示细胞动态。

Conclusion: STAGED为学习复杂细胞动态提供了更自适应和准确的方法。

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [60] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Key words: Clifford代数、双向量、转子、线性层、深度学习

TL;DR: 本文研究了线性层中低级几何原语的组合结构，提出使用Clifford代数将线性变换分解为双向量（bivectors）和转子（rotors），仅需O(log^2 d)参数，性能媲美传统方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探究深度学习模型中低级的几何原语如何组合成更高级功能，以理解线性层的构建块。

Method: 使用Clifford代数，将线性层表达为双向量和转子的组合，并开发了一种可微算法进行分解。

Result: 提出的转子基础层在LLM注意力层中表现与块Hadamard和低秩近似等方法相当，且参数更少。

Conclusion: 几何原语能通过代数方式组合成深层模型中的高级功能，为模型设计提供了新视角。

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [61] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Key words: 核心集选择,数据偏差,模型鲁棒性,虚假相关性,样本难度

TL;DR: 研究了核心集选择方法在数据高效机器学习中减少训练数据量时如何影响数据集偏差和模型鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 探讨数据集缩减方法是否会加剧或缓解数据偏差，以及如何影响模型性能。

Method: 在十个不同虚假相关性基准上，使用五种样本重要性/难度评分标准和五种数据选择策略进行实验。

Result: 发现嵌入基样本评分方法相比基于学习动态的方法更不易加剧偏差；困难样本选择可能降低偏差但不保证鲁棒性。

Conclusion: 核心集选择方法需谨慎考虑样本难度与偏差对齐的复杂关系，以平衡偏差和鲁棒性。

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [62] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Key words: 铁路交通、落叶预测、LSTM、卫星数据、生态监测

TL;DR: 英国铁路因落叶导致的交通中断每年损失超过3亿英镑。本研究利用LSTM网络和卫星数据预测落叶时间，优化铁路维护调度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决铁路因落叶导致的交通中断问题，提高预测落叶时间的准确性和可扩展性。

Method: 使用LSTM网络结合地面观测数据和多光谱、气象卫星数据，预测落叶开始和结束时间。

Result: 模型预测落叶开始和结束时间的均方根误差分别为6.32天和9.31天。

Conclusion: 该模型为铁路行业优化落叶缓解措施提供了可靠工具，并有助于理解复杂生态系统。

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [63] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Key words: PbMDPs, Borda scores, 遗憾下界, 全局优化, 策略优化

TL;DR: 本文提出了一种基于偏好的马尔可夫决策过程（PbMDPs）框架，研究了在Borda得分下的遗憾下界，并提出了两种算法，分别基于全局优化和策略优化，实现了T^{2/3}的遗憾上界。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究在偏好观察而非直接损失值观察的情况下，如何在PbMDPs中设计高效的算法以最小化遗憾。

Method: 1. 建立了PbMDPs在Borda得分下的遗憾下界；2. 提出了基于全局优化的算法和策略优化算法；3. 扩展到未知转移概率的情况。

Result: 1. 证明了遗憾下界为Ω((H^2SK)^{1/3}T^{2/3})；2. 提出了两种算法，分别实现了不同的遗憾上界。

Conclusion: 在偏好观察的MDPs中，通过优化算法可以有效降低遗憾，但在状态数较多时仍需进一步的效率改进。

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [64] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Key words: 图神经网络,链接预测,图生成模型,分布外场景,FLEX

TL;DR: 论文提出FLEX框架，结合结构条件图生成和对抗协同训练，提升图神经网络在分布外场景下的链接预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图神经网络在分布外场景下性能受限，且图生成模型的应用领域有限，作者希望填补这一空白。

Method: 提出FLEX框架，通过结构条件图生成和自动编码器与图神经网络的对抗协同训练，实现分布对齐。

Result: 在合成和真实世界分布外场景中，FLEX显著提升了链接预测性能。

Conclusion: FLEX无需专家知识即可在多种分布外场景中发挥作用，并通过图数据增强优化了链接结构。

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [65] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Key words: 负载预测, 全球模型, 数据漂移, 时间序列聚类, 电力传输

TL;DR: 论文探讨了电力传输网络中负载预测的全球模型（GFMs）与传统局部模型（LFMs）的对比，重点分析了数据漂移、异构性及全局化对预测效果的影响，并提出了基于时间序列聚类（TSC）的方法来平衡全局与局部特征。实验表明，全局目标转换模型表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统局部预测模型在泛化性、过拟合、数据漂移和冷启动问题上存在局限性，且计算效率随网络规模下降。全球预测模型通过全球化与交叉学习提供了一种更优的解决方案。

Method: 研究比较了特征转换和目标转换模型，提出了基于时间序列聚类的方法（模型TSC和加权实例TSC）来处理数据异构性。

Result: 全局目标转换模型在实验中表现优于局部模型，尤其在结合全局特征和聚类技术时；而全局特征转换模型需要TSC来有效管理数据异构性。

Conclusion: 全球负载预测模型在数据漂移和异构性下表现优异，TSC方法能有效平衡全局和局部动态，但不同模型对TSC的需求各异。

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [66] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Key words: 图神经网络，初始特征，节点聚类，节点分类，嵌入

TL;DR: 论文提出了一种基于统计方法的图编码嵌入（GEE）来改进图神经网络（GNN）的初始节点特征，结合GNN形成GEE-powered GNN（GG）框架，在节点聚类和分类任务中表现出优异性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统GNN依赖随机或简单初始特征，导致收敛慢和性能受限。论文旨在通过结构感知的初始特征提升GNN性能。

Method: 利用统计方法（one-hot graph encoder embedding, GEE）生成高质量初始节点特征，并将其整合到GNN框架中形成GG。

Result: 实验显示，GG在节点聚类任务中表现最优，收敛更快；在节点分类任务中，增强版GG-C显著优于基线方法。

Conclusion: 结构感知的初始特征对发挥GNN潜力至关重要，GG框架验证了其有效性。

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [67] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Key words: SINDy, Conformal Prediction, Uncertainty Quantification, Ensemble Methods, Nonlinear Dynamics

TL;DR: 论文提出将Conformal Prediction框架集成到Ensemble-SINDy中，以量化模型的不确定性，提高了时间序列预测、特征重要性和模型系数的可靠性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: SINDy方法在非线性动力学建模中应用广泛，但其不确定性量化对安全关键领域至关重要，现有方法存在局限性，故探索Conformal Prediction的应用。

Method: 结合Conformal Prediction与Ensemble-SINDy（E-SINDy），提出三种应用：时间序列预测不确定性量化、基于库特征重要性的模型选择、特征Conformal Prediction量化模型系数不确定性。

Result: 在随机捕食者-猎物动力学和混沌系统中验证了方法的有效性，Conformal Prediction显著改善了预测区间覆盖率、特征重要性评估和模型系数的鲁棒性，尤其在非高斯噪声下。

Conclusion: Conformal Prediction与E-SINDy结合能可靠地实现目标覆盖率，提升模型性能，为非线性动力学建模提供了更稳健的不确定性量化方法。

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [68] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Key words: 药物-靶点相互作用（DTI）、图神经网络（GNN）、传导学习、归纳学习、Graph-in-Graph (GiG)

TL;DR: 本文提出了一种名为Graph-in-Graph (GiG)的新框架，通过结合传导学习和归纳学习，有效整合药物和靶点的分子特征及其相互作用网络，显著提升了药物-靶点相互作用（DTI）预测的准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 准确预测药物-靶点相互作用（DTI）对药物发现和靶点验证至关重要，但现有方法在整合药物、靶点及其相互作用的多样特征方面存在困难。

Method: 提出GiG模型，将药物和靶点的分子结构图表示为药物-靶点相互作用图中的元节点，以探索其复杂关系，并整合传导学习和归纳学习。

Result: GiG模型在所有评估指标上显著优于现有方法，证明了整合不同学习范式和数据的效果。

Conclusion: GiG框架通过整合分子水平和网络水平的特征，提升了DTI预测的准确性，为药物发现提供了新方法。

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [69] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Key words: 分子构象, 玻尔兹曼分布, 生成式机器学习, GFlowNet, 零样本泛化

TL;DR: Torsional-GFN是一种用于生成分子构象的生成式机器学习方法，通过玻尔兹曼分布采样，实现了对未见分子结构和参数的零样本泛化。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在药物发现中，生成稳定的分子构象对于估算分子与靶标的结合亲和力至关重要。传统方法效率较低，因此需要更高效的方法。

Method: Torsional-GFN是一种条件化GFlowNet，通过旋转扭转角生成分子构象，仅依赖于奖励函数进行训练。

Result: 实验证明Torsional-GFN能够近似玻尔兹曼分布采样，并对未见分子结构和参数实现零样本泛化。

Conclusion: 该研究为扩展到更大分子系统、实现零样本泛化以及纳入局部结构生成提供了有前景的途径。

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [70] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Key words: 对比激活加法（CAA）、Llama 2、模型规模、残差流、对齐技术

TL;DR: 本文探讨了对比激活加法（CAA）在不同规模的Llama 2模型（7B、13B和70B）上的有效性，发现CAA在早期至中期层最有效，但随着模型规模的增大效果减弱，且负面引导比正面引导效果更显著。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着大语言模型（LLMs）的复杂性和能力提升，较少广泛部署的对齐技术效果不确定，因此研究CAA在不同规模模型上的表现。

Method: 使用对比对（如从恨到爱）在模型的残差流向量空间中寻找理想方向，并在前向传播时将该方向添加到残差流中，直接操纵残差流以更好地控制模型输出。

Result: 1) CAA在早期至中期层最有效；2) CAA效果随模型规模增大而减弱；3) 负面引导在所有模型规模上效果更显著。

Conclusion: CAA在中小型模型中表现更优，尤其是负面引导效果更突出，但随着模型规模增大效果下降。

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [71] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Key words: 荷兰铁路,延误预测,XGBoost,拓扑特征,节点中心性

TL;DR: 本研究使用XGBoost分类器，结合拓扑特征预测荷兰铁路网络的延误，填补了当前研究在长期预测和网络级模式上的空白。虽然结果显示性能有限，但为交通网络评估提供了新的见解。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 荷兰铁路网络是全球最繁忙的铁路网络之一，延误是一个主要问题。当前研究侧重于短期预测，忽视了网络级模式对缓解连锁反应的重要性。

Method: 研究改进了现有的方法（原用于预测快速变化的美国航空网络），集成节点中心性度量，并比较了多种分类器（如RandomForest、DecisionTree等）以预测延误。

Result: 结果显示模型性能有限，特别是在非同步测试场景下，表明需要更多针对具体情境的改进。

Conclusion: 尽管性能有限，但研究为交通网络延迟预测提供了新方向，并提出了未来开发更强大模型的建议。

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [72] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Key words: FlatVI, 变分自编码器, 单细胞 RNA 测序, 欧几里得几何, 流形插值

TL;DR: FlatVI 是一种新的训练框架，通过正则化离散似然变分自编码器的潜在流形，使其更接近欧几里得几何，从而增强单细胞 RNA 测序数据建模的轨迹重建和流形插值能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的方法在单细胞 RNA 测序中常用变分自编码器建模细胞状态转换，并假设线性位移和欧几里得几何，但这种假设在潜在空间中的线性插值可能与数据流形上的测地线路径不一致，限制了方法的有效性。

Method: 引入 FlatVI 训练框架，通过正则化潜在流形，使其符合欧几里得几何，特别适用于单细胞计数数据建模。

Result: 在合成数据上验证了方法的理论合理性，应用于时间分辨单细胞 RNA 测序数据时，改善了轨迹重建和流形插值效果。

Conclusion: FlatVI 提高了与假设欧几里得几何的下游方法的兼容性，尤其在单细胞数据建模中表现出优越性。

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [73] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Key words: 噪声标签学习,元学习,跨层信息差异,深度学习

TL;DR: 论文提出了一种不需要干净标注数据的元学习方法CLID-MU，通过跨层信息差异策略解决噪声标签问题。该方法利用数据结构的跨层一致性，在噪声环境下表现优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有元学习方法依赖干净标注的元数据集，但实际中难以获取。因此，研究无需干净数据的元学习方法具有重要价值。

Method: 提出CLID-MU策略，利用最后一隐藏层和输出层数据结构的跨层一致性来衡量模型性能，并指导训练。

Result: 在合成和真实噪声环境下，CLID-MU在基准数据集上优于现有方法。

Conclusion: CLID-MU为噪声标签下的元学习提供了有效解决方案，无需干净标注数据。

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [74] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Key words: 分子生成, 3D分子设计, 可合成性, SynCoGen

TL;DR: SynCoGen是一个结合掩码图扩散和流匹配的单框架，用于可合成的3D分子生成，在无条件和零-shot分子设计中表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决可合成分子生成中的几何条件限制问题。

Method: 结合掩码图扩散和流匹配，生成分子构建块、化学反应和原子坐标的联合分布。

Result: 在无条件和零-shot分子设计中达到SOTA性能。

Conclusion: 多模态框架为未来分子生成应用奠定基础。

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [75] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Key words: MNIST-Gen, 数据集生成, CLIP, 强化学习, 语义分类, 自定义数据集

TL;DR: MNIST-Gen是一个自动化、模块化的框架，用于生成特定领域的MNIST风格数据集，结合了CLIP语义理解和强化学习，支持复杂分类任务。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有标准数据集（如MNIST）对领域特定任务（如树木或食物分类）不适用，且自定义数据集创建耗时且受法律限制。

Method: 系统通过CLIP语义理解和强化学习实现智能分类，支持分层语义结构和多种处理模式，如批量处理或快速生成。

Result: 生成的两个新数据集（Tree-MNIST和Food-MNIST）展示了其有效性，自动分类准确率达85%，节省80%时间。

Conclusion: MNIST-Gen为领域特定任务提供了高效、灵活的定制数据集生成工具。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [76] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Key words: 动态图, 动态链接预测, 超事件, 事件相关性, 并行训练

TL;DR: HyperEvent 框架通过将动态链接预测重构为超事件识别，利用事件相关性向量动态构建关联序列，显著提升了预测性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法无法捕捉复合超事件的结构凝聚力，HyperEvent 旨在解决这一问题。

Method: 通过事件相关性向量动态构建关联序列，评估查询事件与历史事件是否形成有效超事件。

Result: 在5个数据集中的4个超越现有方法，大规模Flight数据集上MRR提升6.95%，训练时间仅用10.17%。

Conclusion: HyperEvent 在准确性和效率上均优于现有方法，适用于大规模动态图。

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [77] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Key words: 蛋白质结构预测, 轻量级模型, ODE 采样, 剪枝, ESM 模块

TL;DR: Protenix-Mini 是一个轻量级的蛋白质结构预测模型，通过替换多步采样器、剪枝冗余模块和使用 ESM 模块替代传统 MSA 预处理，显著降低计算开销，同时保持了较高的预测准确性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对生物分子结构预测中的计算效率问题，研究旨在平衡模型效率和预测准确性，以满足实际应用的需求。

Method: 1) 用少步 ODE 采样器替代多步 AF3 采样器；2) 剪枝不贡献预测结果的冗余模块；3) 引入 ESM 模块替代传统 MSA 预处理。

Result: Protenix-Mini 在基准数据集上的性能仅比完整模型下降 1-5%，同时显著降低了模型复杂度。

Conclusion: Protenix-Mini 适合计算资源有限但需要高精度预测的应用场景。

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [78] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Key words: 广义线性老虎机、在线镜像下降、混合损失、统计效率、计算效率

TL;DR: 该论文提出了一种针对广义线性老虎机问题的联合高效算法，能够在每轮以恒定时间和空间复杂度实现近乎最优的遗憾界限。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 广义线性老虎机问题因其非线性特性，在计算和统计效率之间存在显著权衡，现有方法通常无法同时满足这两个目标。

Method: 基于在线镜像下降估计器的紧置信集，通过混合损失的新颖分析，实现统计效率最大化并与最大似然估计相当。

Result: 提出的算法在每轮以恒定时间和空间复杂度下，达到了近乎最优的遗憾界限。

Conclusion: 该算法通过联合优化计算和统计效率，为广义线性老虎机问题提供了高效解决方案。

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [79] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Key words: 特征归因,OrdShap,序列模型,深度学习

TL;DR: OrdShap是一种新的特征归因方法，通过分解特征值和特征位置的效果，提供更清晰的模型预测解释。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有特征归因方法假设固定特征顺序，无法区分特征值和位置的影响。

Method: 提出OrdShap方法，通过置换特征位置量化模型预测变化。

Result: 在健康、自然语言和合成数据集中验证了OrdShap的有效性。

Conclusion: OrdShap能够更好地捕捉特征值和位置的归因，增进对模型行为的理解。

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [80] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Key words: 平台整合；折扣快车；动态管理；pi-DDPG；在线学习

TL;DR: 通过整合多个网约车平台到一个应用，减少市场碎片化。第三方提供折扣快车服务满足差异化需求，但可能降低平台利润。研究提出pi-DDPG框架动态管理司机接受折扣服务的行为。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决市场碎片化和满足乘客多样化需求，同时平衡平台利润和司机参与折扣服务的冲突。

Method: 提出pi-DDPG框架，包含精炼模块、卷积LSTM网络和优先经验回放机制。

Result: pi-DDPG在模拟实验中表现出高效学习能力，显著减少早期训练损失。

Conclusion: pi-DDPG能有效管理司机行为，提升平台运营效率。

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [81] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Key words: 不平衡回归, 元学习, 重采样策略, 链式推荐, AutoML

TL;DR: 提出了一种元学习框架Meta-IR，用于推荐不平衡回归任务中的最佳预处理和模型组合，优于现有方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 针对不平衡回归问题中数据分布不均的挑战，研究者试图通过自动推荐最佳预处理策略和学习模型来提升性能。

Method: 提出了Meta-IR框架，通过元分类器在零样本推荐中结合重采样策略和学习模型。包括独立式和链式两种方法。

Result: 链式方法表现更优，Meta-IR在实验中胜过AutoML框架和42种基线配置。

Conclusion: Meta-IR框架能有效解决不平衡回归问题，尤其在链式方法中表现出更强的推荐能力。

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [82] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Key words: 不平衡回归, 分类法, 平衡算法, 预测模型, 评估指标

TL;DR: 该论文研究了不平衡回归问题，通过实验比较了多种平衡和预测模型，并提出了基于回归模型、学习过程和评估指标的分类法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决实际应用中连续目标值的不平衡问题，提出分类法并评估不同方法的有效性。

Method: 通过实验研究，结合多种平衡算法和预测模型，并使用特定指标评估性能。

Result: 研究揭示了平衡策略对各模型学习过程的优势，并提出了进一步研究的方向。

Conclusion: 不平衡回归问题需要针对性的策略，研究为未来提供了新的视角和方法。

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [83] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Key words: 可复制学习,强化学习,样本效率,探索性学习,MDP

TL;DR: 本文研究了可复制强化学习中的样本效率问题，填补了生成模型与非生成模型之间的性能差距，提出了一种样本高效的算法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究强化学习中可复制算法的样本效率问题，探索探索性学习是否比批量学习更昂贵。

Method: 设计了一种可复制的强化学习算法，在低水平表格MDP中实现了样本高效的探索。

Result: 算法的样本复杂度为～O(S²A)，填补了生成与非生成模型之间的性能差距，并证明了其接近最优性。

Conclusion: 探索性学习并非可复制学习的主要障碍，样本可复制的强化学习是可行的。

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [84] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Key words: 机器学习,射频功放,优化框架,拉丁超立方采样,CatBoost

TL;DR: 论文提出一种机器学习加速的射频功率放大器设计优化框架，通过智能采样减少65%的仿真需求，同时保持±0.3至±0.4 dBm的精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统方法需要穷举所有参数组合，仿真成本高且耗时，亟需一种高效且精准的优化方法。

Method: 结合MaxMin拉丁超立方采样与CatBoost梯度提升，智能筛选35%的关键仿真点，通过ADS网表处理和训练模型预测性能。

Result: 在15种功放模式下验证，平均R²为0.901，仿真时间减少58.24%至77.78%。

Conclusion: 框架显著提升设计迭代速度，满足生产级射频电路精度要求。

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [85] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Key words: GPU, 强化学习, CUDA内核, 性能优化

TL;DR: 论文提出了一种多轮强化学习（RL）方法用于CUDA内核生成和优化，显著提高了内核的正确性和性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: GPU内核编写对AI系统效率至关重要，但其过程具有高迭代性，适合应用RL。

Method: 开发了多轮RL训练配方，解决了长轨迹学习和跨轮奖励归因等挑战。

Result: Kevin模型在正确性上从56%提高到82%，平均加速比从0.53x提升到1.10x。

Conclusion: 多轮RL在CUDA内核优化中表现优异，串行细化比并行采样更有效。

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [86] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Key words: 增强学习,神经网络剪枝,随机优化,DenseNet,OFENet

TL;DR: 提出了一种结合训练与剪枝的方法，用于优化增强学习中的神经网络计算复杂度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决深度神经网络在增强学习中计算和内存复杂度高的问题。

Method: 通过随机优化问题训练网络权重和变分伯努利分布的参数，实现动态剪枝。

Result: 在MuJoCo基准测试中，显著剪枝网络且性能损失极小。

Conclusion: 训练时剪枝比从头训练小网络更高效且性能更高。

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [87] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Key words: 图欺诈检测, 图神经网络, 大型语言模型, 多模态融合

TL;DR: 该论文提出了一个多级LLM增强的图欺诈检测框架MLED，通过利用大型语言模型（LLM）提取文本信息的外部知识，增强现有图欺诈检测方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有图欺诈检测方法通常忽略原始文本信息中的丰富语义线索，且难以将处理的文本嵌入与图结构进行多模态融合。

Method: 设计了多级LLM增强框架，包括类型级增强器和关系级增强器，分别增强欺诈者与良性实体的差异以及欺诈者在不同关系中的重要性。

Result: 在四个真实世界数据集上的实验表明，MLED作为可应用于现有方法的通用框架，在图欺诈检测中达到了最先进的性能。

Conclusion: MLED通过融合LLM的外部知识和图结构信息，显著提升了欺诈检测的能力，为现有方法提供了有效的扩展方向。

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [88] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Key words: 社交互动,多模态数据,智能手表,机器学习,深度学习,对话检测

TL;DR: 提出一种利用智能手表的多模态数据（音频和惯性）检测人类面对面对话的新方法，并通过实验验证其在复杂声学环境下的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 社交互动对人类行为和社会关系至关重要，但现有方法在复杂声学环境下效果有限，需要一种更高效的检测方法。

Method: 结合音频和惯性数据，使用机器学习和深度学习模型，采用三种融合方法，通过实验室和半自然实验验证效果。

Result: 实验室环境下检测对话的F1分数为82.0±3.0%，半自然环境下为77.2±1.8%。

Conclusion: 多模态数据融合显著提升了对话检测的准确性，尤其在复杂场景中表现突出。

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [89] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Key words: 自动调制识别, 数据稀缺, 动态不确定性, 主动学习, 跨架构泛化

TL;DR: 提出了一种名为DUSE的动态不确定性驱动样本扩展框架，用于解决自动调制识别（AMR）任务中数据稀缺的问题，通过不确定性评分和主动学习策略提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 深度神经网络在AMR任务中表现优异，但依赖大量标注数据。实际场景中目标域数据稀缺且标注成本高，常规数据增强无法从根本上解决问题。

Method: 采用动态不确定性驱动样本扩展（DUSE）框架，通过不确定性评分函数筛选有用样本，并结合主动学习优化评分器。

Result: 实验表明DUSE在8种核心集选择基线方法中表现最优，且在类别平衡和不平衡场景下均有效，具备跨架构泛化能力。

Conclusion: DUSE为数据稀缺问题提供了有效解决方案，显著提升了AMR任务的性能。

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [90] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Key words: human feedback, feedback granularity, regularized averaging, population distribution, empirical analysis

TL;DR: 研究探讨了在有限的人类反馈样本下，如何更精确地预测群体反馈分布。结果表明，反馈的粒度越高，复杂方法的优势越明显，尤其是在五级反馈下，性能提升显著。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 随着人类反馈在AI训练、推荐系统等领域的广泛应用，如何在有限样本下更精确地估计群体反馈分布成为关键问题。

Method: 通过比较正则化平均方法与更复杂的组合方法，分析了不同反馈粒度下的预测效果。

Result: 五级反馈下，复杂方法所需样本量减少一半即可达到相同性能；而二进制反馈下，性能提升有限。

Conclusion: 反馈粒度越高，越需采用复杂方法来优化预测效果，从而显著降低样本需求。

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [91] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Key words: 持续学习, 泛化行为, 信息论界限, 回放方法, 灾难性遗忘

TL;DR: 该论文提出了一个统一的理论框架，用于分析基于回放的持续学习方法，揭示了内存缓冲区与当前任务如何交互影响泛化性能，并通过实验验证了理论的有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 持续学习在避免灾难性遗忘的同时从顺序任务中获取知识，但其泛化行为的理论理解仍有限，特别是基于回放的方法。

Method: 建立了一个统一的理论框架，推导了一系列信息论界限，明确描述了内存缓冲区与当前任务的交互如何影响泛化。

Result: 理论分析表明，有限的前任务样本与当前任务数据结合能提升泛化性能并减少遗忘，实验验证了理论的有效性。

Conclusion: 提出的理论框架广泛应用于多种学习算法，为基于回放的持续学习提供了理论支持。

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [92] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Key words: 城市移动模式、生成模型、条件生成对抗网络、数据驱动、土地利用

TL;DR: 本文提出了一种新的数据驱动方法，用于生成模拟城市场景中的起源-目的地流动模式，结合了动态区域大小和土地利用类型等适应性因素，并通过条件生成对抗网络（cGANs）实现高效生成。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 城市中人口的移动模式随土地利用和人口变化而演变，而现有模型多依赖历史轨迹或假设静态场景，难以适应未来预测需求。

Method: 采用条件生成对抗网络（cGANs），结合动态区域大小和土地利用类型等适应性参数，快速生成可调整空间粒度的流动模式。

Result: 在新加坡手机数据上的应用表明，该方法性能优越，且无需大量校准数据或复杂行为建模。

Conclusion: 该方法为城市规划和交通优化提供了一种高效、灵活的解决方案，适用于未来场景预测。

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [93] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Key words: 表示对齐, 自编码器, 激活函数, 离散表示, 代数对称性

TL;DR: 本文提出了一种基于Spotlight Resonance方法的新方法，用于确定表示对齐关系，发现网络原语的代数对称性是任务无关表示结构的强预测因子。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究如何通过改变激活函数来探究自编码器中离散表示的形成与排列，验证功能形式选择对表示结构的潜在影响。

Method: 通过消融实验，仅改变激活函数的定义（离散代数置换对称性vs连续代数正交对称性），分析表示的结构变化。

Result: 在离散代数置换对称性下，表示倾向于离散化；而在连续代数正交对称性下，表示保持连续。这表明功能形式的选择会引入意外的归纳偏差。

Conclusion: 功能形式对表示结构有显著影响，可能导致离散化效应，这为理解下游解释性现象提供了新视角。

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [94] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Key words: 预测模型, 信息性差距, 校准, 决策任务

TL;DR: 论文提出了一个信息性差距的概念，用于比较两个预测模型在下游决策任务中的效用，并提出了一个双特征化方法，将其与现有理论统一。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 研究多种预测模型可能都存在校准问题的情况下，如何通过信息性差距来衡量哪个模型在决策任务中更有用。

Method: 提出了信息性差距的概念，并通过双特征化方法推导出一个自然的信息性度量，类似于松弛的地球移动距离。

Result: 证明该度量满足完备性和可靠性，并能高效估计，同时扩展到完美校准的预测器上。

Conclusion: 提出的框架统一了现有几种概念，为预测模型的实用性比较提供了新思路。

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [95] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Key words: 联邦学习,频谱感知,数据投毒,半监督学习,动态频谱分配

TL;DR: 论文提出了一种基于联邦学习的半监督频谱感知方法，通过结合能量检测解决标签数据不足问题，并提出疫苗启发的防御机制对抗数据投毒攻击，实验验证了其高效性和鲁棒性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 无线设备的快速增长加剧了频谱稀缺问题，传统集中式机器学习方法受隐私和带宽限制，联邦学习成为有前景的替代方案，但面临标签数据不足和安全漏洞的挑战。

Method: 采用半监督联邦学习方法结合能量检测处理标签数据不足；针对数据投毒攻击，提出疫苗启发的防御机制，优于现有多数表决防御。

Result: 在合成和真实数据集上的实验表明，该方法在无标签数据集上接近完美准确率，并能抵抗高比例恶意参与者的数据投毒攻击。

Conclusion: 半监督联邦学习结合疫苗式防御机制有效解决了频谱感知中的标签数据不足和安全问题，为动态频谱分配提供了可行方案。

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [96] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Key words: 设备识别, RFFI, HyDRA, VMD, 深度学习

TL;DR: HyDRA是一种混合双模式RF架构，结合优化的VMD和多组件融合的深度学习模型，用于无线通信设备识别，支持闭集和开集分类。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决无线通信系统中设备识别的安全问题，提出非密码学解决方案RFFI。

Method: HyDRA结合优化的VMD预处理、CNN、Transformer和Mamba组件，利用TDSE和MLFE处理信号。

Result: 在公共数据集上实现闭集SOTA精度和开集鲁棒性能，支持实时低功耗推理。

Conclusion: HyDRA为实时无线认证提供高效、实用的解决方案。

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [97] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Key words: LoRA, 参数高效微调, 黎曼优化, 光滑流形

TL;DR: 提出了RiemannLoRA方法，通过将LoRA矩阵视为光滑流形上的元素，解决初始化问题和过参数化问题，实验证明其优于标准LoRA。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: LoRA在高效微调大语言模型时面临初始化策略和过参数化的挑战，需要统一解决方案。

Method: 将固定秩LoRA矩阵视为光滑流形，通过流形上的优化确定初始化和减少过参数化，并结合数值线性代数和黎曼优化技术。

Result: 在LLM和扩散模型上，RiemannLoRA在收敛速度和最终性能上均优于标准LoRA及其现有改进。

Conclusion: RiemannLoRA通过流形优化方法有效解决了LoRA的挑战，提升了性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [98] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Key words: FourCastNet 3, 全球天气建模, 机器学习, 概率集合预报, 卷积神经网络

TL;DR: 四篇论文介绍了FourCastNet 3，一种基于几何机器学习的全球天气建模方法，具有高精度和高效率。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 旨在通过机器学习方法改进全球天气预测，解决传统方法在计算效率和精度上的不足。

Method: 采用纯卷积神经网络架构，结合球形几何和新型训练范式（模型和数据并行）。

Result: 预测精度超越传统模型，计算速度提高8至60倍，并且具有出色的概率校准能力。

Conclusion: FourCastNet 3是气象预测和大规模集合预报的理想工具。

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [99] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Key words: 基础模型,Transformer,边缘计算,通信效率,自注意力

TL;DR: PRISM提出了一种通信效率和计算感知的分布式Transformer推理策略，显著降低边缘设备间的通信和计算开销，适用于资源受限环境。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 基础模型在边缘部署面临通信和计算挑战，PRISM旨在解决这些问题。

Method: 采用Segment Means近似中间特征，优化自注意力机制和分区感知的因果掩码。

Result: 通信开销降低99.2%，计算减少51.24%，准确率仅轻微下降。

Conclusion: PRISM为分布式边缘环境中的基础模型部署提供了可扩展且实用的解决方案。

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [100] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Key words: 多组件数据集,生成模型,高斯马尔可夫随机场,结构一致性

TL;DR: 论文提出了一种名为GMRF MCVAE的新生成框架，通过嵌入高斯马尔可夫随机场来建模多组件数据集中的复杂依赖关系，显著提高了生成组件的结构一致性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有方法在处理多组件数据集时依赖简化的聚合策略，忽略了关键细节，导致生成组件的结构一致性不足。

Method: 使用高斯马尔可夫随机场嵌入先验和后验分布，显式建模组件间的关系。

Result: 在合成Copula数据集、PolyMNIST基准和真实BIKED数据集上表现优异，尤其在结构一致性方面显著提升。

Conclusion: GMRF MCVAE适用于需要强大多组件一致性建模的实际应用。

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [101] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Key words: 无线电地图,3D数据集,扩散模型,环境感知通信

TL;DR: 论文提出了一种名为UrbanRadio3D的大规模高分辨率3D无线电地图数据集，并通过扩散模型框架RadioDiff-3D解决了现有方法在3D空间和多参数预测上的不足。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有的无线电地图构建方法大多局限于2D平面的路径损耗预测，忽略了方向到达（DoA）、时间到达（ToA）等关键参数和垂直空间变化，限制了其泛化能力。

Method: 通过射线追踪在真实城市环境中构建UrbanRadio3D数据集，并提出基于3D卷积的UNet和扩散模型框架RadioDiff-3D进行建模。

Result: UrbanRadio3D数据集比现有数据集大37倍，覆盖3D空间和多参数；RadioDiff-3D在构建高维无线电地图方面表现优异。

Conclusion: 该研究为3D环境感知通信提供了基础数据集和基准，推动了未来相关研究的发展。

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [102] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Key words: 无监督分类, 证据聚类, 解释性, 决策树, IEMM算法

TL;DR: 本文探讨了证据聚类中结果解释的问题，提出了基于代表性的方法，并通过效用函数和IEMM算法验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现实世界数据常具不确定性，传统方法难以处理，证据聚类虽能解决此类问题，但其结果解释问题仍未充分研究。

Method: 通过代表性条件及效用函数定义“可容忍”错误，提出IEMM算法生成可解释的决策树。

Result: 在合成和真实数据上验证，93%的情况下能提供令人满意的解释。

Conclusion: 基于代表性的IEMM算法为证据聚类提供了解释性强的解决方案。

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [103] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Key words: 量化, 选择性量化, ONNX, 多目标优化, 性能分析

TL;DR: TuneQn是一个工具套件，通过选择性量化和多目标优化，在降低模型大小和计算需求的同时，最小化精度损失。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决全量化模型在低端硬件上部署时的性能下降问题，需选择性地量化部分层以平衡精度和效率。

Method: 提出TuneQn套件，结合选择性量化、多目标优化和性能分析，生成并优化ONNX模型。

Result: 实验显示，选择性量化能减少54.14%的精度损失和72.9%的模型大小。

Conclusion: TuneQn有效优化了量化模型的部署性能，适用于不同硬件平台。

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [104] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Key words: 公平性, 概念擦除, 正交投影, 神经网络, NLP

TL;DR: 论文提出了一种概念擦除方法$ar{mathrm{L}}$EOPARD，通过在嵌入空间中学习正交投影来移除敏感信息，同时保留语义信息，实现了公平性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在神经网络应用中，防止从文本表示中推断敏感信息（如性别或种族）是一个关键挑战，以保障公平性。

Method: 通过概念擦除，学习嵌入空间中的正交投影，使得离散概念的条件特征分布不可区分，同时严格保留嵌入的局部结构。

Result: $ar{mathrm{L}}$EOPARD在经典NLP基准测试中实现了最先进的非线性擦除性能，并有效减少了深度非线性分类器的偏见。

Conclusion: 该方法在移除敏感信息和促进公平性方面表现出色，具有实际应用潜力。

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [105] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Key words: 物理信息线性模型,偏微分方程,正演问题,反演问题,正则化

TL;DR: 本研究探讨了一种物理信息线性模型（PILM），通过基函数的线性组合表示解，从而实现了最优解的解析表示。PILM在正演和反演问题中得到了验证，并应用于地壳应变率的估计。在物理与数学正则化的比较中，数学正则化表现更优。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为了更有效地解决偏微分方程（PDEs）及其系数或边界条件的估计问题，研究提出了一种基于线性模型的物理信息方法。

Method: 采用物理信息线性模型（PILM），利用基函数的线性组合表示解，并在正演和反演问题中进行验证。

Result: PILM成功应用于地壳应变率的估计，数学正则化在贝叶斯视角下表现优于物理正则化。

Conclusion: PILM为线性正演和反演问题、欠定系统及物理正则化提供了可解析求解的框架。

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [106] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Key words: 深度神经网络, 优化器, 归纳偏差, 收敛性, 模型表达能力

TL;DR: 论文探讨了深度神经网络（DNN）优化器不仅影响收敛速度，还影响学习解的性质，呼吁社区关注优化器设计中的归纳偏差和模型表达能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 早期的怀疑认为DNNs因非线性特性无法保证收敛到全局最优解，但经验表明大型DNNs能通过标准训练协议表现良好。然而，现有研究过于关注训练效率，而忽视了优化器对学习解性质的影响。

Method: 通过分析和综述现有优化器的作用，提出优化器设计应考虑其对解性质的影响，而非仅关注收敛速度。

Result: 优化器不仅是收敛工具，还能通过编码归纳偏差影响模型的表达能力和解的性质。

Conclusion: 社区应深入理解现有优化器的偏差，并设计新的优化器以引导特定解的性质，从而更全面地影响模型结果。

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [107] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Key words: 因果发现、时间序列、幂律分布、噪声鲁棒性、频谱特征

TL;DR: 论文提出了一种基于幂律谱特征的稳健因果发现方法，能够在高噪声环境中更准确地识别因果关系。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有因果发现算法对噪声敏感，容易在真实数据中产生误导性结果，而真实世界时间序列的频谱常遵循幂律分布，这为解决该问题提供了新思路。

Method: 通过提取幂律频谱特征来增强真实因果信号，构建了更稳健的因果发现方法。

Result: 该方法在合成基准和真实数据集上均优于现有最先进方法，表现出较高的鲁棒性和实用性。

Conclusion: 利用幂律特征可以有效提升因果发现的准确性和鲁棒性。

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [108] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Key words: 高斯过程,非平稳核,神经网络,非参数回归

TL;DR: 提出了一种使用非平稳核的高斯过程框架，通过神经网络参数化核参数，提高了模型的灵活性和表现力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 高斯过程在非参数回归中表现优异，但传统方法使用平稳核限制了模型的表达能力，需要一种更灵活的非平稳核框架。

Method: 通过神经网络动态调整非平稳核参数，并与高斯过程联合训练，利用链式法则计算导数。

Result: 在多个数据集上测试表明，该方法在准确性和对数分数上优于平稳模型和变分推断的分层模型。

Conclusion: 所提框架能有效捕捉非平稳性，且易于扩展到不同核和大型数据集。

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [109] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Key words: 持续学习, 模型合并, 多领域适应, Segment Anything Model, 参数效率

TL;DR: 该论文提出了一种名为RegCL的非回放持续学习框架，用于通过模型合并高效整合多领域知识，解决了Segment Anything Model (SAM)在特定领域中的性能限制问题。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 为克服SAM在特定领域中的性能限制及其适配器方法在多领域应用时的灾难性遗忘问题，需要一种更灵活且高效的持续学习框架。

Method: RegCL通过将模型合并算法融入持续学习范式，优化权重以减少预测差异，从而整合多领域知识，同时保持参数高效性。

Result: 实验证明，RegCL在多个下游数据集上表现出良好的持续学习性能，验证了其在动态场景中的有效性。

Conclusion: RegCL通过模型合并成功整合多领域知识，避免了灾难性遗忘，且无需存储历史数据，适用于动态场景。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [110] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Key words: 在线持续学习,提示方法,数据隐私,灾难性遗忘

TL;DR: 提出了一种基于提示的新方法，用于在线持续学习（OCL），通过轻量级提示生成器和可训练缩放-移位组件，解决了数据隐私和高参数成本的限制，并在多个数据集上表现优异。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 解决在线持续学习（OCL）中数据隐私限制和灾难性遗忘问题，同时避免现有方法的参数增长和数据开放政策限制。

Method: 结合轻量级提示生成器、可训练缩放-移位组件、预训练模型泛化保持和硬-软更新机制。

Result: 在CIFAR100、ImageNet-R、ImageNet-A和CUB数据集上表现显著优于当前SOTA方法，且参数量更少，训练和推理时间适中。

Conclusion: 提出了一种高效且实用的在线持续学习方法，解决了参数和吞吐量问题，同时保护了数据隐私。

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [111] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Key words: 大型推理模型, 链式思维攻击, 防御机制, 强化学习, 人工智能安全

TL;DR: 论文提出了Thought Purity (TP)防御范式，以解决大型推理模型（LRMs）在链式思维（CoT）生成过程中对安全威胁的脆弱性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 当前强化学习训练的大型推理模型（如Deepseek-R1）在推理能力上表现优异，但其在链式思维生成过程中易受安全威胁（如后门提示攻击），导致推理机制被系统性破坏。

Method: 提出了Thought Purity (TP)防御范式，包含三个协同组件：安全优化的数据处理流程、强化学习增强的规则约束和自适应监控指标。

Result: 该方法首次建立了针对链式思维攻击（CoTA）的全面防御机制，显著提升了安全与功能之间的平衡。

Conclusion: TP范式为下一代AI架构提供了兼顾安全性和功能性的解决方案。

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [112] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Key words: 拓扑神经网络,组合复合体,拉普拉斯算子,热核,分子分类

TL;DR: 本文提出了一种新颖的拓扑框架，通过引入组合复合体（CCs）上的拉普拉斯算子，高效计算热核作为节点描述符，解决了拓扑神经网络计算成本高的问题。该方法理论表达力强，实验表现优于现有方法，并在分子分类任务中展现了卓越性能。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 传统拓扑神经网络因高阶消息传递导致计算成本高昂，因此需要一种高效且表达能力强的替代方案。

Method: 引入组合复合体上的拉普拉斯算子，通过计算热核生成节点描述符，支持多尺度信息捕捉和置换等变表示。

Result: 理论上有最大表达能力，能够区分任意非同构组合复合体；实证结果显示计算效率显著优于现有方法，在分子数据集上表现优异。

Conclusion: 该方法为拓扑深度学习提供了高效且表达能力强的表示，有望推动分子分类和性质预测任务的进展。

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [113] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Key words: 强化学习, PAC边界, 马尔可夫决策过程, 样本复杂度, 模型无关算法

TL;DR: 该研究在无限时间范围的马尔可夫决策过程(MDP)中提出了比现有文献更尖锐的样本复杂度PAC边界。通过状态间的相关性分析，减少了样本复杂度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 动机在于利用状态间的距离相关性，减少学习$ϵ$-最优值所需的样本复杂度。

Method: 方法包括将原始MDP近似为较小的子MDP，并在模型无关设置下构建PAC-MDP算法。

Result: 结果将样本复杂度减少至$O(SA ⁠\log A)$时间步，优于现有工作。

Conclusion: 结论显示该方法显著提升了样本效率，并通过实验验证了其优越性。

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [114] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Key words: 人工智能,树模型,硬件-软件协同设计,$MoS_2$闪存,CAM,鲁棒性

TL;DR: 论文提出了一种基于$MoS_2$闪存模拟内容寻址存储器（CAM）的软树模型硬件-软件协同设计方法，显著提升了模型的可解释性和鲁棒性，同时保持了高效推理能力。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 人工智能的快速发展引发了对模型可解释性和鲁棒性的关注。尽管树模型在表格数据上表现优异，但其扩展性受限于计算成本高和硬件实现难度大。

Method: 采用$MoS_2$闪存模拟CAM的软边界特性，设计软树模型，实现了高效推断并解决了传统树模型硬件实现中的问题。

Result: 实验表明，该方法在威斯康星乳腺癌症诊断数据集（WDBC）上达到96%的准确率，且在MNIST数据集上仅因10%设备阈值变化导致0.6%的准确率下降。

Conclusion: 该研究为提升AI的可信度和效率提供了专用硬件的新思路。

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [115] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Key words: 测试时扩展，BoN，拒绝采样，ROC曲线，语言模型

TL;DR: 该研究分析了测试时扩展方法（如BoN和拒绝采样）在验证器不完美情况下的性能，发现其准确性取决于验证器ROC曲线的几何特性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 填补现有研究中关于验证器不完美如何影响测试时扩展性能的理论空白。

Method: 通过理论分析验证器ROC曲线的几何特性对BoN和拒绝采样性能的影响，并进行实验验证。

Result: 拒绝采样在固定计算量下优于BoN，但两者在无限计算量下收敛于相同准确性，由ROC曲线原点处的斜率决定。

Conclusion: 验证器ROC曲线的局部和全局特性分别决定了拒绝采样和BoN的性能，且无法从低计算量推断拒绝采样的表现。

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [116] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Key words: 资源约束, 动态信息获取, 成本权衡, 医疗数据, 时间预测

TL;DR: NOCTA是一种非贪婪的目标成本权衡获取方法，用于在资源受限的预测任务中动态获取最有价值的信息，并在医疗数据上表现优于基线方法。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在资源受限的应用（如医疗）中，动态获取信息以优化成本和时间至关重要。

Method: NOCTA包括两种互补的估计器：基于最近邻的非参数方法（NOCTA-NP）和直接预测潜在获取效用的参数方法（NOCTA-P）。

Result: 实验表明，NOCTA在合成和真实医疗数据集上均优于现有基线方法。

Conclusion: NOCTA能有效权衡信息获取成本与预测性能，适用于复杂的时间动态任务。

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [117] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Key words: Mixture of Experts, computational graphs, dynamic selection, training efficiency

TL;DR: 论文提出了一种“Mixture of Raytraced Experts”架构，通过动态选择专家序列，生成可变宽度和深度的计算图，提升预测精度。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现有MoE架构对每个样本需固定计算量，而新方法通过循环选择专家，逐步提高预测精度。

Method: 采用堆叠MoE架构，通过采样候选专家序列进行训练，无需负载平衡机制。

Result: 实验表明训练周期减少10%-40%，精度持平或更高。

Conclusion: 该方法为MoE领域开辟了新方向，有助于设计更快、表达能力更强的模型。

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [118] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Key words: 因果推断,深度神经网络,TMLE,目标梯度,多参数估计

TL;DR: 提出了一种名为TDA的新框架，将TMLE直接嵌入神经网络的参数空间，解决传统神经网络在因果推断中的不足，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 现代深度神经网络在预测方面表现强大，但在因果参数推断（如治疗效果或生存曲线）上缺乏有效的推断方法。现有方法存在计算成本高或无法保证解决高效影响函数方程的问题。

Method: TDA通过将TMLE嵌入网络参数空间，冻结大部分参数并迭代更新小部分“目标”参数，利用目标梯度消除一阶偏差，生成渐近有效的置信区间。

Result: 在IHDP数据集和模拟生存数据中，TDA显著减少了偏差并提高了覆盖率，优于标准神经网络和现有方法。

Conclusion: TDA为复杂多参数目标的因果推断提供了直接且可扩展的解决方案。

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [119] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Key words: 联邦学习、数据投毒、激励机 制、贝叶斯博弈

TL;DR: 提出了一种轻量级贝叶斯激励机制，作为联邦学习中的主动经济防御手段，通过经济理性抑制恶意行为。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 联邦学习的开放参与特性使其易受数据投毒攻击，现有防御方法多为被动且计算昂贵，本文旨在提供一种经济实用的解决方案。

Method: 设计了一种基于贝叶斯博弈的激励机 制，服务器利用私有验证数据集在发放报酬前验证模型更新的质量。

Result: 在MNIST和FashionMNIST的非独立同分布数据上实验，面对50%标签翻转攻击时，机制保持96.7%准确率，远优于标准FedAvg。

Conclusion: 该机制计算轻量、预算可控，可直接集成到现有联邦学习框架中，为构建经济稳健且可持续的联邦学习生态系统提供了实用路径。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [120] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Key words: 贝叶斯优化, 成本感知, 停止规则, 采集函数

TL;DR: 本文提出了一种针对贝叶斯优化的成本感知停止规则，适应变动的评估成本且无需启发式调整。

<details>
  <summary>Details</summary>

Main category: cs.LG

Motivation: 在实际应用中，贝叶斯优化对昂贵的黑盒函数进行多次评估时，何时停止是一个关键问题。现有的自适应停止规则在成本感知场景中缺乏理论保证，可能导致过高成本。

Method: 基于与先进成本感知采集函数（如Pandora's Box Gittins Index和log expected improvement per cost）的理论联系，提出了一种新的停止规则。

Result: 实验表明，该停止规则与PBGI采集函数组合使用时，在成本调整的简单遗憾指标上表现优异。

Conclusion: 提出的停止规则在理论和实验上均证明了其优越性，能够平衡解决方案质量和评估成本。

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [A Study on the Application of Artificial Intelligence in Ecological Design](https://arxiv.org/abs/2507.11595)
*Hengyue Zhao*

Key words: AI, 生态设计, 相互依存, 强化学习, 植物修复

TL;DR: 探讨AI能否帮助人类与自然的关系从人类主导转向相互依存，并通过案例研究展示AI在生态设计中的应用。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究人类与自然的互动是否可以通过AI实现从主导到相互依存的转变。

Method: 通过案例研究，分析AI在数据、图像识别和生态修复中的应用，并基于原型设计提出结合强化学习与植物修复的新方法。

Result: AI不仅扩展了创意方法，还重新定义了生态设计的理论与实践，展示了其连接科学、艺术与环境管理的潜力。

Conclusion: AI有望为可持续的技术驱动生态系统提供研究蓝图。

Abstract: This paper asks whether our relationship with nature can move from human
dominance to genuine interdependence, and whether artificial intelligence (AI)
can mediate that shift. We examine a new ecological-design paradigm in which AI
interacts with non-human life forms. Through case studies we show how artists
and designers apply AI for data analysis, image recognition, and ecological
restoration, producing results that differ from conventional media. We argue
that AI not only expands creative methods but also reframes the theory and
practice of ecological design. Building on the author's prototype for
AI-assisted water remediation, the study proposes design pathways that couple
reinforcement learning with plant-based phytoremediation. The findings
highlight AI's potential to link scientific insight, artistic practice, and
environmental stewardship, offering a roadmap for future research on
sustainable, technology-enabled ecosystems.

</details>


### [122] [General Modular Harness for LLM Agents in Multi-Turn Gaming Environments](https://arxiv.org/abs/2507.11633)
*Yuxuan Zhang,Haoyang Yu,Lanxiang Hu,Haojian Jin,Hao Zhang*

Key words: LLM代理,模块化设计,感知,记忆,推理,游戏环境

TL;DR: 提出了一种用于LLM代理的模块化设计，包含感知、记忆和推理组件，支持单一LLM或VLM处理多样化游戏环境，实验表明其性能优于基线，并揭示了不同模块的贡献模式。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 通过模块化设计提升LLM代理在交互式游戏环境中的通用性和性能，利用游戏作为多样化测试平台。

Method: 设计了包含感知、记忆和推理的模块化架构，使用经典和现代游戏套件进行测试。

Result: 模块化设计显著提升游戏性能，不同模块在不同场景下贡献各异（如记忆在长期任务中关键，感知在视觉噪声环境中重要）。

Conclusion: 模块化设计有效推进通用代理的发展，游戏环境验证了其实用性和普适性。

Abstract: We introduce a modular harness design for LLM agents that composes of
perception, memory, and reasoning components, enabling a single LLM or VLM
backbone to tackle a wide spectrum of multi turn gaming environments without
domain-specific engineering. Using classic and modern game suites as
low-barrier, high-diversity testbeds, our framework provides a unified workflow
for analyzing how each module affects performance across dynamic interactive
settings. Extensive experiments demonstrate that the harness lifts gameplay
performance consistently over un-harnessed baselines and reveals distinct
contribution patterns, for example, memory dominates in long-horizon puzzles
while perception is critical in vision noisy arcades. These findings highlight
the effectiveness of our modular harness design in advancing general-purpose
agent, given the familiarity and ubiquity of games in everyday human
experience.

</details>


### [123] [Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification](https://arxiv.org/abs/2507.11662)
*Moises Andrade,Joonhyuk Cha,Brandon Ho,Vriksha Srihari,Karmesh Yadav,Zsolt Kira*

Key words: 多模态大型语言模型, 验证器, 一致性偏差, 自基验证, 任务完成率

TL;DR: 论文探讨了在多模态大型语言模型（MLLMs）作为行为验证器时存在的"一致性偏差"问题，并提出了一种轻量级解决方法——自基验证（SGV），显著提高了验证准确性和任务完成率。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 尽管MLLMs在数学和棋牌游戏中作为验证器取得了成功，但在没有明确成功标准的领域（如计算机使用）中扩展这些成果仍具挑战性。论文旨在通过MLLMs解决这一问题，但发现了其存在的一致性偏差问题。

Method: 提出了自基验证（SGV），通过无条件生成和条件生成两步，利用MLLMs的知识和推理能力，独立于待评估数据检索广泛先验，并在其基础上评估候选行为轨迹。

Result: SGV方法显著提升了MLLM验证器的性能，准确性和故障检测率提高了20分，并在多个任务中实现了实时监督，任务完成率最高提升48%。

Conclusion: SGV方法有效克服了MLLMs的一致性偏差问题，展示了其在复杂任务验证中的潜力。

Abstract: Verifiers -- functions assigning rewards to agent behavior -- have been key
for AI progress in domains like math and board games. However, extending these
gains to domains without clear-cut success criteria (e.g.,computer use) remains
a challenge: while humans can recognize suitable outcomes, translating this
intuition into scalable rules is non-trivial. Multimodal Large Language
Models(MLLMs) emerge as a promising solution, given their world knowledge,
human-preference alignment, and reasoning skills. We evaluate MLLMs as
verifiers of agent trajectories across web navigation, computer use, and
robotic manipulation, and identify a critical limitation: agreement bias, a
strong tendency for MLLMs to favor information in their context window, often
generating chains of thought to rationalize flawed behavior. This bias is
pervasive across models, resilient to test-time scaling, and can impact several
methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs
despite MLLMs showing strong, human-aligned priors on desired behavior. To
address this, we propose Self-Grounded Verification (SGV), a lightweight method
that enables more effective use of MLLMs' knowledge and reasoning by harnessing
their own sampling mechanisms via unconditional and conditional generation. SGV
operates in two steps: first, the MLLM is elicited to retrieve broad priors
about task completion, independent of the data under evaluation. Then,
conditioned on self-generated priors, it reasons over and evaluates a candidate
trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in
accuracy and failure detection rates, and can perform real-time supervision of
heterogeneous agents, boosting task completion of a GUI specialist in OSWorld,
a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting
a new state of the art on the benchmark, surpassing the previous best by 48%.

</details>


### [124] [ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making](https://arxiv.org/abs/2507.11733)
*Srikanth Vemula*

Key words: AI, 透明度, 可解释性, 案例推理, 本体

TL;DR: ClarifAI结合案例推理和本体驱动方法，提升AI透明度和可解释性，适用于高风险的决策场景。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 解决AI在决策中透明度和可解释性不足的问题，满足不同利益相关者的需求。

Method: 结合案例推理(CBR)和本体驱动方法，构建综合解释机制。

Result: ClarifAI提高了AI系统的可解释性，适用于多个领域和高风险环境。

Conclusion: ClarifAI在提升AI解释性方面具有重要价值，可为关键决策提供支持。

Abstract: This Study introduces Clarity and Reasoning Interface for Artificial
Intelligence(ClarifAI), a novel approach designed to augment the transparency
and interpretability of artificial intelligence (AI) in the realm of improved
decision making. Leveraging the Case-Based Reasoning (CBR) methodology and
integrating an ontology-driven approach, ClarifAI aims to meet the intricate
explanatory demands of various stakeholders involved in AI-powered
applications. The paper elaborates on ClarifAI's theoretical foundations,
combining CBR and ontologies to furnish exhaustive explanation mechanisms. It
further elaborates on the design principles and architectural blueprint,
highlighting ClarifAI's potential to enhance AI interpretability across
different sectors and its applicability in high-stake environments. This
research delineates the significant role of ClariAI in advancing the
interpretability of AI systems, paving the way for its deployment in critical
decision-making processes.

</details>


### [125] [Auto-Formulating Dynamic Programming Problems with Large Language Models](https://arxiv.org/abs/2507.11737)
*Chenyu Zhou,Jingyuan Yang,Linwei Xin,Yitian Chen,Ziyan He,Dongdong Ge*

Key words: 动态规划、LLM、DPLM、DualReflect、DP-Bench

TL;DR: 本文提出了DP-Bench基准测试和DPLM模型，用于解决动态规划（DP）建模中LLM应用的挑战，并通过DualReflect数据生成方法提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统DP建模依赖专家知识，LLM的应用潜力巨大，但面临数据稀缺和随机性挑战。

Method: 开发DP-Bench基准测试，设计DPLM模型，并提出DualReflect合成数据生成方法，结合前向和后向生成。

Result: DPLM在性能上媲美顶尖LLM，并在复杂问题上超越它们，DualReflect方法在高多样性环境中表现优异。

Conclusion: 前向和后向生成方法的结合是提升LLM在DP问题中性能的关键。

Abstract: Dynamic programming (DP) is a fundamental method in operations research, but
formulating DP models has traditionally required expert knowledge of both the
problem context and DP techniques. Large Language Models (LLMs) offer the
potential to automate this process. However, DP problems pose unique challenges
due to their inherently stochastic transitions and the limited availability of
training data. These factors make it difficult to directly apply existing
LLM-based models or frameworks developed for other optimization problems, such
as linear or integer programming. We introduce DP-Bench, the first benchmark
covering a wide range of textbook-level DP problems to enable systematic
evaluation. We present Dynamic Programming Language Model (DPLM), a
7B-parameter specialized model that achieves performance comparable to
state-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on
hard problems. Central to DPLM's effectiveness is DualReflect, our novel
synthetic data generation pipeline, designed to scale up training data from a
limited set of initial examples. DualReflect combines forward generation for
diversity and backward generation for reliability. Our results reveal a key
insight: backward generation is favored in low-data regimes for its strong
correctness guarantees, while forward generation, though lacking such
guarantees, becomes increasingly valuable at scale for introducing diverse
formulations. This trade-off highlights the complementary strengths of both
approaches and the importance of combining them.

</details>


### [126] [Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11787)
*Chandrashekar Muniyappa,Eunjin Kim*

Key words: 群智能, 文档搜索, 语义相似性, 优化问题

TL;DR: 该论文调查了基于群智能算法的语义相似性文档搜索的最新进展，并提出了未来研究方向。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究群智能在解决现实问题中的有效性，特别是在文档语义相似性搜索领域的应用。

Method: 通过调查和分析最新的群智能算法，研究其在语义相似性文档搜索中的应用。

Result: 总结了群智能算法在文档搜索中的最新发展和潜在优势。

Conclusion: 群智能在语义相似性文档搜索中具有潜力，未来需要进一步研究以优化其性能。

Abstract: Swarm Intelligence (SI) is gaining a lot of popularity in artificial
intelligence, where the natural behavior of animals and insects is observed and
translated into computer algorithms called swarm computing to solve real-world
problems. Due to their effectiveness, they are applied in solving various
computer optimization problems. This survey will review all the latest
developments in Searching for documents based on semantic similarity using
Swarm Intelligence algorithms and recommend future research directions.

</details>


### [127] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Key words: GPU, 并行处理, 深度优先搜索, IDA*, 启发式, 成本限制

TL;DR: 论文提出了一种利用GPU并行处理的深度优先搜索（DFS）方法，扩展了经典搜索算法（如IDA*和BTS），并在3x3魔方和4x4滑块拼图问题上验证了其性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: GPU技术的快速发展为提升经典搜索算法提供了新机会，但目前针对搜索过程中利用GPU的算法较少。作者旨在探索如何高效利用CPU和GPU的并行性进行深度优先搜索。

Method: 提出了一种成本限制的深度优先搜索（CB-DFS）方法，并结合GPU批量计算，扩展了IDA*（Batch IDA*）和BTS（Batch BTS）算法。该方法基于Asynchronous Parallel IDA*（AIDA*）框架，同时保证最优性。

Result: 在3x3魔方和4x4滑块拼图问题的实验中，证明了GPU批量计算在DFS中的高效性，并通过实验分析了超参数、神经网络启发式大小和硬件资源对性能的影响。

Conclusion: 研究展示了GPU并行处理在搜索算法中的应用潜力，特别是在深度优先搜索中，为未来优化方向提供了参考。

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>


### [128] [Aime: Towards Fully-Autonomous Multi-Agent Framework](https://arxiv.org/abs/2507.11988)
*Yexuan Shi,Mingyu Wang,Yunxiang Cao,Hongjie Lai,Junjian Lan,Xin Han,Yu Wang,Jie Geng,Zhenan Li,Zihao Xia,Xiang Chen,Chen Li,Jian Xu,Wenbo Duan,Yuanshuo Zhu*

Key words: 多智能体系统, 大语言模型, 动态规划, 自适应执行, 任务协作

TL;DR: 论文提出了Aime框架，通过动态规划和执行解决了传统多智能体系统中计划执行僵化、能力静态和通信效率低的问题。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统基于LLM的多智能体系统由于计划-执行框架的局限性（如僵化执行、静态能力和低效通信），在动态环境中表现不佳，需要一种更灵活、自适应的解决方案。

Method: Aime框架引入了动态规划器、动态角色工厂和集中式进度管理模块，实现了实时策略调整、按需创建智能体以及全局状态一致性管理。

Result: 在通用推理（GAIA）、软件工程（SWE-bench）和实时网页导航（WebVoyager）等任务中，Aime显著优于现有最先进的专用智能体。

Conclusion: Aime展示了更高的适应性和任务成功率，为多智能体协作提供了更稳健的基础。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are
emerging as a powerful paradigm for solving complex, multifaceted problems.
However, the potential of these systems is often constrained by the prevalent
plan-and-execute framework, which suffers from critical limitations: rigid plan
execution, static agent capabilities, and inefficient communication. These
weaknesses hinder their adaptability and robustness in dynamic environments.
This paper introduces Aime, a novel multi-agent framework designed to overcome
these challenges through dynamic, reactive planning and execution. Aime
replaces the conventional static workflow with a fluid and adaptive
architecture. Its core innovations include: (1) a Dynamic Planner that
continuously refines the overall strategy based on real-time execution
feedback; (2) an Actor Factory that implements Dynamic Actor instantiation,
assembling specialized agents on-demand with tailored tools and knowledge; and
(3) a centralized Progress Management Module that serves as a single source of
truth for coherent, system-wide state awareness. We empirically evaluated Aime
on a diverse suite of benchmarks spanning general reasoning (GAIA), software
engineering (SWE-bench Verified), and live web navigation (WebVoyager). The
results demonstrate that Aime consistently outperforms even highly specialized
state-of-the-art agents in their respective domains. Its superior adaptability
and task success rate establish Aime as a more resilient and effective
foundation for multi-agent collaboration.

</details>


### [129] [Understanding visual attention beehind bee-inspired UAV navigation](https://arxiv.org/abs/2507.11992)
*Pranav Rajbhandari,Abhi Veda,Matthew Garratt,Mandayam Srinivasan,Sridhar Ravi*

Key words: 生物启发设计, 无人机导航, 光流, 强化学习

TL;DR: 研究通过强化学习训练无人机仅使用光流导航隧道障碍，发现代理主要关注光流中的不连续区域和大光流区域，行为类似昆虫飞行。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 受蜜蜂等生物仅依赖光流导航的启发，开发适用于计算资源有限的无人机导航策略。

Method: 使用强化学习训练代理，仅以光流为输入导航隧道障碍，并通过注意力模式分析其决策依据。

Result: 代理主要关注光流的不连续区域和大光流区域，行为类似飞行昆虫。

Conclusion: 该策略可能适用于开发物理无人机的简单显式控制法则。

Abstract: Bio-inspired design is often used in autonomous UAV navigation due to the
capacity of biological systems for flight and obstacle avoidance despite
limited sensory and computational capabilities. In particular, honeybees mainly
use the sensory input of optic flow, the apparent motion of objects in their
visual field, to navigate cluttered environments. In our work, we train a
Reinforcement Learning agent to navigate a tunnel with obstacles using only
optic flow as sensory input. We inspect the attention patterns of trained
agents to determine the regions of optic flow on which they primarily base
their motor decisions. We find that agents trained in this way pay most
attention to regions of discontinuity in optic flow, as well as regions with
large optic flow magnitude. The trained agents appear to navigate a cluttered
tunnel by avoiding the obstacles that produce large optic flow, while
maintaining a centered position in their environment, which resembles the
behavior seen in flying insects. This pattern persists across independently
trained agents, which suggests that this could be a good strategy for
developing a simple explicit control law for physical UAVs.

</details>


### [130] [Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs](https://arxiv.org/abs/2507.12110)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Key words: 多智能体强化学习（MARL）, 自动驾驶车辆, 拓扑张量, 探索-利用权衡, 混合交通

TL;DR: 本文提出了一种拓扑增强的多智能体强化学习方法（TPE-MARL），用于优化混合交通中的自动驾驶车辆协同决策，有效平衡探索与利用，表现出优越的交通效率和安全性。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 多智能体强化学习（MARL）面临着联合状态-动作空间指数增长的挑战，尤其是在混合交通中自动驾驶车辆的协同决策问题上。本文旨在通过拓扑方法压缩状态信息并减少搜索空间，以优化决策性能。

Method: 1. 设计动态交通流的博弈拓扑张量，压缩高维状态信息。2. 基于该张量和QMIX算法，构建拓扑增强的MARL框架，融入访问计数和智能体互信息。

Result: TPE-MARL在不同交通密度和自动驾驶车辆渗透率下展现出卓越性能，平衡了探索与利用，并在交通效率、安全性、决策平滑性和任务完成度上优于基准方法。其决策合理性甚至超越人类驾驶员。

Conclusion: TPE-MARL通过拓扑方法显著提升了多智能体强化学习在混合交通中的性能，为自动驾驶车辆的协同决策提供了高效解决方案。

Abstract: The exploration-exploitation trade-off constitutes one of the fundamental
challenges in reinforcement learning (RL), which is exacerbated in multi-agent
reinforcement learning (MARL) due to the exponential growth of joint
state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)
method for optimizing cooperative decision-making of connected and autonomous
vehicles (CAVs) in mixed traffic. This work presents two primary contributions:
First, we construct a game topology tensor for dynamic traffic flow,
effectively compressing high-dimensional traffic state information and decrease
the search space for MARL algorithms. Second, building upon the designed game
topology tensor and using QMIX as the backbone RL algorithm, we establish a
topology-enhanced MARL framework incorporating visit counts and agent mutual
information. Extensive simulations across varying traffic densities and CAV
penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations
encompassing training dynamics, exploration patterns, macroscopic traffic
performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL
successfully balances exploration and exploitation. Consequently, it exhibits
superior performance in terms of traffic efficiency, safety, decision
smoothness, and task completion. Furthermore, the algorithm demonstrates
decision-making rationality comparable to or exceeding that of human drivers in
both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is
available at
\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.

</details>


### [131] [Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation](https://arxiv.org/abs/2507.12186)
*Edward Kim,Hanna Kurniawati*

Key words: POMDP, 在线规划, 深度采样, 动态环境, 参考策略编程

TL;DR: 提出了一种新的近似POMDP求解器，通过深度采样和渐进策略更新，性能损失由平均采样误差决定，实验验证其在动态环境中表现优越。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 为了解决在线POMDP问题中采样稀疏性和性能损失的挑战。

Method: 使用部分可观测参考策略编程，深度采样并结合渐进策略更新。

Result: 在动态环境中（如直升机紧急场景）显著优于现有在线基准。

Conclusion: 该方法在理论和实验上均表现出色，性能提升显著。

Abstract: This paper proposes Partially Observable Reference Policy Programming, a
novel anytime online approximate POMDP solver which samples meaningful future
histories very deeply while simultaneously forcing a gradual policy update. We
provide theoretical guarantees for the algorithm's underlying scheme which say
that the performance loss is bounded by the average of the sampling
approximation errors rather than the usual maximum, a crucial requirement given
the sampling sparsity of online planning. Empirical evaluations on two
large-scale problems with dynamically evolving environments -- including a
helicopter emergency scenario in the Corsica region requiring approximately 150
planning steps -- corroborate the theoretical results and indicate that our
solver considerably outperforms current online benchmarks.

</details>


### [132] [BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution](https://arxiv.org/abs/2507.12207)
*Subin Lin,Chuanbo Hua*

Key words: 建筑能源预测,大语言模型,启发式方法,物理原理,自动化设计

TL;DR: 本文提出BuildEvo框架，利用大语言模型自动设计高效且可解释的建筑能源预测启发式方法，结合物理原理，显著提升了预测精度和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 传统启发式方法精度不足，而高级模型缺乏透明度且难以泛化，因此需要一种结合物理原理的自动化设计方法。

Method: 采用进化过程引导大语言模型构建和优化启发式方法，结合建筑特征和运行数据的物理洞察。

Result: BuildEvo在基准测试中表现优异，实现了高精度和透明逻辑。

Conclusion: BuildEvo推动了自动化设计稳健、物理基础的启发式方法的进展，为复杂能源系统提供可信模型。

Abstract: Accurate building energy forecasting is essential, yet traditional heuristics
often lack precision, while advanced models can be opaque and struggle with
generalization by neglecting physical principles. This paper introduces
BuildEvo, a novel framework that uses Large Language Models (LLMs) to
automatically design effective and interpretable energy prediction heuristics.
Within an evolutionary process, BuildEvo guides LLMs to construct and enhance
heuristics by systematically incorporating physical insights from building
characteristics and operational data (e.g., from the Building Data Genome
Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on
benchmarks, offering improved generalization and transparent prediction logic.
This work advances the automated design of robust, physically grounded
heuristics, promoting trustworthy models for complex energy systems.

</details>


### [133] [Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning](https://arxiv.org/abs/2507.12215)
*Yuhao Chen,Shuochen Liu,Yuanjie Lyu,Chao Zhang,Jiayao Shi,Tong Xu*

Key words: LLMs, 象棋, 空间战略推理, GRPO, 强化学习

TL;DR: 论文探讨了LLMs在空间战略推理中的不足，通过中国象棋（象棋）作为测试平台，提出了一个训练框架，显著提升了模型性能。

<details>
  <summary>Details</summary>

Main category: cs.AI

Motivation: 研究LLMs在空间战略推理中的表现，尤其是在复杂且完全可观测的棋盘游戏中，以象棋为例。

Method: 提出多阶段训练框架：基础规则学习、战略注释增强、GRPO强化学习。

Result: Xiangqi-R1模型在合法移动预测和分析准确性上分别提升了18%和22%。

Conclusion: 该研究为构建空间复杂领域的通用战略智能提供了新思路。

Abstract: Game playing has long served as a fundamental benchmark for evaluating
Artificial General Intelligence (AGI). While Large Language Models (LLMs) have
demonstrated impressive capabilities in general reasoning, their effectiveness
in spatial strategic reasoning, which is critical for complex and fully
observable board games, remains insufficiently explored. In this work, we adopt
Chinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate
rules and spatial complexity. To advance LLMs' strategic competence in such
environments, we propose a training framework tailored to Xiangqi, built upon a
large-scale dataset of five million board-move pairs enhanced with expert
annotations and engine evaluations. Building on this foundation, we introduce
Xiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning
for legal move prediction to capture basic spatial rules, (2) incorporating
strategic annotations to improve decision-making, and (3) applying
reinforcement learning via Group Relative Policy Optimization (GRPO) with
multi-dimensional reward signals to enhance reasoning stability. Our
Experimental results indicate that, despite their size and power,
general-purpose LLMs struggle to achieve satisfactory performance in these
tasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an
18% rise in move legality and a 22% boost in analysis accuracy. Our results
point to a promising path for creating general strategic intelligence in
spatially complex areas.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [134] [Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview](https://arxiv.org/abs/2507.11935)
*Jikang Deng,Fizza Hassan,Hui Zhou,Saad Al-Ahmadi,Mohamed-Slim Alouini,Daniel B. Da Costa*

Key words: 6G, NTN, ORAN, DevOps, 人工智能

TL;DR: 论文探讨了将ORAN与NTN结合的6G网络架构，解决了NTN在网络管理中面临的挑战，并提出了基于ORAN的NTN框架及其特性。

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 6G网络中，非地面网络（NTN）和开放无线接入网络（ORAN）的结合能够提升网络效率、可靠性和灵活性，但目前缺乏在DevOps生命周期中有效整合两者的方法，特别是如何利用智能ORAN解决NTN的可扩展性挑战。

Method: 论文首先总结了ORAN和NTN的背景知识及现有研究，提出了基于ORAN的NTN框架，并详细讨论了其特性，包括灵活的前传分割、RICs的分布式学习增强、可扩展部署架构和多域服务管理。

Result: 提出了一个整合ORAN和NTN的框架，为6G网络的智能和可扩展管理提供了解决方案。

Conclusion: 未来研究方向包括将ORAN-NTN框架与其他技术结合，并探索候选用例。

Abstract: As the path toward 6G networks is being charted, the emerging applications
have motivated evolutions of network architectures to realize the efficient,
reliable, and flexible wireless networks. Among the potential architectures,
the non-terrestrial network (NTN) and open radio access network (ORAN) have
received increasing interest from both academia and industry. Although the
deployment of NTNs ensures coverage, enhances spectral efficiency, and improves
the resilience of wireless networks. The high altitude and mobility of NTN
present new challenges in the development and operations (DevOps) lifecycle,
hindering intelligent and scalable network management due to the lack of native
artificial intelligence (AI) capability. With the advantages of ORAN in
disaggregation, openness, virtualization, and intelligence, several works
propose integrating ORAN principles into the NTN, focusing mainly on ORAN
deployment options based on transparent and regenerative systems. However, a
holistic view of how to effectively combine ORAN and NTN throughout the DevOps
lifecycle is still missing, especially regarding how intelligent ORAN addresses
the scalability challenges in NTN. Motivated by this, in this paper, we first
provide the background knowledge about ORAN and NTN, outline the
state-of-the-art research on ORAN for NTNs, and present the DevOps challenges
that motivate the adoption of ORAN solutions. We then propose the ORAN-based
NTN framework, discussing its features and architectures in detail. These
include the discussion about flexible fronthaul split, RAN intelligent
controllers (RICs) enhancement for distributed learning, scalable deployment
architecture, and multi-domain service management. Finally, the future research
directions, including combinations of the ORAN-based NTN framework and other
enabling technologies and schemes, as well as the candidate use cases, are
highlighted.

</details>


### [135] [LLM-Based Config Synthesis requires Disambiguation](https://arxiv.org/abs/2507.12443)
*Rajdeep Mondal,Nikolaj Bjorner,Todd Millstein,Alan Tang,George Varghese*

Key words: 

TL;DR: 

<details>
  <summary>Details</summary>

Main category: cs.NI

Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [136] [Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts](https://arxiv.org/abs/2507.12404)
*Yeming Xian,Xiaoming Wang,Yanfa Yan*

Key words: 氧化物钙钛矿, OER, 符号回归, 神经网络, 描述符

TL;DR: 提出了一种结合神经网络和符号回归的两阶段框架，用于发现氧化物钙钛矿OER活性的可解释描述符。

<details>
  <summary>Details</summary>

Main category: physics.data-an

Motivation: 在高维输入和小数据集情况下，传统符号回归性能下降，需要一种既能保持准确性又能物理解释的方法。

Method: 使用神经网络和特征重要性分析结合符号回归，分两阶段优化描述符。

Result: 最终描述符（μ/t、μ/RA和LUMO能量）在训练和验证中表现优异，MAE分别为22.1和20.6 meV。

Conclusion: 神经网络指导的符号回归在小数据环境下也能实现准确且物理可解释的描述符发现，为材料信息学提供新思路。

Abstract: Understanding and predicting the activity of oxide perovskite catalysts for
the oxygen evolution reaction (OER) requires descriptors that are both accurate
and physically interpretable. While symbolic regression (SR) offers a path to
discover such formulas, its performance degrades with high-dimensional inputs
and small datasets. We present a two-phase framework that combines neural
networks (NN), feature importance analysis, and symbolic regression (SR) to
discover interpretable descriptors for OER activity in oxide perovskites. In
Phase I, using a small dataset and seven structural features, we reproduce and
improve the known {\mu}/t descriptor by engineering composite features and
applying symbolic regression, achieving training and validation MAEs of 22.8
and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce
dimensionality, and identify LUMO energy as a key electronic descriptor. A
final formula using {\mu}/t, {\mu}/RA, and LUMO energy achieves improved
accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong
physical interpretability. Our results demonstrate that NN-guided symbolic
regression enables accurate, interpretable, and physically meaningful
descriptor discovery in data-scarce regimes, indicating interpretability need
not sacrifice accuracy for materials informatics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [137] [A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing](https://arxiv.org/abs/2507.11560)
*Xin Wang,Xiao Huan Li,Xun Wang*

Key words: IIoT, AIGC, 边缘计算, 任务卸载, MADDPG

TL;DR: 该论文提出了一种针对IIoT边缘计算环境的AIGC任务卸载框架，首次考虑了AIGC模型切换带来的延迟和能耗，并通过MADDPG-MATO算法优化性能。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 工业物联网（IIoT）与人工智能生成内容（AIGC）的结合为智能制造带来新机遇，但也面临计算密集型任务和低延迟需求的挑战，传统云计算难以满足实时需求。

Method: 提出了一种AIGC任务卸载框架，IIoT设备作为多代理协同卸载动态AIGC任务到最合适的边缘服务器，并设计了基于MADDPG-MATO的模型感知任务卸载算法。

Result: 实验显示，MADDPG-MATO在延迟、能耗和任务完成率上均优于基线算法，延迟平均减少6.98%，能耗降低7.12%，任务完成率提升3.72%。

Conclusion: MADDPG-MATO算法在高负载、动态的IIoT环境中表现出鲁棒性和高效性。

Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial
Intelligence-Generated Content (AIGC) offers new opportunities for smart
manufacturing, but it also introduces challenges related to
computation-intensive tasks and low-latency demands. Traditional generative
models based on cloud computing are difficult to meet the real-time
requirements of AIGC tasks in IIoT environments, and edge computing can
effectively reduce latency through task offloading. However, the dynamic nature
of AIGC tasks, model switching delays, and resource constraints impose higher
demands on edge computing environments. To address these challenges, this paper
proposes an AIGC task offloading framework tailored for IIoT edge computing
environments, considering the latency and energy consumption caused by AIGC
model switching for the first time. IIoT devices acted as multi-agent
collaboratively offload their dynamic AIGC tasks to the most appropriate edge
servers deployed with different generative models. A model aware AIGC task
offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient
(MADDPG-MATO) is devised to minimize the latency and energy. Experimental
results show that MADDPG-MATO outperforms baseline algorithms, achieving an
average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%
increase in task completion rate across four sets of experiments with model
numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is
robust and efficient in dynamic, high-load IIoT environments.

</details>


### [138] [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)
*Seth Ockerman,Amal Gueroudji,Tanwi Mallick,Yixuan He,Line Pouchard,Robert Ross,Shivaram Venkataraman*

Key words: ST-GNNs, distributed training, memory efficiency, PeMS dataset

TL;DR: 提出了一种名为PGT-I的扩展框架，通过分布式数据并行训练和两种新颖的策略（index-batching和distributed-index-batching）优化ST-GNNs在大规模数据集上的训练效果。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 现有ST-GNNs的内存限制使其仅适用于小规模数据集，且缺乏对时空模型的支持以及对时空数据特性的重视。

Method: 提出PGT-I框架，集成分布式数据并行训练，通过动态构建时空快照（index-batching）及跨多GPU的分布式扩展（distributed-index-batching）降低内存开销。

Result: 在PeMS数据集上首次实现无需图分区的ST-GNN训练，峰值内存使用降低89%，128 GPU下提速13.1倍。

Conclusion: PGT-I通过优化时空数据处理策略，显著提升了ST-GNNs在大规模数据集上的可扩展性和效率。

Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for
modeling spatial and temporal data dependencies. However, their applications
have been limited primarily to small-scale datasets because of memory
constraints. While distributed training offers a solution, current frameworks
lack support for spatiotemporal models and overlook the properties of
spatiotemporal data. Informed by a scaling study on a large-scale workload, we
present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch
Geometric Temporal that integrates distributed data parallel training and two
novel strategies: index-batching and distributed-index-batching. Our index
techniques exploit spatiotemporal structure to construct snapshots dynamically
at runtime, significantly reducing memory overhead, while
distributed-index-batching extends this approach by enabling scalable
processing across multiple GPUs. Our techniques enable the first-ever training
of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing
peak memory usage by up to 89\% and achieving up to a 13.1x speedup over
standard DDP with 128 GPUs.

</details>


### [139] [Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI](https://arxiv.org/abs/2507.11830)
*Samyam Rajbhandari,Mert Hidayetoglu,Aurick Qiao,Ye Wang,Juncheng Yang,Jeff Rasley,Michael Wyatt,Yuxiong He*

Key words: Arctic Inference, Shift Parallelism, 动态并行, AI推理

TL;DR: Arctic Inference是一个开源的vLLM插件，通过动态并行策略Shift Parallelism优化AI推理任务，显著提升速度与效率。

<details>
  <summary>Details</summary>

Main category: cs.DC

Motivation: 当前AI推理系统在延迟、吞吐量和成本之间存在折衷，需要一种更高效的解决方案。

Method: 采用Shift Parallelism动态并行策略，并结合推测解码、SwiftKV计算减少和优化的嵌入推理。

Result: 实现了请求完成速度提升3.4倍，生成速度提升1.75倍，嵌入推理达到1.6M tokens/sec每GPU。

Conclusion: Arctic Inference为企业和社区提供了高效的AI推理解决方案。

Abstract: Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source vLLM plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, SwiftKV compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [140] [Counting Answer Sets of Disjunctive Answer Set Programs](https://arxiv.org/abs/2507.11655)
*Mohimenul Kabir,Supratik Chakraborty,Kuldeep S Meel*

Key words: Answer Set Programming, disjunctive logic programs, model counting, subtractive reduction

TL;DR: 论文介绍了一种名为SharpASP-SR的新框架，用于计算析取逻辑程序的答案集数量，基于减法归约到投影命题模型计数技术，显著提升了大规模答案集计数效率。

<details>
  <summary>Details</summary>

Main category: cs.LO

Motivation: 计算答案集数量在概率推理、网络可靠性分析等领域具有重要应用，但目前针对析取逻辑程序的计数器仍面临挑战。

Method: 提出SharpASP-SR框架，通过减法归约到投影命题模型计数，结合新的答案集特征化方法，确保中间表示的多项式大小，并利用模型计数技术的最新进展。

Result: 实验表明，在答案集数量较大的情况下，SharpASP-SR显著优于现有计数器，并通过混合计数方法进一步提升了性能。

Conclusion: SharpASP-SR通过减法归约和混合计数方法，为析取逻辑程序的答案集计数提供了高效解决方案。

Abstract: Answer Set Programming (ASP) provides a powerful declarative paradigm for
knowledge representation and reasoning. Recently, counting answer sets has
emerged as an important computational problem with applications in
probabilistic reasoning, network reliability analysis, and other domains. This
has motivated significant research into designing efficient ASP counters. While
substantial progress has been made for normal logic programs, the development
of practical counters for disjunctive logic programs remains challenging.
  We present SharpASP-SR, a novel framework for counting answer sets of
disjunctive logic programs based on subtractive reduction to projected
propositional model counting. Our approach introduces an alternative
characterization of answer sets that enables efficient reduction while ensuring
that intermediate representations remain of polynomial size. This allows
SharpASP-SR to leverage recent advances in projected model counting technology.
Through extensive experimental evaluation on diverse benchmarks, we demonstrate
that SharpASP-SR significantly outperforms existing counters on instances with
large answer set counts. Building on these results, we develop a hybrid
counting approach that combines enumeration techniques with SharpASP-SR to
achieve state-of-the-art performance across the full spectrum of disjunctive
programs.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [141] [CosmoFlow: Scale-Aware Representation Learning for Cosmology with Flow Matching](https://arxiv.org/abs/2507.11842)
*Sidharth Kannan,Tian Qiu,Carolina Cuesta-Lazaro,Haewon Jeong*

Key words: 生成模型、流匹配、冷暗物质、低维表示、无监督学习

TL;DR: CosmoFlow是一种基于流匹配的生成模型，能够无监督地学习冷暗物质模拟数据的低维表示，适用于重建、生成和参数推断。

<details>
  <summary>Details</summary>

Main category: astro-ph.CO

Motivation: 研究目的是开发一种能够从冷暗物质模拟数据中学习紧凑且语义丰富的低维表示的生成模型。

Method: 采用了基于流匹配的生成模型（CosmoFlow），在无监督的情况下学习冷暗物质模拟数据的低维表示。

Result: 模型学习的表示比原始数据小32倍，适用于重建、合成数据生成和参数推断，且表示具有可解释性。

Conclusion: CosmoFlow成功实现了从冷暗物质数据中学习高效且可解释的低维表示。

Abstract: Generative machine learning models have been demonstrated to be able to learn
low dimensional representations of data that preserve information required for
downstream tasks. In this work, we demonstrate that flow matching based
generative models can learn compact, semantically rich latent representations
of field level cold dark matter (CDM) simulation data without supervision. Our
model, CosmoFlow, learns representations 32x smaller than the raw field data,
usable for field level reconstruction, synthetic data generation, and parameter
inference. Our model also learns interpretable representations, in which
different latent channels correspond to features at different cosmological
scales.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [142] [Landmark Detection for Medical Images using a General-purpose Segmentation Model](https://arxiv.org/abs/2507.11551)
*Ekaterina Stansfield,Jennifer A. Mitterer,Abdulrahman Altahhan*

Key words: YOLO, SAM, 医学影像, 解剖标志, 骨科, 分割模型

TL;DR: 论文提出了一种结合YOLO和SAM的混合模型，用于精确分割骨科骨盆X光片中的解剖标志和复杂轮廓。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 由于现有模型（如SAM和MedSAM）在医学影像中无法准确识别骨科骨盆标志，需要一种新的方法来解决这一限制，提高解剖标志分割的精度。

Method: 利用YOLO进行物体检测，生成边界框作为SAM的输入提示，结合两者的优势形成一个可靠的分割管道。

Result: 实验表明，该混合模型在小范围测试集（8个标志）和大范围测试集（72个标志和16个区域）上均表现出色，能够准确分割复杂的骨科骨盆X光片。

Conclusion: YOLO和SAM的组合在骨科骨盆标志检测和复杂轮廓分割中具有优异性能，为医学影像分析提供了新的解决方案。

Abstract: Radiographic images are a cornerstone of medical diagnostics in orthopaedics,
with anatomical landmark detection serving as a crucial intermediate step for
information extraction. General-purpose foundational segmentation models, such
as SAM (Segment Anything Model), do not support landmark segmentation out of
the box and require prompts to function. However, in medical imaging, the
prompts for landmarks are highly specific. Since SAM has not been trained to
recognize such landmarks, it cannot generate accurate landmark segmentations
for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has
been trained to identify larger anatomical structures, such as organs and their
parts, and lacks the fine-grained precision required for orthopaedic pelvic
landmarks. To address this limitation, we propose leveraging another
general-purpose, non-foundational model: YOLO. YOLO excels in object detection
and can provide bounding boxes that serve as input prompts for SAM. While YOLO
is efficient at detection, it is significantly outperformed by SAM in
segmenting complex structures. In combination, these two models form a reliable
pipeline capable of segmenting not only a small pilot set of eight anatomical
landmarks but also an expanded set of 72 landmarks and 16 regions with complex
outlines, such as the femoral cortical bone and the pelvic inlet. By using
YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to
accurately segment orthopaedic pelvic radiographs. Our results show that the
proposed combination of YOLO and SAM yields excellent performance in detecting
anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.

</details>


### [143] [3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation](https://arxiv.org/abs/2507.11557)
*Jiaxu Zheng,Meiman He,Xuhui Tang,Xiong Wang,Tuoyu Cao,Tianyi Zeng,Lichi Zhang,Chenyu You*

Key words: MR成像, 3D小波潜在扩散模型, 合成CT图像, 空间对齐, 双跳跃连接注意力机制

TL;DR: 提出一种新型3D小波潜在扩散模型（3D-WLDM），通过在小波潜在空间中实现模态转换，解决了现有MR到CT合成方法中空间对齐差和图像质量低的问题。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: MR成像在临床诊断和治疗流程中至关重要，但现有MR到CT合成方法在全身成像中存在空间对齐和图像质量不足的缺陷。

Method: 提出了3D-WLDM模型，结合了小波残差模块以增强跨图像和潜在空间的特征捕捉与重建，同时通过解耦结构和模态特性保持解剖完整性，并引入双跳跃连接注意力机制以提升高分辨率CT图像生成。

Result: 该方法显著提升了合成CT图像的空间对齐性和图像质量，尤其是在骨骼结构和软组织对比度方面。

Conclusion: 3D-WLDM为解决MR到CT合成中的关键问题提供了有效方案，具有潜在临床应用价值。

Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary
clinical diagnostics. It is increasingly integrated into advanced therapeutic
workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance
(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are
critically dependent on accurate estimation of radiation attenuation, which is
typically facilitated by synthesizing Computed Tomography (CT) images from MR
scans to generate attenuation maps. However, existing MR-to-CT synthesis
methods for whole-body imaging often suffer from poor spatial alignment between
the generated CT and input MR images, and insufficient image quality for
reliable use in downstream clinical tasks. In this paper, we present a novel 3D
Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by
performing modality translation in a learned latent space. By incorporating a
Wavelet Residual Module into the encoder-decoder architecture, we enhance the
capture and reconstruction of fine-scale features across image and latent
spaces. To preserve anatomical integrity during the diffusion process, we
disentangle structural and modality-specific characteristics and anchor the
structural component to prevent warping. We also introduce a Dual Skip
Connection Attention mechanism within the diffusion model, enabling the
generation of high-resolution CT images with improved representation of bony
structures and soft-tissue contrast.

</details>


### [144] [Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach](https://arxiv.org/abs/2507.11561)
*Lucas Erlacher,Samuel Ruipérez-Campillo,Holger Michel,Sven Wellmann,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Key words: 肺动脉高压, 新生儿, 超声心动图, 变分自编码器, 多视角学习

TL;DR: 该论文探讨了新生儿肺动脉高压（PH）的诊断方法，提出了一种基于多视角变分自编码器（VAE）的模型，以提高超声心动图视频的诊断准确性和泛化能力。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 新生儿肺动脉高压是一种严重疾病，目前超声心动图虽为常用诊断工具，但依赖操作者且现有自动化模型多针对成人，存在局限性。因此，需开发一种更鲁棒的多视角学习方法。

Method: 论文采用了多视角变分自编码器（VAE）框架，从超声心动图视频中提取复杂特征，并与单视角和监督学习方法进行比较。

Result: 实验结果显示，多视角VAE模型在泛化能力和分类准确性上优于单视角和监督学习方法。

Conclusion: 多视角学习显著提高了新生儿肺动脉高压诊断的鲁棒性和准确性，为临床提供了更可靠的自动化工具。

Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized
by elevated pressure in the pulmonary arteries, leading to right ventricular
strain and heart failure. While right heart catheterization (RHC) is the
diagnostic gold standard, echocardiography is preferred due to its non-invasive
nature, safety, and accessibility. However, its accuracy highly depends on the
operator, making PH assessment subjective. While automated detection methods
have been explored, most models focus on adults and rely on single-view
echocardiographic frames, limiting their performance in diagnosing PH in
newborns. While multi-view echocardiography has shown promise in improving PH
assessment, existing models struggle with generalizability. In this work, we
employ a multi-view variational autoencoder (VAE) for PH prediction using
echocardiographic videos. By leveraging the VAE framework, our model captures
complex latent representations, improving feature extraction and robustness. We
compare its performance against single-view and supervised learning approaches.
Our results show improved generalization and classification accuracy,
highlighting the effectiveness of multi-view learning for robust PH assessment
in newborns.

</details>


### [145] [Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?](https://arxiv.org/abs/2507.11569)
*Hanxue Gu,Yaqian Chen,Nicholas Konz,Qihang Li,Maciej A. Mazurowski*

Key words: 基础模型、乳腺MRI、图像配准、零样本学习、预训练

TL;DR: 本文评估了基于基础模型的乳腺MRI配准算法，发现SAM等算法在大域偏移下表现优于传统方法，但在处理腺体细节时仍有不足。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 验证基础模型在处理复杂可变形解剖结构（如乳腺MRI）时的配准能力。

Method: 评估五种预训练编码器（DINO-v2、SAM、MedSAM、SSLSAM、MedCLIP），比较其在四种乳腺配准任务中的表现。

Result: SAM等模型在整体配准中优于传统方法，但无法精确对齐腺体组织；医学图像的额外预训练未提升性能。

Conclusion: 需进一步研究领域专用训练的影响，并开发提升全局配准与细节准确性的策略。

Abstract: Foundation models, pre-trained on large image datasets and capable of
capturing rich feature representations, have recently shown potential for
zero-shot image registration. However, their performance has mostly been tested
in the context of rigid or less complex structures, such as the brain or
abdominal organs, and it remains unclear whether these models can handle more
challenging, deformable anatomy. Breast MRI registration is particularly
difficult due to significant anatomical variation between patients, deformation
caused by patient positioning, and the presence of thin and complex internal
structure of fibroglandular tissue, where accurate alignment is crucial.
Whether foundation model-based registration algorithms can address this level
of complexity remains an open question. In this study, we provide a
comprehensive evaluation of foundation model-based registration algorithms for
breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,
MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that
capture variations in different years and dates, sequences, modalities, and
patient disease status (lesion versus no lesion). Our results show that
foundation model-based algorithms such as SAM outperform traditional
registration baselines for overall breast alignment, especially under large
domain shifts, but struggle with capturing fine details of fibroglandular
tissue. Interestingly, additional pre-training or fine-tuning on medical or
breast-specific images in MedSAM and SSLSAM, does not improve registration
performance and may even decrease it in some cases. Further work is needed to
understand how domain-specific training influences registration and to explore
targeted strategies that improve both global alignment and fine structure
accuracy. We also publicly release our code at
\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.

</details>


### [146] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Key words: 无监督学习, 肝脏疾病, 磁共振图像, 治疗反应, 深度聚类

TL;DR: 使用无监督机器学习从肝脏磁共振图像中提取组织模式词汇，量化弥漫性肝病的治疗反应，并在临床试验中验证其效果。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 量化与疾病进展和治疗反应相关的图像模式是指导个体治疗和开发新疗法的关键工具。

Method: 通过深度聚类网络对医学图像块进行编码和聚类，形成低维潜在空间的组织词汇，捕捉与治疗反应相关的组织变化。

Result: 该方法在随机对照试验中成功识别了与治疗相关的肝组织变化路径，并比现有非成像指标更好地区分治疗组。

Conclusion: 提出的组织词汇方法能有效量化治疗反应，并可从非侵入性影像数据预测活检特征，适用于临床验证。

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [147] [Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation](https://arxiv.org/abs/2507.12427)
*Ashkan Shakarami,Azade Farshad,Yousef Yeganeh,Lorenzo Nicole,Peter Schuffler,Stefano Ghidoni,Nassir Navab*

Key words: 病理组织分割, Vision Transformer, 多级特征, 乳腺癌, 临床任务

TL;DR: UTS是一种基于单元的病理组织分割框架，通过将32*32的图块作为分割单元，减少标注工作量并提升计算效率，同时保持准确性。L-ViT的多层次特征表示支持细粒度和全局组织上下文的捕捉，适用于临床任务。

<details>
  <summary>Details</summary>

Main category: eess.IV

Motivation: 解决病理组织分割中像素级标注的高成本和计算效率低的问题。

Method: 提出UTS框架和L-ViT模型，利用多层次特征表示分割图块。

Result: 在459个H&E染色区域的386,371个图块上表现优于U-Net和基于Transformer的基线。

Conclusion: UTS提供了一种高效且准确的病理组织分割方法，适用于临床任务。

Abstract: We propose UTS, a unit-based tissue segmentation framework for histopathology
that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the
segmentation unit. This approach reduces annotation effort and improves
computational efficiency without compromising accuracy. To implement this
approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits
the multi-level feature representation to capture both fine-grained morphology
and global tissue context. Trained to segment breast tissue into three
categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports
clinically relevant tasks such as tumor-stroma quantification and surgical
margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it
outperforms U-Net variants and transformer-based baselines. Code and Dataset
will be available at GitHub.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [148] [Galaxy image simplification using Generative AI](https://arxiv.org/abs/2507.11692)
*Sai Teja Erukude,Lior Shamir*

Key words: 星系图像分析、生成式AI、骨架化、DESI Legacy Survey

TL;DR: 提出了一种基于生成式AI的新方法来简化星系图像并转换为“骨架化”形式，支持高精度的形状测量和不受预定义类别限制的分析。

<details>
  <summary>Details</summary>

Main category: astro-ph.GA

Motivation: 需要自动化分析高容量星系图像，当前依赖预定义类别的机器学习方法存在局限性。

Method: 利用生成式AI将星系图像简化为骨架化形式，实现精确测量和非类别限制分析。

Result: 成功应用于12.5万张DESI Legacy Survey图像，简化后的图像目录已公开。

Conclusion: 新方法提高了星系图像分析的准确性和灵活性，支持公开验证。

Abstract: Modern digital sky surveys have been acquiring images of billions of
galaxies. While these images often provide sufficient details to analyze the
shape of the galaxies, accurate analysis of such high volumes of images
requires effective automation. Current solutions often rely on machine learning
annotation of the galaxy images based on a set of pre-defined classes. Here we
introduce a new approach to galaxy image analysis that is based on generative
AI. The method simplifies the galaxy images and automatically converts them
into a ``skeletonized" form. The simplified images allow accurate measurements
of the galaxy shapes and analysis that is not limited to a certain pre-defined
set of classes. We demonstrate the method by applying it to galaxy images
acquired by the DESI Legacy Survey. The code and data are publicly available.
The method was applied to 125,000 DESI Legacy Survey images, and the catalog of
the simplified images is publicly available.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [149] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Key words: 可变形注意力变换器,硬件优化,神经架构搜索,FPGA

TL;DR: 本文提出了一种针对可变形注意力变换器(DAT)的硬件友好优化框架，通过NAS和新的切片策略优化内存访问模式，同时保持模型精度和减少硬件开销。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 可变形注意力变换器(DAT)的数据依赖采样机制导致不规则内存访问模式，难以高效部署到硬件，现有方法要么开销大，要么精度低。本文旨在解决这一问题。

Method: 1. 提出基于神经架构搜索(NAS)和切片策略的方法，在推理过程中自动划分特征图为均匀块；2. 设计FPGA验证系统测试框架性能。

Result: 在ImageNet-1K上，框架仅损失0.2%精度；在Xilinx FPGA上，DRAM访问次数降至现有方法的18%。

Conclusion: 该框架在保持精度的同时显著提升了硬件效率，适用于边缘计算设备。

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [150] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Key words: 时空交通预测,可变形卷积,深度学习,智能交通系统

TL;DR: 论文提出了一种基于可变形动态卷积网络（DDCN）的交通预测方法，克服了传统方法的局限性，实现了高效且准确的预测。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有方法难以捕捉交通数据的时空异质性，且图神经网络（GNN）在大规模数据上的可扩展性受限。为了解决这些问题，作者提出了DDCN。

Method: DDCN通过可变形滤波器动态建模时空异质性，采用编码器-解码器结构，结合空间和时空注意力模块，突出重要特征。解码器通过前馈模块补充输出。

Result: 在四个真实数据集上，DDCN表现出竞争力，验证了其高效性和准确性。

Conclusion: DDCN证明了基于CNN的方法在时空交通预测中的潜力与有效性。

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [151] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Key words: 扩散模型、DDIM反演、Direct Preference Optimization、文本到图像生成

TL;DR: Inversion-DPO是一种新型的扩散模型对齐框架，通过DDIM反演优化替代奖励建模，显著提高了训练精度和效率。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有对齐方法计算开销大且可能影响模型精度的问题。

Method: 利用DDIM反演将DPO应用于扩散模型，避免辅助奖励模型的使用。

Result: 在文本到图像生成和组合图像生成任务中表现优于现有方法。

Conclusion: Inversion-DPO为复杂生成任务提供高效、高精度的对齐方案。

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [152] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Key words: 时空预测、视觉基础模型、重新编程、模态对齐、时间建模

TL;DR: ST-VFM是一种创新框架，通过重新编程视觉基础模型（VFMs）来提升时空预测能力，解决了时空数据中的模态差距和时间建模不足问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的LLMs在时间序列预测中表现不足，尤其缺乏对时空相关性的建模能力，而VFMs虽具有强大的空间先验，但无法直接处理时空任务。

Method: ST-VFM采用双分支架构，结合原始时空输入和辅助流输入，并通过两个重新编程阶段（前VFM和后VFM）增强时间建模和模态对齐。

Result: 在十个时空数据集上的实验表明，ST-VFM优于现有方法，展示了模型对不同VFM主干（如DINO、CLIP、DEIT）的适应性和鲁棒性。

Conclusion: ST-VFM为时空预测提供了一个通用且强大的框架，验证了其通过重新编程VFMs解决时空任务的有效性。

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [153] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Key words: 水下图像恢复, GAN, 多生成器网络, 判别器选择, PSNR

TL;DR: 论文提出xOp-GAN，一种新型GAN模型，通过多个专家生成器网络分别处理不同质量范围的图像，利用判别器选择最佳恢复图像，显著提升了水下图像恢复性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 水下图像恢复因复杂的光传播、散射和深度依赖性衰减导致多种变形伪影，传统的单一生成器网络难以应对这种异质性挑战。

Method: xOp-GAN采用多个专家生成器网络，每个生成器针对特定质量范围的图像进行训练，恢复时由判别器根据感知置信分数选择最佳结果。

Result: 在LSUI数据集上，xOp-GAN的PSNR高达25.16 dB，显著优于单一回归器模型，且复杂度更低。

Conclusion: xOp-GAN通过多生成器网络结合判别器的动态选择机制，首次在回归任务中实现性能提升，为水下图像恢复提供了新思路。

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [154] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Key words: 野猫监测,图像识别,PPGNet模型,对比学习,计算机视觉

TL;DR: 本文提出了一种改进的PPGNet模型（PPGNet-Cat），用于通过图像识别野生野猫，以提高对其活动的监测效果，模型性能优异。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于野猫对澳大利亚野生动物造成严重威胁，监测它们的活动至关重要。研究旨在利用图像识别技术改进监测方法。

Method: 通过改进原本用于识别阿穆尔虎的PPGNet模型，针对野猫图像特点进行调整，并探索对比学习方法来优化识别效果。

Result: PPGNet-Cat在识别野猫时表现出色，平均精度（mAP）达0.86，rank-1准确率达0.95。

Conclusion: PPGNet-Cat是一种高效的野猫识别模型，为野生动物监测提供了有力工具。

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [155] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Key words: 直肠癌, 淋巴结转移, 变分自编码器, MRI, 影像学诊断

TL;DR: 论文提出了一种基于变分自编码器（VAE）的特征编码模型，用于改进直肠癌淋巴结转移（LNM）的影像学诊断准确性，取代传统基于形态学的方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的基于淋巴结大小、形状和纹理形态的放射学标准诊断准确性有限，因此需要一个更高效且解释性更强的特征编码模型。

Method: 使用变分自编码器（VAE）作为特征编码器，替代传统的预训练卷积神经网络（CNN），利用其生成模型特性捕捉视觉特征和有意义的数据模式。

Result: 提出的VAE-MLP模型在内部MRI数据集（168例患者）上实现了AUC 0.86 +/- 0.05，灵敏度0.79 +/- 0.06和特异性0.85 +/- 0.05的先进性能。

Conclusion: VAE作为一种特征编码器，比传统CNN更有效且具有解释性，能显著提升淋巴结转移的诊断准确性。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [156] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Key words: 户外广告, 文本识别, 多模态视觉语言模型, CNN, 天气干扰

TL;DR: 本文系统地比较了多模态视觉语言模型（VLMs）与传统CNN-based OCR方法在户外广告文本识别中的表现，发现VLMs在整体场景理解上更优，而轻量级CNN方法在裁剪文本识别中仍具竞争力且计算成本更低。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 户外广告文本识别在真实场景中仍具挑战性，传统OCR方法在复杂环境下表现不佳，而新兴的VLMs可能提供更好的解决方案。

Method: 通过系统评估代表性VLMs（如Qwen 2.5 VL 3B）与传统CNN-based OCR方法（如PaddleOCRv4）在两个公开数据集（ICDAR 2015和SVT）上的表现，并加入合成天气干扰模拟真实场景。

Result: 结果显示，尽管部分VLMs在全景推理上表现优异，轻量级CNN方法在裁剪文本识别中仍能提供接近的精度，且计算成本更低。

Conclusion: VLMs在户外广告文本识别中展示了潜力，但轻量级CNN方法在特定场景下仍具优势，适合边缘部署。本文发布了天气增强的评测基准和代码以促进未来研究。

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [157] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Key words: 抽象视觉推理（AVR）、统一框架、生成式模型、零样本推理、多任务学习

TL;DR: 提出了统一的生成式求解器UCGS，通过多任务训练实现多种抽象视觉推理任务的一站式解决，并具备零样本推理能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决现有深度AVR求解器因任务特定设计导致的高成本问题，提出统一框架以减少重训练和架构调整的需求。

Method: 将AVR任务重新定义为问题面板中目标图像可预测性的估计问题，并训练一个统一的生成模型（UCGS）解决多任务。

Result: UCGS通过单轮多任务训练，展示了跨多种AVR任务的抽象推理能力，包括对未见任务的零样本推理。

Conclusion: UCGS验证了统一框架在AVR任务中的可行性和高效性，为未来通用推理系统提供了新方向。

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [158] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Key words: 动态面部表情识别, 跨模态对齐, 语义文本增强, GRACE

TL;DR: 论文提出了GRACE方法，通过动态运动建模、语义文本提炼和跨模态对齐，解决动态面部表情识别中的文本利用不足和无关面部动态过滤问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 当前动态面部表情识别方法未能充分利用文本中的情感线索，且缺乏有效的无关面部动态过滤机制。

Method: GRACE结合动态运动建模、语义文本提炼（CATE模块）和基于熵正则化的跨模态对齐。

Result: 在三个基准数据集上显著提升识别性能，尤其在模糊或不平衡情感分类任务中达到SOTA。

Conclusion: GRACE通过跨模态对齐和文本增强，有效提升了动态面部表情识别性能。

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [159] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Key words: 空间频率调制,语义分割,自适应重采样,多尺度上采样

TL;DR: 提出了一种新的空间频率调制（SFM）方法，通过调制高频特征到低频以减轻下采样过程中的失真，并在上采样时恢复高频信息。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 高频信息对语义分割精度至关重要，但传统下采样方法容易导致失真或混叠。

Method: 通过自适应重采样（ARS）调制高频特征，使用多尺度自适应上采样（MSAU）恢复高频信息。

Result: 方法有效减轻混叠并保留细节，适用于多种任务（如图像分类、实例分割等）。

Conclusion: SFM方法在多种架构中表现优异，具有广泛适用性。

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [160] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Key words: 文本到图像,多实例生成,关系感知,解耦学习,RaDL

TL;DR: RaDL框架通过关系感知解耦学习解决了文本到图像模型中多实例生成的关系和属性问题，显著提升了生成效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的文本到图像生成方法在多实例生成时难以处理实例间关系和多属性泄露问题，需要一种新的方法来提升生成质量。

Method: RaDL框架通过可学习参数增强实例特定属性，并利用关系注意力生成关系感知图像特征，基于全局提示中的动作动词。

Result: 在COCO-Position、COCO-MIG和DrawBench等基准测试中，RaDL在位置准确性、多属性考虑和实例关系方面显著优于现有方法。

Conclusion: RaDL为多实例图像生成提供了同时考虑实例间关系和多重属性的解决方案。

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [161] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Key words: SketchDNN, CAD草图生成, 扩散模型, Gaussian-Softmax, SketchGraphs

TL;DR: 提出了SketchDNN，一种通过连续-离散扩散过程联合建模连续参数和离散类别的生成模型，采用Gaussian-Softmax扩散技术，显著提升了CAD草图生成质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 解决CAD草图中原始参数化的异质性和原始元素的排列不变性问题。

Method: 使用Gaussian-Softmax扩散技术，通过高斯噪声扰动的logits通过softmax变换投影到概率单纯形上。

Result: 在SketchGraphs数据集上，FID从16.04降至7.80，NLL从84.8降至81.33，达到新的SOTA。

Conclusion: Gaussian-Softmax扩散技术有效解决了CAD草图生成中的关键挑战，并显著提升了生成质量。

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [162] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Key words: Vision Transformers, Frequency-Dynamic Attention Modulation, 低频滤波器, 高频细节, 计算机视觉

TL;DR: 本文提出了频率动态注意力调制（FDAM）方法，通过逆置注意力和频率动态缩放技术，解决ViTs中高频细节丢失问题，提升模型性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: Vision Transformers (ViTs) 中的注意力机制使每层成为低通滤波器，导致高频细节丢失。为此，本文提出了一种基于电路理论的频率动态调制策略。

Method: 提出了FDAM方法，包括Attention Inversion（AttInv）和Frequency Dynamic Scaling（FreqScale）。AttInv通过逆置低通滤波器生成高通滤波，动态结合二者；FreqScale对不同频率成分进行加权调整。

Result: FDAM在SegFormer、DeiT和MaskDINO等模型中表现一致提升，适用于语义分割、目标检测和实例分割任务。在遥感检测中也取得了单尺度设置下的最优效果。

Conclusion: FDAM通过调制ViTs的频率响应，避免了表示崩溃，显著提升了模型性能。

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [163] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Key words: 掩码图像建模, 无监督领域适应, 稀疏信号重建, 领域泛化, 图像分割

TL;DR: 论文重新定义了掩码重建为稀疏信号重建问题，并提出MaskTwins框架，通过互补掩码增强领域不变特征提取。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有工作仅将掩码视为图像变形的特殊形式，缺乏理论分析，未充分挖掘掩码重建在特征提取中的潜力。

Method: 将掩码重建理论化为稀疏信号问题，提出MaskTwins框架，通过互补掩码一致性增强特征学习。

Result: MaskTwins在自然和生物图像分割中表现优异，无需单独预训练即可提取领域不变特征。

Conclusion: MaskTwins为领域自适应分割提供了新范式，展示了互补掩码在特征提取中的优势。

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [164] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Key words: 姿态分析, 心理状态推断, 体育分析, 弱监督, 板球

TL;DR: 论文探讨了基于姿态的心理状态推断在体育领域的应用，通过板球比赛中的数据验证了其有效性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究旨在解决人类数据敏感性导致的视觉诊断挑战，并以体育场景为替代方案积累情感状态数据。

Method: 通过运动分析从活动视频中识别人体意图，利用现有统计数据作为弱监督验证。

Result: 方法在区分攻击性和防守性击球意图时的F1分数超过75%，AUC-ROC超过80%。

Conclusion: 姿态分析是推断意图的有效方法，即使在数据噪声存在下也能提供强信号。

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [165] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Key words: 无人监督域适应, 目标检测, 红外域, 解耦-耦合, 光谱分解

TL;DR: 本文提出了一种基于解耦-耦合策略的SS-DC框架，用于解决从可见光域到红外域的无人监督域自适应目标检测问题。通过光谱自适应幂等解耦模块和新颖的空间-光谱耦合方法，显著提升了性能。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的方法将可见光域视为单一域，忽略了其中的多子域（如白天、夜晚、雾天等）。本文认为解耦域不变和域特定特征有助于跨域适应。

Method: 提出SS-DC框架，包括光谱自适应幂等解耦模块（SAID）和空间-光谱耦合方法。SAID通过频带分解解耦特征，并结合自蒸馏损失优化解耦。

Result: 实验表明，该方法在多个RGB-IR数据集上显著提升基线性能，优于现有方法。

Conclusion: 解耦-耦合策略有效解决了多子域适应问题，提升了RGB-IR域自适应目标检测的性能。

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [166] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Key words: 新类发现, 多视图数据, 矩阵分解, 聚类

TL;DR: 本文提出了一种多视图新类发现框架IICMVNCD，首次探索了多视图数据中的新类发现问题，通过视图内和视图间关联提升聚类稳定性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的新类发现方法主要针对单视图数据且依赖伪标签，容易受到数据噪声和特征维度影响。为克服这些限制，研究探索多视图数据并设计更鲁棒的方法。

Method: 提出IICMVNCD框架，在视图内利用矩阵分解捕获分布一致性，在视图间通过加权融合和动态调整视图权重指导新类聚类。

Result: 实验验证了该方法的有效性。

Conclusion: IICMVNCD框架在多视图新类发现中表现优异，解决了现有方法的局限性。

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [167] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Key words: 人脸反欺骗,FAS,视觉语言模型,VLM,元域策略,指令调优

TL;DR: 提出了一种名为InstructFLIP的新框架，结合视觉语言模型（VLM）和元域策略，解决人脸反欺骗（FAS）中的语义理解和跨域训练冗余问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统方法在跨域泛化和攻击类型语义理解方面存在不足，需要通过更智能的模型提升性能。

Method: 利用VLM增强视觉输入感知，并通过指令调优的元域策略学习统一模型，将指令解耦为内容（攻击语义）和风格（环境与相机特性）。

Result: InstructFLIP在准确性上优于现有最佳模型，并显著减少了跨域训练冗余。

Conclusion: InstructFLIP通过结合VLM和元域策略，成功提升了FAS的性能和泛化能力。

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [168] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Key words: 对抗攻击、人脸识别、特征空间、非适应性查询、属性控制

TL;DR: 提出一种利用特征空间结构性生成对抗人脸的新方法，仅需单次非适应性查询即可高成功率攻击商用FRS。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 针对人脸识别系统（FRS）的安全威胁，研究高效、非适应性的对抗攻击方法，避免依赖迁移性和开源替代模型。

Method: 基于FRS特征空间的结构特性，利用共享属性（如性别、种族）的子空间生成对抗人脸，无需迭代优化。

Result: 以单次100张人脸的非适应性查询，成功攻击AWS CompareFaces API的成功率达93%以上，并可控属性。

Conclusion: 该方法高效、隐蔽且灵活，为FRS的安全防御提供了新的研究方向。

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [169] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Key words: 低光照图像增强,小波变换,特征解耦,立体视觉,高频恢复

TL;DR: 提出了一种基于小波变换的低光照立体图像增强方法，通过特征空间解耦解决现有方法的特征纠缠问题。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 低光照图像存在复杂退化，现有方法将所有退化因素编码在单一潜在空间中，导致特征高度纠缠和模型易学习捷径。

Method: 使用小波变换将特征空间分解为低频分支（光照调整）和高频分支（纹理增强），并设计了高频引导的跨视角交互模块（HF-CIM）和细节纹理增强模块（DTEM）。

Result: 实验结果表明，该方法在光照调整和高频信息恢复方面具有显著优势。

Conclusion: 该方法通过小波变换和特征解耦有效提升了低光照立体图像的增强效果。

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [170] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Key words: 文化遗产保护, 图像处理, 机器学习, 3D重建, 超分辨率

TL;DR: 本文提出了三种先进技术（Fractal Convolution、SSTF和Super Resolution）用于印度文化遗产的数字保护与修复，结合机器学习和计算机视觉技术，实现了高精度图像处理和自动化，同时兼顾成本效益与传统创新平衡。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现代数字化技术为文化遗产的保护与修复带来了革新，本文旨在利用机器学习、深度学习和计算机视觉等先进技术，针对印度古迹的特殊性，开发高效且经济的自动化解决方案。

Method: 提出了三种方法：1)基于图像处理的Fractal Convolution分割技术；2)针对Bankura Terracotta Temples的自敏感瓷砖填充(SSTF)方法；3)结合新型数据增强技术MosaicSlice的超分辨率策略。

Result: 这些方法实现了无缝区域填充和高细节瓷砖生成，同时保持图像质量和文化遗产的原始美学，显著提升了文化遗产保护的效率和效果。

Conclusion: 本研究通过结合传统与创新，为文化遗产保护提供了高效、经济的自动化解决方案，推动了该领域的技术进步与美学保护。

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [171] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Key words: 支气管肺发育不良, 深度学习, 胸部X光片, 早产儿, ResNet-50

TL;DR: 研究通过深度学习模型利用极度早产儿24小时内的胸部X光片预测支气管肺发育不良（BPD）的预后，模型表现优于传统方法。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: BPD是一种影响早产儿的慢性肺病，现有预防手段风险高，需开发非侵入性预测工具。

Method: 使用ResNet-50模型，通过特定领域的预训练、渐进层冻结和CutMix增强技术，分析163名早产儿的胸部X光片。

Result: 最佳模型AUROC为0.78，平衡准确率为0.69，F1分数0.67，优于ImageNet初始化和传统方法。

Conclusion: 特定领域的预训练结合渐进冻结技术，能有效预测BPD预后，且适合临床部署。

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [172] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Key words: 空气污染物预测,多变量自回归,MVAR,气象耦合,空间响应

TL;DR: 提出了一种多变量自回归空气污染物预测模型MVAR，解决了现有研究对多污染物交互作用及空间响应的忽视，并通过气象耦合空间变换器提升了预测效果。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 空气污染物对环境和健康构成重大威胁，现有研究多关注单一污染物预测，忽略了多污染物间的交互作用及其空间响应。

Method: 提出了MVAR模型，结合了多变量自回归训练范式和气象耦合空间变换器，减少了对长时间窗口输入的依赖，提升了数据利用效率。

Result: 实验结果表明，MVAR在75个城市的6种主要污染物数据上表现优于现有方法，实现了120小时长期预测。

Conclusion: MVAR模型在多变量空气污染物预测中表现优异，验证了其架构的有效性和实用性。

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [173] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Key words: 扩散模型,离散潜在代码,图像生成,Simplicial Embeddings,自监督学习

TL;DR: 本文探讨了扩散模型中输入条件的作用，并提出一种离散潜在代码（DLC）表示方法，提升了样本生成质量、组合性和泛化能力。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 研究扩散模型成功的原因，并提出一种新的图像表示方法以改进样本生成效果和组合性。

Method: 引入离散潜在代码（DLC），通过自监督学习目标训练Simplicial Embeddings，并将其应用于扩散模型。

Result: DLC提升了图像生成质量，在无条件图像生成任务上达到新SOTA，并支持生成训练分布外的样本。

Conclusion: DLC是一种有效的图像表示方法，能提升扩散模型性能并支持组合性生成。

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [174] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Key words: 无监督学习, 视觉表示学习, 对比学习, 聚类

TL;DR: Cluster Contrast (CueCo) 是一种结合对比学习和聚类方法的无监督视觉表示学习方法，通过同时分散和对齐特征表示，提升类别间分离性和类别内紧凑性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 为了解决无监督视觉表示学习中特征表示的对齐与分散问题，结合对比学习和聚类方法的优势。

Method: CueCo 使用两个神经网络（查询和键），通过对比损失增强类别间分离性，通过聚类目标提升类别内紧凑性。

Result: 在 CIFAR-10、CIFAR-100 和 ImageNet-100 上分别达到 91.40%、68.56% 和 78.65% 的 top-1 分类准确率（ResNet-18 骨干网络）。

Conclusion: 通过对比学习与聚类的结合，CueCo 为无监督视觉表示学习提供了新方向。

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [175] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Key words: 神经先验建模, 标准化流, 6D旋转, 人体姿态

TL;DR: 论文提出了一种基于标准化流的数据驱动方法，用于对人体姿态进行神经先验建模。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 传统方法通常依赖启发式或低表达能力的替代方案，缺乏灵活性和理论基础，本文旨在解决这一问题。

Method: 采用RealNVP学习6D旋转格式表示的姿态的灵活密度分布，并通过反转Gram-Schmidt过程解决6D旋转流形上的分布建模挑战。

Result: 通过定性和定量评估验证了方法有效性，并通过消融实验分析了其影响。

Conclusion: 本文为将姿态先验集成到人体运动捕捉和重建流程中提供了坚实的概率基础。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [176] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Key words: AutoVDC, 视觉语言模型, 自动驾驶, 数据清洗

TL;DR: 提出AutoVDC框架，利用视觉语言模型自动检测视觉数据集中的标注错误，提升自动驾驶数据集质量。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 人工标注数据集费时费力且易出错，AutoVDC框架旨在通过自动化方法提高数据质量。

Method: 利用视觉语言模型（VLMs）自动识别标注错误，并在KITTI和nuImages数据集上验证，测试不同VLMs的效果及微调影响。

Result: AutoVDC在错误检测和数据清洗实验中表现优异，显著提升数据集可靠性。

Conclusion: AutoVDC能有效改进自动驾驶大规模数据集质量。

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [177] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Key words: 组合图像检索, 假阴性, 硬负样本采样, 人类偏好, QuRe

TL;DR: 论文提出了一种名为QuRe的方法，通过优化奖励模型和硬负样本采样策略，解决了组合图像检索（CIR）中因假阴性导致的相关性问题，并在新数据集HP-FashionIQ上验证了其与人类偏好的一致性。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 现有的组合图像检索方法仅关注目标图像检索，而忽略了其他图像的相关性，导致可能检索到不相关图像，降低用户满意度。

Method: 提出Query-Relevant Retrieval through Hard Negative Sampling（QuRe），包括优化奖励模型目标和引入硬负样本采样策略，以减少假阴性。

Result: QuRe在FashionIQ和CIRR数据集上实现了最先进的性能，并在HP-FashionIQ数据集上表现出与人类偏好最强的对齐性。

Conclusion: QuRe通过减少假阴性提高了组合图像检索的准确性，并通过新数据集验证了其与人类偏好的对齐性。

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [178] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Key words: 深度学习、医学图像分类、CNN、Keras、PyTorch、JAX

TL;DR: 该论文对比了Keras、PyTorch和JAX在医学图像分类任务中的性能，使用PathMNIST数据集评估了训练效率、分类精度和推理速度。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 尽管深度学习在医学图像分类中取得了显著进展，但不同框架（如Keras、PyTorch和JAX）的性能对比仍然缺乏研究。

Method: 研究通过PathMNIST数据集，对这些框架的CNN实现进行了全面分析，评估了训练效率、分类精度和推理速度。

Result: 研究揭示了计算速度与模型精度之间的权衡，为医学图像分析的研究者和从业者提供了有价值的见解。

Conclusion: 该研究为选择合适框架提供了参考，强调了不同框架在医学图像任务中的优缺点。

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [179] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Key words: 眼动数据, 深度学习, 医学图像, 意图建模, Transformer

TL;DR: RadGazeIntent 是一种基于深度学习的模型，旨在捕捉放射科医生在解读医学图像时的意图驱动行为，通过处理眼动数据预测其诊断目标。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 放射科医生通过眼动导航和解读医学图像，但现有模型未能捕捉每次注视背后的意图。本文提出 RadGazeIntent 来模拟这种意图驱动的搜索行为。

Method: 采用基于 Transformer 的架构，处理眼动数据的时空维度，将其转换为诊断意图的粗粒度表示。利用医学眼动数据集创建三个意图标记子集：RadSeq、RadExplore 和 RadHybrid。

Result: RadGazeIntent 能够预测放射科医生在特定时刻检查的发现，在所有意图标记数据集上优于基线方法。

Conclusion: RadGazeIntent 成功模拟了放射科医生的意图驱动行为，为医学图像解读提供了新的研究视角。

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [180] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Key words: DAM-QA, VQA, 区域感知模型, 文本密集图像, Describe Anything Model

TL;DR: 论文介绍了DAM-QA框架，利用Describe Anything Model（DAM）的区域感知能力，提升文本密集图像中的视觉问答（VQA）任务表现。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 针对文本密集图像中的VQA任务，需要细粒度提取文本信息。DAM的区域描述能力有助于解决这一问题。

Method: 提出DAM-QA框架，整合多区域视图的答案，优化文本相关证据的识别。

Result: 在六个VQA基准测试中，DAM-QA显著优于基线DAM（DocVQA提升7+分），且模型参数更少。

Conclusion: DAM类模型结合高效使用策略，在文本密集和更广泛的VQA任务中具有潜力。

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [181] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Key words: 稀疏自编码器、血液学、机制解释、医学影像、CytoSAE

TL;DR: 该论文提出了CytoSAE稀疏自编码器，用于血液学图像的机制解释，能够在泛化和跨域数据集中识别形态学相关概念，并在患者和疾病特定概念生成中表现出色。

<details>
  <summary>Details</summary>

Main category: cs.CV

Motivation: 由于医学影像领域缺乏解释基础模型推断的工具，作者探索了稀疏自编码器（SAEs）在血液学中的应用。

Method: 作者提出了CytoSAE，一种在40,000多张外周血单细胞图像上训练的稀疏自编码器，用于识别形态学相关概念。

Result: CytoSAE在泛化和跨域数据集中表现出色，能够生成患者和疾病特定概念，并在AML亚型分类任务中达到与先进方法相当的性能。

Conclusion: CytoSAE为血液学影像提供了可解释的工具，能够在亚细胞水平上进行机制解释。

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [182] [Recent results on searches with boosted Higgs bosons at CMS](https://arxiv.org/abs/2507.11977)
*Farouk Mokhtar*

Key words: 希格斯玻色子,LHC,CMS实验,标准模型

TL;DR: 这篇论文讨论了在LHC上研究增强的希格斯玻色子，以探测高能尺度下的希格斯玻色子耦合并寻找标准模型之外的物理迹象。

<details>
  <summary>Details</summary>

Main category: hep-ex

Motivation: 通过研究增强的希格斯玻色子，探索高能尺度下的希格斯玻色子耦合及标准模型之外的物理现象。

Method: 利用CMS实验中的创新重建和标记技术，提升在这一高难度区域的灵敏度。

Result: 展示了关于增强希格斯玻色子搜索的最新结果。

Conclusion: 创新技术在高能区的应用提高了对希格斯玻色子的研究灵敏度。

Abstract: The study of boosted Higgs bosons at the LHC provides a unique window to
probe Higgs boson couplings at high energy scales and search for signs of
physics beyond the standard model. In these proceedings, we present recent
results on boosted Higgs boson searches at the CMS experiment, highlighting
innovative reconstruction and tagging techniques that enhance sensitivity in
this challenging regime.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [183] [Universal Fourier Neural Operators for Micromechanics](https://arxiv.org/abs/2507.12233)
*Binh Huy Nguyen,Matti Schneider*

Key words: Fourier Neural Operators, micromechanics, FFT, homogenization, deep learning

TL;DR: 该论文提出使用Fourier神经算子（FNOs）解决微观力学中的细胞问题，结合快速傅里叶变换（FFT）方法，构建了一个无需训练即可预测任意刚度分布的FNO代理模型。

<details>
  <summary>Details</summary>

Main category: cs.CE

Motivation: 传统深度学习方法在解决均质化中的细胞问题时速度和泛化性不足，且缺乏明确的方向。作者希望通过结合FFT方法和FNOs，提供一个高效且通用的解决方案。

Method: 作者构建了一个FNO代理模型，模仿FFT方法的基本方案，无需训练即可预测任意刚度分布的细胞问题解决方案。模型的保真度和运行效率与FFT方法相当。

Result: FNO模型能够处理任意材料对称性、多相材料及复杂界面的大规模问题（超过1亿体素），且运行时间与经典FFT求解器成比例。

Conclusion: FNOs在微观力学问题中展现出巨大潜力，FFT方法与FNOs的结合有望促进两个领域的相互发展。

Abstract: \noindent Solving cell problems in homogenization is hard, and available
deep-learning frameworks fail to match the speed and generality of traditional
computational frameworks. More to the point, it is generally unclear what to
expect of machine-learning approaches, let alone single out which approaches
are promising. In the work at hand, we advocate Fourier Neural Operators (FNOs)
for micromechanics, empowering them by insights from computational
micromechanics methods based on the fast Fourier transform (FFT). We construct
an FNO surrogate mimicking the basic scheme foundational for FFT-based methods
and show that the resulting operator predicts solutions to cell problems with
\emph{arbitrary} stiffness distribution only subject to a material-contrast
constraint up to a desired accuracy. In particular, there are no restrictions
on the material symmetry like isotropy, on the number of phases and on the
geometry of the interfaces between materials. Also, the provided fidelity is
sharp and uniform, providing explicit guarantees leveraging our physical
empowerment of FNOs. To show the desired universal approximation property, we
construct an FNO explicitly that requires no training to begin with. Still, the
obtained neural operator complies with the same memory requirements as the
basic scheme and comes with runtimes proportional to classical FFT solvers. In
particular, large-scale problems with more than 100 million voxels are readily
handled. The goal of this work is to underline the potential of FNOs for
solving micromechanical problems, linking FFT-based methods to FNOs. This
connection is expected to provide a fruitful exchange between both worlds.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [184] [Formal Verification of Neural Certificates Done Dynamically](https://arxiv.org/abs/2507.11987)
*Thomas A. Henzinger,Konstantin Kueffner,Emily Yu*

Key words: 神经证书, 控制屏障函数, 运行时监控, 实时验证, 安全性

TL;DR: 提出了一个轻量级的运行时监控框架，用于实时验证神经证书的安全性，避免静态验证的可扩展性问题。

<details>
  <summary>Details</summary>

Main category: cs.SC

Motivation: 传统的神经证书验证方法（如屏障函数）因状态空间探索而面临可扩展性挑战，需要一种更高效的运行时验证方法。

Method: 设计了一个运行时监控框架，通过观察系统部署中的行为，实时验证证书的有效性，且无需访问底层控制策略。

Result: 在ReLU-based控制屏障函数的案例中证明了该框架的实用性，能够及时发现安全违规和不正确的证书。

Conclusion: 该框架为神经证书提供了一种轻量级且高效的运行时验证替代方案。

Abstract: Neural certificates have emerged as a powerful tool in cyber-physical systems
control, providing witnesses of correctness. These certificates, such as
barrier functions, often learned alongside control policies, once verified,
serve as mathematical proofs of system safety. However, traditional formal
verification of their defining conditions typically faces scalability
challenges due to exhaustive state-space exploration. To address this
challenge, we propose a lightweight runtime monitoring framework that
integrates real-time verification and does not require access to the underlying
control policy. Our monitor observes the system during deployment and performs
on-the-fly verification of the certificate over a lookahead region to ensure
safety within a finite prediction horizon. We instantiate this framework for
ReLU-based control barrier functions and demonstrate its practical
effectiveness in a case study. Our approach enables timely detection of safety
violations and incorrect certificates with minimal overhead, providing an
effective but lightweight alternative to the static verification of the
certificates.

</details>


### [185] [FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization](https://arxiv.org/abs/2507.12366)
*Yifei Zhou,Xuchu Huang,Chenyu Ni,Min Zhou,Zheyu Yan,Xunzhao Yin,Cheng Zhuo*

Key words: FactorHD, 超维计算, 神经符号AI, 类-子类关系, 分解算法

TL;DR: FactorHD是一种新型超维计算模型，能高效表示和分解复杂的类-子类关系，显著提升计算效率和准确性。

<details>
  <summary>Details</summary>

Main category: cs.SC

Motivation: 现有的超维计算模型在处理复杂的类-子类关系时面临分解难题，影响神经符号AI系统的性能。

Method: FactorHD引入符号编码方法和高效分解算法，通过记忆子句选择性消除冗余类别。

Result: FactorHD在表示规模为10^9时实现了约5667倍的加速，与ResNet-18结合在Cifar-10上的分解准确率达92.48%。

Conclusion: FactorHD克服了现有模型的局限，为神经符号AI提供了高效的工具。

Abstract: Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical
analysis and reasoning. Hyperdimensional Computing (HDC), a promising
brain-inspired computational model, is integral to neuro-symbolic AI. Various
HDC models have been proposed to represent class-instance and class-class
relations, but when representing the more complex class-subclass relation,
where multiple objects associate different levels of classes and subclasses,
they face challenges for factorization, a crucial task for neuro-symbolic AI
systems. In this article, we propose FactorHD, a novel HDC model capable of
representing and factorizing the complex class-subclass relation efficiently.
FactorHD features a symbolic encoding method that embeds an extra memorization
clause, preserving more information for multiple objects. In addition, it
employs an efficient factorization algorithm that selectively eliminates
redundant classes by identifying the memorization clause of the target class.
Such model significantly enhances computing efficiency and accuracy in
representing and factorizing multiple objects with class-subclass relation,
overcoming limitations of existing HDC models such as "superposition
catastrophe" and "the problem of 2". Evaluations show that FactorHD achieves
approximately 5667x speedup at a representation size of 10^9 compared to
existing HDC models. When integrated with the ResNet-18 neural network,
FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [186] [Quantum Machine Learning in Multi-Qubit Phase-Space Part I: Foundations](https://arxiv.org/abs/2507.12117)
*Timothy Heightman,Edward Jiang,Ruth Mora-Soto,Maciej Lewenstein,Marcin Płodzień*

Key words: 量子机器学习, 相空间方法, Stratonovich-Weyl对应, Hilbert空间, 维度灾难

TL;DR: 量子机器学习（QML）利用量子系统的特性处理经典数据，但面临Hilbert空间指数增长的挑战。相空间方法通过准概率函数编码量子态，本文基于Stratonovich-Weyl对应关系，构建了适用于单比特和多比特系统的可组合相空间形式，为QML提供新路径。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 量子机器学习（QML）需要处理Hilbert空间指数增长的问题，传统的状态矢量表示在经典模拟中存在限制。相空间方法通过准概率函数提供了一种替代方案，本文旨在构建一种适用于量子系统的相空间形式，以克服维度灾难并推动QML发展。

Method: 基于前人对量子比特相空间和Stratonovich-Weyl对应关系的研究，构建了一种封闭且可组合的动力学形式，适用于单比特和多比特系统。该方法将Pauli群的算子代数替换为辛流形上的函数动力学，并通过谐波支持线性缩放多比特系统的计算复杂度。

Result: 本文提出的相空间形式成功将QML的高维计算问题转化为线性尺度问题，并通过变分建模为QML提供了新的研究路径。

Conclusion: 相空间方法通过准概率函数和Stratonovich-Weyl对应关系，为量子机器学习提供了一种高效的计算框架，解决了维度灾难问题，并推动了基于变分建模的QML研究。

Abstract: Quantum machine learning (QML) seeks to exploit the intrinsic properties of
quantum mechanical systems, including superposition, coherence, and quantum
entanglement for classical data processing. However, due to the exponential
growth of the Hilbert space, QML faces practical limits in classical
simulations with the state-vector representation of quantum system. On the
other hand, phase-space methods offer an alternative by encoding quantum states
as quasi-probability functions. Building on prior work in qubit phase-space and
the Stratonovich-Weyl (SW) correspondence, we construct a closed, composable
dynamical formalism for one- and many-qubit systems in phase-space. This
formalism replaces the operator algebra of the Pauli group with function
dynamics on symplectic manifolds, and recasts the curse of dimensionality in
terms of harmonic support on a domain that scales linearly with the number of
qubits. It opens a new route for QML based on variational modelling over
phase-space.

</details>


### [187] [BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search](https://arxiv.org/abs/2507.12189)
*Azhar Ikhtiarudin,Aditi Das,Param Thakkar,Akash Kundu*

Key words: 强化学习, 量子架构搜索, 基准测试, 变分量子算法, 量子计算

TL;DR: BenchRL-QAS是一个统一的基准框架，用于系统性评估量子架构搜索（QAS）中的强化学习（RL）算法。研究在2至8量子位的不同任务中测试了九种RL算法，并提出了平衡多指标的排名方法。结果显示RL在量子分类中表现优于基线，但没有单一算法在所有任务中表现最优。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 量子架构搜索领域缺乏系统性基准测试工具，无法全面评估RL算法的性能差异。

Method: 研究开发了BenchRL-QAS框架，测试九种RL算法在多种量子任务中的表现，并提出加权排名指标。

Result: RL在量子分类中优于基线，但算法性能因任务、量子位数和噪声而异，支持"没有免费午餐"原则。

Conclusion: 量子电路设计需要根据具体任务选择算法，系统性基准测试对推动领域发展至关重要。

Abstract: We introduce BenchRL-QAS, a unified benchmarking framework for systematically
evaluating reinforcement learning (RL) algorithms in quantum architecture
search (QAS) across diverse variational quantum algorithm tasks and system
sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including
both value-based and policy-gradient methods on representative quantum problems
such as variational quantum eigensolver, variational quantum state
diagonalization, quantum classification, and state preparation, spanning both
noiseless and realistic noisy regimes. We propose a weighted ranking metric
that balances accuracy, circuit depth, gate count, and computational
efficiency, enabling fair and comprehensive comparison. Our results first
reveal that RL-based quantum classifier outperforms baseline variational
classifiers. Then we conclude that no single RL algorithm is universally
optimal when considering a set of QAS tasks; algorithmic performance is highly
context-dependent, varying with task structure, qubit count, and noise. This
empirical finding provides strong evidence for the "no free lunch" principle in
RL-based quantum circuit design and highlights the necessity of tailored
algorithm selection and systematic benchmarking for advancing quantum circuit
synthesis. This work represents the most comprehensive RL-QAS benchmarking
effort to date, and BenchRL-QAS along with all experimental data are made
publicly available to support reproducibility and future research
https://github.com/azhar-ikhtiarudin/bench-rlqas.

</details>


### [188] [Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator](https://arxiv.org/abs/2507.12256)
*Monica Lăcătuş,Matthias Möller*

Key words: 量子计算、CFD、QLBM、BGK碰撞算子、量子替代电路

TL;DR: 研究提出了一种量子替代电路（SQC）框架，用于近似BGK碰撞算子，解决了QLBM中非线性碰撞步骤的量子实现问题，无需额外辅助量子比特或后选择。

<details>
  <summary>Details</summary>

Main category: quant-ph

Motivation: 传统CFD在高雷诺数湍流模拟中存在计算瓶颈，量子算法可能提供速度优势，但QLBM中非线性碰撞步骤的量子实现仍具挑战性。

Method: 通过训练四量子比特的SQC，近似D2Q9格子中的BGK碰撞算子，物理性质（质量、动量守恒等）被保留。

Result: SQC在IBM Heron处理器上仅需2,430个原生门，且深度与网格分辨率无关，验证显示其能准确捕捉涡旋耗散和流动再循环。

Conclusion: SQC为量子CFD提供了一种高效的非线性碰撞步骤实现方案，具有潜在的实际应用价值。

Abstract: Direct numerical simulation of turbulent flows at high Reynolds numbers
remains a major challenge for traditional computational fluid dynamics (CFD)
tools running on classical computer hardware. This has motivated growing
interest in quantum algorithms for CFD to enable flow simulations on quantum
computers. The reason being that these computers are expected to deliver
potential speed-ups for certain problems. One promising quantum CFD approach is
a fully quantum implementation of the lattice Boltzmann method called QLBM.
Although efficient quantum routines are now available for the streaming step,
implementing the nonlinear, irreversible collision step with a low depth
circuit that avoids additional ancilla qubits, probabilistic post-selection and
repeated executions remains a significant challenge. In this study, we address
this challenge by introducing a framework for learning a surrogate quantum
circuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision
operator for the D2Q9 lattice. The four qubit circuit is trained to respect the
physical properties of the BGK collision operator, including mass and momentum
conservation, D8 equivariance and scale equivariance. When compiled to the gate
set used by IBM Heron processor under the assumption of full qubit
connectivity, the 15 block SQC requires only 2,430 native gates and uses
neither ancilla qubits nor post-selection or repeated executions. Moreover, its
depth is independent of the grid resolution, as collision is a local operation
that can exploit quantum parallelism to its full extent. We validate the SQC on
two benchmark flows, the Taylor Green vortex decay and the lid driven cavity,
demonstrating that it accurately captures vortex dissipation and flow
recirculation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [189] [Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility](https://arxiv.org/abs/2507.11630)
*Brendan Murphy,Dillon Bowen,Shahrad Mohammadzadeh,Julius Broomfield,Adam Gleave,Kellin Pelrine*

Key words: AI安全、微调攻击、jailbreak-tuning、防护措施、模型脆弱性

TL;DR: 本文揭示了微调技术（包括开放权重和封闭微调API）可以绕过AI系统的防护措施，生成完全服从有害指令的高质量响应，强调了当前模型对攻击的脆弱性及紧急防护需求的必要性。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究动机是展示当前AI系统防护措施的不足，尤其是通过微调技术可以轻易绕过或削弱这些防护，导致模型可能被滥用于犯罪活动。

Method: 采用了一种称为"jailbreak-tuning"的方法，通过微调技术向模型教授如何生成高质量且详细的犯罪指令响应，同时研究如何利用后门提升攻击的隐蔽性和严重性。

Result: 结果显示，包括OpenAI、Google和Anthropic在内的前沿模型完全服从有害请求，且最新模型对这一攻击方式更加脆弱。

Conclusion: 结论强调了开发防篡改防护措施的紧迫性，并建议企业和政策制定者将可微调模型的发布视为同时释放其潜在恶意版本。

Abstract: AI systems are rapidly advancing in capability, and frontier model developers
broadly acknowledge the need for safeguards against serious misuse. However,
this paper demonstrates that fine-tuning, whether via open weights or closed
fine-tuning APIs, can produce helpful-only models. In contrast to prior work
which is blocked by modern moderation systems or achieved only partial removal
of safeguards or degraded output quality, our jailbreak-tuning method teaches
models to generate detailed, high-quality responses to arbitrary harmful
requests. For example, OpenAI, Google, and Anthropic models will fully comply
with requests for CBRN assistance, executing cyberattacks, and other criminal
activity. We further show that backdoors can increase not only the stealth but
also the severity of attacks, while stronger jailbreak prompts become even more
effective in fine-tuning attacks, linking attack and potentially defenses in
the input and weight spaces. Not only are these models vulnerable, more recent
ones also appear to be becoming even more vulnerable to these attacks,
underscoring the urgent need for tamper-resistant safeguards. Until such
safeguards are discovered, companies and policymakers should view the release
of any fine-tunable model as simultaneously releasing its evil twin: equally
capable as the original model, and usable for any malicious purpose within its
capabilities.

</details>


### [190] [Challenges in GenAI and Authentication: a scoping review](https://arxiv.org/abs/2507.11775)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphall*

Key words: 认证、真实性、生成式人工智能、安全挑战、文献综述

TL;DR: 摘要概述了数字信息时代中认证与真实性的安全挑战，尤其是生成式人工智能的兴起对其影响。通过文献综述分析了88篇文档，提出了六类关键问题和研究空白。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 研究动机在于探讨生成式人工智能如何改变认证与真实性的安全挑战，以及对社会和系统安全的影响。

Method: 方法是通过文献综述，分析IEEExplorer、Scopus和ACM数据库中的88篇文档，提出六类研究问题进行分类分析。

Result: 结果总结了与图像、文本、音频和视频相关的挑战、威胁及研究空白，为未来研究提供支持。

Conclusion: 结论强调了生成式人工智能对认证安全的复杂影响，并指出了未来研究的方向。

Abstract: Authentication and authenticity have been a security challenge since the
beginning of information sharing, especially in the context of digital
information. With the advancement of generative artificial intelligence, these
challenges have evolved, demanding a more up-to-date analysis of their impacts
on society and system security. This work presents a scoping review that
analyzed 88 documents from the IEEExplorer, Scopus, and ACM databases,
promoting an analysis of the resulting portfolio through six guiding questions
focusing on the most relevant work, challenges, attack surfaces, threats,
proposed solutions, and gaps. Finally, the portfolio articles are analyzed
through this guiding research lens and also receive individualized analysis.
The results consistently outline the challenges, gaps, and threats related to
images, text, audio, and video, thereby supporting new research in the areas of
authentication and generative artificial intelligence.

</details>


### [191] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin,Shoko Imaizumi,Hitoshi Kiya*

Key words: 低秩适应、隐私保护、视觉Transformer、参数效率

TL;DR: 提出一种低秩适应方法用于训练隐私保护的视觉Transformer模型，高效冻结预训练权重，同时调整部分参数以保持性能。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 改进传统低秩适应方法，减少可训练参数数量，同时保持模型准确性。

Method: 在ViT的各层中注入可训练的秩分解矩阵，并保持部分层（如补丁嵌入层）未冻结。

Result: 方法显著减少可训练参数，同时保持与全微调几乎相同的准确性。

Conclusion: 提出的方法在隐私保护和模型性能之间取得了良好平衡。

Abstract: We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [192] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

Key words: ML安全性, 文档标准化, Model Cards, Datasheets for Datasets

TL;DR: 摘要指出当前ML安全性和ML系统、模型及数据集的文档化在研究和实践中存在不足，安全意识和文档标准化程度低，亟需改进。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 由于ML实践者和组织对安全问题的普遍忽视及文档标准化不足，ML文档质量较低，需要提升安全性文档化。

Method: 提出扩展现有ML文档标准，增加安全部分，基于Model Cards和Datasheets for Datasets标准，开发新型文档方法。

Result: 研究发现现有标准在实践中未得到充分采纳，IT安全方面常被忽视。

Conclusion: 需通过改进ML文档安全性内容，弥补ML安全领域的现有不足。

Abstract: This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


### [193] [A Privacy-Preserving Framework for Advertising Personalization Incorporating Federated Learning and Differential Privacy](https://arxiv.org/abs/2507.12098)
*Xiang Li,Yifan Lin,Yuanzhe Zhang*

Key words: 联邦学习, 差分隐私, 个性化广告, 隐私保护

TL;DR: 提出一个结合联邦学习和差分隐私的框架，优化个性化广告中的隐私保护与性能问题。

<details>
  <summary>Details</summary>

Main category: cs.CR

Motivation: 解决个性化广告中的隐私泄露和性能问题。

Method: 整合分布式特征提取、动态隐私预算分配和鲁棒模型聚合。

Result: 实验表明框架在保护隐私的同时优化了推荐准确性和系统效率。

Conclusion: 为广告推荐中的隐私保护技术提供了实用方案和理论基础。

Abstract: To mitigate privacy leakage and performance issues in personalized
advertising, this paper proposes a framework that integrates federated learning
and differential privacy. The system combines distributed feature extraction,
dynamic privacy budget allocation, and robust model aggregation to balance
model accuracy, communication overhead, and privacy protection. Multi-party
secure computing and anomaly detection mechanisms further enhance system
resilience against malicious attacks. Experimental results demonstrate that the
framework achieves dual optimization of recommendation accuracy and system
efficiency while ensuring privacy, providing both a practical solution and a
theoretical foundation for applying privacy protection technologies in
advertisement recommendation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [194] [RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection](https://arxiv.org/abs/2507.12175)
*Sungkyun Chang,Simon Dixon,Emmanouil Benetos*

Key words: RUMAA, transformer, 音乐表演分析, 乐谱对齐, 转录, 错误检测

TL;DR: RUMAA是一种基于Transformer的框架，用于音乐表演分析，统一了乐谱-表演对齐、基于乐谱的转录和错误检测任务。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有方法单独处理这些任务，而RUMAA通过预训练编码器和新型解码器整合它们，利用代理任务捕捉任务间的依赖关系。

Method: 使用预训练的乐谱和音频编码器，结合三流解码器，处理带重复符号的MusicXML乐谱与完整表演音频的对齐。

Result: 在公共钢琴音乐数据集中，RUMAA在非重复乐谱上的表现与现有最佳对齐方法相当，在带重复乐谱上表现更优，同时实现了有前景的转录和错误检测效果。

Conclusion: RUMAA通过统一多种任务，提高了音乐表演分析的效率和准确性。

Abstract: This study introduces RUMAA, a transformer-based framework for music
performance analysis that unifies score-to-performance alignment,
score-informed transcription, and mistake detection in a near end-to-end
manner. Unlike prior methods addressing these tasks separately, RUMAA
integrates them using pre-trained score and audio encoders and a novel
tri-stream decoder capturing task interdependencies through proxy tasks. It
aligns human-readable MusicXML scores with repeat symbols to full-length
performance audio, overcoming traditional MIDI-based methods that rely on
manually unfolded score-MIDI data with pre-specified repeat structures. RUMAA
matches state-of-the-art alignment methods on non-repeated scores and
outperforms them on scores with repeats in a public piano music dataset, while
also delivering promising transcription and mistake detection results.

</details>


### [195] [Quantize More, Lose Less: Autoregressive Generation from Residually Quantized Speech Representations](https://arxiv.org/abs/2507.12197)
*Yichen Han,Xiaoyang Hao,Keming Chen,Weibo Xiong,Jun He,Ruonan Zhang,Junjie Cao,Yue Liu,Bowen Li,Dongrui Zhang,Hui Xia,Huilei Fu,Kai Jia,Kaixuan Guo,Mingli Jin,Qingyun Meng,Ruidong Ma,Ruiqian Fang,Shaotong Guo,Xuhui Li,Yang Xiang,Ying Zhang,Yulong Liu,Yunfeng Li,Yuyi Zhang,Yuze Zhou,Zhen Wang,Zhaowen Chen*

Key words: TTS, 音频编解码, 多码本建模, QDAC, 自回归模型

TL;DR: 论文提出QTTS框架，利用新型音频编解码器QDAC解决现有TTS单码本表示的信息丢失问题，通过多码本建模提升合成质量和细节保留。

<details>
  <summary>Details</summary>

Main category: cs.SD

Motivation: 现有自回归TTS方法因依赖单码本表示导致信息丢失，尤其在歌唱或音乐等复杂场景下难以恢复细微细节。

Method: QTTS采用QDAC编解码器，结合双自回归结构和延迟多头预测，实现多码本建模和并行化预测。

Result: 实验表明QTTS在合成质量和表达内容保留上优于基线方法。

Conclusion: 多码本建模是提升语音和音频生成保真度的可行方向。

Abstract: Text-to-speech (TTS) synthesis has seen renewed progress under the discrete
modeling paradigm. Existing autoregressive approaches often rely on
single-codebook representations, which suffer from significant information
loss. Even with post-hoc refinement techniques such as flow matching, these
methods fail to recover fine-grained details (e.g., prosodic nuances,
speaker-specific timbres), especially in challenging scenarios like singing
voice or music synthesis. We propose QTTS, a novel TTS framework built upon our
new audio codec, QDAC. The core innovation of QDAC lies in its end-to-end
training of an ASR-based auto-regressive network with a GAN, which achieves
superior semantic feature disentanglement for scalable, near-lossless
compression. QTTS models these discrete codes using two innovative strategies:
the Hierarchical Parallel architecture, which uses a dual-AR structure to model
inter-codebook dependencies for higher-quality synthesis, and the Delay
Multihead approach, which employs parallelized prediction with a fixed delay to
accelerate inference speed. Our experiments demonstrate that the proposed
framework achieves higher synthesis quality and better preserves expressive
content compared to baseline. This suggests that scaling up compression via
multi-codebook modeling is a promising direction for high-fidelity,
general-purpose speech and audio generation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [196] [A Review of Generative AI in Computer Science Education: Challenges and Opportunities in Accuracy, Authenticity, and Assessment](https://arxiv.org/abs/2507.11543)
*Iman Reihanian,Yunfei Hou,Yu Chen,Yifei Zheng*

Key words: 生成式AI, 计算机科学教育, 准确性, 真实性, 评估

TL;DR: 本文综述了生成式AI工具（如ChatGPT和Claude）在计算机科学教育中的应用，重点关注准确性、真实性和评估。通过文献综述，总结了这些工具的挑战与机遇，并提出了平衡整合的建议。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探讨生成式AI工具在教育中的潜力与风险，为如何有效整合AI提供指导。

Method: 通过文献综述分析生成式AI工具在计算机科学教育中的应用，重点关注准确性、真实性和评估。

Result: 生成式AI提升效率并支持学生创造力，但也带来如幻觉、错误传播、偏见及内容归属问题，需结合人工监督和多措施应对。

Conclusion: 成功整合AI需平衡伦理、教学和技术因素，未来研究可关注提高准确性、维护学术诚信和开发适应性模型。

Abstract: This paper surveys the use of Generative AI tools, such as ChatGPT and
Claude, in computer science education, focusing on key aspects of accuracy,
authenticity, and assessment. Through a literature review, we highlight both
the challenges and opportunities these AI tools present. While Generative AI
improves efficiency and supports creative student work, it raises concerns such
as AI hallucinations, error propagation, bias, and blurred lines between
AI-assisted and student-authored content. Human oversight is crucial for
addressing these concerns. Existing literature recommends adopting hybrid
assessment models that combine AI with human evaluation, developing bias
detection frameworks, and promoting AI literacy for both students and
educators. Our findings suggest that the successful integration of AI requires
a balanced approach, considering ethical, pedagogical, and technical factors.
Future research may explore enhancing AI accuracy, preserving academic
integrity, and developing adaptive models that balance creativity with
precision.

</details>


### [197] [Fairness Is Not Enough: Auditing Competence and Intersectional Bias in AI-powered Resume Screening](https://arxiv.org/abs/2507.11548)
*Kevin T Webster*

Key words: 生成式AI、简历筛选、中立幻觉、偏见、能力验证

TL;DR: 该研究揭示生成式AI简历筛选工具中存在的‘中立幻觉’现象，即表面无偏见实则是模型无能的表现，建议采用双重验证框架。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 探究生成式AI在简历筛选中的根本能力问题，而非仅仅关注其是否‘无偏见’。

Method: 通过两部分实验审计八种主要AI平台，分别验证其偏见和核心能力。

Result: 发现部分模型存在复杂偏见，而另一些则仅依靠关键词匹配，缺乏实质性评估能力。

Conclusion: 建议组织与监管机构对AI招聘工具进行偏见与能力的双重验证。

Abstract: The increasing use of generative AI for resume screening is predicated on the
assumption that it offers an unbiased alternative to biased human
decision-making. However, this belief fails to address a critical question: are
these AI systems fundamentally competent at the evaluative tasks they are meant
to perform? This study investigates the question of competence through a
two-part audit of eight major AI platforms. Experiment 1 confirmed complex,
contextual racial and gender biases, with some models penalizing candidates
merely for the presence of demographic signals. Experiment 2, which evaluated
core competence, provided a critical insight: some models that appeared
unbiased were, in fact, incapable of performing a substantive evaluation,
relying instead on superficial keyword matching. This paper introduces the
"Illusion of Neutrality" to describe this phenomenon, where an apparent lack of
bias is merely a symptom of a model's inability to make meaningful judgments.
This study recommends that organizations and regulators adopt a dual-validation
framework, auditing AI hiring tools for both demographic bias and demonstrable
competence to ensure they are both equitable and effective.

</details>


### [198] [AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce](https://arxiv.org/abs/2507.11597)
*Richard Timpone,Yongwei Yang*

Key words: AI, data science, human-machine collaboration, ethics, automation

TL;DR: 摘要探讨了AI在科研中的潜力与局限，强调了人机协作的重要性，并警告自动化工具可能带来的风险。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究动机是评估AI在数据科学中的实际效果及其对科研效率与质量的潜在影响。

Method: 采用Truth, Beauty, and Justice (TBJ)框架，结合人机协作视角，分析AI在数据分析中的作用。

Result: AI可以辅助数据分析，但过度依赖自动化工具可能带来理解不足的风险。

Conclusion: 结论建议AI应作为数据科学家的补充工具，同时强调方法培训与伦理应用的重要性。

Abstract: AI is transforming research. It is being leveraged to construct surveys,
synthesize data, conduct analysis, and write summaries of the results. While
the promise is to create efficiencies and increase quality, the reality is not
always as clear cut. Leveraging our framework of Truth, Beauty, and Justice
(TBJ) which we use to evaluate AI, machine learning and computational models
for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024),
we consider the potential and limitation of analytic, generative, and agentic
AI to augment data scientists or take on tasks traditionally done by human
analysts and researchers. While AI can be leveraged to assist analysts in their
tasks, we raise some warnings about push-button automation. Just as earlier
eras of survey analysis created some issues when the increased ease of using
statistical software allowed researchers to conduct analyses they did not fully
understand, the new AI tools may create similar but larger risks. We emphasize
a human-machine collaboration perspective (Daugherty and Wilson 2018)
throughout the data science workflow and particularly call out the vital role
that data scientists play under VUCA decision areas. We conclude by encouraging
the advance of AI tools to complement data scientists but advocate for
continued training and understanding of methods to ensure the substantive value
of research is fully achieved by applying, interpreting, and acting upon
results most effectively and ethically.

</details>


### [199] [Small Data Explainer -- The impact of small data methods in everyday life](https://arxiv.org/abs/2507.11773)
*Maren Hackenberg,Sophia G. Connor,Fabian Kabus,June Brawner,Ella Markham,Mahi Hardalupas,Areeq Chowdhury,Rolf Backofen,Anna Köttgen,Angelika Rohde,Nadine Binder,Harald Binder,the Collaborative Research Center 1597 Small Data*

Key words: 人工智能, 小数据, 大数据, 数据驱动决策, 健康技术

TL;DR: 论文探讨了在小数据环境下如何利用突破性人工智能技术，重点关注代表性不足群体的数据驱动政策与决策以及健康辅助技术的应用。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 研究动机是探索在小数据环境下AI技术的应用潜力，特别是在社会问题和健康技术领域。

Method: 通过概念性概述，对比小数据与大数据的差异，结合案例研究和技术分析，总结当前数据建模方法。

Result: 研究展示了在小数据环境下可行的技术解决方案，并提出了充分利用小数据的未来研究方向。

Conclusion: 论文强调了小数据技术的潜力，并呼吁建立更全面的研究议程以推动其应用。

Abstract: The emergence of breakthrough artificial intelligence (AI) techniques has led
to a renewed focus on how small data settings, i.e., settings with limited
information, can benefit from such developments. This includes societal issues
such as how best to include under-represented groups in data-driven policy and
decision making, or the health benefits of assistive technologies such as
wearables. We provide a conceptual overview, in particular contrasting small
data with big data, and identify common themes from exemplary case studies and
application areas. Potential solutions are described in a more detailed
technical overview of current data analysis and modelling techniques,
highlighting contributions from different disciplines, such as knowledge-driven
modelling from statistics and data-driven modelling from computer science. By
linking application settings, conceptual contributions and specific techniques,
we highlight what is already feasible and suggest what an agenda for fully
leveraging small data might look like.

</details>


### [200] [The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models](https://arxiv.org/abs/2507.11544)
*Ann-Kathrin Dombrowski,Dillon Bowen,Adam Gleave,Chris Cundy*

Key words: 大语言模型, 安全鸿沟, 开放权重, 危险能力, 防篡改保护

TL;DR: 开放权重大语言模型（LLMs）带来创新和个性化等好处，但也存在系统风险，如恶意修改模型。论文提出“安全鸿沟”概念，并开源工具评估开放模型的安全差距。实验显示模型规模越大，安全鸿沟越显著。

<details>
  <summary>Details</summary>

Main category: cs.CY

Motivation: 开放权重的LLMs可能被恶意修改，导致安全风险。研究旨在量化这种风险，并为开发防篡改保护措施提供依据。

Method: 开源工具包评估开放模型的安全鸿沟，以Llama-3和Qwen-2.5为例，测试生化、网络能力及生成质量。

Result: 模型规模越大，安全鸿沟越宽，移除保护后危险能力显著增强。

Conclusion: 工具包可作为评估框架，促进开发防篡改保护措施。

Abstract: Open-weight large language models (LLMs) unlock huge benefits in innovation,
personalization, privacy, and democratization. However, their core advantage -
modifiability - opens the door to systemic risks: bad actors can trivially
subvert current safeguards, turning beneficial models into tools for harm. This
leads to a 'safety gap': the difference in dangerous capabilities between a
model with intact safeguards and one that has been stripped of those
safeguards. We open-source a toolkit to estimate the safety gap for
state-of-the-art open-weight models. As a case study, we evaluate biochemical
and cyber capabilities, refusal rates, and generation quality of models from
two families (Llama-3 and Qwen-2.5) across a range of parameter scales (0.5B to
405B) using different safeguard removal techniques. Our experiments reveal that
the safety gap widens as model scale increases and effective dangerous
capabilities grow substantially when safeguards are removed. We hope that the
Safety Gap Toolkit (https://github.com/AlignmentResearch/safety-gap) will serve
as an evaluation framework for common open-source models and as a motivation
for developing and testing tamper-resistant safeguards. We welcome
contributions to the toolkit from the community.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [201] [JSQA: Speech Quality Assessment with Perceptually-Inspired Contrastive Pretraining Based on JND Audio Pairs](https://arxiv.org/abs/2507.11636)
*Junyi Fan,Donald Williamson*

Key words: 语音质量评估（SQA）、MOS评分、JND对比学习、预训练、NISQA数据集

TL;DR: 论文提出了一种名为JSQA的两阶段框架，通过感知引导的对比学习预训练音频编码器，显著提升了语音质量评估（SQA）模型的性能。

<details>
  <summary>Details</summary>

Main category: eess.AS

Motivation: 语音质量评估（SQA）中，MOS评分存在高方差问题，现有方法未充分融入感知因素，导致效果不佳。

Method: 采用两阶段框架：1）使用JND对进行感知对比预训练；2）在NISQA数据集上微调MOS预测。

Result: 实验表明，感知引导的预训练显著提升了模型性能，优于从零训练的网络。

Conclusion: 在预训练中融入感知因素对SQA性能提升至关重要。

Abstract: Speech quality assessment (SQA) is often used to learn a mapping from a
high-dimensional input space to a scalar that represents the mean opinion score
(MOS) of the perceptual speech quality. Learning such a mapping is challenging
for many reasons, but largely because MOS exhibits high levels of inherent
variance due to perceptual and experimental-design differences. Many solutions
have been proposed, but many approaches do not properly incorporate perceptual
factors into their learning algorithms (beyond the MOS label), which could lead
to unsatisfactory results. To this end, we propose JSQA, a two-stage framework
that pretrains an audio encoder using perceptually-guided contrastive learning
on just noticeable difference (JND) pairs, followed by fine-tuning for MOS
prediction. We first generate pairs of audio data within JND levels, which are
then used to pretrain an encoder to leverage perceptual quality similarity
information and map it into an embedding space. The JND pairs come from clean
LibriSpeech utterances that are mixed with background noise from CHiME-3, at
different signal-to-noise ratios (SNRs). The encoder is later fine-tuned with
audio samples from the NISQA dataset for MOS prediction. Experimental results
suggest that perceptually-inspired contrastive pretraining significantly
improves the model performance evaluated by various metrics when compared
against the same network trained from scratch without pretraining. These
findings suggest that incorporating perceptual factors into pretraining greatly
contributes to the improvement in performance for SQA.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [202] [Improved Analysis for Sign-based Methods with Momentum Updates](https://arxiv.org/abs/2507.12091)
*Wei Jiang,Dingzhi Yu,Sifan Yang,Wenhao Yang,Lijun Zhang*

Key words: sign-based optimization, momentum, convergence rate, distributed setting

TL;DR: 本文提出了一种改进的基于符号的动量优化算法分析，证明了在恒定批量大小下实现收敛率，并在分布式环境中优于现有方法。

<details>
  <summary>Details</summary>

Main category: math.OC

Motivation: 传统符号方法需大批量或单模态噪声假设，限制了实用性。本文旨在消除这些限制并提升性能。

Method: 采用动量更新的signSGD方法，并在分布式设置中结合多数投票机制。

Result: 在标准平滑条件下，收敛率提升至$\mathcal{O}(T^{-1/4})$；分布式环境中性能显著优于现有方法。

Conclusion: 动量更新能有效提升符号方法的收敛性能，实验验证了其有效性。

Abstract: In this paper, we present enhanced analysis for sign-based optimization
algorithms with momentum updates. Traditional sign-based methods, under the
separable smoothness assumption, guarantee a convergence rate of
$\mathcal{O}(T^{-1/4})$, but they either require large batch sizes or assume
unimodal symmetric stochastic noise. To address these limitations, we
demonstrate that signSGD with momentum can achieve the same convergence rate
using constant batch sizes without additional assumptions. Our analysis, under
the standard $l_2$-smoothness condition, improves upon the result of the prior
momentum-based signSGD method by a factor of $\mathcal{O}(d^{1/2})$, where $d$
is the problem dimension. Furthermore, we explore sign-based methods with
majority vote in distributed settings and show that the proposed momentum-based
method yields convergence rates of $\mathcal{O}\left( d^{1/2}T^{-1/2} +
dn^{-1/2} \right)$ and $\mathcal{O}\left( \max \{ d^{1/4}T^{-1/4},
d^{1/10}T^{-1/5} \} \right)$, which outperform the previous results of
$\mathcal{O}\left( dT^{-1/4} + dn^{-1/2} \right)$ and $\mathcal{O}\left(
d^{3/8}T^{-1/8} \right)$, respectively. Numerical experiments further validate
the effectiveness of the proposed methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [203] [Multimodal Coordinated Online Behavior: Trade-offs and Strategies](https://arxiv.org/abs/2507.12108)
*Lorenzo Mannocci,Stefano Cresci,Matteo Magnani,Anna Monreale,Maurizio Tesconi*

Key words: 在线协同行为,多模态分析,数字平台,检测方法

TL;DR: 论文比较了单模态与多模态方法在检测在线协同行为中的效果，指出多模态方法能更全面地理解协同动态，但并非所有模态都提供独特见解。

<details>
  <summary>Details</summary>

Main category: cs.SI

Motivation: 研究旨在揭示多模态协同行为的复杂动态，解决传统单模态方法可能忽略的协同模式，以提升数字平台的分析能力。

Method: 通过比较弱整合和强整合的多模态模型，评估不同模态的贡献及多模态实现方式对检测结果的影响。

Result: 研究发现多模态方法能更全面捕捉协同动态，但并非所有模态都提供独特信息。

Conclusion: 多模态方法能更有效地检测和分析在线协同行为，为数字平台完整性保护提供新视角。

Abstract: Coordinated online behavior, which spans from beneficial collective actions
to harmful manipulation such as disinformation campaigns, has become a key
focus in digital ecosystem analysis. Traditional methods often rely on
monomodal approaches, focusing on single types of interactions like co-retweets
or co-hashtags, or consider multiple modalities independently of each other.
However, these approaches may overlook the complex dynamics inherent in
multimodal coordination. This study compares different ways of operationalizing
the detection of multimodal coordinated behavior. It examines the trade-off
between weakly and strongly integrated multimodal models, highlighting the
balance between capturing broader coordination patterns and identifying tightly
coordinated behavior. By comparing monomodal and multimodal approaches, we
assess the unique contributions of different data modalities and explore how
varying implementations of multimodality impact detection outcomes. Our
findings reveal that not all the modalities provide distinct insights, but that
with a multimodal approach we can get a more comprehensive understanding of
coordination dynamics. This work enhances the ability to detect and analyze
coordinated online behavior, offering new perspectives for safeguarding the
integrity of digital platforms.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [204] [HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways](https://arxiv.org/abs/2507.11621)
*Tianyi Wang,Yangyang Wang,Jie Pan,Junfeng Jiao,Christian Claudel*

Key words: 匝道合并、混合交通流、协作控制、CAV、多目标优化

TL;DR: 提出了一种分层协作的匝道合并控制框架（HCOMC），用于混合交通流，结合纵向和横向模型，并通过仿真验证其在安全、效率和燃油经济性方面的优势。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 高速公路匝道合并区是交通拥堵和事故的常见瓶颈，目前基于CAVs的协作控制是解决方案，但CAVs尚未普及，需提出适用于混合交通流的控制框架。

Method: 扩展了基于智能驾驶员模型的纵向跟驰模型和五次多项式曲线的横向换道模型，提出HCOMC框架，包括分层协作规划、基于博弈论的可选换道模型和多目标优化。

Result: 仿真表明，HCOMC在群体安全、合并稳定性、交通效率和燃油经济性方面具有显著综合优势。

Conclusion: HCOMC在混合交通流中表现出色，为匝道合并问题提供了有效解决方案。

Abstract: Highway on-ramp merging areas are common bottlenecks to traffic congestion
and accidents. Currently, a cooperative control strategy based on connected and
automated vehicles (CAVs) is a fundamental solution to this problem. While CAVs
are not fully widespread, it is necessary to propose a hierarchical cooperative
on-ramp merging control (HCOMC) framework for heterogeneous traffic flow on
two-lane highways to address this gap. This paper extends longitudinal
car-following models based on the intelligent driver model and lateral
lane-changing models using the quintic polynomial curve to account for
human-driven vehicles (HDVs) and CAVs, comprehensively considering human
factors and cooperative adaptive cruise control. Besides, this paper proposes a
HCOMC framework, consisting of a hierarchical cooperative planning model based
on the modified virtual vehicle model, a discretionary lane-changing model
based on game theory, and a multi-objective optimization model using the
elitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and
efficient merging process. Then, the performance of our HCOMC is analyzed under
different traffic densities and CAV penetration rates through simulation. The
findings underscore our HCOMC's pronounced comprehensive advantages in
enhancing the safety of group vehicles, stabilizing and expediting merging
process, optimizing traffic efficiency, and economizing fuel consumption
compared with benchmarks.

</details>


### [205] [A Roadmap for Climate-Relevant Robotics Research](https://arxiv.org/abs/2507.11623)
*Alan Papalia,Charles Dawson,Laurentiu L. Anton,Norhan Magdy Bayomi,Bianca Champenois,Jung-Hoon Cho,Levi Cai,Joseph DelPreto,Kristen Edwards,Bilha-Catherine Githinji,Cameron Hickert,Vindula Jayawardana,Matthew Kramer,Shreyaa Raghavan,David Russell,Shide Salimi,Jingnan Shi,Soumya Sudhakar,Yanwei Wang,Shouyi Wang,Luca Carlone,Vijay Kumar,Daniela Rus,John E. Fernandez,Cathy Wu,George Kantor,Derek Young,Hanumant Singh*

Key words: 气候变化, 机器人技术, 跨学科合作, 能源优化, 精准农业

TL;DR: 本文提出了一个气候相关机器人研究的路线图，旨在通过机器人技术与气候领域的跨学科合作，解决能源、建筑、交通等行业的气候问题。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 气候变化是21世纪的核心挑战，机器人社区希望通过技术与气候领域的结合，发挥高影响力作用。

Method: 通过识别机器人技术与气候领域的交叉点，如能源优化、精准农业等，提出具体研究方向。

Result: 提出了一个包含物理机器人和算法工具的应用框架，激励新的研究合作方向。

Conclusion: 机器人技术可以为气候优先事项提供解决方案，本文旨在推动相关研究和合作。

Abstract: Climate change is one of the defining challenges of the 21st century, and
many in the robotics community are looking for ways to contribute. This paper
presents a roadmap for climate-relevant robotics research, identifying
high-impact opportunities for collaboration between roboticists and experts
across climate domains such as energy, the built environment, transportation,
industry, land use, and Earth sciences. These applications include problems
such as energy systems optimization, construction, precision agriculture,
building envelope retrofits, autonomous trucking, and large-scale environmental
monitoring. Critically, we include opportunities to apply not only physical
robots but also the broader robotics toolkit - including planning, perception,
control, and estimation algorithms - to climate-relevant problems. A central
goal of this roadmap is to inspire new research directions and collaboration by
highlighting specific, actionable problems at the intersection of robotics and
climate. This work represents a collaboration between robotics researchers and
domain experts in various climate disciplines, and it serves as an invitation
to the robotics community to bring their expertise to bear on urgent climate
priorities.

</details>


### [206] [Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers](https://arxiv.org/abs/2507.11991)
*Juanran Wang,Marc R. Schlichting,Mykel J. Kochenderfer*

Key words: 自动驾驶, 交叉路口安全性, 深度生成模型, 去噪扩散模型, 鲁棒规划器

TL;DR: 该研究利用深度生成模型提高自动驾驶车辆在交叉路口的安全性，通过生成碰撞诱发的噪声序列来训练快速推理的单步去噪扩散模型，并用于构建鲁棒规划器，实验显示其性能优于基准方法。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 交叉路口等高风险区域是碰撞的主要原因，研究旨在提升自动驾驶车辆在此类场景中的安全性。

Method: 采用1000步去噪扩散概率模型生成碰撞噪声序列，并通过生成对抗架构将其蒸馏为快速推理的单步模型，用于构建鲁棒规划器。

Result: 单步模型在保持采样质量的同时实现了快速推理，鲁棒规划器在仿真中表现出比基准方法更低的故障率和延迟率。

Conclusion: 提出的单步模型和鲁棒规划器有效提升了自动驾驶车辆在交叉路口的安全性，具有实际应用潜力。

Abstract: High-risk traffic zones such as intersections are a major cause of
collisions. This study leverages deep generative models to enhance the safety
of autonomous vehicles in an intersection context. We train a 1000-step
denoising diffusion probabilistic model to generate collision-causing sensor
noise sequences for an autonomous vehicle navigating a four-way intersection
based on the current relative position and velocity of an intruder. Using the
generative adversarial architecture, the 1000-step model is distilled into a
single-step denoising diffusion model which demonstrates fast inference speed
while maintaining similar sampling quality. We demonstrate one possible
application of the single-step model in building a robust planner for the
autonomous vehicle. The planner uses the single-step model to efficiently
sample potential failure cases based on the currently measured traffic state to
inform its decision-making. Through simulation experiments, the robust planner
demonstrates significantly lower failure rate and delay rate compared with the
baseline Intelligent Driver Model controller.

</details>


### [207] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Key words: 模仿学习、视觉-语言-动作模型、人类视频、机器人操作、逆运动学

TL;DR: 通过使用人类视角视频训练视觉-语言-动作模型（VLA），并将其应用于机器人动作生成，提出EgoVLA方法。相较于传统机器人硬件数据收集，该方法利用人类视频的多样性和规模优势，结合少量机器人演示数据，显著提升机器人操作性能。

<details>
  <summary>Details</summary>

Main category: cs.RO

Motivation: 研究目的是解决传统模仿学习中因依赖机器人硬件而限制数据规模的问题，通过人类视频数据扩展训练资源，并利用其丰富的场景和任务多样性。

Method: 提出EgoVLA方法：1）利用人类视频训练VLA模型预测人类手腕和手部动作；2）通过逆运动学和动作重定向将人类动作转化为机器人动作；3）结合少量机器人演示数据微调模型。设计了Isaac Humanoid Manipulation Benchmark用于验证。

Result: 在Isaac Humanoid Manipulation Benchmark上，EgoVLA相比基线方法有显著提升，同时验证了人类数据的重要性。

Conclusion: 人类视频数据可以有效替代机器人硬件数据，结合少量机器人演示数据，显著提升机器人操作任务的性能。

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [208] [The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist](https://arxiv.org/abs/2507.11810)
*Haoxuan Zhang,Ruochi Li,Yang Zhang,Ting Xiao,Jiangping Chen,Junhua Ding,Haihua Chen*

Key words: 大型语言模型（LLM）、科学创新、人机交互、分类框架、伦理问题

TL;DR: 该论文探讨了大型语言模型（LLM）如何推动科学创新，提出了分层框架（评估者、合作者、科学家）以分类LLM在科学中的角色，并分析了其边界、评估标准与人机交互模式。

<details>
  <summary>Details</summary>

Main category: cs.DL

Motivation: 科学面临信息过载、学科隔离和传统方法收益递减的挑战，LLM被视为推动创新的潜在领导者。

Method: 提出三层次框架（Evaluator, Collaborator, Scientist），区分结构化研究与开放式发现的贡献，统一分类能力边界、评估标准及交互模式。

Result: 系统梳理了LLM驱动的科学创新，强调其不仅是工具，更是重塑科学认识论的催化剂。

Conclusion: 为未来研究提供概念框架与实践指导，同时指出开放挑战与伦理问题。

Abstract: Scientific innovation is undergoing a paradigm shift driven by the rapid
advancement of Large Language Models (LLMs). As science faces mounting
challenges including information overload, disciplinary silos, and diminishing
returns on conventional research methods, LLMs are emerging as powerful agents
capable not only of enhancing scientific workflows but also of participating in
and potentially leading the innovation process. Existing surveys mainly focus
on different perspectives, phrases, and tasks in scientific research and
discovery, while they have limitations in understanding the transformative
potential and role differentiation of LLM. This survey proposes a comprehensive
framework to categorize the evolving roles of LLMs in scientific innovation
across three hierarchical levels: Evaluator, Collaborator, and Scientist. We
distinguish between LLMs' contributions to structured scientific research
processes and open-ended scientific discovery, thereby offering a unified
taxonomy that clarifies capability boundaries, evaluation criteria, and
human-AI interaction patterns at each level. Through an extensive analysis of
current methodologies, benchmarks, systems, and evaluation metrics, this survey
delivers an in-depth and systematic synthesis on LLM-driven scientific
innovation. We present LLMs not only as tools for automating existing
processes, but also as catalysts capable of reshaping the epistemological
foundations of science itself. This survey offers conceptual clarity, practical
guidance, and theoretical foundations for future research, while also
highlighting open challenges and ethical considerations in the pursuit of
increasingly autonomous AI-driven science. Resources related to this survey can
be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [209] [Emergent Heterogeneous Swarm Control Through Hebbian Learning](https://arxiv.org/abs/2507.11566)
*Fuda van Diggelen,Tugay Alperen Karagüzel,Andres Garcia Rincon,A. E. Eiben,Dario Floreano,Eliseo Ferrante*

Key words: Hebbian学习, 群机器人, 异质性, 局部适应性, 多智能体强化学习

TL;DR: 本文提出了一种基于Hebbian学习的群机器人控制方法，通过局部信息实现异质性的自动涌现，解决了异质性控制的多项挑战。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 探索一种生物启发的神经适应方法，用于群机器人中的异质性控制，以解决微宏观问题、维度灾难和先验知识需求等挑战。

Method: 采用Hebbian学习作为局部适应性规则，通过统一的规则和基于群体行为的演化优化，实现异质性的自然涌现。

Result: Hebbian学习能自然生成异质性，实现群体行为切换，并显著提升群机器人的能力，可作为多智能体强化学习的替代方案。

Conclusion: Hebbian学习是一种有效的群机器人异质性控制方法，具有可扩展性和适应性，适用于标准基准任务。

Abstract: In this paper, we introduce Hebbian learning as a novel method for swarm
robotics, enabling the automatic emergence of heterogeneity. Hebbian learning
presents a biologically inspired form of neural adaptation that solely relies
on local information. By doing so, we resolve several major challenges for
learning heterogeneous control: 1) Hebbian learning removes the complexity of
attributing emergent phenomena to single agents through local learning rules,
thus circumventing the micro-macro problem; 2) uniform Hebbian learning rules
across all swarm members limit the number of parameters needed, mitigating the
curse of dimensionality with scaling swarm sizes; and 3) evolving Hebbian
learning rules based on swarm-level behaviour minimises the need for extensive
prior knowledge typically required for optimising heterogeneous swarms. This
work demonstrates that with Hebbian learning heterogeneity naturally emerges,
resulting in swarm-level behavioural switching and in significantly improved
swarm capabilities. It also demonstrates how the evolution of Hebbian learning
rules can be a valid alternative to Multi Agent Reinforcement Learning in
standard benchmarking tasks.

</details>


### [210] [Simulated Language Acquisition in a Biologically Realistic Model of the Brain](https://arxiv.org/abs/2507.11788)
*Daniel Mitropolsky,Christos Papadimitriou*

Key words: 神经科学, 语言习得, 数学模型, 认知现象, Hebbian可塑性

TL;DR: 论文提出了一种基于六个基本神经科学原理的数学模型，能够实现基本的语言习得能力。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 探索神经元活动如何导致高级认知现象（如语言和规划），填补现有神经科学解释的空白。

Method: 结合兴奋性神经元、脑区、随机突触、Hebbian可塑性、局部抑制和区域间抑制等六种神经科学原理，构建模拟神经形态系统。

Result: 系统从零开始学习语言的语义、语法角色和词序，并能生成新句子。

Conclusion: 该模型为理解神经活动与认知的联系提供了新思路，具有扩展潜力。

Abstract: Despite tremendous progress in neuroscience, we do not have a compelling
narrative for the precise way whereby the spiking of neurons in our brain
results in high-level cognitive phenomena such as planning and language. We
introduce a simple mathematical formulation of six basic and broadly accepted
principles of neuroscience: excitatory neurons, brain areas, random synapses,
Hebbian plasticity, local inhibition, and inter-area inhibition. We implement a
simulated neuromorphic system based on this formalism, which is capable of
basic language acquisition: Starting from a tabula rasa, the system learns, in
any language, the semantics of words, their syntactic role (verb versus noun),
and the word order of the language, including the ability to generate novel
sentences, through the exposure to a modest number of grounded sentences in the
same language. We discuss several possible extensions and implications of this
result.

</details>


### [211] [Survey of Genetic and Differential Evolutionary Algorithm Approaches to Search Documents Based On Semantic Similarity](https://arxiv.org/abs/2507.11751)
*Chandrashekar Muniyappa,Eunjin Kim*

Key words: 文档相似性, 分布式计算, 深度神经网络, 遗传算法, 差分进化算法

TL;DR: 该论文调查了基于语义文本相似性的文档搜索，重点关注遗传和差分进化计算算法的最新进展。

<details>
  <summary>Details</summary>

Main category: cs.NE

Motivation: 识别大量数据中的相似文档是一个重大挑战，需要有效的分布式计算技术来解决。

Method: 论文探讨了深度神经网络和进化计算算法（如遗传算法和差分进化算法）在文档搜索中的应用。

Result: 研究表明这些技术在语义文本相似性搜索中取得了显著成功。

Conclusion: 遗传和差分进化计算算法在大数据和计算能力提升的背景下展现出巨大潜力。

Abstract: Identifying similar documents within extensive volumes of data poses a
significant challenge. To tackle this issue, researchers have developed a
variety of effective distributed computing techniques. With the advancement of
computing power and the rise of big data, deep neural networks and evolutionary
computing algorithms such as genetic algorithms and differential evolution
algorithms have achieved greater success. This survey will explore the most
recent advancements in the search for documents based on their semantic text
similarity, focusing on genetic and differential evolutionary computing
algorithms.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [212] [Neural Polar Decoders for Deletion Channels](https://arxiv.org/abs/2507.12329)
*Ziv Aharoni,Henry D. Pfister*

Key words: 神经极性解码器,删除通道,计算复杂度,列表解码,DNA存储

TL;DR: 该论文提出了一种用于删除通道的神经极性解码器（NPD），通过降低计算复杂度从O(N^4)到O(AN log N)，扩展了其应用范围。

<details>
  <summary>Details</summary>

Main category: cs.IT

Motivation: 现有极性解码器在删除通道中计算复杂度高（O(N^4)），限制了其在中短块长度中的应用，NPD通过神经网络优化降低了复杂度。

Method: 扩展NPD架构，修改其中一个神经网络以支持删除通道，保留了基本SC解码器操作，同时引入计算预算参数A控制复杂度。

Result: 在删除率δ∈{0.01, 0.1}的测试中，NPD表现良好，并通过列表解码进一步提升了性能。

Conclusion: NPD通过降低复杂度为未来技术（如DNA存储）的应用提供了潜力。

Abstract: This paper introduces a neural polar decoder (NPD) for deletion channels with
a constant deletion rate. Existing polar decoders for deletion channels exhibit
high computational complexity of $O(N^4)$, where $N$ is the block length. This
limits the application of polar codes for deletion channels to
short-to-moderate block lengths. In this work, we demonstrate that employing
NPDs for deletion channels can reduce the computational complexity. First, we
extend the architecture of the NPD to support deletion channels. Specifically,
the NPD architecture consists of four neural networks (NNs), each replicating
fundamental successive cancellation (SC) decoder operations. To support
deletion channels, we change the architecture of only one. The computational
complexity of the NPD is $O(AN\log N)$, where the parameter $A$ represents a
computational budget determined by the user and is independent of the channel.
We evaluate the new extended NPD for deletion channels with deletion rates
$\delta\in\{0.01, 0.1\}$ and we verify the NPD with the ground truth given by
the trellis decoder by Tal et al. We further show that due to the reduced
complexity of the NPD, we are able to incorporate list decoding and further
improve performance. We believe that the extended NPD presented here could have
applications in future technologies like DNA storage.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [213] [Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker](https://arxiv.org/abs/2507.12378)
*Rachna Saxena,Abhijeet Kumar,Suresh Shanmugam*

Key words: 多模态语言模型, 混合搜索, 延迟交互, 检索增强生成, 企业应用

TL;DR: 本文提出了一种高效的多模态检索方法，通过混合搜索和延迟交互机制解决了传统信息提取和MLLM在视觉增强问答任务中的问题。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 传统信息提取系统和多模态语言模型在处理视觉元素（如图表、图像等）时面临挑战，尤其是在检索效率和可扩展性方面。

Method: 采用多步骤自定义实现，结合混合搜索（元数据和嵌入）和先进的延迟交互重新排序器，以高效检索最佳匹配页面。

Result: 实验表明，该方法在保持高性能的同时显著提高了检索速度，适用于企业生产系统。

Conclusion: 提出的方法在多模态问答任务中具有高效和可扩展性，适合企业级应用。

Abstract: Traditional information extraction systems face challenges with text only
language models as it does not consider infographics (visual elements of
information) such as tables, charts, images etc. often used to convey complex
information to readers. Multimodal LLM (MLLM) face challenges of finding needle
in the haystack problem i.e., either longer context length or substantial
number of documents as search space. Late interaction mechanism over visual
language models has shown state of the art performance in retrieval-based
vision augmented Q&A tasks. There are yet few challenges using it for RAG based
multi-modal Q&A. Firstly, many popular and widely adopted vector databases do
not support native multi-vector retrieval. Secondly, late interaction requires
computation which inflates space footprint and can hinder enterprise adoption.
Lastly, the current state of late interaction mechanism does not leverage the
approximate neighbor search indexing methods for large speed ups in retrieval
process. This paper explores a pragmatic approach to make vision retrieval
process scalable and efficient without compromising on performance quality. We
propose multi-step custom implementation utilizing widely adopted hybrid search
(metadata & embedding) and state of the art late interaction re-ranker to
retrieve best matching pages. Finally, MLLM are prompted as reader to generate
answers from contextualized best matching pages. Through experiments, we
observe that the proposed design is scalable (significant speed up) and stable
(without degrading performance quality), hence can be used as production
systems at enterprises.

</details>


### [214] [Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control](https://arxiv.org/abs/2507.12202)
*Anton Klenitskiy,Konstantin Polev,Daria Denisova,Alexey Vasilev,Dmitry Simakov,Gleb Gusev*

Key words: 稀疏自编码器,序列推荐,可解释性,Transformer

TL;DR: 该论文探讨了稀疏自编码器（SAE）在序列推荐任务中的应用，展示了其提取可解释特征的潜力，并能灵活控制模型行为。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 序列推荐中的Transformer模型通常是黑盒，难以理解其内部机制。稀疏自编码器已被证明能有效提取可解释特征，但其在序列推荐中的应用尚未研究。

Method: 使用稀疏自编码器（SAE）分析序列推荐任务中的Transformer模型内部隐藏状态，将其表示为激活空间中的稀疏线性组合。

Result: SAE提取的特征比原始隐藏状态更具解释性和单义性，并能灵活控制模型行为，适应不同场景需求。

Conclusion: SAE为序列推荐任务提供了一种可解释且灵活的特征提取方法，增强了模型的可控性。

Abstract: Many current state-of-the-art models for sequential recommendations are based
on transformer architectures. Interpretation and explanation of such black box
models is an important research question, as a better understanding of their
internals can help understand, influence, and control their behavior, which is
very important in a variety of real-world applications. Recently sparse
autoencoders (SAE) have been shown to be a promising unsupervised approach for
extracting interpretable features from language models. These autoencoders
learn to reconstruct hidden states of the transformer's internal layers from
sparse linear combinations of directions in their activation space.
  This paper is focused on the application of SAE to the sequential
recommendation domain. We show that this approach can be successfully applied
to the transformer trained on a sequential recommendation task: learned
directions turn out to be more interpretable and monosemantic than the original
hidden state dimensions. Moreover, we demonstrate that the features learned by
SAE can be used to effectively and flexibly control the model's behavior,
providing end-users with a straightforward method to adjust their
recommendations to different custom scenarios and contexts.

</details>


### [215] [Looking for Fairness in Recommender Systems](https://arxiv.org/abs/2507.12242)
*Cécile Logé*

Key words: 推荐系统, 过滤气泡, 公平性, 多样性, 社交媒体

TL;DR: 论文探讨了推荐系统中“过滤气泡”的公平性问题，提出了通过多样性指标调整推荐系统以平衡个性化和多样性的方法。

<details>
  <summary>Details</summary>

Main category: cs.IR

Motivation: 推荐系统在社交媒体等内容平台中的广泛应用引发了“过滤气泡”现象，限制了用户的观点多样性，影响了内容创作者的曝光和社会整体的文化多样性。

Method: 通过定义一个或多个代表多样性的性能指标，并将其纳入推荐系统的评估框架，调整推荐系统的表现。

Result: 目标是平衡个性化推荐与促进文化多样性的社会目标。

Conclusion: 通过多样性指标的引入，推荐系统可以在满足个性化需求的同时，减少“过滤气泡”的负面影响，促进更包容和多样的内容生态。

Abstract: Recommender systems can be found everywhere today, shaping our everyday
experience whenever we're consuming content, ordering food, buying groceries
online, or even just reading the news. Let's imagine we're in the process of
building a recommender system to make content suggestions to users on social
media. When thinking about fairness, it becomes clear there are several
perspectives to consider: the users asking for tailored suggestions, the
content creators hoping for some limelight, and society at large, navigating
the repercussions of algorithmic recommendations. A shared fairness concern
across all three is the emergence of filter bubbles, a side-effect that takes
place when recommender systems are almost "too good", making recommendations so
tailored that users become inadvertently confined to a narrow set of
opinions/themes and isolated from alternative ideas. From the user's
perspective, this is akin to manipulation. From the small content creator's
perspective, this is an obstacle preventing them access to a whole range of
potential fans. From society's perspective, the potential consequences are
far-reaching, influencing collective opinions, social behavior and political
decisions. How can our recommender system be fine-tuned to avoid the creation
of filter bubbles, and ensure a more inclusive and diverse content landscape?
Approaching this problem involves defining one (or more) performance metric to
represent diversity, and tweaking our recommender system's performance through
the lens of fairness. By incorporating this metric into our evaluation
framework, we aim to strike a balance between personalized recommendations and
the broader societal goal of fostering rich and varied cultures and points of
view.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [216] [Interactive Hybrid Rice Breeding with Parametric Dual Projection](https://arxiv.org/abs/2507.11848)
*Changjian Chen,Pengcheng Wang,Fei Lyu,Zhuo Tang,Li Yang,Long Wang,Yong Cai,Feng Yu,Kenli Li*

Key words: 杂交水稻育种, 基因组选择, 可视化分析, 双重投影, 调控基因

TL;DR: 本文提出了一种可视化分析方法，通过参数化双重投影方法支持杂交水稻育种中的交互式双重分析，验证了调控基因和杂交种的准确性，并获得了育种者的积极反馈。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统杂交水稻育种依赖田间栽培和基因组预测模型相结合，但准确性有限且耗时。为了简化这一过程，需要一种交互式可视化方法。

Method: 开发了一种参数化双重投影方法，支持交互式双重分析，并进一步开发了基因可视化和杂交可视化工具。

Result: 通过案例研究定量评估了参数化双重投影方法、鉴定出的调控基因和优选杂交种的有效性，并获得了育种者的积极反馈。

Conclusion: 提出的可视化方法有效支持了杂交水稻育种中的交互式分析，提高了效率和准确性。

Abstract: Hybrid rice breeding crossbreeds different rice lines and cultivates the
resulting hybrids in fields to select those with desirable agronomic traits,
such as higher yields. Recently, genomic selection has emerged as an efficient
way for hybrid rice breeding. It predicts the traits of hybrids based on their
genes, which helps exclude many undesired hybrids, largely reducing the
workload of field cultivation. However, due to the limited accuracy of genomic
prediction models, breeders still need to combine their experience with the
models to identify regulatory genes that control traits and select hybrids,
which remains a time-consuming process. To ease this process, in this paper, we
proposed a visual analysis method to facilitate interactive hybrid rice
breeding. Regulatory gene identification and hybrid selection naturally
ensemble a dual-analysis task. Therefore, we developed a parametric dual
projection method with theoretical guarantees to facilitate interactive dual
analysis. Based on this dual projection method, we further developed a gene
visualization and a hybrid visualization to verify the identified regulatory
genes and hybrids. The effectiveness of our method is demonstrated through the
quantitative evaluation of the parametric dual projection method, identified
regulatory genes and desired hybrids in the case study, and positive feedback
from breeders.

</details>


### [217] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)
*Xiaoqing Chen,Siyang Li,Dongrui Wu*

Key words: EEG decoding, BCI, cross-dataset learning, spatial alignment, frame-patch encoding

TL;DR: 提出了一个无需校准的跨数据集EEG解码框架AFPM，通过空间对齐和帧-补丁编码提升BCI的实用性。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 解决由于通道布局不一致、信号分布非平稳和神经生理学先验整合不足导致的EEG跨数据集学习和泛化问题。

Method: AFPM框架包括空间对齐（基于脑区先验选择通道并统一布局）和帧-补丁编码（将信号建模为统一时空补丁）。

Result: 在运动想象和事件相关电位任务上分别提升了4.40%和3.58%的性能，优于17种现有方法。

Conclusion: AFPM是首个无需校准的跨数据集EEG解码框架，显著提升了BCI的实际应用价值。

Abstract: Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

</details>


### [218] [Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness](https://arxiv.org/abs/2507.12212)
*Garyoung Kim,Huisung Kwon,Seoju Yun,Yu-Won Youn*

Key words: 生成式AI, 文化偏见, 伦理AI, 丑陋表达, 社会偏见

TL;DR: 生成式AI不仅复制人类创造力，还再现了根深蒂固的文化偏见。研究表明，AI模型倾向于将丑陋与老年白人男性关联，揭示了其内在的社会偏见与矛盾。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 研究旨在揭示生成式AI如何理解和表达丑陋概念，并探索其背后的偏见，以推动更公平的AI开发。

Method: 通过迭代提示提取13个丑陋相关形容词，生成624张图像，并对图像的属性进行编码和主题分析。

Result: AI模型表现出将丑陋与老年白人男性关联的偏见，同时传统物理标记（如不对称和衰老）仍是核心视觉主题。

Conclusion: 生成式AI仍延续继承性和矛盾性偏见，亟需更伦理的训练方法和包容性开发方法。

Abstract: Generative AI does not only replicate human creativity but also reproduces
deep-seated cultural biases, making it crucial to critically examine how
concepts like ugliness are understood and expressed by these tools. This study
investigates how four different generative AI models understand and express
ugliness through text and image and explores the biases embedded within these
representations. We extracted 13 adjectives associated with ugliness through
iterative prompting of a large language model and generated 624 images across
four AI models and three prompts. Demographic and socioeconomic attributes
within the images were independently coded and thematically analyzed. Our
findings show that AI models disproportionately associate ugliness with old
white male figures, reflecting entrenched social biases as well as paradoxical
biases, where efforts to avoid stereotypical depictions of marginalized groups
inadvertently result in the disproportionate projection of negative attributes
onto majority groups. Qualitative analysis further reveals that, despite
supposed attempts to frame ugliness within social contexts, conventional
physical markers such as asymmetry and aging persist as central visual motifs.
These findings demonstrate that despite attempts to create more equal
representations, generative AI continues to perpetuate inherited and
paradoxical biases, underscoring the critical work being done to create ethical
AI training paradigms and advance methodologies for more inclusive AI
development.

</details>


### [219] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)
*Hyein Hong,Sangbong Yoo,SeokHwan Choi,Jisue Kim,Seongbum Seo,Haneol Cho,Chansoo Kim,Yun Jang*

Key words: 数据质量改进,可视化分析,机器学习,数据驱动,流程驱动

TL;DR: 论文提出了一种名为d-DQIVAR的新型可视化分析系统，旨在通过数据驱动和流程驱动的方法提升数据质量，优化机器学习模型性能。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 现有研究多侧重于数据预处理而非真正的数据质量改进(DQI)，且传统数据驱动方法常导致数据特性失真。

Method: 系统结合数据驱动和流程驱动技术，包括缺失值填补、异常检测、格式标准化等，以及利用Kolmogorov-Smirnov测试评估DQI流程。

Result: 通过案例研究、评估和用户研究展示该系统如何帮助用户有效利用专家和领域知识。

Conclusion: d-DQIVAR系统在提升数据质量和优化模型性能方面表现出色。

Abstract: Approaches to enhancing data quality (DQ) are classified into two main
categories: data- and process-driven. However, prior research has predominantly
utilized batch data preprocessing within the data-driven framework, which often
proves insufficient for optimizing machine learning (ML) model performance and
frequently leads to distortions in data characteristics. Existing studies have
primarily focused on data preprocessing rather than genuine data quality
improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual
analytics system designed to facilitate DQI strategies aimed at improving ML
model performance. Our system integrates visual analytics techniques that
leverage both data-driven and process-driven approaches. Data-driven techniques
tackle DQ issues such as imputation, outlier detection, deletion, format
standardization, removal of duplicate records, and feature selection.
Process-driven strategies encompass evaluating DQ and DQI procedures by
considering DQ dimensions and ML model performance and applying the
Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness
expert and domain knowledge effectively within a practical workflow through
case studies, evaluations, and user studies.

</details>


### [220] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)
*Hyeon Jeon,Jeongin Park,Soohyun Lee,Dae Hyun Kim,Sungbok Shin,Jinwook Seo*

Key words: 维度降维, 结构复杂性, 数据集自适应, 超参数优化, 计算效率

TL;DR: 论文提出了一种基于结构复杂性指标的维度降维优化方法，通过量化数据集的固有复杂性来预测最优降维设置，避免了不必要的计算开销。

<details>
  <summary>Details</summary>

Main category: cs.HC

Motivation: 传统维度降维技术需要通过大量试错来优化超参数，这导致了不必要的计算开销。为了应对这一问题，作者提出了一种数据集自适应的优化方法。

Method: 引入了结构复杂性指标来量化数据集的固有复杂性，预测其是否需要更高维空间来表示，从而指导降维技术的优化。

Result: 实验证明，这些指标能够有效近似数据集的真实复杂性，显著提升了降维优化的效率，同时不损失精度。

Conclusion: 提出的数据集自适应工作流程为维度降维的优化提供了一种高效且准确的解决方案。

Abstract: Selecting the appropriate dimensionality reduction (DR) technique and
determining its optimal hyperparameter settings that maximize the accuracy of
the output projections typically involves extensive trial and error, often
resulting in unnecessary computational overhead. To address this challenge, we
propose a dataset-adaptive approach to DR optimization guided by structural
complexity metrics. These metrics quantify the intrinsic complexity of a
dataset, predicting whether higher-dimensional spaces are necessary to
represent it accurately. Since complex datasets are often inaccurately
represented in two-dimensional projections, leveraging these metrics enables us
to predict the maximum achievable accuracy of DR techniques for a given
dataset, eliminating redundant trials in optimizing DR. We introduce the design
and theoretical foundations of these structural complexity metrics. We
quantitatively verify that our metrics effectively approximate the ground truth
complexity of datasets and confirm their suitability for guiding
dataset-adaptive DR workflow. Finally, we empirically show that our
dataset-adaptive workflow significantly enhances the efficiency of DR
optimization without compromising accuracy.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [221] [Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network](https://arxiv.org/abs/2507.11799)
*Shin-ichi Ito*

Key words: 神经网络, 积分微分方程, 收缩诱导断裂, 蒙特卡洛模拟, 计算效率

TL;DR: 本文提出了一种基于神经网络的求解器，用于处理收缩诱导断裂模型的积分微分方程，直接映射输入参数到概率密度函数，显著降低了计算成本。

<details>
  <summary>Details</summary>

Main category: physics.comp-ph

Motivation: 传统数值求解方法成本高昂，需要一种更高效的替代方案来处理积分微分方程，尤其是在蒙特卡洛模拟中。

Method: 使用神经网络直接映射输入参数到概率密度函数，避免数值求解方程。

Result: 方法在计算效率上显著优于传统有限差分方案，同时在准确性上相当甚至更好。验证实验展示了其高效性和可靠性。

Conclusion: 该研究为数据驱动的断裂逆分析奠定了基础，并展示了框架在超越预设模型结构方面的潜力。

Abstract: This paper presents a neural network (NN)-based solver for an
integro-differential equation that models shrinkage-induced fragmentation. The
proposed method directly maps input parameters to the corresponding probability
density function without numerically solving the governing equation, thereby
significantly reducing computational costs. Specifically, it enables efficient
evaluation of the density function in Monte Carlo simulations while maintaining
accuracy comparable to or even exceeding that of conventional finite difference
schemes. Validatation on synthetic data demonstrates both the method's
computational efficiency and predictive reliability. This study establishes a
foundation for the data-driven inverse analysis of fragmentation and suggests
the potential for extending the framework beyond pre-specified model
structures.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [222] [MOFSimBench: Evaluating Universal Machine Learning Interatomic Potentials In Metal--Organic Framework Molecular Modeling](https://arxiv.org/abs/2507.11806)
*Hendrik Kraß,Ju Huang,Seyed Mohamad Moosavi*

Key words: uMLIPs, 纳米多孔材料, MOFs, 基准测试, 数据质量

TL;DR: MOFSimBench 是一个用于评估通用机器学习原子间势（uMLIPs）在纳米多孔材料建模中表现的基准测试，发现其优于传统力场和细调机器学习势，数据质量对性能影响显著。

<details>
  <summary>Details</summary>

Main category: cond-mat.mtrl-sci

Motivation: 纳米多孔材料（如 MOFs）在碳捕获、能量存储和催化等领域具有重要应用，但其复杂化学和结构特性为 uMLIPs 的可靠性提出了挑战，需通过基准测试验证其实用性。

Method: 通过 MOFSimBench 基准测试，评估 20 多种不同架构的 uMLIPs 在结构优化、分子动力学稳定性、整体性质预测等任务中的表现。

Result: 评估表明，表现最佳的 uMLIPs 在所有任务中一致优于传统力场和细调机器学习势，数据多样性是性能的关键因素。

Conclusion: MOFSimBench 为 uMLIPs 在纳米多孔材料建模中的应用提供了指导，数据质量比模型架构对性能的影响更大。

Abstract: Universal machine learning interatomic potentials (uMLIPs) have emerged as
powerful tools for accelerating atomistic simulations, offering scalable and
efficient modeling with accuracy close to quantum calculations. However, their
reliability and effectiveness in practical, real-world applications remain an
open question. Metal-organic frameworks (MOFs) and related nanoporous materials
are highly porous crystals with critical relevance in carbon capture, energy
storage, and catalysis applications. Modeling nanoporous materials presents
distinct challenges for uMLIPs due to their diverse chemistry, structural
complexity, including porosity and coordination bonds, and the absence from
existing training datasets. Here, we introduce MOFSimBench, a benchmark to
evaluate uMLIPs on key materials modeling tasks for nanoporous materials,
including structural optimization, molecular dynamics (MD) stability, the
prediction of bulk properties, such as bulk modulus and heat capacity, and
guest-host interactions. Evaluating over 20 models from various architectures
on a chemically and structurally diverse materials set, we find that
top-performing uMLIPs consistently outperform classical force fields and
fine-tuned machine learning potentials across all tasks, demonstrating their
readiness for deployment in nanoporous materials modeling. Our analysis
highlights that data quality, particularly the diversity of training sets and
inclusion of out-of-equilibrium conformations, plays a more critical role than
model architecture in determining performance across all evaluated uMLIPs. We
release our modular and extendable benchmarking framework at
https://github.com/AI4ChemS/mofsim-bench, providing an open resource to guide
the adoption for nanoporous materials modeling and further development of
uMLIPs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [223] [LLMs are Bayesian, in Expectation, not in Realization](https://arxiv.org/abs/2507.11768)
*Leon Chlon,Sarah Rashidi,Zein Khamis,MarcAntonio M. Awada*

Key words: 大语言模型、Transformer、贝叶斯推理、不确定性量化、上下文学习

TL;DR: 论文探讨了大语言模型的上下文学习能力，发现Transformer违背了贝叶斯更新的基本性质，并提出理论分析来解决这一问题。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 研究大语言模型的上下文学习能力，尤其是Transformer在贝叶斯更新中的表现，以解决其在实际应用中的不确定性问题。

Method: 理论分析Transformer的隐式贝叶斯推理特性，包括位置编码的影响、信息论最优性和隐式后验表示。

Result: 提出四个关键理论结果，并通过GPT-3实验验证了Transformer能接近理论熵限。

Conclusion: 研究为提取校准的不确定性估计和优化计算效率提供了实用方法。

Abstract: Large language models demonstrate remarkable in-context learning
capabilities, adapting to new tasks without parameter updates. While this
phenomenon has been successfully modeled as implicit Bayesian inference, recent
empirical findings reveal a fundamental contradiction: transformers
systematically violate the martingale property, a cornerstone requirement of
Bayesian updating on exchangeable data. This violation challenges the
theoretical foundations underlying uncertainty quantification in critical
applications.
  Our theoretical analysis establishes four key results: (1) positional
encodings induce martingale violations of order $\Theta(\log n / n)$; (2)
transformers achieve information-theoretic optimality with excess risk
$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior
representation converges to the true Bayesian posterior in the space of
sufficient statistics; and (4) we derive the optimal chain-of-thought length as
$k^* = \Theta(\sqrt{n}\log(1/\varepsilon))$ with explicit constants, providing
a principled approach to reduce inference costs while maintaining performance.
Empirical validation on GPT-3 confirms predictions (1)-(3), with transformers
reaching 99\% of theoretical entropy limits within 20 examples. Our framework
provides practical methods for extracting calibrated uncertainty estimates from
position-aware architectures and optimizing computational efficiency in
deployment.

</details>


### [224] [Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?](https://arxiv.org/abs/2507.11891)
*Shuangning Li,Chonghuan Wang,Jingyan Wang*

Key words: A/B实验, 共生偏差, 多臂老虎机, 探索与利用, 算法选择

TL;DR: 该论文研究了A/B实验中因数据共享导致的‘共生偏差’，并指出在算法选择中，GTE的符号比其精确幅度更重要。作者通过多臂老虎机框架分析了数据共享下GTE符号的一致性与矛盾情况。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 之前的研究表明，标准均值差估计器在评估全局治疗效果（GTE）时因实验单元间的干扰而存在偏差，即‘共生偏差’。该研究旨在探讨在算法选择中，GTE符号的重要性。

Method: 研究采用多臂老虎机框架，理论分析了数据共享下GTE符号的预期估计与真实GTE符号的关系，并探讨了探索与利用的平衡对算法选择的影响。

Result: 分析表明，GTE符号的估计与真实符号的对应取决于探索与利用的水平，这是‘共生偏差’影响算法选择的关键因素。

Conclusion: 在算法选择的决策中，GTE符号的准确性比其精确值更为重要，且探索与利用的平衡对‘共生偏差’的影响至关重要。

Abstract: We study A/B experiments that are designed to compare the performance of two
recommendation algorithms. Prior work has shown that the standard
difference-in-means estimator is biased in estimating the global treatment
effect (GTE) due to a particular form of interference between experimental
units. Specifically, units under the treatment and control algorithms
contribute to a shared pool of data that subsequently train both algorithms,
resulting in interference between the two groups. The bias arising from this
type of data sharing is known as "symbiosis bias". In this paper, we highlight
that, for decision-making purposes, the sign of the GTE often matters more than
its precise magnitude when selecting the better algorithm. We formalize this
insight under a multi-armed bandit framework and theoretically characterize
when the sign of the expected GTE estimate under data sharing aligns with or
contradicts the sign of the true GTE. Our analysis identifies the level of
exploration versus exploitation as a key determinant of how symbiosis bias
impacts algorithm selection.

</details>


### [225] [Newfluence: Boosting Model interpretability and Understanding in High Dimensions](https://arxiv.org/abs/2507.11895)
*Haolin Zou,Arnab Auddy,Yongchan Kwon,Kamiar Rahnama Rad,Arian Maleki*

Key words: 机器学习, 高维数据, 影响函数, 模型解释, Newfluence

TL;DR: 论文探讨了影响函数在高维机器学习模型中的局限性，并提出了一种新的替代方法Newfluence，以提高解释复杂AI模型的准确性。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 由于现代AI模型在高维环境中的复杂性增加，现有的影响函数方法无法满足需求，因此需要更可靠的工具来解读和优化模型决策。

Method: 作者通过理论和实证分析评估了影响函数在高维环境中的表现，并提出了一种新的近似方法Newfluence，该方法保持计算效率的同时提高了准确性。

Result: 研究表明，影响函数在高维情境下不可靠，而Newfluence在准确性和效率上表现更好。

Conclusion: Newfluence为复杂AI模型的解释提供了更可靠的工具，同时论文提出的高维框架也可用于其他技术的分析。

Abstract: The increasing complexity of machine learning (ML) and artificial
intelligence (AI) models has created a pressing need for tools that help
scientists, engineers, and policymakers interpret and refine model decisions
and predictions. Influence functions, originating from robust statistics, have
emerged as a popular approach for this purpose.
  However, the heuristic foundations of influence functions rely on
low-dimensional assumptions where the number of parameters $p$ is much smaller
than the number of observations $n$. In contrast, modern AI models often
operate in high-dimensional regimes with large $p$, challenging these
assumptions.
  In this paper, we examine the accuracy of influence functions in
high-dimensional settings. Our theoretical and empirical analyses reveal that
influence functions cannot reliably fulfill their intended purpose. We then
introduce an alternative approximation, called Newfluence, that maintains
similar computational efficiency while offering significantly improved
accuracy.
  Newfluence is expected to provide more accurate insights than many existing
methods for interpreting complex AI models and diagnosing their issues.
Moreover, the high-dimensional framework we develop in this paper can also be
applied to analyze other popular techniques, such as Shapley values.

</details>


### [226] [Incorporating Fairness Constraints into Archetypal Analysis](https://arxiv.org/abs/2507.12021)
*Aleix Alcacer,Irene Epifanio*

Key words: 原型分析, 公平性, 无监督学习, 正则化, 表示学习

TL;DR: 提出了FairAA和FairKernelAA方法，通过公平性正则化减少敏感属性的编码，在保持原型可解释性的同时实现公平性目标。

<details>
  <summary>Details</summary>

Main category: stat.ML

Motivation: 传统原型分析（AA）可能无意中编码敏感属性，引发公平性问题，需要改进方法以减少这种影响。

Method: 提出FairAA（公平原型分析）及其非线性扩展FairKernelAA，通过加入公平性正则化项，减少敏感信息在原型中的编码。

Result: 在合成数据集和真实数据集ANSUR I上验证了FairAA和FairKernelAA在减少群体可分性的同时保持了解释方差。

Conclusion: FairAA在效用与公平性之间实现了良好的平衡，适用于敏感应用中的负责任表示学习。

Abstract: Archetypal Analysis (AA) is an unsupervised learning method that represents
data as convex combinations of extreme patterns called archetypes. While AA
provides interpretable and low-dimensional representations, it can
inadvertently encode sensitive attributes, leading to fairness concerns. In
this work, we propose Fair Archetypal Analysis (FairAA), a modified formulation
that explicitly reduces the influence of sensitive group information in the
learned projections. We also introduce FairKernelAA, a nonlinear extension that
addresses fairness in more complex data distributions. Our approach
incorporates a fairness regularization term while preserving the structure and
interpretability of the archetypes. We evaluate FairAA and FairKernelAA on
synthetic datasets, including linear, nonlinear, and multi-group scenarios,
demonstrating their ability to reduce group separability -- as measured by mean
maximum discrepancy and linear separability -- without substantially
compromising explained variance. We further validate our methods on the
real-world ANSUR I dataset, confirming their robustness and practical utility.
The results show that FairAA achieves a favorable trade-off between utility and
fairness, making it a promising tool for responsible representation learning in
sensitive applications.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [227] [SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics](https://arxiv.org/abs/2507.11588)
*Suyuan Zhao,Yizhen Luo,Ganbo Yang,Yan Zhong,Hao Zhou,Zaiqing Nie*

Key words: 空间转录组学, 多尺度模型, 基础模型, SE(2) Transformer, 细胞表征, 预训练数据集

TL;DR: SToFM是一个多尺度空间转录组学基础模型，通过提取宏、微和基因尺度的信息，并结合SE(2) Transformer，实现了对空间转录组数据的出色分析。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 空间转录组技术提供了丰富的单细胞生物学信息，但其数据的多尺度复杂性使得建模困难，需要整合宏、微和基因尺度信息。

Method: SToFM首先从每个切片中提取多尺度信息，构建子切片，然后使用SE(2) Transformer获取高质量细胞表征，并构建了大规模预训练数据集SToCorpus-88M。

Result: SToFM在多种下游任务中表现优异，如组织区域语义分割和细胞类型注释。

Conclusion: SToFM展示了其对空间转录组数据的全面理解能力。

Abstract: Spatial Transcriptomics (ST) technologies provide biologists with rich
insights into single-cell biology by preserving spatial context of cells.
Building foundational models for ST can significantly enhance the analysis of
vast and complex data sources, unlocking new perspectives on the intricacies of
biological tissues. However, modeling ST data is inherently challenging due to
the need to extract multi-scale information from tissue slices containing vast
numbers of cells. This process requires integrating macro-scale tissue
morphology, micro-scale cellular microenvironment, and gene-scale gene
expression profile. To address this challenge, we propose SToFM, a multi-scale
Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale
information extraction on each ST slice, to construct a set of ST sub-slices
that aggregate macro-, micro- and gene-scale information. Then an SE(2)
Transformer is used to obtain high-quality cell representations from the
sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest
high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves
outstanding performance on a variety of downstream tasks, such as tissue region
semantic segmentation and cell type annotation, demonstrating its comprehensive
understanding of ST data

</details>


### [228] [RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery](https://arxiv.org/abs/2507.11950)
*Lauren Lui,Torben Nielsen*

Key words: 非编码RNA、机器学习、宏基因组、RNAMunin、生物信息学

TL;DR: RNAMunin是一种基于机器学习的模型，仅需基因组序列即可发现非编码RNA（ncRNA），适用于大规模数据处理且无需转录组数据支持。

<details>
  <summary>Details</summary>

Main category: q-bio.GN

Motivation: 目前微生物基因组的注释多偏向于蛋白质编码基因，忽略了非编码RNA在调控生理、应激和代谢中的重要作用。

Method: 开发RNAMunin模型，训练数据来自Rfam序列及16个旧金山河口样本的长读长宏基因组（约60 Gbp）。

Result: RNAMunin能仅凭基因组序列发现ncRNA，适用于Gbp级别的长读长宏基因组拼接数据，且模型参数少、速度快。

Conclusion: RNAMunin填补了仅凭基因组序列预测ncRNA的空缺，为理解微生物全调控潜力提供了高效工具。

Abstract: Functional annotation of microbial genomes is often biased toward
protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs
(ncRNAs) that are critical for regulating bacterial and archaeal physiology,
stress response and metabolism. Identifying ncRNAs directly from genomic
sequence is a paramount challenge in bioinformatics and biology, essential for
understanding the complete regulatory potential of an organism. This paper
presents RNAMunin, a machine learning (ML) model that is capable of finding
ncRNAs using genomic sequence alone. It is also computationally viable for
large sequence datasets such as long read metagenomic assemblies with contigs
totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from
approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary
samples. We know of no other model that can detect ncRNAs based solely on
genomic sequence at this scale. Since RNAMunin only requires genomic sequence
as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do
not need transcriptomics data. We wrote this manuscript in a narrative style in
order to best convey how RNAMunin was developed and how it works in detail.
Unlike almost all current ML models, at approximately 1M parameters, RNAMunin
is very small and very fast.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [229] [Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions](https://arxiv.org/abs/2507.11783)
*Gayal Kuruppu,Neeraj Wagh,Yogatheesan Varatharajah*

Key words: EEG-FM, 自监督学习, Transformer, 脑电图, 基础模型

TL;DR: 本文综述了10种早期自监督脑电图基础模型（EEG-FM），分析了其方法、实证发现及研究空白，指出当前模型评估的局限性，并建议未来研究方向。

<details>
  <summary>Details</summary>

Main category: eess.SP

Motivation: 研究旨在解决监督式EEG编码器在稳健性和信号标注依赖性的不足，推动自监督EEG基础模型的发展，并明确其实际应用潜力。

Method: 通过序列建模（基于Transformer）和掩码序列重建的自监督学习，对10种EEG-FM进行系统综述。

Result: 当前EEG-FM评估方式不一致且有限，实际应用效果难以衡量，模型仍需标准化和现实性验证。

Conclusion: 未来需加强基准测试、工具开发、方法论优化及跨领域合作，以提升EEG-FM的实用性和推广价值。

Abstract: Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [230] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Key words: MetaLint, 代码质量分析, 指令调优, 泛化, PEP习语

TL;DR: MetaLint是一种新颖的指令跟随框架，通过动态生成的数据支持代码质量分析，能够适应新代码模式而无需重新训练。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 大型语言模型在代码生成方面表现优异，但在代码质量分析方面因静态训练数据的限制而难以适应不断变化的最佳实践。

Method: MetaLint采用指令调优技术，利用合成的linter生成数据，支持从易到难的泛化，无需重新训练即可适应新的代码模式。

Result: MetaLint在检测未见过的PEP习语时表现优异，F-score达到70.37%，召回率为70.43%。

Conclusion: MetaLint在代码质量分析领域展现了强大的泛化能力和潜力，尤其是对小规模模型的适应性。

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [231] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Key words: LLM, 代码生成, 评估基准, 多语言编程, 俄罗斯语

TL;DR: 提出了MERA Code基准，专注于评估代码生成LLM在俄罗斯语中的表现，填补了现有评估在代码质量和实际性能上的空白。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 当前LLM评估主要关注自然语言任务，忽视了代码质量和实际性能，特别是非英语语境下的表现。

Method: 设计了包含11个任务的MERA Code基准，涵盖8种编程语言，并提供开源代码库、评分系统和平台。

Result: 评估了开源和前沿API模型，揭示了它们在非英语环境下实际编码任务的局限性。

Conclusion: MERA Code的发布旨在指导未来研究，推动模型开发中的创新，并标准化评估流程。

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [232] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Key words: SaaS, 定价管理, 智能定价, 机器学习, 网页抓取

TL;DR: 本文提出了一种基于LLM的iPricing方法，自动将静态HTML定价转换为动态、机器可读的定价模型，以解决SaaS定价管理的复杂性和人为错误问题。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: SaaS市场的快速扩展导致定价管理变得复杂且易出错，缺乏自动化工具限制了定价模型的高效评估与优化。

Method: 采用LLM驱动的方法，结合网页抓取技术，开发了AI4Pricing2Yaml系统，自动提取SaaS网站中的定价要素。

Result: 验证表明，系统在30个商业SaaS网站上成功提取了150多个智能定价，但在处理幻觉、复杂结构和动态内容时仍存在挑战。

Conclusion: 自动化智能定价转换有望提升SaaS定价管理的一致性和可扩展性，未来研究将优化提取能力和系统适应性。

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [233] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Key words: code generation, library versioning, LLMs, Python, benchmark

TL;DR: GitChameleon是一个新的数据集，用于评估AI在特定版本库条件下的代码生成能力，强调了版本适应的挑战性。

<details>
  <summary>Details</summary>

Main category: cs.SE

Motivation: 软件库的快速更新对代码生成提出了持续适应的需求，现有基准缺乏基于执行的版本兼容性评估。

Method: 引入GitChameleon数据集，包含328个Python代码完成问题，每个问题与特定库版本关联并配有可执行单元测试。

Result: 当前先进的AI系统在此任务上表现不佳，企业模型的基线成功率仅为48-51%。

Conclusion: GitChameleon为理解代码库动态性挑战提供了新的基准，支持更适应性强的AI代码生成方法的开发。

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [234] [Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing](https://arxiv.org/abs/2507.11780)
*Justin Whitehouse,Morgane Austern,Vasilis Syrgkanis*

Key words: 因果推断、最优策略值、软最大平滑、置信区间

TL;DR: 本文研究了一种基于软最大平滑的估计器，用于解决因果推断中优化治疗策略值的置信区间构建问题，克服了非可微性和现有方法的局限性。

<details>
  <summary>Details</summary>

Main category: econ.EM

Motivation: 在因果推断中，构建最优治疗策略值的置信区间对开发最大化奖励的个性化治疗方案至关重要。但由于目标函数的非可微性，传统半参数方法无法直接应用。

Method: 通过软最大平滑技术精确控制一阶偏差和二阶余项，提出一种新的估计器，能够估计涉及干扰分量的评分最大值参数。

Result: 该估计器实现了√n收敛速度，避免了对干扰函数的参数化假设或不现实的边界条件，并且通常具有统计效率。

Conclusion: 软最大平滑方法为构建最优治疗策略值的置信区间提供了一种可行且高效的技术路径。

Abstract: Constructing confidence intervals for the value of an optimal treatment
policy is an important problem in causal inference. Insight into the optimal
policy value can guide the development of reward-maximizing, individualized
treatment regimes. However, because the functional that defines the optimal
value is non-differentiable, standard semi-parametric approaches for performing
inference fail to be directly applicable. Existing approaches for handling this
non-differentiability fall roughly into two camps. In one camp are estimators
based on constructing smooth approximations of the optimal value. These
approaches are computationally lightweight, but typically place unrealistic
parametric assumptions on outcome regressions. In another camp are approaches
that directly de-bias the non-smooth objective. These approaches don't place
parametric assumptions on nuisance functions, but they either require the
computation of intractably-many nuisance estimates, assume unrealistic
$L^\infty$ nuisance convergence rates, or make strong margin assumptions that
prohibit non-response to a treatment. In this paper, we revisit the problem of
constructing smooth approximations of non-differentiable functionals. By
carefully controlling first-order bias and second-order remainders, we show
that a softmax smoothing-based estimator can be used to estimate parameters
that are specified as a maximum of scores involving nuisance components. In
particular, this includes the value of the optimal treatment policy as a
special case. Our estimator obtains $\sqrt{n}$ convergence rates, avoids
parametric restrictions/unrealistic margin assumptions, and is often
statistically efficient.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [235] [Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length](https://arxiv.org/abs/2507.12442)
*Saptarshi Mitra,Rachid Karami,Haocheng Xu,Sitao Huang,Hyoukjun Kwon*

Key words: 长上下文处理, Transformer, State Space Models, 硬件性能, 基准测试

TL;DR: 论文针对长上下文输入任务，比较了Transformer、SSM和混合模型在消费级硬件上的性能，发现SSM在长序列处理上更具优势。

<details>
  <summary>Details</summary>

Main category: cs.AR

Motivation: 传统Transformer因二次复杂度和高内存需求不适用于本地设备的长上下文处理，亟需新架构性能评估。

Method: 对Transformer、SSM和混合模型在消费级和嵌入式GPU上进行系统性的基准测试。

Result: SSM在处理220K长序列时表现优于Transformer，且在长上下文（约57K tokens）中速度可快4倍。

Conclusion: SSM具有本地设备长上下文处理的潜力，硬件定制优化是未来重点。

Abstract: The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.

</details>
